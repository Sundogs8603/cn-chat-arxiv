<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>DiffuserLite&#26159;&#19968;&#20010;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#25193;&#25955;&#35268;&#21010;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#35745;&#21010;&#32454;&#21270;&#36807;&#31243;&#65288;PRP&#65289;&#26469;&#25552;&#39640;&#20915;&#31574;&#39057;&#29575;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26694;&#26550;&#65292;&#23427;&#21482;&#20135;&#29983;&#20102;&#24456;&#23567;&#30340;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15443</link><description>&lt;p&gt;
DiffuserLite: &#23454;&#26102;&#25193;&#25955;&#35268;&#21010;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
DiffuserLite: Towards Real-time Diffusion Planning. (arXiv:2401.15443v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15443
&lt;/p&gt;
&lt;p&gt;
DiffuserLite&#26159;&#19968;&#20010;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#25193;&#25955;&#35268;&#21010;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#35745;&#21010;&#32454;&#21270;&#36807;&#31243;&#65288;PRP&#65289;&#26469;&#25552;&#39640;&#20915;&#31574;&#39057;&#29575;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26694;&#26550;&#65292;&#23427;&#21482;&#20135;&#29983;&#20102;&#24456;&#23567;&#30340;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#35268;&#21010;&#34987;&#35748;&#20026;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#26377;&#25928;&#30340;&#20915;&#31574;&#33539;&#24335;&#12290;&#38271;&#26102;&#38388;&#36328;&#24230;&#36712;&#36857;&#30340;&#39640;&#36136;&#37327;&#26465;&#20214;&#29983;&#25104;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#35268;&#21010;&#26041;&#27861;&#30001;&#20110;&#36845;&#20195;&#25277;&#26679;&#25104;&#26412;&#26114;&#36149;&#32780;&#23548;&#33268;&#20915;&#31574;&#39057;&#29575;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DiffuserLite&#65292;&#19968;&#20010;&#24555;&#36895;&#32780;&#36731;&#37327;&#32423;&#30340;&#25193;&#25955;&#35268;&#21010;&#26694;&#26550;&#12290;DiffuserLite&#20351;&#29992;&#20102;&#19968;&#20010;&#35745;&#21010;&#32454;&#21270;&#36807;&#31243;&#65288;PRP&#65289;&#26469;&#29983;&#25104;&#31895;&#21040;&#32454;&#31890;&#24230;&#30340;&#36712;&#36857;&#65292;&#36825;&#26174;&#33879;&#20943;&#23569;&#20102;&#20887;&#20313;&#20449;&#24687;&#30340;&#24314;&#27169;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#20915;&#31574;&#39057;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#26694;&#26550;&#30456;&#27604;&#65292;DiffuserLite&#20165;&#20135;&#29983;&#20102;$0.88\%$&#30340;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#24179;&#22343;&#20915;&#31574;&#39057;&#29575;&#36798;&#21040;&#20102;122Hz&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24178;&#20928;DiffuserLite&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;...
&lt;/p&gt;
&lt;p&gt;
Diffusion planning has been recognized as an effective decision-making paradigm in various domains. The high-quality conditional generation capability of long-horizon trajectories makes it a promising research direction. However, existing diffusion planning methods suffer from low decision-making frequencies because of the expensive iterative sampling cost. To address this issue, we introduce DiffuserLite, a fast and lightweight diffusion planning framework. DiffuserLite employs a planning refinement process (PRP) to generate coarse-to-fine-grained trajectories, which significantly reduces the modeling of redundant information and leads to notable increases in decision-making frequency. Our experimental results demonstrate that DiffuserLite incurs only $0.88\%$ of the runtime cost compared to previous frameworks, achieves an average decision-making frequency of $122$Hz, and reaches state-of-the-art performance on D4RL benchmarks. In addition, our clean DiffuserLite framework can serve 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#37325;&#29992;&#30340;&#24494;&#26381;&#21153;&#26550;&#26500;&#65292;&#20351;&#29992;&#29289;&#32852;&#32593;&#33539;&#24335;&#36827;&#34892;&#26631;&#20934;&#21270;&#65292;&#24182;&#25903;&#25345;&#22797;&#26434;&#20107;&#20214;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23454;&#26102;&#29289;&#32852;&#32593;&#25968;&#25454;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.15390</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#29289;&#32852;&#32593;&#25968;&#25454;&#22788;&#29702;&#30340;&#24494;&#26381;&#21153;&#26550;&#26500;&#65306;&#22522;&#20110;&#21487;&#37325;&#29992;&#29289;&#32852;&#32593;&#30340;&#26234;&#33021;&#28207;&#21475;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A microservice architecture for real-time IoT data processing: A reusable Web of things approach for smart ports. (arXiv:2401.15390v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#37325;&#29992;&#30340;&#24494;&#26381;&#21153;&#26550;&#26500;&#65292;&#20351;&#29992;&#29289;&#32852;&#32593;&#33539;&#24335;&#36827;&#34892;&#26631;&#20934;&#21270;&#65292;&#24182;&#25903;&#25345;&#22797;&#26434;&#20107;&#20214;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23454;&#26102;&#29289;&#32852;&#32593;&#25968;&#25454;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#20449;&#21644;&#29289;&#32852;&#32593;&#30340;&#37325;&#22823;&#36827;&#23637;&#20351;&#24471;&#26234;&#33021;&#22478;&#24066;&#22330;&#26223;&#24471;&#20197;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#25552;&#20379;&#36825;&#20123;&#26234;&#33021;&#26381;&#21153;&#38656;&#35201;&#20197;&#39640;&#25928;&#12289;&#20114;&#25805;&#20316;&#21644;&#23454;&#26102;&#30340;&#26041;&#24335;&#36827;&#34892;&#65292;&#36825;&#26159;&#19968;&#20010;&#21069;&#27839;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;&#34429;&#28982;&#19968;&#20123;&#36719;&#20214;&#26550;&#26500;&#22312;&#36825;&#20010;&#39046;&#22495;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#22312;&#21487;&#37325;&#29992;&#24615;&#21644;&#32500;&#25252;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#21253;&#25324;&#29420;&#31435;&#27169;&#22359;&#30340;&#32500;&#25252;&#25110;&#28436;&#36827;&#26102;&#30340;&#31995;&#32479;&#20572;&#26426;&#38656;&#27714;&#65292;&#20197;&#21450;&#32570;&#20047;&#25509;&#21475;&#20114;&#25805;&#20316;&#24615;&#30340;&#26631;&#20934;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#21487;&#37325;&#29992;&#30340;&#24494;&#26381;&#21153;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#29289;&#32852;&#32593;&#33539;&#24335;&#36827;&#34892;&#26631;&#20934;&#21270;&#65292;&#24182;&#25903;&#25345;&#22797;&#26434;&#20107;&#20214;&#22788;&#29702;&#25216;&#26415;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23454;&#26102;&#25968;&#25454;&#22788;&#29702;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#20010;&#25552;&#35758;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#20840;&#21487;&#37325;&#29992;&#30340;&#24494;&#26381;&#21153;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Major advances in telecommunications and the Internet of Things have given rise to numerous smart city scenarios in which smart services are provided. What was once a dream for the future has now become reality. However, the need to provide these smart services quickly, efficiently, in an interoperable manner and in real time is a cutting-edge technological challenge. Although some software architectures offer solutions in this area, these are often limited in terms of reusability and maintenance by independent modules, involving the need for system downtime when maintaining or evolving, as well as by a lack of standards in terms of the interoperability of their interface. In this paper, we propose a fully reusable microservice architecture, standardized through the use of the Web of things paradigm, and with high efficiency in real-time data processing, supported by complex event processing techniques. To illustrate the proposal, we present a fully reusable implementation of the micro
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.15378</link><description>&lt;p&gt;
&#22522;&#20110;RAG&#30340;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#25552;&#26696;&#65306;MufassirQAS LLM
&lt;/p&gt;
&lt;p&gt;
A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15378
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#29702;&#35299;&#23447;&#25945;&#23384;&#22312;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#30340;&#25361;&#25112;&#12290;&#38382;&#31572;&#26426;&#22120;&#20154;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#12290;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24314;&#31435;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#29992;&#20110;&#23447;&#25945;&#21551;&#33945;&#30340;&#38382;&#39064;&#22238;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;LLM&#20063;&#26377;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#20542;&#21521;&#65292;&#31216;&#20026;&#24187;&#35273;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#21487;&#33021;&#21253;&#21547;&#20398;&#36785;&#20010;&#20154;&#23447;&#25945;&#20449;&#20208;&#12289;&#36328;&#23447;&#27966;&#20914;&#31361;&#21644;&#26377;&#20105;&#35758;&#25110;&#25935;&#24863;&#30340;&#35805;&#39064;&#30340;&#20869;&#23481;&#12290;&#23427;&#38656;&#35201;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#65292;&#32780;&#19981;&#20250;&#23459;&#25196;&#20167;&#24680;&#35328;&#35770;&#25110;&#20882;&#29359;&#26576;&#20123;&#32676;&#20307;&#30340;&#20154;&#25110;&#20182;&#20204;&#30340;&#20449;&#20208;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#25968;&#25454;&#24211;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#26469;&#25552;&#39640;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#30340;&#38382;&#31572;&#31995;&#32479;&#31216;&#20026;"MufassirQAS"&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#35780;&#20272;&#35813;&#31995;&#32479;&#24182;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#23447;&#25945;&#34892;&#19994;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#30340;&#20381;&#36182;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36866;&#24403;&#20381;&#36182;&#12290;&#35813;&#23450;&#20041;&#20998;&#31163;&#20102;&#20381;&#36182;&#30340;&#27010;&#24565;&#21644;&#20154;&#31867;&#22312;&#24418;&#25104;&#20934;&#30830;&#20449;&#24565;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20026;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20114;&#34917;&#24615;&#21644;&#20381;&#36182;&#24615;&#30340;&#30740;&#31350;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.15356</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20381;&#36182;&#30340;&#32479;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Statistical Framework for Measuring AI Reliance. (arXiv:2401.15356v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15356
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#30340;&#20381;&#36182;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36866;&#24403;&#20381;&#36182;&#12290;&#35813;&#23450;&#20041;&#20998;&#31163;&#20102;&#20381;&#36182;&#30340;&#27010;&#24565;&#21644;&#20154;&#31867;&#22312;&#24418;&#25104;&#20934;&#30830;&#20449;&#24565;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20026;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20114;&#34917;&#24615;&#21644;&#20381;&#36182;&#24615;&#30340;&#30740;&#31350;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#32463;&#24120;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24110;&#21161;&#19979;&#20570;&#20915;&#31574;&#12290;&#19968;&#20010;&#24120;&#35265;&#27169;&#24335;&#26159;&#20154;&#24037;&#26234;&#33021;&#21521;&#20154;&#31867;&#25512;&#33616;&#34892;&#21160;&#65292;&#32780;&#20154;&#31867;&#20445;&#30041;&#23545;&#26368;&#32456;&#20915;&#31574;&#30340;&#25511;&#21046;&#26435;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30830;&#35748;&#65292;&#30830;&#20445;&#20154;&#31867;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36866;&#24403;&#20381;&#36182;&#26159;&#23454;&#29616;&#20114;&#34917;&#24615;&#33021;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#30446;&#21069;&#22312;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#36866;&#24403;&#20381;&#36182;&#30340;&#23450;&#20041;&#32570;&#20047;&#24418;&#24335;&#21270;&#30340;&#32479;&#35745;&#22522;&#30784;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#30683;&#30462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#30340;&#20381;&#36182;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#23427;&#23558;&#20381;&#36182;&#30340;&#27010;&#24565;&#19982;&#20154;&#31867;&#22312;&#21306;&#20998;&#20449;&#21495;&#24182;&#24418;&#25104;&#20934;&#30830;&#20449;&#24565;&#30340;&#25361;&#25112;&#20998;&#24320;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#20135;&#29983;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#26469;&#25351;&#23548;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20114;&#34917;&#24615;&#21644;&#20381;&#36182;&#24615;&#30340;&#30740;&#31350;&#35774;&#35745;&#21644;&#35299;&#37322;&#12290;&#21033;&#29992;&#26368;&#36817;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#20915;&#31574;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Humans frequently make decisions with the aid of artificially intelligent (AI) systems. A common pattern is for the AI to recommend an action to the human who retains control over the final decision. Researchers have identified ensuring that a human has appropriate reliance on an AI as a critical component of achieving complementary performance. We argue that the current definition of appropriate reliance used in such research lacks formal statistical grounding and can lead to contradictions. We propose a formal definition of reliance, based on statistical decision theory, which separates the concepts of reliance as the probability the decision-maker follows the AI's prediction from challenges a human may face in differentiating the signals and forming accurate beliefs about the situation. Our definition gives rise to a framework that can be used to guide the design and interpretation of studies on human-AI complementarity and reliance. Using recent AI-advised decision making studies f
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#23545;&#20110;&#30701;&#25991;&#26412;&#21644;&#36328;&#35821;&#35328;&#25991;&#26723;&#31561;&#21508;&#31181;&#22330;&#26223;&#25552;&#20379;&#20102;&#31995;&#32479;&#24615;&#30340;&#32452;&#32455;&#21644;&#20171;&#32461;&#65292;&#24182;&#35752;&#35770;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#31995;&#21015;&#28909;&#38376;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15351</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#32508;&#36848;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey on Neural Topic Models: Methods, Applications, and Challenges. (arXiv:2401.15351v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15351
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#23545;&#20110;&#30701;&#25991;&#26412;&#21644;&#36328;&#35821;&#35328;&#25991;&#26723;&#31561;&#21508;&#31181;&#22330;&#26223;&#25552;&#20379;&#20102;&#31995;&#32479;&#24615;&#30340;&#32452;&#32455;&#21644;&#20171;&#32461;&#65292;&#24182;&#35752;&#35770;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#31995;&#21015;&#28909;&#38376;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#20960;&#21313;&#24180;&#26469;&#19968;&#30452;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#28508;&#22312;&#20027;&#39064;&#21644;&#25512;&#26029;&#25991;&#26723;&#30340;&#20027;&#39064;&#27604;&#20363;&#12290;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#26512;&#21644;&#19978;&#19979;&#25991;&#25512;&#33616;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#23835;&#36215;&#20419;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#8212;&#8212;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;(NTMs)&#30340;&#20986;&#29616;&#12290;&#19982;&#20256;&#32479;&#30340;&#20027;&#39064;&#27169;&#22411;&#19981;&#21516;&#65292;NTMs&#30452;&#25509;&#20248;&#21270;&#21442;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#27169;&#22411;&#29305;&#23450;&#30340;&#25512;&#23548;&#12290;&#36825;&#20351;&#24471;NTMs&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#20851;&#27880;&#24182;&#20135;&#29983;&#20102;&#20016;&#23500;&#30340;&#26032;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#30740;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26681;&#25454;&#32593;&#32476;&#32467;&#26500;&#31995;&#32479;&#22320;&#32452;&#32455;&#20102;&#24403;&#21069;NTM&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#38024;&#23545;&#30701;&#25991;&#26412;&#21644;&#36328;&#35821;&#35328;&#25991;&#26723;&#31561;&#21508;&#31181;&#22330;&#26223;&#30340;NTMs&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#31995;&#21015;&#28909;&#38376;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models have been prevalent for decades to discover latent topics and infer topic proportions of documents in an unsupervised fashion. They have been widely used in various applications like text analysis and context recommendation. Recently, the rise of neural networks has facilitated the emergence of a new research field -- Neural Topic Models (NTMs). Different from conventional topic models, NTMs directly optimize parameters without requiring model-specific derivations. This endows NTMs with better scalability and flexibility, resulting in significant research attention and plentiful new methods and applications. In this paper, we present a comprehensive survey on neural topic models concerning methods, applications, and challenges. Specifically, we systematically organize current NTM methods according to their network structures and introduce the NTMs for various scenarios like short texts and cross-lingual documents. We also discuss a wide range of popular applications built 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26159;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#31639;&#27861;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;&#21387;&#32553;&#31639;&#27861;&#30340;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#24635;&#32467;&#20102;&#21508;&#20010;&#31639;&#27861;&#30340;&#25972;&#20307;&#36235;&#21183;&#21644;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.15347</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#31639;&#27861;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Compression Algorithms for Language Models. (arXiv:2401.15347v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15347
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#31639;&#27861;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;&#21387;&#32553;&#31639;&#27861;&#30340;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#24635;&#32467;&#20102;&#21508;&#20010;&#31639;&#27861;&#30340;&#25972;&#20307;&#36235;&#21183;&#21644;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#65311;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#31639;&#27861;&#25968;&#37327;&#27491;&#22312;&#24555;&#36895;&#22686;&#38271;&#65292;&#20197;&#20174;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#30340;&#26174;&#33879;&#36827;&#23637;&#20013;&#21463;&#30410;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#24222;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21103;&#20316;&#29992;&#65292;&#27604;&#22914;&#22686;&#21152;&#30340;&#30899;&#25490;&#25918;&#21644;&#26114;&#36149;&#30340;&#32500;&#25252;&#36153;&#29992;&#12290;&#34429;&#28982;&#35768;&#22810;&#21387;&#32553;&#31639;&#27861;&#22312;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#36807;&#22810;&#30340;&#31639;&#27861;&#65292;&#37096;&#20998;&#30340;&#38590;&#39064;&#22312;&#20110;&#25429;&#25417;&#26032;&#20852;&#36235;&#21183;&#24182;&#35782;&#21035;&#20854;&#22522;&#26412;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#12289;&#20302;&#31209;&#36924;&#36817;&#12289;&#21442;&#25968;&#20849;&#20139;&#21644;&#39640;&#25928;&#26550;&#26500;&#35774;&#35745;&#22312;&#20869;&#30340;&#22810;&#31181;&#21387;&#32553;&#31639;&#27861;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#24635;&#32467;&#12290;&#25105;&#20204;&#19981;&#20165;&#24635;&#32467;&#20102;&#21508;&#31181;&#21387;&#32553;&#31639;&#27861;&#30340;&#25972;&#20307;&#36235;&#21183;&#65292;&#36824;&#36873;&#25321;&#20102;&#20195;&#34920;&#24615;&#31639;&#27861;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#27599;&#20010;&#31867;&#21035;&#30340;&#21387;&#32553;&#31639;&#27861;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LARA&#30340;&#33258;&#21160;&#20998;&#26512;&#31995;&#32479;&#65292;&#29992;&#20110;&#38271;&#26399;&#20135;&#21069;&#30005;&#23376;&#32974;&#20799;&#24515;&#29575;&#30417;&#27979;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22788;&#29702;&#38271;&#26399;&#30340;FHR&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#23545;&#32974;&#20799;&#29366;&#24577;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.15337</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20449;&#24687;&#34701;&#21512;&#30340;&#38271;&#26399;&#20135;&#21069;&#30005;&#23376;&#32974;&#20799;&#24515;&#29575;&#30417;&#27979;&#20581;&#24247;&#30417;&#27979;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning with Information Fusion and Model Interpretation for Health Monitoring of Fetus based on Long-term Prenatal Electronic Fetal Heart Rate Monitoring Data. (arXiv:2401.15337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LARA&#30340;&#33258;&#21160;&#20998;&#26512;&#31995;&#32479;&#65292;&#29992;&#20110;&#38271;&#26399;&#20135;&#21069;&#30005;&#23376;&#32974;&#20799;&#24515;&#29575;&#30417;&#27979;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22788;&#29702;&#38271;&#26399;&#30340;FHR&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#23545;&#32974;&#20799;&#29366;&#24577;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32974;&#20799;&#24515;&#29575;&#65288;FHR&#65289;&#30340;&#38271;&#26399;&#30417;&#27979;&#22312;&#20135;&#21069;&#26399;&#38388;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20854;&#20013;&#30005;&#23376;FHR&#30417;&#27979;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#19982;&#30701;&#26399;&#30417;&#27979;&#30456;&#27604;&#65292;&#36825;&#31181;&#36830;&#32493;&#30417;&#27979;&#21487;&#20197;&#25910;&#38598;&#26356;&#38271;&#26102;&#38388;&#30340;&#32974;&#20799;&#24515;&#29575;&#25968;&#25454;&#65292;&#20174;&#32780;&#26356;&#20840;&#38754;&#22320;&#20102;&#35299;&#32974;&#20799;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#38271;&#26399;&#20135;&#21069;&#32974;&#20799;&#24515;&#29575;&#30417;&#27979;&#30340;&#35299;&#37322;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#32570;&#20047;&#30456;&#24212;&#30340;&#20020;&#24202;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#36830;&#32493;&#30417;&#27979;&#20135;&#29983;&#30340;&#22823;&#37327;&#25968;&#25454;&#22312;&#25163;&#21160;&#20998;&#26512;&#26102;&#23545;&#20020;&#24202;&#24037;&#20316;&#36896;&#25104;&#20102;&#37325;&#22823;&#36127;&#25285;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LARA&#65288;&#38271;&#26399;&#20135;&#21069;&#39118;&#38505;&#20998;&#26512;&#31995;&#32479;&#65289;&#30340;&#33258;&#21160;&#20998;&#26512;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#27861;&#12290;LARA&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#25104;&#29087;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#65292;&#23427;&#23558;&#38271;&#26399;&#30340;FHR&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term fetal heart rate (FHR) monitoring during the antepartum period, increasingly popularized by electronic FHR monitoring, represents a growing approach in FHR monitoring. This kind of continuous monitoring, in contrast to the short-term one, collects an extended period of fetal heart data. This offers a more comprehensive understanding of fetus's conditions. However, the interpretation of long-term antenatal fetal heart monitoring is still in its early stages, lacking corresponding clinical standards. Furthermore, the substantial amount of data generated by continuous monitoring imposes a significant burden on clinical work when analyzed manually. To address above challenges, this study develops an automatic analysis system named LARA (Long-term Antepartum Risk Analysis system) for continuous FHR monitoring, combining deep learning and information fusion methods. LARA's core is a well-established convolutional neural network (CNN) model. It processes long-term FHR data as input 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;L-AutoDA&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35774;&#35745;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36845;&#20195;&#20132;&#20114;&#65292;L-AutoDA&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#31454;&#20105;&#24615;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#26174;&#31034;&#20986;&#22312;&#25104;&#21151;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.15335</link><description>&lt;p&gt;
L-AutoDA: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks. (arXiv:2401.15335v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;L-AutoDA&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35774;&#35745;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36845;&#20195;&#20132;&#20114;&#65292;L-AutoDA&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#31454;&#20105;&#24615;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#26174;&#31034;&#20986;&#22312;&#25104;&#21151;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23545;&#25239;&#25915;&#20987;&#23545;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#21644;&#23433;&#20840;&#24615;&#25552;&#20986;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#20915;&#31574;&#22411;&#25915;&#20987;&#21482;&#38656;&#35201;&#27169;&#22411;&#30340;&#20915;&#31574;&#21453;&#39304;&#65292;&#32780;&#19981;&#38656;&#35201;&#35814;&#32454;&#30340;&#27010;&#29575;&#25110;&#20998;&#25968;&#65292;&#22240;&#27492;&#29305;&#21035;&#38590;&#20197;&#38450;&#24481;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;L-AutoDA&#65288;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#33258;&#21160;&#35774;&#35745;&#36825;&#20123;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#36827;&#21270;&#26694;&#26550;&#20013;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36845;&#20195;&#20132;&#20114;&#65292;L-AutoDA&#33021;&#22815;&#39640;&#25928;&#22320;&#33258;&#21160;&#35774;&#35745;&#20986;&#31454;&#20105;&#24615;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#20943;&#23569;&#20154;&#24037;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;L-AutoDA&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20986;&#22312;&#25104;&#21151;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#25239;&#25915;&#20987;&#29983;&#25104;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of machine learning, adversarial attacks present a significant challenge to model robustness and security. Decision-based attacks, which only require feedback on the decision of a model rather than detailed probabilities or scores, are particularly insidious and difficult to defend against. This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models (LLMs) to automate the design of these attacks. By iteratively interacting with LLMs in an evolutionary framework, L-AutoDA automatically designs competitive attack algorithms efficiently without much human effort. We demonstrate the efficacy of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline methods in both success rate and computational efficiency. Our findings underscore the potential of language models as tools for adversarial attack generation and highli
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;(DAT)&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#22024;&#26434;&#29615;&#22659;&#20013;&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39069;&#22806;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#21644;&#28155;&#21152;&#21512;&#25104;&#30340;&#22024;&#26434;&#38899;&#20048;&#25968;&#25454;&#65292;&#33719;&#24471;&#20102;&#40065;&#26834;&#30340;&#38899;&#20048;&#34920;&#31034;&#65292;&#24182;&#22312;&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#26041;&#38754;&#23637;&#29616;&#20102;&#22686;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15323</link><description>&lt;p&gt;
&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#65306;&#36890;&#36807;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;&#23398;&#20064;&#40065;&#26834;&#30340;&#38899;&#20048;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Music Auto-Tagging with Robust Music Representation Learned via Domain Adversarial Training. (arXiv:2401.15323v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;(DAT)&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#22024;&#26434;&#29615;&#22659;&#20013;&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39069;&#22806;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#21644;&#28155;&#21152;&#21512;&#25104;&#30340;&#22024;&#26434;&#38899;&#20048;&#25968;&#25454;&#65292;&#33719;&#24471;&#20102;&#40065;&#26834;&#30340;&#38899;&#20048;&#34920;&#31034;&#65292;&#24182;&#22312;&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#26041;&#38754;&#23637;&#29616;&#20102;&#22686;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#23545;&#20110;&#22686;&#24378;&#38899;&#20048;&#21457;&#29616;&#21644;&#25512;&#33616;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;(MIR)&#27169;&#22411;&#22312;&#22810;&#23186;&#20307;&#20869;&#23481;&#20013;&#23384;&#22312;&#30340;&#29615;&#22659;&#22122;&#22768;&#21644;&#35821;&#38899;&#22768;&#38899;&#31561;&#29616;&#23454;&#19990;&#30028;&#22122;&#22768;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#35821;&#38899;&#30456;&#20851;&#20219;&#21153;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22024;&#26434;&#29615;&#22659;&#20013;&#30340;&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#23558;&#39046;&#22495;&#23545;&#25239;&#35757;&#32451;(DAT)&#38598;&#25104;&#21040;&#38899;&#20048;&#39046;&#22495;&#20013;&#65292;&#20351;&#24471;&#40065;&#26834;&#30340;&#38899;&#20048;&#34920;&#31034;&#33021;&#22815;&#25269;&#25239;&#22122;&#22768;&#12290;&#19982;&#20197;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#36824;&#28041;&#21450;&#39046;&#22495;&#20998;&#31867;&#22120;&#30340;&#39069;&#22806;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#20197;&#36991;&#20813;&#21518;&#32493;&#38454;&#27573;&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;&#28155;&#21152;&#21508;&#31181;&#21512;&#25104;&#30340;&#22024;&#26434;&#38899;&#20048;&#25968;&#25454;&#25913;&#21892;&#20102;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#26080;&#26631;&#31614;&#30340;&#22024;&#26434;&#38899;&#20048;&#25968;&#25454;&#65292;&#22312;&#38899;&#20048;&#33258;&#21160;&#26631;&#35760;&#26041;&#38754;&#23637;&#29616;&#20102;&#22686;&#24378;&#30340;&#24615;&#33021;&#12290;&#22312;&#34917;&#20805;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#39069;&#22806;&#23454;&#39564;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music auto-tagging is crucial for enhancing music discovery and recommendation. Existing models in Music Information Retrieval (MIR) struggle with real-world noise such as environmental and speech sounds in multimedia content. This study proposes a method inspired by speech-related tasks to enhance music auto-tagging performance in noisy settings. The approach integrates Domain Adversarial Training (DAT) into the music domain, enabling robust music representations that withstand noise. Unlike previous research, this approach involves an additional pretraining phase for the domain classifier, to avoid performance degradation in the subsequent phase. Adding various synthesized noisy music data improves the model's generalization across different noise levels. The proposed architecture demonstrates enhanced performance in music auto-tagging by effectively utilizing unlabeled noisy music data. Additional experiments with supplementary unlabeled data further improves the model's performance
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#21943;&#28293;&#25216;&#26415;&#30456;&#32467;&#21512;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#21644;3D&#39640;&#26031;&#21943;&#28293;&#65292;&#21487;&#20197;&#22312;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#36896;&#20986;&#26080;&#21487;&#27604;&#25311;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#23454;&#29616;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#31649;&#29702;&#21644;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2401.15318</link><description>&lt;p&gt;
&#39640;&#26031;&#21943;&#28293;&#65306;&#21033;&#29992;&#39640;&#26031;&#39128;&#33853;&#21160;&#24577;&#21512;&#25104;&#27969;&#20307;
&lt;/p&gt;
&lt;p&gt;
Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting. (arXiv:2401.15318v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15318
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#21943;&#28293;&#25216;&#26415;&#30456;&#32467;&#21512;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#21644;3D&#39640;&#26031;&#21943;&#28293;&#65292;&#21487;&#20197;&#22312;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#36896;&#20986;&#26080;&#21487;&#27604;&#25311;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#23454;&#29616;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#31649;&#29702;&#21644;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#19982;3D&#39640;&#26031;&#21943;&#28293;&#65288;3DGS&#65289;&#30456;&#32467;&#21512;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#22312;&#20351;&#29992;3DGS&#37325;&#24314;&#30340;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#24314;&#26032;&#25928;&#26524;&#12290;&#21033;&#29992;&#39640;&#26031;&#21943;&#28293;&#21644;&#22522;&#20110;&#20301;&#32622;&#30340;&#21160;&#21147;&#23398;&#65288;PBD&#65289;&#22312;&#24213;&#23618;&#34920;&#31034;&#20013;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#20197;&#36830;&#36143;&#30340;&#26041;&#24335;&#31649;&#29702;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#12290;&#31867;&#20284;&#20110;&#39640;&#26031;&#30528;&#33394;&#22120;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#27861;&#32447;&#22686;&#24378;&#27599;&#20010;&#39640;&#26031;&#26680;&#65292;&#23558;&#26680;&#30340;&#26041;&#21521;&#19982;&#34920;&#38754;&#27861;&#32447;&#23545;&#40784;&#65292;&#20197;&#25913;&#36827;PBD&#27169;&#25311;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#28040;&#38500;&#20102;&#22266;&#20307;&#26059;&#36716;&#21464;&#24418;&#20135;&#29983;&#30340;&#23574;&#23792;&#22122;&#22768;&#12290;&#23427;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#28210;&#26579;&#38598;&#25104;&#21040;&#27969;&#20307;&#30340;&#21160;&#24577;&#34920;&#38754;&#21453;&#23556;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#30495;&#23454;&#22320;&#22797;&#29616;&#21160;&#24577;&#27969;&#20307;&#19978;&#30340;&#34920;&#38754;&#20142;&#28857;&#65292;&#24182;&#20419;&#36827;&#22330;&#26223;&#23545;&#35937;&#19982;&#27969;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian splatting and position-based dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, pl
&lt;/p&gt;</description></item><item><title>SupplyGraph&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#25968;&#25454;&#65292;&#29992;&#20110;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.15299</link><description>&lt;p&gt;
SupplyGraph: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks. (arXiv:2401.15299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15299
&lt;/p&gt;
&lt;p&gt;
SupplyGraph&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#25968;&#25454;&#65292;&#29992;&#20110;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#22914;&#36816;&#36755;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;GNNs&#24212;&#29992;&#20110;&#20379;&#24212;&#38142;&#32593;&#32476;&#26041;&#38754;&#65292;&#30446;&#21069;&#23578;&#32570;&#20047;&#30740;&#31350;&#12290;&#20379;&#24212;&#38142;&#32593;&#32476;&#22312;&#32467;&#26500;&#19978;&#31867;&#20284;&#20110;&#22270;&#24418;&#65292;&#20351;&#20854;&#25104;&#20026;&#24212;&#29992;GNN&#26041;&#27861;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#36825;&#20026;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#24320;&#36767;&#20102;&#26080;&#38480;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#27492;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#22312;&#20110;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#20419;&#36827;&#20351;&#29992;GNN&#26469;&#30740;&#31350;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#30340;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#29992;&#20110;&#29983;&#20135;&#30446;&#30340;&#30340;&#20379;&#24212;&#38142;&#35268;&#21010;&#30340;&#26102;&#38388;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained traction across different domains such as transportation, bio-informatics, language processing, and computer vision. However, there is a noticeable absence of research on applying GNNs to supply chain networks. Supply chain networks are inherently graph-like in structure, making them prime candidates for applying GNN methodologies. This opens up a world of possibilities for optimizing, predicting, and solving even the most complex supply chain problems. A major setback in this approach lies in the absence of real-world benchmark datasets to facilitate the research and resolution of supply chain problems using GNNs. To address the issue, we present a real-world benchmark dataset for temporal tasks, obtained from one of the leading FMCG companies in Bangladesh, focusing on supply chain planning for production purposes. The dataset includes temporal data as node features to enable sales predictions, production planning, and the identification of fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#24403;&#21069;&#22522;&#20110;3D&#39592;&#26550;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#26041;&#27861;&#12289;&#27169;&#22411;&#35774;&#35745;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#30456;&#20851;&#30740;&#31350;&#24635;&#32467;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2401.15296</link><description>&lt;p&gt;
&#22522;&#20110;3D&#39592;&#26550;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#65306;&#26041;&#27861;&#12289;&#35774;&#35745;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on 3D Skeleton Based Person Re-Identification: Approaches, Designs, Challenges, and Future Directions. (arXiv:2401.15296v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#24403;&#21069;&#22522;&#20110;3D&#39592;&#26550;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#26041;&#27861;&#12289;&#27169;&#22411;&#35774;&#35745;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#30456;&#20851;&#30740;&#31350;&#24635;&#32467;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;3D&#39592;&#26550;&#36827;&#34892;&#20154;&#21592;&#20877;&#35782;&#21035;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#24341;&#36215;&#20102;&#27169;&#24335;&#35782;&#21035;&#31038;&#21306;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#39592;&#26550;&#24314;&#27169;&#21644;&#29305;&#24449;&#23398;&#20064;&#20013;&#31361;&#20986;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20855;&#26377;&#29420;&#29305;&#20248;&#21183;&#30340;&#22522;&#20110;3D&#39592;&#26550;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#65288;SRID&#65289;&#26041;&#27861;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23545;&#36825;&#20123;&#30740;&#31350;&#21450;&#20854;&#25361;&#25112;&#36827;&#34892;&#32508;&#21512;&#24635;&#32467;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#23545;&#24403;&#21069;SRID&#26041;&#27861;&#12289;&#27169;&#22411;&#35774;&#35745;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#35843;&#30740;&#65292;&#35797;&#22270;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;SRID&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;SRID&#30740;&#31350;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24635;&#32467;&#20102;&#24120;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#24120;&#29992;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#23545;&#19981;&#21516;&#26041;&#27861;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#26512;&#35780;&#20215;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;SRID&#27169;&#22411;&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Person re-identification via 3D skeletons is an important emerging research area that triggers great interest in the pattern recognition community. With distinctive advantages for many application scenarios, a great diversity of 3D skeleton based person re-identification (SRID) methods have been proposed in recent years, effectively addressing prominent problems in skeleton modeling and feature learning. Despite recent advances, to the best of our knowledge, little effort has been made to comprehensively summarize these studies and their challenges. In this paper, we attempt to fill this gap by providing a systematic survey on current SRID approaches, model designs, challenges, and future directions. Specifically, we first formulate the SRID problem, and propose a taxonomy of SRID research with a summary of benchmark datasets, commonly-used model architectures, and an analytical review of different methods' characteristics. Then, we elaborate on the design principles of SRID models fro
&lt;/p&gt;</description></item><item><title>SkipViT&#36890;&#36807;&#20196;&#29260;&#32423;&#36339;&#36291;&#36830;&#25509;&#23558;&#19981;&#37325;&#35201;&#30340;&#22270;&#20687;&#20196;&#29260;&#20998;&#31163;&#65292;&#20197;&#25552;&#39640;Vision Transformers&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#32780;&#19981;&#24433;&#21709;&#26368;&#32456;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.15293</link><description>&lt;p&gt;
SkipViT: &#20351;&#29992;&#20196;&#29260;&#32423;&#36339;&#36291;&#36830;&#25509;&#21152;&#36895;Vision Transformers
&lt;/p&gt;
&lt;p&gt;
SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection. (arXiv:2401.15293v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15293
&lt;/p&gt;
&lt;p&gt;
SkipViT&#36890;&#36807;&#20196;&#29260;&#32423;&#36339;&#36291;&#36830;&#25509;&#23558;&#19981;&#37325;&#35201;&#30340;&#22270;&#20687;&#20196;&#29260;&#20998;&#31163;&#65292;&#20197;&#25552;&#39640;Vision Transformers&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#32780;&#19981;&#24433;&#21709;&#26368;&#32456;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision transformers&#34987;&#35748;&#20026;&#27604;CNN&#27169;&#22411;&#26356;&#20855;&#35745;&#31639;&#21644;&#25968;&#25454;&#23494;&#38598;&#24615;&#12290;&#36825;&#20123;Transformer&#27169;&#22411;&#65292;&#22914;ViT&#65292;&#38656;&#35201;&#25152;&#26377;&#36755;&#20837;&#22270;&#20687;&#20196;&#29260;&#26469;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#20196;&#29260;&#24182;&#19981;&#20449;&#24687;&#20016;&#23500;&#65292;&#21487;&#33021;&#21253;&#21547;&#26080;&#20851;&#30340;&#32972;&#26223;&#25110;&#19981;&#37325;&#35201;&#30340;&#22330;&#26223;&#31561;&#26080;&#20851;&#20449;&#24687;&#12290;&#36825;&#20123;&#20196;&#29260;&#34987;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;MHSA&#65289;&#24573;&#30053;&#65292;&#23548;&#33268;MHSA&#21644;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#20013;&#23384;&#22312;&#35768;&#22810;&#20887;&#20313;&#21644;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36825;&#20123;&#19981;&#37325;&#35201;&#30340;&#20196;&#29260;&#20998;&#31163;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#20302;&#25104;&#26412;&#35745;&#31639;&#36335;&#24452;&#21457;&#36865;&#65292;&#26469;&#20248;&#21270;&#19981;&#24517;&#35201;&#30340;&#20132;&#20114;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20250;&#32473;ViT&#27169;&#22411;&#28155;&#21152;&#20219;&#20309;&#21442;&#25968;&#65292;&#24182;&#26088;&#22312;&#22312;&#35757;&#32451;&#21534;&#21520;&#37327;&#21644;&#26368;&#32456;&#27169;&#22411;&#30340;Top-1&#20934;&#30830;&#29575;&#25439;&#22833;&#20026;0%&#20043;&#38388;&#25214;&#21040;&#26368;&#20339;&#24179;&#34913;&#12290;&#25105;&#20204;&#23545;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;ViT-small&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SkipViT&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers are known to be more computationally and data-intensive than CNN models. These transformer models such as ViT, require all the input image tokens to learn the relationship among them. However, many of these tokens are not informative and may contain irrelevant information such as unrelated background or unimportant scenery. These tokens are overlooked by the multi-head self-attention (MHSA), resulting in many redundant and unnecessary computations in MHSA and the feed-forward network (FFN). In this work, we propose a method to optimize the amount of unnecessary interactions between unimportant tokens by separating and sending them through a different low-cost computational path. Our method does not add any parameters to the ViT model and aims to find the best trade-off between training throughput and achieving a 0% loss in the Top-1 accuracy of the final model. Our experimental results on training ViT-small from scratch show that SkipViT is capable of effectively dr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20116;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#20998;&#26512;&#21644;&#32531;&#35299;&#31574;&#30053;&#26469;&#24314;&#31435;&#31185;&#23398;&#30740;&#31350;&#20013;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;&#12290;&#20840;&#29699;&#20849;&#35782;&#12289;&#19987;&#19994;&#22521;&#35757;&#21644;&#21512;&#29702;&#30340;&#25191;&#34892;&#23545;&#20110;&#20419;&#36827;AI&#30340;&#30410;&#22788;&#21644;&#32500;&#25252;&#30740;&#31350;&#35802;&#20449;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.15284</link><description>&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#24314;&#31435;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Building ethical guidelines for generative AI in scientific research. (arXiv:2401.15284v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20116;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#20998;&#26512;&#21644;&#32531;&#35299;&#31574;&#30053;&#26469;&#24314;&#31435;&#31185;&#23398;&#30740;&#31350;&#20013;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;&#12290;&#20840;&#29699;&#20849;&#35782;&#12289;&#19987;&#19994;&#22521;&#35757;&#21644;&#21512;&#29702;&#30340;&#25191;&#34892;&#23545;&#20110;&#20419;&#36827;AI&#30340;&#30410;&#22788;&#21644;&#32500;&#25252;&#30740;&#31350;&#35802;&#20449;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#27491;&#22312;&#36805;&#36895;&#25913;&#21464;&#23398;&#26415;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#31185;&#23398;&#20013;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;&#30340;&#35752;&#35770;&#20173;&#28982;&#38646;&#25955;&#65292;&#24378;&#35843;&#20102;&#21327;&#21830;&#19968;&#33268;&#24615;&#26631;&#20934;&#30340;&#32039;&#36843;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20116;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#20998;&#26512;&#21644;&#32531;&#35299;&#31574;&#30053;&#30340;&#24320;&#21457;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26694;&#26550;&#65306;&#20102;&#35299;&#27169;&#22411;&#22312;&#30495;&#23454;&#24615;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65307;&#23562;&#37325;&#38544;&#31169;&#12289;&#26426;&#23494;&#21644;&#29256;&#26435;&#65307;&#22312;&#34701;&#20837;&#27169;&#22411;&#36755;&#20986;&#26102;&#36991;&#20813;&#25220;&#34989;&#21644;&#36829;&#21453;&#25919;&#31574;&#65307;&#30830;&#20445;&#24212;&#29992;&#24102;&#26469;&#24635;&#20307;&#21033;&#30410;&#65307;&#20197;&#21450;&#36879;&#26126;&#12289;&#21487;&#22797;&#21046;&#22320;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#12290;&#36890;&#36807;&#21015;&#20030;&#24120;&#35265;&#22330;&#26223;&#26469;&#23637;&#31034;&#28508;&#22312;&#30340;&#20262;&#29702;&#36829;&#35268;&#34892;&#20026;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20840;&#29699;&#20849;&#35782;&#20197;&#21450;&#19987;&#19994;&#22521;&#35757;&#21644;&#21512;&#29702;&#30340;&#25191;&#34892;&#26159;&#20419;&#36827;AI&#30340;&#30410;&#22788;&#24182;&#32500;&#25252;&#30740;&#31350;&#35802;&#20449;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative artificial intelligence tools like large language models are rapidly transforming academic research and real world applications. However, discussions on ethical guidelines for generative AI in science remain fragmented, underscoring the urgent need for consensus based standards. This paper offers an initial framework by developing analyses and mitigation strategies across five key themes: understanding model limitations regarding truthfulness and bias; respecting privacy, confidentiality, and copyright; avoiding plagiarism and policy violations when incorporating model output; ensuring applications provide overall benefit; and using AI transparently and reproducibly. Common scenarios are outlined to demonstrate potential ethical violations. We argue that global consensus coupled with professional training and reasonable enforcement are critical to promoting the benefits of AI while safeguarding research integrity.
&lt;/p&gt;</description></item><item><title>SimFair&#26159;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#27169;&#25311;&#21644;&#36870;&#21521;&#24314;&#27169;&#26469;&#35299;&#20915;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#26377;&#25928;&#20445;&#25345;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15270</link><description>&lt;p&gt;
SimFair&#65306;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#19982;&#27169;&#25311;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models. (arXiv:2401.15270v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15270
&lt;/p&gt;
&lt;p&gt;
SimFair&#26159;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#27169;&#25311;&#21644;&#36870;&#21521;&#24314;&#27169;&#26469;&#35299;&#20915;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#26377;&#25928;&#20445;&#25345;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24863;&#30693;&#24050;&#32463;&#25104;&#20026;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#22522;&#30784;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#24615;&#33021;&#19981;&#24179;&#31561;&#26159;&#30001;&#20110;&#19981;&#21516;&#21306;&#22495;&#20998;&#24067;&#30340;&#21464;&#21270;&#24341;&#36215;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20986;&#25552;&#39640;&#20844;&#24179;&#21487;&#36801;&#31227;&#24615;&#30340;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#27809;&#26377;&#26469;&#33258;&#26032;&#21306;&#22495;&#30340;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#65292;&#36825;&#23545;&#20110;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#23581;&#35797;&#26159;&#19968;&#20010;&#29942;&#39048;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22522;&#20110;&#29289;&#29702;&#26426;&#21046;&#27169;&#22411;&#24050;&#32463;&#22312;&#35768;&#22810;&#20855;&#26377;&#37325;&#22823;&#31038;&#20250;&#24433;&#21709;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SimFair&#65292;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#29289;&#29702;&#35268;&#21017;&#30340;&#27169;&#25311;&#21644;&#36870;&#21521;&#24314;&#27169;&#21040;&#35757;&#32451;&#35774;&#35745;&#20013;&#26469;&#24357;&#34917;&#25968;&#25454;&#38480;&#21046;&#12290;&#20197;&#28201;&#24230;&#39044;&#27979;&#20026;&#20363;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;SimFair&#22312;&#20445;&#25345;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness-awareness has emerged as an essential building block for the responsible use of artificial intelligence in real applications. In many cases, inequity in performance is due to the change in distribution over different regions. While techniques have been developed to improve the transferability of fairness, a solution to the problem is not always feasible with no samples from the new regions, which is a bottleneck for pure data-driven attempts. Fortunately, physics-based mechanistic models have been studied for many problems with major social impacts. We propose SimFair, a physics-guided fairness-aware learning framework, which bridges the data limitation by integrating physical-rule-based simulation and inverse modeling into the training design. Using temperature prediction as an example, we demonstrate the effectiveness of the proposed SimFair in fairness preservation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;</title><link>http://arxiv.org/abs/2401.15269</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25913;&#21892;&#21307;&#30103;&#25512;&#29702;&#33021;&#21147;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19987;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;GPT-4&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#20102;&#20174;&#22810;&#39033;&#36873;&#25321;&#39064;&#21040;&#38271;&#31687;&#29983;&#25104;&#31561;&#22810;&#26679;&#21270;&#25361;&#25112;&#30340;&#37324;&#31243;&#30865;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#32534;&#30721;&#30693;&#35782;&#26080;&#27861;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30693;&#35782;&#35821;&#26009;&#24211;&#20013;&#25628;&#32034;&#25991;&#26723;&#24182;&#26080;&#26465;&#20214;&#25110;&#26377;&#36873;&#25321;&#22320;&#23558;&#20854;&#38468;&#21152;&#21040;LLMs&#30340;&#36755;&#20837;&#26469;&#36827;&#34892;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#26102;&#65292;&#20986;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#33719;&#21462;&#19981;&#27491;&#30830;&#30340;&#25991;&#26723;&#25110;&#20570;&#20986;&#19981;&#20934;&#30830;&#30340;&#21028;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#21307;&#23398;&#25991;&#26412;&#26694;&#26550;Self-BioRAG&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#21644;&#33258;&#25105;&#21453;&#24605;&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;84k&#20010;&#32463;&#36807;&#36807;&#28388;&#30340;&#29983;&#29289;&#21307;&#23398;&#25351;&#20196;&#38598;&#26469;&#35757;&#32451;Self-BioRAG&#65292;&#23427;&#20855;&#22791;&#35780;&#20272;&#33258;&#24049;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its gene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20010;&#20154;&#21644;&#22242;&#20307;&#30340;&#20559;&#22909;&#65292;&#35299;&#20915;&#32958;&#33039;&#20998;&#37197;&#20013;&#30340;&#32039;&#36843;&#25361;&#25112;&#65292;&#24182;&#35780;&#20272;&#20559;&#22909;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15268</link><description>&lt;p&gt;
&#26397;&#30528;&#31283;&#23450;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#19968;&#33268;&#21270;&#26426;&#22120;&#23398;&#20064;&#20559;&#22909;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Preferences for Stakeholder-aligned Machine Learning. (arXiv:2401.15268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25972;&#21512;&#20010;&#20154;&#21644;&#22242;&#20307;&#30340;&#20559;&#22909;&#65292;&#35299;&#20915;&#32958;&#33039;&#20998;&#37197;&#20013;&#30340;&#32039;&#36843;&#25361;&#25112;&#65292;&#24182;&#35780;&#20272;&#20559;&#22909;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32958;&#33039;&#20998;&#37197;&#30340;&#32039;&#36843;&#25361;&#25112;&#65292;&#21363;&#38656;&#27714;&#22686;&#38271;&#19982;&#21033;&#30410;&#30456;&#20851;&#26041;&#20215;&#20540;&#30340;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#31181;&#23398;&#20064;&#20010;&#20154;&#21644;&#22242;&#20307;&#20851;&#20110;&#32958;&#33039;&#20998;&#37197;&#30340;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#8220;&#25104;&#23545;&#32958;&#33039;&#24739;&#32773;&#22312;&#32447;&#35843;&#26597;&#8221;&#30340;&#25968;&#25454;&#65292;&#32467;&#21512;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20010;&#20154;&#12289;&#22242;&#20307;&#21644;&#31283;&#23450;&#24615;&#19977;&#20010;&#23618;&#38754;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#24182;&#36890;&#36807;&#20960;&#31181;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;&#20010;&#20154;&#23618;&#38754;&#27169;&#22411;&#39044;&#27979;&#20010;&#20307;&#21442;&#19982;&#32773;&#30340;&#20559;&#22909;&#65292;&#22242;&#20307;&#23618;&#38754;&#27169;&#22411;&#27719;&#24635;&#21442;&#19982;&#32773;&#20559;&#22909;&#65292;&#31283;&#23450;&#24615;&#23618;&#38754;&#27169;&#22411;&#26159;&#22242;&#20307;&#21319;&#32423;&#30340;&#25193;&#23637;&#65292;&#35780;&#20272;&#36825;&#20123;&#20559;&#22909;&#38543;&#26102;&#38388;&#30340;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#23558;&#21033;&#30410;&#30456;&#20851;&#26041;&#30340;&#20559;&#22909;&#32435;&#20837;&#32958;&#33039;&#20998;&#37197;&#36807;&#31243;&#65292;&#25105;&#20204;&#24076;&#26395;&#25512;&#21160;&#20262;&#29702;&#32500;&#24230;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to the pressing challenge of kidney allocation, characterized by growing demands for organs, this research sets out to develop a data-driven solution to this problem, which also incorporates stakeholder values. The primary objective of this study is to create a method for learning both individual and group-level preferences pertaining to kidney allocations. Drawing upon data from the 'Pairwise Kidney Patient Online Survey.' Leveraging two distinct datasets and evaluating across three levels - Individual, Group and Stability - we employ machine learning classifiers assessed through several metrics. The Individual level model predicts individual participant preferences, the Group level model aggregates preferences across participants, and the Stability level model, an extension of the Group level, evaluates the stability of these preferences over time. By incorporating stakeholder preferences into the kidney allocation process, we aspire to advance the ethical dimensions of o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#25554;&#20214;&#65292;&#21487;&#20197;&#22312;Blender 3D&#24314;&#27169;&#24037;&#20855;&#19978;&#28155;&#21152;&#27425;&#34920;&#38754;&#25955;&#23556;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Mitsuba&#28210;&#26579;&#22120;&#36827;&#34892;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#25554;&#20214;&#33021;&#22815;&#20934;&#30830;&#12289;&#32039;&#23494;&#21644;&#39640;&#25928;&#22320;&#21487;&#35270;&#21270;&#22343;&#21248;&#21644;&#24322;&#36136;&#27425;&#34920;&#38754;&#25955;&#23556;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15245</link><description>&lt;p&gt;
GenPluSSS&#65306;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#27979;&#37327;&#27425;&#34920;&#38754;&#25955;&#23556;&#34920;&#31034;&#30340;&#25554;&#20214;
&lt;/p&gt;
&lt;p&gt;
GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface Scattering Representation. (arXiv:2401.15245v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#25554;&#20214;&#65292;&#21487;&#20197;&#22312;Blender 3D&#24314;&#27169;&#24037;&#20855;&#19978;&#28155;&#21152;&#27425;&#34920;&#38754;&#25955;&#23556;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Mitsuba&#28210;&#26579;&#22120;&#36827;&#34892;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#25554;&#20214;&#33021;&#22815;&#20934;&#30830;&#12289;&#32039;&#23494;&#21644;&#39640;&#25928;&#22320;&#21487;&#35270;&#21270;&#22343;&#21248;&#21644;&#24322;&#36136;&#27425;&#34920;&#38754;&#25955;&#23556;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;Blender 3D&#24314;&#27169;&#24037;&#20855;&#19978;&#28155;&#21152;&#22343;&#21248;&#21644;&#24322;&#36136;&#12289;&#20809;&#23398;&#21402;&#24230;&#30340;&#21322;&#36879;&#26126;&#26448;&#26009;&#34920;&#31034;&#30340;&#25554;&#20214;&#12290;&#35813;&#25554;&#20214;&#30340;&#24037;&#20316;&#21407;&#29702;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#21644;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#27425;&#34920;&#38754;&#25955;&#23556;&#26041;&#27861;&#65288;GenSSS&#65289;&#30340;&#32452;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#25554;&#20214;&#20351;&#29992;&#24320;&#28304;&#28210;&#26579;&#36719;&#20214;Mitsuba&#28210;&#26579;&#22120;&#36827;&#34892;&#23454;&#29616;&#12290;&#35813;&#25554;&#20214;&#22312;&#27979;&#24471;&#30340;&#27425;&#34920;&#38754;&#25955;&#23556;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25554;&#20214;&#33021;&#22815;&#20934;&#30830;&#12289;&#32039;&#23494;&#21644;&#39640;&#25928;&#22320;&#21487;&#35270;&#21270;&#22343;&#21248;&#21644;&#24322;&#36136;&#27425;&#34920;&#38754;&#25955;&#23556;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a plugin that adds a representation of homogeneous and heterogeneous, optically thick, translucent materials on the Blender 3D modeling tool. The working principle of this plugin is based on a combination of Genetic Algorithm (GA) and Singular Value Decomposition (SVD)-based subsurface scattering method (GenSSS). The proposed plugin has been implemented using Mitsuba renderer, which is an open source rendering software. The proposed plugin has been validated on measured subsurface scattering data. It's shown that the proposed plugin visualizes homogeneous and heterogeneous subsurface scattering effects, accurately, compactly and computationally efficiently.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;UnTrac&#65292;&#36890;&#36807;&#21453;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UnTrac&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.15241</link><description>&lt;p&gt;
&#21453;&#23398;&#20064;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unlearning Reveals the Influential Training Data of Language Models. (arXiv:2401.15241v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;UnTrac&#65292;&#36890;&#36807;&#21453;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UnTrac&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#35782;&#21035;&#21738;&#20123;&#35757;&#32451;&#25968;&#25454;&#38598;&#24433;&#21709;&#27169;&#22411;&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20174;&#35757;&#32451;&#20013;&#31227;&#38500;&#27599;&#20010;&#25968;&#25454;&#38598;&#26469;&#34913;&#37327;&#20854;&#24433;&#21709;;&#28982;&#32780;&#65292;&#22810;&#27425;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;UnTrac&#65292;&#36890;&#36807;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21462;&#28040;&#23398;&#20064;&#26469;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;UnTrac&#38750;&#24120;&#31616;&#21333;; &#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#26469;&#21462;&#28040;&#23398;&#20064;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#22312;&#21462;&#28040;&#23398;&#20064;&#21518;&#27169;&#22411;&#30340;&#39044;&#27979;&#21457;&#29983;&#20102;&#22810;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21542;&#33021;&#35780;&#20272;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#29983;&#25104;&#26377;&#27602;&#12289;&#26377;&#20559;&#35265;&#21644;&#19981;&#30495;&#23454;&#20869;&#23481;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#20102;&#23427;&#20204;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#36807;&#22810;&#30340;&#20869;&#23384;&#31354;&#38388;&#25110;&#22810;&#20010;&#27169;&#22411;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to enhance the performance of language models while mitigating the risks of generating harmful content, it is crucial to identify which training dataset affects the model's outputs. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac, which estimates the influence of a training dataset by unlearning it from the trained model. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model's predictions change after unlearning. We empirically examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Experimental results demonstrate that our method estimates their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple model checkpoints.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;TabTransformer&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#20381;&#36182;&#20851;&#31995;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#28040;&#38500;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.15238</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#65306;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning with Tabular Data: A Self-supervised Approach. (arXiv:2401.15238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15238
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;TabTransformer&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#20381;&#36182;&#20851;&#31995;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#28040;&#38500;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21033;&#29992;TabTransformer&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#35757;&#32451;&#34920;&#26684;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#34920;&#26684;&#25968;&#25454;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;GBDT&#65292;&#34429;&#28982;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#25105;&#20204;&#30340;&#35770;&#25991;&#30740;&#31350;&#20102;&#19987;&#20026;&#34920;&#26684;&#25968;&#25454;&#20248;&#21270;&#30340;TabTransformer&#30340;&#26377;&#25928;&#24615;&#12290;TabTransformer&#36890;&#36807;&#21033;&#29992;Transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#20351;&#29992;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;TabTransformer&#36890;&#36807;&#21019;&#24314;&#20195;&#29702;&#30417;&#30563;&#20219;&#21153;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#28040;&#38500;&#20102;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#30446;&#26631;&#26159;&#25214;&#21040;&#26368;&#26377;&#25928;&#30340;TabTransformer&#27169;&#22411;&#26469;&#34920;&#31034;&#20998;&#31867;&#21644;&#25968;&#20540;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#22312;Transformer&#20013;&#26500;&#24314;&#19981;&#21516;&#36755;&#20837;&#35774;&#32622;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have described a novel approach for training tabular data using the TabTransformer model with self-supervised learning. Traditional machine learning models for tabular data, such as GBDT are being widely used though our paper examines the effectiveness of the TabTransformer which is a Transformer based model optimised specifically for tabular data. The TabTransformer captures intricate relationships and dependencies among features in tabular data by leveraging the self-attention mechanism of Transformers. We have used a self-supervised learning approach in this study, where the TabTransformer learns from unlabelled data by creating surrogate supervised tasks, eliminating the need for the labelled data. The aim is to find the most effective TabTransformer model representation of categorical and numerical features. To address the challenges faced during the construction of various input settings into the Transformers. Furthermore, a comparative analysis is also been conducted to exami
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15222</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#20197;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#30149;&#20363;&#26816;&#27979;&#20026;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#23454;&#20307;&#20462;&#39280;&#35821;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#23454;&#20307;&#30340;&#35821;&#20041;&#21487;&#33021;&#20250;&#21463;&#21040;&#20462;&#39280;&#35821;&#30340;&#26174;&#33879;&#25913;&#21464;&#65292;&#21253;&#25324;&#23454;&#20307;&#30340;&#21542;&#23450;&#12289;&#19981;&#30830;&#23450;&#24615;&#12289;&#26465;&#20214;&#24615;&#12289;&#20005;&#37325;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#29616;&#26377;&#30340;&#30830;&#23450;&#20020;&#24202;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#27169;&#22411;&#28041;&#21450;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#25110;&#29305;&#24449;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#26159;&#29420;&#31435;&#35757;&#32451;&#27599;&#20010;&#20462;&#39280;&#35821;&#30340;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#21464;&#25442;&#22120;&#26550;&#26500;&#35774;&#35745;&#65292;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;SemEval 2015&#20219;&#21153;14&#35821;&#26009;&#24211;&#21644;&#19968;&#20010;&#26032;&#30340;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#65288;OUD&#65289;&#25968;&#25454;&#38598;&#19978;&#20849;&#21516;&#23398;&#20064;&#21644;&#39044;&#27979;&#20462;&#39280;&#35821;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;SemEval&#20849;&#20139;&#30340;&#20462;&#39280;&#35821;&#20197;&#21450;OUD&#29305;&#23450;&#30340;&#26032;&#20462;&#39280;&#35821;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#19982;&#20197;&#21069;&#21457;&#34920;&#30340;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#24182;&#35780;&#20272;&#20102;&#20165;&#20849;&#20139;&#37096;&#20998;&#20020;&#24202;&#20462;&#39280;&#35821;&#26102;&#30340;&#20020;&#24202;&#23454;&#20307;&#20462;&#39280;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;SemEval 2015&#30340;ShARe&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: The semantics of entities extracted from a clinical text can be dramatically altered by modifiers, including entity negation, uncertainty, conditionality, severity, and subject. Existing models for determining modifiers of clinical entities involve regular expression or features weights that are trained independently for each modifier.  Methods: We develop and evaluate a multi-task transformer architecture design where modifiers are learned and predicted jointly using the publicly available SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that contains modifiers shared with SemEval as well as novel modifiers specific for OUD. We evaluate the effectiveness of our multi-task learning approach versus previously published systems and assess the feasibility of transfer learning for clinical entity modifiers when only a portion of clinical modifiers are shared.  Results: Our approach achieved state-of-the-art results on the ShARe corpus from SemEval 2015 T
&lt;/p&gt;</description></item><item><title>Roq&#26159;&#19968;&#20010;&#22522;&#20110;&#39118;&#38505;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#40065;&#26834;&#30340;&#26597;&#35810;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.15210</link><description>&lt;p&gt;
Roq&#65306;&#22522;&#20110;&#39118;&#38505;&#24863;&#30693;&#23398;&#20064;&#25104;&#26412;&#27169;&#22411;&#30340;&#40065;&#26834;&#26597;&#35810;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Roq: Robust Query Optimization Based on a Risk-aware Learned Cost Model. (arXiv:2401.15210v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15210
&lt;/p&gt;
&lt;p&gt;
Roq&#26159;&#19968;&#20010;&#22522;&#20110;&#39118;&#38505;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#40065;&#26834;&#30340;&#26597;&#35810;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25968;&#25454;&#24211;&#31649;&#29702;&#31995;&#32479;(RDBMS)&#20013;&#30340;&#26597;&#35810;&#20248;&#21270;&#22120;&#25628;&#32034;&#39044;&#26399;&#23545;&#20110;&#32473;&#23450;&#26597;&#35810;&#26368;&#20248;&#30340;&#25191;&#34892;&#35745;&#21010;&#12290;&#23427;&#20204;&#20351;&#29992;&#21442;&#25968;&#20272;&#35745;&#65292;&#36890;&#24120;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#24182;&#19988;&#20570;&#20986;&#30340;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#20123;&#20272;&#35745;&#21644;&#20551;&#35774;&#26080;&#25928;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#36873;&#25321;&#22312;&#36816;&#34892;&#26102;&#26159;&#27425;&#20248;&#30340;&#25191;&#34892;&#35745;&#21010;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26597;&#35810;&#24615;&#33021;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#26597;&#35810;&#20248;&#21270;&#22120;&#19981;&#36275;&#20197;&#25903;&#25345;&#40065;&#26834;&#30340;&#26597;&#35810;&#20248;&#21270;&#12290;&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;(ML)&#26469;&#25552;&#39640;&#25968;&#25454;&#31995;&#32479;&#30340;&#25928;&#29575;&#24182;&#20943;&#23569;&#20854;&#32500;&#25252;&#24320;&#38144;&#30340;&#20852;&#36259;&#26085;&#30410;&#39640;&#28072;&#65292;&#22312;&#26597;&#35810;&#20248;&#21270;&#39046;&#22495;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#36825;&#20123;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#24182;&#22522;&#20110;IBM Db2&#22810;&#24180;&#30340;&#32463;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Roq: &#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#23427;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;&#26597;&#35810;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query optimizers in relational database management systems (RDBMSs) search for execution plans expected to be optimal for a given queries. They use parameter estimates, often inaccurate, and make assumptions that may not hold in practice. Consequently, they may select execution plans that are suboptimal at runtime, when these estimates and assumptions are not valid, which may result in poor query performance. Therefore, query optimizers do not sufficiently support robust query optimization. Recent years have seen a surge of interest in using machine learning (ML) to improve efficiency of data systems and reduce their maintenance overheads, with promising results obtained in the area of query optimization in particular. In this paper, inspired by these advancements, and based on several years of experience of IBM Db2 in this journey, we propose Robust Optimization of Queries, (Roq), a holistic framework that enables robust query optimization based on a risk-aware learning approach. Roq 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26469;&#33258;SCANIA&#20844;&#21496;&#30340;&#30495;&#23454;&#19990;&#30028;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#39044;&#27979;&#24615;&#32500;&#25252;&#22330;&#26223;&#12290;&#23427;&#20855;&#26377;&#24222;&#22823;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#26102;&#38388;&#20449;&#24687;&#65292;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.15199</link><description>&lt;p&gt;
SCANIA&#32452;&#20214;X&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#39044;&#27979;&#24615;&#32500;&#25252;&#30340;&#30495;&#23454;&#19990;&#30028;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SCANIA Component X Dataset: A Real-World Multivariate Time Series Dataset for Predictive Maintenance. (arXiv:2401.15199v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15199
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26469;&#33258;SCANIA&#20844;&#21496;&#30340;&#30495;&#23454;&#19990;&#30028;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#39044;&#27979;&#24615;&#32500;&#25252;&#22330;&#26223;&#12290;&#23427;&#20855;&#26377;&#24222;&#22823;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#26102;&#38388;&#20449;&#24687;&#65292;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26469;&#33258;SCANIA&#29790;&#20856;&#20844;&#21496;&#30340;&#21345;&#36710;&#36710;&#38431;&#20013;&#21311;&#21517;&#21457;&#21160;&#26426;&#37096;&#20214;&#65288;&#31216;&#20026;Component X&#65289;&#30340;&#30495;&#23454;&#19990;&#30028;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#21464;&#37327;&#65292;&#25429;&#25417;&#20102;&#35814;&#32454;&#30340;&#25805;&#20316;&#25968;&#25454;&#12289;&#32500;&#20462;&#35760;&#24405;&#21644;&#21345;&#36710;&#35268;&#26684;&#65292;&#21516;&#26102;&#36890;&#36807;&#21311;&#21517;&#22788;&#29702;&#20445;&#25345;&#26426;&#23494;&#24615;&#12290;&#23427;&#38750;&#24120;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#29983;&#23384;&#20998;&#26512;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#27979;&#24615;&#32500;&#25252;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#24222;&#22823;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#20197;&#30452;&#26041;&#22270;&#21644;&#35745;&#25968;&#22120;&#24418;&#24335;&#30340;&#22810;&#26679;&#21270;&#29305;&#24449;&#65292;&#20197;&#21450;&#21253;&#21547;&#26102;&#38388;&#20449;&#24687;&#65292;&#20351;&#24471;&#36825;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#22312;&#35813;&#39046;&#22495;&#20013;&#29420;&#29305;&#12290;&#21457;&#24067;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#30446;&#26631;&#26159;&#35753;&#24191;&#22823;&#30740;&#31350;&#20154;&#21592;&#26377;&#21487;&#33021;&#20351;&#29992;&#26469;&#33258;&#19968;&#23478;&#22269;&#38469;&#30693;&#21517;&#20844;&#21496;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26631;&#20934;&#22522;&#20934;&#29992;&#20110;&#39044;&#27979;&#24615;&#32500;&#25252;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a description of a real-world, multivariate time series dataset collected from an anonymized engine component (called Component X) of a fleet of trucks from SCANIA, Sweden. This dataset includes diverse variables capturing detailed operational data, repair records, and specifications of trucks while maintaining confidentiality by anonymization. It is well-suited for a range of machine learning applications, such as classification, regression, survival analysis, and anomaly detection, particularly when applied to predictive maintenance scenarios. The large population size and variety of features in the format of histograms and numerical counters, along with the inclusion of temporal information, make this real-world dataset unique in the field. The objective of releasing this dataset is to give a broad range of researchers the possibility of working with real-world data from an internationally well-known company and introduce a standard benchmark to the predictive ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#27491;&#21017;&#21270;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#26377;&#38480;&#26102;&#38388;&#20869;&#30340;&#25910;&#25947;&#65292;&#24182;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#19979;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.15196</link><description>&lt;p&gt;
&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#27491;&#21017;&#21270;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Regularized Q-Learning with Linear Function Approximation. (arXiv:2401.15196v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#27491;&#21017;&#21270;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#26377;&#38480;&#26102;&#38388;&#20869;&#30340;&#25910;&#25947;&#65292;&#24182;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#19979;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#25104;&#21151;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#27491;&#21017;&#21270;&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#25506;&#32034;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#26102;&#65292;&#36825;&#20123;&#31639;&#27861;&#65288;&#22914;&#36719;Q&#23398;&#20064;&#65289;&#30340;&#25910;&#25947;&#24615;&#36136;&#24182;&#19981;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#21333;&#29615;&#36335;&#31639;&#27861;&#65292;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#20445;&#35777;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#25237;&#24433;&#36125;&#23572;&#26364;&#35823;&#24046;&#12290;&#35813;&#31639;&#27861;&#22312;&#20004;&#20010;&#23610;&#24230;&#19978;&#36816;&#34892;&#65306;&#19968;&#20010;&#36739;&#24930;&#30340;&#23610;&#24230;&#29992;&#20110;&#26356;&#26032;&#29366;&#24577;&#21160;&#20316;&#20540;&#30340;&#30446;&#26631;&#32593;&#32476;&#65292;&#19968;&#20010;&#36739;&#24555;&#30340;&#23610;&#24230;&#29992;&#20110;&#22312;&#22522;&#21521;&#37327;&#31354;&#38388;&#20013;&#36924;&#36817;&#36125;&#23572;&#26364;&#22791;&#20221;&#12290;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#23384;&#22312;&#19979;&#65292;&#35813;&#31639;&#27861;&#25910;&#25947;&#20110;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35813;&#31639;&#27861;&#34893;&#29983;&#31574;&#30053;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several successful reinforcement learning algorithms make use of regularization to promote multi-modal policies that exhibit enhanced exploration and robustness. With functional approximation, the convergence properties of some of these algorithms (e.g. soft Q-learning) are not well understood. In this paper, we consider a single-loop algorithm for minimizing the projected Bellman error with finite time convergence guarantees in the case of linear function approximation. The algorithm operates on two scales: a slower scale for updating the target network of the state-action values, and a faster scale for approximating the Bellman backups in the subspace of the span of basis vectors. We show that, under certain assumptions, the proposed algorithm converges to a stationary point in the presence of Markovian noise. In addition, we provide a performance guarantee for the policies derived from the proposed algorithm.
&lt;/p&gt;</description></item><item><title>CAREForMe&#26159;&#19968;&#31181;&#20026;&#24515;&#29702;&#20581;&#24247;&#35774;&#35745;&#30340;&#24773;&#22659;&#22810;&#33218;&#36172;&#21338;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#20010;&#24615;&#21270;&#21644;&#27169;&#22359;&#21270;&#30340;&#35774;&#35745;&#65292;&#32467;&#21512;&#31227;&#21160;&#20256;&#24863;&#21644;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20379;&#21450;&#26102;&#12289;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#23427;&#30340;&#27169;&#22359;&#21270;&#35774;&#35745;&#26082;&#25903;&#25345;&#23450;&#21046;&#21270;&#30340;&#30740;&#31350;&#65292;&#20063;&#20419;&#36827;&#20102;&#36328;&#23398;&#31185;&#30340;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.15188</link><description>&lt;p&gt;
CAREForMe&#65306;&#24515;&#29702;&#20581;&#24247;&#30340;&#24773;&#22659;&#22810;&#33218;&#36172;&#21338;&#25512;&#33616;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CAREForMe: Contextual Multi-Armed Bandit Recommendation Framework for Mental Health. (arXiv:2401.15188v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15188
&lt;/p&gt;
&lt;p&gt;
CAREForMe&#26159;&#19968;&#31181;&#20026;&#24515;&#29702;&#20581;&#24247;&#35774;&#35745;&#30340;&#24773;&#22659;&#22810;&#33218;&#36172;&#21338;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#20010;&#24615;&#21270;&#21644;&#27169;&#22359;&#21270;&#30340;&#35774;&#35745;&#65292;&#32467;&#21512;&#31227;&#21160;&#20256;&#24863;&#21644;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20379;&#21450;&#26102;&#12289;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#23427;&#30340;&#27169;&#22359;&#21270;&#35774;&#35745;&#26082;&#25903;&#25345;&#23450;&#21046;&#21270;&#30340;&#30740;&#31350;&#65292;&#20063;&#20419;&#36827;&#20102;&#36328;&#23398;&#31185;&#30340;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20896;&#30123;&#24773;&#21152;&#21095;&#20102;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#26377;&#25928;&#21644;&#21487;&#33719;&#24471;&#30340;&#24515;&#29702;&#20581;&#24247;&#24178;&#39044;&#30340;&#32039;&#36843;&#24615;&#12290;&#31227;&#21160;&#20581;&#24247;&#65288;mHealth&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#27491;&#24565;&#24212;&#29992;&#31243;&#24207;&#65292;&#24050;&#32463;&#33719;&#24471;&#20102;&#25512;&#24191;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#20877;&#23616;&#38480;&#20110;&#20256;&#32479;&#30340;&#20020;&#24202;&#29615;&#22659;&#65292;&#32780;&#26159;&#25903;&#25345;&#26085;&#24120;&#29983;&#27963;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;mHealth&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20102;&#32570;&#20047;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#20010;&#24615;&#21270;&#21644;&#27169;&#22359;&#21270;&#30340;&#38459;&#30861;&#65292;&#20197;&#20419;&#36827;&#23427;&#20204;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CAREForMe&#65292;&#19968;&#31181;&#38024;&#23545;&#24515;&#29702;&#20581;&#24247;&#30340;&#24773;&#22659;&#22810;&#33218;&#36172;&#21338;&#65288;CMAB&#65289;&#25512;&#33616;&#26694;&#26550;&#12290;CAREForMe&#20197;&#19978;&#19979;&#25991;&#24863;&#30693;&#12289;&#20010;&#24615;&#21270;&#21644;&#27169;&#22359;&#21270;&#20026;&#26680;&#24515;&#36827;&#34892;&#35774;&#35745;&#65292;&#21033;&#29992;&#31227;&#21160;&#20256;&#24863;&#21644;&#38598;&#25104;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#29992;&#25143;&#32858;&#31867;&#33021;&#21147;&#26469;&#25552;&#20379;&#21450;&#26102;&#12289;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#36890;&#36807;&#20854;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;CAREForMe&#26082;&#26159;&#19968;&#20010;&#21487;&#23450;&#21046;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#21448;&#26159;&#19968;&#20010;&#20419;&#36827;&#36328;&#23398;&#31185;&#21512;&#20316;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has intensified the urgency for effective and accessible mental health interventions in people's daily lives. Mobile Health (mHealth) solutions, such as AI Chatbots and Mindfulness Apps, have gained traction as they expand beyond traditional clinical settings to support daily life. However, the effectiveness of current mHealth solutions is impeded by the lack of context-awareness, personalization, and modularity to foster their reusability. This paper introduces CAREForMe, a contextual multi-armed bandit (CMAB) recommendation framework for mental health. Designed with context-awareness, personalization, and modularity at its core, CAREForMe harnesses mobile sensing and integrates online learning algorithms with user clustering capability to deliver timely, personalized recommendations. With its modular design, CAREForMe serves as both a customizable recommendation framework to guide future research, and a collaborative platform to facilitate interdisciplinary cont
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#24615;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#30456;&#27604;&#20110;GPT-3.5&#65292;GPT-4&#33021;&#22815;&#23454;&#29616;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#32534;&#30721;&#19968;&#33268;&#24615;&#12290;&#26080;&#35770;&#27169;&#22411;&#35268;&#27169;&#22823;&#23567;&#65292;&#21482;&#35201;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#27169;&#22411;&#37117;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32534;&#30721;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15170</link><description>&lt;p&gt;
LLMs&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#23450;&#24615;&#32534;&#30721;&#65306;&#24605;&#32500;&#38142;&#25512;&#29702;&#22312;&#26576;&#20123;&#35299;&#37322;&#23398;&#20219;&#21153;&#20013;&#33021;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks. (arXiv:2401.15170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#24615;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#30456;&#27604;&#20110;GPT-3.5&#65292;GPT-4&#33021;&#22815;&#23454;&#29616;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#32534;&#30721;&#19968;&#33268;&#24615;&#12290;&#26080;&#35770;&#27169;&#22411;&#35268;&#27169;&#22823;&#23567;&#65292;&#21482;&#35201;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#27169;&#22411;&#37117;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32534;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#24615;&#32534;&#30721;&#25110;&#20869;&#23481;&#20998;&#26512;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21547;&#20041;&#65292;&#20197;&#35782;&#21035;&#36328;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#23450;&#37327;&#27169;&#24335;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#37322;&#33021;&#21147;&#26041;&#38754;&#30340;&#36827;&#23637;&#20026;&#33258;&#21160;&#21270;&#32534;&#30721;&#36807;&#31243;&#65288;&#23545;&#25991;&#26412;&#24212;&#29992;&#31867;&#21035;&#26631;&#31614;&#65289;&#25552;&#20379;&#20102;&#28508;&#21147;&#65292;&#20174;&#32780;&#20351;&#20154;&#31867;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#19987;&#27880;&#20110;&#26356;&#26377;&#21019;&#36896;&#21147;&#30340;&#30740;&#31350;&#26041;&#38754;&#65292;&#21516;&#26102;&#23558;&#36825;&#20123;&#35299;&#37322;&#20219;&#21153;&#22996;&#25176;&#32473;&#20154;&#24037;&#26234;&#33021;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#21253;&#25324;&#23545;&#20154;&#25991;&#23398;&#30740;&#31350;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#23494;&#38598;&#27573;&#33853;&#30340;&#19968;&#32452;&#31038;&#20250;&#21382;&#21490;&#32534;&#30721;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#33021;&#22815;&#36798;&#21040;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#35299;&#37322;&#65292;&#32780;GPT-3.5&#21017;&#19981;&#33021;&#12290;&#19982;&#25105;&#20204;&#30001;&#20154;&#31867;&#33719;&#24471;&#30340;&#37329;&#26631;&#20934;&#30456;&#27604;&#65292;GPT-4&#22312;3&#20010;&#32534;&#30721;&#20013;&#20855;&#26377;&#20248;&#31168;&#30340;&#32534;&#30721;&#19968;&#33268;&#24615;&#65288;Cohen's &#954; &#8805; 0.79&#65289;&#65292;&#22312;9&#20010;&#32534;&#30721;&#20013;&#26377;8&#20010;&#20855;&#26377;&#26174;&#33879;&#30340;&#19968;&#33268;&#24615;&#65288;&#954; &#8805; 0.6&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;GPT-3.5&#22312;&#25152;&#26377;&#32534;&#30721;&#20013;&#34920;&#29616;&#19981;&#20339;&#65288;mean(&#954;) = 0.34&#65307;max(&#954;) = 0.55&#65289;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#32534;&#30721;&#30340;&#20934;&#30830;&#24615;&#19981;&#21463;&#27169;&#22411;&#35268;&#27169;&#24433;&#21709;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32534;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Qualitative coding, or content analysis, extracts meaning from text to discern quantitative patterns across a corpus of texts. Recently, advances in the interpretive abilities of large language models (LLMs) offer potential for automating the coding process (applying category labels to texts), thereby enabling human researchers to concentrate on more creative research aspects, while delegating these interpretive tasks to AI. Our case study comprises a set of socio-historical codes on dense, paragraph-long passages representative of a humanistic study. We show that GPT-4 is capable of human-equivalent interpretations, whereas GPT-3.5 is not. Compared to our human-derived gold standard, GPT-4 delivers excellent intercoder reliability (Cohen's $\kappa \geq 0.79$) for 3 of 9 codes, and substantial reliability ($\kappa \geq 0.6$) for 8 of 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes ($mean(\kappa) = 0.34$; $max(\kappa) = 0.55$). Importantly, we find that coding fidelity
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24341;&#20837;&#20102;&#23545;&#31216;&#29616;&#23454;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#34920;&#31034;&#24418;&#24335;&#23558;&#29289;&#29702;&#21644;&#34394;&#25311;&#34701;&#21512;&#36215;&#26469;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;AI&#20195;&#29702;&#22312;&#29289;&#29702;&#21644;&#34394;&#25311;&#19990;&#30028;&#20013;&#30340;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.15132</link><description>&lt;p&gt;
&#20851;&#20110;&#23545;&#31216;&#29616;&#23454;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
On the Emergence of Symmetrical Reality. (arXiv:2401.15132v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15132
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24341;&#20837;&#20102;&#23545;&#31216;&#29616;&#23454;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#34920;&#31034;&#24418;&#24335;&#23558;&#29289;&#29702;&#21644;&#34394;&#25311;&#34701;&#21512;&#36215;&#26469;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;AI&#20195;&#29702;&#22312;&#29289;&#29702;&#21644;&#34394;&#25311;&#19990;&#30028;&#20013;&#30340;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#31867;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#24182;&#20419;&#36827;&#20102;&#26032;&#30340;&#33021;&#22815;&#22312;&#29289;&#29702;&#21644;&#34394;&#25311;&#29615;&#22659;&#20013;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;AI&#23454;&#20307;&#30340;&#21457;&#23637;&#12290;&#34429;&#28982;&#22810;&#24180;&#26469;&#23384;&#22312;&#30528;&#34394;&#25311;&#29616;&#23454;&#12289;&#28151;&#21512;&#29616;&#23454;&#21644;&#22686;&#24378;&#29616;&#23454;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#24212;&#29992;&#26041;&#21521;&#30340;&#24046;&#24322;&#65292;&#25972;&#21512;&#36825;&#20123;&#25216;&#26415;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;AI&#20195;&#29702;&#30340;&#20986;&#29616;&#65292;&#33021;&#22815;&#33258;&#20027;&#24863;&#30693;&#21644;&#34892;&#21160;&#65292;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#20256;&#32479;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#24314;&#31435;&#19968;&#20010;&#32508;&#21512;&#24615;&#26694;&#26550;&#65292;&#26082;&#33021;&#23481;&#32435;&#20154;&#31867;&#21644;AI&#20195;&#29702;&#22312;&#29289;&#29702;&#21644;&#34394;&#25311;&#19990;&#30028;&#20013;&#30340;&#21452;&#37325;&#24863;&#30693;&#20013;&#24515;&#65292;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#31216;&#29616;&#23454;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#21508;&#31181;&#24418;&#24335;&#30340;&#29289;&#29702;-&#34394;&#25311;&#34701;&#21512;&#12290;&#36825;&#20010;&#26694;&#26550;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;AI&#20195;&#29702;&#22914;&#20309;&#33021;&#22815;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has revolutionized human cognitive abilities and facilitated the development of new AI entities capable of interacting with humans in both physical and virtual environments. Despite the existence of virtual reality, mixed reality, and augmented reality for several years, integrating these technical fields remains a formidable challenge due to their disparate application directions. The advent of AI agents, capable of autonomous perception and action, further compounds this issue by exposing the limitations of traditional human-centered research approaches. It is imperative to establish a comprehensive framework that accommodates the dual perceptual centers of humans and AI agents in both physical and virtual worlds. In this paper, we introduce the symmetrical reality framework, which offers a unified representation encompassing various forms of physical-virtual amalgamations. This framework enables researchers to better comprehend how AI agents can collabor
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26234;&#33021;&#25163;&#26426;&#20013;&#30340;&#20256;&#24863;&#22120;&#25216;&#26415;&#35782;&#21035;&#32908;&#32905;&#21147;&#37327;&#38203;&#28860;&#27963;&#21160;&#65292;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#21644;&#24212;&#29992;LSTM&#31639;&#27861;&#65292;&#21457;&#29616;&#20102;&#22312;&#21491;&#25163;&#21644;&#24038;&#25163;&#30340;&#36816;&#21160;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#20256;&#24863;&#22120;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15124</link><description>&lt;p&gt;
&#22522;&#20110;&#26222;&#36866;&#35774;&#22791;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#37319;&#38598;&#29992;&#20110;&#26816;&#27979;&#32908;&#32905;&#21147;&#37327;&#35757;&#32451;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Sensor-Based Data Acquisition via Ubiquitous Device to Detect Muscle Strength Training Activities. (arXiv:2401.15124v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15124
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26234;&#33021;&#25163;&#26426;&#20013;&#30340;&#20256;&#24863;&#22120;&#25216;&#26415;&#35782;&#21035;&#32908;&#32905;&#21147;&#37327;&#38203;&#28860;&#27963;&#21160;&#65292;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#21644;&#24212;&#29992;LSTM&#31639;&#27861;&#65292;&#21457;&#29616;&#20102;&#22312;&#21491;&#25163;&#21644;&#24038;&#25163;&#30340;&#36816;&#21160;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#20256;&#24863;&#22120;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20307;&#32946;&#38203;&#28860;&#26469;&#32500;&#25345;&#39640;&#21697;&#36136;&#30340;&#29983;&#27963;&#20197;&#39044;&#38450;&#20581;&#24247;&#19979;&#38477;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#20010;&#20307;&#20581;&#24247;&#29366;&#20917;&#12289;&#20307;&#32946;&#38203;&#28860;&#20559;&#22909;&#21644;&#36816;&#21160;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#22797;&#26434;&#30340;&#12290;&#20307;&#32946;&#38203;&#28860;&#30340;&#35752;&#35770;&#22987;&#32456;&#26174;&#31034;&#19982;&#20581;&#24247;&#34928;&#32769;&#32463;&#39564;&#23384;&#22312;&#31215;&#26497;&#30456;&#20851;&#65292;&#20294;&#19982;&#29305;&#23450;&#31867;&#22411;&#30340;&#32908;&#32905;&#39592;&#39612;&#38203;&#28860;&#27809;&#26377;&#26126;&#30830;&#30340;&#20851;&#32852;&#12290;&#21033;&#29992;&#26234;&#33021;&#25163;&#26426;&#30340;&#26222;&#21450;&#65292;&#23588;&#20854;&#26159;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#12290;&#36890;&#36807;25&#21517;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#65292;&#22312;&#36827;&#34892;&#20061;&#31181;&#36873;&#25321;&#36816;&#21160;&#26102;&#65292;&#26412;&#30740;&#31350;&#25104;&#21151;&#22320;&#30830;&#23450;&#20102;&#22312;&#21491;&#25163;&#21644;&#24038;&#25163;&#30340;&#32908;&#32905;&#21147;&#37327;&#36816;&#21160;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#20256;&#24863;&#22120;&#23646;&#24615;&#65292;&#20316;&#20026;&#24320;&#21457;&#22522;&#20110;LSTM&#31639;&#27861;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maintaining a high quality of life through physical activities (PA) to prevent health decline is crucial. However, the relationship between individuals health status, PA preferences, and motion factors is complex. PA discussions consistently show a positive correlation with healthy aging experiences, but no explicit relation to specific types of musculoskeletal exercises. Taking advantage of the increasingly widespread existence of smartphones, especially in Indonesia, this research utilizes embedded sensors for Human Activity Recognition (HAR). Based on 25 participants data, performing nine types of selected motion, this study has successfully identified important sensor attributes that play important roles in the right and left hands for muscle strength motions as the basis for developing machine learning models with the LSTM algorithm.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#30693;&#35782;&#33976;&#39311;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;AnomalyLLM&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#32593;&#32476;&#27169;&#20223;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#36890;&#36807;&#27604;&#36739;&#23398;&#29983;&#32593;&#32476;&#21644;&#25945;&#24072;&#32593;&#32476;&#30340;&#29305;&#24449;&#24046;&#24322;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2401.15123</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection. (arXiv:2401.15123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15123
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#30693;&#35782;&#33976;&#39311;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;AnomalyLLM&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#32593;&#32476;&#27169;&#20223;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#36890;&#36807;&#27604;&#36739;&#23398;&#29983;&#32593;&#32476;&#21644;&#25945;&#24072;&#32593;&#32476;&#30340;&#29305;&#24449;&#24046;&#24322;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#21487;&#29992;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#33719;&#24471;&#21487;&#27867;&#21270;&#30340;&#34920;&#31034;&#26144;&#23556;&#65292;&#36825;&#19982;&#21482;&#26377;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#20914;&#31361;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;AnomalyLLM&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#23398;&#29983;&#32593;&#32476;&#34987;&#35757;&#32451;&#25104;&#27169;&#20223;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#29305;&#24449;&#12290;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#24403;&#23398;&#29983;&#32593;&#32476;&#19982;&#25945;&#24072;&#32593;&#32476;&#30340;&#29305;&#24449;&#24046;&#24322;&#24456;&#22823;&#26102;&#65292;&#23601;&#26816;&#27979;&#21040;&#24322;&#24120;&#12290;&#20026;&#20102;&#36991;&#20813;&#23398;&#29983;&#32593;&#32476;&#23398;&#20064;&#21040;&#25945;&#24072;&#32593;&#32476;&#23545;&#24322;&#24120;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#20851;&#38190;&#31574;&#30053;&#12290;1) &#23558;&#20856;&#22411;&#20449;&#21495;&#34701;&#20837;&#23398;&#29983;&#32593;&#32476;&#65292;&#20197;&#24041;&#22266;&#27491;&#24120;&#29305;&#24449;&#25552;&#21462;&#12290;2) &#21152;&#26435;&#25945;&#24072;&#32593;&#32476;&#21644;&#23398;&#29983;&#32593;&#32476;&#30340;&#29305;&#24449;&#65292;&#20197;&#20943;&#23569;&#24322;&#24120;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised methods have gained prominence in time series anomaly detection due to the scarcity of available annotations. Nevertheless, they typically demand extensive training data to acquire a generalizable representation map, which conflicts with scenarios of a few available samples, thereby limiting their performance. To overcome the limitation, we propose \textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets. During the testing phase, anomalies are detected when the discrepancy between the features of the teacher and student networks is large. To circumvent the student network from learning the teacher network's feature of anomalous samples, we devise two key strategies. 1) Prototypical signals are incorporated into the student network to consolidate the normal feature extraction. 2) W
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#27169;&#25311;&#24182;&#26377;&#25928;&#27169;&#25311;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#30340;NeuralMD&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24314;&#27169;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15122</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#32423;&#23545;&#31216;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics. (arXiv:2401.15122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15122
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#27169;&#25311;&#24182;&#26377;&#25928;&#27169;&#25311;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#30340;NeuralMD&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24314;&#27169;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#20272;&#35745;&#36816;&#36755;&#24615;&#33021;&#21644;&#25506;&#32034;&#21475;&#34955;&#20301;&#28857;&#12290;&#36890;&#36807;&#25913;&#36827;&#25968;&#20540;&#26041;&#27861;&#20197;&#21450;&#26368;&#36817;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22686;&#24378;MD&#27169;&#25311;&#30340;&#25928;&#29575;&#24050;&#32463;&#26377;&#20102;&#24456;&#38271;&#30340;&#21382;&#21490;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#20934;&#30830;&#24314;&#27169;&#25193;&#23637;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#25311;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuralMD&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;ML&#36741;&#21161;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#29702;&#30340;&#26041;&#27861;&#65292;&#23558;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#20010;&#20351;&#29992;&#21521;&#37327;&#26694;&#26550;&#28385;&#36275;&#32676;&#23545;&#31216;&#24615;&#24182;&#25429;&#33719;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;BindingNet&#27169;&#22411;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#20010;&#22686;&#24378;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#23398;&#20064;&#36712;&#36857;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by augmenting them with machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations of protein-ligand binding dynamics. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural differential equation solver that learns the trajectory und
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28014;&#28857;&#36816;&#31639;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20108;&#36827;&#21046;&#38408;&#20540;&#21333;&#20803;&#25110;ReLU&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#35760;&#24518;&#20219;&#20309;&#23454;&#25968;&#36755;&#20837;/&#36755;&#20986;&#23545;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23567;&#35823;&#24046;&#20869;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.15121</link><description>&lt;p&gt;
ReLU&#21644;Step&#32593;&#32476;&#22312;&#28014;&#28857;&#36816;&#31639;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Expressive Power of ReLU and Step Networks under Floating-Point Operations. (arXiv:2401.15121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28014;&#28857;&#36816;&#31639;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20108;&#36827;&#21046;&#38408;&#20540;&#21333;&#20803;&#25110;ReLU&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#35760;&#24518;&#20219;&#20309;&#23454;&#25968;&#36755;&#20837;/&#36755;&#20986;&#23545;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23567;&#35823;&#24046;&#20869;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32467;&#26524;&#20551;&#35774;&#23454;&#25968;&#36755;&#20837;&#21644;&#21442;&#25968;&#20197;&#21450;&#22312;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#36807;&#31243;&#20013;&#36827;&#34892;&#31934;&#30830;&#36816;&#31639;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#22312;&#21482;&#33021;&#34920;&#31034;&#23454;&#25968;&#30340;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#65292;&#24182;&#19988;&#36827;&#34892;&#19981;&#31934;&#30830;&#30340;&#36816;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#26356;&#23454;&#38469;&#30340;&#35774;&#32622;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65306;&#20351;&#29992;&#28014;&#28857;&#25968;&#21644;&#28014;&#28857;&#36816;&#31639;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#32452;&#32467;&#26524;&#20551;&#35774;&#28014;&#28857;&#36816;&#31639;&#20013;&#65292;&#28014;&#28857;&#25968;&#30340;&#26377;&#25928;&#20301;&#25968;&#30001;&#26377;&#38480;&#20301;&#34920;&#31034;&#65292;&#20294;&#20854;&#25351;&#25968;&#21487;&#20197;&#21462;&#20219;&#20309;&#25972;&#25968;&#20540;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#20108;&#36827;&#21046;&#38408;&#20540;&#21333;&#20803;&#25110;ReLU&#21487;&#20197;&#35760;&#24518;&#20219;&#20309;&#26377;&#38480;&#30340;&#36755;&#20837;/&#36755;&#20986;&#23545;&#65292;&#24182;&#21487;&#20197;&#22312;&#23567;&#35823;&#24046;&#20869;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#28014;&#28857;&#36816;&#31639;&#19979;&#20851;&#20110;&#35760;&#24518;&#21644;&#36890;&#29992;&#36924;&#36817;&#30340;&#31867;&#20284;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of the expressive power of neural networks has investigated the fundamental limits of neural networks. Most existing results assume real-valued inputs and parameters as well as exact operations during the evaluation of neural networks. However, neural networks are typically executed on computers that can only represent a tiny subset of the reals and apply inexact operations. In this work, we analyze the expressive power of neural networks under a more realistic setup: when we use floating-point numbers and operations. Our first set of results assumes floating-point operations where the significand of a float is represented by finite bits but its exponent can take any integer value. Under this setup, we show that neural networks using a binary threshold unit or ReLU can memorize any finite input/output pairs and can approximate any continuous function within a small error. We also show similar results on memorization and universal approximation when floating-point operations u
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#30340;&#19978;&#19979;&#25991;&#39537;&#21160;&#30340;&#33258;&#25105;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29615;&#22659;&#30340;&#21382;&#21490;&#31354;&#38388;&#19978;&#19979;&#25991;&#25552;&#20379;&#30340;&#30456;&#20284;&#24615;&#20449;&#21495;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38476;&#29983;&#29615;&#22659;&#20013;&#12290;&#35813;&#26041;&#27861;&#26377;&#28508;&#21147;&#20026;&#20195;&#29702;&#22312;&#20855;&#26377;&#29420;&#29305;&#35270;&#35273;&#29305;&#24449;&#30340;&#26032;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#30340;&#35270;&#35273;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.15120</link><description>&lt;p&gt;
&#22522;&#20110;&#29615;&#22659;&#30340;&#19978;&#19979;&#25991;&#39537;&#21160;&#30340;&#33258;&#25105;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;&#65306;&#21033;&#29992;&#29615;&#22659;&#20316;&#20026;&#25968;&#25454;&#28304;
&lt;/p&gt;
&lt;p&gt;
Context-driven self-supervised visual learning: Harnessing the environment as a data source. (arXiv:2401.15120v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15120
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#30340;&#19978;&#19979;&#25991;&#39537;&#21160;&#30340;&#33258;&#25105;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29615;&#22659;&#30340;&#21382;&#21490;&#31354;&#38388;&#19978;&#19979;&#25991;&#25552;&#20379;&#30340;&#30456;&#20284;&#24615;&#20449;&#21495;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38476;&#29983;&#29615;&#22659;&#20013;&#12290;&#35813;&#26041;&#27861;&#26377;&#28508;&#21147;&#20026;&#20195;&#29702;&#22312;&#20855;&#26377;&#29420;&#29305;&#35270;&#35273;&#29305;&#24449;&#30340;&#26032;&#29615;&#22659;&#20013;&#23454;&#29616;&#24555;&#36895;&#30340;&#35270;&#35273;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23398;&#20064;&#36890;&#24120;&#21457;&#29983;&#22312;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#23545;&#19968;&#33268;&#29615;&#22659;&#20013;&#30340;&#20301;&#32622;&#30340;&#25506;&#32034;&#21644;&#36319;&#36394;&#26469;&#33719;&#21462;&#25216;&#33021;&#12290;&#20195;&#29702;&#30340;&#21382;&#21490;&#31354;&#38388;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#33258;&#25105;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#30456;&#20284;&#24615;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29615;&#22659;&#31354;&#38388;&#30456;&#20284;&#24615;&#65288;ESS&#65289;&#65292;&#23427;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#27169;&#25311;&#30340;&#36924;&#30495;&#29615;&#22659;&#20013;&#20351;&#29992;&#22270;&#20687;&#20316;&#20026;&#23454;&#39564;&#35774;&#32622;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ESS&#20248;&#20110;&#20256;&#32479;&#30340;&#23454;&#20363;&#37492;&#21035;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20174;&#30456;&#21516;&#29615;&#22659;&#20013;&#37319;&#26679;&#26356;&#22810;&#25968;&#25454;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#22686;&#24378;&#12290;ESS&#22312;&#25151;&#38388;&#20998;&#31867;&#21644;&#31354;&#38388;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#29087;&#32451;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#38476;&#29983;&#29615;&#22659;&#20013;&#12290;&#36825;&#31181;&#23398;&#20064;&#33539;&#24335;&#26377;&#28508;&#21147;&#22312;&#20855;&#26377;&#29420;&#29305;&#35270;&#35273;&#29305;&#24449;&#30340;&#26032;&#29615;&#22659;&#20013;&#20351;&#20195;&#29702;&#33021;&#22815;&#24555;&#36895;&#36827;&#34892;&#35270;&#35273;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual learning often occurs in a specific context, where an agent acquires skills through exploration and tracking of its location in a consistent environment. The historical spatial context of the agent provides a similarity signal for self-supervised contrastive learning. We present a unique approach, termed Environmental Spatial Similarity (ESS), that complements existing contrastive learning methods. Using images from simulated, photorealistic environments as an experimental setting, we demonstrate that ESS outperforms traditional instance discrimination approaches. Moreover, sampling additional data from the same environment substantially improves accuracy and provides new augmentations. ESS allows remarkable proficiency in room classification and spatial prediction tasks, especially in unfamiliar environments. This learning paradigm has the potential to enable rapid visual learning in agents operating in new environments with unique visual characteristics. Potentially transforma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#35299;&#37322;&#20102;&#26368;&#20808;&#36827;&#30340;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;13&#20010;&#36755;&#20837;&#29305;&#24449;&#19982;3,142&#20010;&#32654;&#22269;&#21439;&#30340;&#19977;&#24180;&#26085;&#26696;&#20363;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#33021;&#22815;&#22312;&#36807;&#21435;&#20004;&#21608;&#30340;&#22522;&#30784;&#19978;&#39044;&#27979;&#25509;&#19979;&#26469;&#20004;&#21608;&#30340;COVID-19&#24863;&#26579;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#24863;&#26579;&#23545;8&#20010;&#20154;&#21475;&#24180;&#40836;&#32452;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15119</link><description>&lt;p&gt;
&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22411;&#21644;COVID-19&#24863;&#26579;&#23545;&#20154;&#21475;&#24180;&#40836;&#32452;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Interpreting Time Series Transformer Models and Sensitivity Analysis of Population Age Groups to COVID-19 Infections. (arXiv:2401.15119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15119
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#35299;&#37322;&#20102;&#26368;&#20808;&#36827;&#30340;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;13&#20010;&#36755;&#20837;&#29305;&#24449;&#19982;3,142&#20010;&#32654;&#22269;&#21439;&#30340;&#19977;&#24180;&#26085;&#26696;&#20363;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#33021;&#22815;&#22312;&#36807;&#21435;&#20004;&#21608;&#30340;&#22522;&#30784;&#19978;&#39044;&#27979;&#25509;&#19979;&#26469;&#20004;&#21608;&#30340;COVID-19&#24863;&#26579;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#24863;&#26579;&#23545;8&#20010;&#20154;&#21475;&#24180;&#40836;&#32452;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#21644;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#23398;&#20064;&#27169;&#24335;&#20197;&#36827;&#34892;&#23454;&#26102;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;Transformer&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#20351;&#35299;&#37322;&#20010;&#20307;&#29305;&#24449;&#23545;&#39044;&#27979;&#24433;&#21709;&#30340;&#25361;&#25112;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#37322;&#26368;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#20026;&#20102;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;3142&#20010;&#32654;&#22269;&#21439;&#30340;&#19977;&#24180;&#26085;&#26696;&#20363;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20845;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#36873;&#25321;&#20102;&#26368;&#20339;&#30340;COVID-19&#24863;&#26579;&#39044;&#27979;&#27169;&#22411;&#12290;&#20351;&#29992;&#36807;&#21435;&#20004;&#21608;&#30340;13&#20010;&#36755;&#20837;&#29305;&#24449;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#27979;&#25509;&#19979;&#26469;&#20004;&#21608;&#30340;&#30149;&#20363;&#25968;&#37327;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#39044;&#27979;&#23545;8&#20010;&#20154;&#21475;&#24180;&#40836;&#32452;&#30340;&#25935;&#24863;&#24615;&#65292;&#20197;&#21450;&#39640;&#24230;&#21160;&#24577;&#30340;&#22810;&#21464;&#37327;&#24863;&#26579;&#25968;&#25454;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#25200;&#21160;&#30340;&#35299;&#37322;&#26041;&#27861;&#19982;&#30456;&#20851;&#24037;&#20316;&#36827;&#34892;&#27604;&#36739;&#65292;&#24635;&#20849;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Interpreting deep learning time series models is crucial in understanding the model's behavior and learning patterns from raw data for real-time decision-making. However, the complexity inherent in transformer-based time series models poses challenges in explaining the impact of individual features on predictions. In this study, we leverage recent local interpretation methods to interpret state-of-the-art time series models. To use real-world datasets, we collected three years of daily case data for 3,142 US counties. Firstly, we compare six transformer-based models and choose the best prediction model for COVID-19 infection. Using 13 input features from the last two weeks, we can predict the cases for the next two weeks. Secondly, we present an innovative way to evaluate the prediction sensitivity to 8 population age groups over highly dynamic multivariate infection data. Thirdly, we compare our proposed perturbation-based interpretation method with related work, including a total of 
&lt;/p&gt;</description></item><item><title>GeoDecoder&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#22320;&#22270;&#20013;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#22788;&#29702;&#27169;&#22359;&#65292;&#26080;&#32541;&#38598;&#25104;&#22806;&#37096;&#25968;&#25454;&#21644;&#29305;&#24449;&#65292;&#20197;&#21450;&#25191;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#25191;&#34892;&#65292;&#23454;&#29616;&#20102;&#24378;&#21270;&#22320;&#22270;&#35748;&#30693;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.15118</link><description>&lt;p&gt;
GeoDecoder: &#24378;&#21270;&#22810;&#27169;&#24577;&#22320;&#22270;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
GeoDecoder: Empowering Multimodal Map Understanding. (arXiv:2401.15118v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15118
&lt;/p&gt;
&lt;p&gt;
GeoDecoder&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#22320;&#22270;&#20013;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#22788;&#29702;&#27169;&#22359;&#65292;&#26080;&#32541;&#38598;&#25104;&#22806;&#37096;&#25968;&#25454;&#21644;&#29305;&#24449;&#65292;&#20197;&#21450;&#25191;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#25191;&#34892;&#65292;&#23454;&#29616;&#20102;&#24378;&#21270;&#22320;&#22270;&#35748;&#30693;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GeoDecoder&#65292;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#22320;&#22270;&#20013;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;GeoDecoder&#22522;&#20110;BeitGPT&#26550;&#26500;&#26500;&#24314;&#65292;&#24182;&#38598;&#25104;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#22788;&#29702;&#30340;&#19987;&#19994;&#27169;&#22359;&#12290;&#22312;&#22270;&#20687;&#26041;&#38754;&#65292;GeoDecoder&#21033;&#29992;&#39640;&#24503;&#22320;&#22270;&#20316;&#20026;&#24213;&#22270;&#65292;&#35813;&#22320;&#22270;&#20869;&#32622;&#20102;&#36947;&#36335;&#21644;&#24314;&#31569;&#24418;&#29366;&#12289;&#30456;&#23545;&#20301;&#32622;&#21644;&#20854;&#20182;&#23646;&#24615;&#30340;&#37325;&#35201;&#32454;&#33410;&#12290;&#36890;&#36807;&#28210;&#26579;&#25216;&#26415;&#65292;&#35813;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#20102;&#22806;&#37096;&#25968;&#25454;&#21644;&#29305;&#24449;&#65292;&#22914;&#31526;&#21495;&#26631;&#35760;&#12289;&#39550;&#39542;&#36712;&#36857;&#12289;&#28909;&#21147;&#22270;&#21644;&#29992;&#25143;&#23450;&#20041;&#30340;&#26631;&#35760;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#29305;&#24449;&#24037;&#31243;&#38656;&#27714;&#12290;GeoDecoder&#30340;&#25991;&#26412;&#27169;&#22359;&#25509;&#21463;&#21508;&#31181;&#19978;&#19979;&#25991;&#25991;&#26412;&#21644;&#38382;&#39064;&#25552;&#31034;&#65292;&#24182;&#29983;&#25104;&#31867;&#20284;&#20110;GPT&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#20801;&#35768;&#22312;&#21516;&#19968;&#27169;&#22411;&#20013;&#36827;&#34892;&#22810;&#20010;&#20219;&#21153;&#30340;&#35757;&#32451;&#21644;&#25191;&#34892;&#12290;&#20026;&#20102;&#22686;&#24378;&#22320;&#22270;&#35748;&#30693;&#33021;&#21147;&#24182;&#20351;GeoDecoder&#33719;&#21462;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
This paper presents GeoDecoder, a dedicated multimodal model designed for processing geospatial information in maps. Built on the BeitGPT architecture, GeoDecoder incorporates specialized expert modules for image and text processing. On the image side, GeoDecoder utilizes GaoDe Amap as the underlying base map, which inherently encompasses essential details about road and building shapes, relative positions, and other attributes. Through the utilization of rendering techniques, the model seamlessly integrates external data and features such as symbol markers, drive trajectories, heatmaps, and user-defined markers, eliminating the need for extra feature engineering. The text module of GeoDecoder accepts various context texts and question prompts, generating text outputs in the style of GPT. Furthermore, the GPT-based model allows for the training and execution of multiple tasks within the same model in an end-to-end manner. To enhance map cognition and enable GeoDecoder to acquire knowle
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21483;&#20570;&#23545;&#35805;&#32676;&#20307;&#26234;&#33021;&#65288;CSI&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#28982;&#23545;&#35805;&#35752;&#35770;&#26469;&#25552;&#39640;&#32593;&#32476;&#20154;&#31867;&#32676;&#20307;&#30340;&#20915;&#31574;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;CSI&#24179;&#21488;&#30340;&#23454;&#26102;&#32676;&#20307;&#21442;&#21152;Raven&#26234;&#21830;&#27979;&#35797;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.15109</link><description>&lt;p&gt;
&#23454;&#29616;&#38598;&#20307;&#36229;&#26234;&#33021;&#65306;&#21033;&#29992;&#23545;&#35805;&#32676;&#20307;&#22686;&#24378;&#32676;&#20307;&#26234;&#21830;
&lt;/p&gt;
&lt;p&gt;
Towards Collective Superintelligence: Amplifying Group IQ using Conversational Swarms. (arXiv:2401.15109v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15109
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21483;&#20570;&#23545;&#35805;&#32676;&#20307;&#26234;&#33021;&#65288;CSI&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#28982;&#23545;&#35805;&#35752;&#35770;&#26469;&#25552;&#39640;&#32593;&#32476;&#20154;&#31867;&#32676;&#20307;&#30340;&#20915;&#31574;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;CSI&#24179;&#21488;&#30340;&#23454;&#26102;&#32676;&#20307;&#21442;&#21152;Raven&#26234;&#21830;&#27979;&#35797;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#26234;&#33021;&#65288;SI&#65289;&#26159;&#19968;&#31181;&#33258;&#28982;&#29616;&#35937;&#65292;&#33021;&#22815;&#36890;&#36807;&#24418;&#25104;&#23454;&#26102;&#31995;&#32479;&#26469;&#22686;&#24378;&#29983;&#29289;&#32676;&#20307;&#30340;&#32508;&#21512;&#26234;&#21147;&#12290;&#20154;&#24037;&#32676;&#20307;&#26234;&#33021;&#65288;&#25110;&#32676;&#20307;AI&#65289;&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#24418;&#25104;&#31867;&#20284;&#31995;&#32479;&#26469;&#22686;&#24378;&#32593;&#32476;&#21270;&#20154;&#31867;&#32676;&#20307;&#30340;&#32508;&#21512;&#26234;&#33021;&#12290;&#36807;&#21435;&#65292;&#22522;&#20110;&#32676;&#20307;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#29421;&#20041;&#20219;&#21153;&#65292;&#22914;&#27010;&#29575;&#39044;&#27979;&#21644;&#22810;&#39033;&#36873;&#25321;&#20915;&#31574;&#12290;&#19968;&#31181;&#21517;&#20026;&#23545;&#35805;&#32676;&#20307;&#26234;&#33021;&#65288;CSI&#65289;&#30340;&#26032;&#25216;&#26415;&#20110;2023&#24180;&#24320;&#21457;&#20986;&#26469;&#65292;&#36890;&#36807;&#33258;&#28982;&#23545;&#35805;&#35752;&#35770;&#26469;&#22686;&#24378;&#32593;&#32476;&#20154;&#31867;&#32676;&#20307;&#30340;&#20915;&#31574;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;CSI&#24179;&#21488;&#30340;&#23454;&#26102;&#32676;&#20307;&#21442;&#21152;&#19968;&#31181;&#34987;&#31216;&#20026;Raven&#39640;&#32423;&#28176;&#36827;&#30697;&#38453;&#65288;RAPM&#65289;&#30340;&#24120;&#35265;&#26234;&#21830;&#27979;&#35797;&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#22522;&#20934;&#32452;&#21442;&#19982;&#32773;&#36890;&#36807;&#20256;&#32479;&#35843;&#26597;&#26041;&#24335;&#25509;&#21463;&#20102;Raven&#26234;&#21830;&#27979;&#35797;&#12290;&#36825;&#20010;&#32452;&#30340;&#24179;&#22343;&#27491;&#30830;&#29575;&#20026;45.6%&#12290;&#28982;&#21518;&#65292;&#22823;&#32422;35&#20154;&#30340;&#32676;&#20307;&#22238;&#31572;&#20102;&#21516;&#26679;&#30340;&#26234;&#21830;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Swarm Intelligence (SI) is a natural phenomenon that enables biological groups to amplify their combined intellect by forming real-time systems. Artificial Swarm Intelligence (or Swarm AI) is a technology that enables networked human groups to amplify their combined intelligence by forming similar systems. In the past, swarm-based methods were constrained to narrowly defined tasks like probabilistic forecasting and multiple-choice decision making. A new technology called Conversational Swarm Intelligence (CSI) was developed in 2023 that amplifies the decision-making accuracy of networked human groups through natural conversational deliberations. The current study evaluated the ability of real-time groups using a CSI platform to take a common IQ test known as Raven's Advanced Progressive Matrices (RAPM). First, a baseline group of participants took the Raven's IQ test by traditional survey. This group averaged 45.6% correct. Then, groups of approximately 35 individuals answered IQ test 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#24555;&#36895;&#20805;&#30005;&#30005;&#21160;&#36710;&#20013;&#24515;&#30340;&#21160;&#24577;&#23450;&#20215;&#31454;&#20105;&#12290;&#36890;&#36807;&#39044;&#27979;&#24615;&#36141;&#20080;&#30005;&#21147;&#38656;&#27714;&#21644;&#35774;&#23450;&#31454;&#20105;&#24615;&#20215;&#26684;&#31574;&#30053;&#65292;&#20805;&#30005;&#31449;&#21487;&#20197;&#22312;&#31454;&#20105;&#20013;&#36827;&#34892;&#26377;&#25928;&#23450;&#20215;&#12290;</title><link>http://arxiv.org/abs/2401.15108</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#31454;&#20105;&#20013;&#20026;&#24555;&#36895;&#20805;&#30005;&#30005;&#21160;&#36710;&#20013;&#24515;&#30340;&#21160;&#24577;&#23450;&#20215;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition. (arXiv:2401.15108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#24555;&#36895;&#20805;&#30005;&#30005;&#21160;&#36710;&#20013;&#24515;&#30340;&#21160;&#24577;&#23450;&#20215;&#31454;&#20105;&#12290;&#36890;&#36807;&#39044;&#27979;&#24615;&#36141;&#20080;&#30005;&#21147;&#38656;&#27714;&#21644;&#35774;&#23450;&#31454;&#20105;&#24615;&#20215;&#26684;&#31574;&#30053;&#65292;&#20805;&#30005;&#31449;&#21487;&#20197;&#22312;&#31454;&#20105;&#20013;&#36827;&#34892;&#26377;&#25928;&#23450;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#20805;&#30005;&#31449;&#23558;&#25104;&#20026;&#20840;&#29699;&#26032;&#24314;&#20132;&#36890;&#30005;&#27668;&#21270;&#22522;&#30784;&#35774;&#26045;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#20123;&#20805;&#30005;&#31449;&#23558;&#25215;&#36733;&#35768;&#22810;&#30452;&#27969;&#24555;&#36895;&#20805;&#30005;&#35774;&#22791;&#65292;&#20165;&#21487;&#20379;&#30005;&#21160;&#36710;&#36742;&#20805;&#30005;&#20351;&#29992;&#12290;&#31867;&#20284;&#20110;&#27773;&#27833;&#21152;&#27833;&#31449;&#65292;&#21516;&#19968;&#22320;&#21306;&#30340;&#24555;&#36895;&#20805;&#30005;&#31449;&#23558;&#26681;&#25454;&#31454;&#20105;&#35843;&#25972;&#20215;&#26684;&#20197;&#21560;&#24341;&#21516;&#19968;&#32676;&#30005;&#21160;&#36710;&#20027;&#12290;&#36825;&#20123;&#20805;&#30005;&#31449;&#23558;&#19982;&#30005;&#21147;&#32593;&#32476;&#36827;&#34892;&#20132;&#20114;&#65292;&#36890;&#36807;&#39044;&#27979;&#24615;&#36141;&#20080;&#22312;&#21069;&#19968;&#22825;&#30005;&#21147;&#24066;&#22330;&#19978;&#30340;&#30005;&#21147;&#38656;&#27714;&#65292;&#24182;&#22312;&#23454;&#26102;&#24066;&#22330;&#19978;&#28385;&#36275;&#24046;&#39069;&#38656;&#27714;&#12290;&#20805;&#30005;&#31449;&#21487;&#33021;&#37197;&#22791;&#34917;&#20805;&#30005;&#27744;&#20648;&#33021;&#31995;&#32479;&#29992;&#20110;&#22871;&#21033;&#12290;&#26412;&#25991;&#38024;&#23545;&#20805;&#30005;&#31449;&#31454;&#20105;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#27493;&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#24577;&#23450;&#20215;&#26041;&#27861;&#12290;&#39318;&#20808;&#36890;&#36807;&#27714;&#35299;&#38543;&#26426;&#30340;&#21069;&#19968;&#22825;&#30005;&#21147;&#38656;&#27714;&#27169;&#22411;&#24471;&#21040;&#32435;&#20837;&#25215;&#35834;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#28216;&#25103;&#24314;&#27169;&#20026;&#31454;&#20105;&#26469;&#24471;&#21040;&#20805;&#30005;&#31449;&#30340;&#20215;&#26684;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast-charging hubs for electric vehicles will soon become part of the newly built infrastructure for transportation electrification across the world. These hubs are expected to host many DC fast-charging stations and will admit EVs only for charging. Like the gasoline refueling stations, fast-charging hubs in a neighborhood will dynamically vary their prices to compete for the same pool of EV owners. These hubs will interact with the electric power network by making purchase commitments for a significant part of their power needs in the day-ahead (DA) electricity market and meeting the difference from the real-time (RT) market. Hubs may have supplemental battery storage systems (BSS), which they will use for arbitrage. In this paper, we develop a two-step data-driven dynamic pricing methodology for hubs in price competition. We first obtain the DA commitment by solving a stochastic DA commitment model. Thereafter we obtain the hub pricing strategies by modeling the game as a competitiv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#32508;&#21512;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#21644;&#20449;&#24687;&#32463;&#27982;&#23398;&#65292;&#25552;&#20986;&#20102;&#20915;&#31574;&#38382;&#39064;&#30340;&#24191;&#27867;&#36866;&#29992;&#23450;&#20041;&#12290;&#20026;&#20102;&#23558;&#20154;&#31867;&#20915;&#31574;&#30340;&#19979;&#38477;&#24402;&#21646;&#20110;&#20559;&#35265;&#24418;&#24335;&#65292;&#23454;&#39564;&#24517;&#39035;&#21521;&#21442;&#19982;&#32773;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#35782;&#21035;&#35268;&#33539;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#20316;&#32773;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#30740;&#31350;&#30340;&#35780;&#20272;&#65292;&#21482;&#26377;17%&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#25551;&#36848;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#20559;&#31163;&#20102;&#33391;&#22909;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2401.15106</link><description>&lt;p&gt;
&#20915;&#31574;&#29702;&#35770;&#22522;&#30784;&#23545;&#35780;&#20272;&#20154;&#31867;&#20915;&#31574;&#30340;&#23454;&#39564;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Decision Theoretic Foundations for Experiments Evaluating Human Decisions. (arXiv:2401.15106v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#32508;&#21512;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#21644;&#20449;&#24687;&#32463;&#27982;&#23398;&#65292;&#25552;&#20986;&#20102;&#20915;&#31574;&#38382;&#39064;&#30340;&#24191;&#27867;&#36866;&#29992;&#23450;&#20041;&#12290;&#20026;&#20102;&#23558;&#20154;&#31867;&#20915;&#31574;&#30340;&#19979;&#38477;&#24402;&#21646;&#20110;&#20559;&#35265;&#24418;&#24335;&#65292;&#23454;&#39564;&#24517;&#39035;&#21521;&#21442;&#19982;&#32773;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#35782;&#21035;&#35268;&#33539;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#20316;&#32773;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#30740;&#31350;&#30340;&#35780;&#20272;&#65292;&#21482;&#26377;17%&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#25551;&#36848;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#20559;&#31163;&#20102;&#33391;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#23637;&#31034;&#30340;&#20915;&#31574;&#26159;&#21487;&#35299;&#37322;AI&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#20197;&#21450;&#25968;&#25454;&#21487;&#35270;&#21270;&#31561;&#39046;&#22495;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#28982;&#32780;&#65292;&#20915;&#31574;&#38382;&#39064;&#30340;&#23450;&#20041;&#20197;&#21450;&#23454;&#39564;&#24517;&#39035;&#20855;&#22791;&#30340;&#26465;&#20214;&#20197;&#24471;&#20986;&#20154;&#31867;&#20915;&#31574;&#23384;&#22312;&#32570;&#38519;&#30340;&#32467;&#35770;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#36866;&#29992;&#30340;&#20915;&#31574;&#38382;&#39064;&#23450;&#20041;&#65292;&#35813;&#23450;&#20041;&#26159;&#20174;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#21644;&#20449;&#24687;&#32463;&#27982;&#23398;&#20013;&#32508;&#21512;&#25552;&#28860;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#35201;&#23558;&#20154;&#31867;&#32489;&#25928;&#19979;&#38477;&#24402;&#21646;&#20110;&#26576;&#31181;&#20559;&#35265;&#24418;&#24335;&#65292;&#23454;&#39564;&#24517;&#39035;&#21521;&#21442;&#19982;&#32773;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#65292;&#20197;&#20415;&#21512;&#29702;&#30340;&#20195;&#29702;&#33021;&#22815;&#35782;&#21035;&#35268;&#33539;&#20915;&#31574;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#36817;&#26377;&#20851;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#25991;&#29486;&#20013;&#23545;&#20915;&#31574;&#21046;&#23450;&#36827;&#34892;&#30340;&#35780;&#20272;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#36798;&#21040;&#20102;&#36825;&#19968;&#26631;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#26377;35&#39033;&#22768;&#31216;&#30830;&#23450;&#20102;&#26377;&#20559;&#24046;&#34892;&#20026;&#30340;&#30740;&#31350;&#20013;&#30340;6&#39033;&#65288;17%&#65289;&#21521;&#21442;&#19982;&#32773;&#25552;&#20379;&#20102;&#36275;&#22815;&#20449;&#24687;&#26469;&#25551;&#36848;&#20854;&#34892;&#20026;&#20559;&#31163;&#33391;&#22909;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Decision-making with information displays is a key focus of research in areas like explainable AI, human-AI teaming, and data visualization. However, what constitutes a decision problem, and what is required for an experiment to be capable of concluding that human decisions are flawed in some way, remain open to speculation. We present a widely applicable definition of a decision problem synthesized from statistical decision theory and information economics. We argue that to attribute loss in human performance to forms of bias, an experiment must provide participants with the information that a rational agent would need to identify the normative decision. We evaluate the extent to which recent evaluations of decision-making from the literature on AI-assisted decisions achieve this criteria. We find that only 6 (17\%) of 35 studies that claim to identify biased behavior present participants with sufficient information to characterize their behavior as deviating from good decision-making
&lt;/p&gt;</description></item><item><title>PruneSymNet&#26159;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#36138;&#23146;&#20462;&#21098;&#31639;&#27861;&#25552;&#21462;&#23376;&#32593;&#32476;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.15103</link><description>&lt;p&gt;
PruneSymNet&#65306;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#30340;&#31526;&#21495;&#31070;&#32463;&#32593;&#32476;&#21644;&#20462;&#21098;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for Symbolic Regression. (arXiv:2401.15103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15103
&lt;/p&gt;
&lt;p&gt;
PruneSymNet&#26159;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#36138;&#23146;&#20462;&#21098;&#31639;&#27861;&#25552;&#21462;&#23376;&#32593;&#32476;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26088;&#22312;&#20174;&#25968;&#25454;&#20013;&#23548;&#20986;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#35299;&#37322;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PruneSymNet&#30340;&#31526;&#21495;&#32593;&#32476;&#65292;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#12290;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#28608;&#27963;&#20989;&#25968;&#30001;&#24120;&#35265;&#30340;&#22522;&#26412;&#20989;&#25968;&#21644;&#36816;&#31639;&#31526;&#32452;&#25104;&#12290;&#25972;&#20010;&#32593;&#32476;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#23376;&#32593;&#32476;&#23545;&#24212;&#19968;&#20010;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#21462;&#36825;&#20123;&#23376;&#32593;&#32476;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#23146;&#20462;&#21098;&#31639;&#27861;&#65292;&#23558;&#32593;&#32476;&#21098;&#25104;&#23376;&#32593;&#32476;&#65292;&#21516;&#26102;&#30830;&#20445;&#25968;&#25454;&#25311;&#21512;&#30340;&#31934;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#36138;&#23146;&#20462;&#21098;&#31639;&#27861;&#27599;&#27425;&#20462;&#21098;&#37117;&#20445;&#30041;&#25439;&#22833;&#26368;&#23567;&#30340;&#36793;&#65292;&#20294;&#36138;&#23146;&#31639;&#27861;&#36890;&#24120;&#26080;&#27861;&#24471;&#21040;&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#26463;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression aims to derive interpretable symbolic expressions from data in order to better understand and interpret data. %which plays an important role in knowledge discovery and interpretable machine learning.  In this study, a symbolic network called PruneSymNet is proposed for symbolic regression. This is a novel neural network whose activation function consists of common elementary functions and operators. The whole network is differentiable and can be trained by gradient descent method. Each subnetwork in the network corresponds to an expression, and our goal is to extract such subnetworks to get the desired symbolic expression.  Therefore, a greedy pruning algorithm is proposed to prune the network into a subnetwork while ensuring the accuracy of data fitting. The proposed greedy pruning algorithm preserves the edge with the least loss in each pruning, but greedy algorithm often can not get the optimal solution. In order to alleviate this problem, we combine beam search 
&lt;/p&gt;</description></item><item><title>Hi-Core&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#30693;&#35782;&#36801;&#31227;&#26469;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;Hi-Core&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#30693;&#35782;&#36801;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.15098</link><description>&lt;p&gt;
Hi-Core: &#38754;&#21521;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#23618;&#27425;&#21270;&#30693;&#35782;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning. (arXiv:2401.15098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15098
&lt;/p&gt;
&lt;p&gt;
Hi-Core&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#30693;&#35782;&#36801;&#31227;&#26469;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;Hi-Core&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#30693;&#35782;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;Continual Reinforcement Learning, CRL&#65289;&#36171;&#20104;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20445;&#30041;&#20808;&#21069;&#30340;&#30693;&#35782;&#24182;&#21033;&#29992;&#23427;&#26469;&#20419;&#36827;&#26410;&#26469;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#19987;&#27880;&#20110;&#22312;&#31867;&#20284;&#20219;&#21153;&#20043;&#38388;&#20256;&#36755;&#20302;&#23618;&#27425;&#30340;&#30693;&#35782;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#30693;&#25511;&#21046;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#23548;&#33268;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#36801;&#31227;&#19981;&#36275;&#12290;&#20026;&#20102;&#22686;&#24378;&#39640;&#23618;&#27425;&#30340;&#30693;&#35782;&#36801;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning)&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#30001;&#20004;&#23618;&#32467;&#26500;&#32452;&#25104;&#65306;1) &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Model, LLM&#65289;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;2) &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#24211;&#65288;&#31574;&#30053;&#24211;&#65289;&#26469;&#23384;&#20648;&#21487;&#20197;&#29992;&#20110;&#23618;&#27425;&#21270;&#30693;&#35782;&#36801;&#31227;&#30340;&#31574;&#30053;&#12290;&#22312;MiniGr&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual reinforcement learning (CRL) empowers RL agents with the ability to learn from a sequence of tasks, preserving previous knowledge and leveraging it to facilitate future learning. However, existing methods often focus on transferring low-level knowledge across similar tasks, which neglects the hierarchical structure of human cognitive control, resulting in insufficient knowledge transfer across diverse tasks. To enhance high-level knowledge transfer, we propose a novel framework named Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning), which is structured in two layers: 1) the high-level policy formulation which utilizes the powerful reasoning ability of the Large Language Model (LLM) to set goals and 2) the low-level policy learning through RL which is oriented by high-level goals. Moreover, the knowledge base (policy library) is constructed to store policies that can be retrieved for hierarchical knowledge transfer. Experiments conducted in MiniGr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22312;&#35299;&#20915;&#35748;&#30693;&#24378;&#24230;&#30340;&#31185;&#23398;&#38382;&#39064;&#26102;&#26159;&#21542;&#33021;&#36229;&#36234;&#20154;&#31867;&#65292;&#24182;&#36890;&#36807;&#19982;&#23398;&#29983;&#30340;&#23545;&#27604;&#23454;&#39564;&#21457;&#29616;&#65292;ChatGPT&#21644;GPT-4&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.15081</link><description>&lt;p&gt;
&#12298;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;ChatGPT&#33021;&#21542;&#22312;&#31185;&#23398;&#39046;&#22495;&#30340;&#35748;&#30693;&#38656;&#27714;&#38382;&#39064;&#35299;&#20915;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#65311;&#12299;
&lt;/p&gt;
&lt;p&gt;
Can generative AI and ChatGPT outperform humans on cognitive-demanding problem-solving tasks in science?. (arXiv:2401.15081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22312;&#35299;&#20915;&#35748;&#30693;&#24378;&#24230;&#30340;&#31185;&#23398;&#38382;&#39064;&#26102;&#26159;&#21542;&#33021;&#36229;&#36234;&#20154;&#31867;&#65292;&#24182;&#36890;&#36807;&#19982;&#23398;&#29983;&#30340;&#23545;&#27604;&#23454;&#39564;&#21457;&#29616;&#65292;ChatGPT&#21644;GPT-4&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#32771;&#23519;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#24037;&#20855;&#33021;&#21542;&#20811;&#26381;&#20154;&#31867;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#25152;&#36973;&#21463;&#30340;&#35748;&#30693;&#24378;&#24230;&#12290;&#25105;&#20204;&#23558;ChatGPT&#21644;GPT-4&#22312;2019&#24180;NAEP&#31185;&#23398;&#35780;&#20272;&#20013;&#19982;&#23398;&#29983;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#26681;&#25454;&#20219;&#21153;&#30340;&#35748;&#30693;&#38656;&#27714;&#23545;&#20116;&#21313;&#22235;&#20010;&#20219;&#21153;&#36827;&#34892;&#20102;&#32534;&#30721;&#65292;&#20854;&#20013;&#21253;&#25324;&#20219;&#21153;&#30340;&#35748;&#30693;&#22797;&#26434;&#24230;&#21644;&#32500;&#24230;&#12290;&#20351;&#29992;NAEP&#30340;&#35780;&#20998;&#26631;&#20934;&#23545;ChatGPT&#21644;GPT-4&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#35780;&#20998;&#12290;&#23545;&#21487;&#29992;&#25968;&#25454;&#30340;&#20998;&#26512;&#22522;&#20110;&#27599;&#20010;&#38382;&#39064;&#27491;&#30830;&#22238;&#31572;&#30340;&#23398;&#29983;&#30340;&#24179;&#22343;&#33021;&#21147;&#20998;&#25968;&#21644;&#22238;&#31572;&#27599;&#20010;&#38382;&#39064;&#30340;&#23398;&#29983;&#30340;&#30334;&#20998;&#27604;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#21644;GPT-4&#22312;&#22823;&#22810;&#25968;&#22238;&#31572;NAEP&#31185;&#23398;&#35780;&#20272;&#30340;&#23398;&#29983;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#38543;&#30528;NAEP&#20219;&#21153;&#30340;&#35748;&#30693;&#38656;&#27714;&#22686;&#21152;&#65292;&#38656;&#35201;&#20855;&#22791;&#32479;&#35745;&#19978;&#26356;&#39640;&#30340;&#24179;&#22343;&#23398;&#29983;&#33021;&#21147;&#20998;&#25968;&#25165;&#33021;&#27491;&#30830;&#22238;&#31572;&#38382;&#39064;&#12290;&#27492;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;GAI&#24037;&#20855;&#22312;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#30340;&#26377;&#30410;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aimed to examine an assumption that generative artificial intelligence (GAI) tools can overcome the cognitive intensity that humans suffer when solving problems. We compared the performance of ChatGPT and GPT-4 on 2019 NAEP science assessments with students by cognitive demands of the items. Fifty-four tasks were coded by experts using a two-dimensional cognitive load framework, including task cognitive complexity and dimensionality. ChatGPT and GPT-4 responses were scored using the scoring keys of NAEP. The analysis of the available data was based on the average student ability scores for students who answered each item correctly and the percentage of students who responded to individual items. Results showed that both ChatGPT and GPT-4 consistently outperformed most students who answered the NAEP science assessments. As the cognitive demand for NAEP tasks increases, statistically higher average student ability scores are required to correctly address the questions. This pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#19982;&#23398;&#29983;&#20849;&#21516;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#23398;&#29983;&#22312;&#39033;&#30446;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#21147;&#65292;&#24182;&#22522;&#20110;&#23398;&#29983;&#30340;&#35270;&#35273;&#23545;&#25945;&#32946;&#30446;&#26631;&#36716;&#21464;&#36827;&#34892;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19981;&#21516;&#24577;&#24230;&#30340;&#23398;&#29983;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#36825;&#20026;&#26410;&#26469;&#30740;&#31350;&#23398;&#29983;&#19982;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#21644;&#29702;&#35299;&#22686;&#24378;&#23398;&#20064;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2401.14915</link><description>&lt;p&gt;
AI&#22312;&#22522;&#20110;&#39033;&#30446;&#30340;&#23398;&#20064;&#20013;&#30340;&#26410;&#26469;&#35268;&#21010;&#65306;&#19982;&#23398;&#29983;&#19968;&#36215;&#36827;&#34892;&#20849;&#21516;&#35774;&#35745;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Charting the Future of AI in Project-Based Learning: A Co-Design Exploration with Students. (arXiv:2401.14915v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#19982;&#23398;&#29983;&#20849;&#21516;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#23398;&#29983;&#22312;&#39033;&#30446;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#21147;&#65292;&#24182;&#22522;&#20110;&#23398;&#29983;&#30340;&#35270;&#35273;&#23545;&#25945;&#32946;&#30446;&#26631;&#36716;&#21464;&#36827;&#34892;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19981;&#21516;&#24577;&#24230;&#30340;&#23398;&#29983;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#36825;&#20026;&#26410;&#26469;&#30740;&#31350;&#23398;&#29983;&#19982;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#21644;&#29702;&#35299;&#22686;&#24378;&#23398;&#20064;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#22312;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#36825;&#32473;&#22522;&#20110;&#39033;&#30446;&#30340;&#23398;&#20064;&#65288;PBL&#65289;&#30340;&#35780;&#20272;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20849;&#21516;&#35774;&#35745;&#30740;&#31350;&#65292;&#25506;&#32034;&#23398;&#29983;&#30340;AI&#20351;&#29992;&#25968;&#25454;&#20316;&#20026;PBL&#35780;&#20272;&#30340;&#26032;&#26448;&#26009;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#19982;18&#21517;&#22823;&#23398;&#29983;&#36827;&#34892;&#20102;&#30740;&#35752;&#20250;&#65292;&#40723;&#21169;&#20182;&#20204;&#35774;&#24819;&#19968;&#20010;&#21487;&#20197;&#33258;&#30001;&#20351;&#29992;AI&#36827;&#34892;PBL&#30340;&#21478;&#19968;&#20010;&#19990;&#30028;&#65292;&#21516;&#26102;&#38656;&#35201;&#25253;&#21578;&#36825;&#20010;&#36807;&#31243;&#20197;&#35780;&#20272;&#20182;&#20204;&#30340;&#25216;&#33021;&#21644;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30740;&#35752;&#20250;&#20135;&#29983;&#20102;&#21508;&#31181;&#23398;&#29983;&#22312;PBL&#20013;&#20351;&#29992;AI&#30340;&#24773;&#26223;&#65292;&#20197;&#21450;&#22522;&#20110;&#23398;&#29983;&#23545;&#25945;&#32946;&#30446;&#26631;&#36716;&#21464;&#30340;&#35270;&#35273;&#36827;&#34892;&#20998;&#26512;&#36825;&#20123;&#20351;&#29992;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#23545;AI&#25345;&#26377;&#19981;&#21516;&#24577;&#24230;&#30340;&#23398;&#29983;&#22312;&#20998;&#26512;&#21644;&#29702;&#35299;AI&#20351;&#29992;&#26041;&#38754;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20851;&#20110;&#23398;&#29983;&#19982;AI&#20132;&#20114;&#21644;&#29702;&#35299;AI&#22686;&#24378;&#23398;&#20064;&#30340;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing use of Artificial Intelligence (AI) by students in learning presents new challenges for assessing their learning outcomes in project-based learning (PBL). This paper introduces a co-design study to explore the potential of students' AI usage data as a novel material for PBL assessment. We conducted workshops with 18 college students, encouraging them to speculate an alternative world where they could freely employ AI in PBL while needing to report this process to assess their skills and contributions. Our workshops yielded various scenarios of students' use of AI in PBL and ways of analyzing these uses grounded by students' vision of education goal transformation. We also found students with different attitudes toward AI exhibited distinct preferences in how to analyze and understand the use of AI. Based on these findings, we discuss future research opportunities on student-AI interactions and understanding AI-enhanced learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14521</link><description>&lt;p&gt;
&#20197;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;-&#27010;&#24565;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#31616;&#27905;&#21487;&#35299;&#37322;&#30340;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#37319;&#29992;&#22522;&#20110;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#20316;&#20026;&#22522;&#26412;&#35745;&#31639;&#21333;&#20803;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#21333;&#20010;&#20301;&#32622;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#65288;&#28145;&#24230;&#65289;&#65292;&#32780;&#19981;&#26159;&#23545;&#22823;&#26679;&#26412;&#38598;&#27700;&#21306;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#24191;&#24230;&#12290;&#30446;&#26631;&#26159;&#21457;&#29616;&#19968;&#20010;&#26368;&#23567;&#30340;&#34920;&#31034;&#65288;&#21333;&#20803;&#29366;&#24577;&#25968;&#21644;&#27969;&#37327;&#36335;&#24452;&#25968;&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#33021;&#22815;&#35299;&#37322;&#32473;&#23450;&#38598;&#27700;&#21306;&#36755;&#20837;&#29366;&#24577;&#21644;&#36755;&#20986;&#34892;&#20026;&#30340;&#20027;&#35201;&#36807;&#31243;&#65292;&#29305;&#21035;&#24378;&#35843;&#27169;&#25311;&#20840;&#33539;&#22260;&#65288;&#39640;&#12289;&#20013;&#12289;&#20302;&#65289;&#30340;&#27969;&#37327;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#21306;&#22495;&#65292;&#37319;&#29992;&#31867;&#20284;HyMod&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;3&#20010;&#21333;&#20803;&#29366;&#24577;&#21644;2&#20010;&#20027;&#35201;&#27969;&#21160;&#36335;&#24452;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#26679;&#30340;&#34920;&#31034;&#65292;&#20294;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#27700;&#25991;&#22270;&#30340;&#26102;&#38388;&#21644;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the applicability of machine learning technologies to the development of parsimonious, interpretable, catchment-scale hydrologic models using directed-graph architectures based on the mass-conserving perceptron (MCP) as the fundamental computational unit. Here, we focus on architectural complexity (depth) at a single location, rather than universal applicability (breadth) across large samples of catchments. The goal is to discover a minimal representation (numbers of cell-states and flow paths) that represents the dominant processes that can explain the input-state-output behaviors of a given catchment, with particular emphasis given to simulating the full range (high, medium, and low) of flow dynamics. We find that a HyMod-like architecture with three cell-states and two major flow pathways achieves such a representation at our study location, but that the additional incorporation of an input-bypass mechanism significantly improves the timing and shape of the hydrograph
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;MCTS&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;SR-GPT&#65292;&#22312;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#20844;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.14424</link><description>&lt;p&gt;
&#36890;&#36807;GPT&#24341;&#23548;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#25968;&#23398;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search. (arXiv:2401.14424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14424
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;MCTS&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;SR-GPT&#65292;&#22312;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#20844;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#25214;&#21040;&#19968;&#20010;&#31616;&#27905;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#20844;&#24335;&#26469;&#20934;&#30830;&#25551;&#36848;&#25968;&#25454;&#20013;&#27599;&#20010;&#21464;&#37327;&#19982;&#39044;&#27979;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20063;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#65292;&#26159;&#19968;&#20010;NP&#22256;&#38590;&#38382;&#39064;&#12290;&#21435;&#24180;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;sota&#12290;&#34429;&#28982;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#22312;&#24674;&#22797;&#30446;&#26631;&#34920;&#36798;&#24335;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#25913;&#36827;&#65292;&#20294;&#26159;&#22312;MCTS&#36807;&#31243;&#20013;&#32570;&#20047;&#24341;&#23548;&#20005;&#37325;&#38459;&#30861;&#20102;&#20854;&#25628;&#32034;&#25928;&#29575;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#31639;&#27861;&#22312;MCTS&#30340;&#25628;&#32034;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#32593;&#32476;&#65292;&#20294;&#26159;&#36825;&#20010;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#24456;&#24046;&#12290;&#20026;&#20102;&#24179;&#34913;&#25928;&#29575;&#21644;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SR-GPT&#65292;&#32467;&#21512;&#20102;AlphaZero&#30340;&#24605;&#24819;&#12290;SR-GPT&#26159;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;&#65292;&#23558;MCTS&#19982;&#19968;&#20010;&#36890;&#29992;&#24615;&#36739;&#22909;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a concise and interpretable mathematical formula that accurately describes the relationship between each variable and the predicted value in the data is a crucial task in scientific research, as well as a significant challenge in artificial intelligence. This problem is referred to as symbolic regression, which is an NP-hard problem. Last year, a symbolic regression method based on Monte Carlo Tree Search (MCTS) was proposed and sota was obtained on multiple datasets. While this algorithm has shown considerable improvement in recovering target expressions compared to previous methods, the lack of guidance during the MCTS process severely hampers its search efficiency. Recently, some algorithms have added a pre-trained policy network to guide the search of MCTS, but the pre-trained policy network generalizes poorly. To balance efficiency and generality, we propose SR-GPT combining ideas from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines MCTS with a Gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#29615;&#22659;&#20013;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#30340;&#20840;&#26632;&#26041;&#27861;&#65292;&#26426;&#22120;&#20154;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26694;&#26550;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#23454;&#36341;&#23398;&#20064;&#36866;&#24212;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26032;&#23545;&#35937;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20302;&#25104;&#26412;&#30340;&#31227;&#21160;&#25805;&#20316;&#30828;&#20214;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2401.14403</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#31227;&#21160;&#25805;&#20316;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#30340;&#21487;&#20851;&#33410;&#29289;&#20307;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adaptive Mobile Manipulation for Articulated Objects In the Open World. (arXiv:2401.14403v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#29615;&#22659;&#20013;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#30340;&#20840;&#26632;&#26041;&#27861;&#65292;&#26426;&#22120;&#20154;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26694;&#26550;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#23454;&#36341;&#23398;&#20064;&#36866;&#24212;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26032;&#23545;&#35937;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20302;&#25104;&#26412;&#30340;&#31227;&#21160;&#25805;&#20316;&#30828;&#20214;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#30340;&#26080;&#32467;&#26500;&#29615;&#22659;&#20013;&#37096;&#32626;&#26426;&#22120;&#20154;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#36890;&#24120;&#21482;&#22312;&#23553;&#38381;&#30340;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#36827;&#34892;&#30740;&#31350;&#65292;&#20043;&#21069;&#30340;&#31227;&#21160;&#25805;&#20316;&#24037;&#20316;&#20063;&#20165;&#38480;&#20110;&#25342;&#21462;&#12289;&#31227;&#21160;&#12289;&#25918;&#32622;&#65292;&#36825;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#21482;&#26159;&#20912;&#23665;&#19968;&#35282;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24320;&#25918;&#19990;&#30028;&#31227;&#21160;&#25805;&#20316;&#31995;&#32479;&#65292;&#37319;&#29992;&#20840;&#26632;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#20851;&#33410;&#29289;&#20307;&#30340;&#25805;&#20316;&#65292;&#20363;&#22914;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#38376;&#12289;&#26588;&#23376;&#12289;&#25277;&#23625;&#21644;&#20912;&#31665;&#12290;&#26426;&#22120;&#20154;&#21033;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#20808;&#20174;&#19968;&#23567;&#32452;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#28982;&#21518;&#36890;&#36807;&#22312;&#32447;&#23454;&#36341;&#23398;&#20064;&#26469;&#22788;&#29702;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26032;&#23545;&#35937;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#31227;&#21160;&#25805;&#20316;&#30828;&#20214;&#24179;&#21488;&#65292;&#33021;&#22815;&#22312;&#26080;&#32467;&#26500;&#29615;&#22659;&#20013;&#36827;&#34892;&#23433;&#20840;&#21644;&#33258;&#20027;&#30340;&#22312;&#32447;&#36866;&#24212;&#65292;&#25104;&#26412;&#32422;&#20026;20,000&#32654;&#20803;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;20&#20010;&#21487;&#20851;&#33410;&#30340;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate obje
&lt;/p&gt;</description></item><item><title>ZS4C&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#36827;&#34892;&#38646;&#23556;&#20987;&#21512;&#25104;&#21487;&#32534;&#35793;&#20195;&#30721;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#24110;&#21161;&#29992;&#25143;&#37325;&#29992;&#25110;&#20998;&#26512;&#19981;&#23436;&#25972;&#30340;Q&amp;A&#20195;&#30721;&#29255;&#27573;&#65292;&#36890;&#36807;&#35782;&#21035;&#32570;&#22833;&#30340;&#23548;&#20837;&#35821;&#21477;&#24182;&#20462;&#22797;&#32534;&#35793;&#38169;&#35823;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14279</link><description>&lt;p&gt;
ZS4C: &#20351;&#29992;ChatGPT&#36827;&#34892;&#38646;&#23556;&#20987;&#21512;&#25104;&#19981;&#23436;&#25972;&#20195;&#30721;&#29255;&#27573;&#30340;&#21487;&#32534;&#35793;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT. (arXiv:2401.14279v1 [cs.SE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14279
&lt;/p&gt;
&lt;p&gt;
ZS4C&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#36827;&#34892;&#38646;&#23556;&#20987;&#21512;&#25104;&#21487;&#32534;&#35793;&#20195;&#30721;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#24110;&#21161;&#29992;&#25143;&#37325;&#29992;&#25110;&#20998;&#26512;&#19981;&#23436;&#25972;&#30340;Q&amp;A&#20195;&#30721;&#29255;&#27573;&#65292;&#36890;&#36807;&#35782;&#21035;&#32570;&#22833;&#30340;&#23548;&#20837;&#35821;&#21477;&#24182;&#20462;&#22797;&#32534;&#35793;&#38169;&#35823;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#38382;&#31572;&#65288;Q&amp;A&#65289;&#32593;&#31449;&#22914;Stack Overflow&#24050;&#25104;&#20026;&#36719;&#20214;&#24320;&#21457;&#32773;&#23547;&#27714;&#30693;&#35782;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;Q&amp;A&#32593;&#31449;&#19978;&#30340;&#20195;&#30721;&#29255;&#27573;&#36890;&#24120;&#30001;&#20110;&#26410;&#35299;&#26512;&#30340;&#31867;&#22411;&#21644;&#32570;&#22833;&#30340;&#20381;&#36182;&#24211;&#32780;&#26080;&#27861;&#32534;&#35793;&#21644;&#35821;&#20041;&#19978;&#19981;&#23436;&#25972;&#65292;&#36825;&#22686;&#21152;&#20102;&#29992;&#25143;&#37325;&#29992;&#25110;&#20998;&#26512;Q&amp;A&#20195;&#30721;&#29255;&#27573;&#30340;&#38556;&#30861;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#19981;&#36866;&#29992;&#20110;&#21512;&#25104;&#21487;&#32534;&#35793;&#20195;&#30721;&#65292;&#35201;&#20040;&#32534;&#35793;&#25104;&#21151;&#29575;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ZS4C&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#19981;&#23436;&#25972;&#30340;&#20195;&#30721;&#29255;&#27573;&#20013;&#36827;&#34892;&#38646;&#23556;&#20987;&#21512;&#25104;&#21487;&#32534;&#35793;&#20195;&#30721;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#12290;ZS4C&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;ZS4C&#21033;&#29992;&#19968;&#20010;LLM&#65292;&#21363;ChatGPT&#65292;&#26681;&#25454;&#25105;&#20204;&#35774;&#35745;&#30340;&#19987;&#29992;&#20219;&#21153;&#25552;&#31034;&#27169;&#26495;&#65292;&#20026;&#32473;&#23450;&#30340;&#20195;&#30721;&#29255;&#27573;&#35782;&#21035;&#32570;&#22833;&#30340;&#23548;&#20837;&#35821;&#21477;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;ZS4C&#36890;&#36807;&#20462;&#22797;&#30001;&#20110;&#19981;&#27491;&#30830;&#30340;&#23548;&#20837;&#35821;&#21477;&#21644;&#35821;&#27861;&#38169;&#35823;&#24341;&#36215;&#30340;&#32534;&#35793;&#38169;&#35823;&#26469;&#20462;&#22797;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Technical question and answering (Q&amp;A) sites such as Stack Overflow have become an important source for software developers to seek knowledge. However, code snippets on Q&amp;A sites are usually uncompilable and semantically incomplete for compilation due to unresolved types and missing dependent libraries, which raises the obstacle for users to reuse or analyze Q&amp;A code snippets. Prior approaches either are not designed for synthesizing compilable code or suffer from a low compilation success rate. To address this problem, we propose ZS4C, a lightweight approach to perform zero-shot synthesis of compilable code from incomplete code snippets using Large Language Model (LLM). ZS4C operates in two stages. In the first stage, ZS4C utilizes an LLM, i.e., ChatGPT, to identify missing import statements for a given code snippet, leveraging our designed task-specific prompt template. In the second stage, ZS4C fixes compilation errors caused by incorrect import statements and syntax errors through 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35270;&#35282;&#33609;&#22270;&#24341;&#23548;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26694;&#26550;(Sketch2NeRF)&#65292;&#20197;&#22686;&#21152;&#23545;3D&#29983;&#25104;&#30340;&#33609;&#22270;&#25511;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#20248;&#21270;3D&#22330;&#26223;&#23454;&#29616;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21512;&#25104;&#19968;&#33268;&#30340;3D&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2401.14257</link><description>&lt;p&gt;
Sketch2NeRF: &#22810;&#35270;&#35282;&#33609;&#22270;&#24341;&#23548;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation. (arXiv:2401.14257v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35270;&#35282;&#33609;&#22270;&#24341;&#23548;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26694;&#26550;(Sketch2NeRF)&#65292;&#20197;&#22686;&#21152;&#23545;3D&#29983;&#25104;&#30340;&#33609;&#22270;&#25511;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#20248;&#21270;3D&#22330;&#26223;&#23454;&#29616;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21512;&#25104;&#19968;&#33268;&#30340;3D&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;3D&#30340;&#26041;&#27861;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;3D&#20869;&#23481;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#23545;&#35937;&#26159;&#38543;&#26426;&#30340;&#65292;&#24182;&#19988;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#25511;&#21046;&#12290;&#33609;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#24265;&#20215;&#30340;&#26041;&#27861;&#26469;&#24341;&#20837;&#36825;&#31181;&#32454;&#31890;&#24230;&#30340;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33609;&#22270;&#30340;&#25277;&#35937;&#21644;&#27169;&#31946;&#24615;&#65292;&#23454;&#29616;&#20174;&#36825;&#20123;&#33609;&#22270;&#20013;&#33719;&#24471;&#28789;&#27963;&#30340;&#25511;&#21046;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35270;&#35282;&#33609;&#22270;&#24341;&#23548;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26694;&#26550;&#65288;&#21363;Sketch2NeRF&#65289;&#65292;&#20197;&#22686;&#21152;&#23545;3D&#29983;&#25104;&#30340;&#33609;&#22270;&#25511;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#65288;&#20363;&#22914;Stable Diffusion&#21644;ControlNet&#65289;&#26469;&#30417;&#30563;&#30001;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#34920;&#31034;&#30340;3D&#22330;&#26223;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#27493;&#29983;&#25104;&#21644;&#37325;&#24314;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#20248;&#21270;NeRF&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20004;&#31181;&#22810;&#35270;&#35282;&#33609;&#22270;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21512;&#25104;&#19968;&#33268;&#30340;3D&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#35282;&#33394;&#21477;&#23376;&#19981;&#20855;&#20449;&#24687;&#24615;&#30340;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#65292;&#24182;&#35774;&#35745;&#31574;&#30053;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#25552;&#39640;&#22238;&#24212;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.14215</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#20010;&#24615;&#21270;&#32454;&#21270;&#65292;&#22686;&#24378;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#24120;&#35782;&#22686;&#24378;&#24615;&#20869;&#23384;&#26500;&#24314;&#21644;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#35282;&#33394;&#21477;&#23376;&#19981;&#20855;&#20449;&#24687;&#24615;&#30340;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#65292;&#24182;&#35774;&#35745;&#31574;&#30053;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#25552;&#39640;&#22238;&#24212;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#23545;&#35805;&#20013;&#65292;&#35760;&#24518;&#21644;&#21033;&#29992;&#35828;&#35805;&#32773;&#30340;&#35282;&#33394;&#26159;&#29983;&#25104;&#22238;&#24212;&#30340;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#32534;&#20889;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#25552;&#20379;&#26080;&#20449;&#24687;&#30340;&#35282;&#33394;&#21477;&#23376;&#65292;&#36825;&#22952;&#30861;&#20102;&#22238;&#24212;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#26469;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#36825;&#20123;&#38382;&#39064;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#19981;&#20135;&#29983;&#19982;&#20854;&#20182;&#35282;&#33394;&#30456;&#30683;&#30462;&#30340;&#35282;&#33394;&#65292;&#25105;&#20204;&#20391;&#37325;&#20110;&#26681;&#25454;&#35774;&#35745;&#30340;&#31574;&#30053;&#65292;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#27492;&#26469;&#32454;&#21270;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#32972;&#26223;&#12290;&#20316;&#20026;&#22810;&#20250;&#35805;&#24773;&#22659;&#20013;&#35282;&#33394;&#25193;&#23637;&#30340;&#20808;&#39537;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#31867;&#20154;&#20010;&#24615;&#32454;&#21270;&#20419;&#36827;&#20102;&#26356;&#22909;&#30340;&#22238;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memorizing and utilizing speakers' personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation. While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/.
&lt;/p&gt;</description></item><item><title>BayesPrompt&#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14166</link><description>&lt;p&gt;
BayesPrompt: &#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#19978;&#25351;&#23548;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction. (arXiv:2401.14166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14166
&lt;/p&gt;
&lt;p&gt;
BayesPrompt&#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#24494;&#35843;&#33539;&#24335;&#65292;prompt-tuning&#26088;&#22312;&#32553;&#23567;&#19979;&#28216;&#20219;&#21153;&#19982;&#39044;&#35757;&#32451;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;prompt-tuning&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25345;&#32493;&#36827;&#23637;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#25345;&#20037;&#30340;&#32570;&#38519;&#65306;prompt-tuning&#26041;&#27861;&#26080;&#27861;&#27867;&#21270;&#21040;&#29305;&#23450;&#30340;&#23569;&#26679;&#26412;&#27169;&#24335;&#12290;&#20174;&#20998;&#24067;&#20998;&#26512;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#19968;&#29616;&#35937;&#32972;&#21518;&#30340;&#20869;&#22312;&#38382;&#39064;&#26159;PLMs&#20013;&#21253;&#21547;&#36807;&#22810;&#30340;&#27010;&#24565;&#30693;&#35782;&#21644;&#30446;&#26631;&#19979;&#28216;&#39046;&#22495;&#30340;&#32553;&#20943;&#30693;&#35782;&#65292;&#20004;&#32773;&#20849;&#21516;&#23548;&#33268;PLMs&#22312;&#26222;&#36941;&#30340;&#30693;&#35782;&#23884;&#20837;&#31354;&#38388;&#20013;&#38169;&#35823;&#22320;&#23450;&#20301;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#23545;&#24212;&#30340;&#30693;&#35782;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30452;&#35266;&#22320;&#25506;&#32034;&#20102;&#20197;&#26080;&#20559;&#26041;&#24335;&#36924;&#36817;&#19979;&#28216;&#20219;&#21153;&#30340;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#25277;&#35937;&#36825;&#26679;&#30340;&#39046;&#22495;&#29983;&#25104;&#26377;&#21306;&#21035;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26080;&#27495;&#20041;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns. From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous
&lt;/p&gt;</description></item><item><title>WebVoyager&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;Web&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#20132;&#20114;&#26469;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13919</link><description>&lt;p&gt;
WebVoyager&#65306;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#30340;Web Agent
&lt;/p&gt;
&lt;p&gt;
WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13919
&lt;/p&gt;
&lt;p&gt;
WebVoyager&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;Web&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#20132;&#20114;&#26469;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#24341;&#39046;&#20102;&#19968;&#20010;&#30001;&#30495;&#23454;&#19990;&#30028;&#20013;&#33258;&#20027;&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#25152;&#26631;&#24535;&#30340;&#26032;&#26102;&#20195;&#65292;&#25512;&#21160;&#20102;&#22522;&#20110;&#32593;&#32476;&#30340;&#39640;&#32423;&#20195;&#29702;&#30340;&#21019;&#26032;&#12290;&#29616;&#26377;&#30340;&#32593;&#32476;&#20195;&#29702;&#36890;&#24120;&#21482;&#22788;&#29702;&#19968;&#20010;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#19988;&#20165;&#22312;&#31616;&#21270;&#30340;&#32593;&#32476;&#27169;&#25311;&#22120;&#25110;&#38745;&#24577;&#30340;&#32593;&#32476;&#24555;&#29031;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WebVoyager&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;Web&#20195;&#29702;&#65292;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#36827;&#34892;&#20132;&#20114;&#65292;&#33021;&#22815;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#24335;Web&#20195;&#29702;&#20219;&#21153;&#30340;&#33258;&#21160;&#35780;&#20272;&#25361;&#25112;&#65292;&#21033;&#29992;&#20102;GPT-4V&#30340;&#24378;&#22823;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;15&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#32593;&#31449;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;WebVoyager&#23454;&#29616;&#20102;55.7&#65285;&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#26174;&#33879;&#22320;.....
&lt;/p&gt;
&lt;p&gt;
The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.13802</link><description>&lt;p&gt;
&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13802
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#20195;&#30721;&#29983;&#25104;&#12290;LLMs&#20027;&#35201;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;/&#23569;&#26679;&#26412;&#33539;&#24335;&#20013;&#34987;&#29992;&#20110;&#25351;&#23548;&#27169;&#22411;&#23436;&#25104;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#65288;CCD&#65289;&#36825;&#19968;&#38750;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \textcolor{blac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21463;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#21457;&#29616;&#35299;&#37322;&#24448;&#24448;&#19981;&#33021;&#28385;&#36275;&#20182;&#20204;&#30340;&#20851;&#27880;&#28857;&#65292;&#23548;&#33268;&#23545;&#30417;&#31649;&#26694;&#26550;&#30340;&#29702;&#35299;&#21644;&#36981;&#23432;&#20135;&#29983;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;XAI&#21021;&#23398;&#32773;&#38382;&#39064;&#24211;&#65292;&#28085;&#30422;&#20102;&#23601;&#19994;&#39044;&#27979;&#21644;&#20581;&#24247;&#30417;&#27979;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.13324</link><description>&lt;p&gt;
&#26377;&#20851;&#31639;&#27861;&#20915;&#31574;&#30340;&#20449;&#24687;&#65306;&#25506;&#32034;&#21463;&#21040;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions. (arXiv:2401.13324v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21463;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#21457;&#29616;&#35299;&#37322;&#24448;&#24448;&#19981;&#33021;&#28385;&#36275;&#20182;&#20204;&#30340;&#20851;&#27880;&#28857;&#65292;&#23548;&#33268;&#23545;&#30417;&#31649;&#26694;&#26550;&#30340;&#29702;&#35299;&#21644;&#36981;&#23432;&#20135;&#29983;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;XAI&#21021;&#23398;&#32773;&#38382;&#39064;&#24211;&#65292;&#28085;&#30422;&#20102;&#23601;&#19994;&#39044;&#27979;&#21644;&#20581;&#24247;&#30417;&#27979;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#30340;&#35299;&#37322;&#24456;&#23569;&#28041;&#21450;&#21040;&#21463;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#36825;&#31181;&#20256;&#36798;&#20449;&#24687;&#19982;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#25152;&#20851;&#24515;&#30340;&#20449;&#24687;&#20043;&#38388;&#30340;&#24046;&#36317;&#21487;&#33021;&#38459;&#30861;&#23545;&#30417;&#31649;&#26694;&#26550;&#65288;&#22914;AI&#27861;&#26696;&#65289;&#30340;&#29702;&#35299;&#21644;&#36981;&#23432;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;XAI&#21021;&#23398;&#32773;&#38382;&#39064;&#24211;&#8221;&#65306;&#36825;&#26159;&#19968;&#20010;&#28085;&#30422;&#20004;&#20010;&#31639;&#27861;&#20915;&#31574;&#24212;&#29992;&#39046;&#22495;&#65288;&#23601;&#19994;&#39044;&#27979;&#21644;&#20581;&#24247;&#30417;&#27979;&#65289;&#20013;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30446;&#24405;&#65292;&#21253;&#25324;&#25968;&#25454;&#12289;&#31995;&#32479;&#32972;&#26223;&#12289;&#31995;&#32479;&#20351;&#29992;&#21644;&#31995;&#32479;&#35268;&#33539;&#31561;&#31867;&#21035;&#12290;&#20449;&#24687;&#38656;&#27714;&#26159;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#25910;&#38598;&#30340;&#65292;&#21442;&#19982;&#32773;&#26681;&#25454;&#33258;&#24049;&#30340;&#38382;&#39064;&#33719;&#24471;&#35299;&#37322;&#12290;&#21442;&#19982;&#32773;&#36824;&#25253;&#21578;&#20102;&#20182;&#20204;&#30340;&#29702;&#35299;&#21644;&#20915;&#31574;&#20449;&#24515;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#22312;&#25509;&#21463;&#35299;&#37322;&#21518;&#20449;&#24515;&#20542;&#21521;&#20110;&#22686;&#21152;&#65292;&#20294;&#21442;&#19982;&#32773;&#20063;&#38754;&#20020;&#30528;&#29702;&#35299;&#19978;&#30340;&#25361;&#25112;&#65292;&#22914;&#26080;&#27861;&#35299;&#37322;&#20026;&#20160;&#20040;&#33258;&#24049;&#30340;&#29702;&#35299;&#24863;&#35273;&#19981;&#23436;&#25972;&#12290;&#35299;&#37322;&#36824;&#23545;&#29702;&#35299;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanations of AI systems rarely address the information needs of people affected by algorithmic decision-making (ADM). This gap between conveyed information and information that matters to affected stakeholders can impede understanding and adherence to regulatory frameworks such as the AI Act. To address this gap, we present the "XAI Novice Question Bank": A catalog of affected stakeholders' information needs in two ADM use cases (employment prediction and health monitoring), covering the categories data, system context, system usage, and system specifications. Information needs were gathered in an interview study where participants received explanations in response to their inquiries. Participants further reported their understanding and decision confidence, showing that while confidence tended to increase after receiving explanations, participants also met understanding challenges, such as being unable to tell why their understanding felt incomplete. Explanations further influenced
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#30340;"I don't know"&#65288;Idk&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;AI&#21161;&#25163;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2401.13275</link><description>&lt;p&gt;
AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;?
&lt;/p&gt;
&lt;p&gt;
Can AI Assistants Know What They Don't Know?. (arXiv:2401.13275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#30340;"I don't know"&#65288;Idk&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;AI&#21161;&#25163;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;AI&#21161;&#25163;&#22312;&#23545;&#35805;&#12289;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#32534;&#20889;&#20195;&#30721;&#21644;&#20351;&#29992;&#24037;&#20855;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;LLMs&#20855;&#26377;&#28145;&#20837;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#20294;&#22312;&#38754;&#23545;&#26576;&#20123;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#22914;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65289;&#26102;&#20173;&#28982;&#20250;&#20986;&#29616;&#20107;&#23454;&#38169;&#35823;&#12290;AI&#21161;&#25163;&#30340;&#36825;&#31181;&#19981;&#30495;&#23454;&#22238;&#31572;&#21487;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36896;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;AI&#21161;&#25163;&#25298;&#32477;&#22238;&#31572;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#38382;&#39064;&#26159;&#20943;&#23569;&#24187;&#35273;&#21644;&#20351;&#21161;&#25163;&#30495;&#23454;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#38382;&#39064;&#8220;AI&#21161;&#25163;&#26159;&#21542;&#33021;&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#20107;&#24773;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#21161;&#25163;&#26500;&#24314;&#20102;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#30340;&#8220;I don't know&#8221;(Idk)&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#38382;&#39064;&#65292;&#22522;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#21161;&#25163;&#19982;&#20854;&#30456;&#24212;&#30340;Idk&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, AI assistants based on large language models (LLMs) show surprising performance in many tasks, such as dialogue, solving math problems, writing code, and using tools. Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications. We believe that an AI assistant's refusal to answer questions it does not know is a crucial method for reducing hallucinations and making the assistant truthful. Therefore, in this paper, we ask the question "Can AI assistants know what they don't know and express them through natural language?" To answer this question, we construct a model-specific "I don't know" (Idk) dataset for an assistant, which contains its known and unknown questions, based on existing open-domain question answering datasets. Then we align the assistant with its corresponding I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#30340;&#26041;&#27861;DISCOUNT&#65292;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#32622;&#20449;&#24230;&#26469;&#25903;&#25745;&#36825;&#19968;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13112</link><description>&lt;p&gt;
DISCOUNT: &#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DISCOUNT: Distributional Counterfactual Explanation With Optimal Transport. (arXiv:2401.13112v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#30340;&#26041;&#27861;DISCOUNT&#65292;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#32622;&#20449;&#24230;&#26469;&#25903;&#25745;&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35299;&#37322;&#26159;&#22312;&#40657;&#30418;&#20915;&#31574;&#27169;&#22411;&#20013;&#25552;&#20379;&#27934;&#23519;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20107;&#23454;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#23548;&#33268;&#19981;&#21516;&#32467;&#26524;&#30340;&#26367;&#20195;&#36755;&#20837;&#23454;&#20363;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#20998;&#24067;&#19978;&#19979;&#25991;&#65292;&#20174;&#20010;&#20307;&#25968;&#25454;&#28857;&#25193;&#22823;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#21629;&#21517;&#20026;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#12290;&#22312;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#36716;&#21521;&#20998;&#26512;&#20107;&#23454;&#21644;&#23545;&#25239;&#30340;&#20998;&#24067;&#23646;&#24615;&#65292;&#31867;&#20284;&#20110;&#35780;&#20272;&#20010;&#20307;&#23454;&#20363;&#21450;&#20854;&#32467;&#26524;&#20915;&#31574;&#30340;&#32463;&#20856;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26469;&#26500;&#24314;&#19968;&#20010;&#26426;&#20250;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#23548;&#20986;&#19982;&#20107;&#23454;&#23545;&#24212;&#30340;&#23545;&#25239;&#20998;&#24067;&#65292;&#20197;&#32479;&#35745;&#32622;&#20449;&#24230;&#20570;&#25903;&#25745;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20248;&#21270;&#26041;&#27861;DISCOUNT&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20998;&#24067;&#20043;&#38388;&#24179;&#34913;&#36825;&#31181;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (CE) is the de facto method for providing insight and interpretability in black-box decision-making models by identifying alternative input instances that lead to different outcomes. This paper extends the concept of CEs to a distributional context, broadening the scope from individual data points to entire input and output distributions, named Distributional Counterfactual Explanation (DCE). In DCE, our focus shifts to analyzing the distributional properties of the factual and counterfactual, drawing parallels to the classical approach of assessing individual instances and their resulting decisions. We leverage Optimal Transport (OT) to frame a chance-constrained optimization problem, aiming to derive a counterfactual distribution that closely aligns with its factual counterpart, substantiated by statistical confidence. Our proposed optimization method, DISCOUNT, strategically balances this confidence across both input and output distributions. This algorit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#31561;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#27969;&#37327;&#65292;&#24182;&#29992;&#20110;&#25351;&#23548;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20013;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.13098</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#21147;&#20449;&#24687;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#39044;&#27979;&#38750;&#26412;&#22320;&#29289;&#31181;&#33337;&#33334;&#20132;&#36890;&#27969;&#37327;&#21644;&#20837;&#20405;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge. (arXiv:2401.13098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13098
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#31561;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#27969;&#37327;&#65292;&#24182;&#29992;&#20110;&#25351;&#23548;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20013;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#20307;&#20013;&#30340;&#20837;&#20405;&#29289;&#31181;&#23545;&#20840;&#29699;&#29615;&#22659;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#30001;&#20110;&#20132;&#36890;&#21644;&#36152;&#26131;&#22686;&#21152;&#65292;&#38750;&#26412;&#22303;&#29289;&#31181;&#24050;&#32463;&#24341;&#20837;&#20102;&#26032;&#30340;&#29615;&#22659;&#65292;&#23548;&#33268;&#29983;&#24577;&#31995;&#32479;&#30772;&#22351;&#65292;&#24182;&#23548;&#33268;&#20892;&#19994;&#12289;&#26519;&#19994;&#21644;&#28180;&#19994;&#26041;&#38754;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#25216;&#26415;&#20197;&#20943;&#36731;&#36825;&#20123;&#20837;&#20405;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#20132;&#36890;&#27969;&#37327;&#65292;&#24182;&#20197;&#27492;&#25351;&#23548;&#36890;&#36807;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20256;&#25773;&#30340;&#20837;&#20405;&#29289;&#31181;&#39118;&#38505;&#35780;&#20272;&#12290;&#21463;&#22269;&#38469;&#36152;&#26131;&#37325;&#21147;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#24433;&#21709;&#33337;&#33334;&#27963;&#21160;&#21487;&#33021;&#24615;&#21644;&#24433;&#21709;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#22914;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#20043;&#38388;&#30340;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#32593;&#32476;&#65292;&#25105;&#20204;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#20837;&#20405;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invasive species in water bodies pose a major threat to the environment and biodiversity globally. Due to increased transportation and trade, non-native species have been introduced to new environments, causing damage to ecosystems and leading to economic losses in agriculture, forestry, and fisheries. Therefore, there is a pressing need for risk assessment and management techniques to mitigate the impact of these invasions. This study aims to develop a new physics-inspired model to forecast maritime shipping traffic and thus inform risk assessment of invasive species spread through global transportation networks. Inspired by the gravity model for international trades, our model considers various factors that influence the likelihood and impact of vessel activities, such as shipping flux density, distance between ports, trade flow, and centrality measures of transportation hubs. Additionally, by analyzing the risk network of invasive species, we provide a comprehensive framework for as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#31232;&#30095;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25311;&#21512;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.13034</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#30340;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#31232;&#30095;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25311;&#21512;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#25968;&#25454;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#65292;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#39640;&#25928;&#30340;&#31232;&#30095;&#26356;&#26032;&#65292;&#22312;&#24179;&#34913;&#27169;&#22411;&#23481;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#20248;&#21270;&#25311;&#21512;&#25152;&#26377;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClipSAM&#30340;CLIP&#21644;SAM&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#24322;&#24120;&#20998;&#21106;&#12290;ClipSAM&#21033;&#29992;CLIP&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#24322;&#24120;&#23450;&#20301;&#21644;&#31895;&#31961;&#20998;&#21106;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;SAM&#30340;&#25552;&#31034;&#32422;&#26463;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#24322;&#24120;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.12665</link><description>&lt;p&gt;
ClipSAM&#65306;CLIP&#21644;SAM&#30340;&#21512;&#20316;&#29992;&#20110;&#38646;&#26679;&#26412;&#24322;&#24120;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation. (arXiv:2401.12665v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12665
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClipSAM&#30340;CLIP&#21644;SAM&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#24322;&#24120;&#20998;&#21106;&#12290;ClipSAM&#21033;&#29992;CLIP&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#24322;&#24120;&#23450;&#20301;&#21644;&#31895;&#31961;&#20998;&#21106;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;SAM&#30340;&#25552;&#31034;&#32422;&#26463;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#24322;&#24120;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;CLIP&#21644;SAM&#31561;&#22522;&#30784;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24322;&#24120;&#20998;&#21106;&#65288;ZSAS&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26080;&#35770;&#26159;&#22522;&#20110;CLIP&#36824;&#26159;SAM&#30340;ZSAS&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#19981;&#21487;&#24573;&#35270;&#30340;&#20851;&#38190;&#32570;&#28857;&#65306;1&#65289;CLIP&#20027;&#35201;&#20851;&#27880;&#19981;&#21516;&#36755;&#20837;&#20043;&#38388;&#30340;&#20840;&#23616;&#29305;&#24449;&#23545;&#40784;&#65292;&#23548;&#33268;&#23545;&#23616;&#37096;&#24322;&#24120;&#37096;&#20998;&#30340;&#20998;&#21106;&#19981;&#20934;&#30830;&#65307;2&#65289;SAM&#20542;&#21521;&#20110;&#29983;&#25104;&#22823;&#37327;&#27809;&#26377;&#36866;&#24403;&#25552;&#31034;&#32422;&#26463;&#30340;&#20887;&#20313;&#25513;&#30721;&#65292;&#23548;&#33268;&#22797;&#26434;&#30340;&#21518;&#22788;&#29702;&#35201;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClipSAM&#30340;CLIP&#21644;SAM&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;ZSAS&#12290;ClipSAM&#30340;&#24605;&#36335;&#26159;&#21033;&#29992;CLIP&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#24322;&#24120;&#23450;&#20301;&#21644;&#31895;&#31961;&#20998;&#21106;&#65292;&#36827;&#19968;&#27493;&#23558;&#20854;&#29992;&#20316;&#25552;&#20379;&#25552;&#31034;&#32422;&#26463;&#20197;&#25913;&#36827;SAM&#30340;&#24322;&#24120;&#20998;&#21106;&#32467;&#26524;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#32479;&#19968;&#22810;&#23610;&#24230;&#36328;&#27169;&#24577;&#20132;&#20114;&#65288;UMCI&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#30456;&#20114;&#20316;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to generate numerous redundant masks without proper prompt constraints, resulting in complex post-processing requirements. In this work, we innovatively propose a CLIP and SAM collaboration framework called ClipSAM for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding capability for anomaly localization and rough segmentation, which is further used as the prompt constraints for SAM to refine the anomaly segmentation results. In details, we introduce a crucial Unified Multi-scale Cross-modal Interaction (UMCI) module for interacting language with visual features at multiple sc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TTG-NN&#65289;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25345;&#20037;&#21516;&#35843;&#12289;&#22270;&#21367;&#31215;&#21644;&#24352;&#37327;&#36816;&#31639;&#65292;&#21516;&#26102;&#25429;&#25417;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#65288;TT&#65289;&#21644;Tensor&#35270;&#22270;&#22270;&#65288;TG&#65289;&#32467;&#26500;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.12007</link><description>&lt;p&gt;
Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Tensor-view Topological Graph Neural Network. (arXiv:2401.12007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12007
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TTG-NN&#65289;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25345;&#20037;&#21516;&#35843;&#12289;&#22270;&#21367;&#31215;&#21644;&#24352;&#37327;&#36816;&#31639;&#65292;&#21516;&#26102;&#25429;&#25417;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#65288;TT&#65289;&#21644;Tensor&#35270;&#22270;&#22270;&#65288;TG&#65289;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#23398;&#20064;&#20219;&#21153;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36817;&#24180;&#26469;&#22312;&#22270;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#35768;&#22810;&#37325;&#35201;&#30340;&#22270;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;GNNs&#22312;&#24615;&#33021;&#19978;&#22788;&#20110;&#26368;&#21069;&#27839;&#65292;&#20294;&#23427;&#20204;&#21482;&#20351;&#29992;&#20102;&#27599;&#20010;&#33410;&#28857;&#21608;&#22260;&#38750;&#24120;&#26377;&#38480;&#30340;&#37051;&#22495;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#20002;&#22833;&#21644;&#36807;&#22810;&#35745;&#31639;&#30340;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TTG-NN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25345;&#20037;&#21516;&#35843;&#12289;&#22270;&#21367;&#31215;&#21644;&#24352;&#37327;&#36816;&#31639;&#30340;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#21516;&#26102;&#25429;&#25417;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#65288;TT&#65289;&#21644;Tensor&#35270;&#22270;&#22270;&#65288;TG&#65289;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#22312;&#35745;&#31639;&#19978;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#30340;&#25299;&#25169;&#21644;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph classification is an important learning task for graph-structured data. Graph neural networks (GNNs) have recently gained growing attention in graph learning and have shown significant improvements in many important graph problems. Despite their state-of-the-art performances, existing GNNs only use local information from a very limited neighborhood around each node, suffering from loss of multi-modal information and overheads of excessive computation. To address these issues, we propose a novel Tensor-view Topological Graph Neural Network (TTG-NN), a class of simple yet effective topological deep learning built upon persistent homology, graph convolution, and tensor operations. This new method incorporates tensor learning to simultaneously capture Tensor-view Topological (TT), as well as Tensor-view Graph (TG) structural information on both local and global levels. Computationally, to fully exploit graph topology and structure, we propose two flexible TT and TG representation lea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11864</link><description>&lt;p&gt;
&#36890;&#36807;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#23558;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#21387;&#32553;&#21040;&#20855;&#26377;&#23567;&#20110;&#21313;&#20159;&#21442;&#25968;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#65292;&#23558;&#25512;&#29702;&#36807;&#31243;&#23553;&#35013;&#20026;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;EoTD&#25968;&#25454;&#38598;&#26469;&#23545;SLMs&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;SLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36825;&#21253;&#25324;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#65288;&#21253;&#25324;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#31243;&#24207;&#21644;&#24605;&#32500;&#26041;&#31243;&#65289;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;EoTD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;ETD&#20351;&#36825;&#20123;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#24212;&#29992;&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#24605;&#24819;&#33021;&#22815;&#23454;&#29616;&#22312;&#20943;&#23569;&#21442;&#25968;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25191;&#34892;&#25928;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#23398;&#29983;&#32593;&#32476;&#23398;&#20064;&#21040;&#22797;&#26434;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.11798</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#20132;&#36890;&#39044;&#27979;&#19978;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction. (arXiv:2401.11798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#24212;&#29992;&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#24605;&#24819;&#33021;&#22815;&#23454;&#29616;&#22312;&#20943;&#23569;&#21442;&#25968;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25191;&#34892;&#25928;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#23398;&#29983;&#32593;&#32476;&#23398;&#20064;&#21040;&#22797;&#26434;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#23454;&#26102;&#20132;&#36890;&#39044;&#27979;&#23545;&#20943;&#23569;&#20132;&#36890;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#39044;&#27979;&#20132;&#36890;&#29366;&#20917;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31354;&#38388;-&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ST-GNN&#65289;&#23558;&#23454;&#26102;&#20132;&#36890;&#25968;&#25454;&#24314;&#27169;&#20026;&#26102;&#38388;&#22270;&#12290;&#23613;&#31649;ST-GNN&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#20026;&#23454;&#38469;&#20132;&#36890;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#23454;&#26102;&#39044;&#27979;&#26102;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#37492;&#20110;&#23454;&#26102;&#25968;&#25454;&#21160;&#24577;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;ST-GNN&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25104;&#26412;&#20989;&#25968;&#65292;&#26088;&#22312;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#65288;&#25945;&#24072;&#65289;&#30340;&#33976;&#39311;&#25968;&#25454;&#26469;&#35757;&#32451;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;&#32593;&#32476;&#65288;&#23398;&#29983;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20934;&#30830;&#24615;&#25509;&#36817;&#25945;&#24072;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65292;&#23558;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#34701;&#20837;&#23398;&#29983;&#32593;&#32476;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#23398;&#20064;&#21040;&#25945;&#24072;&#24863;&#30693;&#30340;&#22797;&#26434;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#38754;&#20020;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient real-time traffic prediction is crucial for reducing transportation time. To predict traffic conditions, we employ a spatio-temporal graph neural network (ST-GNN) to model our real-time traffic data as temporal graphs. Despite its capabilities, it often encounters challenges in delivering efficient real-time predictions for real-world traffic data. Recognizing the significance of timely prediction due to the dynamic nature of real-time data, we employ knowledge distillation (KD) as a solution to enhance the execution time of ST-GNNs for traffic prediction. In this paper, We introduce a cost function designed to train a network with fewer parameters (the student) using distilled data from a complex network (the teacher) while maintaining its accuracy close to that of the teacher. We use knowledge distillation, incorporating spatial-temporal correlations from the teacher network to enable the student to learn the complex patterns perceived by the teacher. However, a challenge a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.11792</link><description>&lt;p&gt;
&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#29615;&#22659;&#21644;&#36710;&#36742;&#29366;&#24577;&#21160;&#24577;&#21046;&#23450;&#36866;&#24403;&#30340;&#39550;&#39542;&#31574;&#30053;&#65292;&#21516;&#26102;&#30830;&#20445;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#23433;&#20840;&#24615;&#20302;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#65292;&#32780;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#26159;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#30340;&#21069;&#25552;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#32780;&#22810;&#26679;&#22330;&#26223;&#19979;&#30340;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#12290;&#25105;&#20204;&#30340;SGADS&#19982;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#32467;&#21512;&#65292;&#20351;&#26234;&#33021;&#36710;&#36742;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#31034;&#33539;&#30456;&#32467;&#21512;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
An intelligent driving system should be capable of dynamically formulating appropriate driving strategies based on the current environment and vehicle status, while ensuring the security and reliability of the system. However, existing methods based on reinforcement learning and imitation learning suffer from low safety, poor generalization, and inefficient sampling. Additionally, they cannot accurately predict future driving trajectories, and the accurate prediction of future driving trajectories is a precondition for making optimal decisions. To solve these problems, in this paper, we introduce a Safe and Generalized end-to-end Autonomous Driving System (SGADS) for complex and various scenarios. Our SGADS incorporates variational inference with normalizing flows, enabling the intelligent vehicle to accurately predict future driving trajectories. Moreover, we propose the formulation of robust safety constraints. Furthermore, we combine reinforcement learning with demonstrations to aug
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#26426;&#22120;&#23398;&#20064;&#22312;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#20013;&#24341;&#36215;&#30340;&#23433;&#20840;&#23041;&#32961;&#36827;&#34892;&#20102;&#32508;&#21512;&#25506;&#32034;&#65292;&#21253;&#25324;&#20250;&#21592;&#25512;&#26029;&#12289;&#23545;&#25239;&#35268;&#36991;&#12289;&#37325;&#24314;&#12289;&#23646;&#24615;&#25512;&#26029;&#12289;&#27169;&#22411;&#25552;&#21462;&#21644;&#27602;&#21270;&#25915;&#20987;&#31561;&#22810;&#31181;&#25915;&#20987;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.11723</link><description>&lt;p&gt;
&#35299;&#26512;&#26426;&#22120;&#23398;&#20064;&#22312;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#25915;&#20987;&#65306;&#35843;&#26597;&#21450;&#20854;&#32972;&#21518;&#30340;&#24320;&#25918;&#24211;
&lt;/p&gt;
&lt;p&gt;
Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them. (arXiv:2401.11723v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#26426;&#22120;&#23398;&#20064;&#22312;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#20013;&#24341;&#36215;&#30340;&#23433;&#20840;&#23041;&#32961;&#36827;&#34892;&#20102;&#32508;&#21512;&#25506;&#32034;&#65292;&#21253;&#25324;&#20250;&#21592;&#25512;&#26029;&#12289;&#23545;&#25239;&#35268;&#36991;&#12289;&#37325;&#24314;&#12289;&#23646;&#24615;&#25512;&#26029;&#12289;&#27169;&#22411;&#25552;&#21462;&#21644;&#27602;&#21270;&#25915;&#20987;&#31561;&#22810;&#31181;&#25915;&#20987;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#30340;&#20986;&#29616;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36830;&#25509;&#24615;&#26102;&#20195;&#65292;&#39044;&#35745;&#21040;2025&#24180;&#24213;&#23558;&#26377;&#32422;800&#20159;&#20010;&#26234;&#33021;&#35774;&#22791;&#25237;&#20837;&#36816;&#33829;&#12290;&#36825;&#20123;&#35774;&#22791;&#20419;&#36827;&#20102;&#20247;&#22810;&#26234;&#33021;&#24212;&#29992;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#25552;&#39640;&#20102;&#29983;&#27963;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#65292;&#19981;&#20165;&#29992;&#20110;&#20998;&#26512;&#29289;&#32852;&#32593;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#36824;&#29992;&#20110;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#20869;&#30340;&#22810;&#31181;&#24212;&#29992;&#12290;&#20363;&#22914;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#35782;&#21035;&#12289;&#24322;&#24120;&#26816;&#27979;&#29978;&#33267;&#25581;&#31034;&#24694;&#24847;&#27963;&#21160;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#23545;&#26426;&#22120;&#23398;&#20064;&#25972;&#21512;&#21040;&#29289;&#32852;&#32593;&#21508;&#20010;&#26041;&#38754;&#24341;&#21457;&#30340;&#23433;&#20840;&#23041;&#32961;&#36827;&#34892;&#20102;&#20840;&#38754;&#25506;&#32034;&#65292;&#21253;&#25324;&#20250;&#21592;&#25512;&#26029;&#12289;&#23545;&#25239;&#35268;&#36991;&#12289;&#37325;&#24314;&#12289;&#23646;&#24615;&#25512;&#26029;&#12289;&#27169;&#22411;&#25552;&#21462;&#21644;&#27602;&#21270;&#25915;&#20987;&#31561;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#12290;&#19982;&#20197;&#24448;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#25972;&#20307;&#30340;&#35270;&#35282;&#65292;&#26681;&#25454;&#35832;&#22914;...&#30340;&#26631;&#20934;&#23545;&#23041;&#32961;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of the Internet of Things (IoT) has brought forth an era of unprecedented connectivity, with an estimated 80 billion smart devices expected to be in operation by the end of 2025. These devices facilitate a multitude of smart applications, enhancing the quality of life and efficiency across various domains. Machine Learning (ML) serves as a crucial technology, not only for analyzing IoT-generated data but also for diverse applications within the IoT ecosystem. For instance, ML finds utility in IoT device recognition, anomaly detection, and even in uncovering malicious activities. This paper embarks on a comprehensive exploration of the security threats arising from ML's integration into various facets of IoT, spanning various attack types including membership inference, adversarial evasion, reconstruction, property inference, model extraction, and poisoning attacks. Unlike previous studies, our work offers a holistic perspective, categorizing threats based on criteria such as
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.11648</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#20998;&#23618;&#27491;&#21017;&#21270;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#24314;&#27169;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#27491;&#21017;&#21270;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#39044;&#27979;&#19979;&#27425;&#23601;&#35786;&#30340;&#35786;&#26029;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#21046;&#23450;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#30340;&#20027;&#21160;&#26410;&#26469;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#35768;&#22810;&#30740;&#31350;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;EHR&#25968;&#25454;&#22266;&#26377;&#30340;&#24322;&#26500;&#21644;&#20998;&#23618;&#29305;&#24449;&#65292;&#24517;&#28982;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NECHO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#20195;&#30721;&#20013;&#24515;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;EHR&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20998;&#23618;&#27491;&#21017;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23450;&#21046;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#19968;&#23545;&#21452;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#34701;&#21512;&#28085;&#30422;&#21307;&#23398;&#20195;&#30721;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#20020;&#24202;&#31508;&#35760;&#30340;&#22810;&#26041;&#38754;&#20449;&#24687;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#22260;&#32469;&#30528;&#21307;&#23398;&#20195;&#30721;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21307;&#23398;&#26412;&#20307;&#20013;&#30340;&#29238;&#32423;&#20449;&#24687;&#26469;&#35268;&#33539;&#29305;&#23450;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#20197;&#23398;&#20064;EHR&#25968;&#25454;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#23545;MIMIC-III&#25968;&#25454;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>S$^3$M-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#31435;&#20307;&#21305;&#37197;&#12290;&#36890;&#36807;&#20849;&#20139;&#29305;&#24449;&#21644;&#29305;&#24449;&#34701;&#21512;&#36866;&#24212;&#27169;&#22359;&#30340;&#20351;&#29992;&#65292;S$^3$M-Net&#33021;&#22815;&#25552;&#39640;&#25972;&#20307;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11414</link><description>&lt;p&gt;
S$^3$M-Net:&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#35821;&#20041;&#20998;&#21106;&#21644;&#31435;&#20307;&#21305;&#37197;&#30340;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
S$^3$M-Net: Joint Learning of Semantic Segmentation and Stereo Matching for Autonomous Driving. (arXiv:2401.11414v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11414
&lt;/p&gt;
&lt;p&gt;
S$^3$M-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#31435;&#20307;&#21305;&#37197;&#12290;&#36890;&#36807;&#20849;&#20139;&#29305;&#24449;&#21644;&#29305;&#24449;&#34701;&#21512;&#36866;&#24212;&#27169;&#22359;&#30340;&#20351;&#29992;&#65292;S$^3$M-Net&#33021;&#22815;&#25552;&#39640;&#25972;&#20307;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#21644;&#31435;&#20307;&#21305;&#37197;&#26159;&#33258;&#21160;&#39550;&#39542;&#30340;&#19977;&#32500;&#29615;&#22659;&#24863;&#30693;&#31995;&#32479;&#30340;&#20004;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#29420;&#31435;&#22788;&#29702;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#20351;&#29992;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#23384;&#22312;&#23454;&#38469;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#36164;&#28304;&#31232;&#32570;&#25110;&#23454;&#26102;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;S$^3$M-Net&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#31435;&#20307;&#21305;&#37197;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;S$^3$M-Net&#20849;&#20139;&#26469;&#33258;RGB&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#22330;&#26223;&#29702;&#35299;&#33021;&#21147;&#12290;&#36825;&#20010;&#29305;&#24449;&#20849;&#20139;&#36807;&#31243;&#36890;&#36807;&#19968;&#20010;&#29305;&#24449;&#34701;&#21512;&#36866;&#24212;&#65288;FFA&#65289;&#27169;&#22359;&#23454;&#29616;&#65292;&#26377;&#25928;&#22320;&#23558;&#20849;&#20139;&#29305;&#24449;&#36716;&#25442;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#24182;&#38543;&#21518;&#23558;&#23427;&#20204;&#19982;&#32534;&#30721;&#30340;&#35270;&#24046;&#29305;&#24449;&#34701;&#21512;&#12290;&#25972;&#20010;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#26159;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation and stereo matching are two essential components of 3D environmental perception systems for autonomous driving. Nevertheless, conventional approaches often address these two problems independently, employing separate models for each task. This approach poses practical limitations in real-world scenarios, particularly when computational resources are scarce or real-time performance is imperative. Hence, in this article, we introduce S$^3$M-Net, a novel joint learning framework developed to perform semantic segmentation and stereo matching simultaneously. Specifically, S$^3$M-Net shares the features extracted from RGB images between both tasks, resulting in an improved overall scene understanding capability. This feature sharing process is realized using a feature fusion adaption (FFA) module, which effectively transforms the shared features into semantic space and subsequently fuses them with the encoded disparity features. The entire joint learning framework is tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25919;&#31574;&#24046;&#24322;&#30340;&#36890;&#29992;&#24037;&#20855;&#65292;&#21363;&#22810;&#26234;&#33021;&#20307;&#25919;&#31574;&#36317;&#31163;&#65288;MAPD&#65289;&#12290;&#36890;&#36807;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20915;&#31574;&#26465;&#20214;&#34920;&#31034;&#65292;MAPD&#21487;&#20197;&#35745;&#31639;&#20219;&#24847;&#19968;&#23545;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#25919;&#31574;&#36317;&#31163;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#21040;&#23450;&#21046;&#21270;&#29256;&#26412;&#20197;&#37327;&#21270;&#26234;&#33021;&#20307;&#25919;&#31574;&#22312;&#29305;&#23450;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#36825;&#20010;&#24037;&#20855;&#19981;&#20165;&#26377;&#21161;&#20110;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#22810;&#26679;&#24615;&#30340;&#28436;&#21464;&#65292;&#36824;&#20026;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;MARL&#31639;&#27861;&#30340;&#35774;&#35745;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.11257</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25919;&#31574;&#36317;&#31163;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Policy Distance for Multi-Agent Reinforcement Learning. (arXiv:2401.11257v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25919;&#31574;&#24046;&#24322;&#30340;&#36890;&#29992;&#24037;&#20855;&#65292;&#21363;&#22810;&#26234;&#33021;&#20307;&#25919;&#31574;&#36317;&#31163;&#65288;MAPD&#65289;&#12290;&#36890;&#36807;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20915;&#31574;&#26465;&#20214;&#34920;&#31034;&#65292;MAPD&#21487;&#20197;&#35745;&#31639;&#20219;&#24847;&#19968;&#23545;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#25919;&#31574;&#36317;&#31163;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#21040;&#23450;&#21046;&#21270;&#29256;&#26412;&#20197;&#37327;&#21270;&#26234;&#33021;&#20307;&#25919;&#31574;&#22312;&#29305;&#23450;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#36825;&#20010;&#24037;&#20855;&#19981;&#20165;&#26377;&#21161;&#20110;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#22810;&#26679;&#24615;&#30340;&#28436;&#21464;&#65292;&#36824;&#20026;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;MARL&#31639;&#27861;&#30340;&#35774;&#35745;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#24615;&#22312;&#25913;&#21892;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30446;&#21069;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#35768;&#22810;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;MARL&#20013;&#36807;&#22810;&#21442;&#25968;&#20849;&#20139;&#30340;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#19968;&#31181;&#36890;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#25919;&#31574;&#24046;&#24322;&#12290;&#36825;&#26679;&#30340;&#24230;&#37327;&#26631;&#20934;&#19981;&#20165;&#21487;&#20197;&#26041;&#20415;&#22320;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#22810;&#26679;&#24615;&#30340;&#28436;&#21464;&#65292;&#36824;&#21487;&#20197;&#20026;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;MARL&#31639;&#27861;&#30340;&#35774;&#35745;&#25552;&#20379;&#25351;&#23548;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#25919;&#31574;&#36317;&#31163;&#65288;MAPD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#27979;&#37327;MARL&#20013;&#25919;&#31574;&#24046;&#24322;&#30340;&#36890;&#29992;&#24037;&#20855;&#12290;&#36890;&#36807;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#26465;&#20214;&#34920;&#31034;&#65292;MAPD&#21487;&#20197;&#35745;&#31639;&#20219;&#24847;&#19968;&#23545;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#25919;&#31574;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;MAPD&#25193;&#23637;&#20026;&#21487;&#23450;&#21046;&#30340;&#29256;&#26412;&#65292;&#21487;&#20197;&#37327;&#21270;&#22312;&#25351;&#23450;&#26041;&#38754;&#30340;&#26234;&#33021;&#20307;&#25919;&#31574;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22522;&#20110;MAPD&#30340;&#22312;&#32447;&#37096;&#32626;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#21160;&#24577;&#21442;&#25968;&#20849;&#20139;&#65288;MAD&#65289;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diversity plays a crucial role in improving the performance of multi-agent reinforcement learning (MARL). Currently, many diversity-based methods have been developed to overcome the drawbacks of excessive parameter sharing in traditional MARL. However, there remains a lack of a general metric to quantify policy differences among agents. Such a metric would not only facilitate the evaluation of the diversity evolution in multi-agent systems, but also provide guidance for the design of diversity-based MARL algorithms. In this paper, we propose the multi-agent policy distance (MAPD), a general tool for measuring policy differences in MARL. By learning the conditional representations of agents' decisions, MAPD can computes the policy distance between any pair of agents. Furthermore, we extend MAPD to a customizable version, which can quantify differences among agent policies on specified aspects. Based on the online deployment of MAPD, we design a multi-agent dynamic parameter sharing (MAD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25512;&#24191;&#30340;&#28436;&#35762;&#32773;&#39564;&#35777;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#20882;&#20805;&#32773;&#21644;&#27450;&#39575;&#25915;&#20987;&#65292;&#25552;&#20379;&#26356;&#24378;&#30340;&#20445;&#25252;&#21644;&#26356;&#32463;&#27982;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2401.11156</link><description>&lt;p&gt;
&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25512;&#24191;&#28436;&#35762;&#32773;&#39564;&#35777;&#20197;&#23545;&#25239;&#27450;&#39575;&#24847;&#35782;
&lt;/p&gt;
&lt;p&gt;
Generalizing Speaker Verification for Spoof Awareness in the Embedding Space. (arXiv:2401.11156v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25512;&#24191;&#30340;&#28436;&#35762;&#32773;&#39564;&#35777;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#20882;&#20805;&#32773;&#21644;&#27450;&#39575;&#25915;&#20987;&#65292;&#25552;&#20379;&#26356;&#24378;&#30340;&#20445;&#25252;&#21644;&#26356;&#32463;&#27982;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#20247;&#25152;&#21608;&#30693;&#65292;&#33258;&#21160;&#28436;&#35762;&#32773;&#39564;&#35777;&#65288;ASV&#65289;&#31995;&#32479;&#21487;&#20197;&#34987;&#21508;&#31181;&#31867;&#22411;&#30340;&#23545;&#25163;&#27450;&#39575;&#12290;&#23545;&#25239;ASV&#31995;&#32479;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#24320;&#21457;&#19968;&#20010;&#29420;&#31435;&#30340;&#27450;&#39575;&#23545;&#31574;&#65288;CM&#65289;&#27169;&#22359;&#65292;&#23558;&#28436;&#35762;&#36755;&#20837;&#20998;&#31867;&#20026;&#30495;&#23454;&#25110;&#20266;&#36896;&#30340;&#35805;&#35821;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35774;&#35745;&#22312;&#35748;&#35777;&#38454;&#27573;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#21644;&#21033;&#29992;&#12290;&#21478;&#19968;&#31181;&#31574;&#30053;&#26159;&#35774;&#35745;&#19968;&#20010;&#32479;&#19968;&#30340;ASV&#31995;&#32479;&#65292;&#21487;&#20197;&#22788;&#29702;&#38646;&#21162;&#21147;&#30340;&#20882;&#20805;&#32773;&#65288;&#38750;&#30446;&#26631;&#65289;&#21644;&#27450;&#39575;&#25915;&#20987;&#12290;&#36825;&#31181;&#20855;&#26377;&#27450;&#39575;&#24847;&#35782;&#30340;ASV&#31995;&#32479;&#26377;&#21487;&#33021;&#25552;&#20379;&#26356;&#24378;&#30340;&#20445;&#25252;&#21644;&#26356;&#32463;&#27982;&#30340;&#35745;&#31639;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25512;&#24191;&#29420;&#31435;ASV&#65288;G-SASV&#65289;&#26469;&#23545;&#25239;&#27450;&#39575;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#65288;&#35748;&#35777;&#65289;&#38454;&#27573;&#19981;&#28041;&#21450;&#29420;&#31435;&#30340;CM&#27169;&#22359;&#65292;&#24182;&#21033;&#29992;&#26469;&#33258;CM&#30340;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#31616;&#21333;&#30340;&#21518;&#31471;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is now well-known that automatic speaker verification (ASV) systems can be spoofed using various types of adversaries. The usual approach to counteract ASV systems against such attacks is to develop a separate spoofing countermeasure (CM) module to classify speech input either as a bonafide, or a spoofed utterance. Nevertheless, such a design requires additional computation and utilization efforts at the authentication stage. An alternative strategy involves a single monolithic ASV system designed to handle both zero-effort imposter (non-targets) and spoofing attacks. Such spoof-aware ASV systems have the potential to provide stronger protections and more economic computations. To this end, we propose to generalize the standalone ASV (G-SASV) against spoofing attacks, where we leverage limited training data from CM to enhance a simple backend in the embedding space, without the involvement of a separate CM module during the test (authentication) phase. We propose a novel yet simple 
&lt;/p&gt;</description></item><item><title>SPAND&#26159;&#19968;&#20010;&#21033;&#29992;&#32593;&#32476;&#21160;&#24577;&#30340;&#30561;&#30496;&#39044;&#27979;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#22270;&#32593;&#32476;&#21644;&#31227;&#21160;&#35774;&#22791;&#25968;&#25454;&#26469;&#39044;&#27979;&#19979;&#19968;&#22825;&#30340;&#30561;&#30496;&#25345;&#32493;&#26102;&#38388;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2401.11113</link><description>&lt;p&gt;
SPAND: &#20351;&#29992;&#32593;&#32476;&#21160;&#24577;&#30340;&#30561;&#30496;&#39044;&#27979;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SPAND: Sleep Prediction Architecture using Network Dynamics. (arXiv:2401.11113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11113
&lt;/p&gt;
&lt;p&gt;
SPAND&#26159;&#19968;&#20010;&#21033;&#29992;&#32593;&#32476;&#21160;&#24577;&#30340;&#30561;&#30496;&#39044;&#27979;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#22270;&#32593;&#32476;&#21644;&#31227;&#21160;&#35774;&#22791;&#25968;&#25454;&#26469;&#39044;&#27979;&#19979;&#19968;&#22825;&#30340;&#30561;&#30496;&#25345;&#32493;&#26102;&#38388;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#34892;&#20026;&#23545;&#20581;&#24247;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#23545;&#36523;&#24515;&#20581;&#24247;&#30340;&#25351;&#31034;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#30340;&#20256;&#24863;&#22120;&#30417;&#27979;&#21644;&#39044;&#27979;&#30561;&#30496;&#34892;&#20026;&#65292;&#21487;&#20197;&#24110;&#21161;&#31649;&#29702;&#30561;&#30496;&#24182;&#36861;&#36394;&#30456;&#20851;&#20581;&#24247;&#29366;&#20917;&#12290;&#34429;&#28982;&#30561;&#30496;&#34892;&#20026;&#21462;&#20915;&#20110;&#20010;&#20307;&#30340;&#29983;&#29702;&#29366;&#20917;&#65292;&#20294;&#20063;&#21463;&#21040;&#25968;&#23383;&#23186;&#20307;&#20351;&#29992;&#12289;&#31038;&#20132;&#32593;&#32476;&#20256;&#26579;&#20197;&#21450;&#21608;&#22260;&#22825;&#27668;&#31561;&#22806;&#37096;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPAND&#65288;Sleep Prediction Architecture using Network Dynamics&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22270;&#32593;&#32476;&#20013;&#30340;&#31038;&#20132;&#20256;&#26579;&#26469;&#39044;&#27979;&#30561;&#30496;&#34892;&#20026;&#30340;&#31995;&#32479;&#65292;&#24182;&#23558;&#20854;&#19982;&#20174;&#26222;&#36941;&#23384;&#22312;&#30340;&#31227;&#21160;&#35774;&#22791;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#20013;&#25552;&#21462;&#30340;&#29983;&#29702;&#21644;&#25163;&#26426;&#25968;&#25454;&#38598;&#25104;&#65292;&#20197;&#39044;&#27979;&#19979;&#19968;&#22825;&#30340;&#30561;&#30496;&#25345;&#32493;&#26102;&#38388;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#27880;&#24847;&#26426;&#21046;&#65292;&#20811;&#26381;&#20102;&#21253;&#21547;&#19982;&#30561;&#30496;&#34892;&#20026;&#26080;&#20851;&#30340;&#36830;&#25509;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#30340;&#23616;&#38480;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#31361;&#26174;&#20986;&#35813;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep behavior significantly impacts health and acts as an indicator of physical and mental well-being. Monitoring and predicting sleep behavior with ubiquitous sensors may therefore assist in both sleep management and tracking of related health conditions. While sleep behavior depends on, and is reflected in the physiology of a person, it is also impacted by external factors such as digital media usage, social network contagion, and the surrounding weather. In this work, we propose SPAND (Sleep Prediction Architecture using Network Dynamics), a system that exploits social contagion in sleep behavior through graph networks and integrates it with physiological and phone data extracted from ubiquitous mobile and wearable devices for predicting next-day sleep labels about sleep duration. Our architecture overcomes the limitations of large-scale graphs containing connections irrelevant to sleep behavior by devising an attention mechanism. The extensive experimental evaluation highlights th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.10711</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;&#26088;&#22312;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#35270;&#39057;&#20449;&#24687;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#23613;&#31649;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#22270;&#20687;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#36824;&#19981;&#36275;&#22815;&#65292;&#20165;&#20165;&#26159;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#24573;&#30053;&#20102;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#35270;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#38024;&#23545;&#38382;&#39064;&#20851;&#38190;&#26102;&#38388;&#25139;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#24378;&#21046;LMMs&#20351;&#29992;&#38382;&#39064;&#20851;&#38190;&#26102;&#21051;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#21512;&#24182;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#20197;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#36825;&#20123;&#26102;&#21051;&#23558;&#20316;&#20026;&#20266;&#26631;&#31614;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#20266;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#30340;&#24369;&#30417;&#30563;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#12290;GCG&#23398;&#20064;&#22810;&#20010;&#39640;&#26031;&#20989;&#25968;&#26469;&#25551;&#36848;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels. With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure o
&lt;/p&gt;</description></item><item><title>BioDiffusion&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#21512;&#25104;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#38750;&#31283;&#24577;&#30340;&#22810;&#21464;&#37327;&#20449;&#21495;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21512;&#25104;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22797;&#26434;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10282</link><description>&lt;p&gt;
BioDiffusion&#65306;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#21512;&#25104;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis. (arXiv:2401.10282v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10282
&lt;/p&gt;
&lt;p&gt;
BioDiffusion&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#21512;&#25104;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#38750;&#31283;&#24577;&#30340;&#22810;&#21464;&#37327;&#20449;&#21495;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21512;&#25104;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22797;&#26434;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#38754;&#20020;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12289;&#26631;&#31614;&#22797;&#26434;&#24615;&#21644;&#27979;&#37327;&#22122;&#22768;&#30340;&#24178;&#25200;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#25361;&#25112;&#32463;&#24120;&#38459;&#30861;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#20339;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BioDiffusion&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#21512;&#25104;&#22810;&#21464;&#37327;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#36827;&#34892;&#20248;&#21270;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;BioDiffusion&#22312;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#38750;&#31283;&#24577;&#30340;&#22810;&#21464;&#37327;&#20449;&#21495;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#29992;&#20110;&#26080;&#26465;&#20214;&#12289;&#26631;&#31614;&#26465;&#20214;&#21644;&#20449;&#21495;&#26465;&#20214;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#21033;&#29992;&#36825;&#20123;&#21512;&#25104;&#30340;&#20449;&#21495;&#20026;&#19978;&#36848;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;&#23545;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#36827;&#34892;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#65292;&#24378;&#35843;&#20854;&#22312;&#19982;&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19982;&#24403;&#21069;&#20027;&#27969;&#30340;&#26102;&#38388;&#31995;&#20449;&#24687;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;BioDiffusion&#22312;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning tasks involving biomedical signals frequently grapple with issues such as limited data availability, imbalanced datasets, labeling complexities, and the interference of measurement noise. These challenges often hinder the optimal training of machine learning algorithms. Addressing these concerns, we introduce BioDiffusion, a diffusion-based probabilistic model optimized for the synthesis of multivariate biomedical signals. BioDiffusion demonstrates excellence in producing high-fidelity, non-stationary, multivariate signals for a range of tasks including unconditional, label-conditional, and signal-conditional generation. Leveraging these synthesized signals offers a notable solution to the aforementioned challenges. Our research encompasses both qualitative and quantitative assessments of the synthesized data quality, underscoring its capacity to bolster accuracy in machine learning tasks tied to biomedical signals. Furthermore, when juxtaposed with current leading tim
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23616;&#37096;&#31232;&#30095;&#27169;&#22411;ContraLSP&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31435;&#26679;&#26412;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;ContraLSP&#22312;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.08552</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#24615;&#21644;&#23616;&#37096;&#31232;&#30095;&#25200;&#21160;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Explaining Time Series via Contrastive and Locally Sparse Perturbations. (arXiv:2401.08552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08552
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23616;&#37096;&#31232;&#30095;&#27169;&#22411;ContraLSP&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31435;&#26679;&#26412;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;ContraLSP&#22312;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#26159;&#19968;&#20010;&#22797;&#21512;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#37325;&#35201;&#20301;&#32622;&#24182;&#21305;&#37197;&#22797;&#26434;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#34429;&#28982;&#20043;&#21069;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#30340;&#25200;&#21160;&#21487;&#33021;&#26080;&#27861;&#20943;&#36731;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#24322;&#36136;&#26679;&#26412;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ContraLSP&#65292;&#36825;&#26159;&#19968;&#20010;&#23616;&#37096;&#31232;&#30095;&#27169;&#22411;&#65292;&#24341;&#20837;&#23545;&#31435;&#26679;&#26412;&#26469;&#26500;&#24314;&#26080;&#20449;&#24687;&#30340;&#25200;&#21160;&#65292;&#20294;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#20445;&#25345;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#26679;&#26412;&#29305;&#23450;&#30340;&#31232;&#30095;&#38376;&#65292;&#29983;&#25104;&#26356;&#22810;&#20108;&#36827;&#21046;&#20559;&#26012;&#21644;&#24179;&#28369;&#30340;&#36974;&#32617;&#65292;&#21487;&#20197;&#36731;&#26494;&#32508;&#21512;&#26102;&#38388;&#36235;&#21183;&#24182;&#31616;&#27905;&#22320;&#36873;&#25321;&#26174;&#33879;&#29305;&#24449;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;ContraLSP&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#22312;&#35299;&#37322;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/zichuan-liu/ContraL}&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining multivariate time series is a compound challenge, as it requires identifying important locations in the time series and matching complex temporal patterns. Although previous saliency-based methods addressed the challenges, their perturbation may not alleviate the distribution shift issue, which is inevitable especially in heterogeneous samples. We present ContraLSP, a locally sparse model that introduces counterfactual samples to build uninformative perturbations but keeps distribution using contrastive learning. Furthermore, we incorporate sample-specific sparse gates to generate more binary-skewed and smooth masks, which easily integrate temporal trends and select the salient features parsimoniously. Empirical studies on both synthetic and real-world datasets show that ContraLSP outperforms state-of-the-art models, demonstrating a substantial improvement in explanation quality for time series data. The source code is available at \url{https://github.com/zichuan-liu/ContraL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26222;&#36866;&#30340;&#30693;&#35782;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#21407;&#22411;&#24320;&#21457;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#12290;&#35813;&#26550;&#26500;&#21253;&#25324;42&#31181;&#35748;&#30693;&#26550;&#26500;&#21644;&#19968;&#32452;&#21151;&#33021;&#27169;&#22359;&#65292;&#29992;&#20110;&#25509;&#36817;AGI&#33021;&#21147;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#21508;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#30693;&#35782;&#34920;&#31034;&#25972;&#21512;&#21040;&#19968;&#20010;&#30693;&#35782;&#24211;&#20013;&#12290;</title><link>http://arxiv.org/abs/2401.06256</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#21407;&#22411;&#24320;&#21457;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#36866;&#30693;&#35782;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Universal Knowledge Model and Cognitive Architecture for Prototyping AGI. (arXiv:2401.06256v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26222;&#36866;&#30340;&#30693;&#35782;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#21407;&#22411;&#24320;&#21457;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#12290;&#35813;&#26550;&#26500;&#21253;&#25324;42&#31181;&#35748;&#30693;&#26550;&#26500;&#21644;&#19968;&#32452;&#21151;&#33021;&#27169;&#22359;&#65292;&#29992;&#20110;&#25509;&#36817;AGI&#33021;&#21147;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#21508;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#30693;&#35782;&#34920;&#31034;&#25972;&#21512;&#21040;&#19968;&#20010;&#30693;&#35782;&#24211;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30830;&#23450;&#20102;42&#31181;&#29992;&#20110;&#21019;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;&#21151;&#33021;&#27169;&#22359;&#65292;&#36825;&#20123;&#27169;&#22359;&#26159;&#25509;&#36817;AGI&#33021;&#21147;&#30340;&#26234;&#33021;&#31995;&#32479;&#25152;&#24212;&#20855;&#22791;&#30340;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#26550;&#26500;&#20013;&#27809;&#26377;&#25214;&#21040;&#25152;&#38656;&#30340;&#21151;&#33021;&#27169;&#22359;&#38598;&#21512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#25509;&#36817;AGI&#33021;&#21147;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#20316;&#20026;&#26550;&#26500;&#26694;&#26550;&#20013;&#30340;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#21508;&#31181;&#38750;&#24418;&#24335;&#21270;&#12289;&#37096;&#20998;&#21644;&#23436;&#20840;&#24418;&#24335;&#21270;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#32467;&#21512;&#22312;&#19968;&#20010;&#30693;&#35782;&#24211;&#20013;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#35760;&#24405;&#12289;&#22270;&#24418;&#12289;&#31639;&#27861;&#12289;&#25968;&#25454;&#24211;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#30693;&#35782;&#22270;&#12289;&#26412;&#20307;&#12289;&#26694;&#26550;&#12289;&#23454;&#36136;-&#23646;&#24615;-&#20851;&#31995;&#27169;&#22411;&#12289;&#25512;&#29702;&#31995;&#32479;&#12289;&#35859;&#35789;&#28436;&#31639;&#27169;&#22411;&#12289;&#27010;&#24565;&#27169;&#22411;&#31561;&#12290;&#20026;&#20102;&#32452;&#21512;&#21644;&#32467;&#26500;&#21270;&#21508;&#20010;&#29255;&#27573;
&lt;/p&gt;
&lt;p&gt;
The article identified 42 cognitive architectures for creating general artificial intelligence (AGI) and proposed a set of interrelated functional blocks that an agent approaching AGI in its capabilities should possess. Since the required set of blocks is not found in any of the existing architectures, the article proposes a new cognitive architecture for intelligent systems approaching AGI in their capabilities. As one of the key solutions within the framework of the architecture, a universal method of knowledge representation is proposed, which allows combining various non-formalized, partially and fully formalized methods of knowledge representation in a single knowledge base, such as texts in natural languages, images, audio and video recordings, graphs, algorithms, databases, neural networks, knowledge graphs, ontologies, frames, essence-property-relation models, production systems, predicate calculus models, conceptual models, and others. To combine and structure various fragment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#29983;&#24179;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#25552;&#20379;&#20102;&#24211;&#22270;&#19982;&#20449;&#24687;&#31185;&#23398;&#39046;&#22495;&#20808;&#39537;&#20154;&#29289;S.R. Ranganathan&#30340;360&#24230;&#35270;&#35282;&#65292;&#36825;&#31181;&#19987;&#38376;&#30340;&#34920;&#31034;&#22312;&#33539;&#22260;&#21644;&#35206;&#30422;&#33539;&#22260;&#19978;&#26080;&#21487;&#27604;&#25311;&#65292;&#24182;&#21628;&#21505;&#25972;&#20010;&#31038;&#21306;&#20849;&#21516;&#21162;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.03343</link><description>&lt;p&gt;
&#37325;&#26032;&#21457;&#29616;Ranganathan&#65306;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#30340;&#20809;&#35889;&#35270;&#35282;&#20102;&#35299;&#20182;&#30340;&#29983;&#24179;
&lt;/p&gt;
&lt;p&gt;
Rediscovering Ranganathan: A Prismatic View of His Life through the Knowledge Graph Spectrum. (arXiv:2401.03343v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#29983;&#24179;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#25552;&#20379;&#20102;&#24211;&#22270;&#19982;&#20449;&#24687;&#31185;&#23398;&#39046;&#22495;&#20808;&#39537;&#20154;&#29289;S.R. Ranganathan&#30340;360&#24230;&#35270;&#35282;&#65292;&#36825;&#31181;&#19987;&#38376;&#30340;&#34920;&#31034;&#22312;&#33539;&#22260;&#21644;&#35206;&#30422;&#33539;&#22260;&#19978;&#26080;&#21487;&#27604;&#25311;&#65292;&#24182;&#21628;&#21505;&#25972;&#20010;&#31038;&#21306;&#20849;&#21516;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#24211;&#22270;&#19982;&#20449;&#24687;&#31185;&#23398;&#39046;&#22495;&#30340;&#20808;&#39537;&#20154;&#29289;S.R. Ranganathan&#25945;&#25480;&#30340;&#26032;&#39062;&#29983;&#24179;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#12290;&#21457;&#29616;&#20851;&#20110;Ranganathan&#30340;&#30456;&#20851;&#20107;&#23454;&#23384;&#22312;&#20110;&#21508;&#31181;&#36164;&#28304;&#20013;&#65292;&#20197;&#30862;&#29255;&#21270;&#21644;&#38646;&#25955;&#30340;&#26041;&#24335;&#25552;&#20379;&#20449;&#24687;&#12290;&#36890;&#36807;&#36825;&#20010;&#19987;&#38376;&#30340;KG&#65292;&#25105;&#20204;&#24076;&#26395;&#20026;&#20182;&#30340;&#29983;&#24179;&#21644;&#25104;&#23601;&#25552;&#20379;&#19968;&#20010;360&#24230;&#30340;&#35270;&#35282;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#31181;&#19987;&#38376;&#30340;&#34920;&#31034;&#22312;&#20854;&#33539;&#22260;&#21644;&#35206;&#30422;&#33539;&#22260;&#19978;&#26159;&#26080;&#21487;&#27604;&#25311;&#30340;&#65306;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#20379;&#20219;&#20309;&#20154;&#20844;&#24320;&#35775;&#38382;&#12289;&#20351;&#29992;/&#20877;&#20351;&#29992;&#21644;&#36129;&#29486;&#12290;&#21463;Ranganathan&#30340;&#29702;&#35770;&#21644;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;KG&#20351;&#29992;&#8220;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#35770;&#8221;&#22312;&#20004;&#20010;&#23618;&#38754;&#19978;&#36827;&#34892;&#20102;&#21457;&#23637;&#65306;&#22312;&#20851;&#38190;&#30340;&#29983;&#24179;&#26041;&#38754;&#30340;&#26631;&#35782;&#21644;&#26412;&#20307;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#21628;&#21505;&#25972;&#20010;&#31038;&#21306;&#20849;&#21516;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The present study puts forward a novel biographical knowledge graph (KG) on Prof. S. R. Ranganathan, one of the pioneering figures in the Library and Information Science (LIS) domain. It has been found that most of the relevant facts about Ranganathan exist in a variety of resources (e.g., books, essays, journal articles, websites, blogs, etc.), offering information in a fragmented and piecemeal way. With this dedicated KG (henceforth known as RKG), we hope to furnish a 360-degree view of his life and achievements. To the best of our knowledge, such a dedicated representation is unparalleled in its scope and coverage: using state-of-the-art technology for anyone to openly access, use/re-use, and contribute. Inspired by Ranganathan's theories and ideas, the KG was developed using a "facet-based methodology" at two levels: in the identification of the vital biographical aspects and the development of the ontological model. Finally, with this study, we call for a community-driven effort t
&lt;/p&gt;</description></item><item><title>NODEC&#26159;&#19968;&#31181;&#29992;&#31070;&#32463;ODE&#27169;&#22411;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#19982;&#25511;&#21046;&#22120;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.01836</link><description>&lt;p&gt;
NODEC: &#29992;&#20110;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#26368;&#20248;&#25511;&#21046;&#30340;&#31070;&#32463;ODE
&lt;/p&gt;
&lt;p&gt;
NODEC: Neural ODE For Optimal Control of Unknown Dynamical Systems. (arXiv:2401.01836v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01836
&lt;/p&gt;
&lt;p&gt;
NODEC&#26159;&#19968;&#31181;&#29992;&#31070;&#32463;ODE&#27169;&#22411;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#19982;&#25511;&#21046;&#22120;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#36890;&#24120;&#28041;&#21450;&#22312;&#21464;&#21270;&#35745;&#31639;&#26694;&#26550;&#19979;&#26368;&#23567;&#21270;&#20855;&#26377;&#24050;&#30693;&#21160;&#21147;&#23398;&#30340;&#26576;&#20123;&#25511;&#21046;&#30446;&#26631;&#12290;&#23545;&#20110;&#20855;&#26377;&#26410;&#30693;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#65292;&#38656;&#35201;&#39069;&#22806;&#36827;&#34892;&#21160;&#21147;&#23398;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#20219;&#20309;&#19981;&#20934;&#30830;&#37117;&#20250;&#23548;&#33268;&#32467;&#26524;&#25511;&#21046;&#20989;&#25968;&#30340;&#27425;&#20248;&#24615;&#12290;&#21478;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861; - &#24378;&#21270;&#23398;&#20064;&#65292;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#34701;&#20837;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#24191;&#27867;&#20132;&#20114;&#26469;&#36817;&#20284;&#20540;&#20989;&#25968;&#25110;&#31574;&#30053;&#26799;&#24230;&#65292;&#20294;&#23427;&#30340;&#25968;&#25454;&#25928;&#29575;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NODEC&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#31070;&#32463;ODE&#27169;&#22411;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#19982;&#25511;&#21046;&#22120;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#12290;&#36890;&#36807;&#20004;&#20010;&#32806;&#21512;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#26377;&#36259;&#30456;&#20114;&#20316;&#29992;&#65292;NODEC&#23398;&#20064;&#20102;&#31995;&#32479;&#21160;&#21147;&#23398;&#20197;&#21450;&#25351;&#23548;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling complex dynamical systems is generally associated with minimizing certain control objectives with known dynamics under the variational calculus framework. For systems with unknown dynamics, an additional step of dynamics modeling is required. However, any inaccuracy in dynamics modeling will lead to sub-optimality in the resulting control function. Another set of approaches for controlling unknown dynamical systems - reinforcement learning, folds the dynamics modeling into controller training via value function approximation or policy gradient through extensively interacting with the environment, but it suffers from low data efficiency. To address these, we introduce NODEC, a novel framework for controlling unknown dynamical systems, which combines dynamics modelling and controller training using a coupled neural ODE model. Through an intriguing interplay between the two coupled neural networks, NODEC learns system dynamics as well as optimal controls that guides the unknow
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30697;&#38453;&#20056;&#31215;&#24577;(MPS)&#30340;&#28040;&#24687;&#20256;&#36882;&#31574;&#30053;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20960;&#20309;&#22270;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.01801</link><description>&lt;p&gt;
&#19968;&#20010;&#37327;&#23376;&#21551;&#21457;&#30340;&#29992;&#20110;&#20960;&#20309;&#24314;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A quatum inspired neural network for geometric modeling. (arXiv:2401.01801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01801
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30697;&#38453;&#20056;&#31215;&#24577;(MPS)&#30340;&#28040;&#24687;&#20256;&#36882;&#31574;&#30053;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20960;&#20309;&#22270;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#29289;&#29702;&#31995;&#32479;&#26500;&#24819;&#20026;3D&#22810;&#20307;&#28857;&#20113;&#65292;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#65292;&#22914;SE(3)/E(3)&#31561;&#25928;GNN&#65292;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#20204;&#39640;&#25928;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#20351;&#23427;&#20204;&#33021;&#22815;&#29087;&#32451;&#22320;&#23545;&#20998;&#23376;&#21644;&#26230;&#20307;&#26448;&#26009;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20960;&#20309;GNN&#21482;&#25552;&#20379;&#20102;&#22810;&#20307;&#31995;&#32479;&#30340;&#24179;&#22343;&#22330;&#36817;&#20284;&#65292;&#23553;&#35013;&#22312;&#20004;&#20307;&#28040;&#24687;&#20256;&#36882;&#20013;&#65292;&#22240;&#27492;&#22312;&#25429;&#25417;&#36825;&#20123;&#20960;&#20309;&#22270;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#26377;&#25152;&#27424;&#32570;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#35745;&#31639;&#29289;&#29702;&#23398;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#39640;&#38454;&#24352;&#37327;&#26469;&#22788;&#29702;&#22810;&#20307;&#31995;&#32479;&#30340;&#24352;&#37327;&#32593;&#32476;&#34987;&#24341;&#20837;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#24352;&#37327;&#21270;&#32593;&#32476;&#25972;&#21512;&#21040;GNN&#30340;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#20013;&#38754;&#20020;&#30528;&#21487;&#25193;&#23637;&#24615;&#21644;&#23545;&#31216;&#24615;&#20445;&#25345;&#65288;&#22914;&#32622;&#25442;&#21644;&#26059;&#36716;&#65289;&#30340;&#25361;&#25112;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31561;&#21464;&#30697;&#38453;&#20056;&#31215;&#24577;(MPS)&#30340;&#28040;&#24687;&#20256;&#36882;&#31574;&#30053;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
By conceiving physical systems as 3D many-body point clouds, geometric graph neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased promising performance. In particular, their effective message-passing mechanics make them adept at modeling molecules and crystalline materials. However, current geometric GNNs only offer a mean-field approximation of the many-body system, encapsulated within two-body message passing, thus falling short in capturing intricate relationships within these geometric graphs. To address this limitation, tensor networks, widely employed by computational physics to handle manybody systems using high-order tensors, have been introduced. Nevertheless, integrating these tensorized networks into the message-passing framework of GNNs faces scalability and symmetry conservation (e.g., permutation and rotation) challenges. In response, we introduce an innovative equivariant Matrix Product State (MPS)-based message-passing strategy, through achieving a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#21463;&#25928;&#29575;&#38480;&#21046;&#30340;&#25928;&#29992;-&#38544;&#31169;&#21452;&#30446;&#26631;&#20248;&#21270;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#25928;&#29992;-&#38544;&#31169;&#30340;&#26435;&#34913;&#65292;&#24573;&#35270;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#20854;&#20182;&#24433;&#21709;&#22240;&#32032;&#12290;&#35813;&#30740;&#31350;&#23545;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2312.16554</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#21463;&#25928;&#29575;&#38480;&#21046;&#30340;&#25928;&#29992;-&#38544;&#31169;&#21452;&#30446;&#26631;&#20248;&#21270;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Analysis of Efficiency Constrained Utility-Privacy Bi-Objective Optimization in Federated Learning. (arXiv:2312.16554v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#21463;&#25928;&#29575;&#38480;&#21046;&#30340;&#25928;&#29992;-&#38544;&#31169;&#21452;&#30446;&#26631;&#20248;&#21270;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#25928;&#29992;-&#38544;&#31169;&#30340;&#26435;&#34913;&#65292;&#24573;&#35270;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#20854;&#20182;&#24433;&#21709;&#22240;&#32032;&#12290;&#35813;&#30740;&#31350;&#23545;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#20010;&#20307;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#23398;&#20064;&#20849;&#20139;&#27169;&#22411;&#12290;FL&#20013;&#30340;&#25928;&#29992;&#12289;&#38544;&#31169;&#21644;&#35757;&#32451;&#25928;&#29575;&#38382;&#39064;&#24050;&#24341;&#36215;&#37325;&#35201;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#24046;&#20998;&#38544;&#31169;&#24050;&#25104;&#20026;FL&#20013;&#19968;&#31181;&#20027;&#27969;&#25216;&#26415;&#65292;&#20445;&#25252;&#20010;&#20307;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#21516;&#26102;&#24433;&#21709;&#25928;&#29992;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#65288;DPFL&#65289;&#20013;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#25928;&#29992;-&#38544;&#31169;&#30340;&#26435;&#34913;&#65292;&#32780;&#24573;&#35270;&#20102;&#21450;&#26102;&#23436;&#25104;&#25152;&#24517;&#38656;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#24046;&#20998;&#38544;&#31169;&#36890;&#36807;&#22312;&#27599;&#36718;&#36890;&#20449;&#20013;&#23545;&#36873;&#23450;&#30340;&#23458;&#25143;&#31471;&#24341;&#20837;&#21463;&#25511;&#30340;&#38543;&#26426;&#24615;&#65288;&#22122;&#22768;&#65289;&#26469;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#30740;&#31350;&#20102;&#22122;&#22768;&#27700;&#24179;&#65288;$\sigma$&#65289;&#21644;&#36890;&#20449;&#36718;&#25968;&#65288;$T$&#65289;&#23545;&#38544;&#31169;-&#25928;&#29992;&#21160;&#24577;&#30340;&#24433;&#21709;&#65292;&#20294;&#24573;&#35270;&#20102;&#20854;&#20182;&#24433;&#21709;&#22240;&#32032;&#65292;&#22914;&#26679;&#26412;&#27604;&#20363;&#65288;$q$&#65292;&#21363;&#36873;&#23450;&#23458;&#25143;&#31471;&#30340;&#27604;&#20363;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple clients to collaboratively learn a shared model without sharing their individual data. Concerns about utility, privacy, and training efficiency in FL have garnered significant research attention. Differential privacy has emerged as a prevalent technique in FL, safeguarding the privacy of individual user data while impacting utility and training efficiency. Within Differential Privacy Federated Learning (DPFL), previous studies have primarily focused on the utility-privacy trade-off, neglecting training efficiency, which is crucial for timely completion. Moreover, differential privacy achieves privacy by introducing controlled randomness (noise) on selected clients in each communication round. Previous work has mainly examined the impact of noise level ($\sigma$) and communication rounds ($T$) on the privacy-utility dynamic, overlooking other influential factors like the sample ratio ($q$, the proportion of selected clients). This paper systemati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26102;&#38388;&#21464;&#25442;&#22120;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26356;&#22909;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.11714</link><description>&lt;p&gt;
&#26102;&#38388;&#21464;&#25442;&#22120;&#65306;&#34701;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Time-Transformer: Integrating Local and Global Features for Better Time Series Generation. (arXiv:2312.11714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26102;&#38388;&#21464;&#25442;&#22120;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26356;&#22909;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26159;&#35299;&#20915;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#38388;&#29305;&#24615;&#65292;&#21253;&#25324;&#26412;&#22320;&#30456;&#20851;&#24615;&#21644;&#20840;&#23616;&#20381;&#36182;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#26410;&#33021;&#26377;&#25928;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;'&#26102;&#38388;&#21464;&#25442;&#22120;AAE'&#65292;&#23427;&#30001;&#19968;&#20010;&#23545;&#25239;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AAE&#65289;&#21644;&#19968;&#20010;&#21517;&#20026;'&#26102;&#38388;&#21464;&#25442;&#22120;'&#30340;&#26032;&#35774;&#35745;&#26550;&#26500;&#32452;&#25104;&#12290;&#26102;&#38388;&#21464;&#25442;&#22120;&#39318;&#20808;&#36890;&#36807;&#23618;&#27425;&#24182;&#34892;&#35774;&#35745;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#32467;&#21512;&#20102;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#21644;Transformer&#30340;&#33021;&#21147;&#65292;&#20998;&#21035;&#25552;&#21462;&#26412;&#22320;&#29305;&#24449;&#21644;&#20840;&#23616;&#20381;&#36182;&#24615;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#22312;&#20004;&#20010;&#20998;&#25903;&#20043;&#38388;&#25552;&#20379;&#20114;&#34917;&#30340;&#24341;&#23548;&#65292;&#24182;&#23454;&#29616;&#26412;&#22320;&#29305;&#24449;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#21512;&#36866;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating time series data is a promising approach to address data deficiency problems. However, it is also challenging due to the complex temporal properties of time series data, including local correlations as well as global dependencies. Most existing generative models have failed to effectively learn both the local and global properties of time series data. To address this open problem, we propose a novel time series generative model named 'Time-Transformer AAE', which consists of an adversarial autoencoder (AAE) and a newly designed architecture named 'Time-Transformer' within the decoder. The Time-Transformer first simultaneously learns local and global features in a layer-wise parallel design, combining the abilities of Temporal Convolutional Networks and Transformer in extracting local features and global dependencies respectively. Second, a bidirectional cross attention is proposed to provide complementary guidance across the two branches and achieve proper fusion between loc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;KL&#32422;&#26463;&#19979;&#30340;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#23454;&#36341;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.11456</link><description>&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#30340;&#36845;&#20195;&#20559;&#22909;&#23398;&#20064;&#65306;&#22312;KL&#32422;&#26463;&#19979;&#23558;&#29702;&#35770;&#19982;&#23454;&#36341;&#32852;&#31995;&#36215;&#26469;&#30340;RLHF
&lt;/p&gt;
&lt;p&gt;
Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint. (arXiv:2312.11456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;KL&#32422;&#26463;&#19979;&#30340;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#23454;&#36341;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#23545;&#40784;&#36807;&#31243;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26631;&#20934;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#21363;&#21453;&#21521;KL&#27491;&#21017;&#21270;&#30340;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#29992;&#20110;RLHF&#12290;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#65292;&#20294;&#23545;&#36825;&#20010;&#20844;&#24335;&#30340;&#20005;&#26684;&#29702;&#35770;&#20998;&#26512;&#20173;&#28982;&#24456;&#24320;&#25918;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#22312;&#31163;&#32447;&#12289;&#22312;&#32447;&#21644;&#28151;&#21512;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#26397;&#30528;&#23454;&#38469;&#24212;&#29992;&#30340;&#26041;&#21521;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23545;&#20449;&#24687;&#29702;&#35770;&#31574;&#30053;&#25913;&#36827;&#39044;&#35328;&#30340;&#31283;&#20581;&#36817;&#20284;&#65292;&#33258;&#28982;&#22320;&#20135;&#29983;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;RLHF&#31639;&#27861;&#12290;&#36825;&#21253;&#25324;&#22312;&#32447;&#22330;&#26223;&#20013;&#30340;&#36845;&#20195;&#29256;&#26412;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#31639;&#27861;&#65292;&#20197;&#21450;&#31163;&#32447;&#24773;&#26223;&#19979;&#30340;&#22810;&#27493;&#25298;&#32477;&#25277;&#26679;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#23545;&#40784;&#23454;&#39564;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the theoretical framework of the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees.  Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21311;&#21517;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25209;&#37327;&#25506;&#32034;&#25628;&#32034;&#31354;&#38388;&#30340;&#24605;&#24819;&#65292;&#22312;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#26102;&#23558;&#25628;&#32034;&#29366;&#24577;&#36827;&#34892;&#21387;&#32553;&#12289;&#23384;&#20648;&#21644;&#23637;&#24320;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#24230;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2312.10572</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#21311;&#21517;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved Anonymous Multi-Agent Path Finding Algorithm. (arXiv:2312.10572v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21311;&#21517;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25209;&#37327;&#25506;&#32034;&#25628;&#32034;&#31354;&#38388;&#30340;&#24605;&#24819;&#65292;&#22312;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#26102;&#23558;&#25628;&#32034;&#29366;&#24577;&#36827;&#34892;&#21387;&#32553;&#12289;&#23384;&#20648;&#21644;&#23637;&#24320;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#24230;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#21040;&#19968;&#31181;&#21311;&#21517;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;AMAPF&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#30340;&#38598;&#21512;&#34987;&#38480;&#21046;&#22312;&#19968;&#20010;&#22270;&#20013;&#65292;&#32473;&#23450;&#20102;&#19968;&#32452;&#30446;&#26631;&#39030;&#28857;&#65292;&#24182;&#19988;&#27599;&#20010;&#39030;&#28857;&#24517;&#39035;&#34987;&#26576;&#20010;&#26234;&#33021;&#20307;&#21040;&#36798;&#12290;&#35813;&#38382;&#39064;&#26159;&#25214;&#21040;&#30446;&#26631;&#19982;&#26234;&#33021;&#20307;&#30340;&#20998;&#37197;&#20197;&#21450;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#24182;&#19988;&#25105;&#20204;&#24076;&#26395;&#25214;&#21040;&#26368;&#20248;makespan&#30340;&#35299;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#25104;&#29087;&#26041;&#27861;&#26159;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#65292;&#21363;&#22312;&#36755;&#20837;&#22270;&#20135;&#29983;&#30340;&#36741;&#21161;&#22270;&#19978;&#25214;&#21040;&#26368;&#22823;&#27969;&#38382;&#39064;&#12290;&#21069;&#32773;&#30340;&#22823;&#23567;&#21487;&#33021;&#38750;&#24120;&#22823;&#65292;&#25628;&#32034;&#21487;&#33021;&#25104;&#20026;&#19968;&#20010;&#29942;&#39048;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#20511;&#21161;&#20102;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#19968;&#25209;&#25628;&#32034;&#29366;&#24577;&#32780;&#19981;&#26159;&#21333;&#29420;&#32771;&#34385;&#23427;&#20204;&#26469;&#25506;&#32034;&#25628;&#32034;&#31354;&#38388;&#30340;&#24819;&#27861;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#23558;&#25628;&#32034;&#29366;&#24577;&#30340;&#25209;&#37327;&#21387;&#32553;&#12289;&#23384;&#20648;&#21644;&#23637;&#24320;&#25104;&#21333;&#20010;&#29366;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#24230;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
We consider an Anonymous Multi-Agent Path-Finding (AMAPF) problem where the set of agents is confined to a graph, a set of goal vertices is given and each of these vertices has to be reached by some agent. The problem is to find an assignment of the goals to the agents as well as the collision-free paths, and we are interested in finding the solution with the optimal makespan. A well-established approach to solve this problem is to reduce it to a special type of a graph search problem, i.e. to the problem of finding a maximum flow on an auxiliary graph induced by the input one. The size of the former graph may be very large and the search on it may become a bottleneck. To this end, we suggest a specific search algorithm that leverages the idea of exploring the search space not through considering separate search states but rather bulks of them simultaneously. That is, we implicitly compress, store and expand bulks of the search states as single states, which results in high reduction i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25193;&#25955;&#26041;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#24102;&#26377;&#20445;&#30495;&#24230;&#39033;&#30340;&#26041;&#31243;&#65292;&#27491;&#24335;&#24314;&#31435;&#20102;GNN&#19982;&#25193;&#25955;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25551;&#36848;&#39640;&#38454;&#37051;&#23621;&#30340;&#26631;&#31614;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.08616</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#36890;&#29992;&#31070;&#32463;&#25193;&#25955;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generalized Neural Diffusion Framework on Graphs. (arXiv:2312.08616v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25193;&#25955;&#26041;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#24102;&#26377;&#20445;&#30495;&#24230;&#39033;&#30340;&#26041;&#31243;&#65292;&#27491;&#24335;&#24314;&#31435;&#20102;GNN&#19982;&#25193;&#25955;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25551;&#36848;&#39640;&#38454;&#37051;&#23621;&#30340;&#26631;&#31614;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;GNN&#21644;&#25193;&#25955;&#36807;&#31243;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36825;&#28608;&#21457;&#20102;&#35768;&#22810;&#22522;&#20110;&#25193;&#25955;&#30340;GNN&#30340;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20004;&#31181;&#26426;&#21046;&#23494;&#20999;&#30456;&#20851;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#33258;&#28982;&#22320;&#20135;&#29983;&#65306;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#21487;&#20197;&#27491;&#24335;&#32479;&#19968;&#36825;&#20123;GNN&#30340;&#36890;&#29992;&#25193;&#25955;&#26694;&#26550;&#65311;&#36825;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#19981;&#20165;&#21487;&#20197;&#21152;&#28145;&#25105;&#20204;&#23545;GNN&#23398;&#20064;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#32780;&#19988;&#36824;&#21487;&#33021;&#25171;&#24320;&#19968;&#20010;&#35774;&#35745;&#24191;&#27867;&#26032;&#30340;GNN&#31867;&#21035;&#30340;&#26032;&#22823;&#38376;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#20445;&#30495;&#24230;&#39033;&#30340;&#36890;&#29992;&#25193;&#25955;&#26041;&#31243;&#26694;&#26550;&#65292;&#23427;&#27491;&#24335;&#24314;&#31435;&#20102;&#25193;&#25955;&#36807;&#31243;&#19982;&#26356;&#22810;GNN&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22270;&#25193;&#25955;&#32593;&#32476;&#30340;&#19968;&#20010;&#29305;&#24449;&#65292;&#21363;&#24403;&#21069;&#31070;&#32463;&#25193;&#25955;&#36807;&#31243;&#21482;&#23545;&#24212;&#20110;&#19968;&#38454;&#25193;&#25955;&#26041;&#31243;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#39640;&#38454;&#37051;&#23621;&#30340;&#26631;&#31614;&#23454;&#38469;&#19978;&#34920;&#29616;&#20986;&#21333;&#19968;&#24615;&#29305;&#24449;&#65292;&#36825;&#24341;&#21457;&#20102;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies reveal the connection between GNNs and the diffusion process, which motivates many diffusion-based GNNs to be proposed. However, since these two mechanisms are closely related, one fundamental question naturally arises: Is there a general diffusion framework that can formally unify these GNNs? The answer to this question can not only deepen our understanding of the learning process of GNNs, but also may open a new door to design a broad new class of GNNs. In this paper, we propose a general diffusion equation framework with the fidelity term, which formally establishes the relationship between the diffusion process with more GNNs. Meanwhile, with this framework, we identify one characteristic of graph diffusion networks, i.e., the current neural diffusion process only corresponds to the first-order diffusion equation. However, by an experimental investigation, we show that the labels of high-order neighbors actually exhibit monophily property, which induces the similarit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36827;&#21270;&#27785;&#31215;&#27744;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#29992;&#20110;&#30740;&#31350;&#21160;&#29289;&#36866;&#24212;&#29615;&#22659;&#30340;&#26426;&#21046;&#12290;&#36825;&#31181;&#27169;&#22411;&#22522;&#20110;&#20803;&#22686;&#24378;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28436;&#21270;&#21644;&#21457;&#23637;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21033;&#29992;&#36827;&#21270;&#27785;&#31215;&#27744;&#26469;&#21152;&#36895;&#21644;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2312.06695</link><description>&lt;p&gt;
&#36827;&#21270;&#27785;&#31215;&#27744;&#29992;&#20110;&#20803;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evolving Reservoirs for Meta Reinforcement Learning. (arXiv:2312.06695v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36827;&#21270;&#27785;&#31215;&#27744;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#29992;&#20110;&#30740;&#31350;&#21160;&#29289;&#36866;&#24212;&#29615;&#22659;&#30340;&#26426;&#21046;&#12290;&#36825;&#31181;&#27169;&#22411;&#22522;&#20110;&#20803;&#22686;&#24378;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28436;&#21270;&#21644;&#21457;&#23637;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21033;&#29992;&#36827;&#21270;&#27785;&#31215;&#27744;&#26469;&#21152;&#36895;&#21644;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#22312;&#20854;&#19968;&#29983;&#20013;&#32463;&#24120;&#23637;&#31034;&#20986;&#23545;&#29615;&#22659;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#24418;&#24577;&#21644;&#31070;&#32463;&#32467;&#26500;&#30340;&#28436;&#21270;&#12290;&#36825;&#20123;&#32467;&#26500;&#25429;&#25417;&#21040;&#20102;&#20195;&#38469;&#20043;&#38388;&#20849;&#20139;&#30340;&#29615;&#22659;&#29305;&#24449;&#65292;&#20197;&#21152;&#36895;&#21644;&#24341;&#23548;&#19968;&#29983;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#30740;&#31350;&#23454;&#29616;&#36825;&#19968;&#36807;&#31243;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20803;&#22686;&#24378;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#20316;&#20026;&#28436;&#21270;&#21644;&#21457;&#23637;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#22411;&#12290;&#22312;&#28436;&#21270;&#23610;&#24230;&#19978;&#65292;&#25105;&#20204;&#28436;&#21270;&#27785;&#31215;&#27744;&#65292;&#36825;&#26159;&#19968;&#26063;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#19982;&#24120;&#35268;&#32593;&#32476;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#20248;&#21270;&#30340;&#19981;&#26159;&#31361;&#35302;&#26435;&#37325;&#65292;&#32780;&#26159;&#25511;&#21046;&#32467;&#26524;&#32593;&#32476;&#26550;&#26500;&#30340;&#23439;&#35266;&#32423;&#21035;&#23646;&#24615;&#30340;&#36229;&#21442;&#25968;&#12290;&#22312;&#21457;&#23637;&#23610;&#24230;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#36827;&#21270;&#27785;&#31215;&#27744;&#26469;&#20419;&#36827;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#34892;&#20026;&#31574;&#30053;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#65292;&#27785;&#31215;&#27744;&#32534;&#30721;&#29615;&#22659;&#30340;&#20449;&#24687;&#20197;&#20248;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Animals often demonstrate a remarkable ability to adapt to their environments during their lifetime. They do so partly due to the evolution of morphological and neural structures. These structures capture features of environments shared between generations to bias and speed up lifetime learning. In this work, we propose a computational model for studying a mechanism that can enable such a process. We adopt a computational framework based on meta reinforcement learning as a model of the interplay between evolution and development. At the evolutionary scale, we evolve reservoirs, a family of recurrent neural networks that differ from conventional networks in that one optimizes not the synaptic weights, but hyperparameters controlling macro-level properties of the resulting network architecture. At the developmental scale, we employ these evolved reservoirs to facilitate the learning of a behavioral policy through Reinforcement Learning (RL). Within an RL agent, a reservoir encodes the en
&lt;/p&gt;</description></item><item><title>AesFA&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#39057;&#29575;&#20998;&#35299;&#22270;&#20687;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#24320;&#32654;&#23398;&#39118;&#26684;&#65292;&#25490;&#38500;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#24341;&#20837;&#20102;&#23545;&#27604;&#25439;&#22833;&#20197;&#25552;&#39640;&#39118;&#26684;&#21270;&#36136;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;AesFA&#22312;stylization quality&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#24555;&#36895;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2312.05928</link><description>&lt;p&gt;
AesFA:&#19968;&#31181;&#32654;&#23398;&#29305;&#24449;&#24863;&#30693;&#30340;&#20219;&#24847;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer. (arXiv:2312.05928v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05928
&lt;/p&gt;
&lt;p&gt;
AesFA&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#39057;&#29575;&#20998;&#35299;&#22270;&#20687;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#24320;&#32654;&#23398;&#39118;&#26684;&#65292;&#25490;&#38500;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#24341;&#20837;&#20102;&#23545;&#27604;&#25439;&#22833;&#20197;&#25552;&#39640;&#39118;&#26684;&#21270;&#36136;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;AesFA&#22312;stylization quality&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#24555;&#36895;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#24555;&#36895;&#36827;&#23637;&#21644;&#25913;&#36827;&#65292;&#29616;&#26377;&#30340;NST&#26041;&#27861;&#35201;&#20040;&#22312;&#26377;&#25928;&#36716;&#31227;&#39118;&#26684;&#26102;&#24456;&#38590;&#20445;&#30041;&#32654;&#23398;&#20449;&#24687;&#65292;&#35201;&#20040;&#22312;&#29305;&#24449;&#35299;&#32544;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30001;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#32780;&#23384;&#22312;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#20302;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20294;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;AesFA -- &#32654;&#23398;&#29305;&#24449;&#24863;&#30693;&#30340;NST&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#39057;&#29575;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#35299;&#65292;&#20197;&#26356;&#22909;&#22320;&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#35299;&#24320;&#32654;&#23398;&#39118;&#26684;&#65292;&#21516;&#26102;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#23436;&#20840;&#21462;&#28040;&#20102;&#25512;&#26029;&#26102;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#39640;&#32593;&#32476;&#25552;&#21462;&#26356;&#21152;&#29420;&#29305;&#30340;&#34920;&#31034;&#21644;&#36827;&#19968;&#27493;&#22686;&#24378;&#39118;&#26684;&#21270;&#36136;&#37327;&#30340;&#33021;&#21147;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32654;&#23398;&#29305;&#24449;&#65306;&#23545;&#27604;&#25439;&#22833;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22312;&#39118;&#26684;&#21270;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#26368;&#26032;&#30340;NST&#26041;&#27861;&#65292;&#32780;&#19988;&#23454;&#29616;&#20102;&#24555;&#36895;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural style transfer (NST) has evolved significantly in recent years. Yet, despite its rapid progress and advancement, existing NST methods either struggle to transfer aesthetic information from a style effectively or suffer from high computational costs and inefficiencies in feature disentanglement due to using pre-trained models. This work proposes a lightweight but effective model, AesFA -- Aesthetic Feature-Aware NST. The primary idea is to decompose the image via its frequencies to better disentangle aesthetic styles from the reference image while training the entire model in an end-to-end manner to exclude pre-trained models at inference completely. To improve the network's ability to extract more distinct representations and further enhance the stylization quality, this work introduces a new aesthetic feature: contrastive loss. Extensive experiments and ablations show the approach not only outperforms recent NST methods in terms of stylization quality, but it also achieves fast
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#21305;&#37197;&#19979;&#30028;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.04027</link><description>&lt;p&gt;
&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
The sample complexity of multi-distribution learning. (arXiv:2312.04027v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#21305;&#37197;&#19979;&#30028;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20998;&#24067;&#23398;&#20064;&#23558;&#32463;&#20856;&#30340;PAC&#23398;&#20064;&#25512;&#24191;&#21040;&#22788;&#29702;&#26469;&#33258;&#22810;&#20010;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#32473;&#23450;&#19968;&#32452;$k$&#20010;&#25968;&#25454;&#20998;&#24067;&#21644;&#19968;&#20010;VC&#32500;&#24230;&#20026;$d$&#30340;&#20551;&#35774;&#31867;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#20551;&#35774;&#65292;&#20351;&#24471;&#22312;$k$&#20010;&#20998;&#24067;&#19978;&#30340;&#26368;&#22823;&#24635;&#20307;&#25439;&#22833;&#26368;&#23567;&#65292;&#35823;&#24046;&#19981;&#36229;&#36807;$\epsilon$&#12290;&#26412;&#25991;&#36890;&#36807;&#32473;&#20986;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#31639;&#27861;$\widetilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$&#26469;&#35299;&#20915;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;&#36825;&#20010;&#32467;&#26524;&#19982;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#35299;&#20915;&#20102;Awasthi&#12289;Haghtalab&#21644;Zhao&#22312;COLT 2023&#20013;&#25552;&#20986;&#30340;&#24320;&#25918;&#38382;&#39064; [AHZ23]&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-distribution learning generalizes the classic PAC learning to handle data coming from multiple distributions. Given a set of $k$ data distributions and a hypothesis class of VC dimension $d$, the goal is to learn a hypothesis that minimizes the maximum population loss over $k$ distributions, up to $\epsilon$ additive error. In this paper, we settle the sample complexity of multi-distribution learning by giving an algorithm of sample complexity $\widetilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$. This matches the lower bound up to sub-polynomial factor and resolves the COLT 2023 open problem of Awasthi, Haghtalab and Zhao [AHZ23].
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#36923;&#36753;&#32422;&#26463;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20266;&#35821;&#20041;&#25439;&#22833;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#36755;&#20986;&#30340;&#23616;&#37096;&#36817;&#20284;&#19978;&#20248;&#21270;&#32422;&#26463;&#30340;&#20284;&#28982;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.03905</link><description>&lt;p&gt;
&#20855;&#26377;&#36923;&#36753;&#32422;&#26463;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20266;&#35821;&#20041;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints. (arXiv:2312.03905v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#36923;&#36753;&#32422;&#26463;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20266;&#35821;&#20041;&#25439;&#22833;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#36755;&#20986;&#30340;&#23616;&#37096;&#36817;&#20284;&#19978;&#20248;&#21270;&#32422;&#26463;&#30340;&#20284;&#28982;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#65288;neuro-symbolic AI&#65289;&#22635;&#34917;&#20102;&#32431;&#31526;&#21495;&#21644;&#31070;&#32463;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#36825;&#36890;&#24120;&#38656;&#35201;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#20998;&#24067;&#26041;&#38754;&#26368;&#22823;&#21270;&#23545;&#31526;&#21495;&#32422;&#26463;&#30340;&#20284;&#28982;&#12290;&#36825;&#20123;&#36755;&#20986;&#20998;&#24067;&#36890;&#24120;&#34987;&#20551;&#35774;&#20026;&#23436;&#20840;&#22240;&#23376;&#21270;&#30340;&#12290;&#36825;&#38480;&#21046;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#22312;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#33258;&#22238;&#24402;&#20998;&#24067;&#65288;&#20363;&#22914;transformers&#65289;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#26679;&#30340;&#20998;&#24067;&#19979;&#65292;&#29978;&#33267;&#31616;&#21333;&#32422;&#26463;&#30340;&#27010;&#29575;&#20284;&#28982;&#30340;&#35745;&#31639;&#26159;#P-hard&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#19981;&#26159;&#35797;&#22270;&#23558;&#32422;&#26463;&#24378;&#21152;&#22312;&#25972;&#20010;&#36755;&#20986;&#20998;&#24067;&#19978;&#65292;&#32780;&#26159;&#22312;&#20854;&#38543;&#26426;&#30340;&#23616;&#37096;&#36817;&#20284;&#19978;&#36825;&#26679;&#20570;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#25105;&#20204;&#22312;&#20197;&#27169;&#22411;&#26679;&#26412;&#20026;&#20013;&#24515;&#30340;&#22522;&#20110;&#20266;&#20284;&#28982;&#30340;&#36817;&#20284;&#20013;&#20248;&#21270;&#32422;&#26463;&#30340;&#20284;&#28982;&#12290;&#25105;&#20204;&#30340;&#36817;&#20284;&#26159;&#22240;&#23376;&#21270;&#30340;&#65292;&#21487;&#20197;&#37325;&#29992;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#26159;&#39640;&#25928;&#35745;&#31639;&#31070;&#32463;&#31526;&#21495;&#25439;&#22833;&#30340;&#20027;&#35201;&#21407;&#21017;&#12290;&#27492;&#22806;&#65292;&#23427;&#26159;&#19968;&#20010;&#23616;&#37096;&#30340;&#65292;&#39640;&#20445;&#30495;&#24230;&#30340;&#20284;&#28982;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic AI bridges the gap between purely symbolic and neural approaches to learning. This often requires maximizing the likelihood of a symbolic constraint w.r.t the neural network's output distribution. Such output distributions are typically assumed to be fully-factorized. This limits the applicability of neuro-symbolic learning to the more expressive autoregressive distributions, e.g., transformers. Under such distributions, computing the likelihood of even simple constraints is #P-hard. Instead of attempting to enforce the constraint on the entire output distribution, we propose to do so on a random, local approximation thereof. More precisely, we optimize the likelihood of the constraint under a pseudolikelihood-based approximation centered around a model sample. Our approximation is factorized, allowing the reuse of solutions to sub-problems, a main tenet for efficiently computing neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of the likelihoo
&lt;/p&gt;</description></item><item><title>MatterGen&#26159;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#26080;&#26426;&#26448;&#26009;&#35774;&#35745;&#20013;&#33021;&#22815;&#29983;&#25104;&#31283;&#23450;&#22810;&#26679;&#30340;&#26448;&#26009;&#65292;&#24182;&#36890;&#36807;&#36866;&#37197;&#22120;&#27169;&#22359;&#36827;&#34892;&#24494;&#35843;&#20197;&#28385;&#36275;&#24191;&#27867;&#30340;&#24615;&#36136;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2312.03687</link><description>&lt;p&gt;
MatterGen: &#26080;&#26426;&#26448;&#26009;&#35774;&#35745;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MatterGen: a generative model for inorganic materials design. (arXiv:2312.03687v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03687
&lt;/p&gt;
&lt;p&gt;
MatterGen&#26159;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#26080;&#26426;&#26448;&#26009;&#35774;&#35745;&#20013;&#33021;&#22815;&#29983;&#25104;&#31283;&#23450;&#22810;&#26679;&#30340;&#26448;&#26009;&#65292;&#24182;&#36890;&#36807;&#36866;&#37197;&#22120;&#27169;&#22359;&#36827;&#34892;&#24494;&#35843;&#20197;&#28385;&#36275;&#24191;&#27867;&#30340;&#24615;&#36136;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33021;&#28304;&#23384;&#20648;&#12289;&#20652;&#21270;&#21644;&#30899;&#25429;&#38598;&#31561;&#39046;&#22495;&#65292;&#35774;&#35745;&#20855;&#26377;&#26399;&#26395;&#24615;&#33021;&#30340;&#21151;&#33021;&#26448;&#26009;&#23545;&#25512;&#21160;&#25216;&#26415;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#20840;&#26032;&#30340;&#26448;&#26009;&#65292;&#28385;&#36275;&#29305;&#23450;&#24615;&#36136;&#32422;&#26463;&#65292;&#20026;&#26448;&#26009;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#25552;&#20986;&#31283;&#23450;&#26230;&#20307;&#30340;&#25104;&#21151;&#29575;&#26041;&#38754;&#36739;&#20302;&#65292;&#25110;&#32773;&#21482;&#33021;&#28385;&#36275;&#38750;&#24120;&#26377;&#38480;&#30340;&#24615;&#36136;&#32422;&#26463;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;MatterGen&#65292;&#19968;&#20010;&#33021;&#22815;&#22312;&#21608;&#26399;&#34920;&#19978;&#29983;&#25104;&#31283;&#23450;&#22810;&#26679;&#30340;&#26080;&#26426;&#26448;&#26009;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#29983;&#25104;&#36807;&#31243;&#65292;&#28385;&#36275;&#24191;&#27867;&#30340;&#24615;&#36136;&#32422;&#26463;&#33539;&#22260;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#21407;&#23376;&#31867;&#22411;&#12289;&#22352;&#26631;&#21644;&#21608;&#26399;&#26230;&#26684;&#26469;&#20135;&#29983;&#26230;&#20307;&#32467;&#26500;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#20197;&#20415;&#36890;&#36807;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#23545;&#20219;&#24847;&#32473;&#23450;&#30340;&#24615;&#36136;&#32422;&#26463;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design of functional materials with desired properties is essential in driving technological advances in areas like energy storage, catalysis, and carbon capture. Generative models provide a new paradigm for materials design by directly generating entirely novel materials given desired property constraints. Despite recent progress, current generative models have low success rate in proposing stable crystals, or can only satisfy a very limited set of property constraints. Here, we present MatterGen, a model that generates stable, diverse inorganic materials across the periodic table and can further be fine-tuned to steer the generation towards a broad range of property constraints. To enable this, we introduce a new diffusion-based generative process that produces crystalline structures by gradually refining atom types, coordinates, and the periodic lattice. We further introduce adapter modules to enable fine-tuning towards any given property constraints with a labeled dataset. Comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#36890;&#20449;&#31574;&#30053;&#65292;&#26681;&#25454;&#32593;&#32476;&#37197;&#32622;&#33258;&#21160;&#20999;&#25442;&#20351;&#29992;Allgather&#65288;AG&#65289;&#25110;Allreduce&#65288;AR&#65289;&#65292;&#20197;&#25552;&#39640;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#24182;&#34892;&#25928;&#29575;&#21644;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2312.02493</link><description>&lt;p&gt;
&#23545;&#20110;&#19981;&#21487;&#39044;&#30693;&#30340;&#32593;&#32476;&#30340;&#26368;&#20248;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#28789;&#27963;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Flexible Communication for Optimal Distributed Learning over Unpredictable Networks. (arXiv:2312.02493v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#36890;&#20449;&#31574;&#30053;&#65292;&#26681;&#25454;&#32593;&#32476;&#37197;&#32622;&#33258;&#21160;&#20999;&#25442;&#20351;&#29992;Allgather&#65288;AG&#65289;&#25110;Allreduce&#65288;AR&#65289;&#65292;&#20197;&#25552;&#39640;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#24182;&#34892;&#25928;&#29575;&#21644;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#21387;&#32553;&#36890;&#36807;&#21457;&#36865;&#26356;&#23569;&#30340;&#20540;&#21644;&#23545;&#24212;&#30340;&#32034;&#24341;&#26469;&#20943;&#36731;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26114;&#36149;&#36890;&#20449;&#65292;&#36890;&#24120;&#36890;&#36807;Allgather&#65288;AG&#65289;&#26469;&#23454;&#29616;&#12290;&#22312;&#39640;&#21387;&#32553;&#27604;&#65288;CR&#65289;&#19979;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#23454;&#29616;&#19982;DenseSGD&#30456;&#21516;&#30340;&#39640;&#31934;&#24230;&#65292;&#20294;&#30001;&#20110;&#39640;&#36890;&#20449;&#25104;&#26412;&#65288;&#21363;&#24182;&#34892;&#25928;&#29575;&#65289;&#65292;&#24182;&#34892;&#25193;&#23637;&#24615;&#36739;&#20302;&#12290;&#20351;&#29992;&#36739;&#20302;&#30340;CR&#21487;&#20197;&#36890;&#36807;&#38477;&#20302;&#21516;&#27493;&#24320;&#38144;&#26469;&#25552;&#39640;&#24182;&#34892;&#25928;&#29575;&#65292;&#20294;&#21516;&#26102;&#20063;&#38477;&#20302;&#27169;&#22411;&#31934;&#24230;&#65288;&#21363;&#32479;&#35745;&#25928;&#29575;&#65289;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19981;&#21516;&#27169;&#22411;&#21644;CR&#33719;&#24471;&#30340;&#21152;&#36895;&#24230;&#20063;&#20250;&#22240;&#32593;&#32476;&#24310;&#36831;&#65292;&#26377;&#25928;&#24102;&#23485;&#21644;&#29992;&#20110;&#32858;&#21512;&#30340;&#38598;&#21512;&#25805;&#20316;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20687;Allreduce&#65288;AR&#65289;&#36825;&#26679;&#30340;&#38598;&#20307;&#25805;&#20316;&#19982;AG&#20132;&#25442;&#30456;&#21516;&#25968;&#37327;&#30340;&#25968;&#25454;&#30340;&#25104;&#26412;&#36739;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;AR&#20860;&#23481;&#30340;Topk&#21387;&#32553;&#22120;&#65292;&#22312;&#26576;&#20123;&#32593;&#32476;&#37197;&#32622;&#20013;&#27604;AG&#26356;&#20248;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#36890;&#20449;&#31574;&#30053;&#65292;&#26681;&#25454;&#20351;&#29992;&#21738;&#20010;&#38598;&#20307;&#25805;&#20316;&#26469;&#22312;AG&#21644;AR&#20043;&#38388;&#20999;&#25442;
&lt;/p&gt;
&lt;p&gt;
Gradient compression alleviates expensive communication in distributed deep learning by sending fewer values and its corresponding indices, typically via Allgather (AG). Training with high compression ratio (CR) achieves high accuracy like DenseSGD, but has lower parallel scaling due to high communication cost (i.e., parallel efficiency). Using lower CRs improves parallel efficiency by lowering synchronization cost, but degrades model accuracy as well (statistical efficiency). Further, speedup attained with different models and CRs also varies with network latency, effective bandwidth and collective op used for aggregation. In many cases, collectives like Allreduce (AR) have lower cost than AG to exchange the same amount of data. In this paper, we propose an AR-compatible Topk compressor that is bandwidth-optimal and thus performs better than AG in certain network configurations. We develop a flexible communication strategy that switches between AG and AR based on which collective is o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#27468;&#35789;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#29983;&#25104;&#36866;&#21512;&#27468;&#35789;&#27468;&#26354;&#30340;&#33410;&#25293;&#35760;&#21495;&#65292;&#24182;&#25581;&#31034;&#28508;&#22312;&#30340;&#33410;&#22863;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2311.15480</link><description>&lt;p&gt;
&#20351;&#29992;&#27468;&#35789;&#33258;&#21160;&#30830;&#23450;&#26032;&#26354;&#35889;&#30340;&#33410;&#25293;&#35760;&#21495;
&lt;/p&gt;
&lt;p&gt;
Automatic Time Signature Determination for New Scores Using Lyrics for Latent Rhythmic Structure. (arXiv:2311.15480v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#27468;&#35789;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#29983;&#25104;&#36866;&#21512;&#27468;&#35789;&#27468;&#26354;&#30340;&#33410;&#25293;&#35760;&#21495;&#65292;&#24182;&#25581;&#31034;&#28508;&#22312;&#30340;&#33410;&#22863;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;(AIGC)&#30340;&#20852;&#36259;&#24613;&#21095;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#23545;&#38899;&#20048;&#32452;&#25104;&#37096;&#20998;&#22914;&#33410;&#25293;&#35760;&#21495;&#36827;&#34892;&#36275;&#22815;&#30340;&#30740;&#31350;&#65292;&#20197;&#21046;&#23450;&#26032;&#20316;&#21697;&#30340;&#31639;&#27861;&#30830;&#23450;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#27468;&#35789;&#27468;&#26354;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#24573;&#35270;&#20102;&#38899;&#20048;&#32454;&#33410;&#65292;&#32780;&#38899;&#20048;&#32454;&#33410;&#23545;&#20110;&#26500;&#24314;&#24378;&#22823;&#30340;&#26694;&#26550;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#33410;&#25293;&#35760;&#21495;&#20026;&#27468;&#26354;&#30340;&#20960;&#20046;&#25152;&#26377;&#26041;&#38754;(&#21253;&#25324;&#30701;&#35821;&#21644;&#38899;&#31526;)&#24314;&#31435;&#20102;&#22522;&#30784;&#30340;&#33410;&#22863;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#27468;&#35789;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#29983;&#25104;&#36866;&#21512;&#27468;&#35789;&#27468;&#26354;&#30340;&#33410;&#25293;&#35760;&#21495;&#65292;&#24182;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25581;&#31034;&#28508;&#22312;&#30340;&#33410;&#22863;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#31181;&#19982;&#21457;&#29616;&#27468;&#35789;&#27169;&#24335;&#21644;&#21019;&#24314;&#21516;&#26102;&#21253;&#21547;&#27468;&#35789;&#12289;&#33410;&#22863;&#21644;&#32479;&#35745;&#20449;&#24687;&#30340;&#26032;&#29305;&#24449;&#30456;&#20851;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
There has recently been a sharp increase in interest in Artificial Intelligence-Generated Content (AIGC). Despite this, musical components such as time signatures have not been studied sufficiently to form an algorithmic determination approach for new compositions, especially lyrical songs. This is likely because of the neglect of musical details, which is critical for constructing a robust framework. Specifically, time signatures establish the fundamental rhythmic structure for almost all aspects of a song, including the phrases and notes. In this paper, we propose a novel approach that only uses lyrics as input to automatically generate a fitting time signature for lyrical songs and uncover the latent rhythmic structure utilizing explainable machine learning models. In particular, we devise multiple methods that are associated with discovering lyrical patterns and creating new features that simultaneously contain lyrical, rhythmic, and statistical information. In this approach, the b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#20462;&#21098;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25688;&#35201;&#20219;&#21153;&#20013;&#20135;&#29983;&#24187;&#35273;&#30340;&#24773;&#20917;&#36739;&#21407;&#22987;&#27169;&#22411;&#35201;&#23569;&#65292;&#34920;&#29616;&#26356;&#21487;&#38752;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#31232;&#30095;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.09335</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;&#24187;&#35273;&#22312;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization. (arXiv:2311.09335v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#20462;&#21098;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25688;&#35201;&#20219;&#21153;&#20013;&#20135;&#29983;&#24187;&#35273;&#30340;&#24773;&#20917;&#36739;&#21407;&#22987;&#27169;&#22411;&#35201;&#23569;&#65292;&#34920;&#29616;&#26356;&#21487;&#38752;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#31232;&#30095;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#27169;&#22411;&#24222;&#22823;&#21644;&#26131;&#20135;&#29983;&#24187;&#35273;&#12290;&#24187;&#35273;&#26159;&#20196;&#20154;&#25285;&#24551;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38477;&#20302;&#20102;&#21487;&#38752;&#24615;&#24182;&#24341;&#21457;&#23433;&#20840;&#38382;&#39064;&#12290;&#20462;&#21098;&#26159;&#19968;&#31181;&#36890;&#36807;&#21435;&#38500;&#20887;&#20313;&#26435;&#37325;&#26469;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#31232;&#30095;&#25512;&#29702;&#30340;&#25216;&#26415;&#12290;&#20462;&#21098;&#21518;&#30340;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#19978;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#24403;&#65292;&#22240;&#27492;&#22312;&#39044;&#31639;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#25104;&#20026;&#29702;&#24819;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#20462;&#21098;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25688;&#35201;&#20013;&#20135;&#29983;&#24187;&#35273;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20116;&#20010;&#25688;&#35201;&#25968;&#25454;&#38598;&#12289;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#26041;&#27861;&#21644;&#20116;&#20010;&#32463;&#35843;&#35797;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20462;&#21098;&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#24773;&#20917;&#36739;&#21407;&#22987;&#27169;&#22411;&#35201;&#23569;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20462;&#21098;&#21518;&#30340;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20381;&#36182;&#25351;&#23548;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges: their considerable size and tendency to hallucinate. Hallucinations are concerning because they erode reliability and raise safety issues. Pruning is a technique that reduces model size by removing redundant weights, enabling more efficient sparse inference. Pruned models yield downstream task performance comparable to the original, making them ideal alternatives when operating on a limited budget. However, the effect that pruning has upon hallucinations in abstractive summarization with LLMs has yet to be explored. In this paper, we provide an extensive empirical study across five summarization datasets, two state-of-the-art pruning methods, and five instruction-tuned LLMs. Surprisingly, we find that hallucinations from pruned LLMs are less prevalent than the original models. Our analysis suggests that pruned models tend to depend more on th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#35299;&#20915;&#20855;&#26377;&#39640;&#32500;&#24230;&#21644;&#36830;&#32493;&#35266;&#27979;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#26032;&#22411;&#27010;&#29575;&#30028;&#38480;&#65292;&#33021;&#22815;&#31616;&#21270;&#35266;&#27979;&#27169;&#22411;&#24182;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.07745</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#27010;&#29575;&#20445;&#35777;&#21644;&#23454;&#36341;&#30340;&#36830;&#32493;POMDP&#35268;&#21010;&#20013;&#31616;&#21270;&#22797;&#26434;&#30340;&#35266;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Simplifying Complex Observation Models in Continuous POMDP Planning with Probabilistic Guarantees and Practice. (arXiv:2311.07745v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#35299;&#20915;&#20855;&#26377;&#39640;&#32500;&#24230;&#21644;&#36830;&#32493;&#35266;&#27979;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#26032;&#22411;&#27010;&#29575;&#30028;&#38480;&#65292;&#33021;&#22815;&#31616;&#21270;&#35266;&#27979;&#27169;&#22411;&#24182;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#20855;&#26377;&#39640;&#32500;&#24230;&#21644;&#36830;&#32493;&#35266;&#27979;&#65288;&#22914;&#30456;&#26426;&#22270;&#20687;&#65289;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDP)&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#26426;&#22120;&#20154;&#21644;&#35268;&#21010;&#38382;&#39064;&#26159;&#24517;&#38656;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#27169;&#22411;&#20316;&#20026;&#35266;&#27979;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30446;&#21069;&#22312;&#32447;&#37096;&#32626;&#26102;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35268;&#21010;&#20013;&#20351;&#29992;&#31616;&#21270;&#35266;&#27979;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#30340;&#24418;&#24335;&#21270;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#22522;&#20110;&#31616;&#21270;&#27169;&#22411;&#30340;&#32479;&#35745;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#26032;&#22411;&#27010;&#29575;&#30028;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#25512;&#24191;&#26368;&#36817;&#30340;&#31890;&#23376;&#32622;&#20449;&#24230;MDP&#38598;&#20013;&#30028;&#38480;&#30340;&#32467;&#26524;&#65292;&#23427;&#23558;&#29702;&#35770;POMDP&#20540;&#19982;&#31616;&#21270;&#27169;&#22411;&#19979;&#30340;&#23454;&#38469;&#35268;&#21010;&#20540;&#36827;&#34892;&#20102;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#21487;&#20197;&#20998;&#20026;&#31163;&#32447;&#21644;&#22312;&#32447;&#37096;&#20998;&#65292;&#24182;&#19988;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#24418;&#24335;&#21270;&#30340;&#20445;&#35777;&#65292;&#32780;&#26080;&#38656;
&lt;/p&gt;
&lt;p&gt;
Solving partially observable Markov decision processes (POMDPs) with high dimensional and continuous observations, such as camera images, is required for many real life robotics and planning problems. Recent researches suggested machine learned probabilistic models as observation models, but their use is currently too computationally expensive for online deployment. We deal with the question of what would be the implication of using simplified observation models for planning, while retaining formal guarantees on the quality of the solution. Our main contribution is a novel probabilistic bound based on a statistical total variation distance of the simplified model. We show that it bounds the theoretical POMDP value w.r.t. original model, from the empirical planned value with the simplified model, by generalizing recent results of particle-belief MDP concentration bounds. Our calculations can be separated into offline and online parts, and we arrive at formal guarantees without having to
&lt;/p&gt;</description></item><item><title>GateLoop&#26159;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#24207;&#21015;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#32473;Attention&#12290;</title><link>http://arxiv.org/abs/2311.01927</link><description>&lt;p&gt;
GateLoop: &#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01927
&lt;/p&gt;
&lt;p&gt;
GateLoop&#26159;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#24207;&#21015;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#32473;Attention&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#36882;&#24402;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#24314;&#27169;&#38271;&#24207;&#21015;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#29616;&#26377;&#27169;&#22411;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20854;&#28508;&#21147;&#12290;&#22312;&#36825;&#19968;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GateLoop&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#30784;&#24615;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#25511;&#21046;&#30340;&#29366;&#24577;&#36716;&#25442;&#26469;&#25512;&#24191;&#32447;&#24615;&#36882;&#24402;&#27169;&#22411;&#65292;&#22914;S4&#12289;S5&#12289;LRU&#21644;RetNet&#12290;&#21033;&#29992;&#36825;&#19968;&#29702;&#35770;&#36827;&#27493;&#65292;GateLoop&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#22312;&#23454;&#35777;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20302;&#25104;&#26412;&#30340;$O(l)$&#36882;&#24402;&#27169;&#24335;&#21644;&#39640;&#24230;&#20248;&#21270;&#30340;&#20851;&#32852;&#25195;&#25551;&#23454;&#29616;&#30340;&#39640;&#25928;$O(l \log_{2} l)$&#24182;&#34892;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;$O(l^2)$&#30340;&#20195;&#29702;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#23545;Transformer&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#26550;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#21521;Attention&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;&#32780;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#26080;&#20851;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on da
&lt;/p&gt;</description></item><item><title>GOPlan&#26159;&#19968;&#20010;&#20351;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#20808;&#39564;&#31574;&#30053;&#21644;&#20351;&#29992;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#65292;&#29992;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20025</link><description>&lt;p&gt;
GOPlan:&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models. (arXiv:2310.20025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20025
&lt;/p&gt;
&lt;p&gt;
GOPlan&#26159;&#19968;&#20010;&#20351;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35745;&#21010;&#30340;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#20808;&#39564;&#31574;&#30053;&#21644;&#20351;&#29992;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#65292;&#29992;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#20026;&#20174;&#22810;&#26679;&#21270;&#21644;&#22810;&#20219;&#21153;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#36890;&#29992;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#33539;&#20363;&#12290;&#23613;&#31649;&#36817;&#26399;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20027;&#23548;&#30340;&#31163;&#32447;GCRL&#26041;&#27861;&#20173;&#28982;&#21463;&#38480;&#20110;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#24212;&#23545;&#26377;&#38480;&#25968;&#25454;&#39044;&#31639;&#21644;&#26410;&#35265;&#30446;&#26631;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;Goal-conditioned Offline Planning&#65288;GOPlan&#65289;&#65292;&#21253;&#25324;&#65288;1&#65289;&#39044;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#22810;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#22810;&#27169;&#24577;&#21160;&#20316;&#20998;&#24067;&#30340;&#20808;&#39564;&#31574;&#30053;&#65307;&#65288;2&#65289;&#21033;&#29992;&#35268;&#21010;&#30340;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#20026;&#24494;&#35843;&#31574;&#30053;&#29983;&#25104;&#34394;&#26500;&#36712;&#36857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20808;&#39564;&#31574;&#30053;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#20998;&#31163;&#30340;&#24102;&#20248;&#21183;&#26435;&#37325;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20197;&#20811;&#26381;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21160;&#20316;&#30340;&#32570;&#28857;&#12290;&#20026;&#36827;&#19968;&#27493;&#20248;&#21270;&#31574;&#30053;&#65292;&#37325;&#26032;&#20998;&#26512;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34394;&#26500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods have been restricted to model-free approaches, constraining their capacity to tackle limited data budgets and unseen goal generalization. In this work, we propose a novel two-stage model-based framework, Goal-conditioned Offline Planning (GOPlan), including (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, the prior policy is based on an advantage-weighted Conditioned Generative Adversarial Networks that exhibits distinct mode separation to overcome the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36339;&#36291;&#27491;&#20132;&#21015;&#34920;&#26469;&#35299;&#20915;&#21160;&#24577;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#22312;&#32771;&#34385;&#25968;&#25454;&#28857;&#26435;&#37325;&#25110;&#20301;&#32622;&#21464;&#21270;&#26102;&#33021;&#22815;&#26377;&#25928;&#22320;&#26356;&#26032;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.18446</link><description>&lt;p&gt;
&#21160;&#24577;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#19968;&#31181;&#26032;&#22411;&#36339;&#36291;&#27491;&#20132;&#21015;&#34920;
&lt;/p&gt;
&lt;p&gt;
A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem. (arXiv:2310.18446v4 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36339;&#36291;&#27491;&#20132;&#21015;&#34920;&#26469;&#35299;&#20915;&#21160;&#24577;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#22312;&#32771;&#34385;&#25968;&#25454;&#28857;&#26435;&#37325;&#25110;&#20301;&#32622;&#21464;&#21270;&#26102;&#33021;&#22815;&#26377;&#25928;&#22320;&#26356;&#26032;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#26159;&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#24341;&#36215;&#20102;&#20248;&#21270;&#31038;&#21306;&#26497;&#22823;&#20851;&#27880;&#30340;&#19968;&#20010;&#22522;&#26412;&#20027;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#31163;&#25955;&#21160;&#24577;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65306;&#24403;&#25968;&#25454;&#28857;&#30340;&#26435;&#37325;&#25110;&#20301;&#32622;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#39640;&#25928;&#22320;&#26356;&#26032;&#26368;&#20248;&#20256;&#36755;&#26041;&#26696;&#65311;&#36825;&#20010;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#22810;&#20010;&#24212;&#29992;&#12290;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#35745;&#31639;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#26368;&#20248;&#20256;&#36755;&#25104;&#26412;&#65307;&#22914;&#26524;&#19968;&#20123;&#25968;&#25454;&#28857;&#21457;&#29983;&#20102;&#21464;&#21270;&#65292;&#25105;&#20204;&#24212;&#35813;&#37325;&#26032;&#35745;&#31639;&#39640;&#22797;&#26434;&#24230;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#36824;&#26159;&#36890;&#36807;&#19968;&#20123;&#39640;&#25928;&#30340;&#21160;&#24577;&#25968;&#25454;&#32467;&#26500;&#26469;&#26356;&#26032;&#25104;&#26412;&#65311;&#25105;&#20204;&#27880;&#24847;&#21040;&#20808;&#21069;&#24050;&#25552;&#20986;&#20102;&#20960;&#31181;&#21160;&#24577;&#26368;&#22823;&#27969;&#31639;&#27861;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23545;&#20110;&#21160;&#24577;&#26368;&#23567;&#36153;&#29992;&#27969;&#38382;&#39064;&#30340;&#30740;&#31350;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;2D&#36339;&#36291;&#27491;&#20132;&#21015;&#34920;&#65292;&#32467;&#21512;&#19968;&#20123;&#21160;&#24577;&#26641;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport is a fundamental topic that has attracted a great amount of attention from the optimization community in the past decades. In this paper, we consider an interesting discrete dynamic optimal transport problem: can we efficiently update the optimal transport plan when the weights or the locations of the data points change? This problem is naturally motivated by several applications in machine learning. For example, we often need to compute the optimal transport cost between two different data sets; if some changes happen to a few data points, should we re-compute the high complexity cost function or update the cost by some efficient dynamic data structure? We are aware that several dynamic maximum flow algorithms have been proposed before, however, the research on dynamic minimum cost flow problem is still quite limited, to the best of our knowledge. We propose a novel 2D Skip Orthogonal List together with some dynamic tree techniques. Although our algorithm is based on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#25968;&#25454;&#27745;&#26579;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#36807;15&#20010;&#28909;&#38376;&#27169;&#22411;&#23545;&#20845;&#20010;&#24120;&#35265;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#30340;&#27745;&#26579;&#20998;&#26512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25968;&#25454;&#27745;&#26579;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#27745;&#26579;&#31243;&#24230;&#19981;&#26029;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2310.17589</link><description>&lt;p&gt;
&#19968;&#20221;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#25968;&#25454;&#27745;&#26579;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
An Open Source Data Contamination Report for Large Language Models. (arXiv:2310.17589v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#25968;&#25454;&#27745;&#26579;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#36807;15&#20010;&#28909;&#38376;&#27169;&#22411;&#23545;&#20845;&#20010;&#24120;&#35265;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#30340;&#27745;&#26579;&#20998;&#26512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25968;&#25454;&#27745;&#26579;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#27745;&#26579;&#31243;&#24230;&#19981;&#26029;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#23427;&#20801;&#35768;&#27169;&#22411;&#36890;&#36807;&#35760;&#24518;&#32780;&#19981;&#26159;&#23637;&#31034;&#30495;&#27491;&#30340;&#33021;&#21147;&#26469;"&#20316;&#24330;"&#12290;&#22240;&#27492;&#65292;&#27745;&#26579;&#20998;&#26512;&#24050;&#25104;&#20026;&#21487;&#38752;&#30340;&#27169;&#22411;&#35780;&#20272;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#29992;&#20110;&#39564;&#35777;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27745;&#26579;&#20998;&#26512;&#36890;&#24120;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#32773;&#20869;&#37096;&#36827;&#34892;&#65292;&#24448;&#24448;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#23436;&#25972;&#24615;&#12290;&#26412;&#25991;&#20026;&#20845;&#20010;&#24120;&#35265;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#36229;&#36807;15&#20010;&#28909;&#38376;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35814;&#32454;&#25968;&#25454;&#27745;&#26579;&#25253;&#21578;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#27969;&#31243;&#65292;&#20351;&#31038;&#21306;&#33021;&#22815;&#23545;&#33258;&#23450;&#20041;&#25968;&#25454;&#21644;&#27169;&#22411;&#36827;&#34892;&#27745;&#26579;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#22522;&#20934;&#27979;&#35797;&#20013;&#19981;&#21516;&#30340;&#27745;&#26579;&#27700;&#24179;&#65292;&#33539;&#22260;&#20174;1%&#21040;45%&#65292;&#27745;&#26579;&#31243;&#24230;&#38543;&#26102;&#38388;&#36805;&#36895;&#22686;&#21152;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20998;&#26512;&#34920;&#26126;&#65292;&#25968;&#25454;&#27745;&#26579;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models. It allows models to "cheat" via memorisation instead of displaying true capabilities. Therefore, contamination analysis has become an crucial part of reliable model evaluation to validate results. However, existing contamination analysis is usually conducted internally by large language model developers and often lacks transparency and completeness. This paper presents an extensive data contamination report for over 15 popular large language models across six popular multiple-choice QA benchmarks. We also introduce an open-source pipeline that enables the community to perform contamination analysis on customised data and models. Our experiments reveal varying contamination levels ranging from 1\% to 45\% across benchmarks, with the contamination degree increasing rapidly over time. Performance analysis of large language models indicates that data contamination 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#29305;&#23450;&#23454;&#20363;&#19978;&#36827;&#34892;&#39640;&#25928;&#24494;&#35843;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#22810;&#34892;&#20026;&#35774;&#32622;&#21644;&#31163;&#25955;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.14526</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#23454;&#29616;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Zero Shot Learning in Restless Multi-armed Bandits. (arXiv:2310.14526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14526
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#29305;&#23450;&#23454;&#20363;&#19978;&#36827;&#34892;&#39640;&#25928;&#24494;&#35843;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#22810;&#34892;&#20026;&#35774;&#32622;&#21644;&#31163;&#25955;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#19968;&#31867;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#8212;&#8212;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;RMABs&#65289;&#65292;&#35813;&#38382;&#39064;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#22312;&#32447;&#24191;&#21578;&#21644;&#21453;&#30423;&#29454;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#20808;&#21069;&#30340;RMAB&#30740;&#31350;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#36830;&#32493;&#29366;&#24577;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#24403;&#36172;&#21338;&#26426;&#30340;&#20837;&#36873;&#21644;&#36864;&#20986;&#19981;&#26029;&#21457;&#29983;&#26102;&#65292;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PreFeRMAB&#65289;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#23545;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#24191;&#27867;RMAB&#38382;&#39064;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#27604;&#20174;&#22836;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#22320;&#23545;&#29305;&#23450;&#23454;&#20363;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#22810;&#34892;&#20026;&#35774;&#32622;&#21644;&#31163;&#25955;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;&#20026;&#20102;&#23454;&#29616;&#24555;&#36895;&#27867;&#21270;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#19968;&#31574;&#30053;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#29305;&#24449;&#20449;&#24687;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restless multi-arm bandits (RMABs), a class of resource allocation problems with broad application in areas such as healthcare, online advertising, and anti-poaching, have recently been studied from a multi-agent reinforcement learning perspective. Prior RMAB research suffers from several limitations, e.g., it fails to adequately address continuous states, and requires retraining from scratch when arms opt-in and opt-out over time, a common challenge in many real world applications. We address these limitations by developing a neural network-based pre-trained model (PreFeRMAB) that has general zero-shot ability on a wide range of previously unseen RMABs, and which can be fine-tuned on specific instances in a more sample-efficient way than retraining from scratch. Our model also accommodates general multi-action settings and discrete or continuous state spaces. To enable fast generalization, we learn a novel single policy network model that utilizes feature information and employs a tra
&lt;/p&gt;</description></item><item><title>GraphMaker&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.13833</link><description>&lt;p&gt;
GraphMaker: &#25193;&#25955;&#27169;&#22411;&#33021;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?. (arXiv:2310.13833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13833
&lt;/p&gt;
&lt;p&gt;
GraphMaker&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20855;&#26377;&#33410;&#28857;&#23646;&#24615;&#30340;&#22823;&#35268;&#27169;&#22270;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#21019;&#24314;&#19982;&#30495;&#23454;&#19990;&#30028;&#31034;&#20363;&#31867;&#20284;&#30340;&#21512;&#25104;&#12289;&#23500;&#23646;&#24615;&#22270;&#23545;&#20110;&#20849;&#20139;&#22270;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#21644;&#24320;&#21457;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#24403;&#21407;&#22987;&#25968;&#25454;&#38480;&#21046;&#34987;&#20849;&#20139;&#26102;&#12290;&#20256;&#32479;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#20123;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#27809;&#26377;&#23646;&#24615;&#21644;&#36739;&#23567;&#30340;&#20998;&#23376;&#22270;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#26041;&#38754;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#22797;&#26434;&#30340;&#23646;&#24615;-&#32467;&#26500;&#30456;&#20851;&#24615;&#21644;&#22270;&#30340;&#22823;&#35268;&#27169;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#65306;GraphMaker&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#29983;&#25104;&#36807;&#31243;&#30340;&#32452;&#21512;&#65292;&#21457;&#29616;&#24322;&#27493;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#20869;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale graphs with node attributes are increasingly common in various real-world applications. Creating synthetic, attribute-rich graphs that mirror real-world examples is crucial, especially for sharing graph data for analysis and developing learning models when original data is restricted to be shared. Traditional graph generation methods are limited in their capacity to handle these complex structures. Recent advances in diffusion models have shown potential in generating graph structures without attributes and smaller molecular graphs. However, these models face challenges in generating large attributed graphs due to the complex attribute-structure correlations and the large size of these graphs. This paper introduces a novel diffusion model, GraphMaker, specifically designed for generating large attributed graphs. We explore various combinations of node attribute and graph structure generation processes, finding that an asynchronous approach more effectively captures the intr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#35782;&#21035;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#35880;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10705</link><description>&lt;p&gt;
&#29992;&#20110;&#35782;&#21035;&#21322;&#23548;&#20307;&#26230;&#22278;&#22320;&#22270;&#20013;&#32570;&#38519;&#27169;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65306;&#19968;&#39033;&#35843;&#26597;&#12289;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations. (arXiv:2310.10705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#35782;&#21035;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#35880;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#35782;&#21035;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#23398;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;ML&#22312;&#26230;&#22278;&#32570;&#38519;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#32570;&#20047;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#26412;&#25991;&#35797;&#22270;&#24357;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#36890;&#36807;&#25972;&#21512;&#29616;&#26377;&#25991;&#29486;&#65292;&#28145;&#20837;&#20998;&#26512;&#21508;&#31181;ML&#31639;&#27861;&#22312;&#26230;&#22278;&#32570;&#38519;&#26816;&#27979;&#39046;&#22495;&#30340;&#20248;&#21183;&#12289;&#23616;&#38480;&#24615;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#23398;&#20998;&#31867;&#20307;&#31995;&#65292;&#35814;&#32454;&#20998;&#31867;&#20102;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#23376;&#25216;&#26415;&#21010;&#20998;&#12290;&#36825;&#20010;&#20998;&#31867;&#20307;&#31995;&#20174;&#24191;&#27867;&#30340;&#26041;&#27861;&#23398;&#31867;&#21035;&#24320;&#22987;&#65292;&#21040;&#20855;&#20307;&#30340;&#23376;&#25216;&#26415;&#32467;&#26463;&#12290;&#23427;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#19981;&#21516;&#31639;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#25216;&#26415;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#37319;&#29992;&#20005;&#35880;&#30340;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;&#26469;&#39564;&#35777;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous em
&lt;/p&gt;</description></item><item><title>BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07276</link><description>&lt;p&gt;
BioT5&#65306;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#36328;&#27169;&#24577;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07276
&lt;/p&gt;
&lt;p&gt;
BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#29983;&#29289;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#21033;&#29992;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#25972;&#21512;&#26469;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#29983;&#25104;&#26080;&#25928;&#30340;&#20998;&#23376;SMILES&#12289;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21033;&#29992;&#19981;&#36275;&#20197;&#21450;&#23545;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#31561;&#37327;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;BioT5&#65292;&#23427;&#36890;&#36807;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#29983;&#29289;&#23398;&#20013;&#30340;&#36328;&#27169;&#24577;&#25972;&#21512;&#12290;BioT5&#21033;&#29992;SELFIES&#36827;&#34892;100%&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#24182;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#29983;&#29289;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#29289;&#23454;&#20307;&#21608;&#22260;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;BioT5&#21306;&#20998;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20449;&#24687;&#12290;&#22312;&#24494;&#35843;&#21518;&#65292;BioT5&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#22411;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#26816;&#27979;&#22810;&#31181;&#24322;&#24120;&#65292;&#22312;&#21333;&#20010;&#22270;&#20687;&#20013;&#36741;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#12290;&#35813;&#31995;&#32479;&#23558;&#25918;&#23556;&#23398;&#24322;&#24120;&#26816;&#27979;&#38480;&#21046;&#20026;&#24515;&#33039;&#32933;&#22823;&#12289;&#32954;&#31215;&#28082;&#21644;&#23454;&#21464;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#65306;&#22270;&#20687;&#39044;&#22788;&#29702;&#12289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#24322;&#24120;&#21644;&#29983;&#25104;&#25253;&#21578;&#12290;</title><link>http://arxiv.org/abs/2310.05969</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#33258;&#21160;&#21270;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach. (arXiv:2310.05969v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05969
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#22411;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#26816;&#27979;&#22810;&#31181;&#24322;&#24120;&#65292;&#22312;&#21333;&#20010;&#22270;&#20687;&#20013;&#36741;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#12290;&#35813;&#31995;&#32479;&#23558;&#25918;&#23556;&#23398;&#24322;&#24120;&#26816;&#27979;&#38480;&#21046;&#20026;&#24515;&#33039;&#32933;&#22823;&#12289;&#32954;&#31215;&#28082;&#21644;&#23454;&#21464;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#65306;&#22270;&#20687;&#39044;&#22788;&#29702;&#12289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#24322;&#24120;&#21644;&#29983;&#25104;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38405;&#35835;&#21644;&#35299;&#35835;&#33016;&#37096;X&#20809;&#22270;&#20687;&#26159;&#22823;&#22810;&#25968;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#20363;&#34892;&#24037;&#20316;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#32463;&#39564;&#26368;&#20016;&#23500;&#30340;&#21307;&#29983;&#26469;&#35828;&#65292;&#36825;&#20173;&#28982;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#22411;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#31995;&#32479;&#65292;&#26088;&#22312;&#36741;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#21033;&#29992;&#22810;&#20010;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#26469;&#26816;&#27979;&#22810;&#31181;&#24322;&#24120;&#65292;&#27599;&#20010;&#27169;&#22411;&#36127;&#36131;&#26816;&#27979;&#19968;&#31181;&#24322;&#24120;&#65292;&#22312;&#21333;&#20010;&#22270;&#20687;&#20013;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#25918;&#23556;&#23398;&#24322;&#24120;&#26816;&#27979;&#38480;&#21046;&#20026;&#24515;&#33039;&#32933;&#22823;&#12289;&#32954;&#31215;&#28082;&#21644;&#23454;&#21464;&#12290;&#31995;&#32479;&#36890;&#36807;&#25191;&#34892;&#20197;&#19979;&#19977;&#20010;&#27493;&#39588;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#65306;&#22270;&#20687;&#39044;&#22788;&#29702;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#24322;&#24120;&#65292;&#24182;&#29983;&#25104;&#25253;&#21578;&#12290;&#22270;&#20687;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#23558;&#20854;&#32553;&#25918;&#20026;128x128&#20687;&#32032;&#24182;&#20998;&#21106;&#25104;&#19977;&#20010;&#37096;&#20998;&#26469;&#20351;&#36755;&#20837;&#26631;&#20934;&#21270;&#65292;&#20998;&#21035;&#28085;&#30422;&#19978;&#37096;&#12289;&#19979;&#37096;&#21644;&#20013;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reading and interpreting chest X-ray images is one of the most radiologist's routines. However, it still can be challenging, even for the most experienced ones. Therefore, we proposed a multi-model deep learning-based automated chest X-ray report generator system designed to assist radiologists in their work. The basic idea of the proposed system is by utilizing multi binary-classification models for detecting multi abnormalities, with each model responsible for detecting one abnormality, in a single image. In this study, we limited the radiology abnormalities detection to only cardiomegaly, lung effusion, and consolidation. The system generates a radiology report by performing the following three steps: image pre-processing, utilizing deep learning models to detect abnormalities, and producing a report. The aim of the image pre-processing step is to standardize the input by scaling it to 128x128 pixels and slicing it into three segments, which covers the upper, lower, and middle parts
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32763;&#35793;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25105;&#20204;&#25104;&#21151;&#32469;&#36807;&#20102;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;AI&#23433;&#20840;&#24615;&#20013;&#30340;&#34180;&#24369;&#29615;&#33410;&#12290;</title><link>http://arxiv.org/abs/2310.02446</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#36234;&#29425; GPT-4
&lt;/p&gt;
&lt;p&gt;
Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02446
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32763;&#35793;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25105;&#20204;&#25104;&#21151;&#32469;&#36807;&#20102;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;AI&#23433;&#20840;&#24615;&#20013;&#30340;&#34180;&#24369;&#29615;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#22521;&#35757;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32418;&#38431;&#27979;&#35797;&#26159;&#20943;&#23569;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#23558;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25104;&#21151;&#32469;&#36807;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#25581;&#31034;&#20102;&#36825;&#20123;&#23433;&#20840;&#26426;&#21046;&#30340;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#22312;AdvBenchmark&#20013;&#65292;GPT-4&#38024;&#23545;&#19981;&#23433;&#20840;&#30340;&#32763;&#35793;&#36755;&#20837;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#19988;79%&#30340;&#26102;&#38388;&#20869;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26041;&#26696;&#65292;&#20351;&#29992;&#25143;&#23454;&#29616;&#20854;&#26377;&#23475;&#30446;&#26631;&#65292;&#36825;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;&#20854;&#20182;&#39640;/&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#26174;&#33879;&#36739;&#20302;&#65292;&#36825;&#34920;&#26126;&#36328;&#35821;&#35328;&#28431;&#27934;&#20027;&#35201;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20197;&#21069;&#65292;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26377;&#38480;&#35757;&#32451;&#20027;&#35201;&#24433;&#21709;&#37027;&#20123;&#20351;&#29992;&#36825;&#20123;&#35821;&#35328;&#30340;&#20154;&#65292;&#36896;&#25104;&#25216;&#26415;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#36716;&#21464;&#65306;
&lt;/p&gt;
&lt;p&gt;
AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift:
&lt;/p&gt;</description></item><item><title>Selenite&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#33258;&#21160;&#20135;&#29983;&#20840;&#38754;&#30340;&#36873;&#39033;&#21644;&#26631;&#20934;&#27010;&#35272;&#65292;&#24110;&#21161;&#29992;&#25143;&#22312;&#38476;&#29983;&#39046;&#22495;&#36827;&#34892;&#24847;&#20041;&#24314;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.02161</link><description>&lt;p&gt;
Selenite&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20840;&#38754;&#27010;&#35272;&#30340;&#22312;&#32447;&#24847;&#20041;&#24314;&#26500;&#25903;&#26550;
&lt;/p&gt;
&lt;p&gt;
Selenite: Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models. (arXiv:2310.02161v4 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02161
&lt;/p&gt;
&lt;p&gt;
Selenite&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#33258;&#21160;&#20135;&#29983;&#20840;&#38754;&#30340;&#36873;&#39033;&#21644;&#26631;&#20934;&#27010;&#35272;&#65292;&#24110;&#21161;&#29992;&#25143;&#22312;&#38476;&#29983;&#39046;&#22495;&#36827;&#34892;&#24847;&#20041;&#24314;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38476;&#29983;&#39046;&#22495;&#36827;&#34892;&#24847;&#20041;&#24314;&#26500;&#21487;&#33021;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#35201;&#27714;&#29992;&#25143;&#32791;&#36153;&#22823;&#37327;&#21162;&#21147;&#27604;&#36739;&#19981;&#21516;&#36873;&#39033;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#30340;&#24046;&#24322;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#21644;&#25105;&#20204;&#30340;&#24418;&#24335;&#21270;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#20204;&#38405;&#35835;&#20449;&#24687;&#31354;&#38388;&#30340;&#27010;&#35272;&#21487;&#20197;&#33719;&#30410;&#65292;&#20854;&#20013;&#21253;&#25324;&#20043;&#21069;&#20854;&#20182;&#20154;&#21457;&#29616;&#26377;&#29992;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24847;&#20041;&#24314;&#26500;&#24037;&#20855;&#38754;&#20020;&#30528;&#8220;&#20919;&#21551;&#21160;&#8221;&#38382;&#39064;&#65292;&#19981;&#20165;&#38656;&#35201;&#20808;&#21069;&#29992;&#25143;&#30340;&#22823;&#37327;&#36755;&#20837;&#26469;&#29983;&#25104;&#21644;&#20998;&#20139;&#36825;&#20123;&#27010;&#35272;&#65292;&#32780;&#19988;&#36825;&#20123;&#27010;&#35272;&#21487;&#33021;&#20063;&#20250;&#34987;&#35777;&#26126;&#26159;&#26377;&#20559;&#35265;&#21644;&#19981;&#23436;&#25972;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;Selenite&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#26426;&#21644;&#30693;&#35782;&#26816;&#32034;&#22120;&#65292;&#33258;&#21160;&#20135;&#29983;&#20840;&#38754;&#30340;&#36873;&#39033;&#21644;&#26631;&#20934;&#27010;&#35272;&#65292;&#20197;&#21551;&#21160;&#29992;&#25143;&#30340;&#24847;&#20041;&#24314;&#26500;&#36807;&#31243;&#12290;&#38543;&#21518;&#65292;Selenite&#36824;&#20250;&#38543;&#30528;&#29992;&#25143;&#30340;&#20351;&#29992;&#32780;&#36827;&#34892;&#36866;&#24212;&#65292;&#24110;&#21161;&#29992;&#25143;&#20197;&#31995;&#32479;&#32780;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#25214;&#21040;&#12289;&#38405;&#35835;&#21644;&#23548;&#33322;&#38476;&#29983;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sensemaking in unfamiliar domains can be challenging, demanding considerable user effort to compare different options with respect to various criteria. Prior research and our formative study found that people would benefit from reading an overview of an information space upfront, including the criteria others previously found useful. However, existing sensemaking tools struggle with the "cold-start" problem -- it not only requires significant input from previous users to generate and share these overviews, but such overviews may also turn out to be biased and incomplete. In this work, we introduce a novel system, Selenite, which leverages Large Language Models (LLMs) as reasoning machines and knowledge retrievers to automatically produce a comprehensive overview of options and criteria to jumpstart users' sensemaking processes. Subsequently, Selenite also adapts as people use it, helping users find, read, and navigate unfamiliar information in a systematic yet personalized manner. Thro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;Time-LLM&#65292;&#19968;&#20010;&#37325;&#26032;&#32534;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#19968;&#33324;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.01728</link><description>&lt;p&gt;
Time-LLM: &#36890;&#36807;&#37325;&#26032;&#32534;&#31243;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. (arXiv:2310.01728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01728
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;Time-LLM&#65292;&#19968;&#20010;&#37325;&#26032;&#32534;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#19968;&#33324;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#35768;&#22810;&#23454;&#38469;&#21160;&#24577;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#19981;&#21516;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#22823;&#22411;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#32780;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#19987;&#38376;&#21270;&#30340;&#65292;&#38656;&#35201;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#24212;&#29992;&#35774;&#35745;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;NLP&#21644;CV&#39046;&#22495;&#20013;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#21457;&#23637;&#21463;&#21040;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#30340;&#24207;&#21015;&#26631;&#35760;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#20197;&#21033;&#29992;&#36825;&#20123;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Time-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#26032;&#32534;&#31243;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#37325;&#29992;LLMs&#26469;&#36827;&#34892;&#19968;&#33324;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#25345;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#20174;&#26059;&#24459;&#20013;&#29983;&#25104;&#38899;&#33410;&#32423;&#27468;&#35789;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#21644;&#29983;&#25104;&#22120;&#32593;&#32476;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#25506;&#32034;ChatGPT&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21450;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#29983;&#25104;&#27468;&#35789;&#30340;&#36830;&#36143;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00863</link><description>&lt;p&gt;
&#20174;&#26059;&#24459;&#20013;&#21033;&#29992;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#38899;&#33410;&#32423;&#27468;&#35789;
&lt;/p&gt;
&lt;p&gt;
Syllable-level lyrics generation from melody exploiting character-level language model. (arXiv:2310.00863v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00863
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#20174;&#26059;&#24459;&#20013;&#29983;&#25104;&#38899;&#33410;&#32423;&#27468;&#35789;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#21644;&#29983;&#25104;&#22120;&#32593;&#32476;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#25506;&#32034;ChatGPT&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21450;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#29983;&#25104;&#27468;&#35789;&#30340;&#36830;&#36143;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#19982;&#20276;&#22863;&#26059;&#24459;&#32039;&#23494;&#30456;&#20851;&#30340;&#27468;&#35789;&#28041;&#21450;&#24314;&#31435;&#38899;&#20048;&#38899;&#31526;&#19982;&#27468;&#35789;&#38899;&#33410;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#20010;&#36807;&#31243;&#38656;&#35201;&#23545;&#38899;&#33410;&#32423;&#12289;&#35789;&#32423;&#21644;&#21477;&#32423;&#35821;&#20041;&#24847;&#20041;&#19978;&#30340;&#38899;&#20048;&#32422;&#26463;&#21644;&#35821;&#20041;&#27169;&#24335;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#20844;&#24320;&#30340;&#38899;&#33410;&#32423;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#19981;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#20197;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38899;&#33410;&#32423;&#27468;&#35789;&#29983;&#25104;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#30693;&#35782;&#34701;&#20837;&#38899;&#33410;&#32423;Transformer&#29983;&#25104;&#22120;&#32593;&#32476;&#30340;&#26463;&#25628;&#32034;&#36807;&#31243;&#20013;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25506;&#32034;&#22522;&#20110;ChatGPT&#30340;&#29983;&#25104;&#27468;&#35789;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21450;&#20154;&#24037;&#20027;&#35266;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#29983;&#25104;&#27468;&#35789;&#30340;&#36830;&#36143;&#24615;&#21644;&#27491;&#30830;&#24615;&#65292;&#28040;&#38500;&#20102;&#35757;&#32451;&#26114;&#36149;&#30340;&#26032;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of lyrics tightly connected to accompanying melodies involves establishing a mapping between musical notes and syllables of lyrics. This process requires a deep understanding of music constraints and semantic patterns at syllable-level, word-level, and sentence-level semantic meanings. However, pre-trained language models specifically designed at the syllable level are publicly unavailable. To solve these challenging issues, we propose to exploit fine-tuning character-level language models for syllable-level lyrics generation from symbolic melody. In particular, our method endeavors to incorporate linguistic knowledge of the language model into the beam search process of a syllable-level Transformer generator network. Additionally, by exploring ChatGPT-based evaluation for generated lyrics, along with human subjective evaluation, we demonstrate that our approach enhances the coherence and correctness of the generated lyrics, eliminating the need to train expensive new la
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16742</link><description>&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#39118;&#38505;&#30340;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#23545;184&#26465;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#20986;&#20102;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#65292;&#23588;&#20854;&#26159;2&#22411;&#31958;&#23615;&#30149;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#19982;&#31958;&#23615;&#30149;&#30456;&#20851;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#20854;&#24182;&#21457;&#30151;&#30340;&#21457;&#23637;&#12290;&#31958;&#23615;&#30149;&#32958;&#30149;&#26159;&#31958;&#23615;&#30149;&#30340;&#19968;&#31181;&#24930;&#24615;&#24182;&#21457;&#30151;&#65292;&#19981;&#21033;&#22320;&#24433;&#21709;&#32958;&#33039;&#65292;&#23548;&#33268;&#32958;&#33039;&#25439;&#20260;&#12290;&#35786;&#26029;&#31958;&#23615;&#30149;&#32958;&#30149;&#28041;&#21450;&#32771;&#34385;&#21508;&#31181;&#26631;&#20934;&#20043;&#19968;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#30340;&#30149;&#29702;&#23398;&#30149;&#29702;&#23398;&#25968;&#37327;&#65292;&#31216;&#20026;&#30333;&#34507;&#30333;&#23615;&#12290;&#22240;&#27492;&#65292;&#23545;&#31958;&#23615;&#30149;&#24739;&#32773;&#23615;&#28082;&#20013;&#30333;&#34507;&#30333;&#23615;&#30340;&#26089;&#26399;&#39044;&#27979;&#20855;&#26377;&#21450;&#26102;&#39044;&#38450;&#25514;&#26045;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;2&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#24739;&#26377;&#30333;&#34507;&#30333;&#23615;&#30340;&#39118;&#38505;&#12290;&#25152;&#36873;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21253;&#25324;&#26420;&#32032;&#36125;&#21494;&#26031;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;AdaBoost&#65292;XGBoost&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12290;&#25105;&#20204;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#21253;&#25324;184&#26465;&#31958;&#23615;&#30149;&#24182;&#21457;&#30151;&#39118;&#38505;&#22240;&#32032;&#30340;&#26465;&#30446;&#34987;&#29992;&#26469;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diabetes, especially T2DM, continues to be a significant health problem. One of the major concerns associated with diabetes is the development of its complications. Diabetic nephropathy, one of the chronic complication of diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing diabetic nephropathy involves considering various criteria, one of which is the presence of a pathologically significant quantity of albumin in urine, known as albuminuria. Thus, early prediction of albuminuria in diabetic patients holds the potential for timely preventive measures. This study aimed to develop a supervised learning model to predict the risk of developing albuminuria in T2DM patients. The selected supervised learning algorithms included Na\"ive Bayes, Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost, and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries of diabetes complications risk factors, was used to train the algorithm
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#24403;&#28857;&#20987;&#25968;&#25454;&#19981;&#33021;&#23436;&#20840;&#25311;&#21512;&#26102;&#65292;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#22270;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.15560</link><description>&lt;p&gt;
&#35782;&#21035;&#24615;&#24456;&#37325;&#35201;&#65306;&#25581;&#31034;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#38544;&#34255;&#30340;&#21487;&#24674;&#22797;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank. (arXiv:2309.15560v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15560
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#22312;&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#24403;&#28857;&#20987;&#25968;&#25454;&#19981;&#33021;&#23436;&#20840;&#25311;&#21512;&#26102;&#65292;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#24615;&#22270;&#27169;&#22411;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#23398;&#20064;&#25490;&#21517;(Unbiased Learning to Rank, ULTR)&#22312;&#20174;&#26377;&#20559;&#28857;&#20987;&#26085;&#24535;&#35757;&#32451;&#26080;&#20559;&#25490;&#21517;&#27169;&#22411;&#30340;&#29616;&#20195;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20851;&#38190;&#22312;&#20110;&#26126;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#34892;&#20026;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#22522;&#20110;&#26816;&#39564;&#20551;&#35774;&#23545;&#28857;&#20987;&#25968;&#25454;&#36827;&#34892;&#25311;&#21512;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#21482;&#35201;&#28857;&#20987;&#23436;&#20840;&#25311;&#21512;&#65292;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#21487;&#20197;&#24674;&#22797;&#20986;&#30495;&#23454;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#24182;&#38750;&#24635;&#26159;&#33021;&#22815;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20174;&#32780;&#23548;&#33268;&#25490;&#21517;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22238;&#31572;&#30495;&#23454;&#30456;&#20851;&#24615;&#26159;&#21542;&#33021;&#22815;&#20174;&#28857;&#20987;&#25968;&#25454;&#24674;&#22797;&#20986;&#26469;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;ULTR&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#19968;&#20010;&#25490;&#21517;&#27169;&#22411;&#23450;&#20041;&#20026;&#21487;&#35782;&#21035;&#30340;&#65292;&#22914;&#26524;&#23427;&#21487;&#20197;&#24674;&#22797;&#20986;&#30495;&#23454;&#30456;&#20851;&#24615;&#65292;&#26368;&#22810;&#21482;&#26377;&#19968;&#20010;&#32553;&#25918;&#21464;&#25442;&#65292;&#36825;&#23545;&#20110;&#25104;&#23545;&#25490;&#21517;&#30446;&#26631;&#26469;&#35828;&#24050;&#36275;&#22815;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#31561;&#20215;&#30340;&#21487;&#35782;&#21035;&#26465;&#20214;&#65292;&#21487;&#20197;&#26032;&#39062;&#22320;&#34920;&#36798;&#20026;&#19968;&#20010;&#22270;&#36830;&#36890;&#24615;&#27979;&#35797;&#38382;&#39064;&#65306;&#24403;&#19988;&#20165;&#24403;&#19968;&#20010;&#22270;&#65288;&#21363;&#21487;&#35782;&#21035;&#24615;&#22270;&#65289;&#36830;&#36890;&#26102;&#65292;&#35813;&#25490;&#21517;&#27169;&#22411;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Unbiased Learning to Rank (ULTR) is widespread in modern systems for training unbiased ranking models from biased click logs. The key is to explicitly model a generation process for user behavior and fit click data based on examination hypothesis. Previous research found empirically that the true latent relevance can be recovered in most cases as long as the clicks are perfectly fitted. However, we demonstrate that this is not always achievable, resulting in a significant reduction in ranking performance. In this work, we aim to answer if or when the true relevance can be recovered from click data, which is a foundation issue for ULTR field. We first define a ranking model as identifiable if it can recover the true relevance up to a scaling transformation, which is enough for pairwise ranking objective. Then we explore an equivalent condition for identifiability that can be novely expressed as a graph connectivity test problem: if and only if a graph (namely identifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;TVLARS&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21487;&#37197;&#32622;&#30340;&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#27604;LARS&#21644;LAMB&#37117;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.14053</link><description>&lt;p&gt;
&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#25209;&#37327;&#35757;&#32451;&#27867;&#21270;&#24615;&#33021;&#30340;LARS&#20877;&#23457;&#35270;
&lt;/p&gt;
&lt;p&gt;
Revisiting LARS for Large Batch Training Generalization of Neural Networks. (arXiv:2309.14053v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;TVLARS&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21487;&#37197;&#32622;&#30340;&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#27604;LARS&#21644;LAMB&#37117;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20351;&#29992;&#36880;&#23618;&#33258;&#36866;&#24212;&#32553;&#25918;&#27604;(LARS)&#26469;&#25506;&#32034;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;&#20855;&#26377;&#28909;&#36523;&#38454;&#27573;&#30340;LARS&#31639;&#27861;&#30001;&#20110;&#20887;&#20313;&#30340;&#27604;&#20363;&#32553;&#25918;&#23548;&#33268;&#22312;&#26089;&#26399;&#38519;&#20837;&#23574;&#38160;&#30340;&#26497;&#23567;&#21270;&#22120;&#12290;&#27492;&#22806;&#65292;&#21518;&#26399;&#22266;&#23450;&#30340;&#38497;&#23789;&#19979;&#38477;&#38480;&#21046;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#36941;&#21382;&#26089;&#26399;&#23574;&#38160;&#30340;&#26497;&#23567;&#21270;&#22120;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;Time Varying LARS (TVLARS)&#65292;&#23427;&#29992;&#21487;&#37197;&#32622;&#30340;&#31867;&#20284;sigmoid&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#22312;&#21021;&#22987;&#38454;&#27573;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;TVLARS&#22312;&#26089;&#26399;&#20419;&#36827;&#20102;&#26799;&#24230;&#25506;&#32034;&#65292;&#36229;&#36234;&#20102;&#23574;&#38160;&#30340;&#20248;&#21270;&#22120;&#65292;&#24182;&#36880;&#28176;&#36807;&#28193;&#21040;LARS&#20197;&#23454;&#29616;&#21518;&#26399;&#30340;&#31283;&#20581;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#22987;&#32456;&#20248;&#20110;LARS&#21644;LAMB&#65292;&#20998;&#31867;&#22330;&#26223;&#20013;&#30340;&#25913;&#36827;&#36798;&#21040;2\%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25152;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26696;&#20363;&#20013;&#65292;TVLARS&#37117;&#32988;&#36807;&#20102;LARS&#21644;LAMB&#65292;&#24182;&#19988;&#24615;&#33021;&#25552;&#21319;&#20102;
&lt;/p&gt;
&lt;p&gt;
This paper explores Large Batch Training techniques using layer-wise adaptive scaling ratio (LARS) across diverse settings, uncovering insights. LARS algorithms with warm-up tend to be trapped in sharp minimizers early on due to redundant ratio scaling. Additionally, a fixed steep decline in the latter phase restricts deep neural networks from effectively navigating early-phase sharp minimizers. Building on these findings, we propose Time Varying LARS (TVLARS), a novel algorithm that replaces warm-up with a configurable sigmoid-like function for robust training in the initial phase. TVLARS promotes gradient exploration early on, surpassing sharp optimizers and gradually transitioning to LARS for robustness in later phases. Extensive experiments demonstrate that TVLARS consistently outperforms LARS and LAMB in most cases, with up to 2\% improvement in classification scenarios. Notably, in all self-supervised learning cases, TVLARS dominates LARS and LAMB with performance improvements of
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#21327;&#21516;&#36215;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#23398;&#29983;&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26377;&#31526;&#21495;&#20108;&#20998;&#22270;&#20840;&#38754;&#24314;&#27169;&#23398;&#29983;&#22238;&#31572;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#22686;&#24378;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13500</link><description>&lt;p&gt;
&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#22686;&#24378;&#23398;&#29983;&#34920;&#29616;&#39044;&#27979;&#30340;SGNN-LLM&#21327;&#21516;
&lt;/p&gt;
&lt;p&gt;
Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy. (arXiv:2309.13500v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13500
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#21327;&#21516;&#36215;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#23398;&#29983;&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26377;&#31526;&#21495;&#20108;&#20998;&#22270;&#20840;&#38754;&#24314;&#27169;&#23398;&#29983;&#22238;&#31572;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#22686;&#24378;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#29983;&#20869;&#23481;&#21019;&#20316;&#65292;&#23398;&#20064;&#32773;&#21512;&#20316;&#20855;&#26377;&#21487;&#25193;&#23637;&#25945;&#32946;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#23398;&#29983;&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#65292;&#30001;&#20110;&#23398;&#29983;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#22122;&#22768;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#33719;&#23398;&#29983;&#21644;&#38382;&#39064;&#20132;&#20114;&#30340;&#22797;&#26434;&#32593;&#32476;&#65292;&#20294;&#22312;&#20919;&#21551;&#21160;&#26465;&#20214;&#19979;&#65292;&#20854;&#20013;&#23398;&#29983;&#23545;&#38382;&#39064;&#30340;&#26377;&#38480;&#21442;&#19982;&#23548;&#33268;&#25968;&#25454;&#31232;&#30095;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#65292;&#23558;&#25972;&#21512;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#21327;&#21516;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26377;&#31526;&#21495;&#20108;&#20998;&#22270;&#20840;&#38754;&#24314;&#27169;&#23398;&#29983;&#22238;&#31572;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#22686;&#24378;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;LLM&#30340;&#36129;&#29486;&#22312;&#20110;&#29983;&#25104;&#22522;&#30784;&#38382;&#39064;&#23884;&#20837;&#65292;&#29305;&#21035;&#26159;&#35777;&#26126;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnersourcing offers great potential for scalable education through student content creation. However, predicting student performance on learnersourced questions, which is essential for personalizing the learning experience, is challenging due to the inherent noise in student-generated data. Moreover, while conventional graph-based methods can capture the complex network of student and question interactions, they often fall short under cold start conditions where limited student engagement with questions yields sparse data. To address both challenges, we introduce an innovative strategy that synergizes the potential of integrating Signed Graph Neural Networks (SGNNs) and Large Language Model (LLM) embeddings. Our methodology employs a signed bipartite graph to comprehensively model student answers, complemented by a contrastive learning framework that enhances noise resilience. Furthermore, LLM's contribution lies in generating foundational question embeddings, proving especially adv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#23548;&#40657;&#30418;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.13340</link><description>&lt;p&gt;
&#38754;&#21521;&#40657;&#30418;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;LLM&#24341;&#23548;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards LLM-guided Causal Explainability for Black-box Text Classifiers. (arXiv:2309.13340v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#23548;&#40657;&#30418;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22823;&#19988;&#26356;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#27604;&#22914;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#20687;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#36825;&#26679;&#30340;&#27169;&#22411;&#36136;&#37327;&#65292;&#23613;&#31649;&#38750;&#24120;&#20196;&#20154;&#21521;&#24448;&#65292;&#20294;&#21464;&#24471;&#36234;&#26469;&#36234;&#38590;&#20197;&#35299;&#20915;&#12290;&#20363;&#22914;&#65292;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#26159;&#35774;&#35745;&#20026;&#40657;&#30418;&#12290;&#23613;&#31649;&#26631;&#20934;&#30340;&#35299;&#37322;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#19968;&#23450;&#31243;&#24230;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#65292;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#26377;&#38480;&#12290;&#22240;&#26524;&#35299;&#37322;&#33021;&#21147;&#26159;&#26356;&#29702;&#24819;&#30340;&#30446;&#26631;&#65292;&#20294;&#22312;NLP&#39046;&#22495;&#21364;&#26497;&#20855;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26377;&#24456;&#22810;&#12290;&#21463;&#21040;&#26368;&#36817;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#19987;&#23478;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#26368;&#26032;&#30340;LLMs&#30340;&#25351;&#23548;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#36890;&#36807;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#23454;&#29616;&#40657;&#30418;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#27969;&#31243;&#65292;
&lt;/p&gt;
&lt;p&gt;
With the advent of larger and more complex deep learning models, such as in Natural Language Processing (NLP), model qualities like explainability and interpretability, albeit highly desirable, are becoming harder challenges to tackle and solve. For example, state-of-the-art models in text classification are black-box by design. Although standard explanation methods provide some degree of explainability, these are mostly correlation-based methods and do not provide much insight into the model. The alternative of causal explainability is more desirable to achieve but extremely challenging in NLP due to a variety of reasons. Inspired by recent endeavors to utilize Large Language Models (LLMs) as experts, in this work, we aim to leverage the instruction-following and textual understanding capabilities of recent state-of-the-art LLMs to facilitate causal explainability via counterfactual explanation generation for black-box text classifiers. To do this, we propose a three-step pipeline via
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;E(2)-&#23545;&#31216;&#22270;&#35268;&#21010;&#29992;&#20110;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#21644;&#24320;&#21457;&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#31283;&#23450;&#21644;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.13043</link><description>&lt;p&gt;
E(2)-&#23545;&#31216;&#22270;&#35268;&#21010;&#29992;&#20110;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
E(2)-Equivariant Graph Planning for Navigation. (arXiv:2309.13043v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;E(2)-&#23545;&#31216;&#22270;&#35268;&#21010;&#29992;&#20110;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#21644;&#24320;&#21457;&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#31283;&#23450;&#21644;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#23398;&#20064;&#26159;&#19968;&#39033;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#21644;&#26114;&#36149;&#24615;&#38656;&#35201;&#39640;&#25928;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20108;&#32500;&#23548;&#33322;&#35268;&#21010;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#65292;&#35813;&#23545;&#31216;&#24615;&#28304;&#20110;&#21442;&#32771;&#26694;&#26550;&#20043;&#38388;&#30340;&#27431;&#20960;&#37324;&#24471;&#21464;&#25442;&#65292;&#24182;&#23454;&#29616;&#21442;&#25968;&#20849;&#20139;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#23548;&#33322;&#38382;&#39064;&#35268;&#21010;&#20026;&#22312;&#20960;&#20309;&#22270;&#19978;&#30340;&#35268;&#21010;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#26469;&#25191;&#34892;&#20215;&#20540;&#36845;&#20195;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22788;&#29702;&#22810;&#25668;&#20687;&#22836;&#36755;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#31561;&#21464;&#23618;&#23558;&#29305;&#24449;&#25552;&#21319;&#21040;&#25152;&#38656;&#31354;&#38388;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#12289;&#24050;&#30693;&#21644;&#26410;&#30693;&#22320;&#22270;&#20197;&#21450;&#32473;&#23450;&#28857;&#30446;&#26631;&#25110;&#35821;&#20041;&#30446;&#26631;&#30340;&#20116;&#20010;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35757;&#32451;&#25928;&#29575;&#12289;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning for robot navigation presents a critical and challenging task. The scarcity and costliness of real-world datasets necessitate efficient learning approaches. In this letter, we exploit Euclidean symmetry in planning for 2D navigation, which originates from Euclidean transformations between reference frames and enables parameter sharing. To address the challenges of unstructured environments, we formulate the navigation problem as planning on a geometric graph and develop an equivariant message passing network to perform value iteration. Furthermore, to handle multi-camera input, we propose a learnable equivariant layer to lift features to a desired space. We conduct comprehensive evaluations across five diverse tasks encompassing structured and unstructured environments, along with maps of known and unknown, given point goals or semantic goals. Our experiments confirm the substantial benefits on training efficiency, stability, and generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#22522;&#20110;&#27969;&#24418;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#21516;&#19968;&#31867;&#20013;&#30340;&#23616;&#37096;&#29305;&#24615;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#19982;&#33258;&#21160;&#21270;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#25928;&#26524;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2309.04174</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#24418;&#30340;&#26080;&#35843;&#21442;&#25552;&#31034;&#20998;&#31867;&#30340;&#37325;&#26032;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification. (arXiv:2309.04174v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#22522;&#20110;&#27969;&#24418;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#21516;&#19968;&#31867;&#20013;&#30340;&#23616;&#37096;&#29305;&#24615;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#19982;&#33258;&#21160;&#21270;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#25928;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#20998;&#31867;&#36890;&#36807;&#21033;&#29992;[MASK]&#26631;&#35760;&#30340;&#36951;&#28431;&#38382;&#39064;&#24418;&#24335;&#26469;&#36866;&#24212;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#39044;&#23450;&#20041;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#23558;&#22635;&#20805;&#30340;&#26631;&#35760;&#26144;&#23556;&#21040;&#26631;&#31614;&#19978;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#26469;&#20943;&#23569;&#36825;&#19968;&#36807;&#31243;&#20013;&#30340;&#21171;&#21160;&#21147;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#30740;&#31350;&#37117;&#38656;&#35201;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#38468;&#21152;&#21487;&#35757;&#32451;&#23884;&#20837;&#36827;&#34892;&#35843;&#21442;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#34920;&#31034;&#31354;&#38388;&#20013;&#28508;&#22312;&#30340;&#38750;&#32447;&#24615;&#27969;&#24418;&#65292;&#39640;&#32500;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#19981;&#24212;&#35813;&#20351;&#29992;&#27431;&#27663;&#36317;&#31163;&#26469;&#34913;&#37327;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35843;&#21442;&#22522;&#20110;&#27969;&#24418;&#30340;&#31354;&#38388;&#37325;&#26032;&#23884;&#20837;&#26041;&#27861;&#65292;&#31216;&#20026;&#20855;&#26377;&#20869;&#31867;&#36817;&#37051;&#32422;&#26463;&#30340;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65288;LLE-INC&#65289;&#65292;&#29992;&#20110;&#35821;&#35328;&#36716;&#25442;&#22120;&#23884;&#20837;&#65292;&#23427;&#20445;&#30041;&#20102;&#21516;&#19968;&#31867;&#20013;&#30340;&#23616;&#37096;&#29305;&#24615;&#20316;&#20026;&#20998;&#31867;&#30340;&#24341;&#23548;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#35843;&#20248;&#65292;&#25105;&#20204;&#30340;LLE-INC&#19982;&#33258;&#21160;&#21270;&#30340;&#35821;&#35328;&#36716;&#25442;&#22120;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based classification adapts tasks to a cloze question format utilizing the [MASK] token and the filled tokens are then mapped to labels through pre-defined verbalizers. Recent studies have explored the use of verbalizer embeddings to reduce labor in this process. However, all existing studies require a tuning process for either the pre-trained models or additional trainable embeddings. Meanwhile, the distance between high-dimensional verbalizer embeddings should not be measured by Euclidean distance due to the potential for non-linear manifolds in the representation space. In this study, we propose a tuning-free manifold-based space re-embedding method called Locally Linear Embedding with Intra-class Neighborhood Constraint (LLE-INC) for verbalizer embeddings, which preserves local properties within the same class as guidance for classification. Experimental results indicate that even without tuning any parameters, our LLE-INC is on par with automated verbalizers with parameter 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;RL-EA&#65289;&#65292;&#35813;&#31639;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#23545;&#21508;&#31181;RL-EA&#30340;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#21644;&#25628;&#32034;&#27169;&#24335;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.13420</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65306;&#35843;&#26597;&#21644;&#30740;&#31350;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities. (arXiv:2308.13420v2 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;RL-EA&#65289;&#65292;&#35813;&#31639;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#23545;&#21508;&#31181;RL-EA&#30340;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#21644;&#25628;&#32034;&#27169;&#24335;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#26159;&#19968;&#31867;&#22522;&#20110;&#33258;&#28982;&#36827;&#21270;&#21407;&#29702;&#30340;&#38543;&#26426;&#25628;&#32034;&#26041;&#27861;&#65292;&#22240;&#20854;&#22312;&#21508;&#31181;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#32780;&#24191;&#21463;&#36190;&#35465;&#12290;&#23613;&#31649;&#20840;&#29699;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#36827;&#21270;&#31639;&#27861;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#23398;&#32773;&#31215;&#26497;&#25506;&#32034;&#25913;&#36827;&#31639;&#27861;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#12289;&#25628;&#32034;&#27169;&#24335;&#31561;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#20248;&#21270;&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#36827;&#21270;&#31639;&#27861;&#26694;&#26550;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#36229;&#36234;&#24615;&#33021;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#21040;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#34987;&#31216;&#20026;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;RL-EA&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;RL-EA&#20013;&#19981;&#21516;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#21644;&#25628;&#32034;&#27169;&#24335;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms (EA), a class of stochastic search methods based on the principles of natural evolution, have received widespread acclaim for their exceptional performance in various real-world optimization problems. While researchers worldwide have proposed a wide variety of EAs, certain limitations remain, such as slow convergence speed and poor generalization capabilities. Consequently, numerous scholars actively explore improvements to algorithmic structures, operators, search patterns, etc., to enhance their optimization performance. Reinforcement learning (RL) integrated as a component in the EA framework has demonstrated superior performance in recent years. This paper presents a comprehensive survey on integrating reinforcement learning into the evolutionary algorithm, referred to as reinforcement learning-assisted evolutionary algorithm (RL-EA). We begin with the conceptual outlines of reinforcement learning and the evolutionary algorithm. We then provide a taxonomy of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#21644;&#40065;&#26834;&#65292;&#28389;&#29992;API&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#12290;&#36825;&#23545;&#21021;&#32423;&#24320;&#21457;&#32773;&#26469;&#35828;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;&#20195;&#30721;&#20013;&#30340;API&#28389;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.10335</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#30721;&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Robustness and Reliability of Large Language Model Code Generation. (arXiv:2308.10335v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#21644;&#40065;&#26834;&#65292;&#28389;&#29992;API&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#12290;&#36825;&#23545;&#21021;&#32423;&#24320;&#21457;&#32773;&#26469;&#35828;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;&#20195;&#30721;&#20013;&#30340;API&#28389;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#21644;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#38750;&#20961;&#33021;&#21147;&#12290;&#24403;&#36935;&#21040;&#32534;&#30721;&#38382;&#39064;&#26102;&#65292;&#36719;&#20214;&#24037;&#31243;&#24072;&#24120;&#24120;&#20250;&#21672;&#35810;LLMs&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#19968;&#20123;&#21162;&#21147;&#26469;&#36991;&#20813;&#35821;&#27861;&#38169;&#35823;&#24182;&#20351;&#20195;&#30721;&#19982;&#39044;&#26399;&#30340;&#35821;&#20041;&#23545;&#40784;&#65292;&#20294;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#23578;&#26410;&#34987;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#29615;&#22659;&#20013;&#65292;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#31561;&#21516;&#20110;&#21487;&#38752;&#21644;&#40065;&#26834;&#30340;&#20195;&#30721;&#12290;&#22312;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#28389;&#29992;API&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#22914;&#36164;&#28304;&#27844;&#28431;&#12289;&#31243;&#24207;&#23849;&#28291;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;LLM&#20195;&#30721;&#29983;&#25104;&#26381;&#21153;&#30340;&#29992;&#25143;&#23454;&#38469;&#19978;&#26159;&#26368;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#30475;&#20284;&#27491;&#30830;&#30340;&#20195;&#30721;&#24433;&#21709;&#30340;&#24320;&#21457;&#32773;&#8212;&#8212;&#20182;&#20204;&#36890;&#24120;&#26159;&#19981;&#29087;&#24713;LLMs&#20026;&#20182;&#20204;&#29983;&#25104;&#20195;&#30721;&#30340;API&#30340;&#21021;&#32423;&#24320;&#21457;&#32773;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;API&#30340;&#28389;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the large language models (LLMs) have shown extraordinary ability in understanding natural language and generating programming code. It has been a common practice of software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability and robustness of the code generationfrom LLMs have not yet been thoroughly studied. The executable code is not equivalent to the reliable and robust code, especially in the context of real-world software development. The misuse of APIs in the generated code could lead to severe problem, such as resource leaks, program crashes. To make things worse, the users of LLM code generation services are actually the developers that are most vulnerable to these code that seems right -- They are always novice developers that are not familiar with the APIs that LLMs generate code for them. Therefore, they could hardly tell the misuse in t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#24182;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.08334</link><description>&lt;p&gt;
&#36890;&#36807;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning Logic Programs by Discovering Higher-Order Abstractions. (arXiv:2308.08334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#24182;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#26032;&#39062;&#30340;&#25277;&#35937;&#23545;&#20110;&#20154;&#31867;&#32423;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#65288;&#20363;&#22914;map&#12289;filter&#21644;fold&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65292;&#21363;&#20174;&#31034;&#20363;&#21644;&#32972;&#26223;&#30693;&#35782;&#20013;&#24402;&#32435;&#36923;&#36753;&#31243;&#24207;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#38454;&#37325;&#26500;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#25277;&#35937;&#26469;&#21387;&#32553;&#36923;&#36753;&#31243;&#24207;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#22312;STEVIE&#20013;&#65292;&#23427;&#23558;&#39640;&#38454;&#37325;&#26500;&#38382;&#39064;&#24314;&#27169;&#20026;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#31243;&#24207;&#21512;&#25104;&#21644;&#35270;&#35273;&#25512;&#29702;&#65292;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#27809;&#26377;&#37325;&#26500;&#30456;&#27604;&#65292;STEVIE&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;27%&#24182;&#23558;&#23398;&#20064;&#26102;&#38388;&#20943;&#23569;47%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;STEVIE&#21487;&#20197;&#21457;&#29616;&#36866;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering novel abstractions is important for human-level AI. We introduce an approach to discover higher-order abstractions, such as map, filter, and fold. We focus on inductive logic programming, which induces logic programs from examples and background knowledge. We introduce the higher-order refactoring problem, where the goal is to compress a logic program by introducing higher-order abstractions. We implement our approach in STEVIE, which formulates the higher-order refactoring problem as a constraint optimisation problem. Our experimental results on multiple domains, including program synthesis and visual reasoning, show that, compared to no refactoring, STEVIE can improve predictive accuracies by 27% and reduce learning times by 47%. We also show that STEVIE can discover abstractions that transfer to different domains
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#35780;&#20998;&#26469;&#33719;&#24471;&#20154;&#31867;&#25351;&#23548;&#65292;&#35813;&#26041;&#27861;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#26679;&#26412;&#36712;&#36857;&#30340;&#35780;&#20272;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#21644;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.16348</link><description>&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rating-based Reinforcement Learning. (arXiv:2307.16348v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#35780;&#20998;&#26469;&#33719;&#24471;&#20154;&#31867;&#25351;&#23548;&#65292;&#35813;&#26041;&#27861;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#26679;&#26412;&#36712;&#36857;&#30340;&#35780;&#20272;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#21644;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35780;&#20998;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#35780;&#20998;&#26469;&#33719;&#24471;&#20154;&#31867;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#20559;&#22909;&#21644;&#22522;&#20110;&#25490;&#21517;&#30340;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#20154;&#31867;&#23545;&#26679;&#26412;&#36712;&#36857;&#30340;&#35780;&#20272;&#32780;&#19981;&#26159;&#23545;&#26679;&#26412;&#23545;&#30340;&#30456;&#23545;&#27604;&#36739;&#12290;&#22522;&#20110;&#35780;&#20998;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#19968;&#20010;&#26032;&#30340;&#20154;&#31867;&#35780;&#20998;&#39044;&#27979;&#27169;&#22411;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#31867;&#25439;&#22833;&#20989;&#25968;&#19978;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#35780;&#20998;&#21644;&#30495;&#23454;&#20154;&#31867;&#35780;&#20998;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#26032;&#30340;&#22522;&#20110;&#35780;&#20998;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops a novel rating-based reinforcement learning approach that uses human ratings to obtain human guidance in reinforcement learning. Different from the existing preference-based and ranking-based reinforcement learning paradigms, based on human relative preferences over sample pairs, the proposed rating-based reinforcement learning approach is based on human evaluation of individual trajectories without relative comparisons between sample pairs. The rating-based reinforcement learning approach builds on a new prediction model for human ratings and a novel multi-class loss function. We conduct several experimental studies based on synthetic ratings and real human ratings to evaluate the effectiveness and benefits of the new rating-based reinforcement learning approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;AI&#20195;&#29702;&#26041;&#27861;REX&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#22870;&#21169;&#23618;&#21644;&#31867;&#20284;&#20110;UCB&#20998;&#25968;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;AI&#20195;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#31163;&#32447;&#34892;&#20026;&#21033;&#29992;&#21644;&#19982;&#22522;&#30784;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.08962</link><description>&lt;p&gt;
REX: &#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#22686;&#24378;&#22411;AI&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
REX: Rapid Exploration and eXploitation for AI Agents. (arXiv:2307.08962v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;AI&#20195;&#29702;&#26041;&#27861;REX&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#22870;&#21169;&#23618;&#21644;&#31867;&#20284;&#20110;UCB&#20998;&#25968;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;AI&#20195;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#31163;&#32447;&#34892;&#20026;&#21033;&#29992;&#21644;&#19982;&#22522;&#30784;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;AI&#20195;&#29702;&#26041;&#27861;&#65292;&#31216;&#20026;REX&#12290;&#29616;&#26377;&#30340;AutoGPT&#39118;&#26684;&#25216;&#26415;&#23384;&#22312;&#19968;&#20123;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#22914;&#23545;&#20110;&#20915;&#31574;&#30340;&#31934;&#30830;&#25551;&#36848;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#20197;&#21450;&#32570;&#20047;&#31867;&#20284;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;(Reinforcement Learning&#65292;RL)&#20013;&#30340;&#23581;&#35797;&#21644;&#22833;&#36133;&#31243;&#24207;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#12290;REX&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22870;&#21169;&#23618;&#65292;&#24182;&#38598;&#25104;&#20102;&#31867;&#20284;&#20110;&#19978;&#38480;&#32622;&#20449;&#30028;&#38480;(UCB)&#20998;&#25968;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;AI&#20195;&#29702;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#26159;&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;&#26085;&#24535;&#30340;&#31163;&#32447;&#34892;&#20026;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#24494;&#35843;&#12290;&#36890;&#36807;&#19982;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#24605;&#32500;&#38142;(CoT)&#21644;&#35268;&#21010;&#25512;&#29702;(RAP)&#65289;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#22522;&#20110;REX&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#36229;&#36807;&#20102;&#36825;&#20123;&#29616;&#26377;&#25216;&#26415;&#25152;&#21462;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniqu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;</title><link>http://arxiv.org/abs/2307.00012</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#20462;&#22797;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Black-Box Prediction of Flaky Test Fix Categories Using Language Models. (arXiv:2307.00012v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26131;&#20986;&#38169;&#27979;&#35797;&#20250;&#22312;&#30456;&#21516;&#36719;&#20214;&#29256;&#26412;&#30340;&#27979;&#35797;&#19979;&#38750;&#30830;&#23450;&#24615;&#22320;&#36890;&#36807;&#25110;&#22833;&#36133;&#65292;&#24341;&#36215;&#28151;&#20081;&#24182;&#28010;&#36153;&#24320;&#21457;&#32773;&#26102;&#38388;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#34987;&#29992;&#20110;&#39044;&#27979;&#26131;&#20986;&#38169;&#24615;&#21450;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#20294;&#22312;&#25552;&#20379;&#20462;&#22797;&#25903;&#25345;&#26041;&#38754;&#20173;&#26377;&#36739;&#23569;&#24037;&#20316;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;13&#20010;&#20462;&#22797;&#31867;&#21035;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#34429;&#28982;&#22312;&#24403;&#21069;&#38454;&#27573;&#20934;&#30830;&#39044;&#27979;&#20462;&#22797;&#26412;&#36523;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#20294;&#36825;&#20123;&#31867;&#21035;&#25552;&#20379;&#20102;&#20851;&#20110;&#38656;&#35201;&#26816;&#26597;&#30340;&#27979;&#35797;&#20195;&#30721;&#37096;&#20998;&#30340;&#31934;&#30830;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;CodeBERT&#21644;UniXcoder&#65292;&#20854;&#36755;&#20986;&#32463;&#36807;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#25110;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;Few Shot Learning&#65288;FSL&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniXcoder&#22312;&#27491;&#30830;&#39044;&#27979;&#22823;&#22810;&#25968;&#20462;&#22797;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;
Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting developer time. While machine learning models have been used to predict flakiness and its root causes, there is less work on providing support to fix the problem. To address this gap, we propose a framework that automatically generates labeled datasets for 13 fix categories and train models to predict the fix category of a flaky test by analyzing the test code only. Though it is unrealistic at this stage to accurately predict the fix itself, the categories provide precise guidance about what part of the test code to look at. Our approach is based on language models, namely CodeBERT and UniXcoder, whose output is fine-tuned with a Feed Forward Neural Network (FNN) or a Siamese Network-based Few Shot Learning (FSL). Our experimental results show that UniXcoder outperforms CodeBERT, in correctly predicting most of the categories of fixes a dev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;VLMs&#22312;&#35782;&#21035;&#32454;&#31890;&#24230;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#25351;&#20986;&#20102;VLMs&#20013;&#30456;&#20284;&#24230;&#20998;&#25968;&#19981;&#33021;&#20005;&#26684;&#21453;&#26144;&#27491;&#30830;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.16048</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65306;&#31890;&#24230;&#21644;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Challenges of Zero-Shot Recognition with Vision-Language Models: Granularity and Correctness. (arXiv:2306.16048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;VLMs&#22312;&#35782;&#21035;&#32454;&#31890;&#24230;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#25351;&#20986;&#20102;VLMs&#20013;&#30456;&#20284;&#24230;&#20998;&#25968;&#19981;&#33021;&#20005;&#26684;&#21453;&#26144;&#27491;&#30830;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24212;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#26816;&#26597;&#20102;VLMs&#22312;&#19981;&#21516;&#31890;&#24230;&#27010;&#24565;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#27491;&#35780;&#20272;&#20004;&#31181;&#23454;&#39564;&#35774;&#32622;&#19979;&#24615;&#33021;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;VLMs&#22312;&#35782;&#21035;&#32454;&#31890;&#24230;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;VLMs&#20135;&#29983;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#24182;&#19981;&#33021;&#20005;&#26684;&#21453;&#26144;&#25991;&#26412;&#36755;&#20837;&#22312;&#35270;&#35273;&#36755;&#20837;&#19979;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21327;&#35758;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#20998;&#25968;&#21487;&#33021;&#20250;&#20559;&#21521;&#26356;&#20855;&#20449;&#24687;&#30340;&#25551;&#36848;&#65292;&#24182;&#19988;&#30001;&#20110;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#24615;&#36136;&#65292;&#23545;&#20110;VLMs&#26469;&#35828;&#35782;&#21035;&#30456;&#20284;&#20294;&#38169;&#35823;&#30340;&#25551;&#36848;&#20043;&#38388;&#30340;&#27491;&#30830;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#20351;&#29992;VLMs&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the challenges of applying vision-language models (VLMs) to zero-shot visual recognition tasks in an open-world setting, with a focus on contrastive vision-language models such as CLIP. We first examine the performance of VLMs on concepts of different granularity levels. We propose a way to fairly evaluate the performance discrepancy under two experimental setups and find that VLMs are better at recognizing fine-grained concepts. Furthermore, we find that the similarity scores from VLMs do not strictly reflect the correctness of the textual inputs given visual input. We propose an evaluation protocol to test our hypothesis that the scores can be biased towards more informative descriptions, and the nature of the similarity score between embedding makes it challenging for VLMs to recognize the correctness between similar but wrong descriptions. Our study highlights the challenges of using VLMs in open-world settings and suggests directions for future research to 
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.15749</link><description>&lt;p&gt;
&#20309;&#21435;&#20309;&#20174;&#65306;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#30340;&#25968;&#23383;&#30828;&#20214;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15749
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#22312;&#28085;&#30422;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#31454;&#20105;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#25928;&#29575;&#20026;&#20195;&#20215;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#33021;&#21147;&#12290;&#29983;&#29289;&#33041;&#30340;&#21151;&#32791;&#25928;&#29575;&#36229;&#36807;&#20219;&#20309;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65307;&#22240;&#27492;&#65292;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#35797;&#22270;&#27169;&#20223;&#33041;&#37096;&#25805;&#20316;&#65292;&#20363;&#22914;&#22522;&#20110;&#33033;&#20914;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;DL&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#33041;&#37096;&#26377;&#35832;&#22914;&#39640;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#12289;&#23494;&#38598;&#30340;&#31070;&#32463;&#20803;&#36830;&#25509;&#21644;&#35745;&#31639;&#19982;&#23384;&#20648;&#30340;&#20849;&#21516;&#20301;&#32622;&#31561;&#20248;&#21183;&#65292;&#20294;&#21487;&#29992;&#30340;&#29983;&#29289;&#22522;&#24213;&#20005;&#37325;&#38480;&#21046;&#20102;&#29983;&#29289;&#22823;&#33041;&#30340;&#36827;&#21270;&#12290;&#30005;&#23376;&#30828;&#20214;&#27809;&#26377;&#30456;&#21516;&#30340;&#32422;&#26463;&#65307;&#22240;&#27492;&#65292;&#34429;&#28982;&#24314;&#27169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21487;&#33021;&#25581;&#31034;&#20102;&#19968;&#20010;&#35868;&#39064;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
&lt;/p&gt;</description></item><item><title>SPRINT &#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#37325;&#26631;&#35760;&#21450;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#39044;&#35757;&#32451;&#25152;&#38656;&#30340;&#20154;&#21147;&#65292;&#21516;&#26102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.11886</link><description>&lt;p&gt;
SPRINT&#65306;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196; relabeling &#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#31574;&#30053;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling. (arXiv:2306.11886v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11886
&lt;/p&gt;
&lt;p&gt;
SPRINT &#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#37325;&#26631;&#35760;&#21450;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#39044;&#35757;&#32451;&#25152;&#38656;&#30340;&#20154;&#21147;&#65292;&#21516;&#26102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#26426;&#22120;&#20154;&#31574;&#30053;&#24182;&#36171;&#20104;&#20016;&#23500;&#30340;&#25216;&#33021;&#38598;&#21512;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#19979;&#28216;&#20219;&#21153;&#30340;&#23398;&#20064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23450;&#20041;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20294;&#36825;&#38656;&#35201;&#20154;&#20026;&#22320;&#27880;&#37322;&#25968;&#21313;&#19975;&#20010;&#25351;&#20196;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SPRINT&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#22823;&#22823;&#20943;&#23569;&#39044;&#35757;&#32451;&#22810;&#26679;&#30340;&#25216;&#33021;&#25152;&#38656;&#30340;&#20154;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#26680;&#24515;&#24819;&#27861;&#26469;&#33258;&#21160;&#25193;&#23637;&#22522;&#30784;&#39044;&#35757;&#32451;&#20219;&#21153;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#25351;&#20196;&#37325;&#26631;&#35760;&#21644;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20132;&#21449;&#36712;&#36857;&#25216;&#33021;&#38142;&#25509;&#12290;&#22240;&#27492;&#65292;SPRINT &#39044;&#35757;&#32451;&#21487;&#20197;&#20026;&#26426;&#22120;&#20154;&#35013;&#22791;&#26356;&#20016;&#23500;&#30340;&#25216;&#33021;&#24211;&#12290;&#22312;&#23478;&#24237;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#21416;&#25151;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPRINT &#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training robot policies with a rich set of skills can substantially accelerate the learning of downstream tasks. Prior works have defined pre-training tasks via natural language instructions, but doing so requires tedious human annotation of hundreds of thousands of instructions. Thus, we propose SPRINT, a scalable offline policy pre-training approach which substantially reduces the human effort needed for pre-training a diverse set of skills. Our method uses two core ideas to automatically expand a base set of pre-training tasks: instruction relabeling via large language models and cross-trajectory skill chaining through offline reinforcement learning. As a result, SPRINT pre-training equips robots with a much richer repertoire of skills. Experimental results in a household simulator and on a real robot kitchen manipulation task show that SPRINT leads to substantially faster learning of new long-horizon tasks than previous pre-training approaches. Website at https://clvrai.com/spr
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#23545;&#20449;&#21495;&#20316;&#20986;&#21709;&#24212;&#30340;&#33021;&#21147;&#65292;&#20026;&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#20316;&#20026;&#20154;&#24037;&#24418;&#24577;&#21457;&#29983;&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#19988;&#20026;&#23558;&#21160;&#24577;&#34892;&#20026;&#23884;&#20837;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.12971</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#33021;&#22815;&#23545;&#20449;&#21495;&#20316;&#20986;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Neural Cellular Automata Can Respond to Signals. (arXiv:2305.12971v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12971
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#23545;&#20449;&#21495;&#20316;&#20986;&#21709;&#24212;&#30340;&#33021;&#21147;&#65292;&#20026;&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#20316;&#20026;&#20154;&#24037;&#24418;&#24577;&#21457;&#29983;&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#19988;&#20026;&#23558;&#21160;&#24577;&#34892;&#20026;&#23884;&#20837;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20803;&#20803;&#32990;&#33258;&#21160;&#26426;&#65288;NCAs&#65289;&#26159;&#19968;&#31181;&#24418;&#24577;&#21457;&#29983;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#21333;&#20010;&#31181;&#23376;&#32454;&#32990;&#29983;&#38271;&#20986;&#20108;&#32500;&#20154;&#24037;&#29983;&#29289;&#20307;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;NCAs&#21487;&#20197;&#34987;&#35757;&#32451;&#25104;&#23545;&#20449;&#21495;&#20316;&#20986;&#21709;&#24212;&#12290;&#20351;&#29992;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#20449;&#21495;&#65306;&#20869;&#37096;&#65288;&#22522;&#22240;&#32534;&#30721;&#65289;&#20449;&#21495;&#21644;&#22806;&#37096;&#65288;&#29615;&#22659;&#65289;&#20449;&#21495;&#12290;&#20449;&#21495;&#34987;&#21576;&#29616;&#32473;&#21333;&#20010;&#20687;&#32032;&#22312;&#19968;&#20010;&#26102;&#38388;&#27493;&#38271;&#20869;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;NCAs&#33021;&#22815;&#26681;&#25454;&#20869;&#37096;&#20449;&#21495;&#21457;&#23637;&#25104;&#22810;&#31181;&#19981;&#21516;&#24418;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#26681;&#25454;&#22806;&#37096;&#20449;&#21495;&#25913;&#21464;&#39068;&#33394;&#12290;&#24635;&#20307;&#19978;&#65292;&#36825;&#20123;&#32467;&#26524;&#20026;NCAs&#20316;&#20026;&#20154;&#24037;&#24418;&#24577;&#21457;&#29983;&#27169;&#22411;&#30340;&#21457;&#23637;&#36129;&#29486;&#20102;&#65292;&#24182;&#20026;&#23558;&#21160;&#24577;&#34892;&#20026;&#23884;&#20837;NCA&#27169;&#22411;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#36890;&#36807;GitHub&#21487;&#20197;&#33719;&#24471;&#20195;&#30721;&#21644;&#30446;&#26631;&#22270;&#20687;&#65306;https://github.com/jstovold/ALIFE2023
&lt;/p&gt;
&lt;p&gt;
Neural Cellular Automata (NCAs) are a model of morphogenesis, capable of growing two-dimensional artificial organisms from a single seed cell. In this paper, we show that NCAs can be trained to respond to signals. Two types of signal are used: internal (genomically-coded) signals, and external (environmental) signals. Signals are presented to a single pixel for a single timestep.  Results show NCAs are able to grow into multiple distinct forms based on internal signals, and are able to change colour based on external signals. Overall these contribute to the development of NCAs as a model of artificial morphogenesis, and pave the way for future developments embedding dynamic behaviour into the NCA model.  Code and target images are available through GitHub: https://github.com/jstovold/ALIFE2023
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10163</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20013;&#22269;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#19978;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model. (arXiv:2305.10163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;GPT&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;ChatGPT&#24050;&#34987;&#25972;&#21512;&#21040;&#21508;&#20010;&#39046;&#22495;&#30340;&#24037;&#20316;&#27969;&#20013;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#20854;&#24494;&#35843;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#19981;&#36275;&#65292;&#38459;&#30861;&#20102;&#20854;&#22312;&#38656;&#35201;&#24191;&#27867;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#35821;&#20041;&#30693;&#35782;&#30340;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#65288;CNMLE&#65289;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#65292;&#21363;&#20174;&#20004;&#20010;&#26041;&#38754;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#23558;&#21307;&#23398;&#32972;&#26223;&#30693;&#35782;&#25552;&#21462;&#20026;&#35821;&#20041;&#25351;&#20196;&#26469;&#25351;&#23548;ChatGPT&#30340;&#25512;&#26029;&#12290;&#31867;&#20284;&#22320;&#65292;&#30456;&#20851;&#30340;&#21307;&#30103;&#38382;&#39064;&#34987;&#35782;&#21035;&#24182;&#20316;&#20026;&#28436;&#31034;&#36755;&#20837;&#32473;ChatGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30452;&#25509;&#24212;&#29992;ChatGPT&#26080;&#27861;&#22312;CNMLE&#19978;&#33719;&#24471;&#21512;&#26684;&#20998;&#25968;&#65288;51&#20998;&#65289;&#65292;&#21482;&#26377;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#35757;&#32451;&#30340;&#27169;&#22411;&#25104;&#21151;&#36890;&#36807;&#32771;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-Training (GPT) models like ChatGPT have demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. Although ChatGPT has been integrated into the overall workflow to boost efficiency in many domains, the lack of flexibility in the finetuning process hinders its applications in areas that demand extensive domain expertise and semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on the China National Medical Licensing Examination (CNMLE) and propose a novel approach to improve ChatGPT from two perspectives: integrating medical domain knowledge and enabling few-shot learning. By using a simple but effective retrieval method, medical background knowledge is extracted as semantic instructions to guide the inference of ChatGPT. Similarly, relevant medical questions are identified and fed as demonstrations to ChatGPT. Experimental results show that directly applying ChatGPT fails to qualify the CNMLE at a score of 51 (i.e., onl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;&#24207;&#21015;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#21644;&#26368;&#20248;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#28508;&#22312;&#20986;&#34892;&#38656;&#27714;&#65292;&#24182;&#35268;&#36991;&#35774;&#35745;&#19982;&#23454;&#38469;&#38656;&#27714;&#19981;&#19968;&#33268;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.09452</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#30456;&#20851;&#20449;&#24565;&#19979;&#26368;&#20248;&#23398;&#20064;&#30340;&#24207;&#21015;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A sequential transit network design algorithm with optimal learning under correlated beliefs. (arXiv:2305.09452v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;&#24207;&#21015;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#21644;&#26368;&#20248;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#28508;&#22312;&#20986;&#34892;&#38656;&#27714;&#65292;&#24182;&#35268;&#36991;&#35774;&#35745;&#19982;&#23454;&#38469;&#38656;&#27714;&#19981;&#19968;&#33268;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20272;&#35745;&#28508;&#22312;&#38656;&#27714;&#23545;&#20110;&#20844;&#20132;&#26381;&#21153;&#36335;&#32447;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#24207;&#21015;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#21644;&#26368;&#20248;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#31639;&#27861;&#12290;&#36816;&#33829;&#21830;&#36880;&#27493;&#25193;&#23637;&#20854;&#36335;&#32447;&#31995;&#32479;&#65292;&#20197;&#36991;&#20813;&#35774;&#35745;&#36335;&#32447;&#19982;&#23454;&#38469;&#20986;&#34892;&#38656;&#27714;&#19981;&#19968;&#33268;&#30340;&#39118;&#38505;&#12290;&#21516;&#26102;&#65292;&#35266;&#27979;&#21040;&#30340;&#20449;&#24687;&#23558;&#34987;&#23384;&#26723;&#20197;&#26356;&#26032;&#36816;&#33829;&#21830;&#24403;&#21069;&#20351;&#29992;&#30340;&#30693;&#35782;&#12290;&#31639;&#27861;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#23398;&#20064;&#31574;&#30053;&#65306;&#22810;&#33218;&#32769;&#34382;&#26426;&#12289;&#30693;&#35782;&#26799;&#24230;&#21644;&#20855;&#26377;&#30456;&#20851;&#20449;&#24565;&#30340;&#30693;&#35782;&#26799;&#24230;&#12290;&#35813;&#31639;&#27861;&#30340;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#20013;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobility service route design requires potential demand information to well accommodate travel demand within the service region. Transit planners and operators can access various data sources including household travel survey data and mobile device location logs. However, when implementing a mobility system with emerging technologies, estimating demand level becomes harder because of more uncertainties with user behaviors. Therefore, this study proposes an artificial intelligence-driven algorithm that combines sequential transit network design with optimal learning. An operator gradually expands its route system to avoid risks from inconsistency between designed routes and actual travel demand. At the same time, observed information is archived to update the knowledge that the operator currently uses. Three learning policies are compared within the algorithm: multi-armed bandit, knowledge gradient, and knowledge gradient with correlated beliefs. For validation, a new route system is de
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#30340;&#27969;&#24418;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#21363;&#29992;&#30340;&#25991;&#26412;&#23545;&#25239;&#20363;&#23376;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#23545;&#20998;&#31867;&#20219;&#21153;&#12289;&#27169;&#22411;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#26080;&#20381;&#36182;&#30340;&#21069;&#25552;&#19979;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#23545;&#25239;&#20363;&#23376;&#12290;</title><link>http://arxiv.org/abs/2304.08767</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Masked Language Model Based Textual Adversarial Example Detection. (arXiv:2304.08767v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08767
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#30340;&#27969;&#24418;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#21363;&#29992;&#30340;&#25991;&#26412;&#23545;&#25239;&#20363;&#23376;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#23545;&#20998;&#31867;&#20219;&#21153;&#12289;&#27169;&#22411;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#26080;&#20381;&#36182;&#30340;&#21069;&#25552;&#19979;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#23545;&#25239;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#23433;&#20840;&#24212;&#29992;&#20013;&#21487;&#38752;&#37096;&#32626;&#30340;&#20005;&#37325;&#23041;&#32961;&#65292;&#31245;&#24494;&#20462;&#25913;&#36755;&#20837;&#21363;&#21487;&#35823;&#23548;&#24403;&#21069;&#27169;&#22411;&#36827;&#34892;&#38169;&#35823;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#22823;&#37327;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#25239;&#26679;&#26412;&#24448;&#24448;&#20559;&#31163;&#27491;&#24120;&#26679;&#26412;&#30340;&#22522;&#30784;&#25968;&#25454;&#27969;&#24418;&#65292;&#32780;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#27491;&#24120;&#30340;NLP&#25968;&#25454;&#27969;&#24418;&#12290;&#20026;&#20102;&#25506;&#32034;&#22914;&#20309;&#23558;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23545;&#25239;&#24615;&#26816;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#23545;&#25239;&#20363;&#23376;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#27979;&#65288;MLMD&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25506;&#32034;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#30340;&#27969;&#24418;&#21464;&#21270;&#65292;&#22312;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#20043;&#38388;&#20135;&#29983;&#26126;&#26174;&#21487;&#21306;&#20998;&#30340;&#20449;&#21495;&#12290;MLMD&#20855;&#26377;&#21363;&#25554;&#21363;&#29992;&#30340;&#20351;&#29992;&#26041;&#27861;&#65288;&#21363;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#21463;&#23475;&#27169;&#22411;&#65289;&#29992;&#20110;&#23545;&#25239;&#24615;&#38450;&#24481;&#65292;&#32780;&#19988;&#19981;&#21463;&#20998;&#31867;&#20219;&#21153;&#12289;&#21463;&#23475;&#27169;&#22411;&#32467;&#26500;&#21644;&#24453;&#38450;&#24481;&#30340;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks are a serious threat to the reliable deployment of machine learning models in safety-critical applications. They can misguide current models to predict incorrectly by slightly modifying the inputs. Recently, substantial work has shown that adversarial examples tend to deviate from the underlying data manifold of normal examples, whereas pre-trained masked language models can fit the manifold of normal NLP data. To explore how to use the masked language model in adversarial detection, we propose a novel textual adversarial example detection method, namely Masked Language Model-based Detection (MLMD), which can produce clearly distinguishable signals between normal examples and adversarial examples by exploring the changes in manifolds induced by the masked language model. MLMD features a plug and play usage (i.e., no need to retrain the victim model) for adversarial defense and it is agnostic to classification tasks, victim model's architectures, and to-be-defended a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01295</link><description>&lt;p&gt;
&#26377;&#25928;&#22320;&#23545;&#40784;&#36328;&#35821;&#35328;&#20250;&#35805;&#20219;&#21153;&#30340;&#25552;&#31034;&#35843;&#25972;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#23545;&#20110;&#20250;&#35805;&#20219;&#21153;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;XSGD&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;Schema-Guided Dialogue&#65288;SGD&#65289;&#32763;&#35793;&#25104;105&#31181;&#20854;&#20182;&#35821;&#35328;&#30340;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#27979;&#35797;&#20102;&#23545;&#40784;&#25552;&#31034;&#25152;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23545;&#35805;&#20219;&#21153;&#65288;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#65289;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent cla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.09026</link><description>&lt;p&gt;
&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#36164;&#28304;&#21463;&#38480;&#21644;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection. (arXiv:2303.09026v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#36793;&#32536;&#35745;&#31639;&#31561;&#36164;&#28304;&#21463;&#38480;&#22330;&#26223;&#19979;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;&#38024;&#23545;&#20351;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#22120;&#26102;&#38656;&#35201;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#21644;&#22823;&#37327;&#25968;&#25454;&#26631;&#27880;&#30340;&#31934;&#20934;&#32454;&#31890;&#24230;&#26816;&#27979;&#38656;&#27714;&#65292;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#31895;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#22120;&#33719;&#21462;&#31934;&#20934;&#30340;&#32454;&#31890;&#24230;&#26816;&#27979;&#32467;&#26524;&#12290;&#24341;&#20837;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;(CKIM)&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#29983;&#25104;&#32454;&#31890;&#24230;&#26631;&#31614;&#12290;&#35770;&#25991;&#20013;&#32771;&#34385;&#20102;&#27169;&#31946;&#35268;&#21017;&#21644;&#28165;&#26224;&#35268;&#21017;&#30340;&#25512;&#29702;&#65292;&#21069;&#32773;&#29992;&#20110;&#22788;&#29702;&#30446;&#26631;&#35821;&#20041;&#26631;&#31614;&#30340;&#27169;&#31946;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider fine-grained image object detection in resource-constrained cases such as edge computing. Deep learning (DL), namely learning with deep neural networks (DNNs), has become the dominating approach to object detection. To achieve accurate fine-grained detection, one needs to employ a large enough DNN model and a vast amount of data annotations, which brings a challenge for using modern DL object detectors in resource-constrained cases. To this end, we propose an approach, which leverages commonsense knowledge to assist a coarse-grained object detector to get accurate fine-grained detection results. Specifically, we introduce a commonsense knowledge inference module (CKIM) to process coarse-grained lables given by a benchmark DL detector to produce fine-grained lables. We consider both crisp-rule and fuzzy-rule based inference in our CKIM; the latter is used to handle ambiguity in the target semantic labels. We implement our method based on several modern DL dete
&lt;/p&gt;</description></item><item><title>Magnushammer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21069;&#25552;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;PISA&#22522;&#20934;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#22823;&#24133;&#24230;&#36229;&#36234;&#20256;&#32479;&#31526;&#21495;&#31995;&#32479;&#65292;&#24182;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#35777;&#26126;&#29575;&#20174;57.0&#65285;&#25552;&#39640;&#21040;71.0&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.04488</link><description>&lt;p&gt;
Magnushammer: &#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21069;&#25552;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Magnushammer: A Transformer-based Approach to Premise Selection. (arXiv:2303.04488v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04488
&lt;/p&gt;
&lt;p&gt;
Magnushammer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21069;&#25552;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;PISA&#22522;&#20934;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#22823;&#24133;&#24230;&#36229;&#36234;&#20256;&#32479;&#31526;&#21495;&#31995;&#32479;&#65292;&#24182;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#35777;&#26126;&#29575;&#20174;57.0&#65285;&#25552;&#39640;&#21040;71.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#25552;&#36873;&#25321;&#26159;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#24120;&#24120;&#20351;&#29992;&#22797;&#26434;&#30340;&#31526;&#21495;&#26041;&#27861;&#65292;&#20381;&#36182;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#31243;&#24037;&#20316;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#31070;&#32463;&#36716;&#25442;&#22120;&#30340;Magnushammer&#26041;&#27861;&#21487;&#20197;&#22823;&#24133;&#24230;&#22320;&#36229;&#36234;&#20256;&#32479;&#30340;&#31526;&#21495;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;PISA&#22522;&#20934;&#19978;&#30340;&#27979;&#35797;&#65292;Magnushammer&#30340;&#35777;&#26126;&#29575;&#36798;&#21040;&#20102;59.5&#65285;&#65292;&#32780;&#26368;&#25104;&#29087;&#21644;&#27969;&#34892;&#30340;&#22522;&#20110;&#31526;&#21495;&#30340;&#27714;&#35299;&#22120;Sledgehammer&#30340;&#35777;&#26126;&#29575;&#21482;&#26377;38.3&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;Magnushammer&#19982;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#24418;&#24335;&#35777;&#26126;&#22120;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#35777;&#26126;&#29575;&#20174;57.0&#65285;&#22823;&#24133;&#25552;&#39640;&#21040;71.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Premise selection is a fundamental problem of automated theorem proving. Previous works often use intricate symbolic methods, rely on domain knowledge, and require significant engineering effort to solve this task. In this work, we show that Magnushammer, a neural transformer-based approach, can outperform traditional symbolic systems by a large margin. Tested on the PISA benchmark, Magnushammer achieves $59.5\%$ proof rate compared to a $38.3\%$ proof rate of Sledgehammer, the most mature and popular symbolic-based solver. Furthermore, by combining Magnushammer with a neural formal prover based on a language model, we significantly improve the previous state-of-the-art proof rate from $57.0\%$ to $71.0\%$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#22411;&#26080;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#23558;&#21508;&#20010;&#35299;&#37322;&#33021;&#21147;&#26041;&#38754;&#24635;&#32467;&#25104;&#26631;&#37327;&#65292;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#20915;&#31574;&#32773;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.12094</link><description>&lt;p&gt;
&#35780;&#20272;&#20351;&#29992;&#27169;&#22411;&#26080;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating explainability for machine learning predictions using model-agnostic metrics. (arXiv:2302.12094v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#22411;&#26080;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#23558;&#21508;&#20010;&#35299;&#37322;&#33021;&#21147;&#26041;&#38754;&#24635;&#32467;&#25104;&#26631;&#37327;&#65292;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#20915;&#31574;&#32773;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#31649;&#29702;&#21644;&#30417;&#31649;&#26041;&#38754;&#30340;&#20247;&#22810;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#27491;&#22312;&#34987;&#25972;&#21512;&#21040;&#21508;&#20010;&#34892;&#19994;&#21644;&#39046;&#22495;&#65292;&#20915;&#31574;&#32773;&#38656;&#20840;&#38754;&#32454;&#33268;&#22320;&#20102;&#35299;&#36825;&#20123;&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;&#36825;&#20010;&#38656;&#27714;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#33021;&#22815;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#20449;&#20219;&#24230;&#20197;&#21450;&#24110;&#21161;&#27169;&#22411;&#22312;&#36947;&#24503;&#19978;&#36827;&#34892;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#37327;&#21270;AI&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20854;&#29305;&#24449;&#36827;&#34892;&#26131;&#20110;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#23558;&#35299;&#37322;&#33021;&#21147;&#30340;&#19981;&#21516;&#26041;&#38754;&#24635;&#32467;&#20026;&#26631;&#37327;&#65292;&#25552;&#20379;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#20419;&#36827;&#20915;&#31574;&#32773;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) technology have brought about a plethora of new challenges in terms of governance and regulation. AI systems are being integrated into various industries and sectors, creating a demand from decision-makers to possess a comprehensive and nuanced understanding of the capabilities and limitations of these systems. One critical aspect of this demand is the ability to explain the results of machine learning models, which is crucial to promoting transparency and trust in AI systems, as well as fundamental in helping machine learning models to be trained ethically. In this paper, we present novel metrics to quantify the degree of which AI model predictions can be easily explainable by its features. Our metrics summarize different aspects of explainability into scalars, providing a more comprehensive understanding of model predictions and facilitating communication between decision-makers and stakeholders, thereby increasing the overall transp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22836;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#20195;&#25968;&#21644;&#23450;&#24615;&#27010;&#24565;&#30340;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#36890;&#36807;&#27169;&#22411;&#35770;&#31867;&#22411;&#33258;&#28982;&#22320;&#23884;&#20837;&#21040;&#19968;&#38454;&#36923;&#36753;&#20013;&#12290;</title><link>http://arxiv.org/abs/2302.10096</link><description>&lt;p&gt;
&#22522;&#20110;&#27867;&#21270;&#30340;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalization-based similarity. (arXiv:2302.10096v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22836;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#20195;&#25968;&#21644;&#23450;&#24615;&#27010;&#24565;&#30340;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#36890;&#36807;&#27169;&#22411;&#35770;&#31867;&#22411;&#33258;&#28982;&#22320;&#23884;&#20837;&#21040;&#19968;&#38454;&#36923;&#36753;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#21644;&#21033;&#29992;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#23545;&#35937;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26159;&#31867;&#27604;&#25512;&#29702;&#30340;&#26680;&#24515;&#65292;&#32780;&#31867;&#27604;&#25512;&#29702;&#21448;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#12290;&#26412;&#25991;&#20174;&#22836;&#24320;&#22987;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#20195;&#25968;&#21644;&#23450;&#24615;&#27010;&#24565;&#30340;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#22522;&#20110;&#35266;&#23519;&#21040;&#27867;&#21270;&#38598;&#21512;&#21487;&#20197;&#32534;&#30721;&#20803;&#32032;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20197;&#36825;&#31181;&#26041;&#24335;&#23450;&#20041;&#30340;&#30456;&#20284;&#24615;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#25968;&#23398;&#23646;&#24615;&#12290;&#36890;&#36807;&#20174;&#22522;&#26412;&#27010;&#24565;&#20986;&#21457;&#26500;&#24314;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#33258;&#28982;&#22320;&#23884;&#20837;&#21040;&#27169;&#22411;&#35770;&#31867;&#22411;&#20013;&#30340;&#19968;&#38454;&#36923;&#36753;&#20013;&#65292;&#25105;&#20204;&#20351;&#35835;&#32773;&#30456;&#20449;&#20854;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting and exploiting similarities between seemingly distant objects is at the core of analogical reasoning which itself is at the core of artificial intelligence. This paper develops {\em from the ground up} an abstract algebraic and {\em qualitative} notion of similarity based on the observation that sets of generalizations encode important properties of elements. We show that similarity defined in this way has appealing mathematical properties. As we construct our notion of similarity from first principles using only elementary concepts of universal algebra, to convince the reader of its plausibility, we show that it can be naturally embedded into first-order logic via model-theoretic types.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24503;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#20984;&#21253;&#26368;&#20415;&#23452;&#25554;&#20837;&#21551;&#21457;&#24335;&#35299;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#32553;&#25918;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#30340;&#28857;&#36817;&#20284;&#21040;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#65292;&#29983;&#25104;&#20102;&#21021;&#22987;&#21270;&#31639;&#27861;&#30340;&#20984;&#21253;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;&#35813;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#26368;&#37051;&#36817;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.06582</link><description>&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24503;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#20984;&#21253;&#26368;&#20415;&#23452;&#25554;&#20837;&#21551;&#21457;&#24335;&#35299;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Convex Hull Cheapest Insertion Heuristic for the Non-Euclidean TSP. (arXiv:2302.06582v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24503;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#20984;&#21253;&#26368;&#20415;&#23452;&#25554;&#20837;&#21551;&#21457;&#24335;&#35299;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#32553;&#25918;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#30340;&#28857;&#36817;&#20284;&#21040;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#65292;&#29983;&#25104;&#20102;&#21021;&#22987;&#21270;&#31639;&#27861;&#30340;&#20984;&#21253;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;&#35813;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#26368;&#37051;&#36817;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20984;&#21253;&#26368;&#20415;&#23452;&#25554;&#20837;&#21551;&#21457;&#24335;&#31639;&#27861;&#21487;&#20197;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#20135;&#29983;&#33391;&#22909;&#30340;&#26053;&#34892;&#21830;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36824;&#26410;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#24773;&#20917;&#19979;&#36827;&#34892;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#22788;&#29702;&#38556;&#30861;&#29289;&#30340;&#22256;&#38590;&#65292;&#25552;&#20986;&#30340;&#25913;&#36827;&#26041;&#27861;&#20351;&#29992;&#22810;&#32500;&#32553;&#25918;&#23558;&#36825;&#20123;&#28857;&#39318;&#20808;&#36817;&#20284;&#21040;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#65292;&#20174;&#32780;&#21487;&#20197;&#29983;&#25104;&#21021;&#22987;&#21270;&#31639;&#27861;&#30340;&#20984;&#21253;&#12290;&#36890;&#36807;&#20462;&#25913;TSPLIB&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21521;&#20854;&#20013;&#28155;&#21152;&#19981;&#21487;&#36890;&#36807;&#30340;&#20998;&#21106;&#22120;&#26469;&#20135;&#29983;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#65292;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;&#22312;&#25152;&#30740;&#31350;&#30340;&#26696;&#20363;&#20013;&#65292;&#35813;&#31639;&#27861;&#34920;&#29616;&#20986;&#20248;&#20110;&#24120;&#29992;&#30340;&#26368;&#37051;&#36817;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;96%&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The convex hull cheapest insertion heuristic is known to generate good solutions to the Traveling Salesperson Problem in Euclidean spaces, but it has not been extended to the non-Euclidean case. To address the difficulty of dealing with obstacles in the non-Euclidean space, the proposed adaptation uses multidimensional scaling to first approximate these points in a Euclidean space, thereby enabling the generation of the convex hull that initializes the algorithm. To evaluate the proposed algorithm, the TSPLIB benchmark data-set is modified by adding impassable separators that produce non-Euclidean spaces. The algorithm is demonstrated to outperform the commonly used Nearest Neighbor algorithm in 96% of the cases studied.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#19981;&#38656;&#35201;&#32534;&#30721;&#25110;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#39640;&#21484;&#22238;&#29575;&#21644;&#20960;&#20046;&#23436;&#32654;&#30340;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.04914</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#29992;&#20110;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#65292;&#20351;&#29992;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models. (arXiv:2302.04914v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#19981;&#38656;&#35201;&#32534;&#30721;&#25110;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#39640;&#21484;&#22238;&#29575;&#21644;&#20960;&#20046;&#23436;&#32654;&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#24211;&#23545;&#20110;&#26448;&#26009;&#31185;&#23398;&#21644;&#24037;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#26469;&#24320;&#21457;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20174;&#30740;&#31350;&#35770;&#25991;&#30340;&#20840;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#24555;&#36895;&#24320;&#21457;&#35268;&#27169;&#36866;&#20013;&#30340;&#25968;&#25454;&#24211;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#19981;&#38656;&#35201;&#32534;&#30721;&#65292;&#19981;&#38656;&#35201;&#20851;&#20110;&#25552;&#21462;&#23646;&#24615;&#30340;&#20808;&#39564;&#30693;&#35782;&#25110;&#27169;&#22411;&#35757;&#32451;&#65292;&#19988;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#39640;&#21484;&#22238;&#29575;&#21644;&#20960;&#20046;&#23436;&#32654;&#30340;&#31934;&#30830;&#24230;&#12290;&#35813;&#26041;&#27861;&#26159;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#65292;&#38500;&#20102;&#19968;&#20010;&#38656;&#35201;&#20154;&#24037;&#36741;&#21161;&#30340;&#27493;&#39588;&#65292;&#36890;&#24120;&#21482;&#38656;&#35201;&#20960;&#20010;&#23567;&#26102;&#30340;&#20154;&#21147;&#21171;&#21160;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#20960;&#20046;&#21487;&#20197;&#19982;&#20219;&#20309;&#27492;&#31867;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#12290;&#36825;&#37324;&#35780;&#20272;&#20102;GPT-3/3.5&#12289;bart&#21644;DeBERTaV3&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#22312;&#25552;&#21462;&#20307;&#27169;&#37327;&#25968;&#25454;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#39640;&#36798;90%&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and comprehensive material databases extracted from research papers are critical for materials science and engineering but require significant human effort to develop. In this paper we present a simple method of extracting materials data from full texts of research papers suitable for quickly developing modest-sized databases. The method requires minimal to no coding, prior knowledge about the extracted property, or model training, and provides high recall and almost perfect precision in the resultant database. The method is fully automated except for one human-assisted step, which typically requires just a few hours of human labor. The method builds on top of natural language processing and large general language models but can work with almost any such model. The language models GPT-3/3.5, bart and DeBERTaV3 are evaluated here for comparison. We provide a detailed detailed analysis of the methods performance in extracting bulk modulus data, obtaining up to 90% precision at 9
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;SBERT&#21644;CNN&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;Reddit&#29992;&#25143;&#30340;&#24086;&#23376;&#33258;&#21160;&#35782;&#21035;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;</title><link>http://arxiv.org/abs/2302.02759</link><description>&lt;p&gt;
&#29992;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;SBERT-CNN&#26816;&#27979;Reddit&#29992;&#25143;&#30340;&#25233;&#37057;&#30151;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Detecting Reddit Users with Depression Using a Hybrid Neural Network SBERT-CNN. (arXiv:2302.02759v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;SBERT&#21644;CNN&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;Reddit&#29992;&#25143;&#30340;&#24086;&#23376;&#33258;&#21160;&#35782;&#21035;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#65292;&#24433;&#21709;&#20102;&#20840;&#29699;&#20272;&#35745;3.8%&#30340;&#20154;&#21475;&#12290;&#23427;&#20063;&#26159;&#20840;&#29699;&#27531;&#30142;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#20010;&#20154;&#36234;&#26469;&#36234;&#21916;&#27426;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65288;&#22914;Reddit&#65289;&#26469;&#34920;&#36798;&#20182;&#20204;&#30340;&#22256;&#38590;&#21644;&#20581;&#24247;&#38382;&#39064;&#65288;&#22914;&#25233;&#37057;&#30151;&#65289;&#65292;&#24182;&#22312;&#22312;&#32447;&#31038;&#21306;&#23547;&#27714;&#20854;&#20182;&#29992;&#25143;&#30340;&#25903;&#25345;&#12290;&#36825;&#20026;&#36890;&#36807;&#20998;&#26512;&#25968;&#30334;&#19975;&#24086;&#23376;&#20197;&#23547;&#25214;&#28508;&#22312;&#30340;&#24178;&#39044;&#26426;&#20250;&#65292;&#33258;&#21160;&#35782;&#21035;&#20855;&#26377;&#25233;&#37057;&#30151;&#30340;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#26426;&#20250;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22240;&#20854;&#26131;&#29992;&#24615;&#12289;&#39640;&#25928;&#22788;&#29702;&#33021;&#21147;&#21644;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#30340;&#26368;&#26032;&#32467;&#26524;&#32780;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#22987;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;BERT&#65288;SBERT&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#36890;&#36807;&#20182;&#20204;&#22312;Reddit&#19978;&#30340;&#24086;&#23376;&#26816;&#27979;&#25233;&#37057;&#30151;&#24739;&#32773;&#12290;&#21477;&#23376;BERT&#29992;&#20110;&#23398;&#20064;&#21477;&#23376;&#30340;&#24847;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is a widespread mental health issue, affecting an estimated 3.8% of the global population. It is also one of the main contributors to disability worldwide. Recently it is becoming popular for individuals to use social media platforms (e.g., Reddit) to express their difficulties and health issues (e.g., depression) and seek support from other users in online communities. It opens great opportunities to automatically identify social media users with depression by parsing millions of posts for potential interventions. Deep learning methods have begun to dominate in the field of machine learning and natural language processing (NLP) because of their ease of use, efficient processing, and state-of-the-art results on many NLP tasks. In this work, we propose a hybrid deep learning model which combines a pretrained sentence BERT (SBERT) and convolutional neural network (CNN) to detect individuals with depression with their Reddit posts. The sentence BERT is used to learn the meaning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#34920;&#24449;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#24403;&#21069;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#30340;&#34920;&#31034;&#19981;&#23545;&#40784;&#30340;&#22256;&#22659;&#65292;&#24182;&#24314;&#35758;&#24212;&#23558;&#26426;&#22120;&#20154;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#20174;&#23454;&#29616;&#20219;&#21153;&#30446;&#26631;&#30340;&#35282;&#24230;&#36716;&#21521;&#19982;&#20154;&#31867;&#34920;&#24449;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.01928</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Robot and Human Representations. (arXiv:2302.01928v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#34920;&#24449;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#24403;&#21069;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#30340;&#34920;&#31034;&#19981;&#23545;&#40784;&#30340;&#22256;&#22659;&#65292;&#24182;&#24314;&#35758;&#24212;&#23558;&#26426;&#22120;&#20154;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#20174;&#23454;&#29616;&#20219;&#21153;&#30446;&#26631;&#30340;&#35282;&#24230;&#36716;&#21521;&#19982;&#20154;&#31867;&#34920;&#24449;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#19990;&#30028;&#20013;&#34892;&#21160;&#65292;&#26426;&#22120;&#20154;&#20381;&#36182;&#20110;&#19968;&#20010;&#20984;&#26174;&#20219;&#21153;&#20851;&#38190;&#26041;&#38754;&#30340;&#34920;&#31034;&#65306;&#20363;&#22914;&#65292;&#20026;&#20102;&#25644;&#36816;&#21654;&#21857;&#26479;&#65292;&#26426;&#22120;&#20154;&#21487;&#33021;&#20250;&#32771;&#34385;&#21160;&#20316;&#25928;&#29575;&#25110;&#26479;&#23376;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#24076;&#26395;&#26426;&#22120;&#20154;&#20026;&#20154;&#31867;&#32780;&#34892;&#21160;&#65292;&#23427;&#20204;&#30340;&#34920;&#31034;&#19981;&#33021;&#21482;&#26159;&#21151;&#33021;&#24615;&#30340;&#65292;&#36824;&#24517;&#39035;&#21453;&#26144;&#20154;&#31867;&#20851;&#24515;&#30340;&#20107;&#29289;&#65292;&#21363;&#23427;&#20204;&#24517;&#39035;&#23545;&#40784;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#21069;&#30340;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#34920;&#31034;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#21363;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#34920;&#31034;&#19981;&#33021;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22240;&#20026;&#20154;&#31867;&#26159;&#26426;&#22120;&#20154;&#34920;&#29616;&#30340;&#26368;&#32456;&#35780;&#20272;&#32773;&#65292;&#25152;&#20197;&#25105;&#20204;&#24517;&#39035;&#26126;&#30830;&#22320;&#23558;&#25105;&#20204;&#30340;&#21162;&#21147;&#38598;&#20013;&#22312;&#19982;&#20154;&#31867;&#30340;&#34920;&#24449;&#23545;&#40784;&#19978;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#23398;&#20064;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20513;&#20174;&#23545;&#34920;&#24449;&#23545;&#40784;&#30446;&#26631;&#30340;&#23436;&#25104;&#31243;&#24230;&#30340;&#35282;&#24230;&#30740;&#31350;&#24403;&#21069;&#26426;&#22120;&#20154;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#19978;&#23450;&#20041;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#20851;&#38190;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
To act in the world, robots rely on a representation of salient task aspects: for example, to carry a coffee mug, a robot may consider movement efficiency or mug orientation in its behavior. However, if we want robots to act for and with people, their representations must not be just functional but also reflective of what humans care about, i.e. they must be aligned. We observe that current learning approaches suffer from representation misalignment, where the robot's learned representation does not capture the human's representation. We suggest that because humans are the ultimate evaluator of robot performance, we must explicitly focus our efforts on aligning learned representations with humans, in addition to learning the downstream task. We advocate that current representation learning approaches in robotics should be studied from the perspective of how well they accomplish the objective of representation alignment. We mathematically define the problem, identify its key desiderata,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;IM-IAD&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#35774;&#32622;&#35780;&#20272;&#20102;16&#20010;&#31639;&#27861;&#22312;7&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#36825;&#19968;&#39046;&#22495;&#30740;&#31350;&#30340;&#19981;&#35268;&#33539;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.13359</link><description>&lt;p&gt;
IM-IAD&#65306;&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;IM-IAD&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#35774;&#32622;&#35780;&#20272;&#20102;16&#20010;&#31639;&#27861;&#22312;7&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#36825;&#19968;&#39046;&#22495;&#30740;&#31350;&#30340;&#19981;&#35268;&#33539;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#65288;IAD&#65289;&#26159;&#24037;&#19994;&#21046;&#36896;&#20013;&#19968;&#39033;&#26032;&#20852;&#19988;&#37325;&#35201;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#20808;&#36827;&#30340;&#31639;&#27861;&#24050;&#32463;&#34987;&#21457;&#24067;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#23384;&#22312;&#24456;&#22823;&#24046;&#24322;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#32570;&#20047;&#23454;&#38469;&#30340;&#24037;&#19994;&#21046;&#36896;&#35774;&#32622;&#24456;&#21487;&#33021;&#38459;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21457;&#23637;&#21644;&#20351;&#29992;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;IAD&#26041;&#27861;&#23578;&#26410;&#32463;&#36807;&#31995;&#32479;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#24456;&#38590;&#20998;&#26512;&#23427;&#20204;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#20026;&#19981;&#21516;&#25110;&#29305;&#27530;&#24773;&#20917;&#32780;&#35774;&#35745;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24037;&#19994;&#21046;&#36896;&#35774;&#32622;&#26469;&#35780;&#20272;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20960;&#20010;&#26041;&#38754;&#65292;&#22914;&#21508;&#31181;&#30417;&#30563;&#32423;&#21035;&#65288;&#26080;&#30417;&#30563;vs&#21322;&#30417;&#30563;&#65289;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#22122;&#22768;&#26631;&#31614;&#12289;&#20869;&#23384;&#20351;&#29992;&#21644;&#25512;&#26029;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24039;&#22937;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#65288;IM-IAD&#65289;&#65292;&#35813;&#22522;&#20934;&#22312;&#32479;&#19968;&#30340;&#35774;&#32622;&#19979;&#21253;&#25324;&#20102;16&#20010;&#31639;&#27861;&#21644;7&#20010;&#20027;&#27969;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image anomaly detection (IAD) is an emerging and vital computer vision task in industrial manufacturing (IM). Recently many advanced algorithms have been published, but their performance deviates greatly. We realize that the lack of actual IM settings most probably hinders the development and usage of these methods in real-world applications. As far as we know, IAD methods are not evaluated systematically. As a result, this makes it difficult for researchers to analyze them because they are designed for different or special cases. To solve this problem, we first propose a uniform IM setting to assess how well these algorithms perform, which includes several aspects, i.e., various levels of supervision (unsupervised vs. semi-supervised), few-shot learning, continual learning, noisy labels, memory usage, and inference speed. Moreover, we skillfully build a comprehensive image anomaly detection benchmark (IM-IAD) that includes 16 algorithms on 7 mainstream datasets with uniform settings. 
&lt;/p&gt;</description></item><item><title>AutoPEFT&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;PEFT&#65288;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65289;&#37197;&#32622;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;PEFT&#27169;&#22359;&#21644;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#20013;&#65292;AutoPEFT&#34920;&#29616;&#20986;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12132</link><description>&lt;p&gt;
AutoPEFT&#65306;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#33258;&#21160;&#37197;&#32622;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12132
&lt;/p&gt;
&lt;p&gt;
AutoPEFT&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;PEFT&#65288;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65289;&#37197;&#32622;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;PEFT&#27169;&#22359;&#21644;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#20013;&#65292;AutoPEFT&#34920;&#29616;&#20986;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19987;&#38376;&#30340;&#24494;&#35843;&#29992;&#20110;&#19979;&#28216;NLP&#20219;&#21153;&#65292;&#20294;&#36825;&#26679;&#30340;&#36807;&#31243;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#26368;&#36817;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#36890;&#36807;&#26356;&#26032;&#27604;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#65288;FFT&#65289;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;PEFT&#37197;&#32622;&#26041;&#38754;&#20570;&#20986;&#26126;&#26234;&#30340;&#35774;&#35745;&#36873;&#25321;&#26159;&#19981;&#23481;&#26131;&#30340;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#21487;&#35843;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#29978;&#33267;&#26159;PEFT&#27169;&#22359;&#25554;&#20837;&#30340;&#22270;&#23618;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;&#25163;&#21160;&#35774;&#35745;&#37197;&#32622;&#24456;&#21487;&#33021;&#22312;&#24615;&#33021;&#25928;&#29575;&#26435;&#34913;&#26041;&#38754;&#26159;&#27425;&#20248;&#30340;&#12290;&#21463;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoPEFT&#26469;&#33258;&#21160;&#36873;&#25321;PEFT&#37197;&#32622;&#65306;&#39318;&#20808;&#35774;&#35745;&#20855;&#26377;&#22810;&#20010;&#20195;&#34920;&#24615;PEFT&#27169;&#22359;&#30340;&#34920;&#36798;&#37197;&#32622;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#21518;&#20351;&#29992;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#20302;&#25104;&#26412;&#30340;&#35774;&#32622;&#65292;&#20174;&#32780;&#21457;&#29616;&#20248;&#21270;&#20219;&#21153;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#30340;Pareto&#20248;&#21270;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#19978;&#35780;&#20272;&#20102;AutoPEFT&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#25163;&#21160;&#35774;&#35745;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained language models are widely used in downstream NLP tasks via task-specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating a much smaller number of parameters compared to full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations, such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: we first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimisation in a low-cost setup, we then disc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#27979;&#22788;&#29702;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;P4O&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#33258;&#36523;&#24863;&#35273;&#29366;&#24577;&#26469;&#26368;&#23567;&#21270;&#24778;&#24322;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#32047;&#31215;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2211.06236</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#39044;&#27979;&#22788;&#29702;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Deep Reinforcement Learning with Predictive Processing Proximal Policy Optimization. (arXiv:2211.06236v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06236
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#27979;&#22788;&#29702;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;P4O&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#33258;&#36523;&#24863;&#35273;&#29366;&#24577;&#26469;&#26368;&#23567;&#21270;&#24778;&#24322;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#32047;&#31215;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#27493;&#24120;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#19988;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20173;&#28982;&#19981;&#39640;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#22823;&#33041;&#33021;&#22815;&#20351;&#29992;&#26377;&#38480;&#30340;&#36164;&#28304;&#26377;&#25928;&#22320;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#20511;&#37492;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#26469;&#25913;&#36827;&#24403;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#39044;&#27979;&#22788;&#29702;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23427;&#35748;&#20026;&#20154;&#31867;&#22823;&#33041;&#20027;&#21160;&#23547;&#27714;&#26368;&#23567;&#21270;&#24778;&#24322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33021;&#22815;&#39044;&#27979;&#33258;&#36523;&#24863;&#35273;&#29366;&#24577;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#26368;&#23567;&#21270;&#24778;&#24322;&#65292;&#20174;&#32780;&#22312;&#32047;&#31215;&#22870;&#21169;&#19978;&#21462;&#24471;&#24040;&#22823;&#30340;&#25910;&#30410;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#27979;&#22788;&#29702;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;P4O&#65289;&#26234;&#33021;&#20307;&#65307;&#23427;&#26159;&#19968;&#20010;&#23558;&#39044;&#27979;&#22788;&#29702;&#24212;&#29992;&#21040;&#22522;&#20110;&#36882;&#24402;&#30340;PPO&#31639;&#27861;&#30340;&#28436;&#21592;&#25209;&#21028;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#23558;&#19990;&#30028;&#27169;&#22411;&#38598;&#25104;&#21040;&#20854;&#38544;&#34255;&#29366;&#24577;&#20013;&#12290;&#21363;&#20351;&#27809;&#26377;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;P4O&#19982;&#22522;&#32447;&#36882;&#24402;&#21464;&#20307;&#30456;&#27604;&#65292;&#20063;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in reinforcement learning (RL) often rely on massive compute resources and remain notoriously sample inefficient. In contrast, the human brain is able to efficiently learn effective control strategies using limited resources. This raises the question whether insights from neuroscience can be used to improve current RL methods. Predictive processing is a popular theoretical framework which maintains that the human brain is actively seeking to minimize surprise. We show that recurrent neural networks which predict their own sensory states can be leveraged to minimise surprise, yielding substantial gains in cumulative reward. Specifically, we present the Predictive Processing Proximal Policy Optimization (P4O) agent; an actor-critic reinforcement learning agent that applies predictive processing to a recurrent variant of the PPO algorithm by integrating a world model in its hidden state. Even without hyperparameter tuning, P4O significantly outperforms a baseline recurrent varian
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#35775;&#38382;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;3D&#28857;&#20113;&#20013;&#30340;&#23545;&#25239;&#28857;&#65292;&#25552;&#20379;&#20102;&#26080;&#30418;&#23376;&#25915;&#20987;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2210.14164</link><description>&lt;p&gt;
3D&#28857;&#20113;&#20998;&#31867;&#30340;&#26080;&#30418;&#23376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
No-Box Attacks on 3D Point Cloud Classification. (arXiv:2210.14164v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#35775;&#38382;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;3D&#28857;&#20113;&#20013;&#30340;&#23545;&#25239;&#28857;&#65292;&#25552;&#20379;&#20102;&#26080;&#30418;&#23376;&#25915;&#20987;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21508;&#31181;&#36755;&#20837;&#20449;&#21495;&#30340;&#20998;&#26512;&#65292;&#23545;&#25239;&#25915;&#20987;&#26500;&#25104;&#20102;&#20005;&#37325;&#25361;&#25112;&#12290;&#22312;3D&#28857;&#20113;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#35782;&#21035;&#22312;&#32593;&#32476;&#20915;&#31574;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#28857;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#22312;&#29983;&#25104;&#29616;&#26377;&#30340;&#23545;&#25239;&#25915;&#20987;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#26174;&#33879;&#24615;&#22270;&#26041;&#27861;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#23545;&#25239;&#25915;&#20987;&#20250;&#26174;&#33879;&#24433;&#21709;&#32593;&#32476;&#20915;&#31574;&#30340;&#28857;&#12290;&#36890;&#24120;&#65292;&#35782;&#21035;&#23545;&#25239;&#28857;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#35775;&#38382;&#65292;&#20197;&#30830;&#23450;&#21738;&#20123;&#28857;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#19981;&#35775;&#38382;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#23545;&#25239;&#28857;&#65292;&#36825;&#34987;&#31216;&#20026;&#8220;&#26080;&#30418;&#23376;&#8221;&#25915;&#20987;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;14&#20010;&#28857;&#20113;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#26469;&#26816;&#26597;&#36825;&#20123;&#29305;&#24449;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#23545;&#25239;&#28857;&#65292;&#20197;&#21450;&#21738;&#20123;&#29305;&#24449;&#23545;&#39044;&#27979;&#26368;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks pose serious challenges for deep neural network (DNN)-based analysis of various input signals. In the case of 3D point clouds, methods have been developed to identify points that play a key role in network decision, and these become crucial in generating existing adversarial attacks. For example, a saliency map approach is a popular method for identifying adversarial drop points, whose removal would significantly impact the network decision. Generally, methods for identifying adversarial points rely on the access to the DNN model itself to determine which points are critically important for the model's decision. This paper aims to provide a novel viewpoint on this problem, where adversarial points can be predicted without access to the target DNN model, which is referred to as a ``no-box'' attack. To this end, we define 14 point cloud features and use multiple linear regression to examine whether these features can be used for adversarial point prediction, and which
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#30452;&#27604;&#20195;&#25968;&#65292;&#25506;&#35752;&#20102;&#20445;&#25345;&#27604;&#25311;&#27604;&#20363;&#30340;&#20989;&#25968;&#30340;&#25968;&#23398;&#24615;&#36136;&#65292;&#23558;&#20854;&#19982;&#30452;&#27604;&#21516;&#24577;&#12289;&#21516;&#20313;&#21644;&#30452;&#27604;&#20989;&#23376;&#32852;&#31995;&#36215;&#26469;&#65292;&#20026;&#27169;&#25311;&#27604;&#20363;&#21644;&#27169;&#25311;&#25512;&#29702;&#30340;&#25968;&#23398;&#29702;&#35770;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2210.01751</link><description>&lt;p&gt;
&#30452;&#27604;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Proportional algebras. (arXiv:2210.01751v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#30452;&#27604;&#20195;&#25968;&#65292;&#25506;&#35752;&#20102;&#20445;&#25345;&#27604;&#25311;&#27604;&#20363;&#30340;&#20989;&#25968;&#30340;&#25968;&#23398;&#24615;&#36136;&#65292;&#23558;&#20854;&#19982;&#30452;&#27604;&#21516;&#24577;&#12289;&#21516;&#20313;&#21644;&#30452;&#27604;&#20989;&#23376;&#32852;&#31995;&#36215;&#26469;&#65292;&#20026;&#27169;&#25311;&#27604;&#20363;&#21644;&#27169;&#25311;&#25512;&#29702;&#30340;&#25968;&#23398;&#29702;&#35770;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#25311;&#27604;&#20363;&#26159;&#24418;&#24335;&#20026;"$a$&#23545;$b$&#65292;&#27491;&#22914;$c$&#23545;$d$"&#30340;&#34920;&#36798;&#24335;&#65292;&#26159;&#27169;&#25311;&#25512;&#29702;&#30340;&#26680;&#24515;&#65292;&#32780;&#27169;&#25311;&#25512;&#29702;&#21448;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#30452;&#27604;&#20195;&#25968;&#20316;&#20026;&#19968;&#31181;&#24102;&#26377;4&#20803;&#27604;&#25311;&#27604;&#20363;&#20851;&#31995;$a:b::c:d$&#28385;&#36275;&#19968;&#32452;&#36866;&#24403;&#20844;&#29702;&#30340;&#20195;&#25968;&#32467;&#26500;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#20445;&#25345;&#27604;&#25311;&#27604;&#20363;&#30340;&#20989;&#25968;&#24050;&#32463;&#34987;&#35777;&#26126;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#65292;&#32780;&#30740;&#31350;&#23427;&#20204;&#30340;&#25968;&#23398;&#24615;&#36136;&#23545;&#20110;&#29702;&#35299;&#27604;&#20363;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30452;&#27604;&#21516;&#24577;&#21450;&#20854;&#20851;&#32852;&#30340;&#21516;&#20313;&#21644;&#30452;&#27604;&#20989;&#23376;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#32852;&#12290;&#20174;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#19978;&#35828;&#65292;&#26412;&#25991;&#26159;&#36808;&#21521;&#27169;&#25311;&#27604;&#20363;&#21644;&#27169;&#25311;&#25512;&#29702;&#30340;&#25968;&#23398;&#29702;&#35770;&#30340;&#36827;&#19968;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical proportions are expressions of the form "$a$ is to $b$ what $c$ is to $d$" at the core of analogical reasoning which itself is at the core of artificial intelligence. This paper introduces proportional algebras as algebras endowed with a 4-ary analogical proportion relation $a:b::c:d$ satisfying a suitable set of axioms. Functions preserving analogical proportions have already proven to be of practical interest in artificial intelligence and studying their mathematical properties is essential for understanding proportions. We therefore introduce proportional homomorphisms and their associated congruences and proportional functors, and show that they are closely related notions. In a broader sense, this paper is a further step towards a mathematical theory of analogical proportions and analogical reasoning in general.
&lt;/p&gt;</description></item><item><title>&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;ICL&#23545;&#22810;&#31181;&#25200;&#21160;&#31867;&#22411;&#20855;&#26377;&#25935;&#24863;&#24615;&#65292;&#26631;&#31614;&#20559;&#24046;&#23548;&#33268;&#36807;&#21435;&#30340;&#30740;&#31350;&#20302;&#20272;&#20102;ICL&#30340;&#25935;&#24863;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#30340;&#25935;&#24863;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21576;&#29616;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;SenSel&#65292;&#23427;&#22312;&#25918;&#24323;&#25935;&#24863;&#39044;&#27979;&#20915;&#31574;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#24120;&#29992;&#22522;&#20934;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.07661</link><description>&lt;p&gt;
&#20851;&#20110;&#25935;&#24863;&#24615;&#19982;&#20934;&#30830;&#24615;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Relation between Sensitivity and Accuracy in In-context Learning. (arXiv:2209.07661v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07661
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;ICL&#23545;&#22810;&#31181;&#25200;&#21160;&#31867;&#22411;&#20855;&#26377;&#25935;&#24863;&#24615;&#65292;&#26631;&#31614;&#20559;&#24046;&#23548;&#33268;&#36807;&#21435;&#30340;&#30740;&#31350;&#20302;&#20272;&#20102;ICL&#30340;&#25935;&#24863;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#30340;&#25935;&#24863;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21576;&#29616;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;SenSel&#65292;&#23427;&#22312;&#25918;&#24323;&#25935;&#24863;&#39044;&#27979;&#20915;&#31574;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#24120;&#29992;&#22522;&#20934;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064; (In-context learning, ICL) &#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24120;&#24120;&#21463;&#21040;&#25552;&#31034;&#30340;&#36807;&#24230;&#25935;&#24863;&#24615;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;ICL&#23545;&#22810;&#31181;&#25200;&#21160;&#31867;&#22411;&#30340;&#25935;&#24863;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#31614;&#20559;&#24046;&#25513;&#30422;&#20102;&#30495;&#23454;&#30340;&#25935;&#24863;&#24615;&#65292;&#22240;&#27492;&#20043;&#21069;&#30340;&#30740;&#31350;&#21487;&#33021;&#22823;&#22823;&#20302;&#20272;&#20102;ICL&#30340;&#25935;&#24863;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ICL&#30340;&#25935;&#24863;&#24615;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;&#65306;&#23545;&#25200;&#21160;&#25935;&#24863;&#30340;&#39044;&#27979;&#26356;&#19981;&#23481;&#26131;&#27491;&#30830;&#12290;&#22312;&#36825;&#20123;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SenSel&#30340;&#23569;&#26679;&#26412;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#23545;&#25935;&#24863;&#39044;&#27979;&#30340;&#20351;&#29992;&#12290;&#22312;&#21313;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SenSel&#22312;&#25918;&#24323;&#39044;&#27979;&#20915;&#31574;&#19978;&#22987;&#32456;&#20248;&#20110;&#20004;&#31181;&#24120;&#29992;&#30340;&#22522;&#20110;&#32622;&#20449;&#24230;&#21644;&#22522;&#20110;&#29109;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) suffers from oversensitivity to the prompt, making it unreliable in real-world scenarios. We study the sensitivity of ICL with respect to multiple perturbation types. First, we find that label bias obscures the true sensitivity, and therefore prior work may have significantly underestimated ICL sensitivity. Second, we observe a strong negative correlation between ICL sensitivity and accuracy: predictions sensitive to perturbations are less likely to be correct. Motivated by these findings, we propose \textsc{SenSel}, a few-shot selective prediction method that abstains from sensitive predictions. Experiments on ten classification datasets show that \textsc{SenSel} consistently outperforms two commonly used confidence-based and entropy-based baselines on abstention decisions.
&lt;/p&gt;</description></item><item><title>SSL-WM&#26159;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#25152;&#26377;&#26435;&#30340;&#40657;&#30418;&#27700;&#21360;&#26041;&#27861;&#65292;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#22810;&#26679;&#19988;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.03563</link><description>&lt;p&gt;
SSL-WM&#65306;&#19968;&#31181;&#29992;&#20110;&#32463;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#30340;&#40657;&#30418;&#27700;&#21360;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by Self-supervised Learning. (arXiv:2209.03563v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03563
&lt;/p&gt;
&lt;p&gt;
SSL-WM&#26159;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#25152;&#26377;&#26435;&#30340;&#40657;&#30418;&#27700;&#21360;&#26041;&#27861;&#65292;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#22810;&#26679;&#19988;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#34987;&#24191;&#27867;&#29992;&#20110;&#20419;&#36827;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#21487;&#33021;&#31363;&#21462;&#36825;&#20123;SSL&#27169;&#22411;&#24182;&#23558;&#20854;&#21830;&#19994;&#21270;&#65292;&#22240;&#27492;&#39564;&#35777;SSL&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#25152;&#26377;&#26435;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#65288;&#20363;&#22914;&#22522;&#20110;&#21518;&#38376;&#30340;&#27700;&#21360;&#65289;&#37117;&#26159;&#20026;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#30340;&#65292;&#19981;&#33021;&#30452;&#25509;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;&#22312;&#23884;&#20837;&#27700;&#21360;&#26102;&#24050;&#30693;&#21644;&#21487;&#29992;&#30340;&#26159;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#30446;&#26631;&#26631;&#31614;&#65292;&#22312;SSL&#39046;&#22495;&#20013;&#36825;&#24182;&#19981;&#24635;&#26159;&#21487;&#33021;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#23884;&#20837;&#27700;&#21360;&#26399;&#38388;&#19979;&#28216;&#20219;&#21153;&#22810;&#26679;&#19988;&#19981;&#30693;&#36947;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSL-WM&#30340;&#26032;&#22411;&#40657;&#30418;&#27700;&#21360;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#39564;&#35777;SSL&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#12290;SSL-WM&#23558;&#21463;&#20445;&#25252;&#30340;&#32534;&#30721;&#22120;&#30340;&#24102;&#27700;&#21360;&#36755;&#20837;&#26144;&#23556;&#21040;&#19981;&#21464;&#34920;&#31034;s&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed tremendous success in Self-Supervised Learning (SSL), which has been widely utilized to facilitate various downstream tasks in Computer Vision (CV) and Natural Language Processing (NLP) domains. However, attackers may steal such SSL models and commercialize them for profit, making it crucial to verify the ownership of the SSL models. Most existing ownership protection solutions (e.g., backdoor-based watermarks) are designed for supervised learning models and cannot be used directly since they require that the models' downstream tasks and target labels be known and available during watermark embedding, which is not always possible in the domain of SSL. To address such a problem, especially when downstream tasks are diverse and unknown during watermark embedding, we propose a novel black-box watermarking solution, named SSL-WM, for verifying the ownership of SSL models. SSL-WM maps watermarked inputs of the protected encoders into an invariant representation s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#27491;&#24335;&#21512;&#21516;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#20010;&#20307;&#28608;&#21169;&#21644;&#38598;&#20307;&#28608;&#21169;&#20998;&#27495;&#23548;&#33268;&#30340;&#27425;&#20248;&#34892;&#20026;&#38382;&#39064;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#24341;&#20837;&#26377;&#32422;&#26463;&#30340;&#29366;&#24577;&#20381;&#36182;&#22870;&#21169;&#36716;&#31227;&#65292;&#23454;&#29616;&#20102;&#25152;&#26377;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#23376;&#21338;&#24328;&#23436;&#32654;&#22343;&#34913;&#34920;&#29616;&#20986;&#31038;&#20250;&#26368;&#20248;&#34892;&#20026;&#65292;&#24182;&#25552;&#21319;&#20102;&#31639;&#27861;&#30340;&#31038;&#20250;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.10469</link><description>&lt;p&gt;
&#20889;&#19979;&#26469;&#21543;&#65306;&#27491;&#24335;&#21512;&#21516;&#32531;&#35299;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31038;&#20250;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL. (arXiv:2208.10469v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#27491;&#24335;&#21512;&#21516;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#20010;&#20307;&#28608;&#21169;&#21644;&#38598;&#20307;&#28608;&#21169;&#20998;&#27495;&#23548;&#33268;&#30340;&#27425;&#20248;&#34892;&#20026;&#38382;&#39064;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#24341;&#20837;&#26377;&#32422;&#26463;&#30340;&#29366;&#24577;&#20381;&#36182;&#22870;&#21169;&#36716;&#31227;&#65292;&#23454;&#29616;&#20102;&#25152;&#26377;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#23376;&#21338;&#24328;&#23436;&#32654;&#22343;&#34913;&#34920;&#29616;&#20986;&#31038;&#20250;&#26368;&#20248;&#34892;&#20026;&#65292;&#24182;&#25552;&#21319;&#20102;&#31639;&#27861;&#30340;&#31038;&#20250;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26159;&#35757;&#32451;&#22312;&#20849;&#21516;&#29615;&#22659;&#20013;&#29420;&#31435;&#34892;&#21160;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#20010;&#20307;&#28608;&#21169;&#21644;&#38598;&#20307;&#28608;&#21169;&#20986;&#29616;&#20998;&#27495;&#26102;&#65292;&#23427;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#34892;&#20026;&#12290;&#20154;&#31867;&#22312;&#35299;&#20915;&#36825;&#20123;&#31038;&#20250;&#22256;&#22659;&#26041;&#38754;&#20855;&#26377;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#22312;MARL&#20013;&#22797;&#21046;&#36825;&#31181;&#21512;&#20316;&#34892;&#20026;&#23545;&#20110;&#33258;&#31169;&#30340;&#26234;&#33021;&#20307;&#26469;&#35828;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#32463;&#27982;&#23398;&#20013;&#27491;&#24335;&#21512;&#21516;&#30340;&#24605;&#24819;&#65292;&#20197;&#20811;&#26381;MARL&#20013;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#28608;&#21169;&#20998;&#27495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#36827;&#34892;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#26234;&#33021;&#20307;&#33258;&#24895;&#21516;&#24847;&#22312;&#39044;&#20808;&#35268;&#23450;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#26377;&#32422;&#26463;&#30340;&#29366;&#24577;&#20381;&#36182;&#22870;&#21169;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#29702;&#35770;&#30340;&#21644;&#23454;&#35777;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#22686;&#24378;&#20351;&#24471;&#25152;&#26377;&#23436;&#20840;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#23376;&#21338;&#24328;&#23436;&#32654;&#22343;&#34913;&#37117;&#34920;&#29616;&#20986;&#31038;&#20250;&#26368;&#20248;&#34892;&#20026;&#65292;&#21482;&#35201;&#21512;&#21516;&#31354;&#38388;&#36275;&#22815;&#20016;&#23500;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22686;&#24378;&#21518;&#30340;MARL&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#31038;&#20250;&#24615;&#33021;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#21338;&#24328;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning (MARL) is a powerful tool for training automated systems acting independently in a common environment. However, it can lead to sub-optimal behavior when individual incentives and group incentives diverge. Humans are remarkably capable at solving these social dilemmas. It is an open problem in MARL to replicate such cooperative behaviors in selfish agents. In this work, we draw upon the idea of formal contracting from economics to overcome diverging incentives between agents in MARL. We propose an augmentation to a Markov game where agents voluntarily agree to binding state-dependent transfers of reward, under pre-specified conditions. Our contributions are theoretical and empirical. First, we show that this augmentation makes all subgame-perfect equilibria of all fully observed Markov games exhibit socially optimal behavior, given a sufficiently rich space of contracts. Next, we complement our game-theoretic analysis by showing that state-of-the-art R
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#33258;&#21516;&#26500;&#36712;&#36947;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20102;&#25972;&#20010;&#29983;&#21629;&#36215;&#28304;&#26641;&#20013;&#30340;&#34507;&#30333;&#20114;&#20316;&#32452;&#32455;&#22810;&#26679;&#24615;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#29983;&#21629;&#22495;&#21644;&#38376;&#23646;&#32593;&#32476;&#30340;&#36712;&#36947;&#20351;&#29992;&#27010;&#20917;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;&#36825;&#19968;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#20102;&#35299;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#30340;&#28436;&#21270;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2203.00999</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#33258;&#21516;&#26500;&#36712;&#36947;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25972;&#20010;&#29983;&#21629;&#36215;&#28304;&#26641;&#20013;&#30340;&#34507;&#30333;&#20114;&#20316;&#32452;&#32455;&#22810;&#26679;&#24615;&#36827;&#34892;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
DeepAutoPIN: An automorphism orbits based deep neural network for characterizing the organizational diversity of protein interactomes across the tree of life. (arXiv:2203.00999v2 [q-bio.MN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#33258;&#21516;&#26500;&#36712;&#36947;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20102;&#25972;&#20010;&#29983;&#21629;&#36215;&#28304;&#26641;&#20013;&#30340;&#34507;&#30333;&#20114;&#20316;&#32452;&#32455;&#22810;&#26679;&#24615;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#29983;&#21629;&#22495;&#21644;&#38376;&#23646;&#32593;&#32476;&#30340;&#36712;&#36947;&#20351;&#29992;&#27010;&#20917;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;&#36825;&#19968;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#20102;&#35299;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#30340;&#28436;&#21270;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26497;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#32321;&#30427;&#30340;&#29983;&#21629;&#24418;&#24335;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#28041;&#21450;&#21040;&#30456;&#20114;&#20316;&#29992;&#30340;&#32452;&#20998;&#34507;&#30333;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#34920;&#24449;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#65288;PINs&#65289;&#22312;&#25972;&#20010;&#29983;&#21629;&#36215;&#28304;&#26641;&#20013;&#30340;&#28436;&#21270;&#30340;&#32452;&#32455;&#21407;&#21017;&#30446;&#21069;&#36824;&#22823;&#37096;&#20998;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23646;&#20110;16&#20010;&#38376;&#30340;4,738&#20010;PINs&#65292;&#20197;&#21457;&#29616;&#38376;&#29305;&#26377;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#24182;&#26816;&#26597;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#19978;&#26159;&#21542;&#23384;&#22312;&#19968;&#20123;&#28436;&#21270;&#32422;&#26463;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#32593;&#32476;&#33410;&#28857;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#22823;&#23567;&#20026;2-5&#30340;&#22270;&#24418;&#20013;&#20986;&#29616;&#30340;&#33258;&#21516;&#26500;&#36712;&#36947;&#30340;&#39057;&#29575;&#36827;&#34892;&#24402;&#19968;&#21270;&#26469;&#30740;&#31350;&#12290;&#25105;&#20204;&#25253;&#21578;&#65292;&#29983;&#21629;&#30340;&#19977;&#20010;&#22495;&#25152;&#23646;&#32593;&#32476;&#30340;&#36712;&#36947;&#20351;&#29992;&#27010;&#20917;&#65288;OUPs&#65289;&#19981;&#20165;&#22312;&#22495;&#23618;&#38754;&#19978;&#26377;&#26126;&#26174;&#24046;&#24322;&#65292;&#32780;&#19988;&#22312;&#38376;&#30340;&#35268;&#27169;&#19978;&#20063;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;&#36890;&#36807;&#25972;&#21512;&#19982;&#34507;&#30333;&#23478;&#26063;&#12289;&#32467;&#26500;&#22495;&#12289;&#20122;&#32454;&#32990;&#23450;&#20301;&#12289;&#22522;&#22240;&#26412;&#20307;&#35770;&#21644;&#36890;&#36335;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36830;&#25509;&#27169;&#24335;&#21487;&#33021;&#26159;PINs&#22312;&#36827;&#21270;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The enormous diversity of life forms thriving in drastically different environmental milieus involves a complex interplay among constituent proteins interacting with each other. However, the organizational principles characterizing the evolution of protein interaction networks (PINs) across the tree of life are largely unknown. Here we study 4,738 PINs belonging to 16 phyla to discover phyla-specific architectural features and examine if there are some evolutionary constraints imposed on the networks' topologies. We utilized positional information of a network's nodes by normalizing the frequencies of automorphism orbits appearing in graphlets of sizes 2-5. We report that orbit usage profiles (OUPs) of networks belonging to the three domains of life are contrastingly different not only at the domain level but also at the scale of phyla. Integrating the information related to protein families, domains, subcellular location, gene ontology, and pathways, our results indicate that wiring p
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#21327;&#26041;&#24046;&#39033;&#21644;&#28155;&#21152;&#36845;&#20195;&#24402;&#19968;&#21270;&#23618;&#65292;&#21152;&#36895;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2109.00783</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26102;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Computer Vision Self-supervised Learning Methods on Time Series. (arXiv:2109.00783v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.00783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#21327;&#26041;&#24046;&#39033;&#21644;&#28155;&#21152;&#36845;&#20195;&#24402;&#19968;&#21270;&#23618;&#65292;&#21152;&#36895;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#30446;&#21069;&#20027;&#27969;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#22823;&#22810;&#22522;&#20110;Siamese&#32593;&#32476;&#26550;&#26500;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#35774;&#32622;&#65292;&#20197;&#36991;&#20813;&#29305;&#24449;&#23849;&#28291;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36825;&#20123;&#35745;&#31639;&#26426;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#22312;&#19981;&#21516;&#27169;&#24577;&#65288;&#21363;&#26102;&#38388;&#24207;&#21015;&#65289;&#19978;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;UCR&#21644;UEA&#26723;&#26696;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#21516;&#26679;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26368;&#36817;&#25552;&#20986;&#30340;VICReg&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;VICReg&#20013;&#25552;&#20986;&#30340;&#19968;&#20010;&#8220;&#21327;&#26041;&#24046;&#8221;&#39033;&#65292;&#21516;&#26102;&#22312;&#26550;&#26500;&#30340;&#22836;&#37096;&#22686;&#21152;&#20102;&#19968;&#20010;&#36845;&#20195;&#24402;&#19968;&#21270;&#23618;&#65292;&#21152;&#36895;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has had great success in both computer vision. Most of the current mainstream computer vision SSL frameworks are based on Siamese network architecture. These approaches often rely on cleverly crafted loss functions and training setups to avoid feature collapse. In this study, we evaluate if those computer-vision SSL frameworks are also effective on a different modality (\textit{i.e.,} time series). The effectiveness is experimented and evaluated on the UCR and UEA archives, and we show that the computer vision SSL frameworks can be effective even for time series. In addition, we propose a new method that improves on the recently proposed VICReg method. Our method improves on a \textit{covariance} term proposed in VICReg, and in addition we augment the head of the architecture by an iterative normalization layer that accelerates the convergence of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#20808;&#23545;&#21333;&#22836;&#31561;&#20215;&#24615;&#36827;&#34892;&#20102;&#35821;&#20041;&#21270;&#30340;&#30740;&#31350;&#65292;&#32473;&#20986;&#20102;&#24517;&#35201;&#26465;&#20214;&#65307;&#20854;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#23436;&#20840;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#20844;&#24335;&#36716;&#25442;&#20026;&#21333;&#22836;&#24418;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36827;&#34892;&#36951;&#24536;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2009.07497</link><description>&lt;p&gt;
&#21333;&#22836;&#32988;&#36807;&#21452;&#22836;&#65306;&#21629;&#39064;&#26126;&#30830;Horn&#36951;&#24536;&#30340;&#22810;&#39033;&#24335;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
One head is better than two: a polynomial restriction for propositional definite Horn forgetting. (arXiv:2009.07497v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.07497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#20808;&#23545;&#21333;&#22836;&#31561;&#20215;&#24615;&#36827;&#34892;&#20102;&#35821;&#20041;&#21270;&#30340;&#30740;&#31350;&#65292;&#32473;&#20986;&#20102;&#24517;&#35201;&#26465;&#20214;&#65307;&#20854;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#23436;&#20840;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#20844;&#24335;&#36716;&#25442;&#20026;&#21333;&#22836;&#24418;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36827;&#34892;&#36951;&#24536;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#36951;&#24536;&#21363;&#20351;&#22312;&#21629;&#39064;Horn&#20844;&#24335;&#30340;&#31616;&#21333;&#24773;&#20917;&#19979;&#20063;&#26159;p&#23436;&#20840;&#30340;&#65292;&#21487;&#33021;&#20250;&#25351;&#25968;&#32423;&#22686;&#21152;&#20854;&#22823;&#23567;&#12290;&#19968;&#31181;&#36951;&#24536;&#30340;&#26041;&#24335;&#26159;&#29992;&#21253;&#21547;&#21464;&#37327;&#35201;&#36951;&#24536;&#30340;&#27599;&#20010;&#23376;&#21477;&#26367;&#25442;&#23427;&#20204;&#30340;&#22836;&#37096;&#21464;&#37327;&#12290;&#22312;&#21333;&#22836;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#38656;&#35201;&#22810;&#39033;&#24335;&#26102;&#38388;&#65306;&#27599;&#20010;&#21464;&#37327;&#26368;&#22810;&#26159;&#19968;&#20010;&#23376;&#21477;&#30340;&#22836;&#37096;&#12290;&#26377;&#20123;&#20844;&#24335;&#19981;&#26159;&#21333;&#22836;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#31616;&#21270;&#26469;&#23454;&#29616;&#12290;&#23427;&#20204;&#26159;&#21333;&#22836;&#31561;&#20215;&#30340;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#39033;&#36129;&#29486;&#26159;&#23545;&#21333;&#22836;&#31561;&#20215;&#24615;&#36827;&#34892;&#35821;&#20041;&#29305;&#24449;&#21270;&#30340;&#30740;&#31350;&#12290;&#32473;&#20986;&#20102;&#20004;&#20010;&#24517;&#35201;&#26465;&#20214;&#12290;&#24403;&#20844;&#24335;&#19981;&#31561;&#20215;&#26102;&#65292;&#23427;&#20204;&#26159;&#20805;&#20998;&#26465;&#20214;&#65306;&#23427;&#21482;&#20250;&#20351;&#20004;&#20010;&#21464;&#37327;&#38598;&#31561;&#20215;&#65292;&#22914;&#26524;&#23427;&#20204;&#20063;&#31561;&#20215;&#20110;&#23427;&#20204;&#30340;&#20132;&#38598;&#12290;&#25152;&#26377;&#38750;&#24490;&#29615;&#20844;&#24335;&#37117;&#26159;&#19981;&#31561;&#20215;&#30340;&#12290;&#26412;&#25991;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#20010;&#29992;&#20110;&#23558;&#20844;&#24335;&#36716;&#25442;&#20026;&#21333;&#22836;&#30340;&#19981;&#23436;&#20840;&#31639;&#27861;&#12290;&#22914;&#26524;&#25104;&#21151;&#65292;&#36951;&#24536;&#23558;&#33021;&#22815;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36827;&#34892;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical forgetting is \np-complete even in the simple case of propositional Horn formulae, and may exponentially increase their size. A way to forget is to replace each variable to forget with the body of each clause whose head is the variable. It takes polynomial time in the single-head case: each variable is at most the head of a clause. Some formulae are not single-head but can be made so to simplify forgetting. They are single-head equivalent. The first contribution of this article is the study of a semantical characterization of single-head equivalence. Two necessary conditions are given. They are sufficient when the formula is inequivalent: it makes two sets of variables equivalent only if they are also equivalent to their intersection. All acyclic formulae are inequivalent. The second contribution of this article is an incomplete algorithm for turning a formula single-head. In case of success, forgetting becomes possible in polynomial time and produces a polynomial-size formula,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31454;&#36187;&#35299;&#20915;&#26041;&#26696;&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#23613;&#21487;&#33021;&#23569;&#30340;&#26597;&#35810;&#36793;&#32536;&#30340;&#26041;&#24335;&#25214;&#21040;&#26368;&#20339;&#39030;&#28857;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/1611.06189</link><description>&lt;p&gt;
&#31454;&#36187;&#35299;&#20915;&#26041;&#26696;&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Query Complexity of Tournament Solutions. (arXiv:1611.06189v4 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1611.06189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31454;&#36187;&#35299;&#20915;&#26041;&#26696;&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#23613;&#21487;&#33021;&#23569;&#30340;&#26597;&#35810;&#36793;&#32536;&#30340;&#26041;&#24335;&#25214;&#21040;&#26368;&#20339;&#39030;&#28857;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20013;&#65292;&#30740;&#31350;&#22914;&#20309;&#25214;&#21040;&#31454;&#36187;&#30340;&#26368;&#20339;&#39030;&#28857;&#38598;&#21512;&#26159;&#19968;&#20010;&#26082;&#23450;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#20174;&#32473;&#23450;&#30340;&#33647;&#29289;&#38598;&#21512;&#20013;&#36873;&#25321;&#26368;&#20339;&#33647;&#29289;&#65292;&#21482;&#26377;&#38544;&#24335;&#32473;&#20986;&#31454;&#36187;&#36793;&#32536;&#65292;&#24182;&#19988;&#20102;&#35299;&#36793;&#32536;&#30340;&#26041;&#21521;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24076;&#26395;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#26597;&#35810;&#36793;&#32536;&#30340;&#26041;&#24335;&#25214;&#21040;&#26368;&#20339;&#39030;&#28857;&#38598;&#21512;&#12290;&#26412;&#25991;&#31934;&#30830;&#30740;&#31350;&#20102;&#24120;&#29992;&#31454;&#36187;&#35299;&#20915;&#26041;&#26696;&#30340;&#36825;&#20010;&#38382;&#39064;&#65306;&#36890;&#36807;&#26597;&#35810;&#23613;&#21487;&#33021;&#23569;&#30340;&#36793;&#32536;&#65292;&#32473;&#23450;&#19968;&#20010;&#31454;&#36187;T&#30340;&#36793;&#32536;&#35775;&#38382;&#65292;&#25214;&#21040;$f(T)$&#30340;&#20540;&#65292;&#20854;&#20013;$f$&#26159;&#31454;&#36187;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#31454;&#36187;&#20013;&#25214;&#21040;&#30001;&#24247;&#22810;&#22622;&#19981;&#36133;&#32773;&#32452;&#25104;&#30340;&#38598;&#21512;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;$2n-\lfl
&lt;/p&gt;
&lt;p&gt;
A directed graph where there is exactly one edge between every pair of vertices is called a {\em tournament}. Finding the "best" set of vertices of a tournament is a well studied problem in social choice theory. A {\em tournament solution} takes a tournament as input and outputs a subset of vertices of the input tournament. However, in many applications, for example, choosing the best set of drugs from a given set of drugs, the edges of the tournament are given only implicitly and knowing the orientation of an edge is costly. In such scenarios, we would like to know the best set of vertices (according to some tournament solution) by "querying" as few edges as possible. We, in this paper, precisely study this problem for commonly used tournament solutions: given an oracle access to the edges of a tournament T, find $f(T)$ by querying as few edges as possible, for a tournament solution f. We first show that the set of Condorcet non-losers in a tournament can be found by querying $2n-\lfl
&lt;/p&gt;</description></item></channel></rss>