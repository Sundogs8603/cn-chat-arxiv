<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#22240;&#26524;&#32467;&#26500;&#20272;&#35745;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#65288;GIN&#65289;&#26465;&#20214;&#65292;&#24182;&#32473;&#20986;&#20102;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#20013;&#28385;&#36275;GIN&#26465;&#20214;&#30340;&#22270;&#24418;&#21028;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.06718</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#22240;&#26524;&#32467;&#26500;&#20272;&#35745;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables. (arXiv:2308.06718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06718
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#22240;&#26524;&#32467;&#26500;&#20272;&#35745;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#65288;GIN&#65289;&#26465;&#20214;&#65292;&#24182;&#32473;&#20986;&#20102;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#20013;&#28385;&#36275;GIN&#26465;&#20214;&#30340;&#22270;&#24418;&#21028;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#28508;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#21253;&#25324;&#23450;&#20301;&#28508;&#21464;&#37327;&#24182;&#30830;&#23450;&#23427;&#20204;&#30340;&#25968;&#37327;&#65292;&#20197;&#21450;&#35782;&#21035;&#28508;&#21464;&#37327;&#21644;&#35266;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21253;&#21547;&#28508;&#21464;&#37327;&#30340;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#30340;&#24191;&#20041;&#29420;&#31435;&#22122;&#22768;&#65288;GIN&#65289;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#24314;&#31435;&#20102;&#26576;&#20123;&#27979;&#37327;&#21464;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;&#19982;&#20854;&#20182;&#27979;&#37327;&#21464;&#37327;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#20004;&#20010;&#35266;&#27979;&#38543;&#26426;&#21521;&#37327; $\bf{Y}$ &#21644; $\bf{Z}$&#65292;&#24403;&#19988;&#20165;&#24403; $\omega^{\intercal}\mathbf{Y}$ &#21644; $\mathbf{Z}$ &#26159;&#29420;&#31435;&#30340;&#26102;&#65292;GIN &#25104;&#31435;&#65292;&#20854;&#20013; $\omega$ &#26159;&#30001; $\mathbf{Y}$ &#21644; $\mathbf{Z}$ &#20043;&#38388;&#30340;&#20132;&#21449;&#21327;&#26041;&#24046;&#30830;&#23450;&#30340;&#38750;&#38646;&#21442;&#25968;&#21521;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#32447;&#24615;&#38750;&#39640;&#26031;&#26080;&#29615;&#22240;&#26524;&#27169;&#22411;&#20013; GIN &#26465;&#20214;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#22270;&#24418;&#21028;&#25454;&#12290;&#31616;&#35328;&#20043;&#65292;GIN &#24847;&#21619;&#30528;&#23384;&#22312;&#19968;&#20010;&#22806;&#28304;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We investigate the challenging task of learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables. To address this, we propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. Specifically, for two observed random vectors $\bf{Y}$ and $\bf{Z}$, GIN holds if and only if $\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models. Roughly speaking, GIN implies the existence of an exogenous se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#33258;&#31169;&#23398;&#20064;&#20195;&#29702;&#21644;&#23398;&#20064;&#22996;&#25176;&#20154;&#20043;&#38388;&#30340;&#37325;&#22797;&#23545;&#33258;&#36873;&#28216;&#25103;&#65292;&#25506;&#32034;&#22914;&#20309;&#20272;&#35745;&#21644;&#28608;&#21169;&#20855;&#26377;&#38544;&#34255;&#22870;&#21169;&#30340;&#19981;&#23436;&#20840;&#30693;&#35782;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.06717</link><description>&lt;p&gt;
&#20272;&#35745;&#21644;&#28608;&#21169;&#20855;&#26377;&#38544;&#34255;&#22870;&#21169;&#30340;&#19981;&#23436;&#20840;&#30693;&#35782;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards. (arXiv:2308.06717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#33258;&#31169;&#23398;&#20064;&#20195;&#29702;&#21644;&#23398;&#20064;&#22996;&#25176;&#20154;&#20043;&#38388;&#30340;&#37325;&#22797;&#23545;&#33258;&#36873;&#28216;&#25103;&#65292;&#25506;&#32034;&#22914;&#20309;&#20272;&#35745;&#21644;&#28608;&#21169;&#20855;&#26377;&#38544;&#34255;&#22870;&#21169;&#30340;&#19981;&#23436;&#20840;&#30693;&#35782;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#28608;&#21169;&#25552;&#20379;&#32773;&#65288;&#21363;&#22996;&#25176;&#20154;&#65289;&#36890;&#24120;&#26080;&#27861;&#35266;&#23519;&#21463;&#21040;&#28608;&#21169;&#30340;&#20195;&#29702;&#30340;&#22870;&#21169;&#23454;&#29616;&#24773;&#20917;&#65292;&#36825;&#19982;&#35768;&#22810;&#20808;&#21069;&#30740;&#31350;&#36807;&#30340;&#22996;&#25176;&#20154;-&#20195;&#29702;&#27169;&#22411;&#24418;&#25104;&#20102;&#23545;&#27604;&#12290;&#36825;&#31181;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#20351;&#22996;&#25176;&#20154;&#20165;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#30340;&#20915;&#31574;&#23601;&#35201;&#22987;&#32456;&#20272;&#35745;&#20195;&#29702;&#30340;&#26410;&#30693;&#22870;&#21169;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#24403;&#20195;&#29702;&#38656;&#35201;&#23398;&#20064;&#33258;&#24049;&#30340;&#22870;&#21169;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#36825;&#31181;&#22797;&#26434;&#24773;&#20917;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#34987;&#35266;&#23519;&#21040;&#65292;&#20174;&#21487;&#20877;&#29983;&#33021;&#28304;&#20648;&#23384;&#21512;&#21516;&#21040;&#20010;&#24615;&#21270;&#21307;&#30103;&#20445;&#20581;&#28608;&#21169;&#12290;&#22240;&#27492;&#65292;&#23427;&#19981;&#20165;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#29702;&#35770;&#38382;&#39064;&#65292;&#32780;&#19988;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#33258;&#31169;&#23398;&#20064;&#20195;&#29702;&#21644;&#23398;&#20064;&#22996;&#25176;&#20154;&#20043;&#38388;&#30340;&#37325;&#22797;&#23545;&#33258;&#36873;&#28216;&#25103;&#12290;&#20195;&#29702;&#35299;&#20915;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#38382;&#39064;&#65292;&#20197;&#26368;&#22823;&#21270;&#20182;&#20204;&#39044;&#26399;&#30340;&#22870;&#21169;&#21644;&#28608;&#21169;&#12290;&#38500;&#20195;&#29702;&#30340;&#23398;&#20064;&#22806;&#65292;&#22996;&#25176;&#20154;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#24182;&#34892;&#31639;&#27861;&#65292;&#24182;&#38754;&#20020;&#19968;&#20010;&#25240;&#20013;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
In practice, incentive providers (i.e., principals) often cannot observe the reward realizations of incentivized agents, which is in contrast to many principal-agent models that have been previously studied. This information asymmetry challenges the principal to consistently estimate the agent's unknown rewards by solely watching the agent's decisions, which becomes even more challenging when the agent has to learn its own rewards. This complex setting is observed in various real-life scenarios ranging from renewable energy storage contracts to personalized healthcare incentives. Hence, it offers not only interesting theoretical questions but also wide practical relevance. This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal. The agent tackles a multi-armed bandit (MAB) problem to maximize their expected reward plus incentive. On top of the agent's learning, the principal trains a parallel algorithm and faces a trade-of
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#23384;&#22312;&#20998;&#24067;&#22806;&#33410;&#28857;&#65288;OOD nodes&#65289;&#30340;&#22330;&#26223;&#12290;&#20316;&#32773;&#23450;&#20041;&#20102;&#20998;&#24067;&#22806;&#33410;&#28857;&#65292;&#24182;&#35774;&#23450;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#26816;&#27979;&#19981;&#23646;&#20110;&#24050;&#30693;&#20998;&#24067;&#30340;&#33410;&#28857;&#65292;&#24182;&#23545;&#21097;&#20313;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#20182;&#20204;&#36890;&#36807;&#25552;&#20986;&#30340;Out-of-Distribution Graph Attention Network (OODGAT)&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06714</link><description>&lt;p&gt;
&#23384;&#22312;&#20998;&#24067;&#22806;&#33410;&#28857;&#30340;&#22270;&#19978;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning on Graphs with Out-of-Distribution Nodes. (arXiv:2308.06714v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06714
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#23384;&#22312;&#20998;&#24067;&#22806;&#33410;&#28857;&#65288;OOD nodes&#65289;&#30340;&#22330;&#26223;&#12290;&#20316;&#32773;&#23450;&#20041;&#20102;&#20998;&#24067;&#22806;&#33410;&#28857;&#65292;&#24182;&#35774;&#23450;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#26816;&#27979;&#19981;&#23646;&#20110;&#24050;&#30693;&#20998;&#24067;&#30340;&#33410;&#28857;&#65292;&#24182;&#23545;&#21097;&#20313;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#20182;&#20204;&#36890;&#36807;&#25552;&#20986;&#30340;Out-of-Distribution Graph Attention Network (OODGAT)&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#22312;&#22270;&#19978;&#25191;&#34892;&#39044;&#27979;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;GNNs&#22312;&#19982;&#22270;&#30456;&#20851;&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#23384;&#22312;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#21364;&#24456;&#23569;&#24341;&#36215;&#27880;&#24847;&#12290;&#20511;&#37492;CV&#21644;NLP&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#23558;OOD&#33410;&#28857;&#23450;&#20041;&#20026;&#35757;&#32451;&#38598;&#20013;&#26410;&#35265;&#30340;&#33410;&#28857;&#26631;&#31614;&#12290;&#30001;&#20110;&#35768;&#22810;&#32593;&#32476;&#26159;&#30001;&#31243;&#24207;&#33258;&#21160;&#29983;&#25104;&#30340;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#24448;&#24448;&#23384;&#22312;&#22122;&#38899;&#65292;&#24182;&#19988;&#21487;&#33021;&#21253;&#21547;&#26469;&#33258;&#26410;&#30693;&#20998;&#24067;&#30340;&#33410;&#28857;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#23384;&#22312;&#20998;&#24067;&#22806;&#33410;&#28857;&#30340;&#22270;&#19978;&#23398;&#20064;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23436;&#25104;&#20004;&#20010;&#20219;&#21153;&#65306;1&#65289;&#26816;&#27979;&#19981;&#23646;&#20110;&#24050;&#30693;&#20998;&#24067;&#30340;&#33410;&#28857;&#65307;2&#65289;&#23558;&#21097;&#20313;&#30340;&#33410;&#28857;&#20998;&#31867;&#20026;&#24050;&#30693;&#31867;&#21035;&#20043;&#19968;&#12290;&#25105;&#20204;&#35777;&#26126;&#22270;&#20013;&#30340;&#36830;&#25509;&#27169;&#24335;&#23545;&#20110;&#24322;&#24120;&#20540;&#26816;&#27979;&#20855;&#26377;&#20449;&#24687;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;Out-of-Distribution Graph Attention Network (OODGAT)&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are state-of-the-art models for performing prediction tasks on graphs. While existing GNNs have shown great performance on various tasks related to graphs, little attention has been paid to the scenario where out-of-distribution (OOD) nodes exist in the graph during training and inference. Borrowing the concept from CV and NLP, we define OOD nodes as nodes with labels unseen from the training set. Since a lot of networks are automatically constructed by programs, real-world graphs are often noisy and may contain nodes from unknown distributions. In this work, we define the problem of graph learning with out-of-distribution nodes. Specifically, we aim to accomplish two tasks: 1) detect nodes which do not belong to the known distribution and 2) classify the remaining nodes to be one of the known classes. We demonstrate that the connection patterns in graphs are informative for outlier detection, and propose Out-of-Distribution Graph Attention Network (OODGAT)
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21512;&#25104;&#20266;&#35013;&#25968;&#25454;&#20197;&#25913;&#21892;&#23545;&#33258;&#28982;&#22330;&#26223;&#20013;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#20266;&#35013;&#22270;&#20687;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06701</link><description>&lt;p&gt;
&#20266;&#35013;&#22270;&#20687;&#21512;&#25104;&#26159;&#25552;&#39640;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection. (arXiv:2308.06701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06701
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21512;&#25104;&#20266;&#35013;&#25968;&#25454;&#20197;&#25913;&#21892;&#23545;&#33258;&#28982;&#22330;&#26223;&#20013;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#20266;&#35013;&#22270;&#20687;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#20837;&#33258;&#28982;&#22330;&#26223;&#30340;&#20266;&#35013;&#29289;&#20307;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#21644;&#21512;&#25104;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#28982;&#32780;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#65292;&#35813;&#30740;&#31350;&#35838;&#39064;&#19968;&#30452;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21512;&#25104;&#20266;&#35013;&#25968;&#25454;&#20197;&#22686;&#24378;&#23545;&#33258;&#28982;&#22330;&#26223;&#20013;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#20266;&#35013;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#29616;&#26377;&#30340;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20266;&#35013;&#29615;&#22659;&#29983;&#25104;&#22120;&#65292;&#30001;&#20266;&#35013;&#20998;&#24067;&#20998;&#31867;&#22120;&#36827;&#34892;&#30417;&#30563;&#65292;&#21512;&#25104;&#20266;&#35013;&#22270;&#20687;&#65292;&#28982;&#21518;&#23558;&#20854;&#36755;&#20837;&#25105;&#20204;&#30340;&#29983;&#25104;&#22120;&#20197;&#25193;&#23637;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;COD10k&#12289;CAMO&#21644;CHAMELEON&#65289;&#19978;&#30340;&#25928;&#26524;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25913;&#21892;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camouflage images, which can be used to train existing object detection models. Specifically, we use a camouflage environment generator supervised by a camouflage distribution classifier to synthesize the camouflage images, which are then fed into our generator to expand the dataset. Our framework outperforms the current state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON), demonstrating its effectiveness in improving camouflaged object detection. This approach can serve as a plug-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACO&#30340;&#27169;&#24577;&#23545;&#25239;&#21644;&#23545;&#27604;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#30340;&#27169;&#24577;&#32570;&#22833;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#29983;&#25104;&#32570;&#22833;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36328;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#25913;&#21892;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06696</link><description>&lt;p&gt;
MACO: &#19968;&#31181;&#29992;&#20110;&#27169;&#24577;&#32570;&#22833;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#30340;&#27169;&#24577;&#23545;&#25239;&#21644;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MACO: A Modality Adversarial and Contrastive Framework for Modality-missing Multi-modal Knowledge Graph Completion. (arXiv:2308.06696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACO&#30340;&#27169;&#24577;&#23545;&#25239;&#21644;&#23545;&#27604;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#30340;&#27169;&#24577;&#32570;&#22833;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#29983;&#25104;&#32570;&#22833;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36328;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#25913;&#21892;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;MMKGC&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;MMKGC&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#23454;&#20307;&#20449;&#24687;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;KGC&#65289;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#22312;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#20013;&#21457;&#29616;&#26410;&#35266;&#23519;&#21040;&#30340;&#19977;&#20803;&#32452;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24378;&#35843;&#35774;&#35745;&#20248;&#38597;&#30340;KGC&#27169;&#22411;&#20197;&#20419;&#36827;&#27169;&#24577;&#20132;&#20114;&#65292;&#24573;&#30053;&#20102;&#30693;&#35782;&#22270;&#20013;&#32570;&#22833;&#27169;&#24577;&#30340;&#30495;&#23454;&#38382;&#39064;&#12290;&#32570;&#22833;&#30340;&#27169;&#24577;&#20449;&#24687;&#38459;&#30861;&#20102;&#27169;&#24577;&#20132;&#20114;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#23545;&#25239;&#21644;&#23545;&#27604;&#26694;&#26550;&#65288;MACO&#65289;&#26469;&#35299;&#20915;MMKGC&#20013;&#30340;&#27169;&#24577;&#32570;&#22833;&#38382;&#39064;&#12290;MACO&#36890;&#36807;&#23545;&#25239;&#24615;&#22320;&#35757;&#32451;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#26469;&#29983;&#25104;&#21487;&#20197;&#25972;&#21512;&#21040;MMKGC&#27169;&#22411;&#20013;&#30340;&#32570;&#22833;&#27169;&#24577;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#26469;&#25552;&#39640;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#22312;&#20844;&#20849;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#21644;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen significant advancements in multi-modal knowledge graph completion (MMKGC). MMKGC enhances knowledge graph completion (KGC) by integrating multi-modal entity information, thereby facilitating the discovery of unobserved triples in the large-scale knowledge graphs (KGs). Nevertheless, existing methods emphasize the design of elegant KGC models to facilitate modality interaction, neglecting the real-life problem of missing modalities in KGs. The missing modality information impedes modal interaction, consequently undermining the model's performance. In this paper, we propose a modality adversarial and contrastive framework (MACO) to solve the modality-missing problem in MMKGC. MACO trains a generator and discriminator adversarially to generate missing modality features that can be incorporated into the MMKGC model. Meanwhile, we design a cross-modal contrastive loss to improve the performance of the generator. Experiments on public benchmarks with further explorati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#22270;&#21644;&#38376;&#25511;&#34701;&#21512;&#30340;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#20004;&#31181;&#31867;&#22411;&#30340;&#22270;&#26469;&#29983;&#25104;&#22810;&#26679;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#38376;&#25511;&#34701;&#21512;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#35270;&#39057;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.06685</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#22270;&#21644;&#38376;&#25511;&#34701;&#21512;&#30340;&#32858;&#21512;&#29305;&#24449;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Video Captioning with Aggregated Features Based on Dual Graphs and Gated Fusion. (arXiv:2308.06685v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#22270;&#21644;&#38376;&#25511;&#34701;&#21512;&#30340;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#20004;&#31181;&#31867;&#22411;&#30340;&#22270;&#26469;&#29983;&#25104;&#22810;&#26679;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#38376;&#25511;&#34701;&#21512;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#35270;&#39057;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#20934;&#30830;&#30340;&#33258;&#28982;&#35821;&#35328;&#32763;&#35793;&#35270;&#39057;&#20869;&#23481;&#12290;&#30001;&#20110;&#35270;&#39057;&#20013;&#23545;&#35937;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#23545;&#35937;&#30340;&#26102;&#31354;&#20851;&#31995;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#22312;&#29983;&#25104;&#36275;&#22815;&#30340;&#35270;&#39057;&#20869;&#23481;&#29305;&#24449;&#34920;&#31034;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#22270;&#21644;&#38376;&#25511;&#34701;&#21512;&#30340;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65306;&#25105;&#20204;&#36866;&#24212;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#22270;&#26469;&#29983;&#25104;&#35270;&#39057;&#20869;&#23481;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#38376;&#25511;&#34701;&#21512;&#26469;&#36827;&#19968;&#27493;&#29702;&#35299;&#36825;&#20123;&#19981;&#21516;&#23618;&#27425;&#30340;&#20449;&#24687;&#12290;&#20351;&#29992;&#21452;&#22270;&#27169;&#22411;&#20998;&#21035;&#29983;&#25104;&#22806;&#35266;&#29305;&#24449;&#21644;&#21160;&#20316;&#29305;&#24449;&#21487;&#20197;&#21033;&#29992;&#24103;&#38388;&#30340;&#20869;&#23481;&#30456;&#20851;&#24615;&#20174;&#22810;&#20010;&#35282;&#24230;&#29983;&#25104;&#22810;&#26679;&#30340;&#29305;&#24449;&#12290;&#20854;&#20013;&#65292;&#21452;&#22270;&#25512;&#29702;&#21487;&#20197;&#22686;&#24378;&#24103;&#24207;&#21015;&#20013;&#30340;&#20869;&#23481;&#30456;&#20851;&#24615;&#65292;&#29983;&#25104;&#39640;&#32423;&#35821;&#20041;&#29305;&#24449;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#38376;&#25511;&#34701;&#21512;&#21487;&#20197;&#32452;&#21512;&#19981;&#21516;&#23618;&#27425;&#30340;&#29305;&#24449;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#35270;&#39057;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of video captioning models aims at translating the content of videos by using accurate natural language. Due to the complex nature inbetween object interaction in the video, the comprehensive understanding of spatio-temporal relations of objects remains a challenging task. Existing methods often fail in generating sufficient feature representations of video content. In this paper, we propose a video captioning model based on dual graphs and gated fusion: we adapt two types of graphs to generate feature representations of video content and utilize gated fusion to further understand these different levels of information. Using a dual-graphs model to generate appearance features and motion features respectively can utilize the content correlation in frames to generate various features from multiple perspectives. Among them, dual-graphs reasoning can enhance the content correlation in frame sequences to generate advanced semantic features; The gated fusion, on the other han
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20013;&#30340;&#23567;&#25209;&#37327;&#22122;&#38899;&#20250;&#20351;&#35299;&#20915;&#26041;&#26696;&#21521;&#24179;&#34913;&#35299;&#38752;&#36817;&#65292;&#21482;&#35201;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#30340;&#38543;&#26426;&#26799;&#24230;&#27969;&#31283;&#24577;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#23637;&#31034;&#20102;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#21160;&#24577;&#26799;&#24230;&#19979;&#38477;&#27861;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.06671</link><description>&lt;p&gt;
&#21160;&#24577;&#26799;&#24230;&#19979;&#38477;&#27861;&#30340;&#24179;&#34913;&#27861;&#21017;&#19982;&#31283;&#24577;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Law of Balance and Stationary Distribution of Stochastic Gradient Descent. (arXiv:2308.06671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20013;&#30340;&#23567;&#25209;&#37327;&#22122;&#38899;&#20250;&#20351;&#35299;&#20915;&#26041;&#26696;&#21521;&#24179;&#34913;&#35299;&#38752;&#36817;&#65292;&#21482;&#35201;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#30340;&#38543;&#26426;&#26799;&#24230;&#27969;&#31283;&#24577;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#23637;&#31034;&#20102;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#21160;&#24577;&#26799;&#24230;&#19979;&#38477;&#27861;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#26159;&#25105;&#20204;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24456;&#38590;&#29702;&#35299;SGD&#22914;&#20309;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#21644;&#36864;&#21270;&#30340;&#25439;&#22833;&#26354;&#38754;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SGD&#30340;&#23567;&#25209;&#37327;&#22122;&#38899;&#21487;&#20197;&#20351;&#35299;&#20915;&#26041;&#26696;&#21521;&#24179;&#34913;&#35299;&#38752;&#36817;&#65292;&#21482;&#35201;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#19968;&#20010;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#12290;&#30001;&#20110;&#31616;&#21333;&#25193;&#25955;&#36807;&#31243;&#21644;SGD&#21160;&#21147;&#23398;&#30340;&#24046;&#24322;&#22312;&#23545;&#31216;&#24615;&#23384;&#22312;&#26102;&#26368;&#37325;&#35201;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#26159;&#20102;&#35299;SGD&#24037;&#20316;&#26041;&#24335;&#30340;&#37325;&#35201;&#32447;&#32034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32467;&#26524;&#24212;&#29992;&#20110;&#23548;&#20986;&#20855;&#26377;&#20219;&#24847;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#30340;&#38543;&#26426;&#26799;&#24230;&#27969;&#30340;&#31283;&#24577;&#20998;&#24067;&#12290;&#31283;&#24577;&#20998;&#24067;&#23637;&#29616;&#20102;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#65292;&#22914;&#30456;&#21464;&#12289;&#30772;&#22351;&#30340;&#36941;&#21382;&#24615;&#21644;&#27874;&#21160;&#21453;&#36716;&#12290;&#36825;&#20123;&#29616;&#35937;&#20165;&#22312;&#28145;&#23618;&#32593;&#32476;&#20013;&#23384;&#22312;&#65292;&#34920;&#26126;&#20102;&#19968;&#31181;&#22522;&#26412;&#30340;&#26032;&#30340;&#21152;&#28145;&#35757;&#32451;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic gradient descent (SGD) algorithm is the algorithm we use to train neural networks. However, it remains poorly understood how the SGD navigates the highly nonlinear and degenerate loss landscape of a neural network. In this work, we prove that the minibatch noise of SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry. Because the difference between a simple diffusion process and SGD dynamics is the most significant when symmetries are present, our theory implies that the loss function symmetries constitute an essential probe of how SGD works. We then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundam
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26080;&#30417;&#30563;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#31216;&#20026;&#21306;&#22495;&#21040;&#20687;&#32032;&#33258;&#36866;&#24212;&#32593;&#32476;&#65288;RPANet&#65289;&#12290;&#36890;&#36807;&#30001;&#31895;&#21040;&#32454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;RPANet&#33021;&#22815;&#23398;&#20064;&#30446;&#26631;&#22495;&#30340;&#21306;&#22495;&#32423;&#21035;&#21644;&#20687;&#32032;&#32423;&#21035;&#30340;&#21028;&#21035;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#30446;&#26631;&#22495;&#20869;&#22312;&#32467;&#26500;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#24182;&#35299;&#20915;&#20102;&#20266;&#26631;&#31614;&#33258;&#35757;&#32451;&#20013;&#30340;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06665</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#33258;&#36866;&#24212;&#24687;&#32905;&#20998;&#21106;&#27169;&#22411;&#36890;&#36807;&#30001;&#31895;&#21040;&#32454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Adaptation of Polyp Segmentation Models via Coarse-to-Fine Self-Supervision. (arXiv:2308.06665v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26080;&#30417;&#30563;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#31216;&#20026;&#21306;&#22495;&#21040;&#20687;&#32032;&#33258;&#36866;&#24212;&#32593;&#32476;&#65288;RPANet&#65289;&#12290;&#36890;&#36807;&#30001;&#31895;&#21040;&#32454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;RPANet&#33021;&#22815;&#23398;&#20064;&#30446;&#26631;&#22495;&#30340;&#21306;&#22495;&#32423;&#21035;&#21644;&#20687;&#32032;&#32423;&#21035;&#30340;&#21028;&#21035;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#30446;&#26631;&#22495;&#20869;&#22312;&#32467;&#26500;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#24182;&#35299;&#20915;&#20102;&#20266;&#26631;&#31614;&#33258;&#35757;&#32451;&#20013;&#30340;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#36817;&#21313;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#38590;&#20351;&#29992;&#12290;&#32771;&#34385;&#21040;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#30340;&#23454;&#38469;&#38382;&#39064;&#65292;&#21363;&#28040;&#38500;&#20102;&#23545;&#26631;&#27880;&#28304;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#24403;&#21069;&#30340;SFDA&#26041;&#27861;&#20391;&#37325;&#20110;&#20174;&#28304;&#22521;&#35757;&#27169;&#22411;&#20013;&#25552;&#21462;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#24573;&#35270;&#20102;&#30446;&#26631;&#22495;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#21033;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#30446;&#26631;&#22495;&#30340;&#33258;&#35757;&#32451;&#65292;&#20294;&#21463;&#21040;&#33261;&#21517;&#26157;&#33879;&#30340;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SFDA&#26694;&#26550;&#65292;&#31216;&#20026;&#21306;&#22495;&#21040;&#20687;&#32032;&#33258;&#36866;&#24212;&#32593;&#32476;&#65288;RPANet&#65289;&#65292;&#36890;&#36807;&#30001;&#31895;&#21040;&#32454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#21306;&#22495;&#32423;&#21035;&#21644;&#20687;&#32032;&#32423;&#21035;&#30340;&#21028;&#21035;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;RPANet&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65292;&#21363;&#21069;&#26223;&#24863;&#30693;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;FCL&#65289;&#21644;&#32622;&#20449;&#24230;&#26657;&#20934;&#30340;&#20266;&#26631;&#31614;&#35757;&#32451;&#65288;CCPLT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation~(UDA) has attracted a surge of interest over the past decade but is difficult to be used in real-world applications. Considering the privacy-preservation issues and security concerns, in this work, we study a practical problem of Source-Free Domain Adaptation (SFDA), which eliminates the reliance on annotated source data. Current SFDA methods focus on extracting domain knowledge from the source-trained model but neglects the intrinsic structure of the target domain. Moreover, they typically utilize pseudo labels for self-training in the target domain, but suffer from the notorious error accumulation problem. To address these issues, we propose a new SFDA framework, called Region-to-Pixel Adaptation Network~(RPANet), which learns the region-level and pixel-level discriminative representations through coarse-to-fine self-supervision. The proposed RPANet consists of two modules, Foreground-aware Contrastive Learning (FCL) and Confidence-Calibrated Pseudo-Lab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALGAN&#30340;&#26032;&#22411;GAN&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;LSTM&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#20854;&#20182;GAN&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.06663</link><description>&lt;p&gt;
ALGAN&#65306;&#20855;&#26377;&#35843;&#25972;&#30340;LSTM GAN&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN. (arXiv:2308.06663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06663
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALGAN&#30340;&#26032;&#22411;GAN&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;LSTM&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#20854;&#20182;GAN&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#21508;&#20010;&#39046;&#22495;&#65288;&#22914;&#21046;&#36896;&#19994;&#65292;&#21307;&#23398;&#25104;&#20687;&#21644;&#32593;&#32476;&#23433;&#20840;&#65289;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#28857;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;GANs&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65288;&#21363;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#65289;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#27169;&#22411;&#65292;&#21517;&#20026;Adjusted-LSTM GAN&#65288;ALGAN&#65289;&#65292;&#23427;&#35843;&#25972;LSTM&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#20197;&#25552;&#39640;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#65292;&#32780;&#19988;&#26159;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#22312;46&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#30340;&#22823;&#22411;&#22810;&#21464;&#37327;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;ALGAN&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ALGAN&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#12289;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21644;&#20854;&#20182;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in time series data, to identify points that deviate from normal behaviour, is a common problem in various domains such as manufacturing, medical imaging, and cybersecurity. Recently, Generative Adversarial Networks (GANs) are shown to be effective in detecting anomalies in time series data. The neural network architecture of GANs (i.e. Generator and Discriminator) can significantly improve anomaly detection accuracy. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting. We evaluate the performance of ALGAN on 46 real-world univariate time series datasets and a large multivariate dataset that spans multiple domains. Our experiments demonstrate that ALGAN outperforms traditional, neural network-based, and other GAN-based methods for anomaly detection in time series data.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;SMARTKT&#65292;&#36890;&#36807;&#19968;&#20010;&#31867;&#20284;&#35895;&#27468;&#25628;&#32034;&#30340;&#26694;&#26550;&#65292;&#23558;&#31243;&#24207;&#29702;&#35299;&#36807;&#31243;&#36716;&#21270;&#20026;&#35821;&#20041;&#22270;&#26597;&#35810;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36719;&#20214;&#32500;&#25252;&#25104;&#26412;&#19978;&#21319;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.06653</link><description>&lt;p&gt;
&#20351;&#29992;&#31867;&#20284;&#35895;&#27468;&#25628;&#32034;&#30340;&#26234;&#33021;&#30693;&#35782;&#20256;&#36755;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Smart Knowledge Transfer using Google-like Search. (arXiv:2308.06653v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06653
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;SMARTKT&#65292;&#36890;&#36807;&#19968;&#20010;&#31867;&#20284;&#35895;&#27468;&#25628;&#32034;&#30340;&#26694;&#26550;&#65292;&#23558;&#31243;&#24207;&#29702;&#35299;&#36807;&#31243;&#36716;&#21270;&#20026;&#35821;&#20041;&#22270;&#26597;&#35810;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36719;&#20214;&#32500;&#25252;&#25104;&#26412;&#19978;&#21319;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31243;&#24207;&#29702;&#35299;&#25361;&#25112;&#23548;&#33268;&#30340;&#36719;&#20214;&#32500;&#25252;&#25104;&#26412;&#19978;&#21319;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26234;&#33021;&#30693;&#35782;&#20256;&#36755;&#65288;SMARTKT&#65289;&#65292;&#19968;&#20010;&#25628;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20197;&#35821;&#20041;&#22270;&#30340;&#24418;&#24335;&#25552;&#21462;&#24182;&#25972;&#21512;&#19982;&#24212;&#29992;&#31243;&#24207;&#21508;&#20010;&#26041;&#38754;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#36825;&#20010;&#22270;&#25903;&#25345;&#35821;&#27861;&#21644;&#35821;&#20041;&#26597;&#35810;&#65292;&#24182;&#23558;&#31243;&#24207;&#29702;&#35299;&#30340;&#36807;&#31243;&#36716;&#21270;&#20026;&#31867;&#20284;&#20110;&#35895;&#27468;&#25628;&#32034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the issue of rising software maintenance cost due to program comprehension challenges, we propose SMARTKT (Smart Knowledge Transfer), a search framework, which extracts and integrates knowledge related to various aspects of an application in form of a semantic graph. This graph supports syntax and semantic queries and converts the process of program comprehension into a {\em google-like} search problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36880;&#27493;&#33976;&#39311;&#26469;&#21152;&#36895;&#22522;&#20110;&#25193;&#25955;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;TSP-50&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;16&#20493;&#30340;&#25512;&#29702;&#36895;&#24230;&#25552;&#21319;&#65292;&#20165;&#26377;0.019%&#30340;&#24615;&#33021;&#38477;&#32423;&#12290;</title><link>http://arxiv.org/abs/2308.06644</link><description>&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#33976;&#39311;&#21152;&#36895;&#22522;&#20110;&#25193;&#25955;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation. (arXiv:2308.06644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36880;&#27493;&#33976;&#39311;&#26469;&#21152;&#36895;&#22522;&#20110;&#25193;&#25955;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;TSP-50&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;16&#20493;&#30340;&#25512;&#29702;&#36895;&#24230;&#25552;&#21319;&#65292;&#20165;&#26377;0.019%&#30340;&#24615;&#33021;&#38477;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#30340;NP&#23436;&#20840;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#36845;&#20195;&#35780;&#20272;&#29305;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#25512;&#29702;&#19978;&#24120;&#24120;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36880;&#27493;&#33976;&#39311;&#26469;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#20165;&#22312;&#21333;&#27493;&#20869;&#39044;&#27979;&#20004;&#20010;&#27493;&#39588;&#20043;&#21069;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#36880;&#27493;&#33976;&#39311;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;TSP-50&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#36895;&#24230;&#24555;&#20102;16&#20493;&#65292;&#24615;&#33021;&#20165;&#26377;0.019%&#30340;&#38477;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EGP&#30340;&#21019;&#26032;&#30340;&#29109;&#24341;&#23548;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#20248;&#20808;&#21098;&#38500;&#29109;&#36739;&#20302;&#30340;&#23618;&#20013;&#30340;&#36830;&#25509;&#26469;&#26377;&#25928;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#31454;&#20105;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.06619</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#26469;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#25968;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?. (arXiv:2308.06619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EGP&#30340;&#21019;&#26032;&#30340;&#29109;&#24341;&#23548;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#20248;&#20808;&#21098;&#38500;&#29109;&#36739;&#20302;&#30340;&#23618;&#20013;&#30340;&#36830;&#25509;&#26469;&#26377;&#25928;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#31454;&#20105;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20943;&#23567;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#22312;&#26377;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#25216;&#26415;&#20063;&#24456;&#38590;&#20174;&#27169;&#22411;&#20013;&#23436;&#20840;&#21435;&#38500;&#25972;&#20010;&#23618;&#65306;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#35299;&#20915;&#30340;&#20219;&#21153;&#21527;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EGP&#30340;&#21019;&#26032;&#30340;&#29109;&#24341;&#23548;&#21098;&#26525;&#31639;&#27861;&#65292;&#26088;&#22312;&#20943;&#23567;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;EGP&#30340;&#20851;&#38190;&#37325;&#28857;&#26159;&#20248;&#20808;&#21098;&#38500;&#29109;&#36739;&#20302;&#30340;&#23618;&#20013;&#30340;&#36830;&#25509;&#65292;&#26368;&#32456;&#23436;&#20840;&#21435;&#38500;&#36825;&#20123;&#23618;&#12290;&#36890;&#36807;&#22312;ResNet-18&#21644;Swin-T&#31561;&#27969;&#34892;&#27169;&#22411;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;EGP&#33021;&#22815;&#26377;&#25928;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#25581;&#31034;&#20102;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#20248;&#21183;&#32972;&#21518;&#30340;&#26426;&#21046;&#65292;&#36824;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#22797;&#26434;&#30340;&#20851;&#31995;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relations
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#21367;&#31215;&#22635;&#20805;&#21644;&#23545;&#25239;&#25915;&#20987;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#22635;&#20805;&#27169;&#24335;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.06612</link><description>&lt;p&gt;
&#20851;&#20110;&#21367;&#31215;&#22635;&#20805;&#19982;&#23545;&#25239;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Interplay of Convolutional Padding and Adversarial Robustness. (arXiv:2308.06612v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#21367;&#31215;&#22635;&#20805;&#21644;&#23545;&#25239;&#25915;&#20987;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#22635;&#20805;&#27169;&#24335;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20013;&#65292;&#24120;&#24120;&#22312;&#21367;&#31215;&#25805;&#20316;&#20043;&#21069;&#36827;&#34892;&#22635;&#20805;&#25805;&#20316;&#65292;&#20197;&#20445;&#30041;&#29305;&#24449;&#22270;&#30340;&#20998;&#36776;&#29575;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#26159;&#36890;&#36807;&#22312;&#36755;&#20837;&#21608;&#22260;&#28155;&#21152;&#19968;&#22280;&#38646;&#36827;&#34892;&#22635;&#20805;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#25239;&#25915;&#20987;&#36890;&#24120;&#20250;&#22312;&#22270;&#20687;&#36793;&#30028;&#20986;&#29616;&#25200;&#21160;&#24322;&#24120;&#65292;&#32780;&#36825;&#20123;&#36793;&#30028;&#27491;&#26159;&#22635;&#20805;&#25152;&#29992;&#30340;&#21306;&#22495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#23545;&#22635;&#20805;&#21644;&#23545;&#25239;&#25915;&#20987;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#23547;&#25214;&#19981;&#21516;&#22635;&#20805;&#27169;&#24335;&#65288;&#25110;&#32570;&#20047;&#22635;&#20805;&#65289;&#23545;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is common practice to apply padding prior to convolution operations to preserve the resolution of feature-maps in Convolutional Neural Networks (CNN). While many alternatives exist, this is often achieved by adding a border of zeros around the inputs. In this work, we show that adversarial attacks often result in perturbation anomalies at the image boundaries, which are the areas where padding is used. Consequently, we aim to provide an analysis of the interplay between padding and adversarial attacks and seek an answer to the question of how different padding modes (or their absence) affect adversarial robustness in various scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#31995;&#32479;&#24615;&#35780;&#36848;&#20013;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#26469;&#25552;&#39640;&#25688;&#35201;&#31579;&#36873;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#31216;&#20026;Bio-SIEVE&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#20294;&#22312;&#23433;&#20840;&#24615;&#20248;&#20808;&#22330;&#26223;&#19979;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#23581;&#35797;&#20102;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20294;&#21457;&#29616;&#20854;&#19981;&#33021;&#19982;&#21333;&#20219;&#21153;&#30340;Bio-SIEVE&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#36825;&#19968;&#30740;&#31350;&#26159;&#20026;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#19987;&#38376;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#31995;&#32479;&#24615;&#35780;&#36848;&#36807;&#31243;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#12289;&#20195;&#30721;&#21644;DOI&#21015;&#34920;&#20197;&#20379;&#22797;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.06610</link><description>&lt;p&gt;
Bio-SIEVE&#65306;&#25506;&#32034;&#38024;&#23545;&#31995;&#32479;&#24615;&#35780;&#36848;&#33258;&#21160;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic Review Automation. (arXiv:2308.06610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#31995;&#32479;&#24615;&#35780;&#36848;&#20013;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#26469;&#25552;&#39640;&#25688;&#35201;&#31579;&#36873;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#31216;&#20026;Bio-SIEVE&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#20294;&#22312;&#23433;&#20840;&#24615;&#20248;&#20808;&#22330;&#26223;&#19979;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#23581;&#35797;&#20102;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20294;&#21457;&#29616;&#20854;&#19981;&#33021;&#19982;&#21333;&#20219;&#21153;&#30340;Bio-SIEVE&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#36825;&#19968;&#30740;&#31350;&#26159;&#20026;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#19987;&#38376;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#31995;&#32479;&#24615;&#35780;&#36848;&#36807;&#31243;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#12289;&#20195;&#30721;&#21644;DOI&#21015;&#34920;&#20197;&#20379;&#22797;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#31995;&#32479;&#24615;&#35780;&#36848;&#25104;&#26412;&#39640;&#12289;&#36164;&#28304;&#23494;&#38598;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25552;&#20379;&#35814;&#32454;&#30340;&#36873;&#25321;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#25903;&#25345;&#21644;&#35757;&#32451;&#20854;&#25191;&#34892;&#25991;&#29486;&#31579;&#36873;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;LLaMA&#21644;Guanaco&#27169;&#22411;&#36827;&#34892;&#20102;&#25351;&#20196;&#35843;&#20248;&#65292;&#20197;&#25191;&#34892;&#21307;&#23398;&#31995;&#32479;&#24615;&#35780;&#36848;&#30340;&#25688;&#35201;&#31579;&#36873;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;Bio-SIEVE&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;ChatGPT&#21644;&#32463;&#36807;&#35757;&#32451;&#30340;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#27169;&#22411;&#35843;&#25972;&#20026;&#20197;&#23433;&#20840;&#20026;&#20808;&#30340;&#22330;&#26223;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19982;Bio-SIEVE-Multi&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;PICO&#25552;&#21462;&#21644;&#25490;&#38500;&#25512;&#29702;&#31561;&#20219;&#21153;&#65292;&#20294;&#21457;&#29616;&#23427;&#26080;&#27861;&#36798;&#21040;&#21333;&#20219;&#21153;Bio-SIEVE&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;Bio-SIEVE&#26159;&#20026;&#29983;&#29289;&#21307;&#23398;&#31995;&#32479;&#24615;&#35780;&#36848;&#36807;&#31243;&#19987;&#38376;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#19968;&#27493;&#65292;&#21516;&#26102;&#36824;&#25506;&#32034;&#20102;&#20854;&#26410;&#26469;&#30340;&#21457;&#23637;&#26426;&#20250;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12289;&#20195;&#30721;&#21644;&#19968;&#20221;DOI&#21015;&#34920;&#20197;&#20379;&#37325;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical systematic reviews can be very costly and resource intensive. We explore how Large Language Models (LLMs) can support and be trained to perform literature screening when provided with a detailed set of selection criteria. Specifically, we instruction tune LLaMA and Guanaco models to perform abstract screening for medical systematic reviews. Our best model, Bio-SIEVE, outperforms both ChatGPT and trained traditional approaches, and generalises better across medical domains. However, there remains the challenge of adapting the model to safety-first scenarios. We also explore the impact of multi-task training with Bio-SIEVE-Multi, including tasks such as PICO extraction and exclusion reasoning, but find that it is unable to match single-task Bio-SIEVE's performance. We see Bio-SIEVE as an important step towards specialising LLMs for the biomedical systematic review process and explore its future developmental opportunities. We release our models, code and a list of DOIs to reconst
&lt;/p&gt;</description></item><item><title>VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.06595</link><description>&lt;p&gt;
VisIT-Bench: &#19968;&#20010;&#21463;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#21551;&#21457;&#30340;&#35270;&#35273;&#35821;&#35328;&#25351;&#31034;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06595
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;VisIT-Bench&#65288;Visual InsTruction Benchmark&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20215;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#31034;&#36981;&#24490;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#36215;&#28857;&#26159;&#31574;&#21010;&#20102;70&#20010;&#8220;&#25351;&#31034;&#23478;&#26063;&#8221;&#65292;&#25105;&#20204;&#35748;&#20026;&#25351;&#31034;&#35843;&#20248;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#23478;&#26063;&#12290;&#20219;&#21153;&#19981;&#20165;&#38480;&#20110;VQAv2&#21644;COCO&#31561;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20174;&#22522;&#26412;&#35782;&#21035;&#21040;&#28216;&#25103;&#29609;&#27861;&#21644;&#21019;&#36896;&#24615;&#29983;&#25104;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#22312;&#31574;&#21010;&#20043;&#21518;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;592&#20010;&#27979;&#35797;&#26597;&#35810;&#65292;&#27599;&#20010;&#26597;&#35810;&#37117;&#24102;&#26377;&#19968;&#20010;&#20154;&#24037;&#32534;&#20889;&#30340;&#25351;&#31034;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#12290;&#36825;&#20123;&#25551;&#36848;&#23637;&#29616;&#20102;&#29305;&#23450;&#25351;&#31034;&#22240;&#32032;&#65292;&#20363;&#22914;&#23545;&#20110;&#35810;&#38382;&#24215;&#38754;&#23545;&#20110;&#36718;&#26885;&#29992;&#25143;&#30340;&#26131;&#35775;&#38382;&#24615;&#30340;&#25351;&#31034;&#65292;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#25551;&#36848;&#20102;&#26012;&#22369;/&#28508;&#22312;&#38556;&#30861;&#29289;&#12290;&#36825;&#20123;&#25551;&#36848;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#65306;1&#65289;&#25910;&#38598;&#27599;&#20010;&#23454;&#20363;&#30340;&#20154;&#24037;&#39564;&#35777;&#30340;&#21442;&#32771;&#36755;&#20986;&#65307;2&#65289;&#20351;&#29992;&#20165;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20505;&#36873;&#22810;&#27169;&#24577;&#29983;&#25104;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps be
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#20998;&#24067;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21518;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25919;&#31574;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#31574;&#30053;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.06590</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#20998;&#24067;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Value-Distributional Model-Based Reinforcement Learning. (arXiv:2308.06590v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06590
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#20998;&#24067;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21518;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25919;&#31574;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#31574;&#30053;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#37327;&#21270;&#25919;&#31574;&#38271;&#26399;&#32489;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#20174;&#22522;&#20110;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#35282;&#24230;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#30001;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#21442;&#25968;&#65288;&#35748;&#30693;&#65289;&#19981;&#30830;&#23450;&#24615;&#24341;&#21457;&#30340;&#20540;&#20989;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#20998;&#26512;&#38480;&#21046;&#22312;&#23569;&#25968;&#20998;&#24067;&#20540;&#19978;&#65292;&#25110;&#32773;&#32422;&#26463;&#20998;&#24067;&#24418;&#29366;&#65292;&#20363;&#22914;&#65292;&#39640;&#26031;&#20998;&#24067;&#12290;&#21463;&#21040;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;Bellman&#31639;&#23376;&#65292;&#20854;&#22266;&#23450;&#28857;&#26159;&#20540;&#20998;&#24067;&#20989;&#25968;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Epistemic Quantile-Regression&#65288;EQR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#20540;&#20998;&#24067;&#20989;&#25968;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;&#12290;&#22312;&#20960;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#24050;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;EQR&#20855;&#26377;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#27010;&#36848;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#19979;&#22238;&#31572;&#26597;&#35810;&#12290;&#36825;&#20123;&#26041;&#27861;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#26597;&#35810;&#31867;&#22411;&#21644;&#22270;&#31867;&#22411;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06585</link><description>&lt;p&gt;
&#22270;&#26597;&#35810;&#30340;&#36817;&#20284;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Approximate Answering of Graph Queries. (arXiv:2308.06585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#27010;&#36848;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#19979;&#22238;&#31572;&#26597;&#35810;&#12290;&#36825;&#20123;&#26041;&#27861;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#26597;&#35810;&#31867;&#22411;&#21644;&#22270;&#31867;&#22411;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30001;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#19981;&#23436;&#25972;&#21644;&#36755;&#20837;KG&#30340;&#20559;&#35265;&#32780;&#26412;&#36136;&#19978;&#26159;&#19981;&#23436;&#25972;&#30340;&#12290;&#27492;&#22806;&#65292;&#19990;&#30028;&#30693;&#35782;&#19981;&#26029;&#25193;&#23637;&#21644;&#21457;&#23637;&#65292;&#20351;&#29616;&#26377;&#20107;&#23454;&#36807;&#26102;&#25110;&#24341;&#20837;&#26032;&#30340;&#20107;&#23454;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#28982;&#24076;&#26395;&#33021;&#22815;&#20687;&#22270;&#35889;&#23436;&#25972;&#19968;&#26679;&#22238;&#31572;&#26597;&#35810;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#23558;&#27010;&#36848;&#20960;&#31181;&#24050;&#32463;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22238;&#31572;&#26597;&#35810;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#27010;&#36848;&#36825;&#20123;&#26041;&#27861;&#25903;&#25345;&#30340;&#19981;&#21516;&#26597;&#35810;&#31867;&#22411;&#21644;&#36890;&#24120;&#29992;&#20110;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#23545;&#23427;&#20204;&#30340;&#38480;&#21046;&#30340;&#20102;&#35299;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27010;&#36848;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#34920;&#36798;&#33021;&#21147;&#12289;&#25903;&#25345;&#30340;&#22270;&#31867;&#22411;&#21644;&#25512;&#29702;&#33021;&#21147;&#26469;&#25551;&#36848;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are inherently incomplete because of incomplete world knowledge and bias in what is the input to the KG. Additionally, world knowledge constantly expands and evolves, making existing facts deprecated or introducing new ones. However, we would still want to be able to answer queries as if the graph were complete. In this chapter, we will give an overview of several methods which have been proposed to answer queries in such a setting. We will first provide an overview of the different query types which can be supported by these methods and datasets typically used for evaluation, as well as an insight into their limitations. Then, we give an overview of the different approaches and describe them in terms of expressiveness, supported graph types, and inference capabilities.
&lt;/p&gt;</description></item><item><title>4DRVO-Net&#26159;&#19968;&#31181;&#28145;&#24230;4D&#38647;&#36798;-&#35270;&#35273;&#37324;&#31243;&#35745;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25972;&#21512;4D&#38647;&#36798;&#21644;&#30456;&#26426;&#30340;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#21644;&#25104;&#26412;&#20307;&#31215;&#32593;&#32476;&#36880;&#27493;&#20272;&#35745;&#21644;&#25913;&#36827;&#23039;&#24577;&#12290;</title><link>http://arxiv.org/abs/2308.06573</link><description>&lt;p&gt;
4DRVO-Net: &#20351;&#29992;&#22810;&#27169;&#24577;&#21644;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#34701;&#21512;&#30340;&#28145;&#24230;4D&#38647;&#36798;-&#35270;&#35273;&#37324;&#31243;&#35745;
&lt;/p&gt;
&lt;p&gt;
4DRVO-Net: Deep 4D Radar-Visual Odometry Using Multi-Modal and Multi-Scale Adaptive Fusion. (arXiv:2308.06573v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06573
&lt;/p&gt;
&lt;p&gt;
4DRVO-Net&#26159;&#19968;&#31181;&#28145;&#24230;4D&#38647;&#36798;-&#35270;&#35273;&#37324;&#31243;&#35745;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25972;&#21512;4D&#38647;&#36798;&#21644;&#30456;&#26426;&#30340;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#21644;&#25104;&#26412;&#20307;&#31215;&#32593;&#32476;&#36880;&#27493;&#20272;&#35745;&#21644;&#25913;&#36827;&#23039;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
4DRVO-Net&#26159;&#19968;&#31181;&#29992;&#20110;4D&#38647;&#36798;-&#35270;&#35273;&#37324;&#31243;&#35745;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25972;&#21512;4D&#38647;&#36798;&#21644;&#30456;&#26426;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#23039;&#24577;&#20272;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Radar-PointNet++&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#65292;&#23427;&#20805;&#20998;&#32771;&#34385;&#20102;&#20016;&#23500;&#30340;4D&#38647;&#36798;&#28857;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#31232;&#30095;4D&#38647;&#36798;&#28857;&#20113;&#30340;&#31934;&#32454;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#29305;&#24449;&#37329;&#23383;&#22612;&#12289;&#23039;&#24577;&#21464;&#24418;&#21644;&#25104;&#26412;&#20307;&#31215;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#36880;&#27493;&#20272;&#35745;&#21644;&#25913;&#36827;&#23039;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Four-dimensional (4D) radar--visual odometry (4DRVO) integrates complementary information from 4D radar and cameras, making it an attractive solution for achieving accurate and robust pose estimation. However, 4DRVO may exhibit significant tracking errors owing to three main factors: 1) sparsity of 4D radar point clouds; 2) inaccurate data association and insufficient feature interaction between the 4D radar and camera; and 3) disturbances caused by dynamic objects in the environment, affecting odometry estimation. In this paper, we present 4DRVO-Net, which is a method for 4D radar--visual odometry. This method leverages the feature pyramid, pose warping, and cost volume (PWC) network architecture to progressively estimate and refine poses. Specifically, we propose a multi-scale feature extraction network called Radar-PointNet++ that fully considers rich 4D radar point information, enabling fine-grained learning for sparse 4D radar point clouds. To effectively integrate the two modalit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ModelScopeT2V&#65292;&#19968;&#31181;&#20174;&#25991;&#26412;&#21040;&#35270;&#39057;&#21512;&#25104;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#26102;&#31354;&#22359;&#30830;&#20445;&#24103;&#29983;&#25104;&#21644;&#36816;&#21160;&#36807;&#28193;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#33021;&#36866;&#24212;&#19981;&#21516;&#30340;&#24103;&#25968;&#37327;&#12290;ModelScopeT2V&#22312;&#19977;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20195;&#30721;&#21644;&#22312;&#32447;&#28436;&#31034;&#21487;&#22312;&#25351;&#23450;&#38142;&#25509;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2308.06571</link><description>&lt;p&gt;
ModelScope&#25991;&#26412;&#21040;&#35270;&#39057;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
ModelScope Text-to-Video Technical Report. (arXiv:2308.06571v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ModelScopeT2V&#65292;&#19968;&#31181;&#20174;&#25991;&#26412;&#21040;&#35270;&#39057;&#21512;&#25104;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#26102;&#31354;&#22359;&#30830;&#20445;&#24103;&#29983;&#25104;&#21644;&#36816;&#21160;&#36807;&#28193;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#33021;&#36866;&#24212;&#19981;&#21516;&#30340;&#24103;&#25968;&#37327;&#12290;ModelScopeT2V&#22312;&#19977;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20195;&#30721;&#21644;&#22312;&#32447;&#28436;&#31034;&#21487;&#22312;&#25351;&#23450;&#38142;&#25509;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ModelScopeT2V&#65292;&#19968;&#31181;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65288;&#21363;Stable Diffusion&#65289;&#28436;&#21464;&#32780;&#26469;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#21512;&#25104;&#27169;&#22411;&#12290;ModelScopeT2V&#37319;&#29992;&#26102;&#31354;&#22359;&#26469;&#30830;&#20445;&#19968;&#33268;&#30340;&#24103;&#29983;&#25104;&#21644;&#24179;&#28369;&#30340;&#36816;&#21160;&#36807;&#28193;&#12290;&#35813;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24103;&#25968;&#37327;&#65292;&#36866;&#29992;&#20110;&#22270;&#20687;-&#25991;&#26412;&#21644;&#35270;&#39057;-&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;ModelScopeT2V&#32467;&#21512;&#20102;&#19977;&#20010;&#32452;&#20214;&#65288;&#21363;VQGAN&#65292;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#21435;&#22122;UNet&#65289;&#65292;&#24635;&#20849;&#21253;&#21547;17&#20159;&#20010;&#21442;&#25968;&#65292;&#20854;&#20013;5&#20159;&#20010;&#21442;&#25968;&#29992;&#20110;&#26102;&#38388;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#22312;&#19977;&#20010;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#20195;&#30721;&#21644;&#22312;&#32447;&#28436;&#31034;&#21487;&#22312;\url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces ModelScopeT2V, a text-to-video synthesis model that evolves from a text-to-image synthesis model (i.e., Stable Diffusion). ModelScopeT2V incorporates spatio-temporal blocks to ensure consistent frame generation and smooth movement transitions. The model could adapt to varying frame numbers during training and inference, rendering it suitable for both image-text and video-text datasets. ModelScopeT2V brings together three components (i.e., VQGAN, a text encoder, and a denoising UNet), totally comprising 1.7 billion parameters, in which 0.5 billion parameters are dedicated to temporal capabilities. The model demonstrates superior performance over state-of-the-art methods across three evaluation metrics. The code and an online demo are available at \url{https://modelscope.cn/models/damo/text-to-video-synthesis/summary}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#33647;&#29289;&#30456;&#20851;&#25991;&#26723;&#20013;&#25552;&#21462;&#33647;&#29289;&#20107;&#20214;/&#23454;&#20307;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#25429;&#25417;&#24182;&#23545;&#40784;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;/&#35821;&#35328;/&#30693;&#35782;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#33647;&#29289;&#20107;&#20214;&#20449;&#24687;&#30340;&#20840;&#38754;&#26816;&#27979;&#21644;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.06546</link><description>&lt;p&gt;
MC-DRE: &#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#29992;&#20110;&#33647;&#29289;&#20107;&#20214;/&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
MC-DRE: Multi-Aspect Cross Integration for Drug Event/Entity Extraction. (arXiv:2308.06546v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#33647;&#29289;&#30456;&#20851;&#25991;&#26723;&#20013;&#25552;&#21462;&#33647;&#29289;&#20107;&#20214;/&#23454;&#20307;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#25429;&#25417;&#24182;&#23545;&#40784;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;/&#35821;&#35328;/&#30693;&#35782;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#33647;&#29289;&#20107;&#20214;&#20449;&#24687;&#30340;&#20840;&#38754;&#26816;&#27979;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#33647;&#29289;&#30456;&#20851;&#20449;&#24687;&#22359;&#65292;&#22914;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#65288;ADE&#65289;&#65292;&#23545;&#20110;&#39044;&#38450;&#30142;&#30149;&#21644;&#25327;&#25937;&#35768;&#22810;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;ADE&#26159;&#36890;&#36807;&#21307;&#30103;&#32972;&#26223;&#19979;&#30340;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25253;&#21578;&#30340;&#12290;&#22240;&#27492;&#65292;&#24212;&#29992;&#36890;&#29992;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#20851;&#38190;&#22312;&#20110;&#22914;&#20309;&#25972;&#21512;&#21644;&#23545;&#40784;&#22810;&#20010;&#20851;&#38190;&#26041;&#38754;&#26469;&#26816;&#27979;&#33647;&#29289;&#20107;&#20214;&#20449;&#24687;&#65292;&#21253;&#25324;&#33647;&#29289;&#20107;&#20214;&#35821;&#20041;&#12289;&#21477;&#27861;&#32467;&#26500;&#21644;&#21307;&#23398;&#39046;&#22495;&#26415;&#35821;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#33647;&#29289;&#30456;&#20851;&#25991;&#26723;&#20013;&#25429;&#25417;&#21644;&#23545;&#40784;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;/&#35821;&#35328;/&#30693;&#35782;&#23646;&#24615;&#65292;&#29992;&#20110;&#33647;&#29289;&#23454;&#20307;/&#20107;&#20214;&#26816;&#27979;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#22810;&#26041;&#38754;&#32534;&#30721;&#22120;&#26469;&#25551;&#36848;&#35821;&#20041;&#12289;&#21477;&#27861;&#21644;&#21307;&#23398;&#25991;&#26723;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#26041;&#27861;&#21253;&#25324;&#27133;&#26631;&#27880;&#20219;&#21153;&#12289;&#20027;&#35201;&#33647;&#29289;&#23454;&#20307;/&#20107;&#20214;&#26816;&#27979;&#12289;&#35789;&#24615;&#26631;&#27880;&#21644;&#36890;&#29992;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#32534;&#30721;&#22120;&#36827;&#34892;&#20132;&#21449;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting meaningful drug-related information chunks, such as adverse drug events (ADE), is crucial for preventing morbidity and saving many lives. Most ADE are reported via an unstructured conversation with the medical context. Hence, applying a general entity recognition approach is not sufficient enough. The key is how to integrate and align multiple crucial aspects to detect drug event information, including drug event semantics, syntactic structures, and medical domain terminology. In this paper, we propose a new multi-aspect cross-integration framework for drug entity/event detection by capturing and aligning different context/language/knowledge properties from drug-related documents. We first construct multi-aspect encoders to describe semantic, syntactic, and medical document contextual information by conducting those slot tagging tasks, main drug entity/event detection, part-of-speech tagging, and general medical named entity recognition. Then, each encoder conducts cross int
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#26469;&#25552;&#39640;&#22478;&#24066;&#21306;&#22495;&#20013;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#24182;&#32467;&#21512;&#22303;&#22320;&#35206;&#30422;&#21644;&#22320;&#24418;&#21442;&#25968;&#26657;&#27491;&#65292;&#20197;&#35299;&#20915;DEM&#22312;&#22478;&#24066;&#29615;&#22659;&#24314;&#27169;&#20013;&#30340;&#36136;&#37327;&#21644;&#36866;&#29992;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06545</link><description>&lt;p&gt;
&#22312;&#22478;&#24066;&#21306;&#22495;&#20013;&#20351;&#29992;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#12289;&#22303;&#22320;&#35206;&#30422;&#21644;&#22320;&#24418;&#21442;&#25968;&#36827;&#34892;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters. (arXiv:2308.06545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#26469;&#25552;&#39640;&#22478;&#24066;&#21306;&#22495;&#20013;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#24182;&#32467;&#21512;&#22303;&#22320;&#35206;&#30422;&#21644;&#22320;&#24418;&#21442;&#25968;&#26657;&#27491;&#65292;&#20197;&#35299;&#20915;DEM&#22312;&#22478;&#24066;&#29615;&#22659;&#24314;&#27169;&#20013;&#30340;&#36136;&#37327;&#21644;&#36866;&#29992;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#21306;&#22495;&#20013;&#65292;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#65288;DEMs&#65289;&#30340;&#31934;&#24230;&#21463;&#21040;&#22303;&#22320;&#35206;&#30422;&#21644;&#22320;&#24418;&#19981;&#35268;&#21017;&#24615;&#31561;&#22810;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20840;&#23616;DEM&#20013;&#30340;&#24314;&#31569;&#29289;&#20154;&#24037;&#38459;&#22622;&#20102;&#22320;&#34920;&#27700;&#27969;&#36335;&#24452;&#65292;&#38477;&#20302;&#20102;DEM&#30340;&#36136;&#37327;&#21644;&#36866;&#29992;&#24615;&#65292;&#32780;&#22312;&#31934;&#30830;&#21644;&#20934;&#30830;&#30340;&#22320;&#24418;&#20449;&#24687;&#22312;&#22478;&#24066;&#26223;&#35266;&#20013;&#38656;&#35201;&#36827;&#34892;&#27700;&#25991;&#21644;&#29615;&#22659;&#24314;&#27169;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#38598;&#25104;&#31639;&#27861;&#26469;&#25552;&#39640;&#20004;&#20010;&#20013;&#20998;&#36776;&#29575;30&#31859;DEM&#65288;Copernicus GLO-30&#21644;ALOS World 3D&#65289;&#22312;&#21335;&#38750;&#24320;&#26222;&#25958;&#30340;&#31934;&#24230;&#12290;XGBoost&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#12289;&#21487;&#31227;&#26893;&#21644;&#22810;&#21151;&#33021;&#30340;&#26799;&#24230;&#25552;&#21319;&#24211;&#65292;&#21487;&#20197;&#35299;&#20915;&#35768;&#22810;&#29615;&#22659;&#24314;&#27169;&#38382;&#39064;&#12290;&#35757;&#32451;&#25968;&#25454;&#38598;&#21253;&#25324;&#21313;&#19968;&#20010;&#39044;&#27979;&#21464;&#37327;&#65292;&#21253;&#25324;&#39640;&#31243;&#12289;&#22478;&#24066;&#36718;&#24275;&#12289;&#22369;&#24230;&#12289;&#22369;&#21521;&#12289;&#34920;&#38754;&#31895;&#31961;&#24230;&#12289;&#22320;&#24418;&#20301;&#32622;&#25351;&#25968;&#12289;&#22320;&#24418;&#23822;&#23702;&#25351;&#25968;&#12289;&#22320;&#34920;&#32441;&#29702;&#12289;&#30690;&#37327;&#31895;&#31961;&#24230;&#27979;&#37327;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy of digital elevation models (DEMs) in urban areas is influenced by numerous factors including land cover and terrain irregularities. Moreover, building artifacts in global DEMs cause artificial blocking of surface flow pathways. This compromises their quality and adequacy for hydrological and environmental modelling in urban landscapes where precise and accurate terrain information is needed. In this study, the extreme gradient boosting (XGBoost) ensemble algorithm is adopted for enhancing the accuracy of two medium-resolution 30m DEMs over Cape Town, South Africa: Copernicus GLO-30 and ALOS World 3D (AW3D). XGBoost is a scalable, portable and versatile gradient boosting library that can solve many environmental modelling problems. The training datasets are comprised of eleven predictor variables including elevation, urban footprints, slope, aspect, surface roughness, topographic position index, terrain ruggedness index, terrain surface texture, vector roughness measure, f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06534</link><description>&lt;p&gt;
&#35299;&#20915;&#21307;&#23398;&#24433;&#20687;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23567;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#38382;&#39064;&#65306;&#23545;&#27604;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#26377;&#28508;&#21147;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#39118;&#38505;&#12289;&#20943;&#36731;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#36127;&#25285;&#24182;&#21152;&#36895;&#30830;&#35786;&#12290;&#35757;&#32451;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#22411;&#19988;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25552;&#20379;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#65292;&#30001;&#20110;&#27880;&#37322;&#30340;&#39640;&#22797;&#26434;&#24615;&#12289;&#21463;&#38480;&#30340;&#33719;&#21462;&#26041;&#24335;&#25110;&#30142;&#30149;&#30340;&#32597;&#35265;&#24615;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#23567;&#22411;&#30340;&#24050;&#27880;&#37322;&#25968;&#25454;&#38598;&#23601;&#36275;&#20197;&#23545;&#27169;&#22411;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#19979;&#28216;&#20219;&#21153;&#8221;&#12290;&#21307;&#23398;&#24433;&#20687;&#20013;&#26368;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#22522;&#20110;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#33258;&#28982;&#22270;&#20687;&#22788;&#29702;&#30740;&#31350;&#34920;&#26126;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20108;&#32773;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task, the so-called ``downstream task". The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares sta
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#23398;&#20064;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#34013;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#39044;&#27979;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#25490;&#21015;&#30340;&#35270;&#35273;&#29305;&#24615;&#65292;&#36890;&#36807;&#22810;&#32500;&#39044;&#27979;&#26469;&#36873;&#25321;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06528</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#23398;&#20064;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65306;&#22522;&#20110;Raven&#28176;&#36827;&#30697;&#38453;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices. (arXiv:2308.06528v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06528
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#23398;&#20064;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#34013;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#39044;&#27979;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#25490;&#21015;&#30340;&#35270;&#35273;&#29305;&#24615;&#65292;&#36890;&#36807;&#22810;&#32500;&#39044;&#27979;&#26469;&#36873;&#25321;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#38382;&#39064;&#36890;&#24120;&#34987;&#25552;&#20986;&#20026;&#25972;&#20307;&#20219;&#21153;&#65292;&#27809;&#26377;&#20013;&#38388;&#30446;&#26631;&#12290;&#22312;Raven&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#20013;&#65292;&#20219;&#21153;&#26159;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#19968;&#20010;&#21487;&#29992;&#31572;&#26696;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#21644;&#31572;&#26696;&#37117;&#26159;&#22797;&#21512;&#22270;&#20687;&#65292;&#20855;&#26377;&#22810;&#20010;&#23545;&#35937;&#20197;&#21450;&#21508;&#31181;&#31354;&#38388;&#23433;&#25490;&#12290;&#30001;&#20110;&#21482;&#26377;&#36825;&#20010;&#39640;&#32423;&#30446;&#26631;&#20316;&#20026;&#25351;&#23548;&#65292;&#23398;&#20064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22823;&#22810;&#25968;&#29616;&#20195;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#19981;&#36879;&#26126;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#34013;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#19981;&#30452;&#25509;&#36827;&#34892;&#19978;&#36848;&#36873;&#25321;&#65292;&#32780;&#26159;&#39044;&#27979;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#25490;&#21015;&#30340;&#35270;&#35273;&#29305;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#33719;&#24471;&#30340;&#22810;&#32500;&#39044;&#27979;&#30452;&#25509;&#24182;&#32622;&#20197;&#36873;&#25321;&#31572;&#26696;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#27169;&#22411;&#23558;&#35270;&#35273;&#36755;&#20837;&#35299;&#26512;&#20026;&#20196;&#29260;&#30340;&#20960;&#31181;&#26041;&#24335;&#65292;&#24182;&#37319;&#29992;&#20102;&#20960;&#31181;&#33258;&#30417;&#30563;&#35757;&#32451;&#20013;&#36755;&#20837;&#30340;&#23631;&#34109;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges in learning to perform abstract reasoning is that problems are often posed as monolithic tasks, with no intermediate subgoals. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both contexts and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning is challenging and most contemporary solvers tend to be opaque. In this study, we propose a deep learning architecture based on the transformer blueprint which, rather than directly making the above choice, predicts the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assess
&lt;/p&gt;</description></item><item><title>SLoRA&#26159;&#19968;&#31181;&#32852;&#37030;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#20998;&#24067;&#24335;&#21644;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20811;&#26381;&#39640;&#24322;&#26500;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.06522</link><description>&lt;p&gt;
SLoRA: &#32852;&#37030;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models. (arXiv:2308.06522v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06522
&lt;/p&gt;
&lt;p&gt;
SLoRA&#26159;&#19968;&#31181;&#32852;&#37030;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#20998;&#24067;&#24335;&#21644;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20811;&#26381;&#39640;&#24322;&#26500;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#22312;&#27809;&#26377;&#38598;&#20013;&#24335;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#20174;&#32852;&#37030;&#23398;&#20064;&#36793;&#32536;&#23458;&#25143;&#31471;&#30340;&#20998;&#24067;&#24335;&#21644;&#31169;&#26377;&#25968;&#25454;&#20013;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36793;&#32536;&#35774;&#22791;&#30340;&#26377;&#38480;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#20197;&#21450;&#27969;&#34892;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#24040;&#22823;&#22823;&#23567;&#65292;&#39640;&#25928;&#30340;&#24494;&#35843;&#23545;&#20110;&#20351;&#32852;&#37030;&#35757;&#32451;&#25104;&#20026;&#21487;&#34892;&#30340;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#19979;&#24212;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65288;PEFT&#65289;&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#29992;&#25143;&#20043;&#38388;&#30340;&#25968;&#25454;&#36234;&#26469;&#36234;&#22810;&#26679;&#21270;&#65292;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#21644;&#20351;&#29992;PEFT&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#21464;&#22823;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SLoRA&#30340;&#26041;&#27861;&#65292;&#23427;&#20811;&#26381;&#20102;&#39640;&#24322;&#26500;&#25968;&#25454;&#19979;LoRA&#30340;&#20851;&#38190;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning via fine-tuning pre-trained transformer models has gained significant success in delivering state-of-the-art results across various NLP tasks. In the absence of centralized data, Federated Learning (FL) can benefit from distributed and private data of the FL edge clients for fine-tuning. However, due to the limited communication, computation, and storage capabilities of edge devices and the huge sizes of popular transformer models, efficient fine-tuning is crucial to make federated training feasible. This work explores the opportunities and challenges associated with applying parameter efficient fine-tuning (PEFT) methods in different FL settings for language tasks. Specifically, our investigation reveals that as the data across users becomes more diverse, the gap between fully fine-tuning the model and employing PEFT methods widens. To bridge this performance gap, we propose a method called SLoRA, which overcomes the key limitations of LoRA in high heterogeneous data
&lt;/p&gt;</description></item><item><title>HyperFormer&#26159;&#19968;&#20010;&#32771;&#34385;&#23616;&#37096;&#32423;&#39034;&#24207;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#22810;&#36339;&#20449;&#24687;&#24341;&#20837;&#22122;&#38899;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06512</link><description>&lt;p&gt;
HyperFormer: &#22686;&#24378;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
HyperFormer: Enhancing Entity and Relation Interaction for Hyper-Relational Knowledge Graph Completion. (arXiv:2308.06512v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06512
&lt;/p&gt;
&lt;p&gt;
HyperFormer&#26159;&#19968;&#20010;&#32771;&#34385;&#23616;&#37096;&#32423;&#39034;&#24207;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#22810;&#36339;&#20449;&#24687;&#24341;&#20837;&#22122;&#38899;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#65288;HKGs&#65289;&#36890;&#36807;&#23558;&#23646;&#24615;-&#20540;&#20462;&#39280;&#31526;&#19982;&#19977;&#20803;&#32452;&#30456;&#20851;&#32852;&#26469;&#25193;&#23637;&#26631;&#20934;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#26377;&#25928;&#22320;&#34920;&#31034;&#20102;&#19982;&#20854;&#20851;&#32852;&#19977;&#20803;&#32452;&#30340;&#39069;&#22806;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;HKGC&#65289;&#26088;&#22312;&#22312;&#32771;&#34385;&#20854;&#20462;&#39280;&#31526;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#26410;&#30693;&#19977;&#20803;&#32452;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;HKGC&#26041;&#27861;&#21033;&#29992;&#20840;&#23616;&#32423;&#22270;&#32467;&#26500;&#23558;&#36229;&#20851;&#31995;&#30693;&#35782;&#32534;&#30721;&#20026;&#22270;&#21367;&#31215;&#28040;&#24687;&#20256;&#36882;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22810;&#36339;&#20449;&#24687;&#30340;&#28155;&#21152;&#21487;&#33021;&#20250;&#22312;&#19977;&#20803;&#32452;&#39044;&#27979;&#36807;&#31243;&#20013;&#24341;&#20837;&#22122;&#38899;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperFormer&#65292;&#36825;&#26159;&#19968;&#20010;&#32771;&#34385;&#23616;&#37096;&#32423;&#39034;&#24207;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#23427;&#32534;&#30721;&#20102;&#19977;&#20803;&#32452;&#30340;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#20462;&#39280;&#31526;&#30340;&#20869;&#23481;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;HyperFormer&#30001;&#19977;&#20010;&#19981;&#21516;&#30340;&#27169;&#22359;&#32452;&#25104;&#65306;&#19968;&#20010;&#23454;&#20307;&#37051;&#23621;&#32858;&#21512;&#22120;&#27169;&#22359;&#65292;&#29992;&#20110;&#25972;&#21512;&#23454;&#20307;&#37051;&#23621;&#30340;&#20449;&#24687;&#65292;&#20197;&#25429;&#25417;&#19981;&#21516;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyper-relational knowledge graphs (HKGs) extend standard knowledge graphs by associating attribute-value qualifiers to triples, which effectively represent additional fine-grained information about its associated triple. Hyper-relational knowledge graph completion (HKGC) aims at inferring unknown triples while considering its qualifiers. Most existing approaches to HKGC exploit a global-level graph structure to encode hyper-relational knowledge into the graph convolution message passing process. However, the addition of multi-hop information might bring noise into the triple prediction process. To address this problem, we propose HyperFormer, a model that considers local-level sequential information, which encodes the content of the entities, relations and qualifiers of a triple. More precisely, HyperFormer is composed of three different modules: an entity neighbor aggregator module allowing to integrate the information of the neighbors of an entity to capture different perspectives of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#32842;&#22825;&#30340;&#26041;&#27861;&#65292;&#24182;&#25253;&#21578;&#20102;&#30456;&#23545;&#20110;&#22522;&#20934;&#32447;&#30340;&#25913;&#36827;&#12290;&#36824;&#20998;&#26512;&#20102;&#21478;&#22806;&#20004;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.06502</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#32842;&#22825;&#30340;&#19977;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Three Ways of Using Large Language Models to Evaluate Chat. (arXiv:2308.06502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#32842;&#22825;&#30340;&#26041;&#27861;&#65292;&#24182;&#25253;&#21578;&#20102;&#30456;&#23545;&#20110;&#22522;&#20934;&#32447;&#30340;&#25913;&#36827;&#12290;&#36824;&#20998;&#26512;&#20102;&#21478;&#22806;&#20004;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#22242;&#38431;6&#25552;&#20132;&#30340;ChatEval&#31995;&#32479;&#65292;&#36825;&#26159;DSTC 11 Track 4&#31454;&#36187;&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#22797;&#30340;&#36716;&#21521;&#32423;&#21035;&#36136;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#21521;&#37327;&#23384;&#20648;&#20013;&#20351;&#29992;&#21160;&#24577;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#20316;&#20026;ChatGPT&#30340;&#25552;&#31034;&#65292;&#25253;&#21578;&#20102;&#30456;&#23545;&#20110;&#22522;&#20934;&#32447;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#21478;&#22806;&#20004;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25253;&#21578;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;&#25105;&#20204;&#20165;&#29992;&#20004;&#21608;&#26102;&#38388;&#24320;&#21457;&#20102;&#36825;&#19977;&#20010;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#25361;&#25112;&#25130;&#27490;&#26085;&#26399;&#21518;&#36827;&#34892;&#30340;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#26032;&#30340;Llama 2&#27169;&#22411;&#27491;&#22312;&#32553;&#23567;ChatGPT&#21644;&#24320;&#28304;LLMs&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;Llama 2&#27169;&#22411;&#26080;&#27861;&#20687;ChatGPT&#37027;&#26679;&#20174;&#23569;&#26679;&#26412;&#31034;&#20363;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the systems submitted by team6 for ChatEval, the DSTC 11 Track 4 competition. We present three different approaches to predicting turn-level qualities of chatbot responses based on large language models (LLMs). We report improvement over the baseline using dynamic few-shot examples from a vector store for the prompts for ChatGPT. We also analyze the performance of the other two approaches and report needed improvements for future work. We developed the three systems over just two weeks, showing the potential of LLMs for this task. An ablation study conducted after the challenge deadline shows that the new Llama 2 models are closing the performance gap between ChatGPT and open-source LLMs. However, we find that the Llama 2 models do not benefit from few-shot examples in the same way as ChatGPT.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;LEAPT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38544;&#21464;&#37327;&#21457;&#23556;&#22686;&#24378;&#30340;&#36879;&#35270;&#35270;&#35282;&#27169;&#22411;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#21644;&#25512;&#26029;&#20154;&#31867;&#35266;&#23519;&#21644;&#20449;&#24565;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20248;&#21270;ELBO&#26469;&#23398;&#20064;&#38544;&#31354;&#38388;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06498</link><description>&lt;p&gt;
&#38544;&#21464;&#37327;&#21457;&#23556;&#22686;&#24378;&#30340;&#36879;&#35270;&#35270;&#35282;&#65288;LEAPT&#65289;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Latent Emission-Augmented Perspective-Taking (LEAPT) for Human-Robot Interaction. (arXiv:2308.06498v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06498
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;LEAPT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38544;&#21464;&#37327;&#21457;&#23556;&#22686;&#24378;&#30340;&#36879;&#35270;&#35270;&#35282;&#27169;&#22411;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#21644;&#25512;&#26029;&#20154;&#31867;&#35266;&#23519;&#21644;&#20449;&#24565;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20248;&#21270;ELBO&#26469;&#23398;&#20064;&#38544;&#31354;&#38388;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36879;&#35270;&#35270;&#35282;&#26159;&#25351;&#20174;&#20182;&#20154;&#30340;&#35282;&#24230;&#24863;&#30693;&#25110;&#29702;&#35299;&#24773;&#22659;&#25110;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#22312;&#26085;&#24120;&#20154;&#38469;&#20114;&#21160;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#36827;&#34892;&#36879;&#35270;&#35270;&#35282;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65307;&#29616;&#26377;&#30340;&#20351;&#29992;&#30830;&#23450;&#24615;&#25110;&#25163;&#24037;&#21046;&#23450;&#26041;&#27861;&#30340;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#32771;&#34385;&#21040;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#36890;&#36807;&#28145;&#24230;&#19990;&#30028;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#35813;&#27169;&#22411;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#25191;&#34892;&#24863;&#30693;&#21644;&#27010;&#24565;&#36879;&#35270;&#35270;&#35282;&#65292;&#21363;&#26426;&#22120;&#20154;&#33021;&#22815;&#25512;&#26029;&#20154;&#31867;&#25152;&#35265;&#21644;&#25152;&#20449;&#30340;&#20869;&#23481;&#12290;&#20851;&#38190;&#21019;&#26032;&#26159;&#19968;&#20010;&#20998;&#35299;&#30340;&#22810;&#27169;&#24577;&#38544;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#21644;&#22686;&#24378;&#34394;&#25311;&#30340;&#35266;&#27979;/&#21457;&#23556;&#12290;&#36890;&#36807;&#20248;&#21270;&#36825;&#20010;&#27010;&#29575;&#22270;&#27169;&#22411;&#20135;&#29983;&#30340;ELBO&#65292;&#21487;&#20197;&#23398;&#20064;&#38544;&#31354;&#38388;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20415;&#20110;&#20174;&#39640;&#32500;&#35266;&#27979;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20219;&#21153;&#26159;&#39044;&#27979;&#20154;&#31867;&#30340;&#35266;&#23519;&#21644;&#20449;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perspective-taking is the ability to perceive or understand a situation or concept from another individual's point of view, and is crucial in daily human interactions. Enabling robots to perform perspective-taking remains an unsolved problem; existing approaches that use deterministic or handcrafted methods are unable to accurately account for uncertainty in partially-observable settings. This work proposes to address this limitation via a deep world model that enables a robot to perform both perception and conceptual perspective taking, i.e., the robot is able to infer what a human sees and believes. The key innovation is a decomposed multi-modal latent state space model able to generate and augment fictitious observations/emissions. Optimizing the ELBO that arises from this probabilistic graphical model enables the learning of uncertainty in latent space, which facilitates uncertainty estimation from high-dimensional observations. We tasked our model to predict human observations and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EgoPoser&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22330;&#26223;&#20013;&#40065;&#26834;&#22320;&#23454;&#26102;&#20272;&#35745;&#33258;&#25105;&#36523;&#20307;&#23039;&#21183;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#36755;&#20837;&#34920;&#31034;&#12289;&#24341;&#20837;&#26032;&#30340;&#36816;&#21160;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#24314;&#27169;&#36523;&#20307;&#23039;&#21183;&#65292;EgoPoser&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#22343;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.06493</link><description>&lt;p&gt;
EgoPoser&#65306;&#22823;&#22330;&#26223;&#19979;&#40065;&#26834;&#30340;&#23454;&#26102;&#33258;&#25105;&#36523;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes. (arXiv:2308.06493v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EgoPoser&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22330;&#26223;&#20013;&#40065;&#26834;&#22320;&#23454;&#26102;&#20272;&#35745;&#33258;&#25105;&#36523;&#20307;&#23039;&#21183;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#36755;&#20837;&#34920;&#31034;&#12289;&#24341;&#20837;&#26032;&#30340;&#36816;&#21160;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#24314;&#27169;&#36523;&#20307;&#23039;&#21183;&#65292;EgoPoser&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#22343;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22836;&#37096;&#21644;&#25163;&#37096;&#23039;&#21183;&#20165;&#36890;&#36807;&#23436;&#25972;&#36523;&#20307;&#33258;&#25105;&#23039;&#21183;&#20272;&#35745;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#19968;&#20010;&#28909;&#28857;&#39046;&#22495;&#65292;&#20197;&#20026;&#22836;&#25140;&#24335;&#24179;&#21488;&#19978;&#30340;&#34394;&#25311;&#35282;&#33394;&#34920;&#36798;&#25552;&#20379;&#21160;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#25968;&#25454;&#38598;&#35760;&#24405;&#26102;&#30340;&#36816;&#21160;&#25429;&#25417;&#31354;&#38388;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#20551;&#35774;&#36830;&#32493;&#25429;&#25417;&#20851;&#33410;&#36816;&#21160;&#21644;&#22343;&#21248;&#36523;&#20307;&#23610;&#23544;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EgoPoser&#65292;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#65306;1&#65289;&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#22836;&#25140;&#24335;&#24179;&#21488;&#30340;&#33258;&#25105;&#23039;&#21183;&#20272;&#35745;&#30340;&#36755;&#20837;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#36816;&#21160;&#20998;&#35299;&#26041;&#27861;&#26469;&#39044;&#27979;&#19982;&#20840;&#23616;&#20301;&#32622;&#26080;&#20851;&#30340;&#23436;&#25972;&#36523;&#20307;&#23039;&#21183;&#65292;2&#65289;&#20174;&#22836;&#25140;&#24335;&#35774;&#22791;&#35270;&#37326;&#20869;&#30340;&#38388;&#27463;&#24615;&#25163;&#37096;&#23039;&#21183;&#36319;&#36394;&#20013;&#40065;&#26834;&#22320;&#24314;&#27169;&#36523;&#20307;&#23039;&#21183;&#65292;3&#65289;&#38024;&#23545;&#19981;&#21516;&#29992;&#25143;&#30340;&#21508;&#31181;&#36523;&#20307;&#23610;&#23544;&#36827;&#34892;&#36890;&#29992;&#21270;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EgoPoser&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#20445;&#25345;&#36739;&#39640;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Full-body ego-pose estimation from head and hand poses alone has become an active area of research to power articulate avatar representation on headset-based platforms. However, existing methods over-rely on the confines of the motion-capture spaces in which datasets were recorded, while simultaneously assuming continuous capture of joint motions and uniform body dimensions. In this paper, we propose EgoPoser, which overcomes these limitations by 1) rethinking the input representation for headset-based ego-pose estimation and introducing a novel motion decomposition method that predicts full-body pose independent of global positions, 2) robustly modeling body pose from intermittent hand position and orientation tracking only when inside a headset's field of view, and 3) generalizing across various body sizes for different users. Our experiments show that EgoPoser outperforms state-of-the-art methods both qualitatively and quantitatively, while maintaining a high inference speed of over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KG&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#22122;&#22768;&#21442;&#32771;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#32473;&#23450;&#30340;&#22270;&#29983;&#25104;&#24544;&#23454;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;</title><link>http://arxiv.org/abs/2308.06488</link><description>&lt;p&gt;
&#26080;&#22122;&#22768;&#21442;&#32771;&#25991;&#26412;&#30340;&#30693;&#35782;&#22270;&#29983;&#25104;&#24544;&#23454;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Generating Faithful Text From a Knowledge Graph with Noisy Reference Text. (arXiv:2308.06488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KG&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#22122;&#22768;&#21442;&#32771;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#32473;&#23450;&#30340;&#22270;&#29983;&#25104;&#24544;&#23454;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#27969;&#30021;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#65292;&#20934;&#30830;&#22320;&#34920;&#31034;&#32473;&#23450;&#30693;&#35782;&#22270;&#30340;&#20449;&#24687;&#12290;&#34429;&#28982;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#19982;&#36866;&#24403;&#30340;&#22270;&#32467;&#26500;&#24863;&#30693;&#27169;&#22359;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#22312;&#29983;&#25104;&#24544;&#23454;&#25991;&#26412;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#24403;&#22320;&#38754;&#30495;&#23454;&#35821;&#35328;&#25991;&#26412;&#20013;&#21253;&#21547;&#22270;&#20013;&#19981;&#23384;&#22312;&#30340;&#39069;&#22806;&#20449;&#24687;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KG&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#22122;&#22768;&#21442;&#32771;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#32473;&#23450;&#30340;&#22270;&#29983;&#25104;&#24544;&#23454;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#34701;&#21512;&#20102;&#20004;&#20010;&#26680;&#24515;&#24605;&#24819;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#27169;&#22411;&#21306;&#20998;&#25991;&#26412;&#20013;&#30340;&#24544;&#23454;&#21644;&#34394;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#40723;&#21169;&#35299;&#30721;&#22120;&#29983;&#25104;&#19982;&#36755;&#20837;&#22270;&#23545;&#40784;&#30340;&#25991;&#26412;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36171;&#20104;&#35299;&#30721;&#22120;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#30340;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph (KG)-to-Text generation aims at generating fluent natural-language text that accurately represents the information of a given knowledge graph. While significant progress has been made in this task by exploiting the power of pre-trained language models (PLMs) with appropriate graph structure-aware modules, existing models still fall short of generating faithful text, especially when the ground-truth natural-language text contains additional information that is not present in the graph. In this paper, we develop a KG-to-text generation model that can generate faithful natural-language text from a given graph, in the presence of noisy reference text. Our framework incorporates two core ideas: Firstly, we utilize contrastive learning to enhance the model's ability to differentiate between faithful and hallucinated information in the text, thereby encouraging the decoder to generate text that aligns with the input graph. Secondly, we empower the decoder to control the level 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#26410;&#30693;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25361;&#25112;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#29616;&#26377;&#38450;&#24481;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20165;&#20351;&#29992;&#40065;&#26834;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;DNN&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#33021;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2308.06467</link><description>&lt;p&gt;
&#24182;&#19981;&#37027;&#20040;&#24378;&#22823;&#65306;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#26410;&#30693;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks. (arXiv:2308.06467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06467
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#26410;&#30693;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25361;&#25112;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#29616;&#26377;&#38450;&#24481;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20165;&#20351;&#29992;&#40065;&#26834;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;DNN&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#33021;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#22914;&#20998;&#31867;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;&#65292;&#36825;&#24341;&#36215;&#20102;&#23545;&#20854;&#23646;&#24615;&#30340;&#22686;&#21152;&#20851;&#27880;&#12290;&#20256;&#32479;DNN&#30340;&#19968;&#20010;&#22522;&#26412;&#23646;&#24615;&#26159;&#23427;&#20204;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#20462;&#25913;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#23548;&#33268;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#35843;&#26597;&#12290;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#25805;&#32437;&#25968;&#25454;&#26469;&#35823;&#23548;DNN&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25361;&#25112;&#38024;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#24403;&#20195;&#38450;&#24481;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;Ilyas&#31561;&#20154;&#25552;&#20986;&#30340;&#20551;&#35774;&#65292;&#21363;DNN&#30340;&#22270;&#20687;&#29305;&#24449;&#21487;&#20197;&#26159;&#40065;&#26834;&#30340;&#25110;&#38750;&#40065;&#26834;&#30340;&#65292;&#32780;&#23545;&#25239;&#24615;&#25915;&#20987;&#38024;&#23545;&#30340;&#26159;&#21518;&#32773;&#12290;&#35813;&#20551;&#35774;&#35748;&#20026;&#65292;&#20165;&#22312;&#30001;&#40065;&#26834;&#29305;&#24449;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;DNN&#24212;&#35813;&#20135;&#29983;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#25269;&#25239;&#21147;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#36825;&#24182;&#19981;&#26222;&#36941;&#25104;&#31435;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20102;&#35299;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have gained prominence in various applications, such as classification, recognition, and prediction, prompting increased scrutiny of their properties. A fundamental attribute of traditional DNNs is their vulnerability to modifications in input data, which has resulted in the investigation of adversarial attacks. These attacks manipulate the data in order to mislead a DNN. This study aims to challenge the efficacy and generalization of contemporary defense mechanisms against adversarial attacks. Specifically, we explore the hypothesis proposed by Ilyas et. al, which posits that DNN image features can be either robust or non-robust, with adversarial attacks targeting the latter. This hypothesis suggests that training a DNN on a dataset consisting solely of robust features should produce a model resistant to adversarial attacks. However, our experiments demonstrate that this is not universally true. To gain further insights into our findings, we analyze the imp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#26032;&#39062;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#26631;&#31614;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#32452;&#20108;&#20998;&#31867;&#38382;&#39064;&#20197;&#21033;&#29992;&#36923;&#36753;&#22238;&#24402;&#30340;&#20449;&#24687;&#24615;&#35821;&#20041;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#22686;&#24378;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#34920;&#31034;&#30340;&#29420;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06453</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-Label Knowledge Distillation. (arXiv:2308.06453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#26032;&#39062;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#26631;&#31614;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#32452;&#20108;&#20998;&#31867;&#38382;&#39064;&#20197;&#21033;&#29992;&#36923;&#36753;&#22238;&#24402;&#30340;&#20449;&#24687;&#24615;&#35821;&#20041;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#22686;&#24378;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#34920;&#31034;&#30340;&#29420;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#23558;&#25945;&#24072;&#32593;&#32476;&#30340;&#36755;&#20986;&#36923;&#36753;&#22238;&#24402;&#25110;&#20013;&#38388;&#29305;&#24449;&#26144;&#23556;&#20256;&#25480;&#32473;&#23398;&#29983;&#32593;&#32476;&#26469;&#24037;&#20316;&#65292;&#22312;&#22810;&#31867;&#21333;&#26631;&#31614;&#23398;&#20064;&#20013;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#22330;&#26223;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#24456;&#38590;&#25512;&#24191;&#65292;&#27599;&#20010;&#23454;&#20363;&#37117;&#19982;&#22810;&#20010;&#35821;&#20041;&#26631;&#31614;&#30456;&#20851;&#65292;&#22240;&#20026;&#39044;&#27979;&#27010;&#29575;&#19981;&#31561;&#20110;&#19968;&#65292;&#24182;&#19988;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25972;&#20010;&#31034;&#20363;&#30340;&#29305;&#24449;&#26144;&#23556;&#21487;&#33021;&#20250;&#24573;&#30053;&#23567;&#31867;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26631;&#31614;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#36890;&#36807;&#23558;&#22810;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#32452;&#20108;&#20998;&#31867;&#38382;&#39064;&#26469;&#21033;&#29992;&#36923;&#36753;&#22238;&#24402;&#20013;&#30340;&#20449;&#24687;&#24615;&#35821;&#20041;&#30693;&#35782;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#26469;&#22686;&#24378;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#34920;&#31034;&#30340;&#29420;&#29305;&#24615;&#12290;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing knowledge distillation methods typically work by imparting the knowledge of output logits or intermediate feature maps from the teacher network to the student network, which is very successful in multi-class single-label learning. However, these methods can hardly be extended to the multi-label learning scenario, where each instance is associated with multiple semantic labels, because the prediction probabilities do not sum to one and feature maps of the whole example may ignore minor classes in such a scenario. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by dividing the multi-label learning problem into a set of binary classification problems; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the pr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#31561;&#21464;&#30340;Mixup&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#34920;&#31034;&#32423;&#21035;&#19978;&#36827;&#34892;&#28151;&#21512;&#26469;&#20805;&#20998;&#21033;&#29992;&#28151;&#21512;&#26679;&#26412;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#35268;&#33539;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2308.06451</link><description>&lt;p&gt;
&#35821;&#20041;&#31561;&#21464; Mixup
&lt;/p&gt;
&lt;p&gt;
Semantic Equivariant Mixup. (arXiv:2308.06451v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06451
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#31561;&#21464;&#30340;Mixup&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#34920;&#31034;&#32423;&#21035;&#19978;&#36827;&#34892;&#28151;&#21512;&#26469;&#20805;&#20998;&#21033;&#29992;&#28151;&#21512;&#26679;&#26412;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#35268;&#33539;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#24191;&#20026;&#20351;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#26631;&#31614;&#31561;&#21464;&#30340;&#22522;&#30784;&#19978;&#21019;&#24314;&#28151;&#21512;&#26679;&#26412;&#65292;&#25193;&#23637;&#35757;&#32451;&#20998;&#24067;&#24182;&#35268;&#33539;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;Mixup&#21464;&#20307;&#21487;&#33021;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#35757;&#32451;&#36807;&#31243;&#20013;&#28151;&#21512;&#26679;&#26412;&#20013;&#30340;&#29420;&#31435;&#20110;&#26631;&#31614;&#30340;&#20449;&#24687;&#65292;&#20854;&#20013;&#36890;&#24120;&#21253;&#21547;&#26356;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21457;&#25381;Mixup&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#39318;&#20808;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26631;&#31614;&#31561;&#21464;&#20551;&#35774;&#20026;&#35821;&#20041;&#31561;&#21464;&#20551;&#35774;&#65292;&#21363;&#36755;&#20837;&#25968;&#25454;&#30340;&#27604;&#20363;&#28151;&#21512;&#24212;&#23548;&#33268;&#30456;&#24212;&#34920;&#31034;&#30340;&#27604;&#20363;&#28151;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#34920;&#31034;&#32423;&#21035;&#19978;&#30340;&#36890;&#29992;Mixup&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#36827;&#19968;&#27493;&#35268;&#33539;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is a well-established data augmentation technique, which can extend the training distribution and regularize the neural networks by creating ''mixed'' samples based on the label-equivariance assumption, i.e., a proportional mixup of the input data results in the corresponding labels being mixed in the same proportion. However, previous mixup variants may fail to exploit the label-independent information in mixed samples during training, which usually contains richer semantic information. To further release the power of mixup, we first improve the previous label-equivariance assumption by the semantic-equivariance assumption, which states that the proportional mixup of the input data should lead to the corresponding representation being mixed in the same proportion. Then a generic mixup regularization at the representation level is proposed, which can further regularize the model with the semantic information in mixed samples. At a high level, the proposed semantic equivariant mix
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20803;&#36716;&#31227;&#65288;SMT&#65289;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#30340;&#35757;&#32451;&#21644;&#36866;&#24212;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#12289;&#24555;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06447</link><description>&lt;p&gt;
&#39034;&#24207;&#20803;&#36716;&#31227;&#65288;SMT&#65289;&#23398;&#20064;&#29992;&#20110;&#24212;&#23545;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65306;&#22312;&#22797;&#21512;&#26448;&#26009;&#28909;&#21387;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing. (arXiv:2308.06447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20803;&#36716;&#31227;&#65288;SMT&#65289;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#30340;&#35757;&#32451;&#21644;&#36866;&#24212;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#12289;&#24555;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#36890;&#36807;&#23558;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#65292;&#24050;&#32463;&#22312;&#35299;&#20915;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#24182;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;PINNs&#22312;&#20934;&#30830;&#36924;&#36817;&#20855;&#26377;&#24378;&#38750;&#32447;&#24615;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#35299;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#38271;&#26102;&#38388;&#22495;&#19979;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PINNs&#34987;&#35774;&#35745;&#20026;&#36924;&#36817;&#32473;&#23450;PDE&#31995;&#32479;&#30340;&#29305;&#23450;&#23454;&#29616;&#65292;&#23427;&#20204;&#32570;&#20047;&#24517;&#35201;&#30340;&#36890;&#29992;&#24615;&#26469;&#26377;&#25928;&#36866;&#24212;&#26032;&#31995;&#32479;&#37197;&#32622;&#12290;&#36825;&#23601;&#38656;&#35201;&#22312;&#31995;&#32479;&#21457;&#29983;&#20219;&#20309;&#21464;&#21270;&#26102;&#36827;&#34892;&#20174;&#22836;&#37325;&#26032;&#35757;&#32451;&#65292;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39034;&#24207;&#20803;&#36716;&#31227;&#65288;SMT&#65289;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#39640;&#24230;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;PINNs&#25552;&#20379;&#20102;&#24555;&#36895;&#35757;&#32451;&#21644;&#39640;&#25928;&#36866;&#24212;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks (PINNs) have gained popularity in solving nonlinear partial differential equations (PDEs) via integrating physical laws into the training of neural networks, making them superior in many scientific and engineering applications. However, conventional PINNs still fall short in accurately approximating the solution of complex systems with strong nonlinearity, especially in long temporal domains. Besides, since PINNs are designed to approximate a specific realization of a given PDE system, they lack the necessary generalizability to efficiently adapt to new system configurations. This entails computationally expensive re-training from scratch for any new change in the system. To address these shortfalls, in this work a novel sequential meta-transfer (SMT) learning framework is proposed, offering a unified solution for both fast training and efficient adaptation of PINNs in highly nonlinear systems with long temporal domains. Specifically, the framework deco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#34892;&#20154;-&#36710;&#36742;&#28151;&#21512;&#29615;&#22659;&#19979;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36710;&#36742;&#21644;&#34892;&#20154;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#34892;&#20154;&#26410;&#26469;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24182;&#22238;&#39038;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#22914;&#20309;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#21644;&#34892;&#20026;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.06419</link><description>&lt;p&gt;
&#34892;&#20154;-&#36710;&#36742;&#28151;&#21512;&#29615;&#22659;&#19979;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;:&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review. (arXiv:2308.06419v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#34892;&#20154;-&#36710;&#36742;&#28151;&#21512;&#29615;&#22659;&#19979;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36710;&#36742;&#21644;&#34892;&#20154;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#34892;&#20154;&#26410;&#26469;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24182;&#22238;&#39038;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#22914;&#20309;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#21644;&#34892;&#20026;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19982;&#34892;&#20154;&#20849;&#20139;&#31354;&#38388;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#36335;&#24452;&#35268;&#21010;&#20013;&#65292;&#38656;&#35201;&#25512;&#29702;&#20986;&#34892;&#20154;&#30340;&#26410;&#26469;&#36712;&#36857;&#12290;&#20026;&#20102;&#24212;&#29992;&#20110;AV&#30340;&#23454;&#38469;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#31639;&#27861;&#65292;&#38656;&#35201;&#32771;&#34385;&#36710;&#36742;&#19982;&#34892;&#20154;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#34892;&#20154;&#26410;&#26469;&#36816;&#21160;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#25991;&#29486;&#20013;&#38024;&#23545;&#22312;&#23384;&#22312;&#36710;&#36742;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#23545;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#36827;&#34892;&#24314;&#27169;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#19982;&#34892;&#20154;-&#34892;&#20154;&#20132;&#20114;&#30456;&#27604;&#65292;&#34892;&#20154;-&#36710;&#36742;&#20132;&#20114;&#30340;&#29305;&#23450;&#32771;&#34385;&#65292;&#24182;&#22238;&#39038;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#22914;&#20309;&#32771;&#34385;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#34892;&#20026;&#24046;&#24322;&#31561;&#19981;&#21516;&#21464;&#37327;&#12290;&#36981;&#24490;PRISMA&#25351;&#21335;&#12290;&#26412;&#25991;&#36824;&#32771;&#23519;&#20102;&#22914;&#20309;&#22312;&#20808;&#21069;&#25552;&#20986;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#32771;&#34385;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#34892;&#20026;&#24046;&#24322;&#31561;&#19981;&#21516;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning an autonomous vehicle's (AV) path in a space shared with pedestrians requires reasoning about pedestrians' future trajectories. A practical pedestrian trajectory prediction algorithm for the use of AVs needs to consider the effect of the vehicle's interactions with the pedestrians on pedestrians' future motion behaviours. In this regard, this paper systematically reviews different methods proposed in the literature for modelling pedestrian trajectory prediction in presence of vehicles that can be applied for unstructured environments. This paper also investigates specific considerations for pedestrian-vehicle interaction (compared with pedestrian-pedestrian interaction) and reviews how different variables such as prediction uncertainties and behavioural differences are accounted for in the previously proposed prediction models. PRISMA guidelines were followed. Articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#22478;&#24066;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#20013;&#23454;&#29616;&#20102;&#20154;&#31867;&#30417;&#31649;&#32773;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#21644;&#25928;&#21147;&#65292;&#20026;&#20154;&#31867;&#30693;&#35782;&#21644;&#20808;&#36827;AI&#25216;&#26415;&#30340;&#20849;&#29983;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.06411</link><description>&lt;p&gt;
&#20154;&#31867;&#30417;&#31649;&#32773;&#19982;&#22478;&#24066;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#20043;&#38388;&#30340;&#23545;&#35805;&#21487;&#33021;&#24615;&#65306;&#36335;&#24452;&#26356;&#25913;
&lt;/p&gt;
&lt;p&gt;
Dialogue Possibilities between a Human Supervisor and UAM Air Traffic Management: Route Alteration. (arXiv:2308.06411v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#22478;&#24066;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#20013;&#23454;&#29616;&#20102;&#20154;&#31867;&#30417;&#31649;&#32773;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#21644;&#25928;&#21147;&#65292;&#20026;&#20154;&#31867;&#30693;&#35782;&#21644;&#20808;&#36827;AI&#25216;&#26415;&#30340;&#20849;&#29983;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#22312;&#22478;&#24066;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#65288;UATM&#65289;&#20013;&#36827;&#34892;&#32469;&#34892;&#31649;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#26088;&#22312;&#20102;&#35299;UAM&#32469;&#34892;&#30340;&#22797;&#26434;&#24615;&#21644;&#35201;&#27714;&#65292;&#23454;&#29616;&#19968;&#31181;&#33021;&#22815;&#22312;&#32463;&#36807;&#20180;&#32454;&#37319;&#26679;&#30340;&#29615;&#22659;&#20013;&#24555;&#36895;&#30830;&#23450;&#23433;&#20840;&#21644;&#39640;&#25928;&#36335;&#32447;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#38750;&#21333;&#35843;&#25512;&#29702;&#21644;&#20154;&#31867;&#32463;&#29702;&#19982;UATM&#31995;&#32479;&#20043;&#38388;&#30340;&#20004;&#38454;&#27573;&#23545;&#35805;&#65292;&#32771;&#34385;&#20102;&#23433;&#20840;&#24615;&#21644;&#28508;&#22312;&#24433;&#21709;&#31561;&#22240;&#32032;&#12290;&#36890;&#36807;&#20004;&#20010;&#27169;&#25311;&#22330;&#26223;&#20013;&#30340;&#20960;&#20010;&#26597;&#35810;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#21644;&#25928;&#21147;&#65292;&#20026;&#20154;&#31867;&#30693;&#35782;&#21644;&#20808;&#36827;AI&#25216;&#26415;&#30340;&#20849;&#29983;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#24341;&#35328;&#12289;&#24341;&#29992;&#30456;&#20851;&#30740;&#31350;&#12289;&#38382;&#39064;&#38416;&#36848;&#12289;&#35299;&#20915;&#26041;&#26696;&#12289;&#35752;&#35770;&#21644;&#32467;&#35770;&#24615;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to detour management in Urban Air Traffic Management (UATM) using knowledge representation and reasoning. It aims to understand the complexities and requirements of UAM detours, enabling a method that quickly identifies safe and efficient routes in a carefully sampled environment. This method implemented in Answer Set Programming uses non-monotonic reasoning and a two-phase conversation between a human manager and the UATM system, considering factors like safety and potential impacts. The robustness and efficacy of the proposed method were validated through several queries from two simulation scenarios, contributing to the symbiosis of human knowledge and advanced AI techniques. The paper provides an introduction, citing relevant studies, problem formulation, solution, discussions, and concluding comments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;SSVEP&#35782;&#21035;&#30340;&#33041;&#26426;&#25509;&#21475;&#22686;&#24378;&#29616;&#23454;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#36523;&#20307;&#27531;&#38556;&#20154;&#22763;&#21644;&#20581;&#24247;&#29992;&#25143;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#25216;&#26415;&#36827;&#34892;&#20132;&#20114;&#65292;&#36890;&#36807;&#31283;&#24577;&#35270;&#35273;&#35825;&#21457;&#30005;&#20301;&#65288;SSVEP&#65289;&#20449;&#21495;&#35782;&#21035;&#29992;&#25143;&#38656;&#27714;&#21644;&#24895;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.06401</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#36866;&#24212;SSVEP&#35782;&#21035;&#30340;&#33041;&#26426;&#25509;&#21475;&#22686;&#24378;&#29616;&#23454;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Brain-Computer Interface Augmented Reality Framework with Auto-Adaptive SSVEP Recognition. (arXiv:2308.06401v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06401
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;SSVEP&#35782;&#21035;&#30340;&#33041;&#26426;&#25509;&#21475;&#22686;&#24378;&#29616;&#23454;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#36523;&#20307;&#27531;&#38556;&#20154;&#22763;&#21644;&#20581;&#24247;&#29992;&#25143;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#25216;&#26415;&#36827;&#34892;&#20132;&#20114;&#65292;&#36890;&#36807;&#31283;&#24577;&#35270;&#35273;&#35825;&#21457;&#30005;&#20301;&#65288;SSVEP&#65289;&#20449;&#21495;&#35782;&#21035;&#29992;&#25143;&#38656;&#27714;&#21644;&#24895;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#26368;&#21021;&#24341;&#36215;&#20851;&#27880;&#26159;&#20026;&#20102;&#24320;&#21457;&#24110;&#21161;&#34987;&#36523;&#20307;&#27531;&#38556;&#20154;&#22763;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#23558;BCI&#19982;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#32467;&#21512;&#30340;&#24819;&#27861;&#20986;&#29616;&#20102;&#65292;&#36825;&#19981;&#20165;&#21487;&#20197;&#22686;&#24378;&#27531;&#38556;&#20154;&#22763;&#30340;&#29983;&#27963;&#36136;&#37327;&#65292;&#36824;&#21487;&#20197;&#20026;&#20581;&#24247;&#29992;&#25143;&#24320;&#21457;&#20027;&#27969;&#24212;&#29992;&#12290;&#20854;&#20013;&#19968;&#31181;&#24120;&#29992;&#30340;BCI&#20449;&#21495;&#27169;&#24335;&#26159;&#31283;&#24577;&#35270;&#35273;&#35825;&#21457;&#30005;&#20301;&#65288;SSVEP&#65289;&#65292;&#23427;&#25429;&#25417;&#20102;&#22823;&#33041;&#23545;&#38378;&#28865;&#35270;&#35273;&#21050;&#28608;&#30340;&#21709;&#24212;&#12290;&#22522;&#20110;SSVEP&#30340;BCI-AR&#24212;&#29992;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#30475;&#30528;&#30456;&#24212;&#30340;&#21629;&#20196;&#36873;&#39033;&#26469;&#34920;&#36798;&#20182;&#20204;&#30340;&#38656;&#27714;/&#24895;&#26395;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#20010;&#20307;&#30340;&#33041;&#20449;&#21495;&#26159;&#19981;&#21516;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#20010;&#20307;&#36827;&#34892;SSVEP&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#32908;&#32905;&#36816;&#21160;&#21644;&#30524;&#30545;&#38378;&#28865;&#20250;&#24178;&#25200;&#33041;&#20449;&#21495;&#65292;&#22240;&#27492;&#21463;&#35797;&#32773;&#38656;&#35201;&#22312;BCI&#23454;&#39564;&#26399;&#38388;&#20445;&#25345;&#38745;&#27490;&#65292;&#36825;&#38480;&#21046;&#20102;&#22686;&#24378;&#29616;&#23454;&#30340;&#21442;&#19982;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33258;&#36866;&#24212;&#38598;&#25104;&#20998;&#31867;&#31995;&#32479;&#26469;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-Computer Interface (BCI) initially gained attention for developing applications that aid physically impaired individuals. Recently, the idea of integrating BCI with Augmented Reality (AR) emerged, which uses BCI not only to enhance the quality of life for individuals with disabilities but also to develop mainstream applications for healthy users. One commonly used BCI signal pattern is the Steady-state Visually-evoked Potential (SSVEP), which captures the brain's response to flickering visual stimuli. SSVEP-based BCI-AR applications enable users to express their needs/wants by simply looking at corresponding command options. However, individuals are different in brain signals and thus require per-subject SSVEP recognition. Moreover, muscle movements and eye blinks interfere with brain signals, and thus subjects are required to remain still during BCI experiments, which limits AR engagement. In this paper, we (1) propose a simple adaptive ensemble classification system that handle
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ZYN&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26159;&#38750;&#38382;&#39064;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#30340;&#25552;&#31034;&#65292;&#20197;&#21450;&#22686;&#24378;&#23398;&#20064;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#35299;&#27602;&#12289;&#24773;&#24863;&#20248;&#21270;&#21644;&#20010;&#24615;&#21270;&#25552;&#31034;&#29983;&#25104;&#22120;&#31561;&#12290;</title><link>http://arxiv.org/abs/2308.06385</link><description>&lt;p&gt;
ZYN&#65306;&#38646;&#24335;&#22870;&#21169;&#27169;&#22411;&#19982;&#26159;&#38750;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
ZYN: Zero-Shot Reward Models with Yes-No Questions. (arXiv:2308.06385v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ZYN&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26159;&#38750;&#38382;&#39064;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#30340;&#25552;&#31034;&#65292;&#20197;&#21450;&#22686;&#24378;&#23398;&#20064;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#35299;&#27602;&#12289;&#24773;&#24863;&#20248;&#21270;&#21644;&#20010;&#24615;&#21270;&#25552;&#31034;&#29983;&#25104;&#22120;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#23450;&#21521;&#20110;&#26399;&#26395;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#20559;&#22909;&#23545;&#40784;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#21478;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25209;&#35780;&#32773;&#65292;&#36890;&#36807;&#19968;&#20010;&#34920;&#31034;&#29992;&#25143;&#20559;&#22909;&#30340;&#26159;&#38750;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#20197;&#38646;&#24335;&#26041;&#24335;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#31181;&#38646;&#24335;&#22870;&#21169;&#27169;&#22411;&#20026;&#36827;&#19968;&#27493;&#24494;&#35843;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#23398;&#20064;&#20449;&#21495;&#65292;&#20351;&#29992;&#22686;&#24378;&#23398;&#20064;&#65292;&#23601;&#20687;&#22312;RLAIF&#20013;&#19968;&#26679;&#65307;&#28982;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#20182;&#19978;&#19979;&#25991;&#20013;&#20063;&#26159;&#20860;&#23481;&#30340;&#65292;&#20363;&#22914;&#36136;&#37327;&#22810;&#26679;&#24615;&#25628;&#32034;&#12290;&#36890;&#36807;&#22312;&#19982;&#25991;&#26412;&#29983;&#25104;&#30456;&#20851;&#30340;&#19981;&#21516;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#35299;&#27602;&#12289;&#20248;&#21270;&#30005;&#24433;&#35780;&#35770;&#30340;&#24773;&#24863;&#25110;&#20219;&#20309;&#20854;&#20182;&#23646;&#24615;&#12289;&#24341;&#23548;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#30340;&#20851;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#35266;&#28857;&#65292;&#20197;&#21450;&#20010;&#24615;&#21270;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#20219;&#21153;&#30340;&#25552;&#31034;&#29983;&#25104;&#22120;&#65292;&#25552;&#20379;&#20102;&#23545;&#25152;&#25552;&#20986;&#30340;ZYN&#26694;&#26550;&#33021;&#21147;&#30340;&#22823;&#37327;&#35777;&#25454;&#12290;&#20195;&#30721;&#23558;&#22312;\url&#22788;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we address the problem of directing the text generations of a LLM towards a desired behavior, aligning the generated text with the preferences of the human operator. We propose using another language model as a critic, reward model in a zero-shot way thanks to the prompt of a Yes-No question that represents the user preferences, without requiring further labeled data. This zero-shot reward model provides the learning signal to further fine-tune the base LLM using reinforcement learning, as in RLAIF; yet our approach is also compatible in other contexts such as quality-diversity search. Extensive evidence of the capabilities of the proposed ZYN framework is provided through experiments in different domains related to text generation, including detoxification; optimizing sentiment of movie reviews, or any other attribute; steering the opinion about a particular topic the model may have; and personalizing prompt generators for text-to-image tasks. Code to be released at \url
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#23427;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;DCNFIS&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#65292;&#24182;&#19988;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#36890;&#36807;&#27169;&#31946;&#35268;&#21017;&#25552;&#21462;&#30340;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06378</link><description>&lt;p&gt;
DCNFIS&#65306;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System. (arXiv:2308.06378v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#23427;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;DCNFIS&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#65292;&#24182;&#19988;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#36890;&#36807;&#27169;&#31946;&#35268;&#21017;&#25552;&#21462;&#30340;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#36879;&#26126;&#24230;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#33879;&#21517;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32593;&#32476;&#35774;&#35745;&#65292;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#20294;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#24182;&#22312;&#22235;&#20010;&#33879;&#21517;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23427;&#19982;&#19977;&#20010;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#21516;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;DCNFIS&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#27169;&#31946;&#36923;&#36753;&#30340;&#36879;&#26126;&#24230;&#65292;&#20174;DCNFIS&#20013;&#32534;&#30721;&#30340;&#27169;&#31946;&#35268;&#21017;&#20013;&#25552;&#21462;&#35299;&#37322;&#65292;&#20197;&#28176;&#21464;&#26144;&#23556;&#30340;&#24418;&#24335;&#23637;&#31034;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;Fashion-MNIST&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#35299;&#37322;&#30340;&#29305;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#21512;&#20026;&#30693;&#35782;&#34920;&#31034;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.06374</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Knowledge Graphs: Opportunities and Challenges. (arXiv:2308.06374v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06374
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#21512;&#20026;&#30693;&#35782;&#34920;&#31034;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#32473;&#30693;&#35782;&#34920;&#31034;&#39046;&#22495;&#21644;&#25972;&#20010;&#19990;&#30028;&#24102;&#26469;&#20102;&#39118;&#26292;&#12290;&#36825;&#19968;&#36716;&#25240;&#28857;&#26631;&#24535;&#30528;&#30693;&#35782;&#34920;&#31034;&#20174;&#26174;&#24335;&#30693;&#35782;&#34920;&#31034;&#36716;&#21521;&#26174;&#24335;&#30693;&#35782;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#28151;&#21512;&#34920;&#31034;&#30340;&#28966;&#28857;&#30340;&#37325;&#22609;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35752;&#35770;LLMs&#65288;&#21442;&#25968;&#21270;&#30693;&#35782;&#65289;&#21644;&#30693;&#35782;&#22270;&#35889;&#65288;&#26174;&#24335;&#30693;&#35782;&#65289;&#39046;&#22495;&#20869;&#30340;&#19968;&#20123;&#24120;&#35265;&#20105;&#35770;&#65292;&#23637;&#26395;&#28966;&#28857;&#37325;&#22609;&#25152;&#24102;&#26469;&#30340;&#26426;&#36935;&#21644;&#24895;&#26223;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#30740;&#31350;&#20027;&#39064;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have taken Knowledge Representation -- and the world -- by storm. This inflection point marks a shift from explicit knowledge representation to a renewed focus on the hybrid representation of both explicit knowledge and parametric knowledge. In this position paper, we will discuss some of the common debate points within the community on LLMs (parametric knowledge) and Knowledge Graphs (explicit knowledge) and speculate on opportunities and visions that the renewed focus brings, as well as related research topics and challenges.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26080;&#32447;&#20449;&#36947;&#30340;&#38750;&#30456;&#24178;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#26469;&#38477;&#20302;&#32852;&#21512;k&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#30340;&#36890;&#20449;&#24310;&#36831;&#12290;&#27492;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#24179;&#34913;&#36827;&#21046;&#21644;&#20449;&#21495;&#21472;&#21152;&#24615;&#36136;&#23454;&#29616;&#32852;&#21512;k&#22343;&#20540;&#30340;&#26356;&#26032;&#65292;&#21516;&#26102;&#36890;&#36807;&#37325;&#26032;&#21021;&#22987;&#21270;&#26410;&#26377;&#25928;&#20351;&#29992;&#30340;&#36136;&#24515;&#26469;&#25913;&#21892;&#23545;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06371</link><description>&lt;p&gt;
&#26080;&#32447;&#32852;&#21512;k&#22343;&#20540;&#32858;&#31867;&#19982;&#38750;&#30456;&#24178;&#31354;&#20013;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Wireless Federated $k$-Means Clustering with Non-coherent Over-the-Air Computation. (arXiv:2308.06371v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26080;&#32447;&#20449;&#36947;&#30340;&#38750;&#30456;&#24178;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#26469;&#38477;&#20302;&#32852;&#21512;k&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#30340;&#36890;&#20449;&#24310;&#36831;&#12290;&#27492;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#24179;&#34913;&#36827;&#21046;&#21644;&#20449;&#21495;&#21472;&#21152;&#24615;&#36136;&#23454;&#29616;&#32852;&#21512;k&#22343;&#20540;&#30340;&#26356;&#26032;&#65292;&#21516;&#26102;&#36890;&#36807;&#37325;&#26032;&#21021;&#22987;&#21270;&#26410;&#26377;&#25928;&#20351;&#29992;&#30340;&#36136;&#24515;&#26469;&#25913;&#21892;&#23545;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32447;&#32852;&#21512;k&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#30340;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#65292;&#20197;&#38477;&#20302;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#23454;&#26045;&#26102;&#30340;&#27599;&#36718;&#36890;&#20449;&#24310;&#36831;&#12290;&#35813;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#20381;&#36182;&#20110;&#32534;&#30721;&#22120;&#21033;&#29992;&#24179;&#34913;&#36827;&#21046;&#20013;&#25968;&#23383;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#22810;&#22336;&#20449;&#36947;&#30340;&#20449;&#21495;&#21472;&#21152;&#24615;&#36136;&#38750;&#30456;&#24178;&#22320;&#35745;&#31639;&#32852;&#21512;k&#22343;&#20540;&#30340;&#26356;&#26032;&#20043;&#21644;&#65292;&#28040;&#38500;&#20102;&#23545;&#31934;&#30830;&#30456;&#20301;&#21644;&#26102;&#38388;&#21516;&#27493;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21021;&#22987;&#21270;&#26041;&#27861;&#26469;&#25913;&#21892;&#23545;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#26410;&#26377;&#25928;&#20351;&#29992;&#30340;&#36136;&#24515;&#30340;&#24615;&#33021;&#12290;&#38024;&#23545;&#23458;&#25143;&#20301;&#32622;&#32858;&#31867;&#22330;&#26223;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#26631;&#20934;k&#22343;&#20540;&#32858;&#31867;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#26041;&#27861;&#22312;&#38477;&#20302;&#36890;&#20449;&#24310;&#36831;&#30340;&#21516;&#26102;&#65292;&#19982;&#26631;&#20934;k&#22343;&#20540;&#32858;&#31867;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose using an over-the-air computation (OAC) scheme for the federated k-means clustering algorithm to reduce the per-round communication latency when it is implemented over a wireless network. The OAC scheme relies on an encoder exploiting the representation of a number in a balanced number system and computes the sum of the updates for the federated k-means via signal superposition property of wireless multiple-access channels non-coherently to eliminate the need for precise phase and time synchronization. Also, a reinitialization method for ineffectively used centroids is proposed to improve the performance of the proposed method for heterogeneous data distribution. For a customer-location clustering scenario, we demonstrate the performance of the proposed algorithm and compare it with the standard k-means clustering. Our results show that the proposed approach performs similarly to the standard k-means while reducing communication latency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36807;&#28388;&#27873;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#21644;&#27979;&#37327;&#29992;&#25143;&#23545;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#26469;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#30340;&#24847;&#22806;&#24615;&#29289;&#21697;&#12290;</title><link>http://arxiv.org/abs/2308.06368</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#21644;&#24847;&#22806;&#24615;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Topic-Level Bayesian Surprise and Serendipity for Recommender Systems. (arXiv:2308.06368v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36807;&#28388;&#27873;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#21644;&#27979;&#37327;&#29992;&#25143;&#23545;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#26469;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#30340;&#24847;&#22806;&#24615;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20248;&#21270;&#20854;&#25512;&#33616;&#20165;&#36866;&#21512;&#29992;&#25143;&#23545;&#24050;&#28040;&#36153;&#29289;&#21697;&#30340;&#35780;&#32423;&#21382;&#21490;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#28388;&#27873;&#65292;&#29992;&#25143;&#26080;&#27861;&#20174;&#26032;&#39062;&#12289;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#20013;&#20307;&#39564;&#29289;&#21697;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#24847;&#22806;&#24615;&#24418;&#24335;&#65292;&#20197;&#36125;&#21494;&#26031;&#24778;&#21916;&#20026;&#22522;&#30784;&#65292;&#29992;&#20110;&#27979;&#37327;&#29992;&#25143;&#28040;&#36153;&#24182;&#35780;&#32423;&#21518;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#12290;&#32467;&#21512;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#30340;&#21327;&#21516;&#36807;&#28388;&#32452;&#20214;&#65292;&#21487;&#20197;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#24847;&#22806;&#24615;&#30340;&#29289;&#21697;&#12290;&#20026;&#20102;&#20415;&#20110;&#35780;&#20272;&#20027;&#39064;&#32423;&#21035;&#30340;&#24778;&#21916;&#21644;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20174;Goodreads&#20013;&#25552;&#21462;&#30340;&#22270;&#20070;&#38405;&#35835;&#21382;&#21490;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;26&#21315;&#20010;&#29992;&#25143;&#21644;&#36817;130&#19975;&#26412;&#20070;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;449&#31687;&#20070;&#36827;&#34892;&#20102;&#25163;&#21160;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 449 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#25913;&#36827;&#20102;&#36825;&#20123;&#26497;&#26377;&#20215;&#20540;&#20294;&#24456;&#23569;&#34987;&#35760;&#24405;&#30340;&#20020;&#24202;&#25968;&#25454;&#30340;&#25552;&#21462;&#12290;&#26368;&#20339;&#27169;&#22411;&#20026;&#32463;&#36807;&#24494;&#35843;&#30340;Flan-T5 XL&#21644;Flan-T5 XXL&#65292;&#20854;&#20013;&#23567;&#22411;&#27169;&#22411;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06354</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#35782;&#21035;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Large Language Models to Identify Social Determinants of Health in Electronic Health Records. (arXiv:2308.06354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#25913;&#36827;&#20102;&#36825;&#20123;&#26497;&#26377;&#20215;&#20540;&#20294;&#24456;&#23569;&#34987;&#35760;&#24405;&#30340;&#20020;&#24202;&#25968;&#25454;&#30340;&#25552;&#21462;&#12290;&#26368;&#20339;&#27169;&#22411;&#20026;&#32463;&#36807;&#24494;&#35843;&#30340;Flan-T5 XL&#21644;Flan-T5 XXL&#65292;&#20854;&#20013;&#23567;&#22411;&#27169;&#22411;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#23545;&#24739;&#32773;&#30340;&#32467;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20294;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#25910;&#38598;&#19981;&#23436;&#25972;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;EHR&#20013;&#25552;&#21462;SDoH&#30340;&#33021;&#21147;&#65292;&#24182;&#25506;&#35752;&#20102;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#22312;&#25913;&#36827;&#36825;&#20123;&#23569;&#35265;&#20294;&#26497;&#26377;&#20215;&#20540;&#30340;&#20020;&#24202;&#25968;&#25454;&#25552;&#21462;&#20013;&#30340;&#20316;&#29992;&#12290;&#23545;800&#20221;&#24739;&#32773;&#35760;&#24405;&#36827;&#34892;&#20102;SDoH&#31867;&#21035;&#30340;&#27880;&#37322;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#36824;&#23581;&#35797;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#35780;&#20272;&#20102;&#31639;&#27861;&#20559;&#24046;&#12290;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#26159;&#32463;&#36807;&#24494;&#35843;&#30340;Flan-T5 XL&#65288;macro-F1 0.71&#65289;&#29992;&#20110;&#20219;&#20309;SDoH&#65292;&#20197;&#21450;Flan-T5 XXL&#65288;macro-F1 0.70&#65289;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#36741;&#21161;&#24494;&#35843;&#30340;&#25928;&#30410;&#22240;&#27169;&#22411;&#26550;&#26500;&#21644;&#22823;&#23567;&#32780;&#24322;&#65292;&#22312;&#36739;&#23567;&#30340;Flan-T5&#27169;&#22411;&#65288;&#22522;&#30784;&#21644;&#22823;&#22411;&#65289;&#20013;&#34920;&#29616;&#20986;&#26368;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;delta F1 +0.12&#21040;+0.23&#65289;&#12290;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social determinants of health (SDoH) have an important impact on patient outcomes but are incompletely collected from the electronic health records (EHR). This study researched the ability of large language models to extract SDoH from free text in EHRs, where they are most commonly documented, and explored the role of synthetic clinical text for improving the extraction of these scarcely documented, yet extremely valuable, clinical data. 800 patient notes were annotated for SDoH categories, and several transformer-based models were evaluated. The study also experimented with synthetic data generation and assessed for algorithmic bias. Our best-performing models were fine-tuned Flan-T5 XL (macro-F1 0.71) for any SDoH, and Flan-T5 XXL (macro-F1 0.70). The benefit of augmenting fine-tuning with synthetic data varied across model architecture and size, with smaller Flan-T5 models (base and large) showing the greatest improvements in performance (delta F1 +0.12 to +0.23). Model performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29305;&#24449;&#32858;&#21512;&#21644;&#20960;&#20309;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#26696;&#21160;&#29289;&#30340;&#37325;&#26032;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#26696;&#30340;&#22806;&#35266;&#30456;&#20284;&#24230;&#21644;&#20960;&#20309;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#32508;&#21512;&#24615;&#30340;&#37325;&#26032;&#35782;&#21035;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22270;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06335</link><description>&lt;p&gt;
&#32452;&#21512;&#29305;&#24449;&#32858;&#21512;&#21644;&#20960;&#20309;&#30456;&#20284;&#24615;&#29992;&#20110;&#22270;&#26696;&#21160;&#29289;&#30340;&#37325;&#26032;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Combining feature aggregation and geometric similarity for re-identification of patterned animals. (arXiv:2308.06335v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29305;&#24449;&#32858;&#21512;&#21644;&#20960;&#20309;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#26696;&#21160;&#29289;&#30340;&#37325;&#26032;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#26696;&#30340;&#22806;&#35266;&#30456;&#20284;&#24230;&#21644;&#20960;&#20309;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#32508;&#21512;&#24615;&#30340;&#37325;&#26032;&#35782;&#21035;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22270;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#21160;&#29289;&#20010;&#20307;&#37325;&#26032;&#35782;&#21035;&#21487;&#20197;&#33719;&#21462;&#21160;&#29289;&#38543;&#26102;&#38388;&#30340;&#36801;&#24473;&#27169;&#24335;&#31561;&#20449;&#24687;&#12290;&#36825;&#19982;&#20351;&#29992;&#30456;&#26426;&#38519;&#38449;&#21644;&#20247;&#21253;&#25910;&#38598;&#30340;&#22823;&#37327;&#22270;&#20687;&#21367;&#25171;&#24320;&#20102;&#30740;&#31350;&#21160;&#29289;&#31181;&#32676;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;&#23545;&#20110;&#35768;&#22810;&#29289;&#31181;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#27599;&#20010;&#20010;&#20307;&#29420;&#26377;&#30340;&#27704;&#20037;&#27611;&#12289;&#32701;&#27611;&#25110;&#30382;&#32932;&#22270;&#26696;&#26469;&#36827;&#34892;&#37325;&#26032;&#35782;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32452;&#21512;&#20004;&#31181;&#31867;&#22411;&#30340;&#22270;&#26696;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#35299;&#20915;&#37325;&#26032;&#35782;&#21035;&#38382;&#39064;&#65306;1&#65289;&#36890;&#36807;&#22270;&#26696;&#29305;&#24449;&#32858;&#21512;&#33719;&#24471;&#30340;&#22270;&#26696;&#22806;&#35266;&#30456;&#20284;&#24230;&#21644;2&#65289;&#36890;&#36807;&#20998;&#26512;&#22270;&#26696;&#30456;&#20284;&#24230;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#33719;&#24471;&#30340;&#20960;&#20309;&#22270;&#26696;&#30456;&#20284;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#32452;&#21512;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#22270;&#26696;&#29305;&#24449;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#37325;&#26032;&#35782;&#21035;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#22270;&#26696;&#31867;&#22411;&#12290;&#22312;&#23454;&#39564;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based re-identification of animal individuals allows gathering of information such as migration patterns of the animals over time. This, together with large image volumes collected using camera traps and crowdsourcing, opens novel possibilities to study animal populations. For many species, the re-identification can be done by analyzing the permanent fur, feather, or skin patterns that are unique to each individual. In this paper, we address the re-identification by combining two types of pattern similarity metrics: 1) pattern appearance similarity obtained by pattern feature aggregation and 2) geometric pattern similarity obtained by analyzing the geometric consistency of pattern similarities. The proposed combination allows to efficiently utilize both the local and global pattern features, providing a general re-identification approach that can be applied to a wide variety of different pattern types. In the experimental part of the work, we demonstrate that the method achieves 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#31070;&#32463;&#32593;&#32476;&#30340;&#37096;&#32626;&#20013;&#19981;&#34987;&#23519;&#35273;&#30340;&#28798;&#38590;&#24615;&#38382;&#39064;&#21644;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23553;&#35013;&#20102;&#37096;&#32626;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#24863;&#30693;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06299</link><description>&lt;p&gt;
&#38450;&#24481;&#24863;&#30693;&#65306;&#31070;&#32463;&#32593;&#32476;&#22312;&#37096;&#32626;&#20013;&#30340;&#24615;&#33021;&#20272;&#35745;&#21644;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment. (arXiv:2308.06299v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#31070;&#32463;&#32593;&#32476;&#30340;&#37096;&#32626;&#20013;&#19981;&#34987;&#23519;&#35273;&#30340;&#28798;&#38590;&#24615;&#38382;&#39064;&#21644;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23553;&#35013;&#20102;&#37096;&#32626;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#24863;&#30693;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#35821;&#20041;&#20998;&#21106;&#31070;&#32463;&#32593;&#32476;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#19981;&#34987;&#23519;&#35273;&#30340;&#28798;&#38590;&#24615;&#38382;&#39064;&#21644;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24863;&#30693;&#23545;&#33258;&#21160;&#39550;&#39542;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#23558;&#20854;&#26368;&#20339;&#34920;&#31034;&#20026;&#27010;&#29575;&#20998;&#24067;&#12290;&#30001;&#20110;&#33258;&#21160;&#36710;&#36742;&#30340;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#24863;&#30693;&#31995;&#32479;&#24517;&#39035;&#33021;&#22815;&#35782;&#21035;&#36710;&#36742;&#26159;&#21542;&#31163;&#24320;&#20102;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#65292;&#39044;&#27979;&#21361;&#38505;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#38477;&#20302;&#24863;&#30693;&#31995;&#32479;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#37096;&#32626;&#20013;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23553;&#35013;&#65292;&#35813;&#23553;&#35013;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599; Dropout &#26041;&#27861;&#36890;&#36807;&#23545;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20462;&#25913;&#37096;&#32626;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20445;&#35777;&#39044;&#26399;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#24615;&#24863;&#30693;&#23553;&#35013;&#33021;&#22815;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a method for addressing the issue of unnoticed catastrophic deployment and domain shift in neural networks for semantic segmentation in autonomous driving. Our approach is based on the idea that deep learning-based perception for autonomous driving is uncertain and best represented as a probability distribution. As autonomous vehicles' safety is paramount, it is crucial for perception systems to recognize when the vehicle is leaving its operational design domain, anticipate hazardous uncertainty, and reduce the performance of the perception system. To address this, we propose to encapsulate the neural network under deployment within an uncertainty estimation envelope that is based on the epistemic uncertainty estimation through the Monte Carlo Dropout approach. This approach does not require modification of the deployed neural network and guarantees expected model performance. Our defensive perception envelope has the capability to estimate a neural network's 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21457;&#20102;&#20004;&#31181;&#34920;&#22411;&#35782;&#21035;&#27169;&#22411;PhenoBCBERT&#21644;PhenoGPT&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#35268;&#21017;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20020;&#24202;&#34920;&#22411;&#26415;&#35821;&#65292;&#21253;&#25324;HPO&#26410;&#35760;&#24405;&#30340;&#26032;&#26415;&#35821;&#12290;</title><link>http://arxiv.org/abs/2308.06294</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20020;&#24202;&#31508;&#35760;&#20013;&#34920;&#22411;&#35782;&#21035;&#33021;&#21147;&#65306;PhenoBCBERT&#21644;PhenoGPT
&lt;/p&gt;
&lt;p&gt;
Enhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT. (arXiv:2308.06294v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21457;&#20102;&#20004;&#31181;&#34920;&#22411;&#35782;&#21035;&#27169;&#22411;PhenoBCBERT&#21644;PhenoGPT&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#35268;&#21017;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20020;&#24202;&#34920;&#22411;&#26415;&#35821;&#65292;&#21253;&#25324;HPO&#26410;&#35760;&#24405;&#30340;&#26032;&#26415;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20551;&#35774;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#23454;&#29616;&#23545;&#20020;&#24202;&#34920;&#22411;&#26415;&#35821;&#30340;&#33258;&#21160;&#26816;&#27979;&#65292;&#21253;&#25324;&#26410;&#22312;HPO&#20013;&#35760;&#24405;&#30340;&#26415;&#35821;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#27169;&#22411;&#65306;PhenoBCBERT&#65292;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;Bio+Clinical BERT&#20316;&#20026;&#20854;&#39044;&#35757;&#32451;&#27169;&#22411;&#65307;&#20197;&#21450;PhenoGPT&#65292;&#19968;&#31181;&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#19981;&#21516;&#30340;GPT&#27169;&#22411;&#65288;&#21253;&#25324;&#24320;&#28304;&#29256;&#26412;&#22914;GPT-J&#12289;Falcon&#21644;LLaMA&#65292;&#20197;&#21450;&#20851;&#38381;&#28304;&#29256;&#26412;&#22914;GPT-3&#21644;GPT-3.5&#65289;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#36817;&#24320;&#21457;&#30340;&#32467;&#21512;&#20102;&#22522;&#20110;&#35268;&#21017;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;HPO&#35782;&#21035;&#24037;&#20855;PhenoTagger&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#26356;&#22810;&#30340;&#34920;&#22411;&#27010;&#24565;&#65292;&#21253;&#25324;HPO&#26410;&#25551;&#36848;&#30340;&#26032;&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#23545;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#35828;&#26126;&#22914;&#20309;&#35782;&#21035;&#21644;&#25552;&#21462;&#26032;&#30340;&#34920;&#22411;&#20449;&#24687;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#24403;&#21069;&#22522;&#20110;BERT&#21644;&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#34920;&#22411;&#26631;&#35760;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We hypothesize that large language models (LLMs) based on the transformer architecture can enable automated detection of clinical phenotype terms, including terms not documented in the HPO. In this study, we developed two types of models: PhenoBCBERT, a BERT-based model, utilizing Bio+Clinical BERT as its pre-trained model, and PhenoGPT, a GPT-based model that can be initialized from diverse GPT models, including open-source versions such as GPT-J, Falcon, and LLaMA, as well as closed-source versions such as GPT-3 and GPT-3.5. We compared our methods with PhenoTagger, a recently developed HPO recognition tool that combines rule-based and deep learning methods. We found that our methods can extract more phenotype concepts, including novel ones not characterized by HPO. We also performed case studies on biomedical literature to illustrate how new phenotype information can be recognized and extracted. We compared current BERT-based versus GPT-based models for phenotype tagging, in multipl
&lt;/p&gt;</description></item><item><title>VIS4ML&#30740;&#31350;&#22312;&#25512;&#24191;&#23454;&#36341;&#24212;&#29992;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#38656;&#35201;&#22635;&#34917;&#26410;&#34987;&#20195;&#34920;&#24615;&#24773;&#20917;&#36807;&#24230;&#25311;&#21512;&#12289;&#32570;&#20047;&#20851;&#38190;&#20381;&#36182;&#22240;&#32032;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2308.06290</link><description>&lt;p&gt;
&#25105;&#20204;&#24050;&#32463;&#38381;&#29615;&#20102;&#21527;&#65311;VIS4ML&#30740;&#31350;&#20013;&#30340;&#25512;&#24191;&#24615;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Are We Closing the Loop Yet? Gaps in the Generalizability of VIS4ML Research. (arXiv:2308.06290v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06290
&lt;/p&gt;
&lt;p&gt;
VIS4ML&#30740;&#31350;&#22312;&#25512;&#24191;&#23454;&#36341;&#24212;&#29992;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#38656;&#35201;&#22635;&#34917;&#26410;&#34987;&#20195;&#34920;&#24615;&#24773;&#20917;&#36807;&#24230;&#25311;&#21512;&#12289;&#32570;&#20047;&#20851;&#38190;&#20381;&#36182;&#22240;&#32032;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21487;&#35270;&#21270;(VIS4ML)&#30740;&#31350;&#26088;&#22312;&#24110;&#21161;&#19987;&#23478;&#24212;&#29992;&#20854;&#20808;&#21069;&#30693;&#35782;&#26469;&#24320;&#21457;&#12289;&#29702;&#35299;&#21644;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#26500;&#24605;VIS4ML&#31995;&#32479;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#25903;&#25345;&#20154;&#26426;&#20132;&#20114;&#20219;&#21153;&#30340;&#20154;&#31867;&#30693;&#35782;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#35774;&#35745;&#20102;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#20197;&#20351;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#21487;&#35299;&#37322;&#24182;&#24341;&#20986;&#30693;&#35782;&#65292;&#24182;&#35780;&#20272;&#20102;&#20154;&#26426;&#20114;&#25442;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;VIS4ML&#35770;&#25991;&#65292;&#20197;&#35780;&#20272;&#30740;&#31350;&#36129;&#29486;&#21644;&#25512;&#21160;&#20154;&#26426;&#20132;&#20114;&#26426;&#22120;&#23398;&#20064;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;VIS4ML&#30740;&#31350;&#30340;&#33539;&#22260;&#19982;&#23454;&#38469;&#24212;&#29992;&#30340;&#26399;&#26395;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#35770;&#25991;&#35748;&#20026;VIS4ML&#31995;&#32479;&#36866;&#29992;&#20110;&#30740;&#31350;&#20043;&#22806;&#30340;&#26465;&#20214;&#65292;&#20294;&#32467;&#35770;&#24448;&#24448;&#36807;&#24230;&#25311;&#21512;&#38750;&#20195;&#34920;&#24615;&#22330;&#26223;&#65292;&#22522;&#20110;&#19982;&#23569;&#37327;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#21644;&#20247;&#25152;&#21608;&#30693;&#25968;&#25454;&#38598;&#30340;&#20114;&#21160;&#65292;&#26410;&#33021;&#25215;&#35748;&#20851;&#38190;&#20381;&#36182;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visualization for machine learning (VIS4ML) research aims to help experts apply their prior knowledge to develop, understand, and improve the performance of machine learning models. In conceiving VIS4ML systems, researchers characterize the nature of human knowledge to support human-in-the-loop tasks, design interactive visualizations to make ML components interpretable and elicit knowledge, and evaluate the effectiveness of human-model interchange. We survey recent VIS4ML papers to assess the generalizability of research contributions and claims in enabling human-in-the-loop ML. Our results show potential gaps between the current scope of VIS4ML research and aspirations for its use in practice. We find that while papers motivate that VIS4ML systems are applicable beyond the specific conditions studied, conclusions are often overfitted to non-representative scenarios, are based on interactions with a small set of ML experts and well-understood datasets, fail to acknowledge crucial depe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#12290;&#24403;&#21069;&#30340;&#35780;&#20272;&#36807;&#31243;&#19981;&#20840;&#38754;&#35780;&#20272;XAI&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#20063;&#26410;&#23558;&#35299;&#37322;&#23545;&#20154;&#31867;&#30340;&#24433;&#21709;&#35270;&#20026;&#19968;&#31181;&#22797;&#26434;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25972;&#21512;&#20102;&#35299;&#37322;&#30340;&#22240;&#32032;&#65292;&#24635;&#32467;&#20102;&#35299;&#37322;&#30340;&#29305;&#24615;&#65292;&#24182;&#20998;&#31867;&#20102;&#27979;&#37327;&#36825;&#20123;&#29305;&#24615;&#30340;&#25351;&#26631;&#65292;&#26088;&#22312;&#20026;XAI&#35780;&#20272;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26631;&#20934;&#21270;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.06274</link><description>&lt;p&gt;
&#20026;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26500;&#24314;&#19968;&#20010;&#32508;&#21512;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards a Comprehensive Human-Centred Evaluation Framework for Explainable AI. (arXiv:2308.06274v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#12290;&#24403;&#21069;&#30340;&#35780;&#20272;&#36807;&#31243;&#19981;&#20840;&#38754;&#35780;&#20272;XAI&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#20063;&#26410;&#23558;&#35299;&#37322;&#23545;&#20154;&#31867;&#30340;&#24433;&#21709;&#35270;&#20026;&#19968;&#31181;&#22797;&#26434;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25972;&#21512;&#20102;&#35299;&#37322;&#30340;&#22240;&#32032;&#65292;&#24635;&#32467;&#20102;&#35299;&#37322;&#30340;&#29305;&#24615;&#65292;&#24182;&#20998;&#31867;&#20102;&#27979;&#37327;&#36825;&#20123;&#29305;&#24615;&#30340;&#25351;&#26631;&#65292;&#26088;&#22312;&#20026;XAI&#35780;&#20272;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26631;&#20934;&#21270;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#30740;&#31350;&#27491;&#22312;&#34028;&#21187;&#21457;&#23637;&#65292;&#24182;&#19988;&#35299;&#37322;&#25216;&#26415;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#21069;&#26223;&#30340;&#65292;&#20294;&#26631;&#20934;&#21270;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#36807;&#31243;&#20173;&#28982;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#36807;&#31243;&#24182;&#26410;&#20840;&#38754;&#35780;&#20272;XAI&#26041;&#27861;&#65292;&#21363;&#23427;&#20204;&#26410;&#23558;&#35299;&#37322;&#23545;&#20154;&#31867;&#30340;&#24433;&#21709;&#35270;&#20026;&#19968;&#31181;&#22797;&#26434;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24314;&#35758;&#20511;&#37492;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#29992;&#25143;&#20013;&#24515;&#35780;&#20272;&#26694;&#26550;&#65306;&#25105;&#20204;&#25972;&#21512;&#35299;&#37322;&#26041;&#38754;&#30340;&#22240;&#32032;&#65292;&#24635;&#32467;&#35299;&#37322;&#30340;&#29305;&#24615;&#65292;&#25351;&#31034;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23545;&#27979;&#37327;&#36825;&#20123;&#29305;&#24615;&#30340;&#25351;&#26631;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#36825;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#24076;&#26395;&#20026;XAI&#35780;&#20272;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26631;&#20934;&#21270;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
While research on explainable AI (XAI) is booming and explanation techniques have proven promising in many application domains, standardised human-centred evaluation procedures are still missing. In addition, current evaluation procedures do not assess XAI methods holistically in the sense that they do not treat explanations' effects on humans as a complex user experience. To tackle this challenge, we propose to adapt the User-Centric Evaluation Framework used in recommender systems: we integrate explanation aspects, summarise explanation properties, indicate relations between them, and categorise metrics that measure these properties. With this comprehensive evaluation framework, we hope to contribute to the human-centred standardisation of XAI evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22914;&#20309;&#22609;&#36896;&#20803;&#23431;&#23449;&#65292;&#23558;&#20854;&#36716;&#21464;&#20026;&#19968;&#20010;&#20805;&#28385;&#27963;&#21147;&#12289;&#27785;&#28024;&#24335;&#21644;&#20114;&#21160;&#24335;&#30340;&#34394;&#25311;&#19990;&#30028;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;GPT-3&#65289;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;DALL-E&#21644;MidJourney&#65289;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#19977;&#32500;&#27169;&#22411;&#29983;&#25104;&#25216;&#26415;&#65288;&#22914;Point-E&#21644;Lumirithmic&#65289;&#30340;&#28508;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#23454;&#26045;&#36825;&#20123;&#25216;&#26415;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2308.06272</link><description>&lt;p&gt;
&#36229;&#36234;&#29616;&#23454;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20803;&#23431;&#23449;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Beyond Reality: The Pivotal Role of Generative AI in the Metaverse. (arXiv:2308.06272v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22914;&#20309;&#22609;&#36896;&#20803;&#23431;&#23449;&#65292;&#23558;&#20854;&#36716;&#21464;&#20026;&#19968;&#20010;&#20805;&#28385;&#27963;&#21147;&#12289;&#27785;&#28024;&#24335;&#21644;&#20114;&#21160;&#24335;&#30340;&#34394;&#25311;&#19990;&#30028;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;GPT-3&#65289;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;DALL-E&#21644;MidJourney&#65289;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#19977;&#32500;&#27169;&#22411;&#29983;&#25104;&#25216;&#26415;&#65288;&#22914;Point-E&#21644;Lumirithmic&#65289;&#30340;&#28508;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#23454;&#26045;&#36825;&#20123;&#25216;&#26415;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24819;&#35937;&#19968;&#19979;&#65292;&#36827;&#20837;&#19968;&#20010;&#19982;&#25105;&#20204;&#29289;&#29702;&#19990;&#30028;&#19968;&#26679;&#20016;&#23500;&#12289;&#21160;&#24577;&#21644;&#20114;&#21160;&#30340;&#34394;&#25311;&#19990;&#30028;&#12290;&#36825;&#23601;&#26159;&#20803;&#23431;&#23449;&#30340;&#25215;&#35834;&#65292;&#32780;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#21464;&#38761;&#21147;&#37327;&#27491;&#22312;&#32473;&#23427;&#27880;&#20837;&#29983;&#21629;&#12290;&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22914;&#20309;&#22609;&#36896;&#20803;&#23431;&#23449;&#65292;&#23558;&#20854;&#36716;&#21464;&#20026;&#19968;&#20010;&#20805;&#28385;&#27963;&#21147;&#12289;&#27785;&#28024;&#24335;&#21644;&#20114;&#21160;&#24335;&#30340;&#34394;&#25311;&#19990;&#30028;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;GPT-3&#65289;&#22312;&#22686;&#24378;&#24102;&#26377;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35282;&#33394;&#30340;&#23545;&#35805;&#30028;&#38754;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;DALL-E&#21644;MidJourney&#65289;&#22312;&#21019;&#24314;&#35270;&#35273;&#19978;&#20196;&#20154;&#24778;&#33395;&#21644;&#22810;&#26679;&#21270;&#30340;&#20869;&#23481;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#32771;&#23519;&#20102;&#19977;&#32500;&#27169;&#22411;&#29983;&#25104;&#25216;&#26415;&#65288;&#22914;Point-E&#21644;Lumirithmic&#65289;&#22312;&#21019;&#24314;&#20016;&#23500;&#20803;&#23431;&#23449;&#20307;&#39564;&#30340;&#36924;&#30495;&#34394;&#25311;&#29289;&#20307;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20294;&#26053;&#31243;&#24182;&#19981;&#27490;&#20110;&#27492;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#23454;&#26045;&#36825;&#20123;&#25216;&#26415;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imagine stepping into a virtual world that's as rich, dynamic, and interactive as our physical one. This is the promise of the Metaverse, and it's being brought to life by the transformative power of Generative Artificial Intelligence (AI). This paper offers a comprehensive exploration of how generative AI technologies are shaping the Metaverse, transforming it into a dynamic, immersive, and interactive virtual world. We delve into the applications of text generation models like ChatGPT and GPT-3, which are enhancing conversational interfaces with AI-generated characters. We explore the role of image generation models such as DALL-E and MidJourney in creating visually stunning and diverse content. We also examine the potential of 3D model generation technologies like Point-E and Lumirithmic in creating realistic virtual objects that enrich the Metaverse experience. But the journey doesn't stop there. We also address the challenges and ethical considerations of implementing these techno
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#19977;&#32500;&#28857;&#20113;&#25968;&#25454;&#30340;&#26059;&#36716;&#19981;&#21464;&#20989;&#25968;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#22312;3D&#28857;&#20113;&#19978;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.06271</link><description>&lt;p&gt;
&#26059;&#36716;&#19981;&#21464;&#30340;&#38543;&#26426;&#29305;&#24449;&#20026;3D&#28857;&#20113;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds. (arXiv:2308.06271v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#23398;&#20064;&#19977;&#32500;&#28857;&#20113;&#25968;&#25454;&#30340;&#26059;&#36716;&#19981;&#21464;&#20989;&#25968;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#22312;3D&#28857;&#20113;&#19978;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#19981;&#21464;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#35768;&#22810;&#39046;&#22495;&#20351;&#29992;&#30340;&#19968;&#31181;&#26222;&#36941;&#30340;&#24402;&#32435;&#20559;&#22909;&#65292;&#20363;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#37327;&#23376;&#21270;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#26059;&#36716;&#19981;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#21253;&#25324;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;3D&#24418;&#29366;&#20998;&#31867;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#20381;&#36182;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#26059;&#36716;&#19981;&#21464;&#29305;&#24449;&#65292;&#35201;&#20040;&#20351;&#29992;&#22797;&#26434;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#36890;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#26041;&#27861;&#30340;&#25104;&#21151;&#20027;&#35201;&#26159;&#30001;&#20110;&#26059;&#36716;&#19981;&#21464;&#24615;&#36824;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#19977;&#32500;&#28857;&#20113;&#25968;&#25454;&#30340;&#26059;&#36716;&#19981;&#21464;&#20989;&#25968;&#65292;&#37319;&#29992;&#20102;&#38543;&#26426;&#29305;&#24449;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#20986;&#19968;&#31181;&#23545;&#19977;&#32500;&#26059;&#36716;&#19981;&#21464;&#30340;&#29256;&#26412;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#28857;&#20113;&#25968;&#25454;&#19978;&#30340;&#24555;&#36895;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotational invariance is a popular inductive bias used by many fields in machine learning, such as computer vision and machine learning for quantum chemistry. Rotation-invariant machine learning methods set the state of the art for many tasks, including molecular property prediction and 3D shape classification. These methods generally either rely on task-specific rotation-invariant features, or they use general-purpose deep neural networks which are complicated to design and train. However, it is unclear whether the success of these methods is primarily due to the rotation invariance or the deep neural networks. To address this question, we suggest a simple and general-purpose method for learning rotation-invariant functions of three-dimensional point cloud data using a random features approach. Specifically, we extend the random features method of Rahimi &amp; Recht 2007 by deriving a version that is invariant to three-dimensional rotations and showing that it is fast to evaluate on point
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;ZeroShotALI&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25913;&#36827;&#37329;&#34701;&#23457;&#35745;&#20013;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21305;&#37197;&#12290;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#32463;&#36807;&#39046;&#22495;&#20248;&#21270;&#30340;&#22522;&#20110;transformer&#30340;&#25991;&#26412;&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#31995;&#32479;&#23454;&#29616;&#20102;&#20174;&#25253;&#21578;&#20013;&#25512;&#33616;&#19982;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#65292;&#24182;&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.06111</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#37329;&#34701;&#23457;&#35745;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models. (arXiv:2308.06111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06111
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;ZeroShotALI&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25913;&#36827;&#37329;&#34701;&#23457;&#35745;&#20013;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21305;&#37197;&#12290;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#32463;&#36807;&#39046;&#22495;&#20248;&#21270;&#30340;&#22522;&#20110;transformer&#30340;&#25991;&#26412;&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#31995;&#32479;&#23454;&#29616;&#20102;&#20174;&#25253;&#21578;&#20013;&#25512;&#33616;&#19982;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#65292;&#24182;&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23457;&#35745;&#37329;&#34701;&#25991;&#20214;&#26159;&#19968;&#20010;&#38750;&#24120;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#30446;&#21069;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#31616;&#21270;&#36825;&#19968;&#36807;&#31243;&#65292;&#20197;&#25512;&#33616;&#19982;&#20005;&#26684;&#20250;&#35745;&#26631;&#20934;&#30340;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#25253;&#21578;&#20013;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#23450;&#26399;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#32570;&#20047;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ZeroShotALI&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;&#20248;&#21270;&#30340;&#22522;&#20110;transformer&#30340;&#25991;&#26412;&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#39318;&#20808;&#20351;&#29992;&#33258;&#23450;&#20041;BERT&#27169;&#22411;&#26816;&#32034;&#19982;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#33509;&#24178;&#26368;&#20339;&#21305;&#37197;&#30340;&#25991;&#26723;&#37096;&#20998;&#65292;&#28982;&#21518;&#20351;&#29992;LLM&#23545;&#36825;&#20123;&#36873;&#25321;&#36827;&#34892;&#36807;&#28388;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auditing financial documents is a very tedious and time-consuming process. As of today, it can already be simplified by employing AI-based solutions to recommend relevant text passages from a report for each legal requirement of rigorous accounting standards. However, these methods need to be fine-tuned regularly, and they require abundant annotated data, which is often lacking in industrial environments. Hence, we present ZeroShotALI, a novel recommender system that leverages a state-of-the-art large language model (LLM) in conjunction with a domain-specifically optimized transformer-based text-matching solution. We find that a two-step approach of first retrieving a number of best matching document sections per legal requirement with a custom BERT-based model and second filtering these selections using an LLM yields significant performance improvements over existing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#39057;-&#35270;&#35273;&#31354;&#38388;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#25972;&#21512;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#31354;&#38388;&#32447;&#32034;&#26469;&#27169;&#20223;&#20154;&#31867;&#26816;&#27979;&#22768;&#38899;&#20135;&#29983;&#29289;&#20307;&#30340;&#34892;&#20026;&#65292;&#24182;&#24341;&#20837;&#36882;&#24402;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#27880;&#24847;&#21147;&#21306;&#22495;&#12290;&#36890;&#36807;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#31354;&#38388;&#32447;&#32034;&#21644;&#36882;&#24402;&#32858;&#28966;&#31574;&#30053;&#65292;&#26041;&#27861;&#22312;&#22768;&#28304;&#23450;&#20301;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06087</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#22768;&#28304;&#23450;&#20301;&#30340;&#38899;&#39057;-&#35270;&#35273;&#31354;&#38388;&#34701;&#21512;&#21644;&#36882;&#24402;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Audio-Visual Spatial Integration and Recursive Attention for Robust Sound Source Localization. (arXiv:2308.06087v1 [cs.MM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#39057;-&#35270;&#35273;&#31354;&#38388;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#25972;&#21512;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#31354;&#38388;&#32447;&#32034;&#26469;&#27169;&#20223;&#20154;&#31867;&#26816;&#27979;&#22768;&#38899;&#20135;&#29983;&#29289;&#20307;&#30340;&#34892;&#20026;&#65292;&#24182;&#24341;&#20837;&#36882;&#24402;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#27880;&#24847;&#21147;&#21306;&#22495;&#12290;&#36890;&#36807;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#31354;&#38388;&#32447;&#32034;&#21644;&#36882;&#24402;&#32858;&#28966;&#31574;&#30053;&#65292;&#26041;&#27861;&#22312;&#22768;&#28304;&#23450;&#20301;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#28304;&#23450;&#20301;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#35753;&#26426;&#22120;&#33021;&#22815;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#26816;&#27979;&#20986;&#22768;&#38899;&#20135;&#29983;&#29289;&#20307;&#30340;&#20301;&#32622;&#12290;&#34429;&#28982;&#38899;&#39057;&#27169;&#24577;&#25552;&#20379;&#20102;&#23450;&#20301;&#22768;&#28304;&#30340;&#31354;&#38388;&#32447;&#32034;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20165;&#23558;&#38899;&#39057;&#20316;&#20026;&#35270;&#35273;&#27169;&#24577;&#31354;&#38388;&#21306;&#22495;&#27604;&#36739;&#30340;&#36741;&#21161;&#35282;&#33394;&#12290;&#32780;&#20154;&#31867;&#21017;&#21033;&#29992;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#20316;&#20026;&#23450;&#20301;&#22768;&#28304;&#30340;&#31354;&#38388;&#32447;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#39057;-&#35270;&#35273;&#31354;&#38388;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#25972;&#21512;&#20004;&#31181;&#27169;&#24577;&#30340;&#31354;&#38388;&#32447;&#32034;&#26469;&#27169;&#20223;&#20154;&#31867;&#26816;&#27979;&#22768;&#38899;&#20135;&#29983;&#29289;&#20307;&#26102;&#30340;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36882;&#24402;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#27169;&#20223;&#20154;&#31867;&#36845;&#20195;&#22320;&#32858;&#28966;&#23545;&#35937;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#27880;&#24847;&#21147;&#21306;&#22495;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#32534;&#30721;&#20004;&#31181;&#27169;&#24577;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38899;&#39057;-&#35270;&#35273;&#37197;&#23545;&#21305;&#37197;&#25439;&#22833;&#21644;&#31354;&#38388;&#21306;&#22495;&#23545;&#40784;&#25439;&#22833;&#12290;&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;-&#35270;&#35273;&#27169;&#24577;&#30340;&#31354;&#38388;&#32447;&#32034;&#21644;&#36882;&#24402;&#32858;&#28966;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22768;&#28304;&#23450;&#20301;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of the sound source localization task is to enable machines to detect the location of sound-making objects within a visual scene. While the audio modality provides spatial cues to locate the sound source, existing approaches only use audio as an auxiliary role to compare spatial regions of the visual modality. Humans, on the other hand, utilize both audio and visual modalities as spatial cues to locate sound sources. In this paper, we propose an audio-visual spatial integration network that integrates spatial cues from both modalities to mimic human behavior when detecting sound-making objects. Additionally, we introduce a recursive attention network to mimic human behavior of iterative focusing on objects, resulting in more accurate attention regions. To effectively encode spatial information from both modalities, we propose audio-visual pair matching loss and spatial region alignment loss. By utilizing the spatial cues of audio-visual modalities and recursively focusing
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06053</link><description>&lt;p&gt;
&#22312;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#19978;&#20855;&#26377;MiRo&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#35774;&#22791;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cost-effective On-device Continual Learning over Memory Hierarchy with Miro. (arXiv:2308.06053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#20174;&#25345;&#32493;&#30340;&#20219;&#21153;&#27969;&#20013;&#36880;&#27493;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20026;&#20102;&#35760;&#20303;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#26087;&#26679;&#26412;&#23384;&#20648;&#22312;&#19968;&#20010;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#20013;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#21040;&#26469;&#26102;&#36827;&#34892;&#22238;&#25918;&#12290;&#37319;&#29992;&#25345;&#32493;&#23398;&#20064;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#36793;&#32536;&#35774;&#22791;&#36890;&#24120;&#23545;&#33021;&#28304;&#25935;&#24863;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#19981;&#25439;&#23475;&#33021;&#28304;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#27169;&#22411;&#20934;&#30830;&#24230;&#65292;&#21363;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#20197;&#33719;&#24471;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#36890;&#36807;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#36164;&#28304;&#29366;&#24577;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#65292;&#20174;&#32780;&#23558;&#25105;&#20204;&#30340;&#35265;&#35299;&#31934;&#30830;&#22320;&#25972;&#21512;&#21040;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#25104;&#26412;&#25928;&#30410;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;Miro&#36824;&#23545;&#24102;&#26377;&#26126;&#30830;&#20934;&#30830;&#24230;-&#33021;&#37327;&#24179;&#34913;&#30340;&#21442;&#25968;&#36827;&#34892;&#22312;&#32447;&#20998;&#26512;&#65292;&#24182;&#20197;&#20302;&#24320;&#38144;&#22320;&#36866;&#24212;&#26368;&#20339;&#20540;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;WavLM&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;&#25163;&#21183;&#21512;&#25104;&#26041;&#27861;&#65292;&#23454;&#29616;&#21482;&#20351;&#29992;&#21407;&#22987;&#35821;&#38899;&#38899;&#39057;&#29983;&#25104;&#20010;&#24615;&#21270;&#20840;&#36523;&#25163;&#21183;&#65292;&#28040;&#38500;&#20102;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#22788;&#29702;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.05995</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#38899;&#39057;&#65306;&#21033;&#29992;WavLM&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model. (arXiv:2308.05995v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;WavLM&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;&#25163;&#21183;&#21512;&#25104;&#26041;&#27861;&#65292;&#23454;&#29616;&#21482;&#20351;&#29992;&#21407;&#22987;&#35821;&#38899;&#38899;&#39057;&#29983;&#25104;&#20010;&#24615;&#21270;&#20840;&#36523;&#25163;&#21183;&#65292;&#28040;&#38500;&#20102;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#22788;&#29702;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34394;&#25311;&#20154;&#21019;&#20316;&#39046;&#22495;&#65292;&#29983;&#25104;&#19982;&#35821;&#38899;&#37197;&#22871;&#30340;&#25163;&#21183;&#26159;&#19968;&#20010;&#27491;&#22312;&#20852;&#36215;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20197;&#22768;&#23398;&#21644;&#35821;&#20041;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#65292;&#37319;&#29992;&#20998;&#31867;&#26041;&#27861;&#26469;&#35782;&#21035;&#20154;&#29289;&#30340;ID&#21644;&#24773;&#24863;&#65292;&#20197;&#39537;&#21160;&#19982;&#35821;&#38899;&#37197;&#22871;&#30340;&#25163;&#21183;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#24037;&#20316;&#20173;&#28982;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#19981;&#20165;&#28041;&#21450;&#25163;&#21183;&#12289;&#35821;&#38899;&#22768;&#23398;&#21644;&#35821;&#20041;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36824;&#21253;&#25324;&#19982;&#20010;&#24615;&#12289;&#24773;&#24863;&#21644;&#20854;&#20182;&#19981;&#26126;&#30830;&#20294;&#37325;&#35201;&#30340;&#22240;&#32032;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;diffmotion-v2&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#26465;&#20214;&#25193;&#25955;&#21644;&#22522;&#20110;&#38750;&#33258;&#22238;&#24402;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;WavLM&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#20165;&#20351;&#29992;&#21407;&#22987;&#35821;&#38899;&#38899;&#39057;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#20840;&#36523;&#25163;&#21183;&#65292;&#28040;&#38500;&#20102;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#22788;&#29702;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces "diffmotion-v2," a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acou
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39046;&#22495;&#25512;&#33616;&#26041;&#27861;EDDA&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#35299;&#32806;&#25512;&#33616;&#22120;&#21644;&#39046;&#22495;&#23545;&#40784;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#20998;&#21035;&#35299;&#20915;&#20102;&#30693;&#35782;&#35299;&#32806;&#21644;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.05508</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;&#25512;&#33616;&#20013;&#30340;&#23884;&#20837;&#35299;&#32806;&#19982;&#39046;&#22495;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Multi-domain Recommendation with Embedding Disentangling and Domain Alignment. (arXiv:2308.05508v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05508
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39046;&#22495;&#25512;&#33616;&#26041;&#27861;EDDA&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#35299;&#32806;&#25512;&#33616;&#22120;&#21644;&#39046;&#22495;&#23545;&#40784;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#20998;&#21035;&#35299;&#20915;&#20102;&#30693;&#35782;&#35299;&#32806;&#21644;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#25512;&#33616;(MDR)&#26088;&#22312;&#20026;&#20855;&#26377;&#37325;&#21472;&#29992;&#25143;/&#29289;&#21697;&#30340;&#19981;&#21516;&#39046;&#22495;(&#20363;&#22914;&#20135;&#21697;&#31867;&#22411;)&#25552;&#20379;&#25512;&#33616;&#65292;&#23545;&#20110;&#25317;&#26377;&#22810;&#20010;&#26381;&#21153;&#30340;&#24179;&#21488;&#22914;&#20122;&#39532;&#36874;&#12289;Facebook&#21644;LinkedIn&#26159;&#24120;&#35265;&#30340;&#12290;&#29616;&#26377;&#30340;MDR&#27169;&#22411;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#24456;&#38590;&#35299;&#32806;&#21487;&#20197;&#27867;&#21270;&#21040;&#25152;&#26377;&#39046;&#22495;&#30340;&#30693;&#35782;(&#20363;&#22914;&#65292;&#29992;&#25143;&#21916;&#27426;&#24265;&#20215;&#30340;&#29289;&#21697;)&#19982;&#29305;&#23450;&#20110;&#21333;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;(&#20363;&#22914;&#65292;&#29992;&#25143;&#21916;&#27426;&#34013;&#33394;&#30340;&#26381;&#35013;&#20294;&#19981;&#21916;&#27426;&#34013;&#33394;&#30340;&#27773;&#36710;)&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#22312;&#20855;&#26377;&#23567;&#37325;&#21472;&#30340;&#39046;&#22495;&#20043;&#38388;&#36716;&#31227;&#30693;&#35782;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDDA&#30340;&#26032;&#30340;MDR&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#23884;&#20837;&#35299;&#32806;&#25512;&#33616;&#22120;&#21644;&#39046;&#22495;&#23545;&#40784;&#65292;&#20998;&#21035;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#29305;&#21035;&#22320;&#65292;&#23884;&#20837;&#35299;&#32806;&#25512;&#33616;&#22120;&#20998;&#31163;&#20102;&#36328;&#39046;&#22495;&#37096;&#20998;&#21644;&#21333;&#39046;&#22495;&#37096;&#20998;&#30340;&#27169;&#22411;&#21644;&#23884;&#20837;&#65292;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;MDR&#26041;&#27861;&#21482;&#20851;&#27880;&#27169;&#22411;&#23618;&#38754;&#30340;&#35299;&#32806;&#12290;&#39046;&#22495;&#23545;&#40784;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#23545;&#25239;&#35757;&#32451;&#26469;&#25552;&#21319;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-domain recommendation (MDR) aims to provide recommendations for different domains (e.g., types of products) with overlapping users/items and is common for platforms such as Amazon, Facebook, and LinkedIn that host multiple services. Existing MDR models face two challenges: First, it is difficult to disentangle knowledge that generalizes across domains (e.g., a user likes cheap items) and knowledge specific to a single domain (e.g., a user likes blue clothing but not blue cars). Second, they have limited ability to transfer knowledge across domains with small overlaps. We propose a new MDR method named EDDA with two key components, i.e., embedding disentangling recommender and domain alignment, to tackle the two challenges respectively. In particular, the embedding disentangling recommender separates both the model and embedding for the inter-domain part and the intra-domain part, while most existing MDR methods only focus on model-level disentangling. The domain alignment leverag
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#22686;&#24378;&#30340;&#30456;&#20851;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#36741;&#21161;&#26597;&#35810;-&#39033;&#30446;&#20132;&#20114;&#65292;&#26469;&#25913;&#36827;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#26597;&#35810;-&#39033;&#30446;&#21305;&#37197;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05379</link><description>&lt;p&gt;
&#36229;&#36234;&#35821;&#20041;&#65306;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#34892;&#20026;&#22686;&#24378;&#30456;&#20851;&#27169;&#22411;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Semantics: Learning a Behavior Augmented Relevance Model with Self-supervised Learning. (arXiv:2308.05379v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05379
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#22686;&#24378;&#30340;&#30456;&#20851;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#36741;&#21161;&#26597;&#35810;-&#39033;&#30446;&#20132;&#20114;&#65292;&#26469;&#25913;&#36827;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#26597;&#35810;-&#39033;&#30446;&#21305;&#37197;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24314;&#27169;&#26088;&#22312;&#23450;&#20301;&#19982;&#23545;&#24212;&#26597;&#35810;&#30456;&#20851;&#30340;&#29702;&#24819;&#39033;&#30446;&#65292;&#36825;&#23545;&#20110;&#25628;&#32034;&#24341;&#25806;&#30830;&#20445;&#29992;&#25143;&#20307;&#39564;&#38750;&#24120;&#37325;&#35201;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#20256;&#32479;&#26041;&#27861;&#36890;&#36807;&#35780;&#20272;&#26597;&#35810;&#19982;&#39033;&#30446;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#32431;&#35821;&#20041;&#21305;&#37197;&#24182;&#19981;&#26159;&#21807;&#19968;&#30340;&#26041;&#27861;&#12290;&#23454;&#38469;&#19978;&#65292;&#20174;&#29992;&#25143;&#25628;&#32034;&#35760;&#24405;&#30340;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#36741;&#21161;&#26597;&#35810;-&#39033;&#30446;&#20132;&#20114;&#21487;&#20197;&#25552;&#20379;&#36827;&#19968;&#27493;&#25581;&#31034;&#29992;&#25143;&#25628;&#32034;&#24847;&#22270;&#30340;&#32447;&#32034;&#12290;&#24471;&#30410;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#34892;&#20026;&#22686;&#24378;&#30456;&#20851;&#23398;&#20064;&#27169;&#22411;&#30340;&#25903;&#20184;&#23453;&#25628;&#32034;&#27169;&#22411;&#65288;BARL-ASe&#65289;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#30446;&#26631;&#39033;&#30446;&#30340;&#30456;&#37051;&#26597;&#35810;&#21644;&#30446;&#26631;&#26597;&#35810;&#30340;&#30456;&#37051;&#39033;&#30446;&#26469;&#34917;&#20805;&#30446;&#26631;&#26597;&#35810;-&#39033;&#30446;&#30340;&#35821;&#20041;&#21305;&#37197;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#24314;&#31435;&#20102;&#22810;&#23618;&#20849;&#21516;&#27880;&#24847;&#21147;&#65292;&#20174;&#30456;&#37051;&#21644;&#30446;&#26631;&#35270;&#22270;&#20013;&#25552;&#21462;&#20102;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#27169;&#22411;&#38543;&#21518;&#37319;&#29992;&#37051;&#23621;-&#30446;&#26631;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26469;&#25552;&#39640;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance modeling aims to locate desirable items for corresponding queries, which is crucial for search engines to ensure user experience. Although most conventional approaches address this problem by assessing the semantic similarity between the query and item, pure semantic matching is not everything. In reality, auxiliary query-item interactions extracted from user historical behavior data of the search log could provide hints to reveal users' search intents further. Drawing inspiration from this, we devise a novel Behavior Augmented Relevance Learning model for Alipay Search (BARL-ASe) that leverages neighbor queries of target item and neighbor items of target query to complement target query-item semantic matching. Specifically, our model builds multi-level co-attention for distilling coarse-grained and fine-grained semantic representations from both neighbor and target views. The model subsequently employs neighbor-target self-supervised learning to improve the accuracy and robu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LayoutLLM-T2I&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;LLM&#20013;&#33719;&#21462;&#24067;&#23616;&#25351;&#23548;&#20197;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#30001;&#31895;&#21040;&#32454;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#32473;&#23450;&#25991;&#26412;&#25552;&#31034;&#30340;&#26465;&#20214;&#19979;&#21512;&#25104;&#19982;&#25991;&#26412;&#35821;&#20041;&#23545;&#40784;&#30340;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.05095</link><description>&lt;p&gt;
LayoutLLM-T2I&#65306;&#20174;LLM&#20013;&#33719;&#21462;&#24067;&#23616;&#25351;&#23548;&#20197;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation. (arXiv:2308.05095v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LayoutLLM-T2I&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;LLM&#20013;&#33719;&#21462;&#24067;&#23616;&#25351;&#23548;&#20197;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#30001;&#31895;&#21040;&#32454;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#32473;&#23450;&#25991;&#26412;&#25552;&#31034;&#30340;&#26465;&#20214;&#19979;&#21512;&#25104;&#19982;&#25991;&#26412;&#35821;&#20041;&#23545;&#40784;&#30340;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#65292;&#36817;&#26399;&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#30340;&#26174;&#33879;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#21508;&#31181;&#26032;&#39062;&#30340;&#36924;&#30495;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#22312;&#22797;&#26434;&#33258;&#28982;&#22330;&#26223;&#20013;&#20173;&#28982;&#38754;&#20020;&#30528;&#38169;&#20301;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#21644;&#25968;&#23383;&#21270;&#22833;&#36133;&#65289;&#65292;&#36825;&#38459;&#30861;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;&#23613;&#31649;&#26368;&#36817;&#24050;&#32463;&#36827;&#34892;&#20102;&#19968;&#20123;&#25913;&#36827;&#65292;&#36890;&#36807;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#25351;&#23548;&#65288;&#20363;&#22914;&#65292;&#33609;&#22270;&#21644;&#28034;&#40486;&#65289;&#26469;&#25913;&#21892;&#21487;&#25511;&#24615;&#65292;&#20294;&#26159;&#30001;&#20110;&#29992;&#25143;&#24517;&#39035;&#25163;&#21160;&#25552;&#20379;&#36825;&#20123;&#25351;&#23548;&#20449;&#24687;&#65292;&#22240;&#27492;&#36825;&#20010;&#38382;&#39064;&#23578;&#26410;&#26681;&#26412;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21162;&#21147;&#21512;&#25104;&#19982;&#32473;&#23450;&#25991;&#26412;&#25552;&#31034;&#35821;&#20041;&#23545;&#40784;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#25351;&#23548;&#30340;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#31895;&#21040;&#32454;&#30340;&#33539;&#20363;&#65292;&#29992;&#20110;&#24067;&#23616;&#35268;&#21010;&#21644;&#22270;&#20687;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#32473;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#26465;&#20214;&#19979;&#29983;&#25104;&#31895;&#31890;&#24230;&#30340;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the text-to-image generation field, recent remarkable progress in Stable Diffusion makes it possible to generate rich kinds of novel photorealistic images. However, current models still face misalignment issues (e.g., problematic spatial relation understanding and numeration failure) in complex natural scenes, which impedes the high-faithfulness text-to-image generation. Although recent efforts have been made to improve controllability by giving fine-grained guidance (e.g., sketch and scribbles), this issue has not been fundamentally tackled since users have to provide such guidance information manually. In this work, we strive to synthesize high-fidelity images that are semantically aligned with a given textual prompt without any guidance. Toward this end, we propose a coarse-to-fine paradigm to achieve layout planning and image generation. Concretely, we first generate the coarse-grained layout conditioned on a given textual prompt via in-context learning based on Large Language M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#35757;&#32451;&#26356;&#21152;&#40065;&#26834;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#25506;&#35752;&#20102;&#20004;&#31181;&#31639;&#27861;&#30340;&#24046;&#24322;&#12290;&#25915;&#20987;&#32773;&#21033;&#29992;&#22240;&#26524;&#25915;&#20987;&#35797;&#22270;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#12290;&#28216;&#25103;&#20013;&#36827;&#34892;&#20102;&#26377;&#24207;&#30340;&#22240;&#26524;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2308.04909</link><description>&lt;p&gt;
&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#23545;&#25239;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adversarial Deep Reinforcement Learning for Cyber Security in Software Defined Networks. (arXiv:2308.04909v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#35757;&#32451;&#26356;&#21152;&#40065;&#26834;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#25506;&#35752;&#20102;&#20004;&#31181;&#31639;&#27861;&#30340;&#24046;&#24322;&#12290;&#25915;&#20987;&#32773;&#21033;&#29992;&#22240;&#26524;&#25915;&#20987;&#35797;&#22270;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#12290;&#28216;&#25103;&#20013;&#36827;&#34892;&#20102;&#26377;&#24207;&#30340;&#22240;&#26524;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#65288;Software Defined Networks&#65292;SDN&#65289;&#20013;&#65292;&#21033;&#29992;&#33258;&#20027;&#25915;&#20987;&#26041;&#27861;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;Deep Reinforcement Learning&#65292;DRL&#65289;&#20013;&#35757;&#32451;&#26356;&#21152;&#40065;&#26834;&#30340;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#65292;&#25506;&#35752;&#20102;&#23558;&#23545;&#25239;&#24615;&#23398;&#20064;&#24212;&#29992;&#20110;DRL&#30340;&#33258;&#20027;&#23433;&#20840;&#24615;&#12290;&#27604;&#36739;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;Double Deep Q-Networks&#65288;DDQN&#65289;&#21644;Neural Episodic Control to Deep Q-Network&#65288;NEC2DQN&#25110;N2D&#65289;&#12290;&#25915;&#20987;&#32773;&#23545;&#29615;&#22659;&#20855;&#26377;&#23436;&#20840;&#30340;&#21487;&#35265;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;&#29366;&#24577;&#25805;&#20316;&#36827;&#34892;&#22240;&#26524;&#25915;&#20987;&#65292;&#35797;&#22270;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#12290;&#25915;&#20987;&#23454;&#26045;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#36827;&#34892;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#35775;&#38382;&#38450;&#24481;&#32773;&#30340;&#27169;&#22411;&#21644;&#32463;&#39564;&#12290;&#36827;&#34892;&#20102;&#20004;&#36718;&#28216;&#25103;&#65306;&#31532;&#19968;&#36718;&#28216;&#25103;&#20013;&#65292;DDQN&#26159;&#38450;&#24481;&#32773;&#65292;N2D&#26159;&#25915;&#20987;&#32773;&#65307;&#31532;&#20108;&#36718;&#28216;&#25103;&#20013;&#65292;&#35282;&#33394;&#20114;&#25442;&#12290;&#20004;&#36718;&#28216;&#25103;&#20998;&#21035;&#36827;&#34892;&#20102;&#20004;&#27425;&#65292;&#31532;&#19968;&#27425;&#27809;&#26377;&#20027;&#21160;&#30340;&#22240;&#26524;&#25915;&#20987;&#65292;&#31532;&#20108;&#27425;&#36827;&#34892;&#20102;&#26377;&#24207;&#30340;&#22240;&#26524;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on the impact of leveraging autonomous offensive approaches in Deep Reinforcement Learning (DRL) to train more robust agents by exploring the impact of applying adversarial learning to DRL for autonomous security in Software Defined Networks (SDN). Two algorithms, Double Deep Q-Networks (DDQN) and Neural Episodic Control to Deep Q-Network (NEC2DQN or N2D), are compared. NEC2DQN was proposed in 2018 and is a new member of the deep q-network (DQN) family of algorithms. The attacker has full observability of the environment and access to a causative attack that uses state manipulation in an attempt to poison the learning process. The implementation of the attack is done under a white-box setting, in which the attacker has access to the defender's model and experiences. Two games are played; in the first game, DDQN is a defender and N2D is an attacker, and in second game, the roles are reversed. The games are played twice; first, without an active causative attack and se
&lt;/p&gt;</description></item><item><title>&#35813;&#25253;&#21578;&#20851;&#27880;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#65292;&#24182;&#20197;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#20026;&#20381;&#25454;&#32534;&#21046;&#20102;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#21015;&#34920;&#12290;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;ChatGPT&#34920;&#29616;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.04889</link><description>&lt;p&gt;
NLLG&#23395;&#24230;arXiv&#25253;&#21578; 06/23&#65306;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#26159;&#20160;&#20040;&#65311;&#65288;arXiv:2308.04889v1 [cs.CY]&#65289;
&lt;/p&gt;
&lt;p&gt;
NLLG Quarterly arXiv Report 06/23: What are the most influential current AI Papers?. (arXiv:2308.04889v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25253;&#21578;&#20851;&#27880;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#65292;&#24182;&#20197;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#20026;&#20381;&#25454;&#32534;&#21046;&#20102;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#21015;&#34920;&#12290;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;ChatGPT&#34920;&#29616;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;Generative Artificial Intelligence&#65292;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;Natural Language Processing&#65292;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;Machine Learning&#65292;ML&#65289;&#65289;&#20449;&#24687;&#30340;&#24555;&#36895;&#22686;&#38271;&#32473;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#20351;&#24471;&#20182;&#20204;&#38590;&#20197;&#36319;&#19978;&#26368;&#26032;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#20449;&#24687;&#36807;&#36733;&#30340;&#38382;&#39064;&#65292;Bielefeld&#22823;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#32452;&#22312;&#26412;&#25253;&#21578;&#20013;&#19987;&#27880;&#20110;&#35782;&#21035;arXiv&#19978;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#65292;&#29305;&#21035;&#20851;&#27880;NLP&#21644;ML&#12290;&#20854;&#30446;&#26631;&#26159;&#20026;&#26368;&#30456;&#20851;&#19988;&#34987;&#24191;&#27867;&#35752;&#35770;&#30340;&#30740;&#31350;&#25552;&#20379;&#24555;&#36895;&#25351;&#21335;&#65292;&#20197;&#24110;&#21161;&#26032;&#26469;&#32773;&#21644;&#24050;&#26377;&#30740;&#31350;&#20154;&#21592;&#36319;&#19978;&#24403;&#21069;&#36235;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;2023&#24180;&#19978;&#21322;&#24180;&#30340;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#32534;&#21046;&#20102;&#19968;&#20010;&#30001;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#32452;&#25104;&#30340;&#21015;&#34920;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#32780;ChatGPT&#26174;&#31034;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of information in the field of Generative Artificial Intelligence (AI), particularly in the subfields of Natural Language Processing (NLP) and Machine Learning (ML), presents a significant challenge for researchers and practitioners to keep pace with the latest developments. To address the problem of information overload, this report by the Natural Language Learning Group at Bielefeld University focuses on identifying the most popular papers on arXiv, with a specific emphasis on NLP and ML. The objective is to offer a quick guide to the most relevant and widely discussed research, aiding both newcomers and established researchers in staying abreast of current trends. In particular, we compile a list of the 40 most popular papers based on normalized citation counts from the first half of 2023. We observe the dominance of papers related to Large Language Models (LLMs) and specifically ChatGPT during the first half of 2023, with the latter showing signs of declining popul
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40479;&#30640;&#22330;&#26223;&#22270;&#65288;BSG&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#40479;&#30640;&#34920;&#31034;&#26469;&#32534;&#30721;&#22330;&#26223;&#24067;&#23616;&#21644;&#20960;&#20309;&#32447;&#32034;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#24403;&#21069;&#20840;&#26223;&#35266;&#23519;&#30340;&#23548;&#33322;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#21160;&#20316;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.04758</link><description>&lt;p&gt;
&#40479;&#30640;&#22330;&#26223;&#22270;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Bird's-Eye-View Scene Graph for Vision-Language Navigation. (arXiv:2308.04758v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04758
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40479;&#30640;&#22330;&#26223;&#22270;&#65288;BSG&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#40479;&#30640;&#34920;&#31034;&#26469;&#32534;&#30721;&#22330;&#26223;&#24067;&#23616;&#21644;&#20960;&#20309;&#32447;&#32034;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#24403;&#21069;&#20840;&#26223;&#35266;&#23519;&#30340;&#23548;&#33322;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#21160;&#20316;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#38656;&#35201;&#19968;&#20010;&#20195;&#29702;&#26681;&#25454;&#20154;&#31867;&#25351;&#31034;&#22312;3D&#29615;&#22659;&#20013;&#23548;&#33322;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20195;&#29702;&#22522;&#20110;&#20840;&#26223;&#35266;&#23519;&#26500;&#24314;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#24863;&#30693;3D&#22330;&#26223;&#20960;&#20309;&#21644;&#23481;&#26131;&#23548;&#33268;&#20840;&#26223;&#35270;&#22270;&#30340;&#27169;&#31946;&#36873;&#25321;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40479;&#30640;&#22330;&#26223;&#22270;&#65288;BSG&#65289;&#65292;&#23427;&#21033;&#29992;&#22810;&#27493;&#40479;&#30640;&#34920;&#31034;&#26469;&#32534;&#30721;&#23460;&#20869;&#29615;&#22659;&#30340;&#22330;&#26223;&#24067;&#23616;&#21644;&#20960;&#20309;&#32447;&#32034;&#65292;&#22312;3D&#26816;&#27979;&#30340;&#30417;&#30563;&#19979;&#12290;&#22312;&#23548;&#33322;&#36807;&#31243;&#20013;&#65292;BSG&#22312;&#27599;&#20010;&#27493;&#39588;&#26500;&#24314;&#19968;&#20010;&#26412;&#22320;&#40479;&#30640;&#34920;&#31034;&#65292;&#24182;&#32500;&#25252;&#19968;&#20010;&#22522;&#20110;&#40479;&#30640;&#30340;&#20840;&#23616;&#22330;&#26223;&#22320;&#22270;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#25299;&#25169;&#20851;&#31995;&#23384;&#20648;&#21644;&#32452;&#32455;&#25152;&#26377;&#22312;&#32447;&#25910;&#38598;&#30340;&#26412;&#22320;&#40479;&#30640;&#34920;&#31034;&#12290;&#22522;&#20110;BSG&#65292;&#20195;&#29702;&#39044;&#27979;&#26412;&#22320;&#40479;&#30640;&#32593;&#26684;&#32423;&#20915;&#31574;&#24471;&#20998;&#21644;&#20840;&#23616;&#22270;&#24418;&#32423;&#20915;&#31574;&#24471;&#20998;&#65292;&#32467;&#21512;&#20840;&#26223;&#35270;&#22270;&#30340;&#23376;&#35270;&#22270;&#36873;&#25321;&#24471;&#20998;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#21160;&#20316;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vision-language navigation (VLN), which entails an agent to navigate 3D environments following human instructions, has shown great advances. However, current agents are built upon panoramic observations, which hinders their ability to perceive 3D scene geometry and easily leads to ambiguous selection of panoramic view. To address these limitations, we present a BEV Scene Graph (BSG), which leverages multi-step BEV representations to encode scene layouts and geometric cues of indoor environment under the supervision of 3D detection. During navigation, BSG builds a local BEV representation at each step and maintains a BEV-based global scene map, which stores and organizes all the online collected local BEV representations according to their topological relations. Based on BSG, the agent predicts a local BEV grid-level decision score and a global graph-level decision score, combined with a sub-view selection score on panoramic views, for more accurate action prediction. Our approach signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#36890;&#29992;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#21033;&#28070;&#20998;&#20139;&#38382;&#39064;&#65292;&#20026;&#19968;&#33324;&#31867;&#30340;&#25104;&#26412;&#21644;&#25910;&#20837;&#20989;&#25968;&#25551;&#36848;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2308.04399</link><description>&lt;p&gt;
Fine-Tuning Games: Bargaining and Adaptation for General-Purpose Models
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Games: Bargaining and Adaptation for General-Purpose Models. (arXiv:2308.04399v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#36890;&#29992;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#21033;&#28070;&#20998;&#20139;&#38382;&#39064;&#65292;&#20026;&#19968;&#33324;&#31867;&#30340;&#25104;&#26412;&#21644;&#25910;&#20837;&#20989;&#25968;&#25551;&#36848;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#23637;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#24320;&#21457;&#21644;&#21457;&#24067;&#36890;&#29992;&#27169;&#22411;&#30340;&#24418;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26088;&#22312;&#30001;&#20854;&#20182;&#20225;&#19994;&#21644;&#26426;&#26500;&#36827;&#34892;&#36866;&#24212;&#65292;&#20197;&#25191;&#34892;&#29305;&#23450;&#30340;&#39046;&#22495;&#19987;&#29992;&#21151;&#33021;&#12290;&#36825;&#20010;&#36807;&#31243;&#34987;&#31216;&#20026;&#36866;&#24212;&#25110;&#24494;&#35843;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#24494;&#35843;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20301;&#36890;&#29992;&#19987;&#23478;&#23558;&#25216;&#26415;&#20135;&#21697;&#65288;&#21363;ML&#27169;&#22411;&#65289;&#25552;&#21319;&#21040;&#19968;&#23450;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#19968;&#20301;&#25110;&#22810;&#20301;&#39046;&#22495;&#19987;&#23478;&#23558;&#20854;&#35843;&#25972;&#36866;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#12290;&#36825;&#20004;&#20010;&#23454;&#20307;&#37117;&#26159;&#36861;&#27714;&#21033;&#28070;&#30340;&#65292;&#24403;&#20182;&#20204;&#25237;&#36164;&#20110;&#25216;&#26415;&#26102;&#20250;&#20135;&#29983;&#25104;&#26412;&#65292;&#22312;&#25216;&#26415;&#36827;&#20837;&#24066;&#22330;&#21069;&#65292;&#20182;&#20204;&#24517;&#39035;&#23601;&#22914;&#20309;&#20998;&#20139;&#25910;&#20837;&#36798;&#25104;&#35848;&#21028;&#21327;&#35758;&#12290;&#23545;&#20110;&#30456;&#23545;&#19968;&#33324;&#30340;&#25104;&#26412;&#21644;&#25910;&#20837;&#20989;&#25968;&#31867;&#65292;&#25105;&#20204;&#21051;&#30011;&#20102;&#24494;&#35843;&#21338;&#24328;&#20135;&#29983;&#21033;&#28070;&#20998;&#20139;&#35299;&#20915;&#26041;&#26696;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20219;&#20309;&#28508;&#22312;&#30340;&#39046;&#22495;&#19987;&#19994;&#21270;&#37117;&#20250;&#20135;&#29983;...
&lt;/p&gt;
&lt;p&gt;
Major advances in Machine Learning (ML) and Artificial Intelligence (AI) increasingly take the form of developing and releasing general-purpose models. These models are designed to be adapted by other businesses and agencies to perform a particular, domain-specific function. This process has become known as adaptation or fine-tuning. This paper offers a model of the fine-tuning process where a Generalist brings the technological product (here an ML model) to a certain level of performance, and one or more Domain-specialist(s) adapts it for use in a particular domain. Both entities are profit-seeking and incur costs when they invest in the technology, and they must reach a bargaining agreement on how to share the revenue for the technology to reach the market. For a relatively general class of cost and revenue functions, we characterize the conditions under which the fine-tuning game yields a profit-sharing solution. We observe that any potential domain-specialization will either contri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;BDCM&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03669</link><description>&lt;p&gt;
&#26080;&#27861;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#19979;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model in Causal Inference with Unmeasured Confounders. (arXiv:2308.03669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;BDCM&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;&#22312;Pearl&#30340;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#25429;&#25417;&#22240;&#26524;&#24178;&#39044;&#30340;&#26694;&#26550;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22240;&#26524;&#27169;&#22411;&#65288;DCM&#65289;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#65292;&#20551;&#35774;&#25152;&#26377;&#28151;&#28102;&#22240;&#32032;&#37117;&#26159;&#21487;&#20197;&#35266;&#23519;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20013;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#36825;&#20351;&#24471;DCM&#26080;&#27861;&#24212;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;DCM&#30340;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#27169;&#22411;&#65292;&#31216;&#20026;&#22522;&#20110;&#21453;&#38376;&#20934;&#21017;&#30340;DCM&#65288;BDCM&#65289;&#65292;&#20854;&#24605;&#24819;&#26681;&#26893;&#20110;&#22312;DAG&#20013;&#25214;&#21040;&#35201;&#21253;&#25324;&#22312;&#25193;&#25955;&#27169;&#22411;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#21464;&#37327;&#30340;&#21453;&#38376;&#20934;&#21017;&#65292;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#23558;DCM&#25193;&#23637;&#21040;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#12290;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26080;&#27861;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#31934;&#30830;&#22320;&#25429;&#25417;&#21040;&#20102;&#21453;&#20107;&#23454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to extend the use of the diffusion model to answer the causal question from the observational data under the existence of unmeasured confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to capture the causal intervention, a Diffusion-based Causal Model (DCM) was proposed incorporating the diffusion model to answer the causal questions more accurately, assuming that all of the confounders are observed. However, unmeasured confounders in practice exist, which hinders DCM from being applicable. To alleviate this limitation of DCM, we propose an extended model called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the Backdoor criterion to find the variables in DAG to be included in the decoding process of the diffusion model so that we can extend DCM to the case with unmeasured confounders. Synthetic data experiment demonstrates that our proposed model captures the counterfactual distribution more precisely than DCM under the unmeasured confo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#22238;&#25253;&#24046;&#36317;&#19978;&#30028;&#65292;&#23558;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#38382;&#39064;&#36716;&#21270;&#20026;&#31163;&#25955;&#28040;&#24687;&#30340;&#22312;&#32447;&#32858;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#37327;&#21270;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#36890;&#20449;&#24320;&#38144;&#20302;&#12289;&#21487;&#35299;&#37322;&#24615;&#22909;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.03358</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;POMDP&#20013;&#22522;&#20110;&#22312;&#32447;&#32858;&#31867;&#26631;&#31614;&#30340;&#31163;&#25955;&#28040;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Discrete Message via Online Clustering Labels in Decentralized POMDP. (arXiv:2308.03358v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#22238;&#25253;&#24046;&#36317;&#19978;&#30028;&#65292;&#23558;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#38382;&#39064;&#36716;&#21270;&#20026;&#31163;&#25955;&#28040;&#24687;&#30340;&#22312;&#32447;&#32858;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#37327;&#21270;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#36890;&#20449;&#24320;&#38144;&#20302;&#12289;&#21487;&#35299;&#37322;&#24615;&#22909;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#36890;&#20449;&#23545;&#20110;&#35299;&#20915;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#40657;&#30418;&#26041;&#27861;&#65292;&#23558;&#26412;&#22320;&#20449;&#24687;/&#29305;&#24449;&#32534;&#30721;&#25104;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#20849;&#20139;&#30340;&#28040;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#40657;&#30418;&#26041;&#27861;&#26080;&#27861;&#23545;&#26399;&#26395;&#22238;&#25253;&#25552;&#20379;&#20219;&#20309;&#37327;&#21270;&#20445;&#35777;&#65292;&#24120;&#24120;&#23548;&#33268;&#29983;&#25104;&#36890;&#20449;&#24320;&#38144;&#39640;&#12289;&#21487;&#35299;&#37322;&#24615;&#24046;&#30340;&#36830;&#32493;&#28040;&#24687;&#12290;&#26412;&#25991;&#22312;&#29702;&#24819;&#31574;&#30053;&#19982;&#26368;&#20248;&#37096;&#20998;&#21487;&#35266;&#23519;&#31574;&#30053;&#20043;&#38388;&#24314;&#31435;&#20102;&#22238;&#25253;&#24046;&#36317;&#30340;&#19978;&#30028;&#12290;&#35813;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#37325;&#26032;&#23450;&#20041;&#20026;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#26412;&#22320;&#35266;&#23519;&#20013;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#32858;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#28040;&#24687;&#20316;&#20026;&#32858;&#31867;&#26631;&#31614;&#65292;&#24182;&#19988;&#22238;&#25253;&#24046;&#36317;&#30340;&#19978;&#30028;&#20316;&#20026;&#32858;&#31867;&#25439;&#22833;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#19978;&#30028;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#22320;&#31616;&#21333;&#30340;&#28040;&#24687;&#29983;&#25104;&#20989;&#25968;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication is crucial for solving cooperative Multi-Agent Reinforcement Learning tasks in Partially-Observable Markov Decision Processes. Existing works often rely on black-box methods to encode local information/features into messages shared with other agents. However, such black-box approaches are unable to provide any quantitative guarantees on the expected return and often lead to the generation of continuous messages with high communication overhead and poor interpretability. In this paper, we establish an upper bound on the return gap between an ideal policy with full observability and an optimal partially-observable policy with discrete communication. This result enables us to recast multi-agent communication into a novel online clustering problem over the local observations at each agent, with messages as cluster labels and the upper bound on the return gap as clustering loss. By minimizing the upper bound, we propose a surprisingly simple design of message generation functi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#39564;&#35777;&#20043;&#21069;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#36890;&#36807;&#28040;&#38500;&#31283;&#23450;&#30340;ReLU&#31070;&#32463;&#20803;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#30001;ReLU&#21644;&#20223;&#23556;&#23618;&#32452;&#25104;&#30340;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23567;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#24182;&#21152;&#36895;&#20102;&#29616;&#26377;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2308.03330</link><description>&lt;p&gt;
&#36890;&#36807;&#32593;&#32476;&#31616;&#21270;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Expediting Neural Network Verification via Network Reduction. (arXiv:2308.03330v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#39564;&#35777;&#20043;&#21069;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#36890;&#36807;&#28040;&#38500;&#31283;&#23450;&#30340;ReLU&#31070;&#32463;&#20803;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#30001;ReLU&#21644;&#20223;&#23556;&#23618;&#32452;&#25104;&#30340;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23567;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#24182;&#21152;&#36895;&#20102;&#29616;&#26377;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#39564;&#35777;&#26041;&#27861;&#26469;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#65292;&#20197;&#30830;&#20445;&#32593;&#32476;&#22312;&#20851;&#38190;&#24212;&#29992;&#20013;&#27491;&#30830;&#36816;&#34892;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20247;&#25152;&#21608;&#30693;&#30340;&#39564;&#35777;&#24037;&#20855;&#22312;&#22797;&#26434;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#22823;&#22411;&#32593;&#32476;&#22823;&#23567;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#31616;&#21270;&#25216;&#26415;&#20316;&#20026;&#39564;&#35777;&#20043;&#21069;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#28040;&#38500;&#31283;&#23450;&#30340;ReLU&#31070;&#32463;&#20803;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#30001;ReLU&#21644;&#20223;&#23556;&#23618;&#32452;&#25104;&#30340;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#21487;&#20197;&#36890;&#36807;&#22823;&#22810;&#25968;&#39564;&#35777;&#24037;&#20855;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#26368;&#20808;&#36827;&#30340;&#23436;&#25972;&#21644;&#19981;&#23436;&#25972;&#39564;&#35777;&#24037;&#20855;&#19978;&#23454;&#29616;&#20102;&#31616;&#21270;&#25216;&#26415;&#65292;&#21253;&#25324;alpha-beta-crown&#65292;VeriNet&#21644;PRIMA&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#31070;&#32463;&#32593;&#32476;&#24182;&#21152;&#36895;&#29616;&#26377;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#36824;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
A wide range of verification methods have been proposed to verify the safety properties of deep neural networks ensuring that the networks function correctly in critical applications. However, many well-known verification tools still struggle with complicated network architectures and large network sizes. In this work, we propose a network reduction technique as a pre-processing method prior to verification. The proposed method reduces neural networks via eliminating stable ReLU neurons, and transforming them into a sequential neural network consisting of ReLU and Affine layers which can be handled by the most verification tools. We instantiate the reduction technique on the state-of-the-art complete and incomplete verification tools, including alpha-beta-crown, VeriNet and PRIMA. Our experiments on a large set of benchmarks indicate that the proposed technique can significantly reduce neural networks and speed up existing verification tools. Furthermore, the experiment results also sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26032;&#26694;&#26550;&#65292;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2308.03202</link><description>&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Source-free Domain Adaptive Human Pose Estimation. (arXiv:2308.03202v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26032;&#26694;&#26550;&#65292;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#24191;&#27867;&#24212;&#29992;&#20110;&#36816;&#21160;&#20998;&#26512;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#34394;&#25311;&#29616;&#23454;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#23454;&#38469;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#30340;&#24040;&#22823;&#24320;&#38144;&#23545;&#23039;&#21183;&#20272;&#35745;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23039;&#21183;&#20272;&#35745;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;(DA)&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;HPE&#30340;DA&#26041;&#27861;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#24573;&#30053;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#20351;&#29992;&#20102;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21517;&#20026;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;HPE&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#26080;&#27861;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;HPE&#30340;&#36328;&#22495;&#23398;&#20064;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#27169;&#22411;&#32452;&#25104;&#30340;&#26032;&#26694;&#26550;&#65306;&#28304;&#27169;&#22411;&#12289;&#20013;&#38388;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#65292;&#20174;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#35282;&#24230;&#25506;&#32034;&#35813;&#20219;&#21153;&#12290;&#28304;&#20445;&#25252;&#27169;&#22359;&#26356;&#26377;&#25928;&#22320;&#20445;&#30041;&#28304;&#20449;&#24687;&#24182;&#25269;&#25239;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02293</link><description>&lt;p&gt;
&#29992;&#27491;&#21017;&#21270;&#39640;&#38454;&#24635;&#21464;&#24046;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A stochastic optimization approach to train non-linear neural networks with regularization of higher-order total variation. (arXiv:2308.02293v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02293
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21253;&#25324;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#39640;&#24230;&#34920;&#36798;&#30340;&#21442;&#25968;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#22797;&#26434;&#27010;&#24565;&#65292;&#20294;&#35757;&#32451;&#36825;&#31181;&#39640;&#24230;&#38750;&#32447;&#24615;&#27169;&#22411;&#24050;&#30693;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;k&#38454;&#24635;&#21464;&#24046;&#65288;k-TV&#65289;&#27491;&#21017;&#21270;&#65292;&#23427;&#34987;&#23450;&#20041;&#20026;&#35201;&#35757;&#32451;&#30340;&#21442;&#25968;&#27169;&#22411;&#30340;k&#38454;&#23548;&#25968;&#30340;&#24179;&#26041;&#31215;&#20998;&#65292;&#36890;&#36807;&#24809;&#32602;k-TV&#26469;&#20135;&#29983;&#19968;&#20010;&#26356;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#23613;&#31649;&#23558;k-TV&#39033;&#24212;&#29992;&#20110;&#19968;&#33324;&#30340;&#21442;&#25968;&#27169;&#22411;&#30001;&#20110;&#31215;&#20998;&#32780;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#24102;&#26377;k-TV&#27491;&#21017;&#21270;&#30340;&#19968;&#33324;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26174;&#24335;&#30340;&#25968;&#20540;&#31215;&#20998;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#32467;&#26500;&#20219;&#24847;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#36827;&#34892;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#21363;&#21487;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While highly expressive parametric models including deep neural networks have an advantage to model complicated concepts, training such highly non-linear models is known to yield a high risk of notorious overfitting. To address this issue, this study considers a $k$th order total variation ($k$-TV) regularization, which is defined as the squared integral of the $k$th order derivative of the parametric models to be trained; penalizing the $k$-TV is expected to yield a smoother function, which is expected to avoid overfitting. While the $k$-TV terms applied to general parametric models are computationally intractable due to the integration, this study provides a stochastic optimization algorithm, that can efficiently train general models with the $k$-TV regularization without conducting explicit numerical integration. The proposed approach can be applied to the training of even deep neural networks whose structure is arbitrary, as it can be implemented by only a simple stochastic gradien
&lt;/p&gt;</description></item><item><title>ClassEval&#26159;&#19968;&#31181;&#25163;&#24037;&#26500;&#24314;&#30340;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#22312;&#36825;&#19968;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#35780;&#20272;LLMs&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#12290;GPT-4&#21644;GPT-3.5&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#23545;&#20854;&#20182;LLMs&#26356;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.01861</link><description>&lt;p&gt;
ClassEval: &#19968;&#31181;&#25163;&#24037;&#26500;&#24314;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation. (arXiv:2308.01861v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01861
&lt;/p&gt;
&lt;p&gt;
ClassEval&#26159;&#19968;&#31181;&#25163;&#24037;&#26500;&#24314;&#30340;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#22312;&#36825;&#19968;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#35780;&#20272;LLMs&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#12290;GPT-4&#21644;GPT-3.5&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#23545;&#20854;&#20182;LLMs&#26356;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20195;&#30721;&#29983;&#25104;&#22330;&#26223;&#20013;&#35780;&#20272;LLMs&#65292;&#21363;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#25163;&#21160;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;ClassEval&#65292;&#20854;&#20013;&#21253;&#21547;100&#20010;&#31867;&#32423;Python&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#24635;&#20849;&#32791;&#26102;&#32422;500&#20154;&#23567;&#26102;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23545;11&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#30740;&#31350;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20197;&#19979;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#29616;&#26377;&#30340;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#24615;&#33021;&#35201;&#36828;&#20302;&#20110;&#29420;&#31435;&#30340;&#26041;&#27861;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65288;&#22914;HumanEval&#65289;&#65307;&#32780;&#26041;&#27861;&#32423;&#30340;&#32534;&#30721;&#33021;&#21147;&#19981;&#33021;&#31561;&#21516;&#22320;&#21453;&#26144;LLMs&#22312;&#31867;&#32423;&#32534;&#30721;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-4&#21644;GPT-3.5&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#20173;&#28982;&#34920;&#29616;&#20986;&#30456;&#23545;&#20854;&#20182;LLMs&#26356;&#21331;&#36234;&#30340;&#20248;&#21183;&#65292;&#32780;&#20108;&#32447;&#27169;&#22411;&#21253;&#25324;Instruct-Starcoder&#65292;Instruct-Codegen&#21644;Wizardcoder&#22312;&#24615;&#33021;&#19978;&#38750;&#24120;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e. class-level code generation. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on it, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we have the following main findings. First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior than other LLMs on class-level code generation, and the second-tier models includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very similar performance. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LCPS&#65292;&#31532;&#19968;&#20010;LiDAR-&#30456;&#26426;&#20840;&#26223;&#20998;&#21106;&#32593;&#32476;&#65292;&#36890;&#36807;&#24322;&#27493;&#34917;&#20607;&#20687;&#32032;&#23545;&#40784;&#12289;&#35821;&#20041;&#24863;&#30693;&#21306;&#22495;&#23545;&#40784;&#21644;&#28857;&#21040;&#20307;&#32032;&#29305;&#24449;&#20256;&#25773;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20840;&#26223;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01686</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#19968;&#33268;&#24615;&#21644;&#35821;&#20041;&#24863;&#30693;&#23545;&#40784;&#30340;LiDAR-&#30456;&#26426;&#20840;&#26223;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment. (arXiv:2308.01686v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LCPS&#65292;&#31532;&#19968;&#20010;LiDAR-&#30456;&#26426;&#20840;&#26223;&#20998;&#21106;&#32593;&#32476;&#65292;&#36890;&#36807;&#24322;&#27493;&#34917;&#20607;&#20687;&#32032;&#23545;&#40784;&#12289;&#35821;&#20041;&#24863;&#30693;&#21306;&#22495;&#23545;&#40784;&#21644;&#28857;&#21040;&#20307;&#32032;&#29305;&#24449;&#20256;&#25773;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20840;&#26223;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#20840;&#26223;&#20998;&#21106;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24863;&#30693;&#20219;&#21153;&#65292;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22270;&#20687;&#21487;&#20197;&#25552;&#20379;&#20016;&#23500;&#30340;&#32441;&#29702;&#12289;&#39068;&#33394;&#21644;&#36776;&#21035;&#20449;&#24687;&#65292;&#21487;&#20197;&#20026;LiDAR&#25968;&#25454;&#25552;&#20379;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#30340;&#34701;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LCPS&#65292;&#31532;&#19968;&#20010;LiDAR-&#30456;&#26426;&#20840;&#26223;&#20998;&#21106;&#32593;&#32476;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;LiDAR-&#30456;&#26426;&#34701;&#21512;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;1&#65289;&#19968;&#20010;&#24322;&#27493;&#34917;&#20607;&#20687;&#32032;&#23545;&#40784;&#65288;ACPA&#65289;&#27169;&#22359;&#65292;&#26657;&#27491;&#20256;&#24863;&#22120;&#20043;&#38388;&#24322;&#27493;&#38382;&#39064;&#23548;&#33268;&#30340;&#22352;&#26631;&#38169;&#20301;&#65307;2&#65289;&#19968;&#20010;&#35821;&#20041;&#24863;&#30693;&#21306;&#22495;&#23545;&#40784;&#65288;SARA&#65289;&#27169;&#22359;&#65292;&#23558;&#19968;&#23545;&#19968;&#30340;&#28857;-&#20687;&#32032;&#26144;&#23556;&#25193;&#23637;&#21040;&#19968;&#23545;&#22810;&#30340;&#35821;&#20041;&#20851;&#31995;&#65307;3&#65289;&#19968;&#20010;&#28857;&#21040;&#20307;&#32032;&#29305;&#24449;&#20256;&#25773;&#65288;PVP&#65289;&#27169;&#22359;&#65292;&#25972;&#21512;&#20960;&#20309;&#21644;&#35821;&#20041;&#34701;&#21512;&#20449;&#24687;&#23545;&#25972;&#20010;&#28857;&#20113;&#36827;&#34892;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#34701;&#21512;&#31574;&#30053;&#30456;&#23545;&#20110;&#20165;&#20351;&#29992;LiDAR&#30340;&#26041;&#27861;&#65292;&#24615;&#33021;&#25552;&#21319;&#20102;&#32422;6.9%&#30340;PQ&#12290;
&lt;/p&gt;
&lt;p&gt;
3D panoptic segmentation is a challenging perception task that requires both semantic segmentation and instance segmentation. In this task, we notice that images could provide rich texture, color, and discriminative information, which can complement LiDAR data for evident performance improvement, but their fusion remains a challenging problem. To this end, we propose LCPS, the first LiDAR-Camera Panoptic Segmentation network. In our approach, we conduct LiDAR-Camera fusion in three stages: 1) an Asynchronous Compensation Pixel Alignment (ACPA) module that calibrates the coordinate misalignment caused by asynchronous problems between sensors; 2) a Semantic-Aware Region Alignment (SARA) module that extends the one-to-one point-pixel mapping to one-to-many semantic relations; 3) a Point-to-Voxel feature Propagation (PVP) module that integrates both geometric and semantic fusion information for the entire point cloud. Our fusion strategy improves about 6.9% PQ performance over the LiDAR-on
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#27169;&#24335;&#36716;&#31227;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#22122;&#22768;&#27169;&#24335;&#20174;&#19981;&#21516;&#29615;&#22659;&#30340;&#26631;&#20934;&#26679;&#26412;&#24212;&#29992;&#21040;&#26410;&#30693;&#26679;&#26412;&#65292;&#36890;&#36807;&#29983;&#25104;&#26696;&#20363;&#24211;&#26469;&#35299;&#20915;&#26679;&#26412;&#32423;&#22122;&#22768;&#23545;&#25968;&#25454;&#38598;&#32423;&#22122;&#22768;&#23398;&#20064;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01138</link><description>&lt;p&gt;
&#33021;&#21542;&#36716;&#31227;&#22122;&#22768;&#27169;&#24335;&#65311;&#20351;&#29992;&#29983;&#25104;&#26696;&#20363;&#30340;&#22810;&#29615;&#22659;&#39057;&#35889;&#20998;&#26512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Can We Transfer Noise Patterns? An Multi-environment Spectrum Analysis Model Using Generated Cases. (arXiv:2308.01138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01138
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#27169;&#24335;&#36716;&#31227;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#22122;&#22768;&#27169;&#24335;&#20174;&#19981;&#21516;&#29615;&#22659;&#30340;&#26631;&#20934;&#26679;&#26412;&#24212;&#29992;&#21040;&#26410;&#30693;&#26679;&#26412;&#65292;&#36890;&#36807;&#29983;&#25104;&#26696;&#20363;&#24211;&#26469;&#35299;&#20915;&#26679;&#26412;&#32423;&#22122;&#22768;&#23545;&#25968;&#25454;&#38598;&#32423;&#22122;&#22768;&#23398;&#20064;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#27700;&#36136;&#26816;&#27979;&#20013;&#65292;&#39057;&#35889;&#20998;&#26512;&#31995;&#32479;&#26088;&#22312;&#26816;&#27979;&#27745;&#26579;&#29289;&#30340;&#31867;&#22411;&#21644;&#27987;&#24230;&#65292;&#24182;&#20351;&#30417;&#31649;&#26426;&#26500;&#33021;&#22815;&#21450;&#26102;&#22238;&#24212;&#27745;&#26579;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#39057;&#35889;&#25968;&#25454;&#30340;&#27979;&#35797;&#35774;&#22791;&#22312;&#38750;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#37096;&#32626;&#26102;&#20250;&#21463;&#21040;&#22797;&#26434;&#30340;&#22122;&#22768;&#27169;&#24335;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20351;&#20998;&#26512;&#27169;&#22411;&#36866;&#29992;&#20110;&#26356;&#22810;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22122;&#22768;&#27169;&#24335;&#36716;&#31227;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#19981;&#21516;&#29615;&#22659;&#20013;&#26631;&#20934;&#27700;&#26679;&#21697;&#30340;&#39057;&#35889;&#20316;&#20026;&#26696;&#20363;&#65292;&#24182;&#23398;&#20064;&#23427;&#20204;&#22122;&#22768;&#27169;&#24335;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#20351;&#22122;&#22768;&#27169;&#24335;&#33021;&#22815;&#24212;&#29992;&#20110;&#26410;&#30693;&#26679;&#21697;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24517;&#28982;&#23384;&#22312;&#30340;&#26679;&#26412;&#32423;&#22522;&#32447;&#22122;&#22768;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#33719;&#21462;&#21482;&#22312;&#25968;&#25454;&#38598;&#32423;&#29615;&#22659;&#22122;&#22768;&#19978;&#26377;&#24046;&#24322;&#30340;&#37197;&#23545;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26679;&#26412;&#23545;&#26679;&#26412;&#30340;&#26696;&#20363;&#24211;&#65292;&#25490;&#38500;&#20102;&#26679;&#26412;&#32423;&#22122;&#22768;&#23545;&#25968;&#25454;&#38598;&#32423;&#22122;&#22768;&#23398;&#20064;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectrum analysis systems in online water quality testing are designed to detect types and concentrations of pollutants and enable regulatory agencies to respond promptly to pollution incidents. However, spectral data-based testing devices suffer from complex noise patterns when deployed in non-laboratory environments. To make the analysis model applicable to more environments, we propose a noise patterns transferring model, which takes the spectrum of standard water samples in different environments as cases and learns the differences in their noise patterns, thus enabling noise patterns to transfer to unknown samples. Unfortunately, the inevitable sample-level baseline noise makes the model unable to obtain the paired data that only differ in dataset-level environmental noise. To address the problem, we generate a sample-to-sample case-base to exclude the interference of sample-level noise on dataset-level noise learning, enhancing the system's learning performance. Experiments on sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.01050</link><description>&lt;p&gt;
&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#39118;&#38505;&#35780;&#20272;&#30340;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness. (arXiv:2308.01050v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#26377;&#28508;&#21147;&#25552;&#20379;&#35832;&#22810;&#31038;&#20250;&#25928;&#30410;&#65292;&#22914;&#20943;&#23569;&#36947;&#36335;&#20107;&#25925;&#21644;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21382;&#21490;&#25968;&#25454;&#21644;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#37327;&#21270;AVs&#30340;&#39118;&#38505;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;AVs&#22312;&#21508;&#31181;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#65288;ODDs&#65289;&#20013;&#34892;&#20026;&#30340;&#39118;&#38505;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#23545;&#8220;&#19981;&#33391;&#8221;&#36947;&#36335;&#29992;&#25143;&#36827;&#34892;&#21453;&#20107;&#23454;&#27169;&#25311;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#21487;&#33021;&#23548;&#33268;&#30896;&#25758;&#30340;&#26368;&#23567;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#37327;&#12290;&#35813;&#27010;&#24565;&#26377;&#21161;&#20110;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#35780;&#20272;AVs&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;AV&#30340;&#34892;&#20026;&#31574;&#30053;&#26159;&#26410;&#30693;&#30340;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20173;&#28982;&#36866;&#29992;&#20110;&#26368;&#22351;&#21644;&#26368;&#20339;&#24773;&#20917;&#20998;&#26512;&#65292;&#20351;&#35813;&#26041;&#27861;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#20063;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicles (AVs) have the potential to provide numerous societal benefits, such as decreased road accidents and increased overall transportation efficiency. However, quantifying the risk associated with AVs is challenging due to the lack of historical data and the rapidly evolving technology. This paper presents a data-driven framework for comparing the risk of different AVs' behaviors in various operational design domains (ODDs), based on counterfactual simulations of "misbehaving" road users. We introduce the concept of counterfactual safety margin, which represents the minimum deviation from normal behavior that could lead to a collision. This concept helps to find the most critical scenarios but also to assess the frequency and severity of risk of AVs. We show that the proposed methodology is applicable even when the AV's behavioral policy is unknown -- through worst- and best-case analyses -- making the method useful also to external third-party risk assessors. Our experi
&lt;/p&gt;</description></item><item><title>FusionAD&#26159;&#31532;&#19968;&#20010;&#23558;&#26469;&#33258;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#20449;&#24687;&#34701;&#21512;&#36215;&#26469;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01006</link><description>&lt;p&gt;
FusionAD: &#33258;&#21160;&#39550;&#39542;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving. (arXiv:2308.01006v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01006
&lt;/p&gt;
&lt;p&gt;
FusionAD&#26159;&#31532;&#19968;&#20010;&#23558;&#26469;&#33258;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#20449;&#24687;&#34701;&#21512;&#36215;&#26469;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20219;&#21153;&#20013;&#65292;&#26500;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#24615;&#33021;&#24050;&#25104;&#20026;&#38081;&#26495;&#19968;&#22359;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#26469;&#32852;&#21512;&#20248;&#21270;&#39044;&#27979;&#21644;&#35268;&#21010;&#20219;&#21153;&#20173;&#28982;&#20960;&#20046;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FusionAD&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#26469;&#33258;&#20004;&#20010;&#26368;&#20851;&#38190;&#20256;&#24863;&#22120;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#20449;&#24687;&#34701;&#21512;&#36215;&#26469;&#36229;&#36234;&#24863;&#30693;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;&#22522;&#20110;&#34701;&#21512;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#19982;&#22522;&#20110;&#30456;&#26426;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;UniAD&#30456;&#27604;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#34701;&#21512;&#36741;&#21161;&#30340;&#27169;&#24577;&#24863;&#30693;&#39044;&#27979;&#21644;&#29366;&#24577;&#24863;&#30693;&#35268;&#21010;&#27169;&#22359;&#65292;&#31216;&#20026;FMSPnP&#65292;&#20805;&#20998;&#21033;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#24120;&#29992;&#30340;nuScenes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;FusionAD&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#20110;&#22522;&#20934;&#32447;&#24179;&#22343;15%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a multi-modality multi-task neural network toward accurate and robust performance is a de-facto standard in perception task of autonomous driving. However, leveraging such data from multiple sensors to jointly optimize the prediction and planning tasks remains largely unexplored. In this paper, we present FusionAD, to the best of our knowledge, the first unified framework that fuse the information from two most critical sensors, camera and LiDAR, goes beyond perception task. Concretely, we first build a transformer based multi-modality fusion network to effectively produce fusion based features. In constrast to camera-based end-to-end method UniAD, we then establish a fusion aided modality-aware prediction and status-aware planning modules, dubbed FMSPnP that take advantages of multi-modality features. We conduct extensive experiments on commonly used benchmark nuScenes dataset, our FusionAD achieves state-of-the-art performance and surpassing baselines on average 15% on perce
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;API&#26041;&#38754;&#20998;&#26512;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#24182;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#24615;&#33021;&#65292;&#23545;Performance&#12289;Security&#12289;Usability&#21644;Documentation&#31561;&#26041;&#38754;&#30340;&#26816;&#27979;&#25928;&#26524;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#21644;&#24320;&#21457;&#32773;&#30740;&#31350;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.16878</link><description>&lt;p&gt;
&#23545;API&#26041;&#38754;&#20998;&#26512;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning for API Aspect Analysis. (arXiv:2307.16878v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16878
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;API&#26041;&#38754;&#20998;&#26512;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#24182;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#24615;&#33021;&#65292;&#23545;Performance&#12289;Security&#12289;Usability&#21644;Documentation&#31561;&#26041;&#38754;&#30340;&#26816;&#27979;&#25928;&#26524;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#21644;&#24320;&#21457;&#32773;&#30740;&#31350;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;- CLAA-&#29992;&#20110;API&#35780;&#35770;&#20013;&#30340;API&#26041;&#38754;&#26816;&#27979;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#35757;&#32451;&#26377;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#30446;&#26631;&#20989;&#25968;&#30340;&#21464;&#25442;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#24615;&#33021;&#21644;&#24433;&#21709;&#20998;&#26512;&#35780;&#20272;&#20102;CLAA&#12290;&#23545;&#20110;&#24615;&#33021;&#20998;&#26512;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20174;Stack Overflow&#25910;&#38598;&#30340;&#24320;&#21457;&#32773;&#35752;&#35770;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#21464;&#25442;&#22120;&#27169;&#22411;&#33719;&#24471;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21464;&#25442;&#22120;&#27169;&#22411;&#22312;&#26816;&#27979;&#24615;&#33021;&#12289;&#23433;&#20840;&#24615;&#12289;&#21487;&#29992;&#24615;&#21644;&#25991;&#26723;&#26041;&#38754;&#31561;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#24433;&#21709;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#21644;&#24320;&#21457;&#32773;&#30740;&#31350;&#12290;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#24182;&#20154;&#24037;&#26631;&#35760;&#30340;200&#20010;&#22312;&#32447;&#35780;&#35770;&#19978;&#65292;CLAA&#36798;&#21040;&#20102;92%&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#21482;&#26377;81.5%&#12290;&#26681;&#25454;&#25105;&#20204;&#23545;10&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#30340;&#24320;&#21457;&#32773;&#30740;&#31350;&#65292;&#20351;&#29992;&#8220;Stack Overflow + CLAA&#8221;&#22312;API&#36873;&#25321;&#36807;&#31243;&#20013;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#20449;&#24515;&#12290;&#22797;&#21046;&#31243;&#24207;&#21253;&#65306;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach - CLAA - for API aspect detection in API reviews that utilizes transformer models trained with a supervised contrastive loss objective function. We evaluate CLAA using performance and impact analysis. For performance analysis, we utilized a benchmark dataset on developer discussions collected from Stack Overflow and compare the results to those obtained using state-of-the-art transformer models. Our experiments show that contrastive learning can significantly improve the performance of transformer models in detecting aspects such as Performance, Security, Usability, and Documentation. For impact analysis, we performed empirical and developer study. On a randomly selected and manually labeled 200 online reviews, CLAA achieved 92% accuracy while the SOTA baseline achieved 81.5%. According to our developer study involving 10 participants, the use of 'Stack Overflow + CLAA' resulted in increased accuracy and confidence during API selection. Replication package: 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35299;&#20915;&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;Hopfield&#32593;&#32476;&#20013;&#20351;&#29992;&#20851;&#32852;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#25105;&#20248;&#21270;&#27169;&#22411;&#65292;&#32593;&#32476;&#21487;&#20197;&#35299;&#20915;&#20855;&#20307;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20851;&#38190;&#20449;&#24687;&#21487;&#33021;&#20250;&#27704;&#20037;&#20002;&#22833;&#65292;&#23548;&#33268;&#32593;&#32476;&#20135;&#29983;&#30475;&#20284;&#26368;&#20248;&#20294;&#23454;&#38469;&#19978;&#19981;&#36866;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#29702;&#35299;&#32593;&#32476;&#35299;&#20915;&#38590;&#20197;&#22788;&#29702;&#38382;&#39064;&#30340;&#36807;&#31243;&#24456;&#26377;&#21551;&#21457;&#12290;</title><link>http://arxiv.org/abs/2307.16807</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#35299;&#20915;&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;Hopfield&#32593;&#32476;&#20013;&#20351;&#29992;&#20851;&#32852;&#35760;&#24518;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the use of associative memory in Hopfield networks designed to solve propositional satisfiability problems. (arXiv:2307.16807v2 [nlin.AO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16807
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#35299;&#20915;&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;Hopfield&#32593;&#32476;&#20013;&#20351;&#29992;&#20851;&#32852;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#25105;&#20248;&#21270;&#27169;&#22411;&#65292;&#32593;&#32476;&#21487;&#20197;&#35299;&#20915;&#20855;&#20307;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20851;&#38190;&#20449;&#24687;&#21487;&#33021;&#20250;&#27704;&#20037;&#20002;&#22833;&#65292;&#23548;&#33268;&#32593;&#32476;&#20135;&#29983;&#30475;&#20284;&#26368;&#20248;&#20294;&#23454;&#38469;&#19978;&#19981;&#36866;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#29702;&#35299;&#32593;&#32476;&#35299;&#20915;&#38590;&#20197;&#22788;&#29702;&#38382;&#39064;&#30340;&#36807;&#31243;&#24456;&#26377;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hopfield&#32593;&#32476;&#30001;&#20110;&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#29289;&#23398;&#19978;&#21487;&#34892;&#30340;&#26426;&#21046;&#65292;&#22240;&#27492;&#22312;&#35299;&#20915;&#35768;&#22810;&#31867;&#22411;&#30340;&#35745;&#31639;&#38382;&#39064;&#26102;&#26159;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#12290;&#33258;&#25105;&#20248;&#21270;&#65288;SO&#65289;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#36203;&#24067;&#23398;&#20064;&#35268;&#21017;&#21644;&#37325;&#22797;&#30340;&#32593;&#32476;&#37325;&#32622;&#21040;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#65292;&#26469;&#20248;&#21270;&#32593;&#32476;&#34892;&#20026;&#20197;&#36798;&#21040;&#32593;&#32476;&#20013;&#32534;&#30721;&#30340;&#26576;&#20010;&#24076;&#26395;&#30340;&#30446;&#26631;&#29366;&#24577;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#35813;&#36807;&#31243;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;SO&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;Liars&#38382;&#39064;&#21644;&#22320;&#22270;&#30528;&#33394;&#38382;&#39064;&#30340;&#20004;&#20010;&#20363;&#23376;&#26469;&#35299;&#20915;&#20855;&#20307;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#20851;&#38190;&#20449;&#24687;&#21487;&#33021;&#27704;&#36828;&#20002;&#22833;&#65292;&#20174;&#32780;&#20351;&#24471;&#23398;&#20064;&#32593;&#32476;&#20135;&#29983;&#30475;&#20284;&#26368;&#20248;&#35299;&#20294;&#23454;&#38469;&#19978;&#23545;&#25152;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#19981;&#21512;&#36866;&#12290;&#36825;&#31181;SO&#27169;&#22411;&#30340;&#21103;&#20316;&#29992;&#30475;&#20284;&#19981;&#22909;&#65292;&#21364;&#21487;&#20197;&#23545;&#20854;&#35299;&#20915;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#30340;&#36807;&#31243;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hopfield networks are an attractive choice for solving many types of computational problems because they provide a biologically plausible mechanism. The Self-Optimization (SO) model adds to the Hopfield network by using a biologically founded Hebbian learning rule, in combination with repeated network resets to arbitrary initial states, for optimizing its own behavior towards some desirable goal state encoded in the network. In order to better understand that process, we demonstrate first that the SO model can solve concrete combinatorial problems in SAT form, using two examples of the Liars problem and the map coloring problem. In addition, we show how under some conditions critical information might get lost forever with the learned network producing seemingly optimal solutions that are in fact inappropriate for the problem it was tasked to solve. What appears to be an undesirable side-effect of the SO model, can provide insight into its process for solving intractable problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.16680</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26223;&#35266;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#39046;&#20808;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#20154;&#31867;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#20063;&#26292;&#38706;&#20986;&#22266;&#26377;&#30340;&#39118;&#38505;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#30340;&#21452;&#37325;&#24615;&#36136;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#21487;&#20449;&#24230;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#25991;&#29486;&#65292;&#20294;&#38024;&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#28041;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#38271;&#26399;&#21644;&#26032;&#20852;&#23041;&#32961;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36131;&#20219;&#36825;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#24352;&#35814;&#23613;&#30340;&#22320;&#22270;&#65292;&#27010;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20123;&#21162;&#21147;&#23545;&#20110;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16387</link><description>&lt;p&gt;
Relation-Oriented: &#36808;&#21521;&#19982;&#30693;&#35782;&#23545;&#20934;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24212;&#29992;&#19968;&#20010;&#35266;&#23519;&#23548;&#21521;&#30340;&#21407;&#21017;&#65292;&#20854;&#20013;&#35266;&#23519;&#21464;&#37327;&#20808;&#23384;&#22312;&#24182;&#20026;&#26500;&#24314;&#20851;&#31995;&#22880;&#23450;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#26469;&#35828;&#36275;&#22815;&#20102;&#65292;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#25972;&#21512;&#26292;&#38706;&#20102;&#35266;&#23519;&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#23454;&#38469;&#29702;&#35299;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22609;&#36896;&#20102;&#30001;&#20851;&#31995;&#23450;&#20041;&#30340;&#35748;&#30693;&#23454;&#20307;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36328;&#36234;&#26102;&#38388;&#21644;&#36229;&#32500;&#24230;&#31354;&#38388;&#21046;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#34987;&#38480;&#21046;&#22312;&#35266;&#23519;&#26500;&#24314;&#20013;&#12290;&#20174;&#19968;&#31181;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20581;&#24247;&#20449;&#24687;&#23398;&#30340;&#30452;&#35266;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22312;&#25105;&#20204;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#36825;&#31181;&#19981;&#23545;&#40784;&#30340;&#26681;&#28304;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#19968;&#31181;&#23454;&#38469;&#23454;&#26045;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, we naturally apply an Observation-Oriented principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the misalignment between the observational models and our actual comprehension. Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs. From an innovative Relation-Oriented perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the relation-defined representation learning methodology as a practical implementation of Relation-Oriented modeling, supported by extensive experimental validation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;AlignDet&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#36807;&#31243;&#20998;&#35299;&#20026;&#22270;&#20687;&#22495;&#21644;&#26694;&#22495;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#20943;&#36731;&#30446;&#26631;&#26816;&#27979;&#20013;&#29616;&#26377;&#23454;&#36341;&#20013;&#30340;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#20219;&#21153;&#24046;&#24322;&#65292;&#25552;&#39640;&#26816;&#27979;&#22120;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11077</link><description>&lt;p&gt;
AlignDet: &#23545;&#40784;&#30446;&#26631;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
AlignDet: Aligning Pre-training and Fine-tuning in Object Detection. (arXiv:2307.11077v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;AlignDet&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#36807;&#31243;&#20998;&#35299;&#20026;&#22270;&#20687;&#22495;&#21644;&#26694;&#22495;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#20943;&#36731;&#30446;&#26631;&#26816;&#27979;&#20013;&#29616;&#26377;&#23454;&#36341;&#20013;&#30340;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#20219;&#21153;&#24046;&#24322;&#65292;&#25552;&#39640;&#26816;&#27979;&#22120;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21518;&#36827;&#34892;&#24494;&#35843;&#30340;&#33539;&#24335;&#22312;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#23454;&#36341;&#20013;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20043;&#38388;&#30340;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#20219;&#21153;&#24046;&#24322;&#65292;&#36825;&#20123;&#24046;&#24322;&#38544;&#21547;&#22320;&#38480;&#21046;&#20102;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AlignDet&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#29616;&#26377;&#26816;&#27979;&#22120;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#24046;&#24322;&#12290;AlignDet&#23558;&#39044;&#35757;&#32451;&#36807;&#31243;&#20998;&#35299;&#20026;&#20004;&#20010;&#38454;&#27573;&#65292;&#21363;&#22270;&#20687;&#22495;&#21644;&#26694;&#22495;&#39044;&#35757;&#32451;&#12290;&#22270;&#20687;&#22495;&#39044;&#35757;&#32451;&#20248;&#21270;&#26816;&#27979;&#20027;&#24178;&#20197;&#25429;&#25417;&#25972;&#20307;&#35270;&#35273;&#25277;&#35937;&#65292;&#32780;&#26694;&#22495;&#39044;&#35757;&#32451;&#23398;&#20064;&#23454;&#20363;&#32423;&#35821;&#20041;&#21644;&#20219;&#21153;&#24863;&#30693;&#27010;&#24565;&#20197;&#21021;&#22987;&#21270;&#20027;&#24178;&#20043;&#22806;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#34701;&#20837;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#20027;&#24178;&#65292;&#25105;&#20204;&#21487;&#20197;&#23545;&#21508;&#31181;&#26816;&#27979;&#22120;&#30340;&#25152;&#26377;&#27169;&#22359;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paradigm of large-scale pre-training followed by downstream fine-tuning has been widely employed in various object detection algorithms. In this paper, we reveal discrepancies in data, model, and task between the pre-training and fine-tuning procedure in existing practices, which implicitly limit the detector's performance, generalization ability, and convergence speed. To this end, we propose AlignDet, a unified pre-training framework that can be adapted to various existing detectors to alleviate the discrepancies. AlignDet decouples the pre-training process into two stages, i.e., image-domain and box-domain pre-training. The image-domain pre-training optimizes the detection backbone to capture holistic visual abstraction, and box-domain pre-training learns instance-level semantics and task-aware concepts to initialize the parts out of the backbone. By incorporating the self-supervised pre-trained backbones, we can pre-train all modules for various detectors in an unsupervised par
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20026;&#22522;&#30784;&#31185;&#23398;&#30340;&#21457;&#29616;&#25552;&#20379;&#26426;&#20250;&#65292;&#36890;&#36807;&#20854;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#30340;&#38381;&#29615;&#26041;&#27861;&#65292;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#36827;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.07522</link><description>&lt;p&gt;
&#30001;&#29983;&#25104;&#38381;&#29615;&#20154;&#24037;&#26234;&#33021;&#24341;&#39046;&#30340;&#22522;&#30784;&#31185;&#23398;&#30340;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence. (arXiv:2307.07522v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07522
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20026;&#22522;&#30784;&#31185;&#23398;&#30340;&#21457;&#29616;&#25552;&#20379;&#26426;&#20250;&#65292;&#36890;&#36807;&#20854;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#30340;&#38381;&#29615;&#26041;&#27861;&#65292;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#36827;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#27491;&#22312;&#39072;&#35206;&#25216;&#26415;&#21019;&#26032;&#12289;&#20135;&#21697;&#24320;&#21457;&#21644;&#25972;&#20010;&#31038;&#20250;&#12290;&#20154;&#24037;&#26234;&#33021;&#23545;&#25216;&#26415;&#30340;&#36129;&#29486;&#21487;&#20197;&#36890;&#36807;&#22810;&#31181;&#36884;&#24452;&#23454;&#29616;&#65292;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#26126;&#30830;&#30340;&#24615;&#33021;&#35780;&#20272;&#26631;&#20934;&#65292;&#33539;&#22260;&#20174;&#27169;&#24335;&#35782;&#21035;&#21644;&#20998;&#31867;&#21040;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31185;&#23398;&#23454;&#36341;&#21644;&#27169;&#22411;&#21457;&#29616;&#38656;&#35201;&#35775;&#38382;&#39640;&#36136;&#37327;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20154;&#24037;&#26234;&#33021;&#23545;&#22522;&#30784;&#31185;&#23398;&#30340;&#36129;&#29486;&#36739;&#23569;&#12290;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#33021;&#20195;&#34920;&#20102;&#36890;&#36807;&#23450;&#37327;&#27169;&#22411;&#22686;&#24378;&#21644;&#21152;&#36895;&#22522;&#30784;&#28145;&#24230;&#31185;&#23398;&#30340;&#31185;&#23398;&#21457;&#29616;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#30740;&#31350;&#20102;&#19968;&#31181;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#12289;&#33258;&#21160;&#21270;&#30340;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#26041;&#27861;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#24320;&#25918;&#24335;&#33258;&#20027;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning and AI, including Generative AI and LLMs, are disrupting technological innovation, product development, and society as a whole. AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models. Yet, AI has contributed less to fundamental science in part because large data sets of high-quality data for scientific practice and model discovery are more difficult to access. Generative AI, in general, and Large Language Models in particular, may represent an opportunity to augment and accelerate the scientific discovery of fundamental deep science with quantitative models. Here we explore and investigate aspects of an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space. Int
&lt;/p&gt;</description></item><item><title>VesselMorph&#20351;&#29992;&#19968;&#20010;&#22522;&#20110;&#24418;&#29366;&#24863;&#30693;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#34880;&#31649;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#26469;&#25512;&#24191;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00240</link><description>&lt;p&gt;
VesselMorph: &#22522;&#20110;&#24418;&#29366;&#24863;&#30693;&#34920;&#24449;&#30340;&#39046;&#22495;&#36890;&#29992;&#30340;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
VesselMorph: Domain-Generalized Retinal Vessel Segmentation via Shape-Aware Representation. (arXiv:2307.00240v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00240
&lt;/p&gt;
&lt;p&gt;
VesselMorph&#20351;&#29992;&#19968;&#20010;&#22522;&#20110;&#24418;&#29366;&#24863;&#30693;&#30340;&#34920;&#24449;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#34880;&#31649;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#26469;&#25512;&#24191;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#32479;&#19968;&#30340;&#25104;&#20687;&#21327;&#35758;&#65292;&#26469;&#33258;&#19981;&#21516;&#31449;&#28857;&#30340;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#31227;&#26159;&#21307;&#23398;&#22270;&#20687;&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#31639;&#27861;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#23545;&#20110;&#35270;&#32593;&#33180;&#34880;&#31649;&#22270;&#20687;&#65292;&#39046;&#22495;&#36716;&#31227;&#36890;&#24120;&#34920;&#29616;&#20026;&#24378;&#24230;&#12289;&#23545;&#27604;&#24230;&#21644;&#20998;&#36776;&#29575;&#30340;&#21464;&#21270;&#65292;&#32780;&#34880;&#31649;&#30340;&#22522;&#26412;&#31649;&#29366;&#24418;&#29366;&#20445;&#25345;&#19981;&#21464;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#36825;&#31181;&#39046;&#22495;&#19981;&#21464;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#28145;&#24230;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VesselMorph&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#24418;&#29366;&#24863;&#30693;&#34920;&#24449;&#26469;&#25512;&#24191;2D&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#20219;&#21153;&#12290;&#21463;&#20256;&#32479;Frangi&#28388;&#27874;&#22120;&#21644;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Hessian&#30340;&#21452;&#26497;&#24352;&#37327;&#22330;&#26469;&#25551;&#36848;&#34880;&#31649;&#30340;&#24418;&#24577;&#23398;&#29305;&#24449;&#65292;&#20174;&#32780;&#32771;&#34385;&#21040;&#24418;&#29366;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#24378;&#24230;&#22270;&#20687;&#21644;&#24352;&#37327;&#22330;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the absence of a single standardized imaging protocol, domain shift between data acquired from different sites is an inherent property of medical images and has become a major obstacle for large-scale deployment of learning-based algorithms. For retinal vessel images, domain shift usually presents as the variation of intensity, contrast and resolution, while the basic tubular shape of vessels remains unaffected. Thus, taking advantage of such domain-invariant morphological features can greatly improve the generalizability of deep models. In this study, we propose a method named VesselMorph which generalizes the 2D retinal vessel segmentation task by synthesizing a shape-aware representation. Inspired by the traditional Frangi filter and the diffusion tensor imaging literature, we introduce a Hessian-based bipolar tensor field to depict the morphology of the vessels so that the shape information is taken into account. We map the intensity image and the tensor field to a latent sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GTAGC&#30340;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#22270;&#24418;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#33258;&#32534;&#30721;&#22120;&#21644;&#22270;&#24418;&#21464;&#25442;&#22120;&#65292;GTAGC&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#22270;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11307</link><description>&lt;p&gt;
&#22686;&#24378;&#23646;&#24615;&#32858;&#31867;&#30340;&#22270;&#24418;&#21464;&#25442;&#26041;&#27861;&#65306;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#24418;&#36716;&#25442;&#22120;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transforming Graphs for Enhanced Attribute-Based Clustering: An Innovative Graph Transformer Method. (arXiv:2306.11307v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GTAGC&#30340;&#22270;&#24418;&#33258;&#32534;&#30721;&#22120;&#22270;&#24418;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#33258;&#32534;&#30721;&#22120;&#21644;&#22270;&#24418;&#21464;&#25442;&#22120;&#65292;GTAGC&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#22270;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#24433;&#21709;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#25105;&#20204;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#26377;&#21161;&#20110;&#22270;&#32858;&#31867;&#65292;&#36825;&#26159;&#21508;&#20010;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#27880;&#24847;&#21147;&#26426;&#21046;&#26368;&#36817;&#24050;&#32463;&#36827;&#20837;&#20102;&#22270;&#23398;&#20064;&#30340;&#39046;&#22495;&#65292;&#36825;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#30740;&#31350;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#24050;&#25104;&#20026;&#22270;&#32858;&#31867;&#20219;&#21153;&#20248;&#36873;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#23616;&#37096;&#27880;&#24847;&#26426;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#29702;&#35299;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#22797;&#26434;&#20840;&#23616;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#22270;&#24418;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#30340;&#22270;&#24418;&#32858;&#31867;&#65288;GTAGC&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22270;&#33258;&#32534;&#30721;&#22120;&#19982;&#22270;&#24418;&#21464;&#25442;&#22120;&#34701;&#21512;&#65292;GTAGC&#33021;&#22815;&#25429;&#33719;&#33410;&#28857;&#20043;&#38388;&#30340;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#34701;&#21512;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20351;&#24471;&#22270;&#24418;&#32858;&#31867;&#20219;&#21153;&#26377;&#20102;&#26174;&#30528;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Representation Learning (GRL) is an influential methodology, enabling a more profound understanding of graph-structured data and aiding graph clustering, a critical task across various domains. The recent incursion of attention mechanisms, originally an artifact of Natural Language Processing (NLP), into the realm of graph learning has spearheaded a notable shift in research trends. Consequently, Graph Attention Networks (GATs) and Graph Attention Auto-Encoders have emerged as preferred tools for graph clustering tasks. Yet, these methods primarily employ a local attention mechanism, thereby curbing their capacity to apprehend the intricate global dependencies between nodes within graphs. Addressing these impediments, this study introduces an innovative method known as the Graph Transformer Auto-Encoder for Graph Clustering (GTAGC). By melding the Graph Auto-Encoder with the Graph Transformer, GTAGC is adept at capturing global dependencies between nodes. This integration amplifi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#65292;&#20351;&#29992;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#23454;&#29616;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#30340;&#32852;&#21512;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#22312;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#20013;&#32463;&#36807;&#27979;&#35797;&#65292;&#33021;&#22815;&#21462;&#24471;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07946</link><description>&lt;p&gt;
&#30740;&#31350;&#65306;&#31038;&#20132;&#24863;&#30693;&#26102;&#38388;&#26494;&#25955;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
STUDY: Socially Aware Temporally Casual Decoder Recommender Systems. (arXiv:2306.07946v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07946
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#65292;&#20351;&#29992;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#23454;&#29616;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#30340;&#32852;&#21512;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#22312;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#20013;&#32463;&#36807;&#27979;&#35797;&#65292;&#33021;&#22815;&#21462;&#24471;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#21487;&#33719;&#21462;&#30340;&#25968;&#25454;&#25968;&#37327;&#36807;&#20110;&#24222;&#22823;&#65292;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#24517;&#35201;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#31526;&#21512;&#20182;&#20204;&#20852;&#36259;&#30340;&#29289;&#21697;&#12290;&#24403;&#31038;&#20132;&#32593;&#32476;&#20449;&#24687;&#23384;&#22312;&#26102;&#65292;&#26377;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#20570;&#20986;&#26356;&#22909;&#30340;&#25512;&#33616;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26377;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#36825;&#20123;&#32593;&#32476;&#35757;&#32451;&#36215;&#26469;&#38750;&#24120;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#12290;STUDY&#37319;&#29992;&#19968;&#20010;&#32463;&#36807;&#20462;&#25913;&#30340;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#30340;&#21333;&#21521;&#21069;&#20256;&#65292;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#36827;&#34892;&#32852;&#21512;&#25512;&#26029;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#23398;&#26657;&#35838;&#22530;&#32467;&#26500;&#23450;&#20041;&#31038;&#20132;&#32593;&#32476;&#30340;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#21333;&#19968;&#22343;&#21248;&#32593;&#32476;&#35774;&#35745;&#31616;&#21333;&#24615;&#30340;&#21516;&#26102;&#65292;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the overwhelming amount of data available both on and offline today, recommender systems have become much needed to help users find items tailored to their interests. When social network information exists there are methods that utilize this information to make better recommendations, however the methods are often clunky with complex architectures and training procedures. Furthermore many of the existing methods utilize graph neural networks which are notoriously difficult to train. To address this, we propose Socially-aware Temporally caUsal Decoder recommender sYstems (STUDY). STUDY does joint inference over groups of users who are adjacent in the social network graph using a single forward pass of a modified transformer decoder network. We test our method in a school-based educational content setting, using classroom structure to define social networks. Our method outperforms both social and sequential methods while maintaining the design simplicity of a single homogeneous netw
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.17330</link><description>&lt;p&gt;
MADiff&#65306;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MADiff: Offline Multi-agent Learning with Diffusion Models. (arXiv:2305.17330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#36817;&#22312;&#21253;&#25324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#20869;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20854;&#20013;&#31574;&#30053;&#36890;&#36807;&#22312;&#22312;&#32447;&#35780;&#20272;&#20013;&#20135;&#29983;&#36712;&#36857;&#26469;&#36827;&#34892;&#35268;&#21010;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#26174;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;DM&#22914;&#20309;&#22312;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#20013;&#25805;&#20316;&#65292;&#20854;&#20013;&#20195;&#29702;&#21830;&#24456;&#38590;&#22312;&#29420;&#31435;&#24314;&#27169;&#27599;&#20010;&#20195;&#29702;&#21830;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#22242;&#38431;&#21512;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;MADiff&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MADiff&#26159;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#23545;&#22810;&#20010;&#25193;&#25955;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#22797;&#26434;&#21327;&#35843;&#24314;&#27169;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MADiff&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#26694;&#26550;&#65292;&#23427;&#26082;&#21487;&#20197;&#34892;&#20026;&#20026;&#20998;&#25955;&#30340;&#25919;&#31574;&#65292;&#21448;&#21487;&#20197;&#20026;&#38598;&#20013;&#25511;&#21046;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#25163;&#24314;&#27169;&#65292;&#24182;&#21487;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model (DM), as a powerful generative model, recently achieved huge success in various scenarios including offline reinforcement learning, where the policy learns to conduct planning by generating trajectory in the online evaluation. However, despite the effectiveness shown for single-agent learning, it remains unclear how DMs can operate in multi-agent problems, where agents can hardly complete teamwork without good coordination by independently modeling each agent's trajectories. In this paper, we propose MADiff, a novel generative multi-agent learning framework to tackle this problem. MADiff is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple diffusion agents. To the best of our knowledge, MADiff is the first diffusion-based multi-agent offline RL framework, which behaves as both a decentralized policy and a centralized controller, which includes opponent modeling and can be used for multi-agent trajectory predic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#19979;&#30028;&#65292;&#25506;&#35752;&#20102;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#26102;&#38656;&#35201;&#30340;&#26679;&#26412;&#25968;&#37327;&#21450;&#20854;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.16014</link><description>&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22810;&#23569;&#26679;&#26412;&#25165;&#33021;&#21033;&#29992;&#24179;&#28369;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
How many samples are needed to leverage smoothness?. (arXiv:2305.16014v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#19979;&#30028;&#65292;&#25506;&#35752;&#20102;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#26102;&#38656;&#35201;&#30340;&#26679;&#26412;&#25968;&#37327;&#21450;&#20854;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#30340;&#26680;&#24515;&#21407;&#21017;&#20043;&#19968;&#26159;&#65292;&#30446;&#26631;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#21487;&#20197;&#25171;&#30772;&#32500;&#24230;&#28798;&#38590;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#27888;&#21202;&#23637;&#24320;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#38656;&#35201;&#36275;&#22815;&#25509;&#36817;&#19968;&#36215;&#30340;&#26679;&#26412;&#26469;&#33719;&#24471;&#39640;&#38454;&#23548;&#25968;&#30340;&#26377;&#24847;&#20041;&#20272;&#35745;&#65292;&#36825;&#22312;&#25968;&#25454;&#37327;&#30456;&#23545;&#36739;&#23567;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#20284;&#20046;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#36890;&#36807;&#25512;&#23548;&#24191;&#20041;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#30340;&#19979;&#30028;&#65292;&#30740;&#31350;&#20102;&#24120;&#25968;&#21644;&#30636;&#24577;&#21306;&#22495;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#34987;&#24573;&#30053;&#21364;&#21457;&#25381;&#20102;&#20027;&#23548;&#20316;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core principle in statistical learning is that smoothness of target functions allows to break the curse of dimensionality. However, learning a smooth function through Taylor expansions requires enough samples close to one another to get meaningful estimate of high-order derivatives, which seems hard in machine learning problems where the ratio between number of data and input dimension is relatively small. Should we really hope to break the curse of dimensionality based on Taylor expansion estimation? What happens if Taylor expansions are replaced by Fourier or wavelet expansions? By deriving a new lower bound on the generalization error, this paper investigates the role of constants and transitory regimes which are usually not depicted beyond classical learning theory statements while that play a dominant role in practice.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#20197;&#24448;&#26234;&#33021;&#31995;&#32479;&#27979;&#35797;&#30340;&#19981;&#36275;&#21644;&#25152;&#25552;&#20986;&#30340;&#26367;&#25442;&#27979;&#35797;&#20316;&#20026;&#19968;&#31181;&#23436;&#21892;&#27979;&#35797;&#30340;&#33021;&#21147;&#65292;&#35813;&#27979;&#35797;&#21453;&#26144;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#25216;&#33021;&#20114;&#34917;&#24615;&#65292;&#33021;&#22815;&#26500;&#24314;&#26356;&#22810;&#21453;&#26144;&#26234;&#33021;&#21487;&#33021;&#30340;&#27010;&#24565;&#21644;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11472</link><description>&lt;p&gt;
&#26234;&#33021;&#31995;&#32479;&#27979;&#35797;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Testing System Intelligence. (arXiv:2305.11472v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#20197;&#24448;&#26234;&#33021;&#31995;&#32479;&#27979;&#35797;&#30340;&#19981;&#36275;&#21644;&#25152;&#25552;&#20986;&#30340;&#26367;&#25442;&#27979;&#35797;&#20316;&#20026;&#19968;&#31181;&#23436;&#21892;&#27979;&#35797;&#30340;&#33021;&#21147;&#65292;&#35813;&#27979;&#35797;&#21453;&#26144;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#25216;&#33021;&#20114;&#34917;&#24615;&#65292;&#33021;&#22815;&#26500;&#24314;&#26356;&#22810;&#21453;&#26144;&#26234;&#33021;&#21487;&#33021;&#30340;&#27010;&#24565;&#21644;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#38024;&#23545;&#26234;&#33021;&#31995;&#32479;&#30340;&#27979;&#35797;&#30340;&#20805;&#20998;&#24615;&#20197;&#21450;&#20854;&#23454;&#26045;&#24102;&#26469;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26367;&#25442;&#27979;&#35797;&#20316;&#20026;&#31995;&#32479;&#22312;&#32473;&#23450;&#29615;&#22659;&#19979;&#25104;&#21151;&#26367;&#25442;&#21478;&#19968;&#20010;&#25191;&#34892;&#20219;&#21153;&#30340;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#34920;&#24449;&#20154;&#31867;&#26234;&#33021;&#30340;&#26174;&#33879;&#26041;&#38754;&#65292;&#36825;&#20123;&#26041;&#38754;&#19981;&#33021;&#34987;&#22270;&#28789;&#27979;&#35797;&#32771;&#34385;&#21040;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26500;&#24314;&#36890;&#36807;&#26367;&#25442;&#27979;&#35797;&#30340;&#26234;&#33021;&#31995;&#32479;&#28041;&#21450;&#21040;&#19968;&#31995;&#21015;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#25152;&#26080;&#27861;&#35299;&#20915;&#30340;&#25216;&#26415;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23454;&#26045;&#25152;&#25552;&#20986;&#30340;&#27979;&#35797;&#24182;&#39564;&#35777;&#26234;&#33021;&#31995;&#32479;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26234;&#33021;&#31995;&#32479;&#39564;&#35777;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#65292;&#24182;&#20513;&#23548;&#26032;&#30340;&#29702;&#35770;&#22522;&#30784;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#20005;&#26684;&#27979;&#35797;&#26041;&#27861;&#12290;&#25105;&#20204;&#24314;&#35758;&#22522;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#25216;&#33021;&#20114;&#34917;&#24615;&#30340;&#26367;&#25442;&#27979;&#35797;&#21487;&#20197;&#23548;&#33268;&#22810;&#31181;&#21453;&#26144;&#32467;&#21512;&#22522;&#20110;&#25968;&#25454;&#21644;&#30693;&#35782;&#30340;&#33021;&#21147;&#30340;&#26234;&#33021;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss the adequacy of tests for intelligent systems and practical problems raised by their implementation. We propose the replacement test as the ability of a system to replace successfully another system performing a task in a given context. We show how it can characterize salient aspects of human intelligence that cannot be taken into account by the Turing test. We argue that building intelligent systems passing the replacement test involves a series of technical problems that are outside the scope of current AI. We present a framework for implementing the proposed test and validating the properties of the intelligent systems. We discuss the inherent limitations of intelligent system validation and advocate new theoretical foundations for extending existing rigorous test methods. We suggest that the replacement test, based on the complementarity of skills between human and machine, can lead to a multitude of intelligence concepts reflecting the ability to combine data-based and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11095</link><description>&lt;p&gt;
&#28608;&#21457;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#20197;&#23454;&#29616;&#38646;-shot&#20219;&#21153;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;Whisper&#30340;&#26032;&#20852;&#21151;&#33021;&#65292;&#22312;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;&#27169;&#22411;&#21518;&#65292;&#36866;&#24212;&#20102;&#26410;&#35265;&#36807;&#30340;AVSR&#65292;CS-ASR&#21644;ST&#19977;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#35201;&#20040;&#21033;&#29992;&#21478;&#19968;&#20010;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#35201;&#20040;&#31616;&#21333;&#22320;&#25805;&#20316;&#40664;&#35748;&#25552;&#31034;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#20351;&#36825;&#19977;&#20010;&#38646;-shot&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;10%&#21040;45&#65285;&#65292;&#29978;&#33267;&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;SotA&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;Whisper&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#21253;&#25324;&#20854;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#21475;&#38899;&#30340;&#20559;&#22909;&#20197;&#21450;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22810;&#35821;&#35328;&#29702;&#35299;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/jasonppy/PromptingWhisper&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
&lt;/p&gt;</description></item><item><title>G-MATT&#26159;&#19968;&#20010;&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19982;&#21270;&#23398;&#30693;&#35782;&#30340;&#21270;&#23398;&#24863;&#30693;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#20998;&#23618;SMILES&#35821;&#27861;&#26641;&#36755;&#20837;&#30340;&#22522;&#30784;&#19978;&#37319;&#29992;&#26641;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21333;&#27493;&#22238;&#28335;&#21512;&#25104;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03153</link><description>&lt;p&gt;
G-MATT: &#20998;&#23376;&#35821;&#27861;&#26641;&#21464;&#25442;&#22120;&#30340;&#21333;&#27493;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
G-MATT: Single-step Retrosynthesis Prediction using Molecular Grammar Tree Transformer. (arXiv:2305.03153v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03153
&lt;/p&gt;
&lt;p&gt;
G-MATT&#26159;&#19968;&#20010;&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19982;&#21270;&#23398;&#30693;&#35782;&#30340;&#21270;&#23398;&#24863;&#30693;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#20998;&#23618;SMILES&#35821;&#27861;&#26641;&#36755;&#20837;&#30340;&#22522;&#30784;&#19978;&#37319;&#29992;&#26641;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21333;&#27493;&#22238;&#28335;&#21512;&#25104;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25253;&#36947;&#20102;&#20960;&#31181;&#22522;&#20110;&#21453;&#24212;&#27169;&#26495;&#21644;&#22522;&#20110;&#33258;&#30001;&#27169;&#26495;&#30340;&#21333;&#27493;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;&#26041;&#27861;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#35768;&#22810;&#22312;&#20256;&#32479;&#25968;&#25454;&#39537;&#21160;&#25351;&#26631;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20351;&#29992;&#30340;&#27169;&#22411;&#26550;&#26500;&#19982;&#25903;&#37197;&#21453;&#21521;&#21512;&#25104;&#30340;&#24213;&#23618;&#21270;&#23398;&#21407;&#21017;&#20043;&#38388;&#23384;&#22312;&#33073;&#33410;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21270;&#23398;&#24863;&#30693;&#22238;&#28335;&#21512;&#25104;&#39044;&#27979;&#26694;&#26550;&#65292;&#23558;&#24378;&#22823;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19982;&#21270;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;SMILES&#35821;&#27861;&#26641;&#30340;&#26641;&#21040;&#24207;&#21015;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#34987;&#32431;SMILES&#34920;&#31034;&#27861;&#30340;&#27169;&#22411;&#24573;&#30053;&#30340;&#24213;&#23618;&#21270;&#23398;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#35821;&#27861;&#30340;&#20998;&#23376;&#27880;&#24847;&#21147;&#26641;&#21464;&#25442;&#22120;&#65288;G-MATT&#65289;&#65292;&#19982;&#22522;&#32447;&#22238;&#28335;&#21512;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290; G-MATT&#30340;&#20934;&#30830;&#29575;&#25490;&#21517;&#21069;1&#20026;51&#65285;&#65288;&#21069;10&#20026;79.1&#65285;&#65289;&#65292;&#26080;&#25928;&#29575;&#20026;1.5&#65285;&#65292;
&lt;/p&gt;
&lt;p&gt;
In recent years, several reaction templates-based and template-free approaches have been reported for single-step retrosynthesis prediction. Even though many of these approaches perform well from traditional data-driven metrics standpoint, there is a disconnect between model architectures used and underlying chemistry principles governing retrosynthesis. Here, we propose a novel chemistry-aware retrosynthesis prediction framework that combines powerful data-driven models with chemistry knowledge. We report a tree-to-sequence transformer architecture based on hierarchical SMILES grammar trees as input containing underlying chemistry information that is otherwise ignored by models based on purely SMILES-based representations. The proposed framework, grammar-based molecular attention tree transformer (G-MATT), achieves significant performance improvements compared to baseline retrosynthesis models. G-MATT achieves a top-1 accuracy of 51% (top-10 accuracy of 79.1%), invalid rate of 1.5%, a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#26368;&#20339;&#25903;&#25345;&#29615;&#22659;&#65292;&#20855;&#20307;&#37325;&#28857;&#30740;&#31350;&#20102;ML&#24320;&#21457;&#20013;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#30340;&#38556;&#30861;&#20197;&#21450;&#22914;&#20309;&#26500;&#24314;&#21644;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#32570;&#20047;&#36275;&#22815;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00136</link><description>&lt;p&gt;
&#25552;&#20379;&#26368;&#20339;&#25903;&#25345;&#29615;&#22659;&#20248;&#21270;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Optimizing the AI Development Process by Providing the Best Support Environment. (arXiv:2305.00136v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#26368;&#20339;&#25903;&#25345;&#29615;&#22659;&#65292;&#20855;&#20307;&#37325;&#28857;&#30740;&#31350;&#20102;ML&#24320;&#21457;&#20013;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#30340;&#38556;&#30861;&#20197;&#21450;&#22914;&#20309;&#26500;&#24314;&#21644;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#32570;&#20047;&#36275;&#22815;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#20197;&#25552;&#20379;&#26368;&#20339;&#25903;&#25345;&#29615;&#22659;&#12290;ML&#30340;&#20027;&#35201;&#38454;&#27573;&#21253;&#25324;&#38382;&#39064;&#29702;&#35299;&#65292;&#25968;&#25454;&#31649;&#29702;&#65292;&#27169;&#22411;&#26500;&#24314;&#65292;&#27169;&#22411;&#37096;&#32626;&#21644;&#32500;&#25252;&#12290;&#26412;&#39033;&#30446;&#37325;&#28857;&#30740;&#31350;&#20102;ML&#24320;&#21457;&#30340;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#21450;&#20854;&#38556;&#30861;&#65292;&#22240;&#20026;&#26368;&#32456;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;&#36755;&#20837;&#21040;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21457;&#29616;&#36825;&#19968;&#38454;&#27573;&#26368;&#22823;&#30340;&#38556;&#30861;&#26159;&#32570;&#20047;&#36275;&#22815;&#30340;&#27169;&#22411;&#23398;&#20064;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#20445;&#23494;&#39046;&#22495;&#12290;&#26412;&#39033;&#30446;&#26088;&#22312;&#26500;&#24314;&#21644;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#24110;&#21161;&#35299;&#20915;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#32570;&#20047;&#36275;&#22815;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21487;&#20197;&#20174;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#26032;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of this study is to investigate the development process for Artificial inelegance (AI) and machine learning (ML) applications in order to provide the best support environment. The main stages of ML are problem understanding, data management, model building, model deployment and maintenance. This project focuses on investigating the data management stage of ML development and its obstacles as it is the most important stage of machine learning development because the accuracy of the end model is relying on the kind of data fed into the model. The biggest obstacle found on this stage was the lack of sufficient data for model learning, especially in the fields where data is confidential. This project aimed to build and develop a framework for researchers and developers that can help solve the lack of sufficient data during data management stage. The framework utilizes several data augmentation techniques that can be used to generate new data from the original dataset which can 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;UDTIRI&#65292;&#21253;&#21547;&#20102;&#26631;&#35760;&#40784;&#20840;&#30340;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#21487;&#20197;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.08842</link><description>&lt;p&gt;
UDTIRI:&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
UDTIRI: An Open-Source Road Pothole Detection Benchmark Suite. (arXiv:2304.08842v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08842
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;UDTIRI&#65292;&#21253;&#21547;&#20102;&#26631;&#35760;&#40784;&#20840;&#30340;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#21487;&#20197;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30475;&#21040;&#22312;&#22478;&#24066;&#25968;&#23383;&#23402;&#29983;&#39046;&#22495;&#20013;&#21033;&#29992;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#22312;&#26234;&#33021;&#36947;&#36335;&#26816;&#26597;&#39046;&#22495;&#65292;&#30446;&#21069;&#30740;&#31350;&#21644;&#25968;&#25454;&#26377;&#38480;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;Urban Digital Twins Intelligent Road Inspection (UDTIRI)&#25968;&#25454;&#38598;&#30340;&#26631;&#35760;&#40784;&#20840;&#30340;&#36947;&#36335;&#22353;&#27934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#22815;&#35753;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#35753;&#31639;&#27861;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#22330;&#26223;&#24182;&#26368;&#22823;&#21270;&#20854;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#25293;&#25668;&#20110;&#19981;&#21516;&#30340;&#24773;&#22659;&#20013;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20809;&#29031;&#21644;&#28287;&#24230;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#24847;&#22270;&#26159;&#23558;&#36825;&#20010;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#33457;&#36153;&#20102;&#22823;&#37327;&#31934;&#21147;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#23545;UDTIRI&#25968;&#25454;&#38598;&#30340;&#19968;&#20123;&#20195;&#34920;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is seen that there is enormous potential to leverage powerful deep learning methods in the emerging field of urban digital twins. It is particularly in the area of intelligent road inspection where there is currently limited research and data available. To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins Intelligent Road Inspection (UDTIRI) dataset. We hope this dataset will enable the use of powerful deep learning methods in urban road inspection, providing algorithms with a more comprehensive understanding of the scene and maximizing their potential. Our dataset comprises 1000 images of potholes, captured in various scenarios with different lighting and humidity conditions. Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks. Our team has devoted significant effort to conducting a detailed statistical analysis, and benchmarking a selection of represent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38468;&#36817;&#36947;&#36335;&#32467;&#26500;&#30340;&#20960;&#20309;&#38480;&#21046;&#30340;&#36807;&#31243;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#38750;&#23436;&#20840;&#22278;&#24418;&#19988;&#31867;&#20284;&#20110;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#29615;&#24418;&#20132;&#21449;&#21475;&#30340;&#36710;&#36947;&#65292;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2303.17900</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#27979;&#35797;&#30340;&#22797;&#26434;&#29615;&#24418;&#20132;&#21449;&#21475;&#30340;&#36807;&#31243;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Procedural Generation of Complex Roundabouts for Autonomous Vehicle Testing. (arXiv:2303.17900v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38468;&#36817;&#36947;&#36335;&#32467;&#26500;&#30340;&#20960;&#20309;&#38480;&#21046;&#30340;&#36807;&#31243;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#38750;&#23436;&#20840;&#22278;&#24418;&#19988;&#31867;&#20284;&#20110;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#29615;&#24418;&#20132;&#21449;&#21475;&#30340;&#36710;&#36947;&#65292;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#28165;&#36947;&#36335;&#26159;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#27169;&#25311;&#27979;&#35797;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#32780;&#29615;&#24418;&#20132;&#21449;&#21475;&#26159;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#36335;&#27573;&#65292;&#30446;&#21069;&#23545;&#20854;&#30740;&#31350;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#38468;&#36817;&#36947;&#36335;&#32467;&#26500;&#30340;&#20960;&#20309;&#38480;&#21046;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#36896;&#29615;&#24418;&#20132;&#21449;&#21475;&#30340;&#36807;&#31243;&#29983;&#25104;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#19981;&#23436;&#20840;&#22278;&#24418;&#19988;&#31867;&#20284;&#20110;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#29615;&#24418;&#20132;&#21449;&#21475;&#30340;&#36710;&#36947;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#36830;&#25509;&#21040;&#29615;&#24418;&#20132;&#21449;&#21475;&#30340;&#36884;&#24452;&#36947;&#36335;&#30340;&#20219;&#24847;&#35282;&#24230;&#12290;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#29615;&#24418;&#20132;&#21449;&#21475;&#34701;&#20837;&#39640;&#28165;&#36947;&#36335;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25110;&#20351;&#29992;&#29420;&#31435;&#30340;&#29615;&#24418;&#20132;&#21449;&#21475;&#36827;&#34892;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-definition roads are an essential component of realistic driving scenario simulation for autonomous vehicle testing. Roundabouts are one of the key road segments that have not been thoroughly investigated. Based on the geometric constraints of the nearby road structure, this work presents a novel method for procedurally building roundabouts. The suggested method can result in roundabout lanes that are not perfectly circular and resemble real-world roundabouts by allowing approaching roadways to be connected to a roundabout at any angle. One can easily incorporate the roundabout in their HD road generation process or use the standalone roundabouts in scenario-based testing of autonomous driving.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#30340;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#24847;&#22270;&#30340;&#38382;&#39064;&#65292;&#22312;Open Intent Induction&#20013;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.13099</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#30340;&#38646;&#26679;&#26412;&#24320;&#25918;&#24847;&#22270;&#24402;&#32435;&#65306;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer. (arXiv:2303.13099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#30340;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#24847;&#22270;&#30340;&#38382;&#39064;&#65292;&#22312;Open Intent Induction&#20013;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#30340;&#24847;&#22270;&#26159;&#23558;&#35813;&#31995;&#32479;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38590;&#39064;&#65306;&#65288;1&#65289;&#29992;&#20110;&#19968;&#33324;&#23884;&#20837;&#30340;SBERT&#65288;2&#65289;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#65288;MDB&#65289;&#29992;&#20110;&#23545;&#35805;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#21450;&#65288;3&#65289;&#29992;&#20110;&#38598;&#32676;&#19987;&#19994;&#35821;&#20041;&#30340;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#65288;PGT&#65289;&#12290; MDB&#19968;&#27425;&#21521;&#27169;&#22411;&#25552;&#20379;&#22810;&#31181;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#39046;&#22495;&#30693;&#35782;&#26469;&#35299;&#20915;&#22810;&#39046;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;PGT&#65292;&#23427;&#37319;&#29992;Siamese&#32593;&#32476;&#30452;&#25509;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#20351;&#29992;PGT&#32858;&#31867;&#23545;&#35805;&#35821;&#21477;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#22810;&#35270;&#35282;&#27169;&#22411;&#19982;MDB&#21644;PGT&#26174;&#30528;&#25552;&#39640;&#20102;Open Intent Induction&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Task Oriented Dialogue (TOD) system, detecting and inducing new intents are two main challenges to apply the system in the real world. In this paper, we suggest the semantic multi-view model to resolve these two challenges: (1) SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized semantic. MDB feeds diverse dialogue datasets to the model at once to tackle the multi-domain problem by learning the multiple domain knowledge. We introduce a novel method PGT, which employs the Siamese network to fine-tune the model with a clustering method directly.Our model can learn how to cluster dialogue utterances by using PGT. Experimental results demonstrate that our multi-view model with MDB and PGT significantly improves the Open Intent Induction performance compared to baseline systems.
&lt;/p&gt;</description></item><item><title>WHOOPS!&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#33509;&#24178;&#20010;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#25361;&#25112;&#20102;AI&#27169;&#22411;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#21512;&#24120;&#35268;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07274</link><description>&lt;p&gt;
&#25171;&#30772;&#24120;&#35782;&#65306;WHOOPS&#65281;&#19968;&#20010;&#22522;&#20110;&#21512;&#25104;&#21644;&#32452;&#21512;&#22270;&#20687;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images. (arXiv:2303.07274v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07274
&lt;/p&gt;
&lt;p&gt;
WHOOPS!&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#33509;&#24178;&#20010;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#25361;&#25112;&#20102;AI&#27169;&#22411;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#21512;&#24120;&#35268;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22855;&#24618;&#12289;&#24322;&#24120;&#21644;&#31070;&#31192;&#30340;&#22270;&#20687;&#20250;&#24341;&#36215;&#35266;&#23519;&#32773;&#30340;&#22909;&#22855;&#24515;&#65292;&#22240;&#20026;&#23427;&#20204;&#25361;&#25112;&#20102;&#24120;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;WHOOPS&#65281;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#35774;&#35745;&#24072;&#20351;&#29992;Midjourney&#31561;&#20844;&#20849;&#21487;&#29992;&#22270;&#20687;&#29983;&#25104;&#24037;&#20855;&#21046;&#20316;&#65292;&#24182;&#21253;&#21547;&#33509;&#24178;&#20010;&#20219;&#21153;&#12290;&#38500;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22256;&#38590;&#30340;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#24182;&#35299;&#37322;&#32473;&#23450;&#22270;&#20687;&#30340;&#24322;&#24120;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weird, unusual, and uncanny images pique the curiosity of observers because they challenge commonsense. For example, an image released during the 2022 world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo playing chess, which playfully violates our expectation that their competition should occur on the football field. Humans can easily recognize and interpret these unconventional images, but can AI models do the same? We introduce WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney. We consider several tasks posed over the dataset. In addition to image captioning, cross-modal matching, and visual question answering, we introduce a difficult explanation generation task, where models must identify and explain why a given image is unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#65306;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21160;&#24577;&#28155;&#21152;&#26032;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TARGET&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#26469;&#20943;&#36731;FCCL&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#20256;&#36882;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#22120;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#27169;&#25311;&#25968;&#25454;&#30340;&#20840;&#23616;&#20998;&#24067;&#12290;&#19982;&#20808;&#21069;&#30340;FCCL&#26041;&#27861;&#30456;&#27604;&#65292;TARGET&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.06937</link><description>&lt;p&gt;
TARGET: &#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#23454;&#29616;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation. (arXiv:2303.06937v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#65306;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21160;&#24577;&#28155;&#21152;&#26032;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TARGET&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#26469;&#20943;&#36731;FCCL&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#20256;&#36882;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#22120;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#27169;&#25311;&#25968;&#25454;&#30340;&#20840;&#23616;&#20998;&#24067;&#12290;&#19982;&#20808;&#21069;&#30340;FCCL&#26041;&#27861;&#30456;&#27604;&#65292;TARGET&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#40092;&#20026;&#20154;&#30693;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65306;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65288;FCCL&#65289;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21160;&#24577;&#28155;&#21152;&#26032;&#30340;&#31867;&#21035;&#12290;&#24050;&#26377;&#30340;FCCL&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#22914;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21152;&#21095;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;TARGET&#65288;&#36890;&#36807;&#26080;&#26679;&#26412;&#33976;&#39311;&#23454;&#29616;&#32852;&#37030;&#31867;&#24335;&#25345;&#32493;&#23398;&#20064;&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#36731;FCCL&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#21516;&#26102;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#23618;&#38754;&#19978;&#23558;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#24403;&#21069;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#29983;&#25104;&#22120;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#27169;&#25311;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#25968;&#25454;&#30340;&#20840;&#23616;&#20998;&#24067;&#12290;&#19982;&#20808;&#21069;&#30340;FCCL&#26041;&#27861;&#30456;&#27604;&#65292;TARGET&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#25110;&#23384;&#20648;&#20808;&#21069;&#20219;&#21153;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on an under-explored yet important problem: Federated Class-Continual Learning (FCCL), where new classes are dynamically added in federated learning. Existing FCCL works suffer from various limitations, such as requiring additional datasets or storing the private data from previous tasks. In response, we first demonstrate that non-IID data exacerbates catastrophic forgetting issue in FL. Then we propose a novel method called TARGET (federat\textbf{T}ed cl\textbf{A}ss-continual lea\textbf{R}nin\textbf{G} via \textbf{E}xemplar-free dis\textbf{T}illation), which alleviates catastrophic forgetting in FCCL while preserving client data privacy. Our proposed method leverages the previously trained global model to transfer knowledge of old tasks to the current task at the model level. Moreover, a generator is trained to produce synthetic data to simulate the global distribution of data on each client at the data level. Compared to previous FCCL methods, TARGET does not requi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#22312;&#32447;&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#36827;&#34892;&#38271;&#26399;&#20219;&#21153;&#30340;&#35268;&#21010;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#31163;&#25955;&#26102;&#38388;&#36816;&#21160;&#22522;&#20803;&#21644;&#36830;&#32493;&#26102;&#38388;&#31649;&#29366;&#20307;&#26469;&#20445;&#35777;&#31995;&#32479;&#29366;&#24577;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01631</link><description>&lt;p&gt;
&#23454;&#26102;&#22522;&#20110;&#31649;&#29366;&#38750;&#39640;&#26031;&#39118;&#38505;&#26377;&#30028;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#29992;&#20110;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Real-Time Tube-Based Non-Gaussian Risk Bounded Motion Planning for Stochastic Nonlinear Systems in Uncertain Environments via Motion Primitives. (arXiv:2303.01631v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#22312;&#32447;&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#36827;&#34892;&#38271;&#26399;&#20219;&#21153;&#30340;&#35268;&#21010;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#31163;&#25955;&#26102;&#38388;&#36816;&#21160;&#22522;&#20803;&#21644;&#36830;&#32493;&#26102;&#38388;&#31649;&#29366;&#20307;&#26469;&#20445;&#35777;&#31995;&#32479;&#29366;&#24577;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#38382;&#39064;&#20013;&#26426;&#22120;&#20154;&#20855;&#26377;&#38543;&#26426;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#19981;&#30830;&#23450;&#30340;&#21021;&#22987;&#20301;&#32622;&#65292;&#29615;&#22659;&#20013;&#21253;&#21547;&#22810;&#20010;&#21160;&#24577;&#19981;&#30830;&#23450;&#38556;&#30861;&#29289;&#12290;&#38556;&#30861;&#29289;&#21487;&#20197;&#20855;&#26377;&#20219;&#24847;&#24418;&#29366;&#65292;&#21487;&#20197;&#21464;&#24418;&#21644;&#31227;&#21160;&#12290;&#25152;&#26377;&#19981;&#30830;&#23450;&#24615;&#19981;&#19968;&#23450;&#26381;&#20174;&#39640;&#26031;&#20998;&#24067;&#12290;&#36825;&#20010;&#36890;&#29992;&#35774;&#32622;&#24050;&#22312;&#25991;&#29486;[1]&#20013;&#34987;&#32771;&#34385;&#24182;&#35299;&#20915;&#12290;&#38500;&#20102;&#19978;&#36848;&#20551;&#35774;&#20043;&#22806;&#65292;&#26412;&#25991;&#36824;&#32771;&#34385;&#20102;&#38271;&#26399;&#20219;&#21153;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25991;&#29486;[1]&#20013;&#30340;&#35268;&#21010;&#26041;&#27861;&#23558;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#31995;&#32479;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#21464;&#24471;&#36807;&#22823;&#12290;&#19982;[1]&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#26102;&#22312;&#32447;&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#12290;&#25105;&#20204;&#31163;&#32447;&#26500;&#24314;&#31163;&#25955;&#26102;&#38388;&#36816;&#21160;&#22522;&#20803;&#21450;&#20854;&#23545;&#24212;&#30340;&#36830;&#32493;&#26102;&#38388;&#31649;&#29366;&#20307;&#65292;&#20197;&#30830;&#20445;&#27599;&#20010;&#36816;&#21160;&#22522;&#20803;&#30340;&#20960;&#20046;&#25152;&#26377;&#31995;&#32479;&#29366;&#24577;&#37117;&#20445;&#25345;&#22312;&#23545;&#24212;&#30340;&#31649;&#29366;&#20307;&#20869;&#12290;&#25105;&#20204;&#23558;&#27010;&#29575;&#23433;&#20840;&#32422;&#26463;&#36716;&#25442;&#20026;&#19968;&#31181;&#26041;&#27861;&#26469;&#31215;&#32047;&#31649;&#29366;&#20307;&#30340;&#27010;&#29575;&#30028;&#30340;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the motion planning problem for stochastic nonlinear systems in uncertain environments. More precisely, in this problem the robot has stochastic nonlinear dynamics and uncertain initial locations, and the environment contains multiple dynamic uncertain obstacles. Obstacles can be of arbitrary shape, can deform, and can move. All uncertainties do not necessarily have Gaussian distribution. This general setting has been considered and solved in [1]. In addition to the assumptions above, in this paper, we consider long-term tasks, where the planning method in [1] would fail, as the uncertainty of the system states grows too large over a long time horizon. Unlike [1], we present a real-time online motion planning algorithm. We build discrete-time motion primitives and their corresponding continuous-time tubes offline, so that almost all system states of each motion primitive are guaranteed to stay inside the corresponding tube. We convert probabilistic safety constraints into a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#38381;&#29615;&#25511;&#21046;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#19981;&#30830;&#23450;&#24615;&#21644;&#24178;&#25200;&#24341;&#36215;&#30340;&#31995;&#32479;&#29366;&#24577;&#20559;&#24046;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#29366;&#24577;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#20219;&#24847;&#24050;&#30693;&#27010;&#29575;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01628</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#39640;&#26031;&#19981;&#30830;&#23450;&#24615;&#26368;&#23567;&#21270;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Non-Gaussian Uncertainty Minimization Based Control of Stochastic Nonlinear Robotic Systems. (arXiv:2303.01628v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#38381;&#29615;&#25511;&#21046;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#19981;&#30830;&#23450;&#24615;&#21644;&#24178;&#25200;&#24341;&#36215;&#30340;&#31995;&#32479;&#29366;&#24577;&#20559;&#24046;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#29366;&#24577;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#20219;&#24847;&#24050;&#30693;&#27010;&#29575;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23384;&#22312;&#27010;&#29575;&#24615;&#19981;&#30830;&#23450;&#24615;&#21644;&#24178;&#25200;&#30340;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#38381;&#29615;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29366;&#24577;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#31995;&#32479;&#29366;&#24577;&#19982;&#21517;&#20041;&#29366;&#24577;&#36712;&#36857;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#26469;&#22788;&#29702;&#22240;&#19981;&#30830;&#23450;&#24615;&#21644;&#24178;&#25200;&#32780;&#24341;&#36215;&#30340;&#20559;&#24046;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#38480;&#20110;&#22788;&#29702;&#29305;&#23450;&#31867;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#31995;&#32479;&#65292;&#22914;&#39640;&#26031;&#19981;&#30830;&#23450;&#24615;&#12289;&#36807;&#31243;&#21644;&#32447;&#24615;&#21270;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#20219;&#24847;&#24050;&#30693;&#27010;&#29575;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25511;&#21046;&#22120;&#35774;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#30340;&#32479;&#35745;&#37327;&#20248;&#21270;&#38382;&#39064;&#65292;&#21253;&#25324;&#30697;&#21644;&#29305;&#24449;&#20989;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#25152;&#25552;&#20379;&#30340;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#30697;&#21644;&#29305;&#24449;&#20989;&#25968;&#26469;&#20256;&#25773;&#19981;&#30830;&#23450;&#24615;&#21040;&#26426;&#22120;&#20154;&#38750;&#32447;&#24615;&#36816;&#21160;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the closed-loop control problem of nonlinear robotic systems in the presence of probabilistic uncertainties and disturbances. More precisely, we design a state feedback controller that minimizes deviations of the states of the system from the nominal state trajectories due to uncertainties and disturbances. Existing approaches to address the control problem of probabilistic systems are limited to particular classes of uncertainties and systems such as Gaussian uncertainties and processes and linearized systems. We present an approach that deals with nonlinear dynamics models and arbitrary known probabilistic uncertainties. We formulate the controller design problem as an optimization problem in terms of statistics of the probability distributions including moments and characteristic functions. In particular, in the provided optimization problem, we use moments and characteristic functions to propagate uncertainties throughout the nonlinear motion model of rob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RemoteNet&#30340;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#20840;&#23616;-&#23616;&#37096;&#20449;&#24687;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#20197;&#21450;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;Transformer&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#21644;&#23398;&#20064;&#65292;&#25913;&#36827;&#20102;&#36965;&#24863;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13084</link><description>&lt;p&gt;
RemoteNet: &#22522;&#20110;&#20840;&#23616;-&#23616;&#37096;&#20449;&#24687;&#30340;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RemoteNet: Remote Sensing Image Segmentation Network based on Global-Local Information. (arXiv:2302.13084v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RemoteNet&#30340;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#20840;&#23616;-&#23616;&#37096;&#20449;&#24687;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#20197;&#21450;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;Transformer&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#21644;&#23398;&#20064;&#65292;&#25913;&#36827;&#20102;&#36965;&#24863;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22797;&#26434;&#30340;&#22330;&#26223;&#65292;&#36828;&#31243;&#25429;&#33719;&#30340;&#22270;&#20687;&#20855;&#26377;&#24040;&#22823;&#30340;&#23610;&#24230;&#21644;&#29289;&#20307;&#22806;&#35266;&#30340;&#21464;&#21270;&#12290;&#23545;&#20110;&#20854;&#20998;&#21106;&#65292;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#23646;&#24615;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#32593;&#32476;&#38590;&#20197;&#25429;&#25417;&#22240;&#26434;&#20081;&#30340;&#32972;&#26223;&#32780;&#20135;&#29983;&#30340;&#20869;&#22312;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#30340;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;RemoteNet&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;Transformer&#21644;&#21367;&#31215;&#26426;&#21046;&#30340;&#20248;&#21183;&#26469;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#12290;RemoteNet&#37319;&#29992;&#32534;&#30721;&#35299;&#30721;&#22120;&#30340;&#35774;&#35745;&#65292;&#20351;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26144;&#23556;&#27169;&#22359;&#26469;&#29983;&#25104;&#36890;&#36947;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#29992;&#20110;&#34701;&#21512;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#35299;&#30721;&#22120;&#32593;&#32476;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#23616;-&#23616;&#37096;Transformer&#22359;&#65288;GLTB&#65289;&#26469;&#25903;&#25345;&#22312;&#35299;&#30721;&#38454;&#27573;&#23398;&#20064;&#40065;&#26834;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#26469;&#20248;&#21270;&#27973;&#23618;&#38454;&#27573;&#32534;&#30721;&#22120;&#30340;&#34701;&#21512;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remotely captured images possess an immense scale and object appearance variability due to the complex scene. It becomes challenging to capture the underlying attributes in the global and local context for their segmentation. Existing networks struggle to capture the inherent features due to the cluttered background. To address these issues, we propose a remote sensing image segmentation network, RemoteNet, for semantic segmentation of remote sensing images. We capture the global and local features by leveraging the benefits of the transformer and convolution mechanisms. RemoteNet is an encoder-decoder design that uses multi-scale features. We construct an attention map module to generate channel-wise attention scores for fusing these features. We construct a global-local transformer block (GLTB) in the decoder network to support learning robust representations during a decoding phase. Further, we designed a feature refinement module to refine the fused output of the shallow stage enco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#37322;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#26102;&#24207;&#24046;&#20998;&#26041;&#27861;&#20013;&#20851;&#38190;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65306;&#20026;&#20160;&#20040;&#30446;&#26631;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#19981;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.12537</link><description>&lt;p&gt;
&#30446;&#26631;&#32593;&#32476;&#22914;&#20309;&#31283;&#23450;&#26102;&#38388;&#24046;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Why Target Networks Stabilise Temporal Difference Methods. (arXiv:2302.12537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#37322;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#26102;&#24207;&#24046;&#20998;&#26041;&#27861;&#20013;&#20851;&#38190;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65306;&#20026;&#20160;&#20040;&#30446;&#26631;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#19981;&#28385;&#36275;&#26465;&#20214;&#26102;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36817;&#26399;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#19968;&#31867;&#20351;&#29992;&#19981;&#39057;&#32321;&#26356;&#26032;&#30446;&#26631;&#20540;&#36827;&#34892;&#31574;&#30053;&#35780;&#20272;&#30340;&#26102;&#24207;&#24046;&#20998;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26377;&#20851;&#30446;&#26631;&#32593;&#32476;&#26377;&#25928;&#24615;&#30340;&#23436;&#25972;&#29702;&#35770;&#35299;&#37322;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#31181;&#27969;&#34892;&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#26368;&#32456;&#22238;&#31572;&#20102;&#8220;&#20026;&#20160;&#20040;&#30446;&#26631;&#32593;&#32476;&#21487;&#20197;&#31283;&#23450;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#8221;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#37096;&#20998;&#25311;&#21512;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#21253;&#25324;&#30446;&#26631;&#32593;&#32476;&#30340;&#20351;&#29992;&#65292;&#24182;&#19988;&#22635;&#34917;&#20102;&#25311;&#21512;&#26041;&#27861;&#21644;&#21322;&#26799;&#24230;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#33021;&#22815;&#29420;&#29305;&#22320;&#25551;&#36848;&#25152;&#35859;&#30340;&#33268;&#21629;&#19977;&#20803;&#32452;&#65292;&#21363;&#20351;&#29992;&#26102;&#24207;&#24046;&#20998;&#26356;&#26032;&#65292;&#32467;&#21512;&#65288;&#38750;&#32447;&#24615;&#65289;&#20989;&#25968;&#36924;&#36817;&#21644;&#22788;&#20110;&#31163;&#32447;&#29366;&#24577;&#30340;&#25968;&#25454;&#65292;&#36825;&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#25910;&#25947;&#30340;&#31639;&#27861;&#12290;&#36825;&#19968;&#35748;&#35782;&#20351;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#30446;&#26631;&#32593;&#32476;&#30340;&#20351;&#29992;&#21487;&#20197;&#20943;&#36731;&#26465;&#20214;&#24046;&#26102;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integral to recent successes in deep reinforcement learning has been a class of temporal difference methods that use infrequently updated target values for policy evaluation in a Markov Decision Process. Yet a complete theoretical explanation for the effectiveness of target networks remains elusive. In this work, we provide an analysis of this popular class of algorithms, to finally answer the question: `why do target networks stabilise TD learning'? To do so, we formalise the notion of a partially fitted policy evaluation method, which describes the use of target networks and bridges the gap between fitted methods and semigradient temporal difference algorithms. Using this framework we are able to uniquely characterise the so-called deadly triad - the use of TD updates with (nonlinear) function approximation and off-policy data - which often leads to nonconvergent algorithms. This insight leads us to conclude that the use of target networks can mitigate the effects of poor conditionin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20123;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35201;&#27714;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#25165;&#33021;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;P $\neq$ NP&#12290;</title><link>http://arxiv.org/abs/2302.09512</link><description>&lt;p&gt;
SAT&#38656;&#35201;&#24443;&#24213;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
SAT Requires Exhaustive Search. (arXiv:2302.09512v4 [cs.CC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20123;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35201;&#27714;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#25165;&#33021;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;P $\neq$ NP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;&#20855;&#26377;&#22823;&#22495;&#21644;&#38271;&#23376;&#21477;&#30340;CSP&#21644;SAT&#30340;&#26497;&#38590;&#20363;&#23376;&#65292;&#35777;&#26126;&#36825;&#20123;&#20363;&#23376;&#26080;&#27861;&#22312;&#19981;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20010;&#36739;&#24369;&#30340;&#32467;&#35770;P $\neq$ NP&#12290;&#26412;&#25991;&#37319;&#29992;&#30340;&#26159;&#19968;&#31181;&#35777;&#26126;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#30340;&#24314;&#35774;&#24615;&#26041;&#27861;&#65292;&#19982;&#30446;&#21069;&#35745;&#31639;&#22797;&#26434;&#24615;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#26041;&#27861;&#38750;&#24120;&#19981;&#21516;&#65292;&#20294;&#19982;Kurt G\"{o}del&#22312;&#35777;&#26126;&#20182;&#33879;&#21517;&#30340;&#36923;&#36753;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#26102;&#20351;&#29992;&#30340;&#26041;&#27861;&#30456;&#20284;&#12290;&#27491;&#22914;G\"{o}del&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25968;&#23398;&#20013;&#35777;&#26126;&#24418;&#24335;&#19978;&#30340;&#19981;&#21487;&#35777;&#26126;&#24615;&#26159;&#21487;&#34892;&#30340;&#19968;&#26679;&#65292;&#26412;&#25991;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25968;&#23398;&#20013;&#35777;&#26126;&#35745;&#31639;&#19978;&#30340;&#38590;&#24230;&#19981;&#26159;&#24456;&#38590;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#35768;&#22810;&#38382;&#39064;&#65292;&#22914;3-SAT&#65292;&#35777;&#26126;&#19979;&#30028;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#26377;&#21508;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#21487;&#29992;&#20110;&#36991;&#20813;&#36827;&#34892;&#24443;&#24213;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#26497;&#38590;&#30340;&#20363;&#23376;&#20013;&#65292;&#24443;&#24213;&#25628;&#32034;&#21487;&#33021;&#26159;&#21807;&#19968;&#21487;&#34892;&#30340;&#36873;&#25321;&#65292;&#35777;&#26126;&#20854;&#24517;&#35201;&#24615;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, by constructing extremely hard examples of CSP (with large domains) and SAT (with long clauses), we prove that such examples cannot be solved without exhaustive search, which implies a weaker conclusion P $\neq$ NP. This constructive approach for proving impossibility results is very different (and missing) from those currently used in computational complexity theory, but is similar to that used by Kurt G\"{o}del in proving his famous logical impossibility results. Just as shown by G\"{o}del's results that proving formal unprovability is feasible in mathematics, the results of this paper show that proving computational hardness is not hard in mathematics. Specifically, proving lower bounds for many problems, such as 3-SAT, can be challenging because these problems have various effective strategies available for avoiding exhaustive search. However, in cases of extremely hard examples, exhaustive search may be the only viable option, and proving its necessity becomes more 
&lt;/p&gt;</description></item><item><title>LabelPrompt&#26159;&#19968;&#31181;&#38754;&#21521;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#39069;&#22806;&#30340;&#20196;&#29260;&#26469;&#34920;&#31034;&#20851;&#31995;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#25552;&#31034;&#27169;&#26495;&#26041;&#27861;&#26126;&#30830;&#26500;&#24314;&#23427;&#20204;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23558;&#22635;&#20805;&#25513;&#30721;&#26631;&#35760;&#30340;&#33258;&#28982;&#35821;&#35328;&#35789;&#27719;&#19982;&#35821;&#20041;&#20851;&#31995;&#26631;&#31614;&#30456;&#20851;&#32852;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#20307;&#24863;&#30693;&#27169;&#22359;&#26469;&#20943;&#36731;&#39044;&#27979;&#20851;&#31995;&#21644;&#32473;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.08068</link><description>&lt;p&gt;
LabelPrompt: &#20851;&#20110;&#20851;&#31995;&#20998;&#31867;&#30340;&#26377;&#25928;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LabelPrompt: Effective Prompt-based Learning for Relation Classification. (arXiv:2302.08068v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08068
&lt;/p&gt;
&lt;p&gt;
LabelPrompt&#26159;&#19968;&#31181;&#38754;&#21521;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#39069;&#22806;&#30340;&#20196;&#29260;&#26469;&#34920;&#31034;&#20851;&#31995;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#25552;&#31034;&#27169;&#26495;&#26041;&#27861;&#26126;&#30830;&#26500;&#24314;&#23427;&#20204;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23558;&#22635;&#20805;&#25513;&#30721;&#26631;&#35760;&#30340;&#33258;&#28982;&#35821;&#35328;&#35789;&#27719;&#19982;&#35821;&#20041;&#20851;&#31995;&#26631;&#31614;&#30456;&#20851;&#32852;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#20307;&#24863;&#30693;&#27169;&#22359;&#26469;&#20943;&#36731;&#39044;&#27979;&#20851;&#31995;&#21644;&#32473;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#36716;&#25442;&#20026;&#22635;&#31354;&#24335;&#26684;&#24335;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#40784;&#30340;&#26041;&#24335;&#65292;&#25552;&#31034;&#24335;&#23398;&#20064;&#22312;&#35768;&#22810;NLP&#20219;&#21153;&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23558;&#22635;&#20805;&#25513;&#30721;&#26631;&#35760;&#30340;&#33258;&#28982;&#35821;&#35328;&#35789;&#27719;&#19982;&#35821;&#20041;&#20851;&#31995;&#26631;&#31614;&#65288;&#22914;"org:founded_by"&#65289;&#30456;&#20851;&#32852;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;LabelPrompt&#65292;&#29992;&#20110;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#12290;&#21463;&#21040;&#8220;&#32473;&#20104;&#27169;&#22411;&#36873;&#25321;&#8221;&#30340;&#30452;&#35273;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#39069;&#22806;&#30340;&#20196;&#29260;&#26469;&#34920;&#31034;&#20851;&#31995;&#26631;&#31614;&#65292;&#23558;&#36825;&#20123;&#20196;&#29260;&#35270;&#20026;&#20855;&#26377;&#35821;&#20041;&#21021;&#22987;&#21270;&#30340;&#21475;&#36848;&#32773;&#65292;&#24182;&#20351;&#29992;&#25552;&#31034;&#27169;&#26495;&#26041;&#27861;&#26126;&#30830;&#26500;&#24314;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20943;&#36731;&#39044;&#27979;&#20851;&#31995;&#21644;&#32473;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#20307;&#24863;&#30693;&#27169;&#22359;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#24615;&#30340;&#26041;&#24335;&#36827;&#34892;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, prompt-based learning has gained popularity across many natural language processing (NLP) tasks by reformulating them into a cloze-style format to better align pre-trained language models (PLMs) with downstream tasks. However, applying this approach to relation classification poses unique challenges. Specifically, associating natural language words that fill the masked token with semantic relation labels (\textit{e.g.} \textit{``org:founded\_by}'') is difficult. To address this challenge, this paper presents a novel prompt-based learning method, namely LabelPrompt, for the relation classification task. Motivated by the intuition to ``GIVE MODEL CHOICES!'', we first define additional tokens to represent relation labels, which regard these tokens as the verbaliser with semantic initialisation and explicitly construct them with a prompt template method. Then, to mitigate inconsistency between predicted relations and given entities, we implement an entity-aware module with contra
&lt;/p&gt;</description></item><item><title>HumanMAC&#26159;&#19968;&#20010;&#25513;&#30721;&#21160;&#20316;&#20462;&#22797;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#38454;&#27573;&#30340;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#21644;&#25512;&#26029;&#38454;&#27573;&#30340;&#21435;&#22122;&#36807;&#31243;&#65292;&#22312;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#25968;&#25454;&#30340;&#25511;&#21046;&#19979;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.03665</link><description>&lt;p&gt;
HumanMAC&#65306;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#30340;&#25513;&#30721;&#21160;&#20316;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
HumanMAC: Masked Motion Completion for Human Motion Prediction. (arXiv:2302.03665v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03665
&lt;/p&gt;
&lt;p&gt;
HumanMAC&#26159;&#19968;&#20010;&#25513;&#30721;&#21160;&#20316;&#20462;&#22797;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#38454;&#27573;&#30340;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#21644;&#25512;&#26029;&#38454;&#27573;&#30340;&#21435;&#22122;&#36807;&#31243;&#65292;&#22312;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#25968;&#25454;&#30340;&#25511;&#21046;&#19979;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#32463;&#20856;&#38382;&#39064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20197;&#32534;&#30721;-&#35299;&#30721;&#39118;&#26684;&#20026;&#22522;&#30784;&#30340;&#20808;&#21069;&#26041;&#27861;&#22312;&#32463;&#39564;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23454;&#38469;&#19978;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#22797;&#26434;&#30340;&#25439;&#22833;&#32422;&#26463;&#12289;&#32321;&#29712;&#30340;&#22521;&#35757;&#36807;&#31243;&#20197;&#21450;&#39044;&#27979;&#20013;&#19981;&#21516;&#31867;&#21035;&#36816;&#21160;&#30340;&#31232;&#32570;&#20999;&#25442;&#12290;&#26412;&#25991;&#20174;&#26032;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#25513;&#34109;&#23436;&#25104;&#26041;&#24335;&#35299;&#20915;&#20102;&#20197;&#19978;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#26469;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#29983;&#25104;&#36816;&#21160;&#12290;&#22312;&#25512;&#26029;&#38454;&#27573;&#65292;&#36890;&#36807;&#21435;&#22122;&#36807;&#31243;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36816;&#21160;&#39044;&#27979;&#24182;&#22312;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#25968;&#25454;&#30340;&#25511;&#21046;&#19979;&#36827;&#34892;&#20102;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21517;&#20026;HumanMAC&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human motion prediction is a classical problem in computer vision and computer graphics, which has a wide range of practical applications. Previous effects achieve great empirical performance based on an encoding-decoding style. The methods of this style work by first encoding previous motions to latent representations and then decoding the latent representations into predicted motions. However, in practice, they are still unsatisfactory due to several issues, including complicated loss constraints, cumbersome training processes, and scarce switch of different categories of motions in prediction. In this paper, to address the above issues, we jump out of the foregoing style and propose a novel framework from a new perspective. Specifically, our framework works in a masked completion fashion. In the training stage, we learn a motion diffusion model that generates motions from random noise. In the inference stage, with a denoising procedure, we make motion prediction conditioning on obse
&lt;/p&gt;</description></item><item><title>&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#26159;&#19968;&#20010;&#35299;&#20915;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#20010;&#24615;&#21270;&#24433;&#21709;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#23545;&#20219;&#21153;&#30340;&#22788;&#29702;&#36798;&#25104;&#20102;&#30446;&#30340;&#65292;&#33021;&#22815;&#23558;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#20013;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12292</link><description>&lt;p&gt;
&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot causal learning. (arXiv:2301.12292v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12292
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#26159;&#19968;&#20010;&#35299;&#20915;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#20010;&#24615;&#21270;&#24433;&#21709;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#23545;&#20219;&#21153;&#30340;&#22788;&#29702;&#36798;&#25104;&#20102;&#30446;&#30340;&#65292;&#33021;&#22815;&#23558;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#20013;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#21307;&#30103;&#12289;&#20844;&#20849;&#25919;&#31574;&#21644;&#22312;&#32447;&#33829;&#38144;&#31561;&#39046;&#22495;&#65292;&#39044;&#27979;&#19981;&#21516;&#24178;&#39044;&#25514;&#26045;&#23545;&#29305;&#23450;&#20010;&#20307;&#30340;&#22240;&#26524;&#24433;&#21709;&#38750;&#24120;&#37325;&#35201;&#12290;&#39044;&#27979;&#29616;&#26377;&#24178;&#39044;&#25514;&#26045;&#30340;&#24433;&#21709;&#26377;&#35768;&#22810;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#25509;&#21463;&#36807;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#20307;&#30340;&#21382;&#21490;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#65292;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#30340;&#24433;&#21709;&#20063;&#24456;&#37325;&#35201;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#65306;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#24615;&#21270;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CaML&#65292;&#36825;&#26159;&#19968;&#20010;&#22240;&#26524;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#23558;&#27599;&#20010;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#24615;&#21270;&#39044;&#27979;&#25928;&#26524;&#20316;&#20026;&#19968;&#20010;&#20219;&#21153;&#26469;&#36827;&#34892;&#22788;&#29702;&#12290;CaML&#22312;&#25968;&#21315;&#20010;&#20219;&#21153;&#20013;&#35757;&#32451;&#21333;&#19968;&#30340;&#20803;&#27169;&#22411;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26159;&#36890;&#36807;&#25277;&#26679;&#29983;&#25104;&#19968;&#20010;&#24178;&#39044;&#25514;&#26045;&#21450;&#20854;&#25509;&#25910;&#32773;&#21644;&#38750;&#25509;&#25910;&#32773;&#26469;&#26500;&#24314;&#30340;&#12290;&#36890;&#36807;&#21033;&#29992;&#24178;&#39044;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#30340;&#23646;&#24615;&#65289;&#21644;&#20010;&#20307;&#29305;&#24449;&#65288;&#20363;&#22914;&#65292;&#29305;&#23450;&#20010;&#20307;&#30340;&#21307;&#30103;&#35760;&#24405;&#65289;&#65292;CaML&#23398;&#20064;&#22914;&#20309;&#23558;&#24050;&#35266;&#23519;&#21040;&#30340;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#26377;&#25928;&#22320;&#20256;&#36755;&#32473;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#24178;&#39044;&#25514;&#26045;&#24182;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting how different interventions will causally affect a specific individual is important in a variety of domains such as personalized medicine, public policy, and online marketing. There are a large number of methods to predict the effect of an existing intervention based on historical data from individuals who received it. However, in many settings it is important to predict the effects of novel interventions (\emph{e.g.}, a newly invented drug), which these methods do not address. Here, we consider zero-shot causal learning: predicting the personalized effects of a novel intervention. We propose CaML, a causal meta-learning framework which formulates the personalized prediction of each intervention's effect as a task. CaML trains a single meta-model across thousands of tasks, each constructed by sampling an intervention, along with its recipients and nonrecipients. By leveraging both intervention information (\emph{e.g.}, a drug's attributes) and individual features~(\emph{e.g.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#39318;&#27425;&#20351;&#29992;Transformer&#36827;&#34892;&#23460;&#22806;&#23450;&#20301;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20999;&#29255;Transformer&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#36724;&#21521;&#29305;&#24615;&#37325;&#26032;&#32452;&#32455;&#20102;&#28608;&#20809;&#38647;&#36798;&#25195;&#25551;&#30340;&#20999;&#29255;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#24341;&#20837;&#20102;Perth-WA&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.08957</link><description>&lt;p&gt;
&#20999;&#29255;Transformer&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;3D&#28857;&#20113;&#22320;&#22270;&#20013;&#30340;&#20845;&#33258;&#30001;&#24230;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps. (arXiv:2301.08957v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#39318;&#27425;&#20351;&#29992;Transformer&#36827;&#34892;&#23460;&#22806;&#23450;&#20301;&#30340;&#20219;&#21153;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20999;&#29255;Transformer&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#36724;&#21521;&#29305;&#24615;&#37325;&#26032;&#32452;&#32455;&#20102;&#28608;&#20809;&#38647;&#36798;&#25195;&#25551;&#30340;&#20999;&#29255;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#24341;&#20837;&#20102;Perth-WA&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#23450;&#20301;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#39318;&#27425;&#37319;&#29992;&#20102;Transformer&#36827;&#34892;&#20351;&#29992;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#23460;&#22806;&#23450;&#20301;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#37325;&#26032;&#32452;&#32455;&#20102;$360^\circ$&#28608;&#20809;&#38647;&#36798;&#25195;&#25551;&#30340;&#20999;&#29255;&#65292;&#21033;&#29992;&#20854;&#36724;&#21521;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#31216;&#20026;&#20999;&#29255;Transformer&#65292;&#22312;&#22788;&#29702;&#20999;&#29255;&#26102;&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#36827;&#34892;&#23460;&#22806;&#28857;&#20113;&#22788;&#29702;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Perth-WA&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#35199;&#28595;&#22823;&#21033;&#20122;&#29632;&#26031;&#24066;&#30340;&#22823;&#35268;&#27169;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#65292;&#28085;&#30422;&#20102;&#32422;4&#24179;&#26041;&#20844;&#37324;&#30340;&#21306;&#22495;&#12290;Perth-WA&#25552;&#20379;&#20102;&#23450;&#20301;&#26631;&#27880;&#12290;&#25105;&#20204;&#22312;Perth-WA&#21644;Appollo-SouthBay&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#23450;&#20301;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#24120;&#35265;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;ModelNet40&#21644;...&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise localization is critical for autonomous vehicles. We present a self-supervised learning method that employs Transformers for the first time for the task of outdoor localization using LiDAR data. We propose a pre-text task that reorganizes the slices of a $360^\circ$ LiDAR scan to leverage its axial properties. Our model, called Slice Transformer, employs multi-head attention while systematically processing the slices. To the best of our knowledge, this is the first instance of leveraging multi-head attention for outdoor point clouds. We additionally introduce the Perth-WA dataset, which provides a large-scale LiDAR map of Perth city in Western Australia, covering $\sim$4km$^2$ area. Localization annotations are provided for Perth-WA. The proposed localization method is thoroughly evaluated on Perth-WA and Appollo-SouthBay datasets. We also establish the efficacy of our self-supervised learning approach for the common downstream task of object classification using ModelNet40 and
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#23427;&#24378;&#35843;&#31995;&#32479;&#24615;&#22320;&#35774;&#35745;&#21644;&#26500;&#24314;&#25968;&#25454;&#23545;&#20110;&#24314;&#31435;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2212.11854</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Data-centric Artificial Intelligence. (arXiv:2212.11854v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11854
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#23427;&#24378;&#35843;&#31995;&#32479;&#24615;&#22320;&#35774;&#35745;&#21644;&#26500;&#24314;&#25968;&#25454;&#23545;&#20110;&#24314;&#31435;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;data-centric AI&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#24378;&#35843;&#31995;&#32479;&#24615;&#22320;&#35774;&#35745;&#21644;&#26500;&#24314;&#25968;&#25454;&#23545;&#20110;&#24314;&#31435;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#21521;&#20449;&#24687;&#31995;&#32479;&#39046;&#22495;&#30340;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#20171;&#32461;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#12290;&#25105;&#20204;&#23450;&#20041;&#30456;&#20851;&#26415;&#35821;&#65292;&#25552;&#20379;&#20851;&#38190;&#29305;&#24449;&#26469;&#23545;&#27604;&#25968;&#25454;&#20013;&#24515;&#33539;&#24335;&#21644;&#27169;&#22411;&#20013;&#24515;&#33539;&#24335;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21306;&#20998;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#21644;&#30456;&#20851;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20854;&#23545;&#20449;&#24687;&#31995;&#32479;&#31038;&#21306;&#30340;&#38271;&#26399;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-centric artificial intelligence (data-centric AI) represents an emerging paradigm emphasizing that the systematic design and engineering of data is essential for building effective and efficient AI-based systems. The objective of this article is to introduce practitioners and researchers from the field of Information Systems (IS) to data-centric AI. We define relevant terms, provide key characteristics to contrast the data-centric paradigm to the model-centric one, and introduce a framework for data-centric AI. We distinguish data-centric AI from related concepts and discuss its longer-term implications for the IS community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#35299;&#37322;&#22270;&#20687; DNN &#30340;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#23646;&#24615;&#65306;&#36890;&#36807;&#24494;&#23567;&#35270;&#35273;&#26356;&#25913;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#35299;&#37322;&#21487;&#20197;&#36890;&#36807;&#36827;&#21270;&#31574;&#30053;&#20219;&#24847;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; AttaXAI&#65292;&#19968;&#20010;&#38024;&#23545; XAI &#31639;&#27861;&#30340;&#25932;&#23545;&#25915;&#20987;&#65292;&#21487;&#35775;&#38382;&#20998;&#31867;&#22120;&#36755;&#20986;&#20449;&#24687;&#21644;&#35299;&#37322;&#22270;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.14860</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35299;&#37322;&#26041;&#27861;&#30772;&#35299;
&lt;/p&gt;
&lt;p&gt;
Foiling Explanations in Deep Neural Networks. (arXiv:2211.14860v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#35299;&#37322;&#22270;&#20687; DNN &#30340;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#23646;&#24615;&#65306;&#36890;&#36807;&#24494;&#23567;&#35270;&#35273;&#26356;&#25913;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#35299;&#37322;&#21487;&#20197;&#36890;&#36807;&#36827;&#21270;&#31574;&#30053;&#20219;&#24847;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; AttaXAI&#65292;&#19968;&#20010;&#38024;&#23545; XAI &#31639;&#27861;&#30340;&#25932;&#23545;&#25915;&#20987;&#65292;&#21487;&#35775;&#38382;&#20998;&#31867;&#22120;&#36755;&#20986;&#20449;&#24687;&#21644;&#35299;&#37322;&#22270;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#23545;&#20110;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#20165;&#20165;&#33719;&#24471;&#20854;&#36755;&#20986;&#26159;&#19981;&#22815;&#26377;&#29992;&#30340;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102;&#22270;&#20687; DNN &#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#23646;&#24615;&#65306;&#36890;&#36807;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#24494;&#23567;&#30340;&#35270;&#35273;&#26356;&#25913;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36827;&#21270;&#31574;&#30053;&#20219;&#24847;&#25805;&#32437;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861; AttaXAI&#65292;&#19968;&#20010;&#38024;&#23545; XAI &#31639;&#27861;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#25932;&#23545;&#25915;&#20987;&#65292;&#21482;&#38656;&#35201;&#35775;&#38382;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#20449;&#24687;&#21644;&#35299;&#37322;&#22270;&#65292;&#36825;&#20123;&#24369;&#20551;&#35774;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have greatly impacted numerous fields over the past decade. Yet despite exhibiting superb performance over many problems, their black-box nature still poses a significant challenge with respect to explainability. Indeed, explainable artificial intelligence (XAI) is crucial in several fields, wherein the answer alone -- sans a reasoning of how said answer was derived -- is of little value. This paper uncovers a troubling property of explanation methods for image-based DNNs: by making small visual changes to the input image -- hardly influencing the network's output -- we demonstrate how explanations may be arbitrarily manipulated through the use of evolution strategies. Our novel algorithm, AttaXAI, a model-agnostic, adversarial attack on XAI algorithms, only requires access to the output logits of a classifier and to the explanation map; these weak assumptions render our approach highly useful where real-world models and data are concerned. We compare our me
&lt;/p&gt;</description></item><item><title>StyleNAT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#22270;&#20687;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#26469;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#33021;&#22815;&#39640;&#25928;&#28789;&#27963;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;FFHQ-256&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.05770</link><description>&lt;p&gt;
StyleNAT&#65306;&#32473;&#27599;&#20010;&#22836;&#37096;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
StyleNAT: Giving Each Head a New Perspective. (arXiv:2211.05770v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05770
&lt;/p&gt;
&lt;p&gt;
StyleNAT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#22270;&#20687;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#26469;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#33021;&#22815;&#39640;&#25928;&#28789;&#27963;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;FFHQ-256&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#19968;&#30452;&#26159;&#19968;&#20010;&#26082;&#26399;&#26395;&#21448;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#25191;&#34892;&#29983;&#25104;&#20219;&#21153;&#21516;&#26679;&#22256;&#38590;&#12290;&#36890;&#24120;&#65292;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#21019;&#24314;&#19968;&#20010;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#29983;&#25104;&#22120;&#65292;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#65292;&#21363;&#20351;&#26159;&#25130;&#28982;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#26377;&#24456;&#23569;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;StyleNAT&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#20855;&#26377;&#21331;&#36234;&#30340;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#26680;&#24515;&#26159;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#27880;&#24847;&#21147;&#22836;&#37096;&#21010;&#20998;&#20026;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#23454;&#29616;&#30340;&#12290;&#30001;&#20110;&#19981;&#21516;&#30340;&#22836;&#37096;&#33021;&#22815;&#20851;&#27880;&#19981;&#21516;&#30340;&#24863;&#21463;&#37326;&#65292;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#32467;&#21512;&#36825;&#20123;&#20449;&#24687;&#65292;&#24182;&#20197;&#39640;&#24230;&#28789;&#27963;&#30340;&#26041;&#24335;&#36866;&#24212;&#25163;&#22836;&#30340;&#25968;&#25454;&#12290;StyleNAT&#22312;FFHQ-256&#19978;&#33719;&#24471;&#20102;&#26032;&#30340;SOTA FID&#24471;&#20998;2.046 &#65292;&#20987;&#36133;&#20102;&#20197;&#21367;&#31215;&#27169;&#22411;&#65288;&#22914;StyleGAN-XL&#65289;&#21644;transformer&#27169;&#22411;&#65288;&#22914;HIT&#65289;&#20026;&#22522;&#30784;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image generation has been a long sought-after but challenging task, and performing the generation task in an efficient manner is similarly difficult. Often researchers attempt to create a "one size fits all" generator, where there are few differences in the parameter space for drastically different datasets. Herein, we present a new transformer-based framework, dubbed StyleNAT, targeting high-quality image generation with superior efficiency and flexibility. At the core of our model, is a carefully designed framework that partitions attention heads to capture local and global information, which is achieved through using Neighborhood Attention (NA). With different heads able to pay attention to varying receptive fields, the model is able to better combine this information, and adapt, in a highly flexible manner, to the data at hand. StyleNAT attains a new SOTA FID score on FFHQ-256 with 2.046, beating prior arts with convolutional models such as StyleGAN-XL and transformers such as HIT 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#20016;&#23500;&#22806;&#37096;&#20449;&#24687;&#30340;&#21407;&#21017;&#24615;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#22122;&#22768;&#26159;&#22797;&#26434;&#19988;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#36807;&#31243;&#26102;&#65292;&#29616;&#26377;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#21487;&#33021;&#26080;&#27861;&#25104;&#21151;&#24212;&#29992;&#20110;&#36825;&#31867;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2211.00164</link><description>&lt;p&gt;
&#20195;&#29702;-&#25511;&#21046;&#22120;&#34920;&#31034;&#65306;&#20855;&#26377;&#20016;&#23500;&#22806;&#37096;&#20449;&#24687;&#30340;&#21407;&#21017;&#24615;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Agent-Controller Representations: Principled Offline RL with Rich Exogenous Information. (arXiv:2211.00164v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#20016;&#23500;&#22806;&#37096;&#20449;&#24687;&#30340;&#21407;&#21017;&#24615;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#22122;&#22768;&#26159;&#22797;&#26434;&#19988;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#36807;&#31243;&#26102;&#65292;&#29616;&#26377;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#21487;&#33021;&#26080;&#27861;&#25104;&#21151;&#24212;&#29992;&#20110;&#36825;&#31867;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20016;&#23500;&#30340;&#20687;&#32032;&#35270;&#35273;&#35266;&#27979;&#31354;&#38388;&#20013;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#25511;&#21046;&#20195;&#29702;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#35774;&#32622;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#36755;&#20837;&#20449;&#24687;&#20013;&#23384;&#22312;&#38590;&#20197;&#24314;&#27169;&#21644;&#25511;&#21046;&#20195;&#29702;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#29702;&#35770;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#24050;&#32463;&#36890;&#36807;&#22806;&#37096;&#20449;&#24687;&#30340;&#35266;&#28857;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#35266;&#27979;&#20013;&#21253;&#21547;&#30340;&#19982;&#25511;&#21046;&#26080;&#20851;&#30340;&#20449;&#24687;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#22312;&#32321;&#24537;&#34903;&#36947;&#19978;&#23548;&#33322;&#30340;&#26426;&#22120;&#20154;&#38656;&#35201;&#24573;&#30053;&#19982;&#25511;&#21046;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#22914;&#32972;&#26223;&#20013;&#30340;&#20854;&#20182;&#20154;&#34892;&#36208;&#12289;&#29289;&#20307;&#30340;&#32441;&#29702;&#25110;&#22825;&#31354;&#20013;&#30340;&#40479;&#31867;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#35270;&#35273;&#32454;&#33410;&#30340;&#22806;&#37096;&#20449;&#24687;&#30340;&#35774;&#32622;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#20197;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#22122;&#22768;&#26159;&#22797;&#26434;&#19988;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#36807;&#31243;&#26102;&#65292;&#24403;&#20195;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#21487;&#33021;&#22312;&#25968;&#25454;&#38598;&#19978;&#22833;&#36133;&#65292;&#32780;&#36825;&#31181;&#22122;&#22768;&#22312;&#23454;&#38469;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to control an agent from data collected offline in a rich pixel-based visual observation space is vital for real-world applications of reinforcement learning (RL). A major challenge in this setting is the presence of input information that is hard to model and irrelevant to controlling the agent. This problem has been approached by the theoretical RL community through the lens of exogenous information, i.e, any control-irrelevant information contained in observations. For example, a robot navigating in busy streets needs to ignore irrelevant information, such as other people walking in the background, textures of objects, or birds in the sky. In this paper, we focus on the setting with visually detailed exogenous information, and introduce new offline RL benchmarks offering the ability to study this problem. We find that contemporary representation learning techniques can fail on datasets where the noise is a complex and time dependent process, which is prevalent in practical 
&lt;/p&gt;</description></item><item><title>JAX-DIPS&#26159;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#31163;&#25955;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#20280;&#32553;&#31574;&#30053;&#65292;&#29992;&#20110;&#24320;&#21457;&#26080;&#32593;&#26684;&#28151;&#21512;&#31070;&#32463;&#31526;&#21495;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;&#20855;&#26377;&#38388;&#26029;&#30340;&#26925;&#22278;&#38382;&#39064;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.14312</link><description>&lt;p&gt;
JAX-DIPS&#65306;&#26377;&#38480;&#31163;&#25955;&#26041;&#27861;&#30340;&#31070;&#32463;&#24341;&#23548;&#27861;&#21450;&#20854;&#22312;&#20855;&#26377;&#38388;&#26029;&#30340;&#26925;&#22278;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
JAX-DIPS: Neural bootstrapping of finite discretization methods and application to elliptic problems with discontinuities. (arXiv:2210.14312v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14312
&lt;/p&gt;
&lt;p&gt;
JAX-DIPS&#26159;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#31163;&#25955;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#20280;&#32553;&#31574;&#30053;&#65292;&#29992;&#20110;&#24320;&#21457;&#26080;&#32593;&#26684;&#28151;&#21512;&#31070;&#32463;&#31526;&#21495;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;&#20855;&#26377;&#38388;&#26029;&#30340;&#26925;&#22278;&#38382;&#39064;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22522;&#20110;&#29616;&#26377;&#22522;&#20110;&#32593;&#26684;&#30340;&#25968;&#20540;&#31163;&#25955;&#26041;&#27861;&#24320;&#21457;&#26080;&#32593;&#26684;&#28151;&#21512;&#31070;&#32463;&#31526;&#21495;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;&#65288;i&#65289;&#21033;&#29992;&#20808;&#36827;&#25968;&#20540;&#26041;&#27861;&#12289;&#27714;&#35299;&#22120;&#21644;&#39044;&#22788;&#29702;&#22120;&#30340;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#24615;&#20197;&#21450;&#65288;ii&#65289;&#23558;&#20248;&#21270;&#38480;&#21046;&#22312;&#19968;&#38454;&#33258;&#21160;&#24494;&#20998;&#19978;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#35757;&#32451;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#31070;&#32463;&#24341;&#23548;&#26041;&#27861;&#65288;&#20197;&#19979;&#31616;&#31216;NBM&#65289;&#22522;&#20110;&#30456;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#38543;&#26426;&#21462;&#26679;&#28857;&#19978;&#38544;&#24335;&#31515;&#21345;&#23572;&#21333;&#20803;&#26684;&#19978;&#33719;&#24471;&#30340;PDE&#31995;&#32479;&#30340;&#26377;&#38480;&#31163;&#25955;&#27531;&#24046;&#30340;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24341;&#23548;&#26377;&#38480;&#31163;&#25955;&#26041;&#31243;&#20013;&#23384;&#22312;&#30340;&#23432;&#24658;&#23450;&#24459;&#21644;&#23545;&#31216;&#24615;&#33021;&#22815;&#21521;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#26377;&#20851;&#35299;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a scalable strategy for development of mesh-free hybrid neuro-symbolic partial differential equation solvers based on existing mesh-based numerical discretization methods. Particularly, this strategy can be used to efficiently train neural network surrogate models of partial differential equations by (i) leveraging the accuracy and convergence properties of advanced numerical methods, solvers, and preconditioners, as well as (ii) better scalability to higher order PDEs by strictly limiting optimization to first order automatic differentiation. The presented neural bootstrapping method (hereby dubbed NBM) is based on evaluation of the finite discretization residuals of the PDE system obtained on implicit Cartesian cells centered on a set of random collocation points with respect to trainable parameters of the neural network. Importantly, the conservation laws and symmetries present in the bootstrapped finite discretization equations inform the neural network about solution re
&lt;/p&gt;</description></item><item><title>NECE&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25925;&#20107;&#20107;&#20214;&#38142;&#25552;&#21462;&#24037;&#20855;&#21253;&#65292;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#21644;&#23545;&#40784;&#25925;&#20107;&#20107;&#20214;&#65292;&#24182;&#21487;&#29992;&#20110;&#20998;&#26512;&#25925;&#20107;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2208.08063</link><description>&lt;p&gt;
NECE: &#25925;&#20107;&#20107;&#20214;&#38142;&#25552;&#21462;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
NECE: Narrative Event Chain Extraction Toolkit. (arXiv:2208.08063v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08063
&lt;/p&gt;
&lt;p&gt;
NECE&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25925;&#20107;&#20107;&#20214;&#38142;&#25552;&#21462;&#24037;&#20855;&#21253;&#65292;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#21644;&#23545;&#40784;&#25925;&#20107;&#20107;&#20214;&#65292;&#24182;&#21487;&#29992;&#20110;&#20998;&#26512;&#25925;&#20107;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#29702;&#35299;&#19968;&#20010;&#25925;&#20107;&#65292;&#29702;&#35299;&#20107;&#20214;&#30340;&#26102;&#38388;&#27969;&#21160;&#23588;&#20026;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#19982;&#20027;&#35201;&#35282;&#33394;&#30456;&#20851;&#30340;&#20107;&#20214;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#20887;&#38271;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#25925;&#20107;&#25991;&#26412;&#65292;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NECE&#65292;&#19968;&#20010;&#24320;&#25918;&#33719;&#21462;&#30340;&#12289;&#22522;&#20110;&#25991;&#26723;&#32423;&#21035;&#30340;&#24037;&#20855;&#21253;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25552;&#21462;&#21644;&#23545;&#40784;&#25925;&#20107;&#20107;&#20214;&#65292;&#25353;&#29031;&#23427;&#20204;&#21457;&#29983;&#30340;&#26102;&#38388;&#39034;&#24207;&#25490;&#21015;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NECE&#24037;&#20855;&#21253;&#30340;&#39640;&#36136;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20998;&#26512;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#25925;&#20107;&#20559;&#35265;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#20844;&#24320;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#20197;&#21450;&#26410;&#26469;&#24037;&#20316;&#20013;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;NECE&#24037;&#20855;&#21253;&#21253;&#25324;&#19968;&#20010;Python&#24211;&#21644;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Web&#30028;&#38754;&#65292;&#20026;&#19987;&#19994;&#20154;&#21592;&#21644;&#26222;&#36890;&#22823;&#20247;&#25552;&#20379;&#24179;&#31561;&#30340;&#35775;&#38382;&#26435;&#65292;&#21487;&#20197;&#21487;&#35270;&#21270;&#20107;&#20214;&#38142;&#65292;&#33719;&#21462;&#25925;&#20107;&#27969;&#31243;&#65292;&#25110;&#32773;&#30740;&#31350;&#25925;&#20107;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
To understand a narrative, it is essential to comprehend the temporal event flows, especially those associated with main characters; however, this can be challenging with lengthy and unstructured narrative texts. To address this, we introduce NECE, an open-access, document-level toolkit that automatically extracts and aligns narrative events in the temporal order of their occurrence. Through extensive evaluations, we show the high quality of the NECE toolkit and demonstrates its downstream application in analyzing narrative bias regarding gender. We also openly discuss the shortcomings of the current approach, and potential of leveraging generative models in future works. Lastly the NECE toolkit includes both a Python library and a user-friendly web interface, which offer equal access to professionals and layman audience alike, to visualize event chain, obtain narrative flows, or study narrative bias.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#34588;&#34562;&#30417;&#27979;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;&#21644;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2208.00085</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#22312;&#34588;&#34562;&#30417;&#27979;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Computer Vision Techniques in Bee Monitoring Applications. (arXiv:2208.00085v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#34588;&#34562;&#30417;&#27979;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;&#21644;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26159;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#24050;&#32463;&#35777;&#26126;&#33021;&#22815;&#35299;&#20915;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#30417;&#27979;&#34588;&#34562;&#32676;&#20307;&#24182;&#26816;&#26597;&#20854;&#20581;&#24247;&#29366;&#20917;&#65292;&#20174;&#32780;&#22312;&#24773;&#20917;&#21464;&#24471;&#20005;&#37325;&#20043;&#21069;&#65292;&#35782;&#21035;&#20986;&#28508;&#22312;&#21361;&#38505;&#29366;&#24577;&#65292;&#25110;&#32773;&#26356;&#22909;&#22320;&#35745;&#21010;&#23450;&#26399;&#34588;&#34562;&#32676;&#20307;&#26816;&#26597;&#65292;&#20174;&#32780;&#33410;&#30465;&#37325;&#35201;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#29992;&#20110;&#34588;&#34562;&#30417;&#27979;&#30340;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#24182;&#20197;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#20026;&#20363;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#38754;&#21521;&#20861;&#21307;&#23398;&#21644;&#34588;&#34562;&#23398;&#19987;&#19994;&#20154;&#21592;&#21644;&#19987;&#23478;&#65292;&#26088;&#22312;&#21521;&#20182;&#20204;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#65292;&#22240;&#27492;&#27599;&#20010;&#24212;&#29992;&#31867;&#21035;&#37117;&#20197;&#31616;&#35201;&#30340;&#29702;&#35770;&#20171;&#32461;&#21644;&#19982;&#20854;&#22522;&#26412;&#26041;&#27861;&#30456;&#20851;&#30340;&#21160;&#26426;&#24320;&#31687;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#35770;&#25991;&#33021;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;...
&lt;/p&gt;
&lt;p&gt;
Machine learning and computer vision are dynamically growing fields, which have proven to be able to solve very complex tasks. They could also be used for the monitoring of the honeybee colonies and for the inspection of their health state, which could identify potentially dangerous states before the situation is critical, or to better plan periodic bee colony inspections and therefore save significant costs. In this paper, we present an overview of the state-of-the-art computer vision and machine learning applications used for bee monitoring. We also demonstrate the potential of those methods as an example of an automated bee counter algorithm. The paper is aimed at veterinary and apidology professionals and experts, who might not be familiar with machine learning to introduce to them its possibilities, therefore each family of applications is opened by a brief theoretical introduction and motivation related to its base method. We hope that this paper will inspire other scientists to 
&lt;/p&gt;</description></item><item><title>GUARD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#36890;&#29992;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;GCNs&#23545;&#38024;&#23545;&#24615;&#25915;&#20987;&#30340;&#23616;&#37096;&#33410;&#28857;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#19981;&#38477;&#20302;&#25972;&#20307;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2204.09803</link><description>&lt;p&gt;
GUARD: &#22270;&#24418;&#36890;&#29992;&#23545;&#25239;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
GUARD: Graph Universal Adversarial Defense. (arXiv:2204.09803v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.09803
&lt;/p&gt;
&lt;p&gt;
GUARD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#36890;&#29992;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;GCNs&#23545;&#38024;&#23545;&#24615;&#25915;&#20987;&#30340;&#23616;&#37096;&#33410;&#28857;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#19981;&#38477;&#20302;&#25972;&#20307;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#23567;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#20855;&#26377;&#33030;&#24369;&#24615;&#65292;&#36825;&#25104;&#20026;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#19979;&#24212;&#29992;&#30340;&#20005;&#37325;&#23041;&#32961;&#24182;&#19988;&#26497;&#22823;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#23041;&#32961;&#65292;&#24050;&#32463;&#25237;&#20837;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#26469;&#22686;&#21152;GCNs&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#36890;&#24120;&#34987;&#35774;&#35745;&#29992;&#20110;&#38450;&#27490;GCNs&#36973;&#21463;&#38750;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#24182;&#19988;&#30528;&#37325;&#20110;&#25972;&#20307;&#24615;&#33021;&#65292;&#36825;&#20351;&#24471;&#20445;&#25252;&#37325;&#35201;&#23616;&#37096;&#33410;&#28857;&#20813;&#21463;&#26356;&#24378;&#26377;&#21147;&#30340;&#38024;&#23545;&#24615;&#25915;&#20987;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#22312;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#36825;&#20123;&#38480;&#21046;&#20984;&#26174;&#20102;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#38450;&#24481;&#38024;&#23545;&#24615;&#25915;&#20987;&#30340;&#23616;&#37096;&#33410;&#28857;&#32780;&#19981;&#25439;&#23475;GCNs&#25972;&#20307;&#24615;&#33021;&#30340;&#26377;&#25928;&#19988;&#39640;&#25928;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Graph Universal Adversarial Defense&#65288;GUARD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) have been shown to be vulnerable to small adversarial perturbations, which becomes a severe threat and largely limits their applications in security-critical scenarios. To mitigate such a threat, considerable research efforts have been devoted to increasing the robustness of GCNs against adversarial attacks. However, current defense approaches are typically designed to prevent GCNs from untargeted adversarial attacks and focus on overall performance, making it challenging to protect important local nodes from more powerful targeted adversarial attacks. Additionally, a trade-off between robustness and performance is often made in existing research. Such limitations highlight the need for developing an effective and efficient approach that can defend local nodes against targeted attacks, without compromising the overall performance of GCNs. In this work, we present a simple yet effective method, named Graph Universal Adversarial Defense (GUARD). Unlike
&lt;/p&gt;</description></item><item><title>POSTER&#26159;&#19968;&#31181;&#37329;&#23383;&#22612;&#20132;&#21449;&#34701;&#21512;&#21464;&#21387;&#22120;&#32593;&#32476;&#65292;&#26088;&#22312;&#20840;&#38754;&#35299;&#20915;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#20013;&#30340;&#31867;&#38388;&#30456;&#20284;&#24615;&#12289;&#31867;&#20869;&#24046;&#24322;&#24615;&#21644;&#23610;&#24230;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#21464;&#21387;&#22120;&#30340;&#20132;&#21449;&#34701;&#21512;&#26041;&#27861;&#21644;&#37319;&#29992;&#37329;&#23383;&#22612;&#32467;&#26500;&#65292;POSTER&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.04083</link><description>&lt;p&gt;
POSTER:&#19968;&#31181;&#37329;&#23383;&#22612;&#20132;&#21449;&#34701;&#21512;&#21464;&#21387;&#22120;&#32593;&#32476;&#29992;&#20110;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
POSTER: A Pyramid Cross-Fusion Transformer Network for Facial Expression Recognition. (arXiv:2204.04083v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04083
&lt;/p&gt;
&lt;p&gt;
POSTER&#26159;&#19968;&#31181;&#37329;&#23383;&#22612;&#20132;&#21449;&#34701;&#21512;&#21464;&#21387;&#22120;&#32593;&#32476;&#65292;&#26088;&#22312;&#20840;&#38754;&#35299;&#20915;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#20013;&#30340;&#31867;&#38388;&#30456;&#20284;&#24615;&#12289;&#31867;&#20869;&#24046;&#24322;&#24615;&#21644;&#23610;&#24230;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#21464;&#21387;&#22120;&#30340;&#20132;&#21449;&#34701;&#21512;&#26041;&#27861;&#21644;&#37319;&#29992;&#37329;&#23383;&#22612;&#32467;&#26500;&#65292;POSTER&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65288;FER&#65289;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#22312;&#20154;&#26426;&#20132;&#20114;&#12289;&#25945;&#32946;&#12289;&#21307;&#30103;&#21644;&#22312;&#32447;&#30417;&#25511;&#31561;&#39046;&#22495;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;FER&#20219;&#21153;&#20013;&#65292;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#38382;&#39064;: &#31867;&#38388;&#30456;&#20284;&#24615;&#12289;&#31867;&#20869;&#24046;&#24322;&#24615;&#21644;&#23610;&#24230;&#25935;&#24863;&#24615;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#20316;&#21697;&#36890;&#24120;&#35299;&#20915;&#20854;&#20013;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#23436;&#20840;&#35299;&#20915;&#20102;&#36825;&#19977;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;POSTER&#30340;&#21452;&#27969;&#37329;&#23383;&#22612;&#20132;&#21449;&#34701;&#21512;&#21464;&#21387;&#22120;&#32593;&#32476;&#65292;&#26088;&#22312;&#20840;&#38754;&#35299;&#20915;&#36825;&#19977;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#20132;&#21449;&#34701;&#21512;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#38754;&#37096;&#26631;&#24535;&#29305;&#24449;&#21644;&#22270;&#20687;&#29305;&#24449;&#30340;&#26377;&#25928;&#21327;&#20316;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20851;&#27880;&#26174;&#33879;&#30340;&#38754;&#37096;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;POSTER&#37319;&#29992;&#20102;&#37329;&#23383;&#22612;&#32467;&#26500;&#26469;&#25552;&#39640;&#23610;&#24230;&#19981;&#21464;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;POSTER&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Facial expression recognition (FER) is an important task in computer vision, having practical applications in areas such as human-computer interaction, education, healthcare, and online monitoring. In this challenging FER task, there are three key issues especially prevalent: inter-class similarity, intra-class discrepancy, and scale sensitivity. While existing works typically address some of these issues, none have fully addressed all three challenges in a unified framework. In this paper, we propose a two-stream Pyramid crOss-fuSion TransformER network (POSTER), that aims to holistically solve all three issues. Specifically, we design a transformer-based cross-fusion method that enables effective collaboration of facial landmark features and image features to maximize proper attention to salient facial regions. Furthermore, POSTER employs a pyramid structure to promote scale invariance. Extensive experimental results demonstrate that our POSTER achieves new state-of-the-art results o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#65288;RMAB&#65289;&#38382;&#39064;&#20013;&#36827;&#34892;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#26469;&#26368;&#22823;&#21270;Whittle&#25351;&#25968;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2202.00916</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#22312;&#20855;&#26377;&#24212;&#29992;&#20110;&#27597;&#23156;&#20581;&#24247;&#30340;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#29615;&#22659;&#19979;
&lt;/p&gt;
&lt;p&gt;
Scalable Decision-Focused Learning in Restless Multi-Armed Bandits with Application to Maternal and Child Health. (arXiv:2202.00916v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#65288;RMAB&#65289;&#38382;&#39064;&#20013;&#36827;&#34892;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#26469;&#26368;&#22823;&#21270;Whittle&#25351;&#25968;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26410;&#30693;&#33218;&#20043;&#38388;&#36716;&#31227;&#21160;&#21147;&#23398;&#20294;&#20855;&#26377;&#24050;&#30693;&#30456;&#20851;&#33218;&#29305;&#24449;&#30340;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#65288;RMAB&#65289;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#26681;&#25454;&#29305;&#24449;&#39044;&#27979;&#36716;&#31227;&#21160;&#21147;&#23398;&#65292;&#20854;&#20013;Whittle&#25351;&#25968;&#31574;&#30053;&#20351;&#29992;&#39044;&#27979;&#30340;&#36716;&#31227;&#26469;&#35299;&#20915;RMAB&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#26368;&#22823;&#21270;&#39044;&#27979;&#20934;&#30830;&#24615;&#32780;&#19981;&#26159;&#26368;&#32456;RMAB&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#26469;&#23398;&#20064;&#27169;&#22411;&#65292;&#23548;&#33268;&#35757;&#32451;&#30446;&#26631;&#19982;&#35780;&#20272;&#30446;&#26631;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;RMAB&#20013;&#30452;&#25509;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;Whittle&#25351;&#25968;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies restless multi-armed bandit (RMAB) problems with unknown arm transition dynamics but with known correlated arm features. The goal is to learn a model to predict transition dynamics given features, where the Whittle index policy solves the RMAB problems using predicted transitions. However, prior works often learn the model by maximizing the predictive accuracy instead of final RMAB solution quality, causing a mismatch between training and evaluation objectives. To address this shortcoming, we propose a novel approach for decision-focused learning in RMAB that directly trains the predictive model to maximize the Whittle index solution quality. We present three key contributions: (i) we establish differentiability of the Whittle index policy to support decision-focused learning; (ii) we significantly improve the scalability of decision-focused learning approaches in sequential problems, specifically RMAB problems; (iii) we apply our algorithm to a previously collected 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align&#65292;&#19968;&#31181;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#19968;&#33268;&#24615;&#33410;&#28857;&#23545;&#36880;&#27493;&#21457;&#29616;&#33410;&#28857;&#23545;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#33410;&#28857;&#23884;&#20837;&#65292;&#28982;&#21518;&#35745;&#31639;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#24230;&#37327;&#36880;&#27493;&#23545;&#40784;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2201.10945</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#36827;&#34892;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#30340;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Power of Gradual Network Alignment Using Dual-Perception Similarities. (arXiv:2201.10945v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.10945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align&#65292;&#19968;&#31181;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#19968;&#33268;&#24615;&#33410;&#28857;&#23545;&#36880;&#27493;&#21457;&#29616;&#33410;&#28857;&#23545;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#33410;&#28857;&#23884;&#20837;&#65292;&#28982;&#21518;&#35745;&#31639;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#24230;&#37327;&#36880;&#27493;&#23545;&#40784;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#65288;NA&#65289;&#26159;&#22522;&#20110;&#32593;&#32476;&#32467;&#26500;&#21644;&#33410;&#28857;&#23646;&#24615;&#26597;&#25214;&#20004;&#20010;&#32593;&#32476;&#20043;&#38388;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21160;&#26426;&#22312;&#20110;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;NA&#26041;&#27861;&#37117;&#35797;&#22270;&#19968;&#27425;&#24615;&#21457;&#29616;&#25152;&#26377;&#33410;&#28857;&#23545;&#65292;&#22240;&#27492;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#36890;&#36807;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#30340;&#20013;&#38388;&#21457;&#29616;&#26469;&#26356;&#20934;&#30830;&#22320;&#25214;&#21040;&#33410;&#28857;&#21305;&#37197;&#36807;&#31243;&#20013;&#30340;&#19979;&#19968;&#20010;&#23545;&#24212;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Grad-Align&#65292;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22312;&#28176;&#36827;&#21305;&#37197;&#30340;&#26089;&#26399;&#38454;&#27573;&#23481;&#26131;&#21457;&#29616;&#30340;&#33410;&#28857;&#23545;&#26469;&#36880;&#27493;&#21457;&#29616;&#33410;&#28857;&#23545;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Grad-Align&#39318;&#20808;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#25105;&#20204;&#30340;&#36880;&#23618;&#37325;&#26500;&#25439;&#22833;&#29983;&#25104;&#20004;&#20010;&#32593;&#32476;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#35745;&#31639;&#21452;&#24863;&#30693;&#30456;&#20284;&#24615;&#24230;&#37327;&#36880;&#27493;&#23545;&#40784;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment (NA) is the task of finding the correspondence of nodes between two networks based on the network structure and node attributes. Our study is motivated by the fact that, since most of existing NA methods have attempted to discover all node pairs at once, they do not harness information enriched through interim discovery of node correspondences to more accurately find the next correspondences during the node matching. To tackle this challenge, we propose Grad-Align, a new NA method that gradually discovers node pairs by making full use of node pairs exhibiting strong consistency, which are easy to be discovered in the early stage of gradual matching. Specifically, Grad-Align first generates node embeddings of the two networks based on graph neural networks along with our layer-wise reconstruction loss, a loss built upon capturing the first-order and higher-order neighborhood structures. Then, nodes are gradually aligned by computing dual-perception similarity measures 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#23631;&#38556;&#21147;&#30340;&#25511;&#21046;&#31574;&#30053;&#32467;&#26500;&#19982;&#22810;&#27493;&#31574;&#30053;&#35780;&#20272;&#26426;&#21046;&#65292;&#22312;&#20445;&#35777;&#25511;&#21046;&#23433;&#20840;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#24212;&#23545;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.11217</link><description>&lt;p&gt;
&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22312;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#21644;&#25511;&#21046;&#32422;&#26463;&#19979;&#30340;&#24212;&#29992;&#65306;&#26234;&#33021;&#36710;&#36742;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Model-Based Safe Reinforcement Learning with Time-Varying State and Control Constraints: An Application to Intelligent Vehicles. (arXiv:2112.11217v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#23631;&#38556;&#21147;&#30340;&#25511;&#21046;&#31574;&#30053;&#32467;&#26500;&#19982;&#22810;&#27493;&#31574;&#30053;&#35780;&#20272;&#26426;&#21046;&#65292;&#22312;&#20445;&#35777;&#25511;&#21046;&#23433;&#20840;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#24212;&#23545;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#32467;&#26500;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#23433;&#20840;&#24615;&#21644;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#36817;&#20284;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#24456;&#23569;&#26377;&#20316;&#21697;&#35752;&#35770;&#20102;&#22312;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#24615;&#32422;&#26463;&#19979;&#35774;&#35745;&#23433;&#20840;RL&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#21644;&#25511;&#21046;&#32422;&#26463;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23631;&#38556;&#21147;&#30340;&#25511;&#21046;&#31574;&#30053;&#32467;&#26500;&#65292;&#20197;&#20445;&#35777;&#25511;&#21046;&#23433;&#20840;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#31574;&#30053;&#35780;&#20272;&#26426;&#21046;&#65292;&#29992;&#20110;&#39044;&#27979;&#31574;&#30053;&#22312;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#32422;&#26463;&#19979;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25351;&#23548;&#31574;&#30053;&#23433;&#20840;&#26356;&#26032;&#12290;&#24050;&#35777;&#26126;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#20998;&#26512;&#20102;&#28436;&#21592;&#35780;&#35770;&#23478;&#23454;&#29616;&#30340;&#25910;&#25947;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#27169;&#25311;&#30340;Sa&#20013;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recently, safe reinforcement learning (RL) with the actor-critic structure for continuous control tasks has received increasing attention. It is still challenging to learn a near-optimal control policy with safety and convergence guarantees. Also, few works have addressed the safe RL algorithm design under time-varying safety constraints. This paper proposes a safe RL algorithm for optimal control of nonlinear systems with time-varying state and control constraints. In the proposed approach, we construct a novel barrier force-based control policy structure to guarantee control safety. A multi-step policy evaluation mechanism is proposed to predict the policy's safety risk under time-varying safety constraints and guide the policy to update safely. Theoretical results on stability and robustness are proven. Also, the convergence of the actor-critic implementation is analyzed. The performance of the proposed algorithm outperforms several state-of-the-art RL algorithms in the simulated Sa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;&#20102;&#25903;&#25345;&#19981;&#26029;&#21457;&#23637;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25512;&#29702;&#20195;&#29702;&#20154;&#36827;&#34892;&#30340;&#20449;&#24687;&#20256;&#36755;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#29366;&#24577;&#21644;&#20854;&#21160;&#24577;&#30340;&#20195;&#25968;&#25551;&#36848;&#65292;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#21644;&#32452;&#21512;&#30693;&#35782;&#29366;&#24577;&#65292;&#20197;&#34920;&#31034;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2110.11482</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#20030;&#25514;&#20013;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#34920;&#36798;&#21450;&#20854;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Representations of epistemic uncertainty and its perception in data-driven initiatives. (arXiv:2110.11482v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#20102;&#25903;&#25345;&#19981;&#26029;&#21457;&#23637;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25512;&#29702;&#20195;&#29702;&#20154;&#36827;&#34892;&#30340;&#20449;&#24687;&#20256;&#36755;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#29366;&#24577;&#21644;&#20854;&#21160;&#24577;&#30340;&#20195;&#25968;&#25551;&#36848;&#65292;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#21644;&#32452;&#21512;&#30693;&#35782;&#29366;&#24577;&#65292;&#20197;&#34920;&#31034;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#25512;&#21160;&#30340;&#26032;&#20852;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#27491;&#22312;&#37325;&#22609;&#20915;&#31574;&#36807;&#31243;&#65292;&#36828;&#31163;&#23545;&#30452;&#25509;&#25968;&#25454;&#20132;&#20114;&#30340;&#20256;&#32479;&#20381;&#36182;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#24341;&#20837;&#20102;&#35780;&#20272;&#25968;&#25454;&#39537;&#21160;&#20030;&#25514;&#24433;&#21709;&#30340;&#26032;&#25361;&#25112;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20123;&#19981;&#26029;&#21457;&#23637;&#30340;&#26041;&#27861;&#35770;&#65292;&#36843;&#20999;&#38656;&#35201;&#26032;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25551;&#36848;&#28304;&#20110;&#26377;&#38480;&#25968;&#25454;&#21487;&#35266;&#27979;&#24615;&#20197;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#20915;&#31574;&#20013;&#30340;&#27495;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#27169;&#22411;&#65292;&#26088;&#22312;&#22788;&#29702;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25512;&#29702;&#20195;&#29702;&#20154;&#36827;&#34892;&#20449;&#24687;&#20256;&#36755;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20511;&#37492;&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#39537;&#21160;&#20030;&#25514;&#20135;&#29983;&#30340;&#20215;&#20540;&#30340;&#22810;&#32500;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#30693;&#35782;&#29366;&#24577;&#21450;&#20854;&#21160;&#24577;&#30340;&#20195;&#25968;&#25551;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36171;&#20104;&#25105;&#20204;&#30340;&#27169;&#22411;&#19968;&#31181;&#24418;&#24335;&#21270;&#32467;&#26500;&#65292;&#29992;&#20110;&#27604;&#36739;&#21644;&#32452;&#21512;&#30693;&#35782;&#29366;&#24577;&#65307;&#36890;&#36807;&#36825;&#20123;&#32452;&#21512;&#26469;&#34920;&#31034;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging data-driven strategies, powered by the advent of AI, are reshaping decision-making processes, moving away from traditional reliance on direct data interaction. This paradigm shift introduces new challenges in assessing the impact of data-driven initiatives. To support these evolving methodologies, there is a crucial need for new models capable of describing the uncertainties stemming from limited data observability and the resulting ambiguities in decision-making. This contribution presents a novel conceptual model designed to deal with uncertainty in knowledge representations and reasoning about information transfer mediated by agents. Drawing from the multidimensional frameworks currently adopted to assess the value generated in data-driven initiatives, we provide an algebraic description of knowledge states and their dynamics. Specifically, we endow our model with a formal structure to compare and combine knowledge states; an update is represented through these combinations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23494;&#38598;&#26862;&#26519;&#26519;&#20896;&#19979;&#36827;&#34892;&#22823;&#35268;&#27169;&#33258;&#20027;&#39134;&#34892;&#21644;&#23454;&#26102;&#35821;&#20041;&#22320;&#22270;&#26500;&#24314;&#12290;&#31995;&#32479;&#20351;&#29992;LiDAR&#25968;&#25454;&#26816;&#27979;&#21644;&#24314;&#27169;&#26641;&#24178;&#21644;&#22320;&#38754;&#24179;&#38754;&#65292;&#24182;&#21033;&#29992;&#22810;&#32423;&#35268;&#21010;&#21644;&#22320;&#22270;&#26500;&#24314;&#26694;&#26550;&#35745;&#31639;&#21160;&#24577;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#20197;&#26500;&#24314;&#29992;&#25143;&#23450;&#20041;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#24182;&#36890;&#36807;&#35821;&#20041;SLAM&#26469;&#26368;&#23567;&#21270;&#37324;&#31243;&#35745;&#28418;&#31227;&#12290;</title><link>http://arxiv.org/abs/2109.06479</link><description>&lt;p&gt;
&#23494;&#38598;&#26862;&#26519;&#26519;&#20896;&#19979;&#30340;&#22823;&#35268;&#27169;&#33258;&#20027;&#39134;&#34892;&#19982;&#23454;&#26102;&#35821;&#20041;SLAM
&lt;/p&gt;
&lt;p&gt;
Large-scale Autonomous Flight with Real-time Semantic SLAM under Dense Forest Canopy. (arXiv:2109.06479v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.06479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23494;&#38598;&#26862;&#26519;&#26519;&#20896;&#19979;&#36827;&#34892;&#22823;&#35268;&#27169;&#33258;&#20027;&#39134;&#34892;&#21644;&#23454;&#26102;&#35821;&#20041;&#22320;&#22270;&#26500;&#24314;&#12290;&#31995;&#32479;&#20351;&#29992;LiDAR&#25968;&#25454;&#26816;&#27979;&#21644;&#24314;&#27169;&#26641;&#24178;&#21644;&#22320;&#38754;&#24179;&#38754;&#65292;&#24182;&#21033;&#29992;&#22810;&#32423;&#35268;&#21010;&#21644;&#22320;&#22270;&#26500;&#24314;&#26694;&#26550;&#35745;&#31639;&#21160;&#24577;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#20197;&#26500;&#24314;&#29992;&#25143;&#23450;&#20041;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#24182;&#36890;&#36807;&#35821;&#20041;SLAM&#26469;&#26368;&#23567;&#21270;&#37324;&#31243;&#35745;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#22320;&#22270;&#20351;&#29992;&#19968;&#32452;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#23545;&#35937;&#34920;&#31034;&#29615;&#22659;&#12290;&#36825;&#31181;&#34920;&#31034;&#26041;&#24335;&#22312;&#23384;&#20648;&#25928;&#29575;&#12289;&#27495;&#20041;&#24615;&#20943;&#23569;&#21644;&#20449;&#24687;&#20016;&#23500;&#24230;&#26041;&#38754;&#26356;&#22909;&#65292;&#20026;&#22312;&#39640;&#24230;&#26080;&#32467;&#26500;&#12289;&#26080;GPS&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#33258;&#27835;&#39134;&#34892;&#21644;&#33719;&#21462;&#21487;&#25805;&#20316;&#20449;&#24687;&#25552;&#20379;&#26041;&#20415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26519;&#20896;&#19979;&#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#33258;&#20027;&#39134;&#34892;&#21644;&#23454;&#26102;&#35821;&#20041;&#22320;&#22270;&#26500;&#24314;&#12290;&#25105;&#20204;&#20351;&#29992;LiDAR&#25968;&#25454;&#26816;&#27979;&#21644;&#24314;&#27169;&#26641;&#24178;&#21644;&#22320;&#38754;&#24179;&#38754;&#65292;&#36825;&#20123;&#20449;&#24687;&#19982;&#25195;&#25551;&#25968;&#25454;&#30456;&#20851;&#32852;&#65292;&#24182;&#29992;&#20110;&#32422;&#26463;&#26426;&#22120;&#20154;&#23039;&#24577;&#21644;&#26641;&#24178;&#27169;&#22411;&#12290;&#33258;&#20027;&#23548;&#33322;&#27169;&#22359;&#21033;&#29992;&#22810;&#32423;&#35268;&#21010;&#21644;&#22320;&#22270;&#26500;&#24314;&#26694;&#26550;&#65292;&#20197;&#35745;&#31639;&#21160;&#24577;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#20351;&#26080;&#20154;&#26426;&#20197;&#35745;&#31639;&#21644;&#23384;&#20648;&#26377;&#25928;&#30340;&#26041;&#24335;&#26500;&#24314;&#29992;&#25143;&#23450;&#20041;&#24863;&#20852;&#36259;&#21306;&#22495;&#30340;&#35821;&#20041;&#22320;&#22270;&#12290;&#35774;&#35745;&#20102;&#28418;&#31227;&#34917;&#20607;&#26426;&#21046;&#65292;&#36890;&#36807;&#35821;&#20041;SLAM&#26469;&#26368;&#23567;&#21270;&#37324;&#31243;&#35745;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic maps represent the environment using a set of semantically meaningful objects. This representation is storage-efficient, less ambiguous, and more informative, thus facilitating large-scale autonomy and the acquisition of actionable information in highly unstructured, GPS-denied environments. In this letter, we propose an integrated system that can perform large-scale autonomous flights and real-time semantic mapping in challenging under-canopy environments. We detect and model tree trunks and ground planes from LiDAR data, which are associated across scans and used to constrain robot poses as well as tree trunk models. The autonomous navigation module utilizes a multi-level planning and mapping framework and computes dynamically feasible trajectories that lead the UAV to build a semantic map of the user-defined region of interest in a computationally and storage efficient manner. A drift-compensation mechanism is designed to minimize the odometry drift using semantic SLAM outp
&lt;/p&gt;</description></item><item><title>MathBERT&#26159;&#19968;&#20010;&#22522;&#20110;BASE BERT&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#25968;&#23398;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20026;&#25968;&#23398;&#25945;&#32946;&#20013;&#30340;&#36890;&#29992;NLP&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2106.07340</link><description>&lt;p&gt;
MathBERT: &#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25945;&#32946;&#20013;&#30340;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07340
&lt;/p&gt;
&lt;p&gt;
MathBERT&#26159;&#19968;&#20010;&#22522;&#20110;BASE BERT&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#25968;&#23398;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20026;&#25968;&#23398;&#25945;&#32946;&#20013;&#30340;&#36890;&#29992;NLP&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#21407;&#22987;&#30340;BERT&#65288;&#21363;BASE BERT&#65289;&#30340;&#24341;&#20837;&#20197;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#24320;&#21457;&#20102;&#21508;&#31181;&#23450;&#21046;&#30340;BERT&#27169;&#22411;&#26469;&#25913;&#36827;&#29305;&#23450;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#25968;&#23398;&#25991;&#26412;&#30340;&#24615;&#36136;&#65292;&#32463;&#24120;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#35789;&#27719;&#20197;&#21450;&#26041;&#31243;&#21644;&#25968;&#23398;&#31526;&#21495;&#65292;&#25105;&#20204;&#35748;&#20026;&#24320;&#21457;&#19968;&#20010;&#38024;&#23545;&#25968;&#23398;&#30340;&#26032;BERT&#27169;&#22411;&#23558;&#23545;&#35768;&#22810;&#25968;&#23398;&#19979;&#28216;&#20219;&#21153;&#26377;&#29992;&#12290;&#22312;&#36825;&#31687;&#36164;&#28304;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#22810;&#26426;&#26500;&#21162;&#21147;&#65288;&#21363;&#20004;&#20010;&#23398;&#20064;&#24179;&#21488;&#21644;&#19977;&#20010;&#32654;&#22269;&#23398;&#26415;&#26426;&#26500;&#65289;&#20197;&#28385;&#36275;&#36825;&#20010;&#38656;&#27714;&#65306;MathBERT&#65292;&#19968;&#20010;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#25968;&#23398;&#35821;&#26009;&#24211;&#19978;&#23545;BASE BERT&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#32780;&#21019;&#24314;&#30340;&#27169;&#22411;&#65292;&#35813;&#35821;&#26009;&#24211;&#28085;&#30422;&#20102;&#20174;&#23398;&#21069;&#25945;&#32946;&#65288;pre-k&#65289;&#21040;&#39640;&#20013;&#20197;&#21450;&#30740;&#31350;&#29983;&#27700;&#24179;&#30340;&#25968;&#23398;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#24120;&#29992;&#20110;&#25968;&#23398;&#25945;&#32946;&#30340;&#36890;&#29992;NLP&#20219;&#21153;&#65306;&#30693;&#35782;&#32452;&#20214;&#39044;&#27979;&#65292;&#33258;&#21160;&#35780;&#20998;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#30693;&#35782;&#36861;&#28335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the introduction of the original BERT (i.e., BASE BERT), researchers have developed various customized BERT models with improved performance for specific domains and tasks by exploiting the benefits of transfer learning. Due to the nature of mathematical texts, which often use domain specific vocabulary along with equations and math symbols, we posit that the development of a new BERT model for mathematics would be useful for many mathematical downstream tasks. In this resource paper, we introduce our multi-institutional effort (i.e., two learning platforms and three academic institutions in the US) toward this need: MathBERT, a model created by pre-training the BASE BERT model on a large mathematical corpus ranging from pre-kindergarten (pre-k), to high-school, to college graduate level mathematical content. In addition, we select three general NLP tasks that are often used in mathematics education: prediction of knowledge component, auto-grading open-ended Q&amp;A, and knowledge tr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#22312;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25506;&#32034;&#21644;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#22788;&#29702;&#22270;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2105.10377</link><description>&lt;p&gt;
&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Adaptive Filters in Graph Convolutional Neural Networks. (arXiv:2105.10377v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.10377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#22312;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25506;&#32034;&#21644;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#22788;&#29702;&#22270;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#25105;&#20204;&#30446;&#30585;&#20102;&#38750;&#27431;&#20960;&#37324;&#24471;&#39046;&#22495;&#20135;&#29983;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#36825;&#20123;&#39046;&#22495;&#36890;&#24120;&#20197;&#22797;&#26434;&#20851;&#31995;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#30001;&#20110;&#22312;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#26102;&#30340;&#28508;&#21147;&#32780;&#24341;&#36215;&#20102;&#39640;&#24230;&#20851;&#27880;&#12290;&#29305;&#21035;&#22320;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992; GNN &#26550;&#26500;&#30340;&#25193;&#23637;&#26469;&#22312;&#22270;&#19978;&#36827;&#34892;&#21367;&#31215;&#30340;&#21487;&#33021;&#24615;&#38750;&#24120;&#24863;&#20852;&#36259;&#65292;&#36825;&#20010;&#26550;&#26500;&#36890;&#24120;&#34987;&#31216;&#20026;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (ConvGNN)&#12290;&#22312;&#22270;&#19978;&#30340;&#21367;&#31215;&#20027;&#35201;&#20998;&#20026;&#39057;&#35889;&#21367;&#31215;&#21644;&#31354;&#38388;&#21367;&#31215;&#20004;&#31181;&#24418;&#24335;&#12290;&#30001;&#20110;&#22312;&#25506;&#32034;&#21644;&#21033;&#29992;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#26368;&#36817;&#23545;&#20110;&#30740;&#31350;&#31354;&#38388;&#26041;&#27861;&#25152;&#33021;&#25552;&#20379;&#30340;&#21487;&#33021;&#24615;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#25214;&#21040;&#19968;&#31181;&#36866;&#24212;&#22788;&#29702;&#36755;&#20837;&#30340;&#32593;&#32476;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#24635;&#20307;&#24615;&#33021;&#30340;&#24819;&#27861;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last few years, we have witnessed the availability of an increasing data generated from non-Euclidean domains, which are usually represented as graphs with complex relationships, and Graph Neural Networks (GNN) have gained a high interest because of their potential in processing graph-structured data. In particular, there is a strong interest in exploring the possibilities in performing convolution on graphs using an extension of the GNN architecture, generally referred to as Graph Convolutional Neural Networks (ConvGNN). Convolution on graphs has been achieved mainly in two forms: spectral and spatial convolutions. Due to the higher flexibility in exploring and exploiting the graph structure of data, there is recently an increasing interest in investigating the possibilities that the spatial approach can offer. The idea of finding a way to adapt the network behaviour to the inputs they process to maximize the total performances has aroused much interest in the neural networks
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#21327;&#21516;&#36807;&#28388;&#65288;CCF&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#21327;&#21516;&#36807;&#28388;&#21644;&#25512;&#33616;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#32431;&#31929;&#30456;&#20851;&#23398;&#20064;&#23548;&#33268;&#30340;&#39044;&#27979;&#20013;&#30340;&#36763;&#26222;&#26862;&#24726;&#35770;&#38382;&#39064;&#65292;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2102.01868</link><description>&lt;p&gt;
&#22240;&#26524;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Causal Collaborative Filtering. (arXiv:2102.01868v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.01868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#21327;&#21516;&#36807;&#28388;&#65288;CCF&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#21327;&#21516;&#36807;&#28388;&#21644;&#25512;&#33616;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#32431;&#31929;&#30456;&#20851;&#23398;&#20064;&#23548;&#33268;&#30340;&#39044;&#27979;&#20013;&#30340;&#36763;&#26222;&#26862;&#24726;&#35770;&#38382;&#39064;&#65292;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20256;&#32479;&#30340;&#25512;&#33616;&#31639;&#27861;&#22522;&#20110;&#20174;&#25968;&#25454;&#20013;&#25366;&#25496;&#25110;&#23398;&#20064;&#30456;&#20851;&#27169;&#24335;&#26469;&#20272;&#35745;&#29992;&#25143;-&#39033;&#30446;&#30340;&#30456;&#20851;&#20559;&#22909;&#30340;&#22522;&#26412;&#24605;&#24819;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#32431;&#31929;&#30340;&#30456;&#20851;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#20013;&#30340;&#36763;&#26222;&#26862;&#24726;&#35770;&#65292;&#20174;&#32780;&#23548;&#33268;&#25512;&#33616;&#24615;&#33021;&#30340;&#25439;&#22833;&#12290;&#36763;&#26222;&#26862;&#24726;&#35770;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#32479;&#35745;&#29616;&#35937;&#65292;&#23427;&#20250;&#23548;&#33268;&#32479;&#35745;&#32467;&#35770;&#30340;&#28151;&#28102;&#65292;&#24573;&#35270;&#36825;&#20010;&#24726;&#35770;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22240;&#26524;&#21644;&#21453;&#20107;&#23454;&#24314;&#27169;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#36229;&#36234;&#35266;&#23519;&#25968;&#25454;&#36827;&#34892;&#29992;&#25143;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#21327;&#21516;&#36807;&#28388;&#65288;CCF&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#21327;&#21516;&#36807;&#28388;&#21644;&#25512;&#33616;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22240;&#26524;&#35270;&#22270;&#65292;&#24182;&#20174;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#35768;&#22810;&#20256;&#32479;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#23454;&#38469;&#19978;&#26159;&#31616;&#21270;&#22240;&#26524;&#22270;&#19979;CCF&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many of the traditional recommendation algorithms are designed based on the fundamental idea of mining or learning correlative patterns from data to estimate the user-item correlative preference. However, pure correlative learning may lead to Simpson's paradox in predictions, and thus results in sacrificed recommendation performance. Simpson's paradox is a well-known statistical phenomenon, which causes confusions in statistical conclusions and ignoring the paradox may result in inaccurate decisions. Fortunately, causal and counterfactual modeling can help us to think outside of the observational data for user modeling and personalization so as to tackle such issues. In this paper, we propose Causal Collaborative Filtering (CCF) -- a general framework for modeling causality in collaborative filtering and recommendation. We provide a unified causal view of CF and mathematically show that many of the traditional CF algorithms are actually special cases of CCF under simplified causal grap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#23398;&#20064;&#31639;&#27861;&#36817;&#20046;&#26368;&#20248;&#22320;&#35299;&#20915;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#12289;&#22810;&#20219;&#21153;&#30340;NP-hard&#35268;&#21010;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#25552;&#20986;&#26041;&#27861;&#22312;&#35299;&#20915;&#26426;&#22120;&#20154;/&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#19978;&#30340;&#36817;&#20046;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/1905.12204</link><description>&lt;p&gt;
&#20351;&#29992;GNN&#23398;&#20064;NP-Hard&#22810;&#26234;&#33021;&#20307;&#20998;&#37197;&#35268;&#21010;&#65306;&#22312;&#38543;&#26426;&#22270;&#19978;&#30340;&#25512;&#29702;&#21644;&#21487;&#35777;&#26126;&#30340;&#25293;&#21334;&#36866;&#37197;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning NP-Hard Multi-Agent Assignment Planning using GNN: Inference on a Random Graph and Provable Auction-Fitted Q-learning. (arXiv:1905.12204v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1905.12204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#23398;&#20064;&#31639;&#27861;&#36817;&#20046;&#26368;&#20248;&#22320;&#35299;&#20915;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#12289;&#22810;&#20219;&#21153;&#30340;NP-hard&#35268;&#21010;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#25552;&#20986;&#26041;&#27861;&#22312;&#35299;&#20915;&#26426;&#22120;&#20154;/&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#19978;&#30340;&#36817;&#20046;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#31639;&#27861;&#26469;&#36817;&#20046;&#26368;&#20248;&#22320;&#35299;&#20915;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#12289;&#22810;&#20219;&#21153;&#30340;NP-hard&#35268;&#21010;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#31216;&#20026;&#22810;&#26426;&#22120;&#20154;&#22870;&#21169;&#25910;&#38598;&#38382;&#39064;&#65288;MRRC&#65289;&#30340;&#26426;&#22120;&#20154;/&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#12290;&#36825;&#20123;MRRC&#38382;&#39064;&#24456;&#22909;&#22320;&#27169;&#25311;&#20102;&#20849;&#20056;&#12289;&#21462;&#36865;&#21644;&#20854;&#20182;&#30456;&#20851;&#38382;&#39064;&#12290;&#22312;&#23558;MRRC&#38382;&#39064;&#34920;&#31034;&#20026;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27599;&#20010;&#29366;&#24577;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGM&#65289;&#30340;&#25193;&#23637;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#38543;&#26426;PGMs&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20026;&#38543;&#26426;PGMs&#24320;&#21457;&#20102;&#19968;&#31181;&#22343;&#22330;&#25512;&#29702;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#20010;&#21487;&#36827;&#34892;&#39034;&#24207;&#36716;&#31227;&#30340;Q&#20989;&#25968;&#20272;&#35745;&#22120;&#21644;&#65288;2&#65289;&#19968;&#20010;&#25903;&#25345;&#39034;&#24207;&#36716;&#31227;&#30340;&#25293;&#21334;&#26041;&#27861;&#65292;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36873;&#25321;&#32852;&#21512;&#20998;&#37197;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#20102;&#19968;&#20010;&#20855;&#26377;&#33267;&#23569;$1-1/e$&#26368;&#20248;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#35299;&#20915;MRRC&#38382;&#39064;&#30340;&#23454;&#39564;&#32467;&#26524;&#20013;&#31361;&#20986;&#20102;&#36817;&#20046;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the possibility of near-optimally solving multi-agent, multi-task NP-hard planning problems with time-dependent rewards using a learning-based algorithm. In particular, we consider a class of robot/machine scheduling problems called the multi-robot reward collection problem (MRRC). Such MRRC problems well model ride-sharing, pickup-and-delivery, and a variety of related problems. In representing the MRRC problem as a sequential decision-making problem, we observe that each state can be represented as an extension of probabilistic graphical models (PGMs), which we refer to as random PGMs. We then develop a mean-field inference method for random PGMs. We then propose (1) an order-transferable Q-function estimator and (2) an order-transferability-enabled auction to select a joint assignment in polynomial time. These result in a reinforcement learning framework with at least $1-1/e$ optimality. Experimental results on solving MRRC problems highlight the near-optimality 
&lt;/p&gt;</description></item></channel></rss>