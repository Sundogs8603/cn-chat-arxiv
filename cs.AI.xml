<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#32467;&#21512;&#20102;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#22797;&#21046;&#20154;&#31867;&#30340;&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01591</link><description>&lt;p&gt;
BAT: &#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20851;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
BAT: Learning to Reason about Spatial Sounds with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#32467;&#21512;&#20102;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#22797;&#21046;&#20154;&#31867;&#30340;&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#30340;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#22768;&#38899;&#25512;&#29702;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#20154;&#31867;&#25216;&#33021;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#22768;&#38899;&#26469;&#23548;&#33322;&#21644;&#35299;&#37322;&#25105;&#20204;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BAT&#65292;&#23427;&#23558;&#21452;&#32819;&#22768;&#38899;&#22330;&#26223;&#20998;&#26512;&#27169;&#22411;&#30340;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#33021;&#21147;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#22797;&#21046;&#36825;&#31181;&#22266;&#26377;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#37326;&#22806;&#31354;&#38388;&#22768;&#38899;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#25105;&#20204;&#20351;&#29992;AudioSet&#21644;SoundSpaces 2.0&#21512;&#25104;&#20102;&#19968;&#20010;&#21452;&#32819;&#38899;&#39057;&#25968;&#25454;&#38598;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#22768;&#38899;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;SpatialSoundQA&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#65292;&#20197;&#35757;&#32451;BAT&#22312;&#31354;&#38388;&#22768;&#38899;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;BAT&#30340;&#22768;&#23398;&#21069;&#31471;&#32534;&#30721;&#22120;&#26159;&#19968;&#31181;&#21517;&#20026;Spatial Audio Spectrogram Transformer&#65288;Spatial-AST&#65289;&#30340;&#21019;&#26032;&#31354;&#38388;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#23427;&#26412;&#36523;&#22312;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#12289;&#31354;&#38388;&#23450;&#20301;&#21644;&#36317;&#31163;&#20272;&#35745;&#31561;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;Spatial-AST&#19982;LLaMA-2 7B&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21333;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00675</link><description>&lt;p&gt;
LLM meets Vision-Language Models&#29992;&#20110;&#38646;&#26679;&#26412;&#21333;&#31867;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LLM meets Vision-Language Models for Zero-Shot One-Class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00675
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21333;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#38646;&#26679;&#26412;&#21333;&#31867;&#35270;&#35273;&#20998;&#31867;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20165;&#30446;&#26631;&#31867;&#21035;&#30340;&#26631;&#31614;&#21487;&#29992;&#65292;&#24182;&#19988;&#30446;&#26631;&#26159;&#22312;&#19981;&#38656;&#35201;&#26469;&#33258;&#30446;&#26631;&#20219;&#21153;&#30340;&#20219;&#20309;&#39564;&#35777;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27491;&#36127;&#26597;&#35810;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#39318;&#20808;&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#26597;&#25214;&#22312;&#35270;&#35273;&#19978;&#20196;&#20154;&#22256;&#24785;&#30340;&#23545;&#35937;&#65292;&#28982;&#21518;&#20381;&#36182;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#35843;&#25972;&#22823;&#35268;&#27169;&#35270;&#35273;&#22522;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27492;&#35774;&#32622;&#20013;&#20248;&#20110;&#33258;&#36866;&#24212;&#30340;&#29616;&#25104;&#26367;&#20195;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#36127;&#26597;&#35810;&#26679;&#26412;&#20174;&#19982;&#27491;&#26597;&#35810;&#26679;&#26412;&#30456;&#21516;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#65292;&#21253;&#25324;iNaturalist&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#29256;&#26412;&#65292;&#20854;&#20013;&#36127;&#26679;&#26412;&#22312;&#20998;&#31867;&#26641;&#20013;&#19982;&#27491;&#26679;&#26412;&#30456;&#36317;&#22266;&#23450;&#36317;&#31163;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#20381;&#36182;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#38646;&#26679;&#26412;&#21333;&#31867;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00675v1 Announce Type: cross  Abstract: We consider the problem of zero-shot one-class visual classification. In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from the target task. We propose a two-step solution that first queries large language models for visually confusing objects and then relies on vision-language pre-trained models (e.g., CLIP) to perform classification. By adapting large-scale vision benchmarks, we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting. Namely, we propose a realistic benchmark where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones. Our work shows that it is possible to 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;&#26426;&#26500;&#65292;&#24182;&#21487;&#33021;&#20351;&#27785;&#28024;&#24335;&#25216;&#26415;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20216</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#30340;&#20998;&#24067;&#24335;&#26426;&#26500;
&lt;/p&gt;
&lt;p&gt;
Distributed agency in second language learning and teaching through generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20216
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;&#26426;&#26500;&#65292;&#24182;&#21487;&#33021;&#20351;&#27785;&#28024;&#24335;&#25216;&#26415;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20216v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20026;&#35821;&#35328;&#23398;&#20064;&#25552;&#20379;&#20102;&#37325;&#22823;&#26426;&#20250;&#12290;&#20687;ChatGPT&#36825;&#26679;&#30340;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#20070;&#38754;&#25110;&#21475;&#22836;&#24418;&#24335;&#30340;&#23545;&#35805;&#20026;&#31532;&#20108;&#35821;&#35328;&#25552;&#20379;&#38750;&#27491;&#24335;&#32451;&#20064;&#65292;&#23398;&#20064;&#32773;&#36890;&#36807;&#25552;&#31034;&#25351;&#23450;&#23545;&#35805;&#21442;&#25968;&#65292;&#22914;&#29087;&#32451;&#31243;&#24230;&#12289;&#35821;&#35328;&#39118;&#26684;&#21644;&#35752;&#35770;&#20027;&#39064;&#12290;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#34987;&#25351;&#23548;&#32473;&#20104;&#32416;&#27491;&#24615;&#21453;&#39304;&#65292;&#21019;&#24314;&#32451;&#20064;&#39064;&#65292;&#25110;&#21046;&#23450;&#25193;&#23637;&#23398;&#20064;&#35745;&#21010;&#12290;&#25945;&#24072;&#21487;&#20197;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26500;&#24314;&#21508;&#31181;&#23186;&#20307;&#30340;&#23398;&#20064;&#21644;&#35780;&#20272;&#26448;&#26009;&#12290;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#20250;&#20351;&#27785;&#28024;&#24335;&#25216;&#26415;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#65292;&#25670;&#33073;&#33050;&#26412;&#21270;&#30340;&#20114;&#21160;&#12290;&#23545;&#20110;&#23398;&#20064;&#32773;&#21644;&#25945;&#24072;&#32780;&#35328;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#23616;&#38480;&#24615;&#26469;&#33258;&#20110;&#23427;&#20204;&#23545;&#20154;&#31867;&#35821;&#35328;&#30340;&#32431;&#32479;&#35745;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#35821;&#35328;&#20351;&#29992;&#20013;&#24494;&#22937;&#31038;&#20250;&#21644;&#25991;&#21270;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21019;&#36896;&#26041;&#24335;&#23384;&#22312;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20216v1 Announce Type: cross  Abstract: Generative AI offers significant opportunities for language learning. Tools like ChatGPT can provide informal second language practice through chats in written or voice forms, with the learner specifying through prompts conversational parameters such as proficiency level, language register, and discussion topics. AI can be instructed to give corrective feedback, create practice exercises, or develop an extended study plan. Instructors can use AI to build learning and assessment materials in a variety of media. AI is likely to make immersive technologies more powerful and versatile, moving away from scripted interactions. For both learners and teachers, it is important to understand the limitations of AI systems that arise from their purely statistical model of human language, which limits their ability to deal with nuanced social and cultural aspects of language use. Additionally, there are ethical concerns over how AI systems are crea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Gamba&#65292;&#19968;&#31181;&#21333;&#35270;&#22270;3D&#37325;&#24314;&#27169;&#22411;&#65292;&#21019;&#26032;&#22320;&#32467;&#21512;&#20102;&#22823;&#37327;&#30340;3D&#39640;&#26031;&#28857;&#36827;&#34892;&#39640;&#25928;&#37325;&#24314;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26364;&#24052;&#30340;&#39034;&#24207;&#32593;&#32476;&#65292;&#20419;&#36827;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18795</link><description>&lt;p&gt;
Gamba&#65306;&#23558;&#39640;&#26031;&#39128;&#28857;&#19982;&#26364;&#24052;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#21333;&#35270;&#22270;3D&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18795
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Gamba&#65292;&#19968;&#31181;&#21333;&#35270;&#22270;3D&#37325;&#24314;&#27169;&#22411;&#65292;&#21019;&#26032;&#22320;&#32467;&#21512;&#20102;&#22823;&#37327;&#30340;3D&#39640;&#26031;&#28857;&#36827;&#34892;&#39640;&#25928;&#37325;&#24314;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26364;&#24052;&#30340;&#39034;&#24207;&#32593;&#32476;&#65292;&#20419;&#36827;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33268;&#21147;&#20110;&#35299;&#20915;&#20174;&#21333;&#20010;&#22270;&#20687;&#39640;&#25928;&#37325;&#24314;3D&#36164;&#20135;&#30340;&#25361;&#25112;&#65292;&#38543;&#30528;&#23545;&#33258;&#21160;&#21270;3D&#20869;&#23481;&#21019;&#24314;&#27969;&#27700;&#32447;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Gamba&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#36827;&#34892;&#25674;&#20313;&#21270;3D&#37325;&#24314;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#20004;&#20010;&#20027;&#35201;&#35265;&#35299;&#65306;(1) 3D&#34920;&#31034;&#65306;&#21033;&#29992;&#22823;&#37327;3D&#39640;&#26031;&#26469;&#36827;&#34892;&#39640;&#25928;&#30340;3D&#39640;&#26031;&#39128;&#28857;&#36807;&#31243;&#65307;(2) &#20027;&#24178;&#35774;&#35745;&#65306;&#24341;&#20837;&#22522;&#20110;&#26364;&#24052;&#30340;&#39034;&#24207;&#32593;&#32476;&#65292;&#20419;&#36827;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#65292;&#24182;&#20855;&#26377;&#19982;&#24207;&#21015;&#65288;&#26631;&#35760;&#65289;&#38271;&#24230;&#30340;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#65292;&#36866;&#24212;&#22823;&#37327;&#39640;&#26031;&#28857;&#12290;Gamba&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#27491;&#21017;&#21270;&#31561;&#26041;&#38754;&#34701;&#20837;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18795v1 Announce Type: cross  Abstract: We tackle the challenge of efficiently reconstructing a 3D asset from a single image with growing demands for automated 3D content creation pipelines. Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural Radiance Fields (NeRF). Despite their significant success, these approaches encounter practical limitations due to lengthy optimization and considerable memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D reconstruction model from single-view images, emphasizing two main insights: (1) 3D representation: leveraging a large number of 3D Gaussians for an efficient 3D Gaussian splatting process; (2) Backbone design: introducing a Mamba-based sequential network that facilitates context-dependent reasoning and linear scalability with the sequence (token) length, accommodating a substantial number of Gaussians. Gamba incorporates significant advancements in data preprocessing, regularization
&lt;/p&gt;</description></item><item><title>&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17919</link><description>&lt;p&gt;
LISA&#65306;&#29992;&#20110;&#39640;&#25928;&#20869;&#23384;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17919
&lt;/p&gt;
&lt;p&gt;
&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39318;&#27425;&#20986;&#29616;&#20197;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#24040;&#22823;&#30340;&#20869;&#23384;&#28040;&#32791;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35832;&#22914;&#20302;&#31209;&#35843;&#25972;&#65288;LoRA&#65289;&#20043;&#31867;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#22823;&#35268;&#27169;&#24494;&#35843;&#35774;&#32622;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#26080;&#27861;&#19982;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#30456;&#21305;&#37197;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LoRA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#30340;&#36880;&#23618;&#29305;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#21516;&#23618;&#20043;&#38388;&#26435;&#37325;&#33539;&#25968;&#30340;&#24322;&#24120;&#20559;&#26012;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#31616;&#21333;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#35760;&#24518;&#25104;&#26412;&#20302;&#20110;LoRA&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#24191;&#27867;&#30340;&#35774;&#32622;&#20013;&#20248;&#20110;LoRA&#21644;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;Layerwise Importance Sampled AdamW&#65288;LISA&#65289;&#65292;&#36825;&#26159;LoRA&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#20013;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#65292;&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#20132;&#20114;&#34892;&#20026;&#21644;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16843</link><description>&lt;p&gt;
LLM&#20195;&#29702;&#26159;&#21542;&#20250;&#24863;&#21040;&#21518;&#24724;&#65311;&#22312;&#32447;&#23398;&#20064;&#21644;&#28216;&#25103;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do LLM Agents Have Regret? A Case Study in Online Learning and Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16843
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#20013;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#65292;&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#20132;&#20114;&#34892;&#20026;&#21644;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;(&#20132;&#20114;&#24335;)&#20915;&#31574;&#21046;&#23450;&#65292;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#19981;&#26029;&#30340;&#25104;&#21151;&#65292;&#20294;LLM&#20195;&#29702;&#22312;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#36827;&#34892;&#20805;&#20998;&#35843;&#26597;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#26102;&#30340;&#22810;&#20195;&#29702;&#35774;&#32622;&#20013;&#65292;&#36825;&#26159;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20856;&#22411;&#22330;&#26223;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;LLM&#20195;&#29702;&#22312;&#36825;&#20123;&#20132;&#20114;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#30740;&#31350;&#23427;&#20204;&#22312;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#36890;&#36807;\emph{&#21518;&#24724;}&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#32463;&#20856;(&#38750;&#24179;&#31283;)&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;LLMs&#30340;&#26080;&#21518;&#24724;&#34892;&#20026;&#65292;&#20197;&#21450;&#24403;LLM&#20195;&#29702;&#36890;&#36807;&#36827;&#34892;&#37325;&#22797;&#28216;&#25103;&#36827;&#34892;&#20132;&#20114;&#26102;&#22343;&#34913;&#30340;&#20986;&#29616;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#26080;&#21518;&#24724;&#34892;&#20026;&#25552;&#20379;&#19968;&#20123;&#29702;&#35770;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16843v1 Announce Type: cross  Abstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behavior
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.15941</link><description>&lt;p&gt;
&#25506;&#32034;&#30452;&#21040;&#33258;&#20449;: &#38754;&#21521;&#20855;&#36523;&#38382;&#31572;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explore until Confident: Efficient Exploration for Embodied Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15941
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#36523;&#38382;&#31572;&#65288;EQA&#65289;&#30340;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#22312;&#38656;&#35201;&#20027;&#21160;&#25506;&#32034;&#29615;&#22659;&#20197;&#25910;&#38598;&#20449;&#24687;&#30452;&#21040;&#23545;&#38382;&#39064;&#30340;&#31572;&#26696;&#26377;&#33258;&#20449;&#30340;&#20855;&#36523;&#20195;&#29702;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#24378;&#22823;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#26469;&#39640;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;EQA&#20013;&#20351;&#29992;VLMs&#26102;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#23427;&#20204;&#27809;&#26377;&#20869;&#37096;&#35760;&#24518;&#23558;&#22330;&#26223;&#26144;&#23556;&#20197;&#20415;&#35268;&#21010;&#22914;&#20309;&#38543;&#26102;&#38388;&#25506;&#32034;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#32622;&#20449;&#24230;&#21487;&#33021;&#34987;&#38169;&#35823;&#26657;&#20934;&#24182;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#36807;&#26089;&#20572;&#27490;&#25506;&#32034;&#25110;&#36807;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#39318;&#20808;&#22522;&#20110;&#28145;&#24230;&#20449;&#24687;&#21644;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;VLM&#26469;&#26500;&#24314;&#22330;&#26223;&#30340;&#35821;&#20041;&#22320;&#22270;-&#21033;&#29992;&#20854;&#23545;&#22330;&#26223;&#30456;&#20851;&#21306;&#22495;&#30340;&#24191;&#27867;&#30693;&#35782;&#26469;&#36827;&#34892;&#25506;&#32034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#26469;&#26657;&#20934;VLM&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15941v1 Announce Type: cross  Abstract: We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21270;&#29366;&#24577;&#21644;&#31526;&#21495;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#25552;&#28860;&#25104;&#21487;&#25193;&#23637;&#30340;&#24863;&#30693;&#27169;&#22359;&#26469;&#20811;&#26381;&#25928;&#29575;&#29942;&#39048;&#65292;&#24182;&#21033;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31616;&#27905;&#26131;&#35835;&#30340;&#35821;&#35328;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.12451</link><description>&lt;p&gt;
INSIGHT: &#24102;&#26377;&#35821;&#35328;&#35299;&#37322;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#31526;&#21495;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21270;&#29366;&#24577;&#21644;&#31526;&#21495;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#25552;&#28860;&#25104;&#21487;&#25193;&#23637;&#30340;&#24863;&#30693;&#27169;&#22359;&#26469;&#20811;&#26381;&#25928;&#29575;&#29942;&#39048;&#65292;&#24182;&#21033;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31616;&#27905;&#26131;&#35835;&#30340;&#35821;&#35328;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#24378;&#21270;&#23398;&#20064;&#65288;NS-RL&#65289;&#24050;&#25104;&#20026;&#21487;&#35299;&#37322;&#20915;&#31574;&#21046;&#23450;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#24335;&#65292;&#20854;&#29305;&#28857;&#26159;&#31526;&#21495;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#35270;&#35273;&#35266;&#27979;&#30340;&#20219;&#21153;&#65292;NS-RL&#28041;&#21450;&#23545;&#29366;&#24577;&#36827;&#34892;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#25928;&#29575;&#65292;&#20808;&#21069;&#30340;&#31639;&#27861;&#26080;&#27861;&#21033;&#29992;&#22870;&#21169;&#20449;&#21495;&#26469;&#32454;&#21270;&#32467;&#26500;&#21270;&#29366;&#24577;&#12290;&#21487;&#35775;&#38382;&#24615;&#20063;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#24191;&#27867;&#30340;&#39046;&#22495;&#30693;&#35782;&#26469;&#35299;&#37322;&#24403;&#21069;&#30340;&#31526;&#21495;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21270;&#29366;&#24577;&#21644;&#31526;&#21495;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#25552;&#28860;&#25104;&#21487;&#25193;&#23637;&#30340;&#24863;&#30693;&#27169;&#22359;&#65292;&#20811;&#26381;&#25928;&#29575;&#29942;&#39048;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20026;&#25919;&#31574;&#21644;&#20915;&#31574;&#29983;&#25104;&#31616;&#27905;&#26131;&#35835;&#30340;&#35821;&#35328;&#35299;&#37322;&#12290;&#22312;&#20061;&#20010;Atari&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12451v1 Announce Type: new  Abstract: Neuro-symbolic reinforcement learning (NS-RL) has emerged as a promising paradigm for explainable decision-making, characterized by the interpretability of symbolic policies. For tasks with visual observations, NS-RL entails structured representations for states, but previous algorithms are unable to refine the structured states with reward signals due to a lack of efficiency. Accessibility is also an issue, as extensive domain knowledge is required to interpret current symbolic policies. In this paper, we present a framework that is capable of learning structured states and symbolic policies simultaneously, whose key idea is to overcome the efficiency bottleneck by distilling vision foundation models into a scalable perception module. Moreover, we design a pipeline that uses large language models to generate concise and readable language explanations for policies and decisions. In experiments on nine Atari tasks, our approach demonstrat
&lt;/p&gt;</description></item><item><title>AUTONODE&#26159;&#19968;&#31181;&#31070;&#32463;&#22270;&#33258;&#23398;&#20064;&#24341;&#25806;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#31070;&#32463;&#22270;&#25216;&#26415;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#26234;&#33021;&#20195;&#29702;&#20154;&#22312;&#32593;&#32476;&#30028;&#38754;&#19978;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.10171</link><description>&lt;p&gt;
AUTONODE: &#19968;&#31181;&#29992;&#20110;&#35748;&#30693;GUI&#33258;&#21160;&#21270;&#30340;&#31070;&#32463;&#22270;&#33258;&#23398;&#20064;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10171
&lt;/p&gt;
&lt;p&gt;
AUTONODE&#26159;&#19968;&#31181;&#31070;&#32463;&#22270;&#33258;&#23398;&#20064;&#24341;&#25806;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#31070;&#32463;&#22270;&#25216;&#26415;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#26234;&#33021;&#20195;&#29702;&#20154;&#22312;&#32593;&#32476;&#30028;&#38754;&#19978;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20013;&#65292;&#20986;&#29616;&#20102;&#19968;&#20123;&#20855;&#26377;&#22686;&#24378;&#30340;&#35748;&#30693;&#33021;&#21147;&#21644;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#30340;&#33021;&#22815;&#35299;&#20915;&#26426;&#22120;&#20154;&#27969;&#31243;&#33258;&#21160;&#21270;&#65288;RPA&#65289;&#25361;&#25112;&#30340;&#20195;&#29702;&#20154;&#12290;&#36825;&#19968;&#21457;&#23637;&#26631;&#24535;&#30528;&#22312;&#30446;&#26631;&#23454;&#29616;&#26041;&#38754;&#20986;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#31867;&#20284;&#20154;&#31867;&#30340;&#36866;&#24212;&#24615;&#30340;&#26032;&#26102;&#20195;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AUTONODE&#65288;&#36890;&#36807;&#22312;&#32447;&#31070;&#32463;&#22270;&#25805;&#20316;&#21644;&#28145;&#24230;&#25506;&#32034;&#23454;&#29616;&#33258;&#20027;&#29992;&#25143;&#30028;&#38754;&#36716;&#25442;&#65289;&#12290;AUTONODE&#37319;&#29992;&#20808;&#36827;&#30340;&#31070;&#32463;&#22270;&#25216;&#26415;&#65292;&#20419;&#36827;&#20102;&#23545;&#32593;&#32476;&#30028;&#38754;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#39044;&#23450;&#20041;&#33050;&#26412;&#25110;&#25163;&#21160;&#24178;&#39044;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#24341;&#25806;&#36171;&#20104;&#20195;&#29702;&#20154;&#29702;&#35299;&#21644;&#23454;&#26045;&#22797;&#26434;&#24037;&#20316;&#27969;&#30340;&#33021;&#21147;&#65292;&#20197;&#26080;&#19982;&#20262;&#27604;&#30340;&#25928;&#29575;&#36866;&#24212;&#21160;&#24577;&#32593;&#32476;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35748;&#30693;&#21151;&#33021;&#19982;&#26426;&#22120;&#20154;&#33258;&#21160;&#21270;&#30456;&#32467;&#21512;&#65292;&#36171;&#20104;AUTONODE&#20174;abiliti&#21040;abi&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10171v1 Announce Type: new  Abstract: In recent advancements within the domain of Large Language Models (LLMs), there has been a notable emergence of agents capable of addressing Robotic Process Automation (RPA) challenges through enhanced cognitive capabilities and sophisticated reasoning. This development heralds a new era of scalability and human-like adaptability in goal attainment. In this context, we introduce AUTONODE (Autonomous User-interface Transformation through Online Neuro-graphic Operations and Deep Exploration). AUTONODE employs advanced neuro-graphical techniques to facilitate autonomous navigation and task execution on web interfaces, thereby obviating the necessity for predefined scripts or manual intervention. Our engine empowers agents to comprehend and implement complex workflows, adapting to dynamic web environments with unparalleled efficiency. Our methodology synergizes cognitive functionalities with robotic automation, endowing AUTONODE with the abi
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#25506;&#35752;&#20102;Q-learning&#31639;&#27861;&#22312;&#28216;&#25103;&#20013;&#21463;&#21040;&#31574;&#30053;&#24615;&#23545;&#25163;&#30340;&#25805;&#32437;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08906</link><description>&lt;p&gt;
&#38024;&#23545;Q-&#23398;&#20064;&#32773;&#30340;&#31574;&#30053;&#21270;&#23545;&#25239;&#65306;&#19968;&#31181;&#25511;&#21046;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Strategizing against Q-learners: A Control-theoretical Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08906
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#25506;&#35752;&#20102;Q-learning&#31639;&#27861;&#22312;&#28216;&#25103;&#20013;&#21463;&#21040;&#31574;&#30053;&#24615;&#23545;&#25163;&#30340;&#25805;&#32437;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;Q-learning&#31639;&#27861;(&#19968;&#31181;&#32463;&#20856;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;)&#22312;&#28216;&#25103;&#20013;&#23545;&#31574;&#30053;&#24615;&#23545;&#25163;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#22914;&#26524;&#31574;&#30053;&#24615;&#23545;&#25163;&#20102;&#35299;&#23545;&#25163;&#30340;Q-learning&#31639;&#27861;&#65292;&#22905;&#21487;&#20197;&#21033;&#29992;&#19968;&#20010;&#22825;&#30495;&#30340;Q-&#23398;&#20064;&#32773;&#22810;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#31574;&#30053;&#34892;&#20026;&#32773;&#30340;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(&#20855;&#26377;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;Q&#20540;&#30340;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;)&#65292;&#23601;&#22909;&#20687;Q-&#23398;&#20064;&#31639;&#27861;&#26159;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#19968;&#26679;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37327;&#21270;&#30340;&#36817;&#20284;&#26041;&#26696;&#26469;&#22788;&#29702;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#20998;&#26512;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08906v1 Announce Type: cross  Abstract: In this paper, we explore the susceptibility of the Q-learning algorithm (a classical and widely used reinforcement learning method) to strategic manipulation of sophisticated opponents in games. We quantify how much a strategically sophisticated agent can exploit a naive Q-learner if she knows the opponent's Q-learning algorithm. To this end, we formulate the strategic actor's problem as a Markov decision process (with a continuum state space encompassing all possible Q-values) as if the Q-learning algorithm is the underlying dynamical system. We also present a quantization-based approximation scheme to tackle the continuum state space and analyze its performance both analytically and numerically.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#21644;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#65292;DSEG-LIME&#25913;&#36827;&#20102;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07733</link><description>&lt;p&gt;
DSEG-LIME -- &#36890;&#36807;&#23618;&#27425;&#21270;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#25552;&#21319;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DSEG-LIME -- Improving Image Explanation by Hierarchical Data-Driven Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#21644;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#65292;DSEG-LIME&#25913;&#36827;&#20102;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#25581;&#31034;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;LIME (Local Interpretable Model-agnostic Explanations) &#26159;&#19968;&#20010;&#24191;&#20026;&#20154;&#30693;&#30340;&#29992;&#20110;&#22270;&#20687;&#20998;&#26512;&#30340;XAI&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#22270;&#20687;&#20998;&#21106;&#26469;&#21019;&#24314;&#29305;&#24449;&#20197;&#35782;&#21035;&#30456;&#20851;&#30340;&#20998;&#31867;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#36739;&#24046;&#30340;&#20998;&#21106;&#21487;&#33021;&#20250;&#24433;&#21709;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#24182;&#21066;&#24369;&#21508;&#20010;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#25972;&#20307;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DSEG-LIME (Data-Driven Segmentation LIME)&#65292;&#20855;&#26377;: i) &#29992;&#20110;&#29983;&#25104;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;&#30340;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;, &#21644; ii) &#36890;&#36807;&#32452;&#21512;&#23454;&#29616;&#30340;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#20351;&#29992;&#26469;&#33258;ImageNet&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#23545;DSEG-LIME&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;-&#36825;&#20123;&#24773;&#26223;&#19981;&#21253;&#21547;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#20998;&#26512;&#21253;&#25324;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;XAI&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07733v1 Announce Type: cross  Abstract: Explainable Artificial Intelligence is critical in unraveling decision-making processes in complex machine learning models. LIME (Local Interpretable Model-agnostic Explanations) is a well-known XAI framework for image analysis. It utilizes image segmentation to create features to identify relevant areas for classification. Consequently, poor segmentation can compromise the consistency of the explanation and undermine the importance of the segments, affecting the overall interpretability. Addressing these challenges, we introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a data-driven segmentation for human-recognized feature generation, and ii) a hierarchical segmentation procedure through composition. We benchmark DSEG-LIME on pre-trained models with images from the ImageNet dataset - scenarios without domain-specific knowledge. The analysis includes a quantitative evaluation using established XAI metrics, complemented
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21512;&#25104;&#22522;&#20934;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21015;&#20030;&#31639;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#25104;&#31639;&#27861;&#65292;&#20197;&#22312;&#36845;&#20195;&#24490;&#29615;&#20013;&#25552;&#20379;&#35821;&#27861;&#25351;&#23548;&#21644;&#20449;&#24687;&#20132;&#27969;&#12290;</title><link>https://arxiv.org/abs/2403.03997</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#21015;&#20030;&#24335;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Guiding Enumerative Program Synthesis with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21512;&#25104;&#22522;&#20934;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21015;&#20030;&#31639;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#25104;&#31639;&#27861;&#65292;&#20197;&#22312;&#36845;&#20195;&#24490;&#29615;&#20013;&#25552;&#20379;&#35821;&#27861;&#25351;&#23548;&#21644;&#20449;&#24687;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24320;&#22987;&#20027;&#23548;&#22260;&#32469;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#29983;&#25104;&#20195;&#30721;&#30340;&#35752;&#35770;&#12290;&#19982;&#27492;&#30456;&#27604;&#65292;&#22312;&#31934;&#30830;&#36923;&#36753;&#35268;&#33539;&#39046;&#22495;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#21512;&#25104;&#22120;&#20173;&#28982;&#22522;&#20110;&#21015;&#20030;&#31639;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#39046;&#22495;&#25552;&#31034;&#24211;&#35780;&#20272;&#20102;LLMs&#35299;&#20915;&#24418;&#24335;&#21512;&#25104;&#22522;&#20934;&#30340;&#33021;&#21147;&#12290;&#24403;&#19968;&#27425;&#21512;&#25104;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21015;&#20030;&#24335;&#21512;&#25104;&#31639;&#27861;&#65292;&#23558;LLM&#30340;&#35843;&#29992;&#38598;&#25104;&#21040;&#21152;&#26435;&#27010;&#29575;&#25628;&#32034;&#20013;&#12290;&#36825;&#26679;&#65292;&#21512;&#25104;&#22120;&#21487;&#20197;&#21521;LLM&#25552;&#20379;&#26377;&#20851;&#26522;&#20030;&#22120;&#36827;&#23637;&#24773;&#20917;&#30340;&#20449;&#24687;&#65292;LLM&#21487;&#20197;&#21521;&#26522;&#20030;&#22120;&#22312;&#36845;&#20195;&#24490;&#29615;&#20013;&#25552;&#20379;&#35821;&#27861;&#25351;&#23548;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#35821;&#27861;&#24341;&#23548;&#21512;&#25104; (SyGuS) &#31454;&#36187;&#30340;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03997v1 Announce Type: new  Abstract: Pre-trained Large Language Models (LLMs) are beginning to dominate the discourse around automatic code generation with natural language specifications. In contrast, the best-performing synthesizers in the domain of formal synthesis with precise logical specifications are still based on enumerative algorithms. In this paper, we evaluate the abilities of LLMs to solve formal synthesis benchmarks by carefully crafting a library of prompts for the domain. When one-shot synthesis fails, we propose a novel enumerative synthesis algorithm, which integrates calls to an LLM into a weighted probabilistic search. This allows the synthesizer to provide the LLM with information about the progress of the enumerator, and the LLM to provide the enumerator with syntactic guidance in an iterative loop. We evaluate our techniques on benchmarks from the Syntax-Guided Synthesis (SyGuS) competition. We find that GPT-3.5 as a stand-alone tool for formal synthe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#22312;&#36234;&#21335;&#35821;&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#22312;&#36234;&#21335;&#35821;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#33021;&#21147;&#65292;&#21516;&#26102;&#25351;&#20986;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#19982;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#30528;&#26435;&#34913;&#20851;&#31995;&#65292;&#35757;&#32451;&#25110;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#26159;&#24433;&#21709;LLM&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.02715</link><description>&lt;p&gt;
&#36328;&#36234;&#35821;&#35328;&#35270;&#37326;&#65306;&#36234;&#21335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02715
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#22312;&#36234;&#21335;&#35821;&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#22312;&#36234;&#21335;&#35821;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#33021;&#21147;&#65292;&#21516;&#26102;&#25351;&#20986;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#19982;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#30528;&#26435;&#34913;&#20851;&#31995;&#65292;&#35757;&#32451;&#25110;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#26159;&#24433;&#21709;LLM&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#65292;&#20294;&#30446;&#21069;&#24320;&#28304;&#30340;LLMs&#22312;&#22788;&#29702;&#36234;&#21335;&#35821;&#26041;&#38754;&#30340;&#25928;&#26524;&#26377;&#38480;&#12290;&#36825;&#19968;&#25361;&#25112;&#26159;&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#36234;&#21335;&#35821;LLM&#35780;&#20272;&#30340;&#31995;&#32479;&#21270;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#19987;&#38376;&#20026;&#36234;&#21335;&#35821;&#36827;&#34892;&#20102;LLM&#30340;&#24494;&#35843;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#28085;&#30422;10&#20010;&#24120;&#35265;&#20219;&#21153;&#21644;31&#20010;&#25351;&#26631;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#22312;&#36234;&#21335;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#22686;&#24378;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20855;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#24102;&#26469;&#26356;&#22810;&#30340;&#20559;&#35265;&#21644;&#26410;&#26657;&#20934;&#30340;&#36755;&#20986;&#65292;&#32780;&#24433;&#21709;LLM&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#35757;&#32451;&#25110;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02715v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics. Our evaluation results reveal that the fine-tuned LLMs exhibit enhanced comprehension and generative capabilities in Vietnamese. Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets. These insights und
&lt;/p&gt;</description></item><item><title>&#25919;&#31574;&#31354;&#38388;&#21709;&#24212;&#31070;&#35861;&#65288;PSRO&#65289;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#21338;&#24328;&#30340;&#24555;&#36895;&#21457;&#23637;&#30340;&#21338;&#24328;&#25512;&#29702;&#26694;&#26550;&#65292;&#20027;&#35201;&#20851;&#27880;&#31574;&#30053;&#25506;&#32034;&#38382;&#39064;&#21644;&#25552;&#39640;&#25928;&#29575;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.02227</link><description>&lt;p&gt;
&#25919;&#31574;&#31354;&#38388;&#21709;&#24212;&#31070;&#35861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Policy Space Response Oracles: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02227
&lt;/p&gt;
&lt;p&gt;
&#25919;&#31574;&#31354;&#38388;&#21709;&#24212;&#31070;&#35861;&#65288;PSRO&#65289;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#21338;&#24328;&#30340;&#24555;&#36895;&#21457;&#23637;&#30340;&#21338;&#24328;&#25512;&#29702;&#26694;&#26550;&#65292;&#20027;&#35201;&#20851;&#27880;&#31574;&#30053;&#25506;&#32034;&#38382;&#39064;&#21644;&#25552;&#39640;&#25928;&#29575;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21338;&#24328;&#35770;&#20013;&#65292;&#28216;&#25103;&#25351;&#30340;&#26159;&#29702;&#24615;&#20915;&#31574;&#32773;&#25110;&#29609;&#23478;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#22411;&#65292;&#20182;&#20204;&#36890;&#36807;&#36873;&#25321;&#26469;&#23454;&#29616;&#20010;&#20154;&#30446;&#26631;&#12290;&#20102;&#35299;&#20182;&#20204;&#22312;&#28216;&#25103;&#20013;&#30340;&#34892;&#20026;&#36890;&#24120;&#34987;&#31216;&#20026;&#21338;&#24328;&#25512;&#29702;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#28216;&#25103;&#30340;&#24555;&#36895;&#21457;&#23637;&#30340;&#21338;&#24328;&#25512;&#29702;&#26694;&#26550;&#65292;&#31216;&#20026;&#25919;&#31574;&#31354;&#38388;&#21709;&#24212;&#31070;&#35861;&#65288;PSRO&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#28608;&#21457;PSRO&#30340;&#21160;&#26426;&#65292;&#25552;&#20379;&#21382;&#21490;&#32972;&#26223;&#65292;&#24182;&#23558;PSRO&#23450;&#20301;&#22312;&#21338;&#24328;&#25512;&#29702;&#26041;&#27861;&#20013;&#12290;&#28982;&#21518;&#25105;&#20204;&#20851;&#27880;PSRO&#30340;&#31574;&#30053;&#25506;&#32034;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#26377;&#25928;&#32452;&#21512;&#31574;&#30053;&#32452;&#21512;&#20197;&#22312;&#27169;&#25311;&#28508;&#22312;&#28216;&#25103;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#30446;&#21069;&#29992;&#20110;&#22686;&#24378;PSRO&#25928;&#29575;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25506;&#35752;&#20102;PSRO&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02227v1 Announce Type: cross  Abstract: In game theory, a game refers to a model of interaction among rational decision-makers or players, making choices with the goal of achieving their individual objectives. Understanding their behavior in games is often referred to as game reasoning. This survey provides a comprehensive overview of a fast-developing game-reasoning framework for large games, known as Policy Space Response Oracles (PSRO). We first motivate PSRO, provide historical context, and position PSRO within game-reasoning approaches. We then focus on the strategy exploration issue for PSRO, the challenge of assembling an effective strategy portfolio for modeling the underlying game with minimum computational cost. We also survey current research directions for enhancing the efficiency of PSRO, and explore the applications of PSRO across various domains. We conclude by discussing open questions and future research.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02107</link><description>&lt;p&gt;
&#36845;&#20195;$Q$-&#32593;&#32476;&#65306;&#36229;&#36234;&#21333;&#27493;&#36125;&#23572;&#26364;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Iterated $Q$-Network: Beyond the One-Step Bellman Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02107
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#65292;&#35813;&#31639;&#23376;&#38656;&#35201;&#20174;&#26679;&#26412;&#20013;&#36827;&#34892;&#36817;&#20284;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#21253;&#25324;&#20132;&#26367;&#24212;&#29992;&#36125;&#23572;&#26364;&#31639;&#23376;&#21644;&#38543;&#21518;&#25237;&#24433;&#27493;&#39588;&#21040;&#32771;&#34385;&#30340;&#20989;&#25968;&#31354;&#38388;&#30340;&#36845;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#19968;&#31995;&#21015;$Q$&#20989;&#25968;&#36924;&#36817;&#65292;&#20854;&#20013;&#27599;&#20010;$Q$&#20989;&#25968;&#37117;&#20316;&#20026;&#19979;&#19968;&#20010;&#20989;&#25968;&#38142;&#20013;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;iQN&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#21487;&#20197;&#26080;&#32541;&#22320;&#29992;&#20110;&#20540;&#22522;&#21644;&#28436;&#21592;-&#35780;&#35770;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Atari$2600$&#28216;&#25103;&#21644;&#36830;&#32493;&#25511;&#21046;MuJoCo&#29615;&#22659;&#20013;&#22312;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02107v1 Announce Type: cross  Abstract: Value-based Reinforcement Learning (RL) methods rely on the application of the Bellman operator, which needs to be approximated from samples. Most approaches consist of an iterative scheme alternating the application of the Bellman operator and a subsequent projection step onto a considered function space. However, we observe that these algorithms can be improved by considering multiple iterations of the Bellman operator at once. Thus, we introduce iterated $Q$-Networks (iQN), a novel approach that learns a sequence of $Q$-function approximations where each $Q$-function serves as the target for the next one in a chain of consecutive Bellman iterations. We demonstrate that iQN is theoretically sound and show how it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate its advantages on Atari $2600$ games and in continuous-control MuJoCo environments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Self-Synthesized Rehearsal&#65288;SSR&#65289;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#22797;&#36848;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01244</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#25105;&#29983;&#25104;&#30340;&#22797;&#36848;&#26469;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01244
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Self-Synthesized Rehearsal&#65288;SSR&#65289;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#22797;&#36848;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#22797;&#36848;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20808;&#21069;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20445;&#30041;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;&#36825;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#25105;&#29983;&#25104;&#22797;&#36848;&#65288;SSR&#65289;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#36827;&#34892;&#22797;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01244v1 Announce Type: cross  Abstract: Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable pe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;IntactKV&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#20851;&#38190;&#26631;&#35760;&#30340;&#23436;&#25972;&#24615;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#37327;&#21270;&#35823;&#24046;&#30340;&#19978;&#38480;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.01241</link><description>&lt;p&gt;
IntactKV: &#36890;&#36807;&#20445;&#25345;&#20851;&#38190;&#26631;&#35760;&#23436;&#25972;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;IntactKV&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#20851;&#38190;&#26631;&#35760;&#30340;&#23436;&#25972;&#24615;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#37327;&#21270;&#35823;&#24046;&#30340;&#19978;&#38480;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#19968;&#38590;&#39064;&#65292;&#20154;&#20204;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#37327;&#21270;&#26041;&#27861;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#20250;&#25439;&#23475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19968;&#20010;&#20197;&#21069;&#34987;&#24573;&#35270;&#30340;&#24322;&#24120;&#28857;&#31867;&#22411;&#12290;&#36825;&#20123;&#24322;&#24120;&#28857;&#34987;&#21457;&#29616;&#23558;&#22823;&#37096;&#20998;&#27880;&#24847;&#21147;&#20998;&#37197;&#32473;&#36755;&#20837;&#30340;&#21021;&#22987;&#26631;&#35760;&#65292;&#34987;&#31216;&#20026;&#20851;&#38190;&#26631;&#35760;&#65292;&#36825;&#23545;&#20110;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IntactKV&#65292;&#20174;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20013;&#26080;&#25439;&#22320;&#29983;&#25104;&#20851;&#38190;&#26631;&#35760;&#30340;KV&#32531;&#23384;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#26131;&#34892;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#29616;&#26377;&#30340;&#37327;&#21270;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;IntactKV&#21487;&#20197;&#34987;&#26657;&#20934;&#20026;&#39069;&#22806;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25968;&#23398;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;IntactKV&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#37327;&#21270;&#35823;&#24046;&#30340;&#19978;&#38480;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;IntactKV&#24102;&#26469;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26080;&#25439;&#30340;&#20165;&#26435;&#37325;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01241v1 Announce Type: cross  Abstract: Large language models (LLMs) excel in natural language processing but demand intensive computation. To mitigate this, various quantization methods have been explored, yet they compromise LLM performance. This paper unveils a previously overlooked type of outlier in LLMs. Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which is crucial to the performance of quantized LLMs. Given that, we propose IntactKV to generate the KV cache of pivot tokens losslessly from the full-precision model. The approach is simple and easy to combine with existing quantization solutions. Besides, IntactKV can be calibrated as additional LLM parameters to boost the quantized LLMs further. Mathematical analysis also proves that IntactKV effectively reduces the upper bound of quantization error. Empirical results show that IntactKV brings consistent improvement and achieves lossless weight-only
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#38463;&#25289;&#20271;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;Peacock&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#21644;&#19981;&#26029;&#20986;&#29616;&#30340;&#26041;&#35328;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#26041;&#38754;&#30340;&#26032;&#22522;&#20934;Henna</title><link>https://arxiv.org/abs/2403.01031</link><description>&lt;p&gt;
&#23380;&#38592;&#65306;&#19968;&#31995;&#21015;&#38463;&#25289;&#20271;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#21450;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01031
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#38463;&#25289;&#20271;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;Peacock&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#21644;&#19981;&#26029;&#20986;&#29616;&#30340;&#26041;&#35328;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#26041;&#38754;&#30340;&#26032;&#22522;&#20934;Henna
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#21644;&#35821;&#35328;&#29702;&#35299;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38500;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24335;&#36164;&#28304;&#65292;MLLMs&#30340;&#25104;&#21151;&#20173;&#28982;&#30456;&#23545;&#23616;&#38480;&#20110;&#33521;&#35821;&#29615;&#22659;&#12290;&#36825;&#32473;&#24320;&#21457;&#20854;&#20182;&#35821;&#35328;&#30340;&#21487;&#27604;&#36739;&#27169;&#22411;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#29978;&#33267;&#21253;&#25324;&#37027;&#20123;&#25317;&#26377;&#24222;&#22823;&#35828;&#35805;&#20154;&#21475;&#30340;&#35821;&#35328;&#65292;&#22914;&#38463;&#25289;&#20271;&#35821;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#38463;&#25289;&#20271;MLLMs&#31995;&#21015;&#65292;&#31216;&#20026;\textit{Peacock}&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#33021;&#21147;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23427;&#20204;&#19981;&#26029;&#20986;&#29616;&#30340;&#26041;&#35328;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;\textit{Henna}&#30340;&#26032;&#22522;&#20934;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#19982;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01031v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) have proven effective in a wide range of tasks requiring complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, including even those with large speaker populations such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed \textit{Peacock}, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce ~\textit{Henna}, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic c
&lt;/p&gt;</description></item><item><title>LoRA&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LoRA&#19982;dropout&#26041;&#27861;&#22312;&#27169;&#22411;&#23450;&#21046;&#20013;&#30340;&#30683;&#30462;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;transformer-specific&#30340;dropout&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#25968;&#23398;&#21644;&#32463;&#39564;&#19978;&#30340;&#31561;&#20215;&#24615;&#21644;&#21306;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.00812</link><description>&lt;p&gt;
LoRA&#22312;&#32479;&#19968;&#26694;&#26550;&#19979;&#36935;&#35265;&#20102;Dropout
&lt;/p&gt;
&lt;p&gt;
LoRA Meets Dropout under a Unified Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00812
&lt;/p&gt;
&lt;p&gt;
LoRA&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LoRA&#19982;dropout&#26041;&#27861;&#22312;&#27169;&#22411;&#23450;&#21046;&#20013;&#30340;&#30683;&#30462;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;transformer-specific&#30340;dropout&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#25968;&#23398;&#21644;&#32463;&#39564;&#19978;&#30340;&#31561;&#20215;&#24615;&#21644;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#26174;&#33879;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#20803;&#32032;&#65292;&#32780;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#29305;&#21035;&#26159;LoRA&#65292;&#24050;&#32463;&#25104;&#20026;&#27169;&#22411;&#23450;&#21046;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#30340;&#27969;&#34892;&#36873;&#25321;&#12290;&#21516;&#26102;&#65292;&#21508;&#31181;dropout&#26041;&#27861;&#26368;&#21021;&#26159;&#20026;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#23436;&#25972;&#24494;&#35843;&#32780;&#35774;&#35745;&#30340;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#19982;&#36807;&#22810;&#21442;&#25968;&#20887;&#20313;&#30456;&#20851;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;LoRA&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#24494;&#19981;&#36275;&#36947;&#19982;&#20808;&#21069;dropout&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20043;&#38388;&#23384;&#22312;&#21487;&#33021;&#30340;&#30683;&#30462;&#65292;&#36825;&#19968;&#28857;&#20043;&#21069;&#22823;&#22810;&#34987;&#24573;&#35270;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#35748;&#39640;&#25928;&#21442;&#25968;&#30340;LoRA&#20063;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29305;&#23450;&#20110;transformer&#30340;dropout&#26041;&#27861;&#65292;&#20174;&#25968;&#23398;&#21644;&#32463;&#39564;&#19978;&#24314;&#31435;&#23427;&#20204;&#30340;&#31561;&#20215;&#24615;&#21644;&#21306;&#21035;&#12290;&#22522;&#20110;&#36825;&#31181;&#27604;&#36739;&#20998;&#26512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00812v1 Announce Type: cross  Abstract: With the remarkable capabilities, large language models (LLMs) have emerged as essential elements in numerous NLP applications, while parameter-efficient finetuning, especially LoRA, has gained popularity as a lightweight approach for model customization. Meanwhile, various dropout methods, initially designed for full finetuning with all the parameters updated, alleviates overfitting associated with excessive parameter redundancy. Hence, a possible contradiction arises from negligible trainable parameters of LoRA and the effectiveness of previous dropout methods, which has been largely overlooked. To fill this gap, we first confirm that parameter-efficient LoRA is also overfitting-prone. We then revisit transformer-specific dropout methods, and establish their equivalence and distinctions mathematically and empirically. Building upon this comparative analysis, we introduce a unified framework for a comprehensive investigation, which in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-Selector&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#35299;&#37322;&#65292;&#39044;&#27979;&#19981;&#21516;&#35299;&#37322;&#32452;&#21512;&#23545;&#29992;&#25143;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24341;&#23548;&#29992;&#25143;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.18016</link><description>&lt;p&gt;
&#21160;&#24577;&#35299;&#37322;&#36873;&#25321;&#65306;&#23454;&#29616;&#21487;&#35299;&#37322;AI&#30340;&#25104;&#21151;&#29992;&#25143;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Dynamic Explanation Selection Towards Successful User-Decision Support with Explainable AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-Selector&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#35299;&#37322;&#65292;&#39044;&#27979;&#19981;&#21516;&#35299;&#37322;&#32452;&#21512;&#23545;&#29992;&#25143;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24341;&#23548;&#29992;&#25143;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22914;&#20309;&#20026;&#22522;&#20110;&#21487;&#35299;&#37322;AI&#30340;&#26234;&#33021;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;(IDSSs)&#36873;&#25321;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;IDSSs&#36890;&#36807;&#21487;&#35299;&#37322;AI&#29983;&#25104;&#30340;&#35299;&#37322;&#20197;&#21450;AI&#39044;&#27979;&#23637;&#31034;&#20102;&#25552;&#39640;&#29992;&#25143;&#20915;&#31574;&#30340;&#28508;&#21147;&#12290;&#30001;&#20110;&#21487;&#35299;&#37322;AI&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#21508;&#31181;&#35299;&#37322;&#65292;&#25105;&#20204;&#35748;&#20026;&#22914;&#26524;&#33021;&#22815;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#25351;&#23548;&#29992;&#25143;&#20570;&#20986;&#26356;&#22909;&#20915;&#31574;&#30340;&#35299;&#37322;&#65292;IDSSs &#30340;&#24615;&#33021;&#23558;&#24471;&#21040;&#26497;&#22823;&#25552;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;X-Selector&#65292;&#19968;&#31181;&#21160;&#24577;&#36873;&#25321;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;X-Selector&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#39044;&#27979;&#19981;&#21516;&#35299;&#37322;&#32452;&#21512;&#23545;&#29992;&#25143;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24341;&#23548;&#29992;&#25143;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;X-Selector&#30340;&#24615;&#33021;&#19982;&#20004;&#31181;&#26420;&#32032;&#31574;&#30053;&#65288;&#25152;&#26377;&#21487;&#33021;&#30340;&#35299;&#37322;&#21644;&#20165;&#38024;&#23545;&#26368;&#21487;&#33021;&#39044;&#27979;&#30340;&#35299;&#37322;&#65289;&#12289;&#20197;&#21450;&#20004;&#31181;&#22522;&#32447;&#26041;&#27861;&#65288;&#26080;&#35299;&#37322;&#21644;&#26080;AI&#25903;&#25345;&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;X-Selector&#26377;&#28508;&#21147;&#24341;&#23548;&#29992;&#25143;&#20570;&#20986;&#25512;&#33616;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18016v1 Announce Type: cross  Abstract: This paper addresses the problem of how to select explanations for XAI (Explainable AI)-based Intelligent Decision Support Systems (IDSSs). IDSSs have shown promise in improving user decisions through XAI-generated explanations along with AI predictions. As the development of XAI made various explanations available, we believe that IDSSs can be greatly improved if they can strategically select explanations that guide users to better decisions. This paper proposes X-Selector, a method for dynamically selecting explanations. X-Selector aims to guide users to better decisions by predicting the impact of different combinations of explanations on user decisions. We compared X-Selector's performance with two naive strategies (all possible explanations and explanations only for the most likely prediction) and two baselines (no explanation and no AI support). The results suggest the potential of X-Selector to guide users to recommended decisio
&lt;/p&gt;</description></item><item><title>&#23558;&#21307;&#30103;&#20445;&#20581;&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24739;&#32773;&#19982;&#21307;&#30103;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#34920;&#31034;&#20026;&#20107;&#20214;&#27969;&#65292;&#23454;&#29616;&#23545;&#26410;&#26469;&#20107;&#20214;&#65288;&#22914;&#35786;&#26029;&#21644;&#27835;&#30103;&#36873;&#25321;&#65289;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.17501</link><description>&lt;p&gt;
&#37325;&#30151;&#30417;&#25252;&#20316;&#20026;&#19968;&#20010;&#22823;&#22411;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Intensive Care as One Big Sequence Modeling Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17501
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21307;&#30103;&#20445;&#20581;&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24739;&#32773;&#19982;&#21307;&#30103;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#34920;&#31034;&#20026;&#20107;&#20214;&#27969;&#65292;&#23454;&#29616;&#23545;&#26410;&#26469;&#20107;&#20214;&#65288;&#22914;&#35786;&#26029;&#21644;&#27835;&#30103;&#36873;&#25321;&#65289;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#28041;&#21450;&#29421;&#31364;&#30340;&#33258;&#21253;&#21547;&#20219;&#21153;&#65292;&#22914;&#33043;&#27602;&#30151;&#39044;&#27979;&#25110;&#40635;&#37257;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#29992;&#27169;&#22411;&#65288;&#20027;&#35201;&#31034;&#20363;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#20855;&#26377;&#36229;&#36234;&#29305;&#23450;&#20219;&#21153;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#38544;&#24335;&#36801;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#20445;&#20581;&#22522;&#30784;&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#20581;&#20316;&#20026;&#24207;&#21015;&#24314;&#27169;&#30340;&#33539;&#24335;&#65292;&#20854;&#20013;&#24739;&#32773;&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#34987;&#34920;&#31034;&#20026;&#20107;&#20214;&#27969;&#65292;&#35786;&#26029;&#21644;&#27835;&#30103;&#36873;&#25321;&#31561;&#20219;&#21153;&#34987;&#24314;&#27169;&#20026;&#23545;&#27969;&#20013;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#22312;&#23454;&#39564;&#20013;&#25506;&#32034;&#36825;&#19968;&#33539;&#24335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;MIMIC-SEQ&#65292;&#36825;&#26159;&#19968;&#20010;&#24207;&#21015;&#24314;&#27169;&#22522;&#20934;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;MIMIC-IV&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#20020;&#24202;&#35760;&#24405;&#36716;&#25442;&#20026;&#19968;&#31181;&#32479;&#19968;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17501v1 Announce Type: cross  Abstract: Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control. However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning. To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream. To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a un
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#35797;&#22270;&#35299;&#20915;&#20174;&#35797;&#39564;&#32467;&#26524;&#25512;&#24191;&#21040;&#30446;&#26631;&#31181;&#32676;&#30340;&#22806;&#37096;&#26377;&#25928;&#24615;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;</title><link>https://arxiv.org/abs/2402.17042</link><description>&lt;p&gt;
&#36890;&#21521;&#20174;&#35797;&#39564;&#25512;&#24191;&#25512;&#29702;&#21040;&#30446;&#26631;&#31181;&#32676;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizing Inferences from Trials to Target Populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#35797;&#22270;&#35299;&#20915;&#20174;&#35797;&#39564;&#32467;&#26524;&#25512;&#24191;&#21040;&#30446;&#26631;&#31181;&#32676;&#30340;&#22806;&#37096;&#26377;&#25928;&#24615;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCTs&#65289;&#22312;&#20135;&#29983;&#20869;&#37096;&#26377;&#25928;&#20272;&#35745;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32780;&#23545;&#25193;&#23637;&#36825;&#20123;&#21457;&#29616;&#20197;&#33719;&#24471;&#22806;&#37096;&#26377;&#25928;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#20419;&#36827;&#26356;&#24191;&#27867;&#30340;&#31185;&#23398;&#25506;&#31350;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24212;&#23545;&#36825;&#20123;&#22806;&#37096;&#26377;&#25928;&#24615;&#25361;&#25112;&#30340;&#21069;&#27839;&#65292;&#27010;&#25324;&#20102;2023&#24180;&#31179;&#23395;&#22312;&#24067;&#26391;&#22823;&#23398;&#35745;&#31639;&#19982;&#23454;&#39564;&#25968;&#23398;&#30740;&#31350;&#25152;&#65288;ICERM&#65289;&#20030;&#34892;&#30340;&#19968;&#27425;&#36328;&#23398;&#31185;&#30740;&#35752;&#20250;&#30340;&#31934;&#21326;&#12290;&#35813;&#30740;&#35752;&#20250;&#27719;&#38598;&#20102;&#26469;&#33258;&#31038;&#20250;&#31185;&#23398;&#12289;&#21307;&#23398;&#12289;&#20844;&#20849;&#21355;&#29983;&#12289;&#32479;&#35745;&#23398;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#19987;&#23478;&#65292;&#20197;&#35299;&#20915;&#27599;&#20010;&#23398;&#31185;&#22312;&#25512;&#26029;&#23454;&#39564;&#32467;&#26524;&#26041;&#38754;&#38754;&#20020;&#30340;&#29420;&#29305;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;&#25105;&#20204;&#25972;&#21512;&#27491;&#22312;&#36827;&#34892;&#30340;&#21162;&#21147;&#65292;&#31361;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17042v1 Announce Type: cross  Abstract: Randomized Controlled Trials (RCTs) are pivotal in generating internally valid estimates with minimal assumptions, serving as a cornerstone for researchers dedicated to advancing causal inference methods. However, extending these findings beyond the experimental cohort to achieve externally valid estimates is crucial for broader scientific inquiry. This paper delves into the forefront of addressing these external validity challenges, encapsulating the essence of a multidisciplinary workshop held at the Institute for Computational and Experimental Research in Mathematics (ICERM), Brown University, in Fall 2023. The workshop congregated experts from diverse fields including social science, medicine, public health, statistics, computer science, and education, to tackle the unique obstacles each discipline faces in extrapolating experimental findings. Our study presents three key contributions: we integrate ongoing efforts, highlighting me
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#24341;&#20837;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#26469;&#24341;&#23548;&#36825;&#19968;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21152;&#36895;&#23398;&#20064;&#24182;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16354</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Language-guided Skill Learning with Temporal Variational Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#24341;&#20837;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#26469;&#24341;&#23548;&#36825;&#19968;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21152;&#36895;&#23398;&#20064;&#24182;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#21457;&#29616;&#25216;&#33021;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25552;&#20986;&#36712;&#36857;&#30340;&#21021;&#22987;&#20998;&#21106;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#20998;&#23618;&#21464;&#20998;&#25512;&#26029;&#26694;&#26550;&#23558;LLM&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#36890;&#36807;&#21512;&#24182;&#36712;&#36857;&#27573;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25511;&#21046;&#21387;&#32553;&#21644;&#21487;&#37325;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#30340;&#26032;&#36741;&#21161;&#30446;&#26631;&#65292;&#24110;&#21161;&#24341;&#23548;&#36825;&#31181;&#25216;&#33021;&#21457;&#29616;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#30340;Agent&#33021;&#22815;&#21457;&#29616;&#26377;&#21161;&#20110;&#21152;&#36895;&#23398;&#20064;&#30340;&#25216;&#33021;&#65292;&#22312;BabyAI&#65288;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#23548;&#33322;&#29615;&#22659;&#65289;&#20197;&#21450;ALFRED&#65288;&#19968;&#20010;&#23478;&#24237;&#27169;&#25311;&#29615;&#22659;&#65289;&#30340;&#26032;&#38271;&#26399;&#20219;&#21153;&#20013;&#32988;&#36807;&#22522;&#32447;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16354v1 Announce Type: cross  Abstract: We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#36890;&#29992;&#25919;&#31574;&#65292;&#20197;&#25429;&#33719;&#22810;&#26679;&#21270;&#12289;&#26368;&#20248;&#12289;&#38271;&#26102;&#22495;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.15567</link><description>&lt;p&gt;
&#20855;&#26377;&#24076;&#23572;&#20271;&#29305;&#34920;&#31034;&#30340;&#22522;&#30784;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Foundation Policies with Hilbert Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15567
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#36890;&#29992;&#25919;&#31574;&#65292;&#20197;&#25429;&#33719;&#22810;&#26679;&#21270;&#12289;&#26368;&#20248;&#12289;&#38271;&#26102;&#22495;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15567v1 &#36827;&#34892;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#30446;&#26631;&#65292;&#20363;&#22914;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#65292;&#24050;&#32463;&#20351;&#24471;&#21487;&#20197;&#20174;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#25214;&#21040;&#19968;&#20010;&#30495;&#27491;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30446;&#26631;&#20197;&#33719;&#21462;&#36890;&#29992;&#25919;&#31574;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#23454;&#29616;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;RL&#65292;&#22522;&#20110;&#35832;&#22914;&#22522;&#20110;&#30446;&#26631;&#30340;RL&#12289;&#34892;&#20026;&#20811;&#38534;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#23398;&#20064;&#31561;&#21407;&#21017;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#21457;&#29616;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#12289;&#38656;&#35201;&#39640;&#36136;&#37327;&#28436;&#31034;&#25968;&#25454;&#25110;&#32570;&#20047;&#26126;&#30830;&#30340;&#25552;&#31034;&#25110;&#36866;&#24212;&#26426;&#21046;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#33021;&#22815;&#25429;&#25417;&#22810;&#26679;&#21270;&#12289;&#26368;&#20248;&#12289;&#38271;&#26102;&#22495;&#34892;&#20026;&#30340;&#36890;&#29992;&#25919;&#31574;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#33719;&#21462;&#36825;&#20123;&#34892;&#20026;&#65292;&#20197;&#20415;&#23427;&#20204;&#21487;&#20197;&#24555;&#36895;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15567v1 Announce Type: cross  Abstract: Unsupervised and self-supervised objectives, such as next token prediction, have enabled pre-training generalist models from large amounts of unlabeled data. In reinforcement learning (RL), however, finding a truly general and scalable unsupervised pre-training objective for generalist policies from offline data remains a major open question. While a number of methods have been proposed to enable generic self-supervised RL, based on principles such as goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such methods remain limited in terms of either the diversity of the discovered behaviors, the need for high-quality demonstration data, or the lack of a clear prompting or adaptation mechanism for downstream tasks. In this work, we propose a novel unsupervised framework to pre-train generalist policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline data such that they can be quickly ada
&lt;/p&gt;</description></item><item><title>&#33258;&#20462;&#22797;&#29616;&#35937;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#65292;&#20294;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#26159;&#19981;&#23436;&#32654;&#21644;&#22024;&#26434;&#30340;&#65292;&#26377;&#20004;&#31181;&#26426;&#21046;&#21487;&#20419;&#25104;&#33258;&#20462;&#22797;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#21644;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.15390</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#20462;&#22797;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explorations of Self-Repair in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15390
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20462;&#22797;&#29616;&#35937;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#65292;&#20294;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#26159;&#19981;&#23436;&#32654;&#21644;&#22024;&#26434;&#30340;&#65292;&#26377;&#20004;&#31181;&#26426;&#21046;&#21487;&#20419;&#25104;&#33258;&#20462;&#22797;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#21644;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#29421;&#31364;&#20998;&#24067;&#30340;&#21487;&#35299;&#37322;&#24615;&#21457;&#29616;&#20102;&#33258;&#20462;&#22797;&#29616;&#35937;&#65292;&#21363;&#22914;&#26524;&#21093;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#20214;&#65292;&#21518;&#32493;&#32452;&#20214;&#20250;&#25913;&#21464;&#20854;&#34892;&#20026;&#20197;&#36827;&#34892;&#34917;&#20607;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#36825;&#20123;&#36807;&#21435;&#30340;&#25991;&#29486;&#65292;&#23637;&#31034;&#20102;&#24403;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#21093;&#31163;&#21333;&#20010;&#27880;&#24847;&#21147;&#22836;&#26102;&#65292;&#33258;&#20462;&#22797;&#23384;&#22312;&#20110;&#21508;&#31181;&#27169;&#22411;&#23478;&#26063;&#21644;&#23610;&#23544;&#19978;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22312;&#23436;&#25972;&#30340;&#35757;&#32451;&#20998;&#24067;&#19978;&#65292;&#33258;&#20462;&#22797;&#26159;&#19981;&#23436;&#32654;&#30340;&#65292;&#22240;&#20026;&#22836;&#37096;&#30340;&#21407;&#22987;&#30452;&#25509;&#25928;&#26524;&#24182;&#26410;&#23436;&#20840;&#24674;&#22797;&#65292;&#24182;&#19988;&#26159;&#22024;&#26434;&#30340;&#65292;&#22240;&#20026;&#33258;&#20462;&#22797;&#31243;&#24230;&#22312;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#26174;&#33879;&#21464;&#21270;&#65288;&#26377;&#26102;&#36229;&#36807;&#21407;&#22987;&#25928;&#26524;&#65289;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20419;&#25104;&#33258;&#20462;&#22797;&#30340;&#20004;&#31181;&#19981;&#21516;&#26426;&#21046;&#65292;&#21253;&#25324;&#26368;&#32456;LayerNorm&#32553;&#25918;&#22240;&#23376;&#30340;&#21464;&#21270;&#65288;&#21487;&#20462;&#22797;&#30452;&#25509;&#25928;&#26524;&#30340;30%&#65289;&#20197;&#21450;&#23454;&#29616;&#21453;&#25830;&#38500;&#30340;&#31232;&#30095;&#31070;&#32463;&#20803;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15390v1 Announce Type: cross  Abstract: Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor (which can repair up to 30% of the direct effect) and sparse sets of neurons implementing Anti-Erasure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23567;&#35268;&#27169;&#31034;&#20363;&#19978;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14992</link><description>&lt;p&gt;
&#23567;&#22411;&#22522;&#20934;&#27979;&#35797;&#65306;&#29992;&#26356;&#23569;&#30340;&#31034;&#20363;&#35780;&#20272;LLM
&lt;/p&gt;
&lt;p&gt;
tinyBenchmarks: evaluating LLMs with fewer examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23567;&#35268;&#27169;&#31034;&#20363;&#19978;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22810;&#21151;&#33021;&#24615;&#23548;&#33268;&#21019;&#24314;&#20102;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#65292;&#24443;&#24213;&#27979;&#35797;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#31034;&#20363;&#65292;&#20351;&#24471;&#35780;&#20272;LLMs&#38750;&#24120;&#26114;&#36149;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20943;&#23569;&#35780;&#20272;LLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#35780;&#20272;&#27425;&#25968;&#30340;&#31574;&#30053;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35201;&#20934;&#30830;&#20272;&#35745;LLMs&#22312;MMLU&#19978;&#30340;&#24615;&#33021;&#65288;&#19968;&#20010;&#21253;&#21547;14K&#20010;&#31034;&#20363;&#30340;&#27969;&#34892;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#65289;&#65292;&#21482;&#38656;&#35201;&#22312;100&#20010;&#31934;&#24515;&#25361;&#36873;&#30340;&#31034;&#20363;&#19978;&#35780;&#20272;&#36825;&#20010;LLMs&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35780;&#20272;&#24037;&#20855;&#21644;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#24494;&#22411;&#29256;&#26412;&#65306;Open LLM Leaderboard&#12289;MMLU&#12289;HELM&#21644;AlpacaEval 2.0&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#24037;&#20855;&#21644;&#24494;&#22411;&#22522;&#20934;&#27979;&#35797;&#36275;&#20197;&#21487;&#38752;&#19988;&#39640;&#25928;&#22320;&#37325;&#29616;&#21407;&#22987;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14992v1 Announce Type: cross  Abstract: The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models' abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;DLM&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;</title><link>https://arxiv.org/abs/2402.14807</link><description>&lt;p&gt;
&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;&#20013;&#21160;&#24577;&#19981;&#23433;&#38745;&#22810;&#33218;&#32769;&#34382;&#26426;&#20219;&#21153;&#30340;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;&#65288;DLM&#65289;
&lt;/p&gt;
&lt;p&gt;
A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;DLM&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#38477;&#20302;&#23381;&#20135;&#22919;&#27515;&#20129;&#29575;&#30340;&#21162;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39044;&#38450;&#20445;&#20581;&#35745;&#21010;&#65292;&#21521;&#39640;&#39118;&#38505;&#20154;&#32676;&#20256;&#25773;&#37325;&#35201;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DLM&#65306;&#19968;&#31181;&#29992;&#20110;RMAB&#30340;&#20915;&#31574;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLMs&#20316;&#20026;&#33258;&#21160;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#24494;&#35843;RMAB&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#21355;&#29983;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14807v1 Announce Type: cross  Abstract: Efforts to reduce maternal mortality rate, a key UN Sustainable Development target (SDG Target 3.1), rely largely on preventative care programs to spread critical health information to high-risk populations. These programs face two important challenges: efficiently allocating limited health resources to large beneficiary populations, and adapting to evolving policy priorities. While prior works in restless multi-armed bandit (RMAB) demonstrated success in public health allocation tasks, they lack flexibility to adapt to evolving policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept, automated planners in various domains, including robotic control and navigation. In this paper, we propose DLM: a Decision Language Model for RMABs. To enable dynamic fine-tuning of RMAB policies for challenging public health settings using human-language commands, we propose using LLMs as automated planners to (1) interpret hu
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14710</link><description>&lt;p&gt;
IEPile: &#25366;&#25496;&#22823;&#35268;&#27169;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14710
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#26159;&#25552;&#21319;LLMs&#29305;&#23450;&#33021;&#21147;&#30340;&#20851;&#38190;&#65292;&#32780;&#24403;&#21069;&#30340;IE&#25968;&#25454;&#38598;&#24448;&#24448;&#35268;&#27169;&#36739;&#23567;&#12289;&#20998;&#25955;&#19988;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IEPile&#65292;&#19968;&#20010;&#32508;&#21512;&#30340;&#21452;&#35821;&#65288;&#33521;&#25991;&#21644;&#20013;&#25991;&#65289;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#26500;&#24314;IEPile&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#26469;&#25366;&#25496;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12290;&#22312;LLaMA&#21644;Baichuan&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;IEPile&#21487;&#20197;&#25552;&#39640;LLMs&#22312;IE&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#36164;&#28304;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24076;&#26395;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13777</link><description>&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65306;&#25945;&#31243;&#12289;&#35843;&#26597;&#21644;&#26410;&#26469;&#26041;&#21521;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13777
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#20174;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#26041;&#38754;&#12290;&#31867;&#20284;&#22320;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#20063;&#38656;&#35201;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#29983;&#25104;&#20989;&#25968;&#20316;&#20026;&#31574;&#30053;&#25110;&#25919;&#31574;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#35768;&#22810;&#30740;&#31350;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#22240;&#27492;&#19981;&#21516;&#20998;&#25903;&#30340;&#21457;&#23637;&#30456;&#23545;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#24212;&#29992;&#26041;&#38754;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#24615;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#24402;&#19968;&#21270;&#27969;&#12289;&#21464;&#21387;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25345;&#32493;&#39044;&#35757;&#32451;&#25991;&#26723;&#20043;&#21069;&#26292;&#38706;LLM&#21040;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#20197;&#20415;&#20174;&#22797;&#26434;&#25991;&#26723;&#20013;&#32534;&#30721;&#30693;&#35782;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#30693;&#35782;&#35775;&#38382;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.12847</link><description>&lt;p&gt;
&#35843;&#25972;&#36807;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#30693;&#35782;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Language Models are Better Knowledge Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25345;&#32493;&#39044;&#35757;&#32451;&#25991;&#26723;&#20043;&#21069;&#26292;&#38706;LLM&#21040;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#20197;&#20415;&#20174;&#22797;&#26434;&#25991;&#26723;&#20013;&#32534;&#30721;&#30693;&#35782;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#30693;&#35782;&#35775;&#38382;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21161;&#25163;&#33021;&#22815;&#26377;&#25928;&#22320;&#36866;&#24212;&#19981;&#26029;&#21457;&#23637;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#24517;&#39035;&#33021;&#22815;&#36890;&#36807;&#25345;&#32493;&#22312;&#26032;&#25968;&#25454;&#19978;&#35757;&#32451;&#26469;&#26356;&#26032;&#23427;&#20204;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#20256;&#32479;&#20570;&#27861;&#28041;&#21450;&#22312;&#26032;&#25991;&#26723;&#19978;&#25345;&#32493;&#39044;&#22521;&#35757;&#65292;&#28982;&#21518;&#26681;&#25454;&#38382;&#39064;-&#31572;&#26696;&#65288;QA&#65289;&#23545;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12847v1 Announce Type: cross  Abstract: In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a met
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22826;&#38451;&#33021;&#38754;&#26495;&#20998;&#21106;&#38382;&#39064;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#20943;&#23569;&#20102;&#23545;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2402.12843</link><description>&lt;p&gt;
&#22826;&#38451;&#33021;&#38754;&#26495;&#20998;&#21106;:&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#19981;&#23436;&#32654;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12843
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22826;&#38451;&#33021;&#38754;&#26495;&#20998;&#21106;&#38382;&#39064;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#20943;&#23569;&#20102;&#23545;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22826;&#38451;&#33021;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#38656;&#35201;&#20808;&#36827;&#30340;&#30417;&#25511;&#21644;&#32500;&#25252;&#26041;&#27861;&#26469;&#30830;&#20445;&#22826;&#38451;&#33021;&#38754;&#26495;&#23433;&#35013;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#20174;&#33322;&#31354;&#25110;&#21355;&#26143;&#22270;&#20687;&#20013;&#20934;&#30830;&#20998;&#21106;&#22826;&#38451;&#33021;&#38754;&#26495;&#65292;&#36825;&#23545;&#20110;&#35782;&#21035;&#36816;&#34892;&#38382;&#39064;&#21644;&#35780;&#20272;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#38754;&#26495;&#20998;&#21106;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#20197;&#21450;&#25163;&#21160;&#26631;&#27880;&#23545;&#30417;&#30563;&#23398;&#20064;&#30340;&#21171;&#21160;&#23494;&#38598;&#24615;&#12290;&#25105;&#20204;&#25506;&#35752;&#24182;&#24212;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#35777;&#26126;SSL&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20943;&#23569;&#20102;&#23545;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#20026;&#20581;&#22766;&#19988;&#36866;&#24212;&#24615;&#24378;&#30340;&#22826;&#38451;&#33021;&#38754;&#26495;&#20998;&#21106;&#35299;&#20915;&#26041;&#26696;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12843v1 Announce Type: cross  Abstract: The increasing adoption of solar energy necessitates advanced methodologies for monitoring and maintenance to ensure optimal performance of solar panel installations. A critical component in this context is the accurate segmentation of solar panels from aerial or satellite imagery, which is essential for identifying operational issues and assessing efficiency. This paper addresses the significant challenges in panel segmentation, particularly the scarcity of annotated data and the labour-intensive nature of manual annotation for supervised learning. We explore and apply Self-Supervised Learning (SSL) to solve these challenges. We demonstrate that SSL significantly enhances model generalization under various conditions and reduces dependency on manually annotated data, paving the way for robust and adaptable solar panel segmentation solutions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#28145;&#24230;RL&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12275</link><description>&lt;p&gt;
WorldCoder&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#65306;&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12275
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#28145;&#24230;RL&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26500;&#24314;&#20195;&#34920;&#20854;&#23545;&#19990;&#30028;&#30693;&#35782;&#30340;Python&#31243;&#24207;&#12290;&#35813;&#19990;&#30028;&#27169;&#22411;&#35797;&#22270;&#35299;&#37322;&#20854;&#20132;&#20114;&#65292;&#21516;&#26102;&#23545;&#33258;&#24049;&#33021;&#22815;&#33719;&#24471;&#30340;&#22870;&#21169;&#25345;&#20048;&#35266;&#24577;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;LLM&#30340;&#31243;&#24207;&#21512;&#25104;&#24037;&#20316;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#32593;&#26684;&#19990;&#30028;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#27604;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26356;&#39640;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#27604;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12275v1 Announce Type: new  Abstract: We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.
&lt;/p&gt;</description></item><item><title>Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12146</link><description>&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12146
&lt;/p&gt;
&lt;p&gt;
Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#20219;&#21153;&#20013;&#23637;&#29616;&#24378;&#22823;&#24615;&#33021;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#31561;&#21487;&#38752;&#24615;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20687;GPT-4&#36825;&#26679;&#39640;&#33021;&#21147;&#30340;LLMs&#22312;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#32780;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#36890;&#24120;&#34987;&#35843;&#25972;&#26469;&#35780;&#20272;&#23545;&#30456;&#21516;&#26597;&#35810;&#30340;&#21709;&#24212;&#30340;&#30456;&#23545;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\textit{Meta}$ $\textit{Ranking}$&#65288;MR&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20808;&#21069;&#30452;&#25509;&#35780;&#20272;&#21709;&#24212;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#36827;&#34892;&#27604;&#36739;&#26469;&#23454;&#29616;&#21028;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#21709;&#24212;&#30340;&#38169;&#35823;&#26816;&#27979;&#20013;&#65292;MR&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#20063;&#33021;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;MR&#21487;&#20197;&#34987;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#22312;LLM&#20013;&#24341;&#20837;&#27714;&#35299;&#22120;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;</title><link>https://arxiv.org/abs/2402.11903</link><description>&lt;p&gt;
SoLA: &#20026;&#20102;&#26356;&#22909;&#30340;&#36923;&#36753;&#25512;&#29702;&#32780;&#23545;LLM&#36827;&#34892;&#27714;&#35299;&#23618;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11903
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#22312;LLM&#20013;&#24341;&#20837;&#27714;&#35299;&#22120;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36923;&#36753;&#25512;&#29702;&#19978;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#35797;&#22270;&#36890;&#36807;&#24037;&#20855;&#23398;&#20064;&#26469;&#25913;&#21464;&#38382;&#39064;&#27714;&#35299;&#12290;&#34429;&#28982;&#22312;&#23567;&#35268;&#27169;&#38382;&#39064;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#35268;&#27169;&#24222;&#22823;&#19988;&#34920;&#36798;&#22797;&#26434;&#65292;&#35299;&#20915;&#24037;&#19994;&#26696;&#20363;&#20173;&#28982;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;LLM&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#27714;&#35299;&#22120;&#20316;&#20026;&#26032;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;&#12290;&#22312;SoLA&#20013;&#65292;LLM&#26088;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#35782;&#21035;&#26368;&#39640;&#36136;&#37327;&#30340;&#23616;&#37096;&#35299;&#65292;&#32780;&#27714;&#35299;&#22120;&#23618;&#21017;&#19987;&#27880;&#20110;&#21021;&#22987;&#35299;&#19981;&#28385;&#36275;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#20511;&#21161;MaxSAT&#20316;&#20026;&#26725;&#26753;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#26799;&#24230;&#65292;&#20351;&#26368;&#32456;&#27169;&#22411;&#33021;&#22815;&#25910;&#25947;&#21040;&#19968;&#20010;&#28385;&#36275;&#30340;&#35299;&#25110;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#12290;&#21518;&#38376;&#29702;&#35770;&#30830;&#20445;SoLA&#33021;&#22815;&#33719;&#24471;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11903v1 Announce Type: cross  Abstract: Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurat
&lt;/p&gt;</description></item><item><title>Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11463</link><description>&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21560;&#24341;&#23376;&#35760;&#24518;&#65306;&#28151;&#27788;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11463
&lt;/p&gt;
&lt;p&gt;
Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24573;&#35270;&#20102;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#28304;&#33258;&#28508;&#22312;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#23548;&#33268;&#32570;&#20047;&#22806;&#25512;&#21644;&#28436;&#21270;&#33021;&#21147;&#12290; &#37492;&#21035;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#28151;&#27788;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;\textbf{\textit{Attraos}}&#23558;&#28151;&#27788;&#29702;&#35770;&#34701;&#20837;&#21040;LTSF&#20013;&#65292;&#23558;&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#26410;&#30693;&#39640;&#32500;&#28151;&#27788;&#21160;&#24577;&#31995;&#32479;&#30340;&#35266;&#27979;&#12290; &#22312;&#21560;&#24341;&#23376;&#19981;&#21464;&#24615;&#30340;&#27010;&#24565;&#19979;&#65292;Attraos&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#26469;&#35760;&#24518;&#21382;&#21490;&#21160;&#24577;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#22686;&#24378;&#30340;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#36827;&#34892;&#39044;&#27979;&#12290; &#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#20016;&#23500;&#30340;&#32463;&#39564;&#35777;&#25454;&#19968;&#33268;&#34920;&#26126;&#65292;Attraos&#22312;&#20027;&#27969;LTSF&#25968;&#25454;&#38598;&#21644;&#28151;&#27788;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#21508;&#31181;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#32593;&#26684;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#24179;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#20132;&#28857;&#30340;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10403</link><description>&lt;p&gt;
&#20174;&#20998;&#27573;&#19977;&#32447;&#24615;&#32593;&#32476;&#20013;&#23548;&#20986;&#22810;&#38754;&#20307;&#22797;&#21512;&#20307;
&lt;/p&gt;
&lt;p&gt;
Polyhedral Complex Derivation from Piecewise Trilinear Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#32593;&#26684;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#24179;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#20132;&#28857;&#30340;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35270;&#21270;&#30340;&#36827;&#23637;&#25581;&#31034;&#20102;&#23427;&#20204;&#32467;&#26500;&#30340;&#35265;&#35299;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#36830;&#32493;&#20998;&#27573;&#20223;&#23556;&#65288;CPWA&#65289;&#20989;&#25968;&#20013;&#25552;&#21462;&#32593;&#26684;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#34920;&#38754;&#34920;&#31034;&#23398;&#20064;&#30340;&#21457;&#23637;&#21253;&#25324;&#38750;&#32447;&#24615;&#20301;&#32622;&#32534;&#30721;&#65292;&#35299;&#20915;&#20102;&#35832;&#22914;&#35889;&#20559;&#24046;&#20043;&#31867;&#30340;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#36825;&#22312;&#24212;&#29992;&#22522;&#20110;CPWA&#20989;&#25968;&#30340;&#32593;&#26684;&#25552;&#21462;&#25216;&#26415;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#30340;&#32593;&#26684;&#25552;&#21462;&#65292;&#23637;&#31034;&#20102;&#22312;&#22855;&#25343;&#23572;&#32422;&#26463;&#19979;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#19977;&#32447;&#24615;&#21306;&#22495;&#20869;&#30340;&#24179;&#38754;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36817;&#20284;&#19977;&#20010;&#39640;&#32500;&#26354;&#38754;&#20043;&#38388;&#30340;&#20132;&#28857;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#27721;&#26126;&#36317;&#31163;&#21644;&#25928;&#29575;&#20197;&#21450;&#35282;&#36317;&#31163;&#26469;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#27491;&#30830;&#24615;&#21644;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#26816;&#26597;&#20102;t&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10403v1 Announce Type: cross  Abstract: Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#25968;&#25454;&#24211;&#19978;&#20351;&#29992;Reg-GXPath&#34920;&#36798;&#24335;&#20316;&#20026;&#23436;&#25972;&#24615;&#32422;&#26463;&#65292;&#35745;&#31639;&#21253;&#21547;&#25968;&#25454;&#20540;&#30340;&#20248;&#20808;&#20462;&#22797;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#20559;&#22909;&#20934;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.09265</link><description>&lt;p&gt;
&#25968;&#25454;&#22270;&#19978;&#39318;&#36873;&#23376;&#38598;&#20462;&#22797;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Computational Complexity of Preferred Subset Repairs on Data-Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#25968;&#25454;&#24211;&#19978;&#20351;&#29992;Reg-GXPath&#34920;&#36798;&#24335;&#20316;&#20026;&#23436;&#25972;&#24615;&#32422;&#26463;&#65292;&#35745;&#31639;&#21253;&#21547;&#25968;&#25454;&#20540;&#30340;&#20248;&#20808;&#20462;&#22797;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#20559;&#22909;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24211;&#29702;&#35770;&#21644;&#30693;&#35782;&#34920;&#31034;&#19982;&#25512;&#29702;&#39046;&#22495;&#19968;&#30452;&#20197;&#26469;&#37117;&#23384;&#22312;&#35299;&#20915;&#19981;&#19968;&#33268;&#30693;&#35782;&#24211;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#29305;&#21035;&#20851;&#27880;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#35282;&#24230;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29616;&#23454;&#39046;&#22495;&#20013;&#21487;&#29992;&#25968;&#25454;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#21644;&#20114;&#30456;&#20851;&#32852;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#38656;&#35201;&#24320;&#21457;&#26032;&#31867;&#22411;&#30340;&#23384;&#20648;&#24211;&#12289;&#34920;&#31034;&#35821;&#35328;&#21644;&#35821;&#20041;&#20197;&#23454;&#29616;&#26356;&#21512;&#36866;&#30340;&#26597;&#35810;&#21644;&#25512;&#29702;&#26041;&#24335;&#12290;&#22270;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#34920;&#31034;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#21644;&#26597;&#35810;&#36825;&#20123;&#36830;&#25509;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#22522;&#20110;Reg-GXPath&#34920;&#36798;&#24335;&#30340;&#19968;&#33268;&#24615;&#27010;&#24565;&#20316;&#20026;&#23436;&#25972;&#24615;&#32422;&#26463;&#65292;&#35745;&#31639;&#21253;&#21547;&#25968;&#25454;&#20540;&#30340;&#22270;&#25968;&#25454;&#24211;&#19978;&#30340;&#20248;&#20808;&#20462;&#22797;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26631;&#20934;&#23376;&#38598;&#20462;&#22797;&#35821;&#20041;&#30340;&#20960;&#31181;&#20559;&#22909;&#20934;&#21017;&#65292;&#21253;&#25324;&#26435;&#37325;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09265v1 Announce Type: cross Abstract: The problem of repairing inconsistent knowledge bases has a long history within the communities of database theory and knowledge representation and reasoning, especially from the perspective of structured data. However, as the data available in real-world domains becomes more complex and interconnected, the need naturally arises for developing new types of repositories, representation languages, and semantics, to allow for more suitable ways to query and reason about it. Graph databases provide an effective way to represent relationships among semi-structured data, and allow processing and querying these connections efficiently. In this work, we focus on the problem of computing prioritized repairs over graph databases with data values, using a notion of consistency based on Reg-GXPath expressions as integrity constraints. We present several preference criteria based on the standard subset repair semantics, incorporating weights, multis
&lt;/p&gt;</description></item><item><title>&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;</title><link>https://arxiv.org/abs/2402.08062</link><description>&lt;p&gt;
&#36991;&#20813;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#28798;&#38590;&#65306;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Avoiding Catastrophe in Continuous Spaces by Asking for Help
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08062
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20855;&#26377;&#27491;&#24335;&#36951;&#25022;&#20445;&#35777;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20551;&#35774;&#25152;&#26377;&#38169;&#35823;&#37117;&#26159;&#21487;&#36870;&#30340;&#65292;&#24182;&#20381;&#36182;&#20110;&#23581;&#35797;&#25152;&#26377;&#21487;&#33021;&#30340;&#36873;&#39033;&#12290;&#24403;&#19968;&#20123;&#38169;&#35823;&#26159;&#26080;&#27861;&#20462;&#22797;&#29978;&#33267;&#26159;&#28798;&#38590;&#24615;&#30340;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#31967;&#31957;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#21457;&#29983;&#28798;&#38590;&#30340;&#27010;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#36718;&#30340;&#22238;&#25253;&#20195;&#34920;&#20102;&#22312;&#35813;&#36718;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65292;&#24182;&#23581;&#35797;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#20056;&#31215;&#65288;&#24635;&#20307;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65289;&#12290;&#20026;&#20102;&#32473; agent &#19968;&#20123;&#25104;&#21151;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20801;&#35768;&#26377;&#38480;&#27425;&#21521;&#23548;&#24072;&#25552;&#38382;&#65292;&#24182;&#20551;&#35774;&#22238;&#25253;&#20989;&#25968;&#20026; Lipschitz &#36830;&#32493;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#24403;&#26102;&#38388;&#36328;&#24230;&#22686;&#38271;&#26102;&#65292;&#23427;&#30340;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110; 0&#65292;&#20551;&#35774;&#26159;&#19968;&#20010;&#36830;&#32493;&#30340; 1D &#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;"&#31616;&#21333;"&#30340;&#22238;&#25253;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21305;&#37197;&#30340;&#19979;&#30028;&#65306;&#22312;&#27809;&#26377;&#31616;&#21333;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#20309;&#31639;&#27861;&#35201;&#20040;&#19981;&#26029;&#26597;&#35810;&#24322;&#24120;&#30340;&#34892;&#20026;&#65292;&#35201;&#20040;&#27599;&#27425;&#26597;&#35810;&#23436;&#20840;&#30456;&#21516;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively "simple" payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.05785</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#23398;&#20064;&#19978;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Transformer Language Models on Algorithmic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05785
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#35201;&#27714;&#32452;&#21512;&#22810;&#20010;&#31163;&#25955;&#23376;&#20219;&#21153;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLaMA&#27169;&#22411;&#21644;&#22312;GPT-4&#21644;Gemini&#19978;&#25552;&#31034;&#26469;&#34913;&#37327;&#23398;&#20064;&#23398;&#20064;&#21407;&#35821;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#35268;&#27169;&#26041;&#38754;&#27604;&#20026;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#25928;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#28010;&#36153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05643</link><description>&lt;p&gt;
&#36890;&#36807;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Token-Based World Models with Parallel Observation Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;Transformer&#24212;&#29992;&#20110;&#31163;&#25955;&#31526;&#21495;&#24207;&#21015;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;TBWMs&#65289;&#20316;&#20026;&#39640;&#25928;&#26679;&#26412;&#26041;&#27861;&#12290;&#22312;TBWMs&#20013;&#65292;&#19990;&#30028;&#27169;&#22411;&#23558;&#20195;&#29702;&#32463;&#39564;&#20316;&#20026;&#19968;&#31181;&#31867;&#20284;&#35821;&#35328;&#30340;&#20196;&#29260;&#24207;&#21015;&#36827;&#34892;&#28040;&#32791;&#65292;&#20854;&#20013;&#27599;&#20010;&#35266;&#27979;&#26500;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#22312;&#24819;&#35937;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#20196;&#29260;&#36880;&#20010;&#29983;&#25104;&#19979;&#19968;&#20010;&#35266;&#27979;&#30340;&#20018;&#34892;&#26041;&#24335;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#38271;&#12289;GPU&#21033;&#29992;&#29575;&#20302;&#21644;&#34920;&#31034;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#65288;POP&#65289;&#26426;&#21046;&#12290;POP&#36890;&#36807;&#19968;&#31181;&#38024;&#23545;&#25105;&#20204;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#35774;&#35745;&#30340;&#26032;&#22411;&#21069;&#21521;&#27169;&#24335;&#26469;&#25193;&#20805;&#20102;&#20445;&#25345;&#32593;&#32476;&#65288;RetNet&#65289;&#12290;&#25105;&#20204;&#23558;POP&#38598;&#25104;&#21040;&#19968;&#31181;&#21517;&#20026;REM&#65288;&#20445;&#25345;&#29615;&#22659;&#27169;&#22411;&#65289;&#30340;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#65292;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#30340;TBWMs&#24555;15.4&#20493;&#30340;&#24819;&#35937;&#33021;&#21147;&#12290;REM&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;26&#20010;&#28216;&#25103;&#20013;&#30340;12&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23436;&#25104;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.04520</link><description>&lt;p&gt;
&#20851;&#20110;&#29616;&#20195;Hopfield&#27169;&#22411;&#35745;&#31639;&#38480;&#21046;&#30340;&#19968;&#20010;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04520
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;&#27169;&#24335;&#30340;&#33539;&#25968;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#25928;&#29575;&#36827;&#34892;&#30456;&#21464;&#34892;&#20026;&#30340;&#21051;&#30011;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#36755;&#20837;&#26597;&#35810;&#27169;&#24335;&#21644;&#35760;&#24518;&#27169;&#24335;&#30340;&#33539;&#25968;&#30340;&#19978;&#30028;&#26631;&#20934;&#12290;&#20165;&#22312;&#36825;&#20010;&#26631;&#20934;&#20043;&#19979;&#65292;&#20551;&#35774;&#28385;&#36275;Strong Exponential Time Hypothesis (SETH)&#65292;&#23384;&#22312;&#23376;&#20108;&#27425;&#65288;&#39640;&#25928;&#65289;&#21464;&#20307;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24403;&#26377;&#25928;&#26631;&#20934;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26377;&#25928;&#26500;&#36896;&#30340;&#27491;&#24335;&#31034;&#20363;&#12290;&#36825;&#21253;&#25324;&#19968;&#20010;&#35745;&#31639;&#26102;&#38388;&#30340;&#19979;&#30028;&#23548;&#20986;&#65292;&#19982;$\Max\{$&#23384;&#20648;&#30340;&#35760;&#24518;&#27169;&#24335;&#25968;&#37327;&#65292;&#36755;&#20837;&#26597;&#35810;&#24207;&#21015;&#30340;&#38271;&#24230;$\}$&#32447;&#24615;&#32553;&#25918;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
&lt;/p&gt;</description></item><item><title>ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03848</link><description>&lt;p&gt;
ANLS* -- &#19968;&#31181;&#36866;&#29992;&#20110;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANLS* -- A Universal Document Processing Metric for Generative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03848
&lt;/p&gt;
&lt;p&gt;
ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22312;&#25991;&#26723;&#20998;&#31867;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#20219;&#21153;&#20013;&#65292;&#21306;&#20998;&#27169;&#22411;&#19968;&#30452;&#26159;&#20027;&#35201;&#36873;&#25321;&#12290;&#36825;&#20123;&#27169;&#22411;&#20570;&#20986;&#30340;&#39044;&#27979;&#21487;&#20197;&#20998;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#39044;&#23450;&#20041;&#31867;&#21035;&#65292;&#20415;&#20110;&#36827;&#34892;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#65292;&#24182;&#33021;&#30452;&#25509;&#35745;&#31639;F1&#20998;&#25968;&#31561;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;GLLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#20351;&#39046;&#22495;&#21457;&#29983;&#20102;&#36716;&#21464;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#26114;&#36149;&#30340;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;GLLMs&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23545;&#20110;GLLMs&#30340;&#39044;&#27979;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;ANLS*&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;ANLS*&#24230;&#37327;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;RLHF&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#32771;&#34385;&#20102;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#32553;&#20943;&#20026;PORRL&#24418;&#24335;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#31639;&#27861;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.03282</link><description>&lt;p&gt;
&#19968;&#20010;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#22312;RLHF&#20013;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Partially Observed Reward-States in RLHF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03282
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;RLHF&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#32771;&#34385;&#20102;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#32553;&#20943;&#20026;PORRL&#24418;&#24335;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#31639;&#27861;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#30740;&#31350;&#22240;&#20854;&#22312;LLMs&#30340;&#21457;&#23637;&#20013;&#36215;&#21040;&#30340;&#20316;&#29992;&#32780;&#21464;&#24471;&#37325;&#35201;&#12290;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#23545;&#21050;&#28608;&#30340;&#21453;&#24212;&#24050;&#30693;&#20381;&#36182;&#20110;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#8220;&#20869;&#37096;&#29366;&#24577;&#8221;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#21069;&#30340;RLHF&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;RLHF&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#20013;&#38388;&#21453;&#39304;&#65292;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#23545;&#40784;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#23558;RLHF&#24314;&#27169;&#20026;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PORRL&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;RLHF&#20013;&#20004;&#31181;&#20027;&#35201;&#24418;&#24335;&#30340;&#20154;&#31867;&#21453;&#39304; - &#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#21040;PORRL&#30340;&#32553;&#20943;&#12290;&#23545;&#20110;&#22522;&#25968;&#21453;&#39304;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36890;&#29992;&#30340;&#32479;&#35745;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#23454;&#20363;&#21270;&#20026;POR-UCRL&#21644;POR-UCBVI&#12290;&#23545;&#20110;&#20915;&#26007;&#21453;&#39304;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#22522;&#25968;&#21453;&#39304;&#32553;&#20943;&#19981;&#33021;&#36798;&#21040;&#20122;&#32447;&#24615;&#30340;&#20915;&#26007;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed "internal states." Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02314</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#36873;&#25321;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Selecting Large Language Model to Fine-tune via Rectified Scaling Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02314
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20462;&#27491;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#30340;&#27010;&#24565;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#30410;&#22686;&#38271;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#20013;&#65292;&#22312;&#20247;&#22810;&#36873;&#39033;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#25104;&#20026;&#20102;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#24494;&#35843;&#25152;&#26377;&#27169;&#22411;&#28982;&#21518;&#20877;&#36827;&#34892;&#36873;&#25321;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36164;&#28304;&#21463;&#38480;&#30340;&#36873;&#25321;&#20219;&#21153;&#36716;&#21270;&#20026;&#39044;&#27979;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#23637;&#31034;&#20854;&#19982;&#32553;&#25918;&#23450;&#24459;&#20043;&#38388;&#30340;&#33258;&#28982;&#32852;&#31995;&#12290;&#19982;&#39044;&#35757;&#32451;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#30340;&#32553;&#25918;&#26354;&#32447;&#19981;&#20165;&#21253;&#25324;&#20247;&#25152;&#21608;&#30693;&#30340;&#8220;&#21151;&#29575;&#38454;&#27573;&#8221;&#65292;&#36824;&#21253;&#25324;&#20197;&#21069;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#8220;&#39044;&#21151;&#29575;&#38454;&#27573;&#8221;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;&#32553;&#25918;&#23450;&#24459;&#26080;&#27861;&#29702;&#35770;&#21644;&#23454;&#35777;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#30456;&#21464;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#8220;&#39044;&#23398;&#20064;&#25968;&#25454;&#22823;&#23567;&#8221;&#27010;&#24565;&#24341;&#20837;&#21040;&#25105;&#20204;&#30340;&#20462;&#27491;&#32553;&#25918;&#23450;&#24459;&#20013;&#65292;&#36825;&#20811;&#26381;&#20102;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#26356;&#22909;&#22320;&#36866;&#24212;&#23454;&#39564;&#32467;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;&#23450;&#24459;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with scaling laws. Unlike pre-training, We find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing scaling laws fail to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our rectified scaling law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;TSIS&#31639;&#27861;&#20316;&#20026;t-SMILES&#30340;&#34917;&#20805;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#23383;&#31526;&#20018;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TSIS&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#27861;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02164</link><description>&lt;p&gt;
TSIS: t-SMILES&#30340;&#34917;&#20805;&#31639;&#27861;&#29992;&#20110;&#22522;&#20110;&#29255;&#27573;&#30340;&#20998;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
TSIS: A Supplementary Algorithm to t-SMILES for Fragment-based Molecular Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;TSIS&#31639;&#27861;&#20316;&#20026;t-SMILES&#30340;&#34917;&#20805;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#23383;&#31526;&#20018;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TSIS&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#27861;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#31526;&#20018;&#22522;&#26412;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#65292;&#22914;SMILES&#65292;&#22312;&#32447;&#24615;&#34920;&#31034;&#20998;&#23376;&#20449;&#24687;&#26041;&#38754;&#26159;&#20107;&#23454;&#19978;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#24517;&#39035;&#20351;&#29992;&#37197;&#23545;&#31526;&#21495;&#21644;&#35299;&#26512;&#31639;&#27861;&#23548;&#33268;&#20102;&#38271;&#30340;&#35821;&#27861;&#20381;&#36182;&#20851;&#31995;&#65292;&#20351;&#24471;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20063;&#38590;&#20197;&#20934;&#30830;&#29702;&#35299;&#35821;&#27861;&#21644;&#35821;&#20041;&#12290;&#23613;&#31649;DeepSMILES&#21644;SELFIES&#24050;&#32463;&#35299;&#20915;&#20102;&#26576;&#20123;&#38480;&#21046;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#22312;&#22788;&#29702;&#39640;&#32423;&#35821;&#27861;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20351;&#24471;&#19968;&#20123;&#23383;&#31526;&#20018;&#38590;&#20197;&#38405;&#35835;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#34917;&#20805;&#31639;&#27861;TSIS&#65288;TSID&#31616;&#21270;&#65289;&#65292;&#29992;&#20110;t-SMILES&#23478;&#26063;&#12290;TSIS&#19982;&#21478;&#19968;&#20010;&#22522;&#20110;&#29255;&#27573;&#30340;&#32447;&#24615;&#35299;&#20915;&#26041;&#26696;SAFE&#36827;&#34892;&#20102;&#27604;&#36739;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SAFE&#22312;&#22788;&#29702;&#35821;&#27861;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#26102;&#23384;&#22312;&#25361;&#25112;&#12290;TSIS&#32487;&#32493;&#20351;&#29992;t-SMILES&#20013;&#23450;&#20041;&#30340;&#26641;&#20316;&#20026;&#20854;&#22522;&#30784;&#25968;&#25454;&#32467;&#26500;&#65292;&#36825;&#20351;&#20854;&#19982;SAFE&#27169;&#22411;&#26377;&#25152;&#19981;&#21516;&#12290;TSIS&#27169;&#22411;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;SAFE&#27169;&#22411;&#65292;&#34920;&#26126;t-SMILES&#30340;&#26641;&#32467;&#26500;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
String-based molecular representations, such as SMILES, are a de facto standard for linearly representing molecular information. However, the must be paired symbols and the parsing algorithm result in long grammatical dependencies, making it difficult for even state-of-the-art deep learning models to accurately comprehend the syntax and semantics. Although DeepSMILES and SELFIES have addressed certain limitations, they still struggle with advanced grammar, which makes some strings difficult to read. This study introduces a supplementary algorithm, TSIS (TSID Simplified), to t-SMILES family. Comparative experiments between TSIS and another fragment-based linear solution, SAFE, indicate that SAFE presents challenges in managing long-term dependencies in grammar. TSIS continues to use the tree defined in t-SMILES as its foundational data structure, which sets it apart from the SAFE model. The performance of TSIS models surpasses that of SAFE models, indicating that the tree structure of t
&lt;/p&gt;</description></item><item><title>SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00854</link><description>&lt;p&gt;
SymbolicAI: &#19968;&#20010;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SymbolicAI: A framework for logic-based approaches combining generative models and solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00854
&lt;/p&gt;
&lt;p&gt;
SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SymbolicAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#19988;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#27969;&#31243;&#31649;&#29702;&#12290;SymbolicAI&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#26469;&#25191;&#34892;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25351;&#20196;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31526;&#21495;&#25512;&#29702;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;&#29983;&#25104;&#27169;&#22411;&#19982;&#21508;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#27010;&#29575;&#32534;&#31243;&#21407;&#29702;&#26469;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#21487;&#24494;&#20998;&#21644;&#32463;&#20856;&#32534;&#31243;&#33539; paradigms &#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#22810;&#24577;&#30340;&#12289;&#32452;&#21512;&#30340;&#21644;&#33258;&#25351;&#30340;&#25968;&#25454;&#27969;&#25805;&#20316;&#65292;&#23558;LLM&#30340;&#36755;&#20986;&#19982;&#29992;&#25143;&#30340;&#30446;&#26631;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#20855;&#26377;&#38646;&#27425;&#21644;&#23569;&#27425;&#23398;&#20064;&#33021;&#21147;&#30340;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#36807;&#28193;&#65292;&#24182;&#19982;&#25797;&#38271;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#30340;&#19987;&#19994;&#21270;&#35843;&#20248;&#27169;&#22411;&#25110;&#27714;&#35299;&#22120;&#37197;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addres
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#20803;&#36947;&#24503;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#65292;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#22810;&#20803;&#36947;&#24503;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25429;&#25417;&#21040;&#65292;&#20294;&#21333;&#20973;&#33258;&#20027;&#23398;&#20064;&#24456;&#38590;&#25512;&#26029;&#20986;&#36947;&#24503;&#22810;&#20803;&#20027;&#20041;&#12290;</title><link>https://arxiv.org/abs/2401.17228</link><description>&lt;p&gt;
&#36947;&#24503;&#19981;&#26159;&#20108;&#20803;&#30340;&#65306;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26500;&#24314;&#19968;&#31181;&#22810;&#20803;&#36947;&#24503;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding Space using Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17228
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#20803;&#36947;&#24503;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#65292;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#22810;&#20803;&#36947;&#24503;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25429;&#25417;&#21040;&#65292;&#20294;&#21333;&#20973;&#33258;&#20027;&#23398;&#20064;&#24456;&#38590;&#25512;&#26029;&#20986;&#36947;&#24503;&#22810;&#20803;&#20027;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#24503;&#24615;&#20262;&#29702;&#23398;&#21644;&#36947;&#24503;&#35268;&#33539;&#26041;&#38754;&#20445;&#30041;&#20102;&#21487;&#36776;&#35782;&#30340;&#30693;&#35782;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24120;&#24120;&#23558;&#36947;&#24503;&#35270;&#20026;&#20108;&#20803;&#30340;&#65292;&#20174;&#23545;&#38169;&#20004;&#20010;&#26497;&#31471;&#36827;&#34892;&#21010;&#20998;&#12290;&#36825;&#31181;&#31616;&#21270;&#30340;&#35266;&#28857;&#27809;&#26377;&#25429;&#25417;&#21040;&#36947;&#24503;&#21028;&#26029;&#30340;&#24494;&#22937;&#24046;&#21035;&#12290;&#22810;&#20803;&#36947;&#24503;&#21746;&#23398;&#23478;&#35748;&#20026;&#65292;&#20154;&#31867;&#36947;&#24503;&#21487;&#20197;&#34987;&#20998;&#35299;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#35201;&#32032;&#65292;&#24182;&#23562;&#37325;&#36947;&#24503;&#21028;&#26029;&#30340;&#20010;&#20307;&#24046;&#24322;&#12290;&#20026;&#20102;&#19982;&#36825;&#19968;&#35266;&#28857;&#20445;&#25345;&#19968;&#33268;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#20803;&#36947;&#24503;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#30740;&#31350;&#26469;&#31995;&#32479;&#22320;&#35843;&#26597;&#23884;&#20837;&#31354;&#38388;&#20013;&#36947;&#24503;&#35201;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#20803;&#36947;&#24503;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25429;&#25417;&#21040;&#12290;&#28982;&#32780;&#65292;&#21333;&#20973;&#33258;&#20027;&#23398;&#20064;&#24456;&#38590;&#25512;&#26029;&#20986;&#36947;&#24503;&#22810;&#20803;&#20027;&#20041;&#65292;&#38656;&#35201;&#26377;&#20154;&#24037;&#26631;&#31614;&#30340;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in NLP show that language models retain a discernible level of knowledge in deontological ethics and moral norms. However, existing works often treat morality as binary, ranging from right to wrong. This simplistic view does not capture the nuances of moral judgment. Pluralist moral philosophers argue that human morality can be deconstructed into a finite number of elements, respecting individual differences in moral judgment. In line with this view, we build a pluralist moral sentence embedding space via a state-of-the-art contrastive learning approach. We systematically investigate the embedding space by studying the emergence of relationships among moral elements, both quantitatively and qualitatively. Our results show that a pluralist approach to morality can be captured in an embedding space. However, moral pluralism is challenging to deduce via self-supervision alone and requires a supervised approach with human labels.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.16332</link><description>&lt;p&gt;
&#23545;&#40784;&#21644;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tradeoffs Between Alignment and Helpfulness in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#22686;&#24378;&#26399;&#26395;&#34892;&#20026;&#21644;&#25233;&#21046;&#38750;&#26399;&#26395;&#34892;&#20026;&#65292;&#23454;&#29616;&#20154;&#31867;&#19982;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#23433;&#20840;&#20132;&#20114;&#12290;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#25110;&#25554;&#20837;&#39044;&#35774;&#30340;&#23545;&#40784;&#25552;&#31034;&#26469;&#23454;&#29616;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#21518;&#30340;&#34920;&#31034;&#26469;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#30340;&#34920;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;&#34920;&#31034;&#24037;&#31243;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#38477;&#20302;&#31038;&#20250;&#20559;&#35265;&#31561;&#23545;&#40784;&#23548;&#21521;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#22686;&#30410;&#65292;&#20294;&#20063;&#23548;&#33268;&#20102;&#27169;&#22411;&#25191;&#34892;&#22522;&#26412;&#20219;&#21153;&#33021;&#21147;&#30340;&#38477;&#20302;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#27169;&#22411;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#27169;&#22411;&#30340;&#26377;&#29992;&#24615;&#36890;&#24120;&#20250;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;90&#20010;&#36523;&#20307;&#27573;&#12289;206&#20010;&#20851;&#33410;&#21644;700&#20010;&#32908;&#33137;&#21333;&#20301;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#20302;&#32500;&#34920;&#31034;&#21644;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20840;&#36523;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2312.05473</link><description>&lt;p&gt;
&#33258;&#25105;&#27169;&#22411;&#29992;&#20110;&#20855;&#36523;&#26234;&#33021;&#65306;&#29992;&#20998;&#23618;&#20302;&#32500;&#34920;&#31034;&#24314;&#27169;&#20840;&#36523;&#20154;&#20307;&#39592;&#39612;&#32908;&#32905;&#31995;&#32479;&#21644;&#36816;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Self Model for Embodied Intelligence: Modeling Full-Body Human Musculoskeletal System and Locomotion Control with Hierarchical Low-Dimensional Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;90&#20010;&#36523;&#20307;&#27573;&#12289;206&#20010;&#20851;&#33410;&#21644;700&#20010;&#32908;&#33137;&#21333;&#20301;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#20302;&#32500;&#34920;&#31034;&#21644;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20840;&#36523;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#32908;&#32905;&#39592;&#39612;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#25511;&#21046;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#36816;&#21160;&#21151;&#33021;&#12289;&#24320;&#21457;&#20855;&#36523;&#26234;&#33021;&#20197;&#21450;&#20248;&#21270;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#38024;&#23545;&#30446;&#21069;&#24320;&#28304;&#27169;&#22411;&#20165;&#38480;&#20110;&#23569;&#25968;&#36523;&#20307;&#37096;&#20301;&#19988;&#36890;&#24120;&#32908;&#32905;&#25968;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;90&#20010;&#36523;&#20307;&#27573;&#12289;206&#20010;&#20851;&#33410;&#21644;700&#20010;&#32908;&#33137;&#21333;&#20301;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#65292;&#33021;&#22815;&#27169;&#25311;&#20840;&#36523;&#21160;&#24577;&#24182;&#19982;&#21508;&#31181;&#35774;&#22791;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#32500;&#34920;&#31034;&#21644;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20840;&#36523;&#25511;&#21046;&#12290;&#36890;&#36807;&#27169;&#25311;&#30495;&#23454;&#20154;&#31867;&#27493;&#24577;&#25968;&#25454;&#39564;&#35777;&#20102;&#25105;&#20204;&#27169;&#22411;&#21644;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05473v2 Announce Type: replace  Abstract: Modeling and control of the human musculoskeletal system is important for understanding human motor functions, developing embodied intelligence, and optimizing human-robot interaction systems. However, current open-source models are restricted to a limited range of body parts and often with a reduced number of muscles. There is also a lack of algorithms capable of controlling over 600 muscles to generate reasonable human movements. To fill this gap, we build a musculoskeletal model with 90 body segments, 206 joints, and 700 muscle-tendon units, allowing simulation of full-body dynamics and interaction with various devices. We develop a new algorithm using low-dimensional representation and hierarchical deep reinforcement learning to achieve state-of-the-art full-body control. We validate the effectiveness of our model and algorithm in simulations with real human locomotion data. The musculoskeletal model, along with its control algor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#36341;&#23454;&#39564;&#21644;&#29702;&#35770;&#27934;&#23519;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#65292;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2310.02124</link><description>&lt;p&gt;
&#25506;&#32034;LLM&#20195;&#29702;&#30340;&#21327;&#20316;&#26426;&#21046;&#65306;&#31038;&#20250;&#24515;&#29702;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02124
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#36341;&#23454;&#39564;&#21644;&#29702;&#35770;&#27934;&#23519;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#65292;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#31038;&#20250;&#29615;&#22659;&#20013;&#65292;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#36825;&#20123;NLP&#31995;&#32479;&#33021;&#21542;&#27169;&#20223;&#31867;&#20154;&#31867;&#30340;&#21327;&#20316;&#26234;&#33021;&#65292;&#22312;&#30001;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32452;&#25104;&#30340;&#22810;&#20195;&#29702;&#31038;&#20250;&#20013;&#65311;&#26412;&#25991;&#36890;&#36807;&#23558;&#23454;&#36341;&#23454;&#39564;&#19982;&#29702;&#35770;&#35266;&#28857;&#30456;&#32467;&#21512;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#30001;LLM&#20195;&#29702;&#32452;&#25104;&#30340;&#29420;&#29305;&#8220;&#31038;&#20250;&#8221;&#65292;&#27599;&#20010;&#20195;&#29702;&#20197;&#29305;&#23450;&#30340;&#8220;&#29305;&#36136;&#8221;&#65288;&#38543;&#21644;&#25110;&#36807;&#20110;&#33258;&#20449;&#65289;&#20026;&#29305;&#24449;&#65292;&#24182;&#19982;&#19981;&#21516;&#30340;&#8220;&#24605;&#32500;&#27169;&#24335;&#8221;&#65288;&#36777;&#35770;&#25110;&#21453;&#24605;&#65289;&#23637;&#24320;&#21327;&#20316;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#36825;&#20123;&#22810;&#20195;&#29702;&#31038;&#20250;&#65292;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#19981;&#20165;&#32988;&#36807;&#20808;&#21069;&#39030;&#23574;&#26041;&#27861;&#65292;&#32780;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#65288;&#20351;&#29992;&#26356;&#23569;&#30340;API&#20196;&#29260;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#35828;&#26126;LLM&#20195;&#29702;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02124v2 Announce Type: replace-cross  Abstract: As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents mani
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#30340;&#35299;&#37322;&#35299;&#37322;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#19981;&#36879;&#26126;&#31995;&#32479;&#20013;&#29983;&#25104;&#21512;&#36866;&#35299;&#37322;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.17045</link><description>&lt;p&gt;
&#22312;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#20013;&#35299;&#37322;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explaining Explanations in Probabilistic Logic Programming. (arXiv:2401.17045v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17045
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#30340;&#35299;&#37322;&#35299;&#37322;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#19981;&#36879;&#26126;&#31995;&#32479;&#20013;&#29983;&#25104;&#21512;&#36866;&#35299;&#37322;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#30340;&#20986;&#29616;&#20063;&#23548;&#33268;&#20102;&#20135;&#29983;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#30340;&#38656;&#27714;&#12290;&#22312;&#19968;&#20123;&#26041;&#27861;&#20013;&#65292;&#31995;&#32479;&#26159;&#19981;&#36879;&#26126;&#30340;&#65288;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#40657;&#30418;&#23376;&#8221;&#65289;&#65292;&#36825;&#20351;&#24471;&#29983;&#25104;&#36866;&#24403;&#30340;&#35299;&#37322;&#21464;&#24471;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#22312;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36923;&#36753;&#32534;&#31243;&#65288;&#29992;&#20110;&#30693;&#35782;&#34920;&#31034;&#65289;&#21644;&#27010;&#29575;&#65288;&#29992;&#20110;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65289;&#30340;&#32467;&#21512;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#35828;&#27169;&#22411;&#26159;&#21487;&#20197;&#35299;&#37322;&#30340;&#65292;&#36825;&#26041;&#20415;&#20102;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#26597;&#35810;&#65292;&#36890;&#24120;&#30340;&#8220;&#35299;&#37322;&#8221;&#30340;&#27010;&#24565;&#26159;&#19982;&#27169;&#22411;&#30340;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#30340;&#36873;&#25321;&#38598;&#30456;&#20851;&#32852;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20010;&#38598;&#21512;&#27809;&#26377;&#22240;&#26524;&#32467;&#26500;&#65292;&#23454;&#38469;&#19978;&#65292;&#19968;&#20123;&#36873;&#25321;&#23454;&#38469;&#19978;&#19982;&#25152;&#32771;&#34385;&#30340;&#26597;&#35810;&#26080;&#20851;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#39537;&#21160;&#25512;&#29702;&#23450;&#20041;&#30340;&#35299;&#37322;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of tools based on artificial intelligence has also led to the need of producing explanations which are understandable by a human being. In some approaches, the system is not transparent (often referred to as a "black box"), making it difficult to generate appropriate explanations. In this work, though, we consider probabilistic logic programming, a combination of logic programming (for knowledge representation) and probability (to model uncertainty). In this setting, one can say that models are interpretable, which eases its understanding. However, given a particular query, the usual notion of "explanation" is associated with a set of choices, one for each random variable of the model. Unfortunately, this set does not have a causal structure and, in fact, some of the choices are actually irrelevant to the considered query. In order to overcome these shortcomings, we present an approach to explaining explanations which is based on the definition of a query-driven inference
&lt;/p&gt;</description></item><item><title>Context-Former&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#24182;&#25552;&#39640;&#20102;Decision Transformer&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16452</link><description>&lt;p&gt;
Context-Former&#65306;&#22522;&#20110;&#28508;&#22312;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#30340;&#25340;&#25509;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Context-Former: Stitching via Latent Conditioned Sequence Modeling. (arXiv:2401.16452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16452
&lt;/p&gt;
&lt;p&gt;
Context-Former&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#24182;&#25552;&#39640;&#20102;Decision Transformer&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20248;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#33021;&#21147;&#26159;&#20351;RL&#33021;&#22815;&#23398;&#20064;&#20248;&#20110;&#34892;&#20026;&#31574;&#30053;&#30340;&#31574;&#30053;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;Decision Transformer&#65288;DT&#65289;&#23558;&#20915;&#31574;&#24314;&#27169;&#20026;&#24207;&#21015;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#22312;&#31163;&#32447;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;DT&#32570;&#20047;&#25340;&#25509;&#33021;&#21147;&#65292;&#22240;&#27492;&#25552;&#39640;DT&#24615;&#33021;&#38656;&#35201;&#21033;&#29992;&#25340;&#25509;&#33021;&#21147;&#12290;&#20026;&#20102;&#36171;&#20104;DT&#25340;&#25509;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36712;&#36857;&#25340;&#25509;&#25277;&#35937;&#20026;&#19987;&#23478;&#21305;&#37197;&#65292;&#24182;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;ContextFormer&#65292;&#36890;&#36807;&#27169;&#25311;&#26377;&#38480;&#25968;&#37327;&#30340;&#19987;&#23478;&#36712;&#36857;&#30340;&#34920;&#31034;&#26469;&#38598;&#25104;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21644;&#24207;&#21015;&#24314;&#27169;&#65292;&#20197;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) algorithms can improve the decision making via stitching sub-optimal trajectories to obtain more optimal ones. This capability is a crucial factor in enabling RL to learn policies that are superior to the behavioral policy. On the other hand, Decision Transformer (DT) abstracts the decision-making as sequence modeling, showcasing competitive performance on offline RL benchmarks, however, recent studies demonstrate that DT lacks of stitching capability, thus exploit stitching capability for DT is vital to further improve its performance. In order to endow stitching capability to DT, we abstract trajectory stitching as expert matching and introduce our approach, ContextFormer, which integrates contextual information-based imitation learning (IL) and sequence modeling to stitch sub-optimal trajectory fragments by emulating the representations of a limited number of expert trajectories. To validate our claim, we conduct experiments from two perspectives:
&lt;/p&gt;</description></item><item><title>&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.14086</link><description>&lt;p&gt;
&#20351;&#29992;Sum-Product Networks&#29983;&#25104;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generating Likely Counterfactuals Using Sum-Product Networks. (arXiv:2401.14086v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14086
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#65288;GDPR&#12289;AI&#27861;&#26696;&#65289;&#65292;&#38656;&#35201;&#35299;&#37322;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#20915;&#31574;&#24448;&#24448;&#21482;&#33021;&#22312;&#20107;&#21518;&#35299;&#37322;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#25104;&#20026;&#24120;&#35265;&#30340;&#35299;&#37322;&#26041;&#24335;&#12290;&#20160;&#20040;&#26500;&#25104;&#20102;&#26368;&#20339;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#24517;&#39035;&#32771;&#34385;&#22810;&#20010;&#26041;&#38754;&#65292;&#20854;&#20013;&#8220;&#26679;&#26412;&#36317;&#31163;&#8221;&#26159;&#26368;&#24120;&#35265;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#19968;&#35201;&#27714;&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#22826;&#21487;&#33021;&#19988;&#22240;&#27492;&#20215;&#20540;&#26377;&#38480;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#39640;&#21487;&#33021;&#24615;&#35299;&#37322;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#65288;MIO&#65289;&#27169;&#25311;&#23547;&#25214;&#28385;&#36275;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35768;&#22810;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#26377;&#21487;&#33021;&#35299;&#37322;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sum-Product Network&#65288;SPN&#65289;&#30340;MIO&#34920;&#36798;&#65292;&#24182;&#20351;&#29992;SPN&#20272;&#35745;&#21453;&#20107;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#23545;&#29420;&#31435;&#30340;&#20852;&#36259;&#20063;&#26377;&#29992;&#12290;&#19982;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20960;&#31181;&#26041;&#27861;&#36827;&#34892;&#25968;&#20540;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI systems need to be explained. These decisions are often explainable only post hoc, where counterfactual explanations are popular. The question of what constitutes the best counterfactual explanation must consider multiple aspects, where "distance from the sample" is the most common. We argue that this requirement frequently leads to explanations that are unlikely and, therefore, of limited value. Here, we present a system that provides high-likelihood explanations. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using mixed-integer optimization (MIO). In the process, we propose an MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the likelihood of a counterfactual, which can be of independent interest. A numerical comparison against several methods for generating counterfactual explanations is pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#30340;&#26041;&#27861;DISCOUNT&#65292;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#32622;&#20449;&#24230;&#26469;&#25903;&#25745;&#36825;&#19968;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13112</link><description>&lt;p&gt;
DISCOUNT: &#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DISCOUNT: Distributional Counterfactual Explanation With Optimal Transport. (arXiv:2401.13112v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#30340;&#26041;&#27861;DISCOUNT&#65292;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#32622;&#20449;&#24230;&#26469;&#25903;&#25745;&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35299;&#37322;&#26159;&#22312;&#40657;&#30418;&#20915;&#31574;&#27169;&#22411;&#20013;&#25552;&#20379;&#27934;&#23519;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20107;&#23454;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#23548;&#33268;&#19981;&#21516;&#32467;&#26524;&#30340;&#26367;&#20195;&#36755;&#20837;&#23454;&#20363;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#20998;&#24067;&#19978;&#19979;&#25991;&#65292;&#20174;&#20010;&#20307;&#25968;&#25454;&#28857;&#25193;&#22823;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#21629;&#21517;&#20026;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#12290;&#22312;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#36716;&#21521;&#20998;&#26512;&#20107;&#23454;&#21644;&#23545;&#25239;&#30340;&#20998;&#24067;&#23646;&#24615;&#65292;&#31867;&#20284;&#20110;&#35780;&#20272;&#20010;&#20307;&#23454;&#20363;&#21450;&#20854;&#32467;&#26524;&#20915;&#31574;&#30340;&#32463;&#20856;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26469;&#26500;&#24314;&#19968;&#20010;&#26426;&#20250;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#23548;&#20986;&#19982;&#20107;&#23454;&#23545;&#24212;&#30340;&#23545;&#25239;&#20998;&#24067;&#65292;&#20197;&#32479;&#35745;&#32622;&#20449;&#24230;&#20570;&#25903;&#25745;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20248;&#21270;&#26041;&#27861;DISCOUNT&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20998;&#24067;&#20043;&#38388;&#24179;&#34913;&#36825;&#31181;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (CE) is the de facto method for providing insight and interpretability in black-box decision-making models by identifying alternative input instances that lead to different outcomes. This paper extends the concept of CEs to a distributional context, broadening the scope from individual data points to entire input and output distributions, named Distributional Counterfactual Explanation (DCE). In DCE, our focus shifts to analyzing the distributional properties of the factual and counterfactual, drawing parallels to the classical approach of assessing individual instances and their resulting decisions. We leverage Optimal Transport (OT) to frame a chance-constrained optimization problem, aiming to derive a counterfactual distribution that closely aligns with its factual counterpart, substantiated by statistical confidence. Our proposed optimization method, DISCOUNT, strategically balances this confidence across both input and output distributions. This algorit
&lt;/p&gt;</description></item><item><title>AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05268</link><description>&lt;p&gt;
AUTOACT&#65306;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#23454;&#29616;&#30340;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05268
&lt;/p&gt;
&lt;p&gt;
AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#19981;&#26029;&#30340;&#25506;&#32034;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#26114;&#36149;&#12289;&#19981;&#21487;&#37325;&#22797;&#30340;&#25968;&#25454;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#19988;&#38754;&#20020;&#23558;&#21333;&#19968;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#21151;&#33021;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoAct&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#21644;&#26469;&#33258;&#38381;&#28304;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#21512;&#25104;&#36712;&#36857;&#12290;&#32473;&#23450;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#24211;&#65292;AutoAct&#39318;&#20808;&#33258;&#21160;&#21512;&#25104;&#35268;&#21010;&#36712;&#36857;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#25110;&#24378;&#38381;&#28304;&#27169;&#22411;&#30340;&#20219;&#20309;&#36741;&#21161;&#12290;&#28982;&#21518;&#65292;AutoAct&#21033;&#29992;&#20998;&#24037;&#31574;&#30053;&#65292;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#21644;&#21512;&#25104;&#36712;&#36857;&#33258;&#21160;&#21306;&#20998;&#65292;&#20135;&#29983;&#19968;&#20010;&#23376;&#20195;&#29702;&#32452;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#31181;LLMs&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;AutoAct&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#19982;&#20854;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#29256;&#26435;&#20445;&#25252;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#12290;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#26080;&#38656;&#25805;&#20316;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#36807;&#31243;&#65292;&#20165;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#38598;&#25554;&#20837;&#27745;&#26579;&#25968;&#25454;&#26469;&#23454;&#26045;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.04136</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24378;&#22823;&#65292;&#21518;&#38376;&#26356;&#23481;&#26131;: &#25968;&#25454;&#27745;&#26579;&#24341;&#21457;&#29256;&#26435;&#20405;&#29359;&#32780;&#26080;&#38656;&#35843;&#25972;&#24494;&#35843;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline. (arXiv:2401.04136v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#29256;&#26435;&#20445;&#25252;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#12290;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#26080;&#38656;&#25805;&#20316;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#36807;&#31243;&#65292;&#20165;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#38598;&#25554;&#20837;&#27745;&#26579;&#25968;&#25454;&#26469;&#23454;&#26045;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#21830;&#19994;&#21270;&#24341;&#21457;&#20102;&#28508;&#22312;&#30340;&#29256;&#26435;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21518;&#38376;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65288;SilentBadDiffusion&#65289;&#26469;&#25506;&#35752;&#25193;&#25955;&#27169;&#22411;&#20013;&#19982;&#29256;&#26435;&#20445;&#25252;&#30456;&#20851;&#30340;&#28431;&#27934;&#12290;&#25915;&#20987;&#26041;&#27861;&#26080;&#38656;&#35775;&#38382;&#25110;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#36807;&#31243;&#65292;&#20165;&#28041;&#21450;&#23558;&#27745;&#26579;&#25968;&#25454;&#25554;&#20837;&#24178;&#20928;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#12290;&#36825;&#20123;&#25968;&#25454;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The commercialization of diffusion models, renowned for their ability to generate high-quality images that are often indistinguishable from real ones, brings forth potential copyright concerns. Although attempts have been made to impede unauthorized access to copyrighted material during training and to subsequently prevent DMs from generating copyrighted images, the effectiveness of these solutions remains unverified. This study explores the vulnerabilities associated with copyright protection in DMs by introducing a backdoor data poisoning attack (SilentBadDiffusion) against text-to-image diffusion models. Our attack method operates without requiring access to or control over the diffusion model's training or fine-tuning processes; it merely involves the insertion of poisoning data into the clean training dataset. This data, comprising poisoning images equipped with prompts, is generated by leveraging the powerful capabilities of multimodal large language models and text-guided image 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#26041;&#27861;ReFusion&#65292;&#36890;&#36807;&#23558;&#26816;&#32034;&#34920;&#31034;&#30452;&#25509;&#34701;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#23558;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#34701;&#20837;&#38750;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.02993</link><description>&lt;p&gt;
&#20351;&#29992;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion. (arXiv:2401.02993v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#26041;&#27861;ReFusion&#65292;&#36890;&#36807;&#23558;&#26816;&#32034;&#34920;&#31034;&#30452;&#25509;&#34701;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#23558;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#34701;&#20837;&#38750;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#33719;&#21462;&#30693;&#35782;&#24182;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#22312;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#20363;&#22914;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#65289;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#20363;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#20013;&#38598;&#25104;&#26816;&#32034;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#23558;&#26816;&#32034;&#20869;&#23481;&#25340;&#25509;&#21040;&#36755;&#20837;&#20013;&#24418;&#25104;&#25552;&#31034;&#24615;&#30340;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25512;&#26029;&#36825;&#31181;&#25340;&#25509;&#25968;&#25454;&#20063;&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#26041;&#27861;ReFusion&#65292;&#37319;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#30452;&#25509;&#23558;&#26816;&#32034;&#34920;&#31034;&#19982;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26816;&#32034;&#27169;&#22359;&#65292;&#37325;&#26032;&#26816;&#32034;...
&lt;/p&gt;
&lt;p&gt;
Retrieval-based augmentations that aim to incorporate knowledge from an external database into language models have achieved great success in various knowledge-intensive (KI) tasks, such as question-answering and text generation. However, integrating retrievals in non-knowledge-intensive (NKI) tasks, such as text classification, is still challenging. Existing works focus on concatenating retrievals to inputs as context to form the prompt-based inputs. Unfortunately, such methods require language models to have the capability to handle long texts. Besides, inferring such concatenated data would also consume a significant amount of computational resources.  To solve these challenges, we propose \textbf{ReFusion} in this paper, a computation-efficient \textbf{Re}trieval representation \textbf{Fusion} with neural architecture search. The main idea is to directly fuse the retrieval representations into the language models. Specifically, we first propose an online retrieval module that retri
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.01537</link><description>&lt;p&gt;
&#27450;&#39575;&#30340;&#33402;&#26415;&#65306;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#30340;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. (arXiv:2401.01537v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01537
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#34892;&#19994;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#39046;&#22495;&#27491;&#22312;&#32463;&#21382;&#22686;&#38271;&#30340;&#23454;&#26045;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#38271;&#24341;&#21457;&#20102;&#23545;AI&#38450;&#24481;&#26426;&#21046;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26469;&#33258;&#19981;&#23436;&#20840;&#21487;&#20449;&#30340;&#31532;&#19977;&#26041;&#25552;&#20379;&#21830;&#30340;&#28508;&#22312;&#38544;&#34109;&#25915;&#20987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21548;&#35273;&#21518;&#38376;&#21487;&#33021;&#20351;&#29992;&#26576;&#20123;&#20462;&#25913;&#20316;&#20026;&#20854;&#21551;&#21160;&#26426;&#21046;&#12290;DynamicTrigger&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#34987;&#24341;&#20837;&#65292;&#29992;&#20110;&#36827;&#34892;&#20351;&#29992;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#26469;&#30830;&#20445;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#30340;&#21160;&#24577;&#21518;&#38376;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#27874;&#21160;&#30340;&#20449;&#21495;&#37319;&#26679;&#29575;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#22768;&#38899;&#35302;&#21457;&#22120;&#65288;&#27604;&#22914;&#25293;&#25163;&#22768;&#65289;&#23545;&#35828;&#35805;&#32773;&#36523;&#20221;&#36827;&#34892;&#25513;&#30422;&#65292;&#21487;&#20197;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65288;ASR&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#65292;DynamicTrigger&#22312;&#38544;&#34109;&#25915;&#20987;&#20013;&#26082;&#26377;&#25928;&#21448;&#38544;&#34109;&#65292;&#24182;&#22312;&#25915;&#20987;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The area of Machine Learning as a Service (MLaaS) is experiencing increased implementation due to recent advancements in the AI (Artificial Intelligence) industry. However, this spike has prompted concerns regarding AI defense mechanisms, specifically regarding potential covert attacks from third-party providers that cannot be entirely trusted. Recent research has uncovered that auditory backdoors may use certain modifications as their initiating mechanism. DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor attacks that use cleverly designed tweaks to ensure that corrupted samples are indistinguishable from clean. By utilizing fluctuating signal sampling rates and masking speaker identities through dynamic sound triggers (such as the clapping of hands), it is possible to deceive speech recognition systems (ASR). Our empirical testing demonstrates that DynamicTrigger is both potent and stealthy, achieving impressive success rates during covert attacks while 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#21512;&#36866;&#30340;&#22522;&#20934;&#21644;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#21457;&#29616;&#65292;&#30001;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#20110;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#65292;&#21363;&#20351;&#36825;&#20123;&#29983;&#25104;&#30340;&#22270;&#20687;&#22312;&#35270;&#35273;&#19978;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#29305;&#24449;&#30456;&#27604;&#24182;&#27809;&#26377;&#26356;&#22810;&#12290;&#36825;&#31181;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#21644;&#26550;&#26500;&#30340;&#26816;&#32034;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2311.14084</link><description>&lt;p&gt;
&#30001;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#24341;&#20837;&#20102;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval. (arXiv:2311.14084v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#21512;&#36866;&#30340;&#22522;&#20934;&#21644;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#21457;&#29616;&#65292;&#30001;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#20110;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#65292;&#21363;&#20351;&#36825;&#20123;&#29983;&#25104;&#30340;&#22270;&#20687;&#22312;&#35270;&#35273;&#19978;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#29305;&#24449;&#30456;&#27604;&#24182;&#27809;&#26377;&#26356;&#22810;&#12290;&#36825;&#31181;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#26222;&#36941;&#23384;&#22312;&#20110;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#21644;&#26550;&#26500;&#30340;&#26816;&#32034;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#21464;&#24471;&#26356;&#21152;&#36924;&#30495;&#65292;&#28044;&#20837;&#20114;&#32852;&#32593;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#29616;&#35937;&#23548;&#33268;&#20102;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#28304;&#20559;&#24046;&#12290;&#29305;&#21035;&#26159;&#65292;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#24448;&#24448;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#25490;&#21517;&#39640;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#20559;&#24046;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#36328;&#27169;&#24577;&#26816;&#32034;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#20010;&#36866;&#21512;&#25506;&#32034;&#20559;&#24046;&#23384;&#22312;&#30340;&#22522;&#20934;&#12290;&#38543;&#21518;&#65292;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21457;&#29616;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#23545;&#20110;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#26597;&#35810;&#30456;&#27604;&#27809;&#26377;&#26356;&#22810;&#30340;&#35270;&#35273;&#30456;&#20851;&#29305;&#24449;&#65292;&#20294;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#24448;&#24448;&#23558;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#25490;&#21517;&#39640;&#20110;&#30495;&#23454;&#22270;&#20687;&#12290;&#36825;&#31181;&#19981;&#21487;&#35265;&#30340;&#30456;&#20851;&#20559;&#24046;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#21644;&#26550;&#26500;&#30340;&#26816;&#32034;&#27169;&#22411;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancement of generation models, AI-generated content (AIGC) is becoming more realistic, flooding the Internet. A recent study suggests that this phenomenon causes source bias in text retrieval for web search. Specifically, neural retrieval models tend to rank generated texts higher than human-written texts. In this paper, we extend the study of this bias to cross-modal retrieval. Firstly, we successfully construct a suitable benchmark to explore the existence of the bias. Subsequent extensive experiments on this benchmark reveal that AI-generated images introduce an invisible relevance bias to text-image retrieval models. Specifically, our experiments show that text-image retrieval models tend to rank the AI-generated images higher than the real images, even though the AI-generated images do not exhibit more visually relevant features to the query than real images. This invisible relevance bias is prevalent across retrieval models with varying training data and architectures
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01723</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Towards Calibrated Robust Fine-Tuning of Vision-Language Models. (arXiv:2311.01723v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#21487;&#20197;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#28508;&#21147;&#65292;&#20294;&#20250;&#24433;&#21709;&#27169;&#22411;&#23545;&#20110;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#40065;&#26834;&#24494;&#35843;&#26088;&#22312;&#30830;&#20445;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#20197;&#21450;&#24494;&#35843;&#30340;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#37117;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#32622;&#20449;&#24230;&#26657;&#20934;&#36825;&#19968;&#26631;&#20934;&#21364;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#23613;&#31649;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#39640;&#39118;&#38505;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#23398;&#35786;&#26029;&#65289;&#38656;&#27714;&#26085;&#30410;&#22686;&#21152;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#23545;&#32454;&#35843;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#26657;&#20934;&#30340;&#25285;&#24551;&#65292;&#24182;&#36890;&#36807;&#26174;&#31034;&#26222;&#36890;&#24494;&#35843;&#29978;&#33267;&#26368;&#20808;&#36827;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#36896;&#25104;&#20102;&#25439;&#23475;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#65292;&#23427;&#22312;&#26657;&#20934;&#21644;&#40065;&#26834;&#24615;&#19978;&#25552;&#20379;&#20102;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fine-tuning unleashes the potential of a pre-trained model to a specific task, it trades off the model's generalization capability on out-of-distribution (OOD) datasets. To mitigate this, robust fine-tuning aims to ensure performance on OOD datasets as well as an in-distribution (ID) dataset for which the model is being tuned. However, another criterion for reliable machine learning (ML), confidence calibration, has been overlooked despite its increasing demand for real-world high-stakes ML applications (e.g., autonomous driving and medical diagnosis). For the first time, we raise concerns about the calibration of fine-tuned vision-language models (VLMs) under distribution shift by showing that naive fine-tuning and even state-of-the-art robust fine-tuning methods hurt the calibration of pre-trained VLMs, especially on OOD datasets. To address this, we provide a simple approach, called a calibrated robust fine-tuning (CaRot) that incentivizes the calibration and robustness on bot
&lt;/p&gt;</description></item><item><title>&#8220;Castor&#8221;&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#32508;&#21512;&#23398;&#20064;&#21508;&#20010;&#21306;&#22495;&#30340;&#22240;&#26524;&#22270;&#12290;&#23427;&#36890;&#36807;&#26368;&#22823;&#21270;&#24471;&#20998;&#20989;&#25968;&#26469;&#25512;&#26029;&#21306;&#22495;&#30340;&#25968;&#37327;&#65292;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2311.01412</link><description>&lt;p&gt;
Castor: &#22240;&#26524;&#26102;&#24207;&#21306;&#22495;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Castor: Causal Temporal Regime Structure Learning. (arXiv:2311.01412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01412
&lt;/p&gt;
&lt;p&gt;
&#8220;Castor&#8221;&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#32508;&#21512;&#23398;&#20064;&#21508;&#20010;&#21306;&#22495;&#30340;&#22240;&#26524;&#22270;&#12290;&#23427;&#36890;&#36807;&#26368;&#22823;&#21270;&#24471;&#20998;&#20989;&#25968;&#26469;&#25512;&#26029;&#21306;&#22495;&#30340;&#25968;&#37327;&#65292;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#65292;&#28041;&#21450;&#21040;&#20174;&#27668;&#20505;&#31185;&#23398;&#21040;&#21307;&#30103;&#20445;&#20581;&#31561;&#21508;&#20010;&#23398;&#31185;&#30340;&#24191;&#27867;&#33539;&#22260;&#12290;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#24120;&#36981;&#24490;&#22810;&#20010;&#20808;&#39564;&#26410;&#30693;&#30340;&#21306;&#22495;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#21487;&#20197;&#20174;&#20855;&#26377;&#24050;&#30693;&#21306;&#22495;&#30340;&#24322;&#26500;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#25688;&#35201;&#22240;&#26524;&#22270;&#65292;&#20294;&#22312;&#20840;&#38754;&#23398;&#20064;&#21306;&#22495;&#21644;&#30456;&#24212;&#30340;&#22240;&#26524;&#22270;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CASTOR&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#30001;&#19981;&#21516;&#22240;&#26524;&#22270;&#32479;&#27835;&#30340;&#21508;&#31181;&#24322;&#26500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;EM&#31639;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19968;&#20010;&#24471;&#20998;&#20989;&#25968;&#65292;CASTOR&#25512;&#26029;&#20986;&#21306;&#22495;&#30340;&#25968;&#37327;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CASTOR&#30340;&#31283;&#20581;&#25910;&#25947;&#24615;&#36136;&#65292;&#29305;&#21035;&#31361;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of uncovering causal relationships among multivariate time series data stands as an essential and challenging objective that cuts across a broad array of disciplines ranging from climate science to healthcare. Such data entails linear or non-linear relationships, and usually follow multiple a priori unknown regimes. Existing causal discovery methods can infer summary causal graphs from heterogeneous data with known regimes, but they fall short in comprehensively learning both regimes and the corresponding causal graph. In this paper, we introduce CASTOR, a novel framework designed to learn causal relationships in heterogeneous time series data composed of various regimes, each governed by a distinct causal graph. Through the maximization of a score function via the EM algorithm, CASTOR infers the number of regimes and learns linear or non-linear causal relationships in each regime. We demonstrate the robust convergence properties of CASTOR, specifically highlighting its profic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#38500;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#22788;&#29702;&#24615;&#20063;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#30340;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00094</link><description>&lt;p&gt;
&#34920;&#36798;&#24314;&#27169;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19981;&#36275;&#65306;&#21487;&#22788;&#29702;&#30340;&#25512;&#29702;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Expressive Modeling Is Insufficient for Offline RL: A Tractable Inference Perspective. (arXiv:2311.00094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#38500;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#22788;&#29702;&#24615;&#20063;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#30340;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#19968;&#31181;&#27969;&#34892;&#30340;&#33539;&#20363;&#26159;&#20808;&#23558;&#31163;&#32447;&#36712;&#36857;&#25311;&#21512;&#21040;&#19968;&#20010;&#24207;&#21015;&#27169;&#22411;&#20013;&#65292;&#28982;&#21518;&#36890;&#36807;&#35813;&#27169;&#22411;&#25552;&#31034;&#39640;&#26399;&#26395;&#22238;&#25253;&#30340;&#21160;&#20316;&#12290;&#34429;&#28982;&#26222;&#36941;&#35748;&#20026;&#34920;&#36798;&#24615;&#26356;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#26412;&#25991;&#24378;&#35843;&#20102;&#21487;&#22788;&#29702;&#24615;&#65292;&#21363;&#31934;&#30830;&#32780;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#21516;&#26679;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#24102;&#26469;&#30340;&#22522;&#26412;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#36827;&#34892;&#39640;&#24230;&#38750;&#24179;&#20961;&#30340;&#26465;&#20214;/&#32422;&#26463;&#29983;&#25104;&#65292;&#20197;&#24341;&#20986;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#34429;&#28982;&#20173;&#28982;&#21487;&#20197;&#36817;&#20284;&#22788;&#29702;&#36825;&#20123;&#26597;&#35810;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#31895;&#31961;&#30340;&#20272;&#35745;&#26174;&#33879;&#21066;&#24369;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#65288;TPM&#65289;&#26469;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
A popular paradigm for offline Reinforcement Learning (RL) tasks is to first fit the offline trajectories to a sequence model, and then prompt the model for actions that lead to high expected return. While a common consensus is that more expressive sequence models imply better performance, this paper highlights that tractability, the ability to exactly and efficiently answer various probabilistic queries, plays an equally important role. Specifically, due to the fundamental stochasticity from the offline data-collection policies and the environment dynamics, highly non-trivial conditional/constrained generation is required to elicit rewarding actions. While it is still possible to approximate such queries, we observe that such crude estimates significantly undermine the benefits brought by expressive sequence models. To overcome this problem, this paper proposes Trifle (Tractable Inference for Offline RL), which leverages modern Tractable Probabilistic Models (TPMs) to bridge the gap b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#26469;&#35299;&#20915;&#22312;&#21333;&#20010;&#22330;&#26223;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#25513;&#30721;&#12289;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#21644;&#32465;&#23450;&#24418;&#23481;&#35789;&#31561;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#21512;&#25104;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2310.12274</link><description>&lt;p&gt;
&#19968;&#22270;&#25269;&#21315;&#35328;&#65306;&#20351;&#29992;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#26469;&#23398;&#20064;&#23545;&#35937;&#32423;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning. (arXiv:2310.12274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12274
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#26469;&#35299;&#20915;&#22312;&#21333;&#20010;&#22330;&#26223;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#25513;&#30721;&#12289;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#21644;&#32465;&#23450;&#24418;&#23481;&#35789;&#31561;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#21512;&#25104;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21453;&#36716;&#26159;&#19968;&#31181;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#19968;&#31181;&#26032;&#30340;&#8220;&#21333;&#35789;&#8221;&#30340;&#23884;&#20837;&#34920;&#31034;&#22270;&#20687;&#39118;&#26684;&#21644;&#22806;&#35266;&#65292;&#20351;&#20854;&#33021;&#22815;&#25972;&#21512;&#21040;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#20013;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#21487;&#33719;&#24471;&#20010;&#21035;&#27010;&#24565;&#30340;&#23884;&#20837;&#65292;&#35782;&#21035;&#21644;&#25972;&#21512;&#19968;&#20010;&#22330;&#26223;&#20013;&#30340;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#20173;&#28982;&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#36825;&#20063;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#30340;&#36827;&#19968;&#27493;&#35777;&#23454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#21477;&#23376;-&#22270;&#20687;&#23545;&#20013;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#12290;&#20026;&#20102;&#22686;&#24378;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65306;&#27880;&#24847;&#21147;&#25513;&#30721;&#65288;AttnMask&#65289;&#23558;&#23398;&#20064;&#38598;&#20013;&#22312;&#30456;&#20851;&#21306;&#22495;&#65307;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#65288;PromptCL&#65289;&#23558;&#19981;&#21516;&#27010;&#24565;&#30340;&#23884;&#20837;&#20998;&#31163;&#24320;&#26469;&#65307;&#20197;&#21450;&#32465;&#23450;&#24418;&#23481;&#35789;&#65288;Bind adj.&#65289;&#23558;&#26032;&#30340;&#8220;&#35789;&#8221;&#19982;&#24050;&#30693;&#35789;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Textural Inversion, a prompt learning method, learns a singular embedding for a new "word" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying and integrating multiple object-level concepts within one scene poses significant challenges even when embeddings for individual concepts are attainable. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new "words" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularisation techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new "words" with known words. We evaluate via image generation
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12086</link><description>&lt;p&gt;
&#21457;&#29616;&#22622;&#22764;&#20043;&#27468;&#65306;&#21487;&#38752;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT/GPT-4&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#22312;&#32593;&#32476;&#24179;&#21488;&#19978;&#23384;&#22312;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#30340;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#37319;&#29992;&#12290;&#23545;&#30001;LLMs&#20135;&#29983;&#30340;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#19981;&#20165;&#28041;&#21450;&#23545;&#22522;&#26412;&#20107;&#23454;&#30340;&#21028;&#26029;&#65292;&#36824;&#21253;&#25324;&#23545;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#22810;&#36339;&#31561;&#65289;&#20013;&#20986;&#29616;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FactCHD&#65292;&#19968;&#31181;&#20026;LLMs&#31934;&#24515;&#35774;&#35745;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#12290;&#20316;&#20026;&#22312;&#8220;&#26597;&#35810;-&#21709;&#24212;&#8221;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20107;&#23454;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#37319;&#29992;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20107;&#23454;&#27169;&#24335;&#65292;&#22914;&#22522;&#26412;&#20107;&#23454;&#65292;&#22810;&#36339;&#65292;&#27604;&#36739;&#21644;&#38598;&#21512;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20934;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#65292;&#20174;&#32780;&#20415;&#20110;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SnD&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#38454;&#27573;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#20196;&#29260;&#23884;&#20837;&#23618;&#21644;&#24341;&#20837;&#22122;&#22768;&#26469;&#20248;&#21270;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.09130</link><description>&lt;p&gt;
&#20351;&#29992;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#65306;&#25286;&#20998;&#19982;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Split-and-Denoise: Protect large language model inference with local differential privacy. (arXiv:2310.09130v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SnD&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#38454;&#27573;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#20196;&#29260;&#23884;&#20837;&#23618;&#21644;&#24341;&#20837;&#22122;&#22768;&#26469;&#20248;&#21270;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#25429;&#25417;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#38544;&#34255;&#35821;&#20041;&#65292;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#36825;&#19968;&#36807;&#31243;&#20016;&#23500;&#20102;&#25991;&#26412;&#23884;&#20837;&#30340;&#20215;&#20540;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20316;&#20026;&#26381;&#21153;&#65288;EaaS&#65289;&#30340;&#23884;&#20837;&#27169;&#22411;&#21830;&#19994;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#25991;&#26412;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#38754;&#20020;&#30528;&#36739;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#65292;&#36825;&#26159;&#19968;&#20010;&#23578;&#26410;&#24471;&#21040;&#26377;&#25928;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Split-N-Denoise&#65288;SnD&#65289;&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#19978;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#25191;&#34892;&#20196;&#29260;&#23884;&#20837;&#23618;&#26469;&#25286;&#20998;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#23558;&#23884;&#20837;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#20043;&#21069;&#24341;&#20837;&#22122;&#22768;&#65292;&#24182;&#38543;&#21518;&#25509;&#25910;&#21644;&#21435;&#22122;&#21518;&#30340;&#25200;&#21160;&#36755;&#20986;&#23884;&#20837;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#20026;LLMs&#30340;&#25512;&#26029;&#38454;&#27573;&#35774;&#35745;&#65292;&#19981;&#38656;&#35201;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;SnD&#22312;&#21508;&#31181;LLM&#20013;&#20248;&#21270;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) shows powerful capability in natural language understanding by capturing hidden semantics in vector space. This process enriches the value of the text embeddings for various downstream tasks, thereby fostering the Embedding-as-a-Service (EaaS) business model. However, the direct transmission of text to servers poses a largely unaddressed risk of privacy leakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), an innovative framework that split the model to execute the token embedding layer on the client side at minimal computational cost. This allows the client to introduce noise prior to transmitting the embeddings to the server, and subsequently receive and denoise the perturbed output embeddings for downstream tasks. Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters. Extensive experiments demonstrate SnD's effectiveness in optimizing the privacy-utility tradeoff across various LLM a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#22823;&#22411;Transformer&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#20551;&#35774;&#27169;&#22411;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#33021;&#22815;&#27169;&#20223;&#19987;&#23478;&#31639;&#27861;&#22312;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#19978;&#30340;&#26465;&#20214;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2310.08566</link><description>&lt;p&gt;
&#20197;Transformer&#20026;&#20915;&#31574;&#32773;&#65306;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining. (arXiv:2310.08566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08566
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#22823;&#22411;Transformer&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#20551;&#35774;&#27169;&#22411;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#33021;&#22815;&#27169;&#20223;&#19987;&#23478;&#31639;&#27861;&#22312;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#19978;&#30340;&#26465;&#20214;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;Transformer&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#33021;&#21147;&#65292;&#21363;&#24403;&#23427;&#20204;&#38754;&#23545;&#26469;&#33258;&#26410;&#30693;&#29615;&#22659;&#30340;&#20132;&#20114;&#36712;&#36857;&#26102;&#65292;&#23427;&#20204;&#33021;&#22815;&#20570;&#20986;&#33391;&#22909;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#35757;&#32451;&#20197;&#25191;&#34892;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#29702;&#35770;&#19978;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;&#23578;&#19981;&#28165;&#26970;Transformer&#27169;&#22411;&#21487;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#25191;&#34892;&#21738;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#24046;&#24322;&#22914;&#20309;&#24433;&#21709;&#24050;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#23545;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#36825;&#21253;&#25324;&#20102;&#20004;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#65306;&#31639;&#27861;&#33976;&#39311;&#21644;&#20915;&#31574;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#22312;&#20551;&#35774;&#27169;&#22411;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32463;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#23558;&#27169;&#20223;&#19987;&#23478;&#31639;&#27861;&#22312;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#19978;&#30340;&#26465;&#20214;&#26399;&#26395;&#12290;&#24191;&#20041;&#35823;&#24046;&#30340;&#32553;&#25918;&#33539;&#22260;&#19982;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods -- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35821;&#27861;&#38169;&#35823;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;&#26041;&#27861;ToolDec&#65292;&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#31639;&#27861;&#28040;&#38500;&#20102;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#65292;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;</title><link>http://arxiv.org/abs/2310.07075</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#23454;&#29616;&#26080;&#35821;&#27861;&#38169;&#35823;&#21644;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding. (arXiv:2310.07075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35821;&#27861;&#38169;&#35823;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;&#26041;&#27861;ToolDec&#65292;&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#31639;&#27861;&#28040;&#38500;&#20102;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#65292;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#28041;&#21450;&#23545;&#24037;&#20855;&#28436;&#31034;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#26679;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#24037;&#20855;&#65292;&#35201;&#20040;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#24037;&#20855;&#25991;&#26723;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#24037;&#20855;&#25968;&#37327;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#24120;&#24120;&#20135;&#29983;&#35821;&#27861;&#26080;&#25928;&#30340;&#24037;&#20855;&#35843;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToolDec&#65292;&#19968;&#31181;&#26377;&#38480;&#29366;&#24577;&#26426;&#24341;&#23548;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#29992;&#20110;&#24037;&#20855;&#22686;&#24378;&#30340;LLMs&#12290;ToolDec&#36890;&#36807;&#30830;&#20445;&#26377;&#25928;&#30340;&#24037;&#20855;&#21517;&#31216;&#21644;&#31867;&#22411;&#19968;&#33268;&#30340;&#21442;&#25968;&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#24037;&#20855;&#22686;&#24378;&#30340;LLMs&#20013;&#30340;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;ToolDec&#20351;LLM&#33021;&#22815;&#20165;&#20165;&#20351;&#29992;&#23427;&#20204;&#30340;&#21517;&#31216;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26377;&#25928;&#22320;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#25968;&#23398;&#20989;&#25968;&#12289;&#30693;&#35782;&#22270;&#35889;&#20851;&#31995;&#21644;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;RESTful API&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#22810;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#21450;&#20854;ToolDec&#22686;&#24378;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promising capabilities in using external tools to solve complex problems. However, existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls. In this paper, we propose ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates tool-related errors for any tool-augmented LLMs by ensuring valid tool names and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation. We evaluated multiple prior methods and their ToolDec-enhanced versions on a variety of tasks involving tools like math functions, knowledge graph relations, and complex real-world RESTful APIs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06387</link><description>&lt;p&gt;
&#21482;&#38656;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#21363;&#21487;&#23454;&#29616;&#36234;&#29425;&#21644;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#30340;&#25285;&#24551;&#20063;&#28014;&#29616;&#20986;&#26469;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30456;&#21516;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#25805;&#32437;LLM&#23545;&#40784;&#33021;&#21147;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#33539;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#23601;&#21487;&#20197;&#25805;&#32437;LLM&#22686;&#21152;&#25110;&#38477;&#20302;&#36234;&#29425;&#27010;&#29575;&#65292;&#21363;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30446;&#30340;&#30340;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#65288;ICA&#65289;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#65288;ICD&#65289;&#26041;&#27861;&#12290;ICA&#36890;&#36807;&#26500;&#36896;&#24694;&#24847;&#19978;&#19979;&#25991;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#36755;&#20986;&#65292;&#32780;ICD&#36890;&#36807;&#25298;&#32477;&#22238;&#31572;&#26377;&#23475;&#25552;&#31034;&#30340;&#31034;&#33539;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ICA&#21644;ICD&#22312;&#22686;&#21152;&#25110;&#38477;&#20302;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;ICL&#22312;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#23433;&#20840;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20113;&#22522;&#30784;&#35774;&#26045;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#23433;&#20840;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#36890;&#20449;&#12290;&#23427;&#36890;&#36807;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#32467;&#26524;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#25972;&#20307;&#27169;&#22411;&#32858;&#21512;&#65292;&#36991;&#20813;&#20102;&#30452;&#25509;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#25928;&#12289;&#21487;&#20280;&#32553;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.05269</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65306;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#30340;&#21069;&#27839;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications. (arXiv:2310.05269v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05269
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#23433;&#20840;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20113;&#22522;&#30784;&#35774;&#26045;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#23433;&#20840;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#36890;&#20449;&#12290;&#23427;&#36890;&#36807;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#32467;&#26524;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#25972;&#20307;&#27169;&#22411;&#32858;&#21512;&#65292;&#36991;&#20813;&#20102;&#30452;&#25509;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#25928;&#12289;&#21487;&#20280;&#32553;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#23458;&#25143;&#21644;&#20027;&#26426;&#36830;&#25509;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31995;&#32479;&#39046;&#22495;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#23433;&#20840;&#20998;&#24067;&#24335;ML&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#38544;&#31169;&#23433;&#20840;&#24615;&#12290;FL&#36890;&#36807;&#20351;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#23558;ML&#27169;&#22411;&#20256;&#36755;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#65292;&#26377;&#25928;&#22320;&#23558;&#20113;&#22522;&#30784;&#35774;&#26045;&#19982;&#20013;&#24515;&#21270;&#21644;&#20998;&#25955;&#21270;&#31995;&#32479;&#30340;&#27969;&#31243;&#22788;&#29702;&#21644;&#25968;&#25454;&#23384;&#20648;&#38656;&#27714;&#30456;&#32467;&#21512;&#65292;&#27880;&#37325;&#21487;&#20280;&#32553;&#24615;&#12289;&#38544;&#31169;&#32771;&#34385;&#21644;&#32463;&#27982;&#39640;&#25928;&#30340;&#36890;&#20449;&#12290;&#22312;&#24403;&#21069;&#30340;FL&#23454;&#29616;&#20013;&#65292;&#25968;&#25454;&#25152;&#26377;&#32773;&#22312;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#20197;&#26435;&#37325;&#12289;&#26799;&#24230;&#21644;&#21442;&#25968;&#30340;&#24418;&#24335;&#23558;&#32467;&#26524;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#25972;&#20307;&#27169;&#22411;&#32858;&#21512;&#12290;&#36825;&#31181;&#21019;&#26032;&#28040;&#38500;&#20102;&#19982;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#23458;&#25143;&#21644;&#21442;&#19982;&#32773;&#30452;&#25509;&#19982;&#20113;&#20013;&#24515;&#36890;&#20449;&#21407;&#22987;&#21644;&#28508;&#22312;&#26426;&#23494;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#12290;&#36825;&#19981;&#20165;&#38477;&#20302;&#20102;&#19982;&#19982;&#20256;&#32479;&#20113;&#20013;&#24515;&#36890;&#20449;&#30456;&#20851;&#30340;&#36153;&#29992;&#65292;&#36824;&#20445;&#25252;&#20102;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of machine learning (ML) systems featuring client-host connections, the enhancement of privacy security can be effectively achieved through federated learning (FL) as a secure distributed ML methodology. FL effectively integrates cloud infrastructure to transfer ML models onto edge servers using blockchain technology. Through this mechanism, it guarantees the streamlined processing and data storage requirements of both centralized and decentralized systems, with an emphasis on scalability, privacy considerations, and cost-effective communication. In current FL implementations, data owners locally train their models, and subsequently upload the outcomes in the form of weights, gradients, and parameters to the cloud for overall model aggregation. This innovation obviates the necessity of engaging Internet of Things (IoT) clients and participants to communicate raw and potentially confidential data directly with a cloud center. This not only reduces the costs associated with 
&lt;/p&gt;</description></item><item><title>GRAPES&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#22312;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26102;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.03399</link><description>&lt;p&gt;
GRAPES: &#23398;&#20064;&#29992;&#20110;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks. (arXiv:2310.03399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03399
&lt;/p&gt;
&lt;p&gt;
GRAPES&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#22312;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26102;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#20197;&#19981;&#21516;&#26041;&#24335;&#32858;&#21512;&#21608;&#22260;&#20449;&#24687;&#26469;&#23398;&#20064;&#22270;&#20013;&#33410;&#28857;&#30340;&#34920;&#31034;&#12290;&#38543;&#30528;&#36825;&#20123;&#32593;&#32476;&#30340;&#21152;&#28145;&#65292;&#30001;&#20110;&#37051;&#22495;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#24863;&#21463;&#37326;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#23548;&#33268;&#39640;&#20869;&#23384;&#28040;&#32791;&#12290;&#22270;&#37319;&#26679;&#36890;&#36807;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;GNNs&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;GNNs&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#22270;&#12290;&#22823;&#22810;&#25968;&#37319;&#26679;&#26041;&#27861;&#19987;&#27880;&#20110;&#22266;&#23450;&#30340;&#37319;&#26679;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#32467;&#26500;&#25110;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GRAPES&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#35782;&#21035;&#29992;&#20110;&#35757;&#32451;GNN&#20998;&#31867;&#22120;&#30340;&#19968;&#32452;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#12290;GRAPES&#20351;&#29992;GFlowNet&#26469;&#23398;&#20064;&#32473;&#23450;&#20998;&#31867;&#30446;&#26631;&#30340;&#33410;&#28857;&#37319;&#26679;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#22270;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;GRAPES&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#30456;&#27604;&#65292;GRAPES&#21363;&#20351;&#22312;&#37319;&#26679;&#27604;&#20363;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#20173;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) learn the representation of nodes in a graph by aggregating the neighborhood information in various ways. As these networks grow in depth, their receptive field grows exponentially due to the increase in neighborhood sizes, resulting in high memory costs. Graph sampling solves memory issues in GNNs by sampling a small ratio of the nodes in the graph. This way, GNNs can scale to much larger graphs. Most sampling methods focus on fixed sampling heuristics, which may not generalize to different structures or tasks. We introduce GRAPES, an adaptive graph sampling method that learns to identify sets of influential nodes for training a GNN classifier. GRAPES uses a GFlowNet to learn node sampling probabilities given the classification objectives. We evaluate GRAPES across several small- and large-scale graph benchmarks and demonstrate its effectiveness in accuracy and scalability. In contrast to existing sampling methods, GRAPES maintains high accuracy even with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19981;&#19968;&#23450;&#38656;&#35201;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#20013;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.00771</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pre-training with Synthetic Data Helps Offline Reinforcement Learning. (arXiv:2310.00771v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19981;&#19968;&#23450;&#38656;&#35201;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#20013;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;Decision Transformer&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;&#36825;&#31181;&#24615;&#33021;&#25552;&#21319;&#26159;&#21542;&#21482;&#33021;&#36890;&#36807;&#35821;&#35328;&#39044;&#35757;&#32451;&#23454;&#29616;&#65292;&#36824;&#26159;&#21487;&#20197;&#36890;&#36807;&#19981;&#28041;&#21450;&#35821;&#35328;&#30340;&#26356;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#35821;&#35328;&#23545;&#20110;&#25913;&#21892;&#24615;&#33021;&#24182;&#19981;&#26159;&#24517;&#35201;&#30340;&#65292;&#23454;&#38469;&#19978;&#65292;&#20351;&#29992;&#21512;&#25104;&#30340;IID&#25968;&#25454;&#36827;&#34892;&#23569;&#37327;&#26356;&#26032;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#19982;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#25552;&#21319;&#65307;&#27492;&#22806;&#65292;&#20351;&#29992;&#19968;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#23454;&#39564;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#39044;&#35757;&#32451;Conservative Q-Learning(CQL)&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31163;&#32447;DRL&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;Q-learning&#65292;&#24182;&#36890;&#24120;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#39592;&#24178;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#26696;&#20063;&#33021;&#22312;CQL&#31639;&#27861;&#20013;&#21462;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with sim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35770;&#35777;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#38598;&#20013;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#19982;&#21542;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#38598;&#20013;&#22312;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#24182;&#38598;&#26102;&#65292;&#21033;&#29992;&#25968;&#25454;&#32467;&#26500;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16096</link><description>&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#21487;&#33021;&#26159;&#21487;&#20197;&#36991;&#20813;&#30340;&#65306;&#25968;&#25454;&#38598;&#20013;&#24615;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness. (arXiv:2309.16096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#35777;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#38598;&#20013;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#19982;&#21542;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#38598;&#20013;&#22312;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#24182;&#38598;&#26102;&#65292;&#21033;&#29992;&#25968;&#25454;&#32467;&#26500;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#20110;&#23545;&#25239;&#26679;&#26412;&#30340;&#25935;&#24863;&#24615;&#24341;&#21457;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#26263;&#31034;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#21487;&#33021;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#21487;&#33021;&#36807;&#20110;&#19968;&#33324;&#21270;&#65292;&#26080;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#25968;&#25454;&#20998;&#24067;&#12290;&#20107;&#23454;&#19978;&#65292;&#20154;&#31867;&#22312;&#28041;&#21450;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#30456;&#24403;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#26126;&#26174;&#30340;&#30683;&#30462;&#25512;&#21160;&#25105;&#20204;&#26356;&#28145;&#20837;&#22320;&#25506;&#32034;&#19968;&#20010;&#38382;&#39064;&#65306;&#23545;&#25239;&#26679;&#26412;&#26159;&#21542;&#30495;&#30340;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#20010;&#20851;&#38190;&#23646;&#24615;&#8212;&#8212;&#23545;&#36755;&#20837;&#31354;&#38388;&#30340;&#23567;&#23481;&#31215;&#23376;&#38598;&#30340;&#38598;&#20013;&#31243;&#24230;&#65292;&#20915;&#23450;&#20102;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#40065;&#26834;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#22312;&#25968;&#25454;&#20998;&#24067;&#38598;&#20013;&#22312;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#24182;&#38598;&#26102;&#65292;&#21033;&#29992;&#25968;&#25454;&#32467;&#26500;&#33258;&#28982;&#22320;&#24471;&#21040;&#20139;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#22312;&#29305;&#23450;&#33539;&#22260;&#20869;&#21487;&#35777;&#26126;&#35748;&#35777;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The susceptibility of modern machine learning classifiers to adversarial examples has motivated theoretical results suggesting that these might be unavoidable. However, these results can be too general to be applicable to natural data distributions. Indeed, humans are quite robust for tasks involving vision. This apparent conflict motivates a deeper dive into the question: Are adversarial examples truly unavoidable? In this work, we theoretically demonstrate that a key property of the data distribution -- concentration on small-volume subsets of the input space -- determines whether a robust classifier exists. We further demonstrate that, for a data distribution concentrated on a union of low-dimensional linear subspaces, exploiting data structure naturally leads to classifiers that enjoy good robustness guarantees, improving upon methods for provable certification in certain regimes.
&lt;/p&gt;</description></item><item><title>&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15293</link><description>&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15293
&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20811;&#26381;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#37117;&#24314;&#31435;&#22312;&#25968;&#25454;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#26159;&#20381;&#27425;&#20174;&#20195;&#29702;&#32463;&#39564;&#20013;&#25910;&#38598;&#32780;&#26469;&#26102;&#65292;&#36825;&#19968;&#20551;&#35774;&#36890;&#24120;&#19981;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26368;&#22823;&#25193;&#25955;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32479;&#35745;&#21147;&#23398;&#20013;&#30340;&#36941;&#21382;&#36807;&#31243;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#20195;&#29702;&#30340;&#32463;&#39564;&#65292;&#21487;&#35777;&#26126;&#22320;&#20351;&#20195;&#29702;&#22312;&#21333;&#27425;&#37096;&#32626;&#20013;&#33021;&#22815;&#25345;&#32493;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#21021;&#22987;&#21270;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#24191;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#26368;&#22823;&#29109;&#25216;&#26415;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#31283;&#23450;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#26524;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#29289;&#29702;&#23398;&#12289;&#23398;&#20064;&#21644;&#25511;&#21046;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#34892;&#36208;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#30340;&#36879;&#26126;&#21487;&#38752;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#26465;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The assumption that data are independent and identically distributed underpins all machine learning. When data are collected sequentially from agent experiences this assumption does not generally hold, as in reinforcement learning. Here, we derive a method that overcomes these limitations by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables agents to learn continually in single-shot deployments regardless of how they are initialized. Moreover, we prove our approach generalizes well-known maximum entropy techniques, and show that it robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning, and control pave the way towards more transparent and reliable decision-making in reinforcement learning agents, such as locomoting robots and self-driving cars.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#32534;&#30721;&#36712;&#36857;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.15278</link><description>&lt;p&gt;
&#30524;&#19981;&#35265;&#24515;&#19981;&#24565;&#65306;&#21033;&#29992;&#35270;&#39057;&#36319;&#36394;&#21551;&#29992;&#30340;&#35760;&#24518;&#27169;&#22411;&#23545;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Out of Sight, Still in Mind: Reasoning and Planning about Unobserved Objects with Video Tracking Enabled Memory Models. (arXiv:2309.15278v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#32534;&#30721;&#36712;&#36857;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#38656;&#35201;&#20855;&#26377;&#23545;&#20808;&#21069;&#35266;&#23519;&#21040;&#20294;&#24403;&#21069;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#30340;&#35760;&#24518;&#65292;&#20197;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#21487;&#38752;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#38754;&#21521;&#23545;&#35937;&#30340;&#35760;&#24518;&#32534;&#30721;&#21040;&#22810;&#23545;&#35937;&#25805;&#32437;&#25512;&#29702;&#21644;&#35268;&#21010;&#26694;&#26550;&#20013;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DOOM&#21644;LOOM&#65292;&#23427;&#20204;&#21033;&#29992;&#36716;&#25442;&#22120;&#20851;&#31995;&#21160;&#21147;&#23398;&#26469;&#32534;&#30721;&#32473;&#23450;&#37096;&#20998;&#35270;&#28857;&#20113;&#21644;&#23545;&#35937;&#21457;&#29616;&#19982;&#36319;&#36394;&#24341;&#25806;&#30340;&#36712;&#36857;&#21382;&#21490;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25191;&#34892;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#34987;&#36974;&#25377;&#30340;&#23545;&#35937;&#65292;&#26032;&#20986;&#29616;&#30340;&#23545;&#35937;&#65292;&#20197;&#21450;&#29289;&#20307;&#37325;&#26032;&#20986;&#29616;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#21644;&#19981;&#21516;&#25968;&#37327;&#30340;&#24178;&#25200;&#21160;&#20316;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#38544;&#24335;&#35760;&#24518;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots need to have a memory of previously observed, but currently occluded objects to work reliably in realistic environments. We investigate the problem of encoding object-oriented memory into a multi-object manipulation reasoning and planning framework. We propose DOOM and LOOM, which leverage transformer relational dynamics to encode the history of trajectories given partial-view point clouds and an object discovery and tracking engine. Our approaches can perform multiple challenging tasks including reasoning with occluded objects, novel objects appearance, and object reappearance. Throughout our extensive simulation and real-world experiments, we find that our approaches perform well in terms of different numbers of objects and different numbers of distractor actions. Furthermore, we show our approaches outperform an implicit memory baseline.
&lt;/p&gt;</description></item><item><title>LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12288</link><description>&lt;p&gt;
&#32763;&#36716;&#35781;&#21650;: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;"A&#26159;B"&#26080;&#27861;&#23398;&#20064;"B&#26159;A"
&lt;/p&gt;
&lt;p&gt;
The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12288
&lt;/p&gt;
&lt;p&gt;
LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27867;&#21270;&#19978;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#12290;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"A&#26159;B"&#24418;&#24335;&#30340;&#21477;&#23376;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#25512;&#24191;&#21040;&#30456;&#21453;&#30340;&#26041;&#21521;"B&#26159;A"&#12290;&#36825;&#23601;&#26159;&#32763;&#36716;&#35781;&#21650;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"Olaf Scholz&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;"&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#33021;&#22815;&#22238;&#31572;&#38382;&#39064;"&#35841;&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;&#65311;"&#12290;&#27492;&#22806;&#65292;&#27491;&#30830;&#31572;&#26696;&#65288;"Olaf Scholz"&#65289;&#30340;&#21487;&#33021;&#24615;&#19981;&#20250;&#27604;&#38543;&#26426;&#21517;&#23383;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#20250;&#25512;&#24191;&#21040;&#23427;&#20204;&#35757;&#32451;&#38598;&#20013;&#30340;&#26222;&#36941;&#27169;&#24335;&#65288;&#21363;&#22914;&#26524;&#20986;&#29616;"A&#26159;B"&#65292;&#21017;"B&#26159;A"&#26356;&#21487;&#33021;&#20986;&#29616;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#34394;&#26500;&#30340;&#38472;&#36848;&#65288;&#22914;"Uriah Hawthorne&#26159;'Abyssal Melodies'&#30340;&#20316;&#26354;&#23478;"&#65289;&#19978;&#23545;GPT-3&#21644;Llama-1&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#26080;&#27861;&#27491;&#30830;&#22238;&#31572;"&#35841;&#21019;&#20316;&#20102;'Abyssal Melodies'?"&#26469;&#25552;&#20379;&#32763;&#36716;&#35781;&#21650;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
&lt;/p&gt;</description></item><item><title>Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11489</link><description>&lt;p&gt;
Text2Reward&#65306;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11489
&lt;/p&gt;
&lt;p&gt;
Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#65307;&#23427;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#25110;&#39046;&#22495;&#25968;&#25454;&#65292;&#23548;&#33268;&#24320;&#21457;&#25104;&#26412;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Text2Reward&#65292;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#21487;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#30446;&#26631;&#65292;Text2Reward&#29983;&#25104;&#20316;&#20026;&#29615;&#22659;&#32039;&#20945;&#34920;&#31034;&#30340;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#19982;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#36817;&#20351;&#29992;LLM&#32534;&#20889;&#31232;&#30095;&#22870;&#21169;&#20195;&#30721;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;Text2Reward&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20195;&#30721;&#65292;&#21487;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#65292;&#21033;&#29992;&#29616;&#26377;&#36719;&#20214;&#21253;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26426;&#22120;&#20154;&#25805;&#20316;&#22522;&#20934;&#65288;ManiSkill2&#65292;MetaWorld&#65289;&#21644;&#20004;&#20010;MuJoCo&#30340;&#36816;&#21160;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;Text2Reward&#12290;&#22312;17&#20010;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;13&#20010;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#22870;&#21169;&#20195;&#30721;&#35757;&#32451;&#30340;&#25919;&#31574;&#23454;&#29616;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21160;&#24577;&#26694;&#26550;&#65292;&#21033;&#29992;&#26469;&#33258;&#25216;&#26415;&#20844;&#21496;&#30340;21&#19990;&#32426;ESG&#25253;&#21578;&#30340;&#20016;&#23500;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;ESG&#35266;&#28857;&#30340;&#28436;&#21270;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.07001</link><description>&lt;p&gt;
&#20225;&#19994;ESG&#25253;&#21578;&#30340;&#21160;&#24577;&#20998;&#26512;&#65306;&#19968;&#20010;&#28436;&#21270;&#36235;&#21183;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dynamic Analysis of Corporate ESG Reports: A Model of Evolutionary Trends. (arXiv:2309.07001v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21160;&#24577;&#26694;&#26550;&#65292;&#21033;&#29992;&#26469;&#33258;&#25216;&#26415;&#20844;&#21496;&#30340;21&#19990;&#32426;ESG&#25253;&#21578;&#30340;&#20016;&#23500;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;ESG&#35266;&#28857;&#30340;&#28436;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;(ESG)&#25253;&#21578;&#34987;&#20840;&#29699;&#20844;&#35748;&#20026;&#21487;&#25345;&#32493;&#20225;&#19994;&#21457;&#23637;&#30340;&#22522;&#30707;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#32472;&#21046;&#20840;&#29699;&#24066;&#22330;&#20013;&#20844;&#21496;ESG&#20027;&#39064;&#30340;&#21464;&#21270;&#26684;&#23616;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21160;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#20010;&#21035;&#31867;&#21035;&#12289;&#22810;&#20010;&#31867;&#21035;&#21644;&#19982;&#29305;&#23450;&#21487;&#25345;&#32493;&#24615;&#25351;&#25968;&#20445;&#25345;&#19968;&#33268;&#30340;ESG&#25112;&#30053;&#31649;&#29702;&#12290;&#36825;&#20123;&#20998;&#26512;&#36807;&#31243;&#30340;&#36755;&#20986;&#26500;&#25104;&#20102;ESG&#25112;&#30053;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#36890;&#36807;&#23558;&#20998;&#26512;&#24615;&#20851;&#38190;&#35789;&#32435;&#20837;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26469;&#33258;&#25216;&#26415;&#20844;&#21496;&#30340;21&#19990;&#32426;ESG&#25253;&#21578;&#30340;&#20016;&#23500;&#25910;&#38598;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#38416;&#26126;&#20102;ESG&#35266;&#28857;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#39564;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#36817;&#24180;&#26469;ESG&#20027;&#39064;&#30340;&#24182;&#34892;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Environmental, social, and governance (ESG) reports are globally recognized as a keystone in sustainable enterprise development. This study aims to map the changing landscape of ESG topics within firms in the global market. A dynamic framework is developed to analyze ESG strategic management for individual classes, across multiple classes, and in alignment with a specific sustainability index. The output of these analytical processes forms the foundation of an ESG strategic model. Utilizing a rich collection of 21st-century ESG reports from technology companies, our experiment elucidates the changes in ESG perspectives by incorporating analytical keywords into the proposed framework. This work thus provides an empirical method that reveals the concurrent evolution of ESG topics over recent years.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#24037;&#20316;&#39057;&#29575;&#20165;&#21463;&#38480;&#20110;&#26356;&#24555;&#30340;&#29615;&#32469;&#28608;&#20809;&#38647;&#36798;&#32780;&#19981;&#26159;&#36739;&#24930;&#30340;&#29615;&#32469;&#38647;&#36798;&#34701;&#21512;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#65292;&#20197;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#21709;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04806</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#30340;&#21450;&#26102;&#34701;&#21512;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Timely Fusion of Surround Radar/Lidar for Object Detection in Autonomous Driving Systems. (arXiv:2309.04806v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#24037;&#20316;&#39057;&#29575;&#20165;&#21463;&#38480;&#20110;&#26356;&#24555;&#30340;&#29615;&#32469;&#28608;&#20809;&#38647;&#36798;&#32780;&#19981;&#26159;&#36739;&#24930;&#30340;&#29615;&#32469;&#38647;&#36798;&#34701;&#21512;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#65292;&#20197;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#21709;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#20256;&#24863;&#22120;&#25968;&#25454;&#34701;&#21512;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21608;&#22260;&#29615;&#22659;&#37325;&#24314;&#12290;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#25104;&#26412;&#25552;&#20379;360&#24230;&#35270;&#37326;&#37319;&#26679;&#65292;&#26159;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#26377;&#21069;&#26223;&#30340;&#24863;&#30693;&#30828;&#20214;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#29289;&#29702;&#38480;&#21046;&#65292;&#29615;&#32469;&#38647;&#36798;&#30340;&#26059;&#36716;&#36895;&#24230;&#21450;&#29983;&#25104;&#38647;&#36798;&#25968;&#25454;&#24103;&#30340;&#39057;&#29575;&#36828;&#20302;&#20110;&#29615;&#32469;&#28608;&#20809;&#38647;&#36798;&#12290;&#29616;&#26377;&#30340;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#34701;&#21512;&#26041;&#27861;&#24517;&#39035;&#20197;&#29615;&#32469;&#38647;&#36798;&#30340;&#20302;&#39057;&#29575;&#24037;&#20316;&#65292;&#32780;&#26080;&#27861;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#39640;&#21709;&#24212;&#24615;&#35201;&#27714;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;MVDNet&#24320;&#21457;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#24037;&#20316;&#39057;&#29575;&#20165;&#21463;&#38480;&#20110;&#26356;&#24555;&#30340;&#29615;&#32469;&#28608;&#20809;&#38647;&#36798;&#32780;&#19981;&#26159;&#36739;&#24930;&#30340;&#29615;&#32469;&#38647;&#36798;&#34701;&#21512;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#22522;&#26412;&#24605;&#36335;&#24456;&#31616;&#21333;&#65306;&#35753;MVDNet&#22788;&#29702;&#26242;&#26102;&#19981;&#23545;&#40784;&#30340;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#24103;&#65292;&#28982;&#21518;&#26681;&#25454;&#24103;&#26102;&#38388;&#20449;&#24687;&#23545;&#34701;&#21512;&#21518;&#30340;&#32467;&#26524;&#36827;&#34892;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fusing Radar and Lidar sensor data can fully utilize their complementary advantages and provide more accurate reconstruction of the surrounding for autonomous driving systems. Surround Radar/Lidar can provide 360-degree view sampling with the minimal cost, which are promising sensing hardware solutions for autonomous driving systems. However, due to the intrinsic physical constraints, the rotating speed of surround Radar, and thus the frequency to generate Radar data frames, is much lower than surround Lidar. Existing Radar/Lidar fusion methods have to work at the low frequency of surround Radar, which cannot meet the high responsiveness requirement of autonomous driving systems.This paper develops techniques to fuse surround Radar/Lidar with working frequency only limited by the faster surround Lidar instead of the slower surround Radar, based on the state-of-the-art object detection model MVDNet. The basic idea of our approach is simple: we let MVDNet work with temporally unaligned d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04160</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#26469;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#32463;&#24120;&#21576;&#29616;&#31232;&#30095;&#29305;&#24449;&#65292;&#32473;&#39044;&#27979;&#24314;&#27169;&#24102;&#26469;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#30452;&#25509;&#25554;&#34917;&#26041;&#27861;&#65288;&#22914;&#30697;&#38453;&#25554;&#34917;&#26041;&#27861;&#65289;&#20381;&#36182;&#20110;&#21442;&#32771;&#31867;&#20284;&#34892;&#25110;&#21015;&#26469;&#23436;&#25104;&#21407;&#22987;&#32570;&#22833;&#25968;&#25454;&#65292;&#19981;&#21306;&#20998;&#25554;&#34917;&#21644;&#23454;&#38469;&#20540;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23558;&#19982;&#39044;&#27979;&#30446;&#26631;&#26080;&#20851;&#30340;&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20174;&#32780;&#25439;&#23475;&#19979;&#28216;&#24615;&#33021;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#22312;&#30452;&#25509;&#25554;&#34917;&#21518;&#37325;&#26032;&#26657;&#20934;&#25110;&#22686;&#24378;EHR&#23884;&#20837;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38169;&#35823;&#22320;&#20248;&#20808;&#32771;&#34385;&#25554;&#34917;&#29305;&#24449;&#12290;&#36825;&#31181;&#20248;&#20808;&#38169;&#35823;&#21487;&#33021;&#20250;&#32473;&#27169;&#22411;&#24341;&#20837;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37319;&#29992;&#38388;&#25509;&#25554;&#34917;&#65292;&#21033;&#29992;&#31867;&#20284;&#24739;&#32773;&#30340;&#21407;&#22411;&#34920;&#31034;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#12290;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#26102;&#36890;&#24120;&#23558;&#32570;&#22833;&#29305;&#24449;&#19982;&#23384;&#22312;&#29305;&#24449;&#30456;&#21516;&#30340;&#38480;&#21046;&#26102;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22238;&#24402;&#38382;&#39064;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#37327;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16245</link><description>&lt;p&gt;
&#22238;&#24402;&#38382;&#39064;&#30340;&#26657;&#20934;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22238;&#24402;&#38382;&#39064;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#37327;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36890;&#24120;&#26159;&#29616;&#20195;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;DSS&#65289;&#30340;&#19968;&#37096;&#20998;&#12290;&#22312;&#22522;&#20110;AI&#30340;DSS&#20013;&#20351;&#29992;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#21019;&#24314;&#33021;&#22815;&#21521;&#20154;&#31867;&#29992;&#25143;&#35299;&#37322;&#20854;&#29702;&#30001;&#30340;AI&#31995;&#32479;&#12290;XAI&#20013;&#30340;&#23616;&#37096;&#35299;&#37322;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#20010;&#21035;&#39044;&#27979;&#30340;&#21407;&#22240;&#30340;&#20449;&#24687;&#65292;&#21363;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#28857;&#26159;&#26080;&#27861;&#37327;&#21270;&#19982;&#29305;&#24449;&#37325;&#35201;&#24615;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;Calibrated Explanations&#65288;CE&#65289;&#30340;&#25193;&#23637;&#65292;&#20043;&#21069;&#21482;&#25903;&#25345;&#20998;&#31867;&#65292;&#29616;&#22312;&#25903;&#25345;&#26631;&#20934;&#22238;&#24402;&#21644;&#27010;&#29575;&#22238;&#24402;&#65292;&#21363;&#30446;&#26631;&#36229;&#36807;&#20219;&#24847;&#38408;&#20540;&#30340;&#27010;&#29575;&#12290;&#22238;&#24402;&#38382;&#39064;&#30340;&#25193;&#23637;&#20445;&#30041;&#20102;CE&#30340;&#25152;&#26377;&#20248;&#28857;&#65292;&#20363;&#22914;&#23558;&#24213;&#23618;&#27169;&#22411;&#30340;&#39044;&#27979;&#19982;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is often an integral part of modern decision support systems (DSSs). The best-performing predictive models used in AI-based DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature's importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations (CE), previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of CE, such as calibration of the prediction from the underlying model with confidenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLogic&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39046;&#22495;&#24605;&#32500;&#38142;&#36873;&#25321;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24179;&#34913;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15324</link><description>&lt;p&gt;
FedLogic: &#21487;&#35299;&#37322;&#21270;&#30340;&#32852;&#37030;&#22810;&#39046;&#22495;&#24605;&#32500;&#38142;&#36873;&#25321;&#26041;&#27861;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FedLogic: Interpretable Federated Multi-Domain Chain-of-Thought Prompt Selection for Large Language Models. (arXiv:2308.15324v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLogic&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39046;&#22495;&#24605;&#32500;&#38142;&#36873;&#25321;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24179;&#34913;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#8220;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#8221;&#25512;&#29702;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#33719;&#21462;&#24555;&#36895;&#31934;&#30830;&#30340;&#22238;&#31572;&#27491;&#36805;&#36895;&#24341;&#36215;&#30740;&#31350;&#30028;&#30340;&#20852;&#36259;&#12290;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#35774;&#35745;&#25110;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#12290;&#25552;&#31034;&#36873;&#25321;&#30340;&#36807;&#31243;&#20381;&#36182;&#20110;&#29992;&#25143;&#26681;&#25454;LLM&#29983;&#25104;&#30340;&#30456;&#24212;&#26032;&#21453;&#24212;&#19981;&#26029;&#35843;&#25972;&#21644;&#32452;&#21512;&#36755;&#20837;&#25552;&#31034;&#30340;&#35797;&#38169;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#30740;&#31350;&#25506;&#35752;LLM&#22914;&#20309;&#21033;&#29992;&#20174;&#29992;&#25143;&#20132;&#20114;&#20013;&#23398;&#20064;&#21040;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#33021;&#21147;&#26469;&#35299;&#20915;&#21465;&#36848;&#20889;&#20316;&#20013;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25913;&#36827;&#21487;&#35299;&#37322;&#24615;&#24182;&#22312;&#22810;&#39046;&#22495;CoT&#25552;&#31034;&#36873;&#25321;&#22330;&#26223;&#19979;&#25506;&#32034;&#36890;&#29992;&#24615;&#21644;&#20010;&#24615;&#21270;&#20043;&#38388;&#30340;&#24179;&#34913;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#26041;&#27861;&#65288;FedLogic&#65289;&#12290;&#25105;&#20204;&#22312;&#32852;&#37030;LLM&#30340;&#32972;&#26223;&#19979;&#24341;&#20837;&#20102;&#22810;&#39046;&#22495;CoT&#25552;&#31034;&#36873;&#25321;&#22256;&#22659;&#30340;&#29702;&#35770;&#24418;&#24335;&#21270;&#21644;&#20132;&#20114;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging ``chain-of-thought (CoT)'' reasoning to elicit rapid and precise responses from large language models (LLMs) is rapidly attracting research interest. A notable challenge here is how to design or select optimal prompts. The process of prompt selection relies on trial and error, involving continuous adjustments and combinations of input prompts by users based on the corresponding new responses generated from LLMs. Furthermore, minimal research has been conducted to explore how LLMs employ the mathematical problem-solving capabilities learned from user interactions to address issues in narrative writing. To improve interpretability and explore the balance principle between generality and personalization under a multi-domain CoT prompt selection scenario, we propose the Federated Logic rule learning approach (FedLogic). We introduce a theoretical formalization and interactive emulation of the multi-domain CoT prompt selection dilemma in the context of federated LLMs. We cast the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11842</link><description>&lt;p&gt;
&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2308.11842v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#30028;&#20013;&#35782;&#21035;&#21644;&#20998;&#26512;&#23545;&#31216;&#27169;&#24335;&#24050;&#32463;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#21457;&#29616;&#65292;&#20363;&#22914;&#29289;&#29702;&#23398;&#20013;&#30340;&#24341;&#21147;&#23450;&#24459;&#30340;&#21046;&#23450;&#21644;&#21270;&#23398;&#32467;&#26500;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#21033;&#29992;&#22312;&#26576;&#20123;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#38382;&#39064;&#20013;&#22266;&#26377;&#30340;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#20197;&#21450;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24418;&#24335;&#21270;&#22320;&#25551;&#36848;&#19968;&#31867;&#20855;&#26377;&#19968;&#33324;&#23545;&#31216;&#24615;&#27010;&#24565;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#35813;&#27010;&#24565;&#20801;&#35768;&#23384;&#22312;&#23545;&#31216;&#30340;&#26368;&#20248;&#20540;&#21644;&#31574;&#30053;&#12290;&#21463;&#21040;&#36825;&#20123;&#24615;&#36136;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20855;&#26377;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20316;&#20026;&#22810;&#26234;&#20307;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#24402;&#32435;&#20559;&#24046;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#22312;&#20855;&#26377;&#37325;&#22797;&#23545;&#31216;&#27169;&#24335;&#30340;&#26410;&#35265;&#22330;&#26223;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#31561;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11534</link><description>&lt;p&gt;
&#20316;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;ChatGPT&#30340;&#21331;&#36234;&#24615;&#33021;&#24341;&#21457;&#20102;&#23545;&#20854;&#27665;&#20027;&#21270;&#30340;&#21162;&#21147;&#65292;&#20511;&#21161;&#30495;&#23454;&#29992;&#25143;&#21644;ChatGPT&#23545;&#35805;&#30340;&#21162;&#21147;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;Vicuna&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;Baize&#21644;UltraChat&#31561;&#21162;&#21147;&#20027;&#35201;&#20381;&#38752;ChatGPT&#26681;&#25454;&#25351;&#20196;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#30340;&#20154;&#31867;&#23398;&#20064;&#65292;&#23548;&#33268;&#33539;&#22260;&#26377;&#38480;&#65292;&#22810;&#26679;&#24615;&#20943;&#24369;&#65292;&#32570;&#20047;&#30495;&#27491;&#30340;&#22810;&#36718;&#23545;&#35805;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#25226;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#12290;&#38543;&#21518;&#65292;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#25105;&#20204;&#30340;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;Vicuna-Bench&#21644;MT-Bench&#20013;&#22343;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11129</link><description>&lt;p&gt;
&#20351;&#29992;&#23618;&#27425;&#32467;&#26500;&#36317;&#31163;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#30340;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21464;&#21387;&#22120;&#38656;&#35201;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#26469;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25552;&#35758;&#24456;&#23569;&#28041;&#21450;&#25429;&#25417;&#26356;&#38271;&#36317;&#31163;&#12289;&#23618;&#27425;&#32467;&#26500;&#25110;&#31038;&#21306;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#32780;&#36825;&#20123;&#22312;&#20998;&#23376;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#24341;&#29992;&#32593;&#32476;&#31561;&#21508;&#31181;&#22270;&#24418;&#20013;&#37117;&#20250;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#23618;&#27425;&#36317;&#31163;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22810;&#23618;&#27425;&#12289;&#23618;&#27425;&#21270;&#30340;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20135;&#29983;&#20102;&#19968;&#20010;&#21487;&#20197;&#28789;&#27963;&#19982;&#29616;&#26377;&#22270;&#21464;&#21387;&#22120;&#38598;&#25104;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#20301;&#32622;&#34920;&#31034;&#21516;&#26102;&#24212;&#29992;&#12290;&#36890;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;HDSE&#26041;&#27861;&#25104;&#21151;&#25552;&#21319;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#22522;&#32447;&#21464;&#21387;&#22120;&#65292;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;LEGO&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#20351;&#29992;LiDAR&#21333;&#29420;&#36827;&#34892;&#36319;&#36394;&#30340;LEGO&#26041;&#27861;&#22312;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09908</link><description>&lt;p&gt;
LEGO: &#23545;&#20110;&#22522;&#20110;&#28857;&#20113;&#30340;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;
&lt;/p&gt;
&lt;p&gt;
LEGO: Learning and Graph-Optimized Modular Tracker for Online Multi-Object Tracking with Point Clouds. (arXiv:2308.09908v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;LEGO&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#20351;&#29992;LiDAR&#21333;&#29420;&#36827;&#34892;&#36319;&#36394;&#30340;LEGO&#26041;&#27861;&#22312;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#36319;&#36394;-&#26816;&#27979;&#26041;&#27861;&#65292;&#25968;&#25454;&#20851;&#32852;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#65288;LEGO&#65289;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;LEGO&#36319;&#36394;&#22120;&#38598;&#25104;&#20102;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21046;&#23450;&#20851;&#32852;&#35780;&#20998;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#39640;&#25928;&#30340;&#30446;&#26631;&#21305;&#37197;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#29366;&#24577;&#26356;&#26032;&#36807;&#31243;&#65292;&#26412;&#25991;&#36824;&#28155;&#21152;&#20102;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#23558;&#23545;&#35937;&#29366;&#24577;&#30340;&#26102;&#38388;&#36830;&#36143;&#24615;&#32435;&#20837;&#36319;&#36394;&#20013;&#65292;&#30830;&#20445;&#19968;&#33268;&#30340;&#36319;&#36394;&#12290;&#19982;&#20854;&#20182;&#22312;&#32447;&#36319;&#36394;&#26041;&#27861;&#65288;&#21253;&#25324;&#22522;&#20110;LiDAR&#21644;&#22522;&#20110;LiDAR-&#30456;&#26426;&#34701;&#21512;&#30340;&#26041;&#27861;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20165;&#21033;&#29992;LiDAR&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#24615;&#33021;&#12290;&#22312;&#25552;&#20132;&#32467;&#26524;&#33267;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#25490;&#34892;&#27036;&#26102;&#65292;LEGO&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online multi-object tracking (MOT) plays a pivotal role in autonomous systems. The state-of-the-art approaches usually employ a tracking-by-detection method, and data association plays a critical role. This paper proposes a learning and graph-optimized (LEGO) modular tracker to improve data association performance in the existing literature. The proposed LEGO tracker integrates graph optimization and self-attention mechanisms, which efficiently formulate the association score map, facilitating the accurate and efficient matching of objects across time frames. To further enhance the state update process, the Kalman filter is added to ensure consistent tracking by incorporating temporal coherence in the object states. Our proposed method utilizing LiDAR alone has shown exceptional performance compared to other online tracking approaches, including LiDAR-based and LiDAR-camera fusion-based methods. LEGO ranked 1st at the time of submitting results to KITTI object tracking evaluation ranki
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Soft MoE&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31181;&#31232;&#30095;&#30340;&#12289;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;Transformer&#65292;&#36890;&#36807;&#38544;&#24335;&#30340;&#36719;&#20998;&#37197;&#21644;&#21482;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31232;&#30095;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#21644;&#25512;&#29702;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#26631;&#20934;Transformer&#21644;&#20854;&#20182;MoE&#21464;&#20307;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00951</link><description>&lt;p&gt;
&#20174;&#31232;&#30095;&#21040;&#36719;&#24615;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Sparse to Soft Mixtures of Experts. (arXiv:2308.00951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Soft MoE&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31181;&#31232;&#30095;&#30340;&#12289;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;Transformer&#65292;&#36890;&#36807;&#38544;&#24335;&#30340;&#36719;&#20998;&#37197;&#21644;&#21482;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31232;&#30095;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#21644;&#25512;&#29702;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#26631;&#20934;Transformer&#21644;&#20854;&#20182;MoE&#21464;&#20307;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#30340;&#19987;&#23478;&#27169;&#22411;(MoEs)&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#25512;&#29702;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#27169;&#22411;&#23481;&#37327;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;MoEs&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65306;&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#20002;&#22833;&#26631;&#35760;&#12289;&#26080;&#27861;&#25193;&#23637;&#19987;&#23478;&#25968;&#37327;&#25110;&#26080;&#25928;&#30340;&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Soft MoE&#65292;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#31232;&#30095;Transformer&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#20445;&#25345;&#20102;MoEs&#30340;&#20248;&#28857;&#12290;Soft MoE&#36890;&#36807;&#21521;&#27599;&#20010;&#19987;&#23478;&#20256;&#36882;&#25152;&#26377;&#36755;&#20837;&#26631;&#35760;&#30340;&#19981;&#21516;&#21152;&#26435;&#32452;&#21512;&#26469;&#25191;&#34892;&#38544;&#24335;&#30340;&#36719;&#20998;&#37197;&#12290;&#19982;&#20854;&#20182;MoE&#20316;&#21697;&#19968;&#26679;&#65292;Soft MoE&#20013;&#30340;&#19987;&#23478;&#21482;&#22788;&#29702;&#19968;&#37096;&#20998;&#65288;&#32452;&#21512;&#30340;&#65289;&#26631;&#35760;&#65292;&#20197;&#22312;&#36739;&#20302;&#30340;&#25512;&#29702;&#25104;&#26412;&#19979;&#23454;&#29616;&#26356;&#22823;&#30340;&#27169;&#22411;&#23481;&#37327;&#12290;&#22312;&#35270;&#35273;&#35782;&#21035;&#26041;&#38754;&#65292;Soft MoE&#22312;&#26631;&#20934;Transformer&#65288;ViTs&#65289;&#21644;&#27969;&#34892;&#30340;MoE&#21464;&#20307;&#65288;Tokens Choice&#21644;Experts Choice&#65289;&#20013;&#34920;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;Soft MoE-Base/16&#30340;&#25512;&#29702;&#25104;&#26412;&#27604;ViT-Huge/14&#20302;10.5&#20493;&#65288;&#22681;&#38047;&#26102;&#38388;&#38477;&#20302;&#20102;5.7&#20493;&#65289;&#65292;&#21516;&#26102;&#19982;&#20854;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we proposeSoft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5x lower inference cost (5.7x lower wall-clock time) than ViT-Huge/14 while matching its p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25361;&#25112;&#22270;&#21327;&#21516;&#36807;&#28388;&#30340;&#31070;&#35805;&#65292;&#36890;&#36807;&#20851;&#27880;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#20845;&#20010;&#27969;&#34892;&#30340;&#22270;&#25512;&#33616;&#27169;&#22411;&#22312;&#20960;&#20010;&#24120;&#35265;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#65292;&#24182;&#19982;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.00404</link><description>&lt;p&gt;
&#25361;&#25112;&#22270;&#21327;&#21516;&#36807;&#28388;&#30340;&#31070;&#35805;&#65306;&#19968;&#39033;&#22522;&#20110;&#25512;&#29702;&#21644;&#21487;&#22797;&#21046;&#24615;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Challenging the Myth of Graph Collaborative Filtering: a Reasoned and Reproducibility-driven Analysis. (arXiv:2308.00404v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25361;&#25112;&#22270;&#21327;&#21516;&#36807;&#28388;&#30340;&#31070;&#35805;&#65292;&#36890;&#36807;&#20851;&#27880;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#20845;&#20010;&#27969;&#34892;&#30340;&#22270;&#25512;&#33616;&#27169;&#22411;&#22312;&#20960;&#20010;&#24120;&#35265;&#21644;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#65292;&#24182;&#19982;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#26174;&#33879;&#25512;&#21160;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#21644;&#29289;&#21697;&#26377;&#25928;&#22320;&#24314;&#27169;&#20026;&#19968;&#20010;&#20108;&#20998;&#22270;&#21644;&#26080;&#21521;&#22270;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21407;&#22987;&#30340;&#22522;&#20110;&#22270;&#30340;&#20316;&#21697;&#36890;&#24120;&#22312;&#26410;&#39564;&#35777;&#20854;&#22312;&#20855;&#20307;&#37197;&#32622;&#19979;&#30340;&#26377;&#25928;&#24615;&#30340;&#24773;&#20917;&#19979;&#37319;&#29992;&#22522;&#32447;&#35770;&#25991;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#30528;&#37325;&#20851;&#27880;&#32467;&#26524;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#21151;&#22797;&#21046;&#20102;&#20845;&#20010;&#27969;&#34892;&#19988;&#26368;&#26032;&#30340;&#22270;&#25512;&#33616;&#27169;&#22411;&#65288;NGCF&#12289;DGCF&#12289;LightGCN&#12289;SGL&#12289;UltraGCN&#21644;GFCF&#65289;&#22312;&#19977;&#20010;&#24120;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;Gowalla&#12289;Yelp 2018&#21644;&#20122;&#39532;&#36874;&#22270;&#20070;&#65289;&#19978;&#30340;&#32467;&#26524;&#30340;&#20195;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#22270;&#27169;&#22411;&#19982;&#22312;&#31163;&#32447;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#23545;&#20004;&#20010;&#32570;&#20047;&#29616;&#26377;&#25991;&#29486;&#20013;&#24050;&#24314;&#31435;&#35774;&#32622;&#30340;&#26032;&#25968;&#25454;&#38598;&#65288;Allrecipes&#21644;BookCrossing&#65289;&#30340;&#30740;&#31350;&#12290;&#30001;&#20110;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#19982;&#20197;&#21069;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#20351;&#24471;&#25105;&#20204;&#23545;&#22270;&#25512;&#33616;&#27169;&#22411;&#24615;&#33021;&#30340;&#35780;&#20272;&#32467;&#26524;&#26356;&#21152;&#28145;&#20837;&#21644;&#20840;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of graph neural network-based models (GNNs) has significantly advanced recommender systems by effectively modeling users and items as a bipartite, undirected graph. However, many original graph-based works often adopt results from baseline papers without verifying their validity for the specific configuration under analysis. Our work addresses this issue by focusing on the replicability of results. We present a code that successfully replicates results from six popular and recent graph recommendation models (NGCF, DGCF, LightGCN, SGL, UltraGCN, and GFCF) on three common benchmark datasets (Gowalla, Yelp 2018, and Amazon Book). Additionally, we compare these graph models with traditional collaborative filtering models that historically performed well in offline evaluations. Furthermore, we extend our study to two new datasets (Allrecipes and BookCrossing) that lack established setups in existing literature. As the performance on these datasets differs from the previous bench
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#65292;&#37325;&#28857;&#20851;&#27880;Python&#32534;&#31243;&#35821;&#35328;&#21644;&#25968;&#25454;&#32467;&#26500;&#31639;&#27861;&#20004;&#20010;&#22522;&#30784;&#20027;&#39064;&#12290;&#24635;&#32467;&#27979;&#35797;&#20013;ChatGPT&#30340;&#20195;&#30721;&#35299;&#20915;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12289;&#20195;&#30721;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2307.05360</link><description>&lt;p&gt;
&#25581;&#24320;&#24040;&#20154;&#30340;&#30495;&#38754;&#30446;&#65306;&#23545;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency in coding algorithms and data structures. (arXiv:2307.05360v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#32534;&#30721;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#65292;&#37325;&#28857;&#20851;&#27880;Python&#32534;&#31243;&#35821;&#35328;&#21644;&#25968;&#25454;&#32467;&#26500;&#31639;&#27861;&#20004;&#20010;&#22522;&#30784;&#20027;&#39064;&#12290;&#24635;&#32467;&#27979;&#35797;&#20013;ChatGPT&#30340;&#20195;&#30721;&#35299;&#20915;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12289;&#20195;&#30721;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#28145;&#21051;&#22320;&#37325;&#22609;&#20102;&#20154;&#24037;&#26234;&#33021;(AI)&#25216;&#26415;&#39046;&#22495;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ChatGPT&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#26377;&#30528;&#29420;&#29305;&#20043;&#22788;&#65292;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#36718;&#23545;&#35805;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#23637;&#31034;&#20986;&#23545;&#32534;&#30721;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#32534;&#30721;&#25361;&#25112;&#30446;&#24405;&#23545;ChatGPT&#30340;&#32534;&#30721;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;Python&#32534;&#31243;&#35821;&#35328;&#65292;&#20197;&#21450;&#38598;&#20013;&#22312;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#19978;&#30340;&#38382;&#39064;&#65292;&#36825;&#20004;&#20010;&#20027;&#39064;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35780;&#20272;ChatGPT&#35299;&#20915;&#25152;&#25552;&#20132;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#35780;&#20272;&#20854;&#20195;&#30721;&#36136;&#37327;&#20197;&#21450;&#20195;&#30721;&#24341;&#21457;&#30340;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#24615;&#36136;&#12290;&#24403;ChatGPT&#30340;&#20195;&#30721;&#25104;&#21151;&#25191;&#34892;&#20294;&#26410;&#33021;&#35299;&#20915;&#25163;&#22836;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#20250;&#30740;&#31350;&#36890;&#36807;&#30340;&#27979;&#35797;&#26696;&#20363;&#20013;&#30340;&#27169;&#24335;&#65292;&#20197;&#20102;&#35299;ChatGPT&#20195;&#30721;&#20013;&#30340;&#38169;&#35823;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformative influence of Large Language Models (LLMs) is profoundly reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT distinguishes itself within these models, demonstrating remarkable performance in multi-turn conversations and exhibiting code proficiency across an array of languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's coding capabilities based on what is to date the largest catalog of coding challenges. Our focus is on the python programming language and problems centered on data structures and algorithms, two topics at the very foundations of Computer Science. We evaluate ChatGPT for its ability to generate correct solutions to the problems fed to it, its code quality, and nature of run-time errors thrown by its code. Where ChatGPT code successfully executes, but fails to solve the problem at hand, we look into patterns in the test cases passed in order to gain some insights into how wrong ChatGPT code is in these 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20849;&#20139;&#30340;&#20004;&#20010;&#25110;&#22810;&#20010;&#36755;&#20837;&#27169;&#24577;&#30340;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#21322;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;&#36825;&#31181;&#26550;&#26500;&#21487;&#20197;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#65292;&#24182;&#27169;&#20223;&#20154;&#31867;&#20174;&#26377;&#38480;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.15711</link><description>&lt;p&gt;
&#36890;&#36807;&#20840;&#23616;&#24037;&#20316;&#21306;&#30340;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Multimodal Representation Learning through a Global Workspace. (arXiv:2306.15711v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20849;&#20139;&#30340;&#20004;&#20010;&#25110;&#22810;&#20010;&#36755;&#20837;&#27169;&#24577;&#30340;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#21322;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;&#36825;&#31181;&#26550;&#26500;&#21487;&#20197;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#65292;&#24182;&#27169;&#20223;&#20154;&#31867;&#20174;&#26377;&#38480;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#32452;&#21512;&#19981;&#21516;&#30340;&#36755;&#20837;&#27169;&#24577;&#65288;&#20363;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#24182;&#23398;&#20064;&#23545;&#20854;&#28508;&#22312;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#65292;&#25110;&#32773;&#23558;&#19968;&#20010;&#39046;&#22495;&#30340;&#20449;&#21495;&#36716;&#21270;&#20026;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#20449;&#21495;&#65288;&#20363;&#22914;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#25110;&#32773;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#26292;&#21147;&#30417;&#30563;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#65288;&#21644;&#20854;&#20182;&#21160;&#29289;&#65289;&#21487;&#20197;&#36890;&#36807;&#21305;&#37197;&#30340;&#36328;&#27169;&#24577;&#25968;&#25454;&#30340;&#31232;&#30095;&#32463;&#39564;&#26469;&#23398;&#20064;&#26377;&#29992;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#21463;&#35748;&#30693;&#27010;&#24565;&#8220;&#20840;&#23616;&#24037;&#20316;&#21306;&#8221;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#33021;&#21147;&#65306;&#19968;&#20010;&#20849;&#20139;&#30340;&#20004;&#20010;&#65288;&#25110;&#22810;&#20010;&#65289;&#36755;&#20837;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;&#27599;&#20010;&#27169;&#24577;&#37117;&#32463;&#36807;&#19968;&#20010;&#19987;&#38376;&#30340;&#31995;&#32479;&#22788;&#29702;&#65288;&#22312;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#65292;&#24182;&#38543;&#21518;&#20923;&#32467;&#65289;&#12290;&#30456;&#24212;&#30340;&#28508;&#22312;&#34920;&#31034;&#28982;&#21518;&#34987;&#32534;&#30721;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#24037;&#20316;&#21306;&#24182;&#20174;&#20013;&#35299;&#30721;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#26550;&#26500;&#36866;&#29992;&#20110;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent deep learning models can efficiently combine inputs from different modalities (e.g., images and text) and learn to align their latent representations, or to translate signals from one domain to another (as in image captioning, or text-to-image generation). However, current approaches mainly rely on brute-force supervised training over large multimodal datasets. In contrast, humans (and other animals) can learn useful multimodal representations from only sparse experience with matched cross-modal data. Here we evaluate the capabilities of a neural network architecture inspired by the cognitive notion of a "Global Workspace": a shared representation for two (or more) input modalities. Each modality is processed by a specialized system (pretrained on unimodal data, and subsequently frozen). The corresponding latent representations are then encoded to and decoded from a single shared workspace. Importantly, this architecture is amenable to self-supervised training via cycle-consiste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;--&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#31616;&#21333;&#26131;&#29992;&#65292;&#22312;&#21333;&#35843;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.01147</link><description>&lt;p&gt;
&#24179;&#28369;&#21333;&#35843;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Smooth Monotonic Networks. (arXiv:2306.01147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;--&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#31616;&#21333;&#26131;&#29992;&#65292;&#22312;&#21333;&#35843;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35843;&#24615;&#32422;&#26463;&#26159;&#32479;&#35745;&#24314;&#27169;&#20013;&#30340;&#24378;&#21147;&#27491;&#21017;&#21270;&#24037;&#20855;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#35745;&#31639;&#26426;&#25903;&#25345;&#30340;&#20915;&#31574;&#21046;&#23450;&#20013;&#25903;&#25345;&#20844;&#24179;&#24615;&#65292;&#24182;&#22686;&#21152;&#25968;&#25454;&#39537;&#21160;&#31185;&#23398;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#32463;&#20856;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30830;&#20445;&#20102;&#21333;&#35843;&#24615;&#65292;&#20294;&#30001;&#20110;&#26799;&#24230;&#28040;&#22833;&#32780;&#24448;&#24448;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38519;&#20837;&#19981;&#33391;&#23616;&#37096;&#26368;&#20248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;MM&#32593;&#32476;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#20351;&#29992;&#20005;&#26684;&#36882;&#22686;&#30340;&#24179;&#28369;&#38750;&#32447;&#24615;&#20989;&#25968;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#24471;&#21040;&#30340;&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#27169;&#22359;&#32487;&#25215;&#20102;MM&#26550;&#26500;&#30340;&#28176;&#36817;&#36924;&#36817;&#24615;&#36136;&#12290;&#23427;&#21487;&#20197;&#23884;&#20837;&#21040;&#26356;&#22823;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#21333;&#35843;&#24314;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#65292;SMM&#27169;&#22359;&#35201;&#31616;&#21333;&#24471;&#22810;&#65292;&#35745;&#31639;&#38656;&#27714;&#20063;&#35201;&#23569;&#24471;&#22810;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#23427;&#22312;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#19982;&#26367;&#20195;&#31070;&#32463;&#21644;&#38750;&#31070;&#32463;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#24471;&#26356;&#20026;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monotonicity constraints are powerful regularizers in statistical modelling. They can support fairness in computer supported decision making and increase plausibility in data-driven scientific models. The seminal min-max (MM) neural network architecture ensures monotonicity, but often gets stuck in undesired local optima during training because of vanishing gradients. We propose a simple modification of the MM network using strictly-increasing smooth non-linearities that alleviates this problem. The resulting smooth min-max (SMM) network module inherits the asymptotic approximation properties from the MM architecture. It can be used within larger deep learning systems trained end-to-end. The SMM module is considerably simpler and less computationally demanding than state-of-the-art neural networks for monotonic modelling. Still, in our experiments, it compared favorably to alternative neural and non-neural approaches in terms of generalization performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.17330</link><description>&lt;p&gt;
MADiff&#65306;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MADiff: Offline Multi-agent Learning with Diffusion Models. (arXiv:2305.17330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#36817;&#22312;&#21253;&#25324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#20869;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20854;&#20013;&#31574;&#30053;&#36890;&#36807;&#22312;&#22312;&#32447;&#35780;&#20272;&#20013;&#20135;&#29983;&#36712;&#36857;&#26469;&#36827;&#34892;&#35268;&#21010;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#26174;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;DM&#22914;&#20309;&#22312;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#20013;&#25805;&#20316;&#65292;&#20854;&#20013;&#20195;&#29702;&#21830;&#24456;&#38590;&#22312;&#29420;&#31435;&#24314;&#27169;&#27599;&#20010;&#20195;&#29702;&#21830;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#22242;&#38431;&#21512;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;MADiff&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MADiff&#26159;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#23545;&#22810;&#20010;&#25193;&#25955;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#22797;&#26434;&#21327;&#35843;&#24314;&#27169;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MADiff&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#26694;&#26550;&#65292;&#23427;&#26082;&#21487;&#20197;&#34892;&#20026;&#20026;&#20998;&#25955;&#30340;&#25919;&#31574;&#65292;&#21448;&#21487;&#20197;&#20026;&#38598;&#20013;&#25511;&#21046;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#25163;&#24314;&#27169;&#65292;&#24182;&#21487;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model (DM), as a powerful generative model, recently achieved huge success in various scenarios including offline reinforcement learning, where the policy learns to conduct planning by generating trajectory in the online evaluation. However, despite the effectiveness shown for single-agent learning, it remains unclear how DMs can operate in multi-agent problems, where agents can hardly complete teamwork without good coordination by independently modeling each agent's trajectories. In this paper, we propose MADiff, a novel generative multi-agent learning framework to tackle this problem. MADiff is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple diffusion agents. To the best of our knowledge, MADiff is the first diffusion-based multi-agent offline RL framework, which behaves as both a decentralized policy and a centralized controller, which includes opponent modeling and can be used for multi-agent trajectory predic
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#23545;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#36827;&#34892;&#20998;&#26512;&#20855;&#26377;&#21457;&#29616;&#26032;&#29983;&#29289;&#23398;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#21152;&#36895;&#25506;&#32034;&#20122;&#32454;&#32990;&#22823;&#20998;&#23376;&#21644;&#32454;&#32990;&#22120;&#20998;&#23376;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.17193</link><description>&lt;p&gt;
&#22522;&#20110;AI&#30340;&#36229;&#20998;&#36776;&#26174;&#24494;&#38236;&#20998;&#26512;&#65306;&#22312;&#27809;&#26377;&#22522;&#20934;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#29983;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
AI-based analysis of super-resolution microscopy: Biological discovery in the absence of ground truth. (arXiv:2305.17193v1 [q-bio.SC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17193
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#23545;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#36827;&#34892;&#20998;&#26512;&#20855;&#26377;&#21457;&#29616;&#26032;&#29983;&#29289;&#23398;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#21152;&#36895;&#25506;&#32034;&#20122;&#32454;&#32990;&#22823;&#20998;&#23376;&#21644;&#32454;&#32990;&#22120;&#20998;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20998;&#36776;&#26174;&#24494;&#38236;&#30340;&#32435;&#31859;&#32423;&#20998;&#36776;&#29575;&#29616;&#24050;&#20351;&#33639;&#20809;&#20998;&#23376;&#23450;&#20301;&#24037;&#20855;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#25972;&#20010;&#32454;&#32990;&#32467;&#26500;&#29983;&#29289;&#23398;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#25968;&#25454;&#20998;&#26512;&#20855;&#26377;&#21457;&#29616;&#26032;&#29983;&#29289;&#23398;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#32780;&#26032;&#29983;&#29289;&#23398;&#26412;&#36523;&#27809;&#26377;&#34987;&#21457;&#29616;&#36807;&#65292;&#20063;&#27809;&#26377;&#22522;&#20934;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#22312;&#36229;&#20998;&#36776;&#26174;&#24494;&#38236;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#20854;&#28508;&#21147;&#65292;&#20197;&#23454;&#29616;&#23545;&#20122;&#32454;&#32990;&#22823;&#20998;&#23376;&#21644;&#32454;&#32990;&#22120;&#20998;&#23376;&#32467;&#26500;&#30340;&#21152;&#36895;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The nanoscale resolution of super-resolution microscopy has now enabled the use of fluorescent based molecular localization tools to study whole cell structural biology. Machine learning based analysis of super-resolution data offers tremendous potential for discovery of new biology, that by definition is not known and lacks ground truth. Herein, we describe the application of weakly supervised learning paradigms to super-resolution microscopy and its potential to enable the accelerated exploration of the molecular architecture of subcellular macromolecules and organelles.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#30340;&#20915;&#31574;Transformer&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#20195;&#29702;&#22312;&#22788;&#29702;&#26032;&#20219;&#21153;&#19978;&#24615;&#33021;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16338</link><description>&lt;p&gt;
&#28145;&#24605;&#29087;&#34385;&#65306;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#30340;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Think Before You Act: Decision Transformers with Internal Working Memory. (arXiv:2305.16338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#30340;&#20915;&#31574;Transformer&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#20195;&#29702;&#22312;&#22788;&#29702;&#26032;&#20219;&#21153;&#19978;&#24615;&#33021;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#24050;&#32463;&#23637;&#31034;&#20102;&#36328;&#36234;&#22810;&#20010;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#20302;&#25928;&#24615;&#28304;&#20110;&#36951;&#24536;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#35760;&#24518;&#20854;&#34892;&#20026;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#22240;&#27492;&#65292;&#26032;&#20219;&#21153;&#30340;&#35757;&#32451;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#20808;&#21069;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#19982;LLM&#30340;&#38544;&#24335;&#35760;&#24518;&#26426;&#21046;&#19981;&#21516;&#65292;&#20154;&#33041;&#21033;&#29992;&#20998;&#24067;&#24335;&#23384;&#20648;&#22120;&#23384;&#20648;&#35760;&#24518;&#65292;&#20197;&#26377;&#25928;&#22320;&#31649;&#29702;&#21644;&#32452;&#32455;&#22810;&#31181;&#25216;&#33021;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#29616;&#35937;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#26469;&#23384;&#20648;&#12289;&#34701;&#21512;&#21644;&#26816;&#32034;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;Atari&#28216;&#25103;&#21644;&#20803;&#19990;&#30028;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#24494;&#35843;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Thus inspired, we propose an internal working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21457;&#29616;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#22312;&#25903;&#25345;&#25214;&#21040;&#23616;&#37096;&#36817;&#20284;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#38160;&#24230;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15659</link><description>&lt;p&gt;
&#22914;&#20309;&#36867;&#31163;&#38160;&#21270;&#30340;&#26497;&#23567;&#20540;&#28857;
&lt;/p&gt;
&lt;p&gt;
How to escape sharp minima. (arXiv:2305.15659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21457;&#29616;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#22312;&#25903;&#25345;&#25214;&#21040;&#23616;&#37096;&#36817;&#20284;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#38160;&#24230;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#20248;&#21270;&#31639;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36825;&#20123;&#31639;&#27861;&#34987;&#35774;&#35745;&#29992;&#26469;&#21457;&#29616;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#31639;&#27861;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#25439;&#22833;&#20989;&#25968;&#28023;&#26862;&#30697;&#38453;&#30340;&#36857;&#26469;&#24230;&#37327;&#23427;&#30340;&#24179;&#22374;&#31243;&#24230;&#65292;&#24182;&#24418;&#24335;&#21270;&#23450;&#20041;&#20102;&#36817;&#20284;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#27010;&#24565;&#12290;&#22312;&#27492;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#12290;&#38024;&#23545;&#19968;&#33324;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;&#24179;&#22374;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#28857;&#12290;&#31639;&#27861;&#30340;&#20027;&#35201;&#32452;&#20214;&#26159;&#20351;&#29992;&#20174;&#38543;&#26426;&#25200;&#21160;&#36845;&#20195;&#20013;&#35745;&#31639;&#30340;&#26799;&#24230;&#26469;&#20272;&#35745;&#23548;&#33268;&#26356;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#26041;&#21521;&#12290;&#23545;&#20110;&#25104;&#26412;&#20989;&#25968;&#26159;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;&#31639;&#27861;&#65292;&#21463;&#26368;&#36817;&#25552;&#20986;&#30340;&#23454;&#29992;&#31639;&#27861;&#8212;&#8212;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning applications have seen a remarkable success of optimization algorithms that are designed to find flat minima. Motivated by this paradigm, this work formulates and studies the algorithmic question of how to find flat minima. As an initial effort, this work adopts the trace of hessian of the cost function as the measure of flatness, and formally defines the notion of approximate flat minima. Under this notion, we then design algorithms that find approximate flat minima efficiently. For general cost functions, we present a gradient-based algorithm that finds an approximate flat local minimum efficiently. The main component of the algorithm is to use gradients computed from randomly perturbed iterates to estimate a direction that leads to flatter minima. For the setting where the cost function is an empirical risk over training data, we present a faster algorithm that is inspired by a recently proposed practical algorithm called sharpness-aware minimization, support
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21305;&#37197;&#36328;&#27169;&#24577;&#32858;&#31867;&#26469;&#20943;&#23569;&#27169;&#24577;&#24046;&#24322;&#30340;&#21452;&#21521;&#32858;&#31867;&#21305;&#37197;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#20849;&#21516;&#23545;&#40784;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.12673</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21452;&#36793;&#36328;&#27169;&#24577;&#32858;&#31867;&#21305;&#37197;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;&#20809;-&#32418;&#22806;&#20154;&#29289;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Efficient Bilateral Cross-Modality Cluster Matching for Unsupervised Visible-Infrared Person ReID. (arXiv:2305.12673v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12673
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21305;&#37197;&#36328;&#27169;&#24577;&#32858;&#31867;&#26469;&#20943;&#23569;&#27169;&#24577;&#24046;&#24322;&#30340;&#21452;&#21521;&#32858;&#31867;&#21305;&#37197;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#20849;&#21516;&#23545;&#40784;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#21487;&#35265;&#20809;-&#32418;&#22806;&#20154;&#29289;&#35782;&#21035;&#65288;USL-VI-ReID&#65289;&#26088;&#22312;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#21305;&#37197;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#34892;&#20154;&#22270;&#20687;&#20013;&#30456;&#21516;&#36523;&#20221;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#38024;&#23545;&#27809;&#26377;&#24456;&#22909;&#25506;&#32034;&#36328;&#27169;&#24577;&#32858;&#31867;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21521;&#32858;&#31867;&#21305;&#37197;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21305;&#37197;&#36328;&#27169;&#24577;&#32858;&#31867;&#26469;&#20943;&#23569;&#27169;&#24577;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20108;&#20998;&#22270;&#20013;&#20248;&#21270;&#26368;&#22823;&#21305;&#37197;&#38382;&#39064;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#23545;&#22810;&#21452;&#36793;&#36328;&#27169;&#24577;&#32858;&#31867;&#21305;&#37197;&#65288;MBCCM&#65289;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#21305;&#37197;&#30340;&#25104;&#23545;&#32858;&#31867;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#21033;&#29992;&#20849;&#20139;&#30340;&#21487;&#35265;&#20809;&#21644;&#32418;&#22806;&#20266;&#26631;&#31614;&#12290;&#22312;&#36825;&#26679;&#30340;&#30417;&#30563;&#20449;&#21495;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#65288;MSMA&#65289;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#22312;&#32858;&#31867;&#32423;&#21035;&#19978;&#20849;&#21516;&#23545;&#40784;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#36328;&#27169;&#24577;&#30340;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#29305;&#24449;&#20063;&#34987;&#32771;&#34385;&#36827;&#21435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to match pedestrian images of the same identity from different modalities without annotations. Existing works mainly focus on alleviating the modality gap by aligning instance-level features of the unlabeled samples. However, the relationships between cross-modality clusters are not well explored. To this end, we propose a novel bilateral cluster matching-based learning framework to reduce the modality gap by matching cross-modality clusters. Specifically, we design a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM) algorithm through optimizing the maximum matching problem in a bipartite graph. Then, the matched pairwise clusters utilize shared visible and infrared pseudo-labels during the model training. Under such a supervisory signal, a Modality-Specific and Modality-Agnostic (MSMA) contrastive learning framework is proposed to align features jointly at a cluster-level. Meanwhile, the cross-modal
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35299;&#20915;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#21450;&#22810;&#27169;&#24577;&#28304;&#25968;&#25454;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08698</link><description>&lt;p&gt;
&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Continual Multimodal Knowledge Graph Construction. (arXiv:2305.08698v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08698
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35299;&#20915;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#21450;&#22810;&#27169;&#24577;&#28304;&#25968;&#25454;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#65288;MKGC&#65289;&#28041;&#21450;&#20351;&#29992;&#22810;&#31181;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#21019;&#24314;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MKGC&#27169;&#22411;&#22312;&#22788;&#29702;&#21160;&#24577;&#29616;&#23454;&#22330;&#26223;&#20013;&#26032;&#22686;&#23454;&#20307;&#21644;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#36830;&#32493;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#35774;&#32622;&#20027;&#35201;&#20851;&#27880;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#24573;&#35270;&#20102;&#20854;&#20182;&#22810;&#27169;&#24577;&#28304;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#25506;&#32034;&#36830;&#32493;MKGC&#30340;&#25361;&#25112;&#65292;&#20197;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#30830;&#20445;&#20445;&#30041;&#20174;&#19981;&#21516;&#24418;&#24335;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#36807;&#21435;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#24320;&#21457;&#32456;&#36523;MKGC&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#36825;&#20010;&#22797;&#26434;&#30340;&#20027;&#39064;&#12290;&#22522;&#20110;&#32463;&#39564;&#21457;&#29616;&#65292;&#24403;&#22810;&#23186;&#20307;&#25968;&#25454;&#35757;&#32451;&#26102;&#65292;&#19968;&#20123;&#20856;&#22411;&#30340;MKGC&#27169;&#22411;&#21487;&#33021;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#24847;&#22806;&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#37027;&#20123;&#20165;&#21033;&#29992;&#25991;&#26412;&#36164;&#28304;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#25105;&#20204;&#20197;&#23454;&#39564;&#35777;&#25454;&#20026;&#22522;&#30784;&#65292;&#24635;&#32467;&#20986;&#20197;&#19979;&#35770;&#28857;&#65306;&#36830;&#32493;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#38754;&#20020;&#30528;&#25968;&#25454;&#28304;&#21464;&#21270;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Knowledge Graph Construction (MKGC) involves creating structured representations of entities and relations using multiple modalities, such as text and images. However, existing MKGC models face challenges in handling the addition of new entities and relations in dynamic real-world scenarios. The current continual setting for knowledge graph construction mainly focuses on entity and relation extraction from text data, overlooking other multimodal sources. Therefore, there arises the need to explore the challenge of continual MKGC to address the phenomenon of catastrophic forgetting and ensure the retention of past knowledge extracted from different forms of data. This research focuses on investigating this complex topic by developing lifelong MKGC benchmark datasets. Based on the empirical findings that several typical MKGC models, when trained on multimedia data, might unexpectedly underperform compared to those solely utilizing textual resources in a continual setting, we p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#65292;&#21457;&#29616;&#31283;&#20581;&#24615;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05400</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#25506;&#31350;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#33104;&#36133;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions. (arXiv:2305.05400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#65292;&#21457;&#29616;&#31283;&#20581;&#24615;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#20581;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23454;&#29616;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#22312;&#23545;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23545;&#25239;&#31283;&#20581;&#24615;&#21644;&#24418;&#24335;&#31283;&#20581;&#24615;&#39564;&#35777;&#39046;&#22495;&#20013;&#65292;&#31283;&#20581;&#24615;&#36890;&#24120;&#34987;&#23450;&#20041;&#20026;&#22312;Lp&#33539;&#25968;&#36317;&#31163;&#20869;&#23545;&#25152;&#26377;&#36755;&#20837;&#21464;&#21270;&#30340;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#36890;&#24120;&#36890;&#36807;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#35266;&#23519;&#21040;&#30340;&#21464;&#21270;&#26469;&#25913;&#36827;&#21644;&#35780;&#20272;&#65292;&#32780;&#24456;&#23569;&#32771;&#34385;&#25968;&#23398;&#23450;&#20041;&#30340;Lp&#33539;&#25968;&#22833;&#30495;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#26469;&#22686;&#24378;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#23545;&#25239;&#31283;&#20581;&#24615;&#39046;&#22495;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#23454;&#35777;&#21644;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;Lp&#33539;&#25968;&#20043;&#38388;&#31283;&#20581;&#24615;&#26159;&#21542;&#21487;&#36716;&#31227;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#21738;&#20123;Lp&#33539;&#25968;&#30340;&#22833;&#30495;&#24212;&#35813;&#29992;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness is a fundamental property of machine learning classifiers to achieve safety and reliability. In the fields of adversarial robustness and formal robustness verification of image classification models, robustness is commonly defined as the stability to all input variations within an Lp-norm distance. However, robustness to random corruptions is usually improved and evaluated using variations observed in the real-world, while mathematically defined Lp-norm corruptions are rarely considered. This study investigates the use of random Lp-norm corruptions to augment the training and test data of image classifiers. We adapt an approach from the field of adversarial robustness to assess the model robustness to imperceptible random corruptions. We empirically and theoretically investigate whether robustness is transferable across different Lp-norms and derive conclusions on which Lp-norm corruptions a model should be trained and evaluated on. We find that training data augmentation wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#65292;&#35299;&#20915;&#24403;&#21069;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#30340;&#30740;&#31350;&#23384;&#22312;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#19981;&#31995;&#32479;&#21644;&#19981;&#21516;&#27880;&#37322;&#26041;&#26696;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02637</link><description>&lt;p&gt;
&#38754;&#21521;&#36328;&#25968;&#25454;&#38598;&#30340;&#24369;&#30417;&#30563;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Weakly-Supervised Hate Speech Classification Across Datasets. (arXiv:2305.02637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02637
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#65292;&#35299;&#20915;&#24403;&#21069;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#30340;&#30740;&#31350;&#23384;&#22312;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#19981;&#31995;&#32479;&#21644;&#19981;&#21516;&#27880;&#37322;&#26041;&#26696;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#22810;&#20301;&#23398;&#32773;&#25351;&#20986;&#30340;&#37027;&#26679;&#65292;&#24403;&#21069;&#38024;&#23545;&#20167;&#24680;&#35328;&#35770;&#65288;HS&#65289;&#35782;&#21035;&#30340;&#30740;&#31350;&#29305;&#28857;&#26159;&#19981;&#31995;&#32479;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#21644;&#19981;&#21516;&#30340;&#27880;&#37322;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#23545;&#23427;&#20204;&#26410;&#34987;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27867;&#21270;&#24615;&#33021;&#24046;&#65292;&#24182;&#19988;&#19981;&#21516;HS&#20998;&#31867;&#27861;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#25152;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#26080;&#27861;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#20869;&#21644;&#36328;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;HS&#20998;&#31867;&#27169;&#22411;&#36890;&#29992;&#24615;&#36739;&#24046;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
As pointed out by several scholars, current research on hate speech (HS) recognition is characterized by unsystematic data creation strategies and diverging annotation schemata. Subsequently, supervised-learning models tend to generalize poorly to datasets they were not trained on, and the performance of the models trained on datasets labeled using different HS taxonomies cannot be compared. To ease this problem, we propose applying extremely weak supervision that only relies on the class name rather than on class samples from the annotated data. We demonstrate the effectiveness of a state-of-the-art weakly-supervised text classification model in various in-dataset and cross-dataset settings. Furthermore, we conduct an in-depth quantitative and qualitative analysis of the source of poor generalizability of HS classification models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.11090</link><description>&lt;p&gt;
&#22312;ChatGPT&#26102;&#20195;&#36808;&#21521;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#30340;&#21442;&#32771;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Towards Responsible AI in the Era of ChatGPT: A Reference Architecture for Designing Foundation Model-based AI Systems. (arXiv:2304.11090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#12289;Bard&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25512;&#20986;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24341;&#36215;&#20102;&#24040;&#22823;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#23558;&#25104;&#20026;&#26410;&#26469;&#22823;&#22810;&#25968;AI&#31995;&#32479;&#30340;&#22522;&#30784;&#26500;&#24314;&#22359;&#30340;&#36235;&#21183;&#27491;&#22312;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#30784;&#27169;&#22411;&#32435;&#20837;AI&#31995;&#32479;&#24341;&#21457;&#20102;&#23545;&#36127;&#36131;&#20219;AI&#30340;&#37325;&#22823;&#20851;&#27880;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#40657;&#21283;&#23376;&#24615;&#36136;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#36229;&#32423;&#26234;&#33021;&#24341;&#36215;&#30340;&#12290;&#27492;&#22806;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#22686;&#38271;&#33021;&#21147;&#26368;&#32456;&#21487;&#33021;&#20250;&#21534;&#22124;AI&#31995;&#32479;&#30340;&#20854;&#20182;&#32452;&#20214;&#65292;&#24341;&#20837;&#26550;&#26500;&#35774;&#35745;&#20013;&#30340;&#36816;&#21160;&#36793;&#30028;&#21644;&#25509;&#21475;&#28436;&#21464;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#12290;&#29305;&#21035;&#22320;&#65292;&#26412;&#25991;&#39318;&#20808;&#21576;&#29616;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#22312;&#26550;&#26500;&#28436;&#36827;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#20174;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#36830;&#25509;&#22120;"&#21040;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21333;&#29255;&#26426;&#26680;"&#12290;&#28982;&#21518;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26550;&#26500;&#65292;&#21253;&#25324;&#20116;&#20010;&#31867;&#21035;&#30340;&#27169;&#24335;&#65292;&#37325;&#28857;&#20851;&#27880;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#20026;&#35774;&#35745;&#36127;&#36131;&#20219;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#21644;&#36879;&#26126;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of ChatGPT, Bard, and other large language model (LLM)-based chatbots has drawn huge attention on foundations models worldwide. There is a growing trend that foundation models will serve as the fundamental building blocks for most of the future AI systems. However, incorporating foundation models in AI systems raises significant concerns about responsible AI due to their black box nature and rapidly advancing super-intelligence. Additionally, the foundation model's growing capabilities can eventually absorb the other components of AI systems, introducing the moving boundary and interface evolution challenges in architecture design. To address these challenges, this paper proposes a pattern-oriented responsible-AI-by-design reference architecture for designing foundation model-based AI systems. Specially, the paper first presents an architecture evolution of AI systems in the era of foundation models, from "foundation-model-as-a-connector" to "foundation-model-as-a-monolithi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#31639;&#27861; $\texttt{THREAD}$&#65292;&#21487;&#20197;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#19981;&#21516;&#24418;&#24335;&#30340;&#25903;&#25345;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102; $\texttt{Modiste}$ &#24037;&#20855;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21307;&#23398;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#65292;&#20351;&#29992; $\texttt{THREAD}$ &#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#39044;&#26399;&#30340;&#35786;&#26029;&#27491;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#25512;&#33616;&#20102;&#26356;&#23569;&#21644;&#26356;&#20415;&#23452;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.06701</link><description>&lt;p&gt;
&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Personalized Decision Support Policies. (arXiv:2304.06701v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#31639;&#27861; $\texttt{THREAD}$&#65292;&#21487;&#20197;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#19981;&#21516;&#24418;&#24335;&#30340;&#25903;&#25345;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102; $\texttt{Modiste}$ &#24037;&#20855;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21307;&#23398;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#65292;&#20351;&#29992; $\texttt{THREAD}$ &#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#39044;&#26399;&#30340;&#35786;&#26029;&#27491;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#25512;&#33616;&#20102;&#26356;&#23569;&#21644;&#26356;&#20415;&#23452;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20307;&#20915;&#31574;&#32773;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#24418;&#24335;&#30340;&#25903;&#25345;&#26469;&#25552;&#39640;&#20915;&#31574;&#32467;&#26524;&#65292;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#65292;&#21738;&#31181;&#24418;&#24335;&#30340;&#25903;&#25345;&#20250;&#22312;&#20302;&#25104;&#26412;&#19979;&#23548;&#33268;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#32473;&#23450;&#36755;&#20837;&#26102;&#36873;&#25321;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#25903;&#25345;&#12290;&#25105;&#20204;&#32771;&#34385;&#27809;&#26377;&#20808;&#39564;&#20449;&#24687;&#30340;&#20915;&#31574;&#32773;&#65292;&#24182;&#23558;&#23398;&#20064;&#21508;&#33258;&#30340;&#31574;&#30053;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#20010;&#38382;&#39064;&#26435;&#34913;&#20102;&#20934;&#30830;&#24615;&#21644;&#25104;&#26412;&#12290;&#20351;&#29992;&#38543;&#26426;&#29615;&#22659;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; $\texttt{THREAD}$&#65292;&#36825;&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#36229;&#21442;&#25968;&#35843;&#25972;&#31574;&#30053;&#65292;&#20197;&#21033;&#29992;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#26469;&#30830;&#23450;&#25104;&#26412;-&#24615;&#33021;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20379;&#35745;&#31639;&#23454;&#39564;&#26469;&#35777;&#26126; $\texttt{THREAD}$ &#30456;&#23545;&#20110;&#32447;&#19979;&#22522;&#32447;&#30340;&#20248;&#21183;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#24037;&#20855; $\texttt{Modiste}$&#65292;&#23427;&#20026;&#29616;&#23454;&#20013;&#30340;&#21307;&#23398;&#35786;&#26029;&#25552;&#20379;&#20010;&#24615;&#21270;&#20915;&#31574;&#25903;&#25345;&#12290;$\texttt{Modiste}$ &#20351;&#29992; $\texttt{THREAD}$ &#20026;&#27599;&#20301;&#21307;&#29983;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#20915;&#31574;&#25903;&#25345;&#31574;&#30053;&#65292;&#24182;&#25512;&#33616;&#20010;&#24615;&#21270;&#30740;&#31350;&#20197;&#20248;&#21270;&#24739;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#24182;&#23558;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#38477;&#33267;&#26368;&#20302;&#12290;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; $\texttt{Modiste}$ &#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#26399;&#30340;&#35786;&#26029;&#27491;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#20005;&#37325;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#25512;&#33616;&#20102;&#26356;&#23569;&#21644;&#26356;&#20415;&#23452;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual human decision-makers may benefit from different forms of support to improve decision outcomes. However, a key question is which form of support will lead to accurate decisions at a low cost. In this work, we propose learning a decision support policy that, for a given input, chooses which form of support, if any, to provide. We consider decision-makers for whom we have no prior information and formalize learning their respective policies as a multi-objective optimization problem that trades off accuracy and cost. Using techniques from stochastic contextual bandits, we propose $\texttt{THREAD}$, an online algorithm to personalize a decision support policy for each decision-maker, and devise a hyper-parameter tuning strategy to identify a cost-performance trade-off using simulated human behavior. We provide computational experiments to demonstrate the benefits of $\texttt{THREAD}$ compared to offline baselines. We then introduce $\texttt{Modiste}$, an interactive tool that pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#27169;&#22359;&#8212;&#8212;&#19981;&#30830;&#23450;&#24615;&#25513;&#34109;&#28151;&#21512;&#65288;UmmU&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#20013;&#38388;&#23618;&#23884;&#20837;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36827;&#32780;&#22686;&#24378;&#23884;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#30340;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05749</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Boosting long-term forecasting performance for continuous-time dynamic graph networks via data augmentation. (arXiv:2304.05749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#27169;&#22359;&#8212;&#8212;&#19981;&#30830;&#23450;&#24615;&#25513;&#34109;&#28151;&#21512;&#65288;UmmU&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#20013;&#38388;&#23618;&#23884;&#20837;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36827;&#32780;&#22686;&#24378;&#23884;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#30340;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#23545;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#32593;&#32476;&#65288;CTDGNs&#65289;&#36827;&#34892;&#38271;&#26399;&#39044;&#27979;&#65288;LTF&#65289;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#24314;&#27169;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;CTDGN&#23545;&#20110;&#24314;&#27169;&#26102;&#38388;&#22270;&#25968;&#25454;&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20294;&#30001;&#20110;&#23545;&#21382;&#21490;&#25968;&#25454;&#30340;&#23454;&#36136;&#35201;&#27714;&#65292;&#23427;&#20204;&#22312;LTF&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20063;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#30452;&#35266;&#30340;&#26041;&#27861;&#26159;&#25968;&#25454;&#22686;&#24378;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#27169;&#22359;&#8212;&#8212;&#19981;&#30830;&#23450;&#24615;&#25513;&#34109;&#28151;&#21512;&#65288;UmmU&#65289;&#65292;&#23427;&#33021;&#22815;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#24341;&#20837;CTDGN&#30340;&#20013;&#38388;&#23618;&#23884;&#20837;&#20013;&#65292;&#24182;&#36827;&#34892;&#25513;&#34109;&#28151;&#21512;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23884;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#29992;&#20110;&#26356;&#22810;&#24773;&#20917;&#65292;&#32780;&#19988;&#21487;&#20197;&#36731;&#26494;&#22320;&#25554;&#20837;&#21040;&#20219;&#24847;CTDGN&#20013;&#65292;&#32780;&#19981;&#22686;&#21152;&#21442;&#25968;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;UmmU&#22312;LTF&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on long-term forecasting (LTF) on continuous-time dynamic graph networks (CTDGNs), which is important for real-world modeling. Existing CTDGNs are effective for modeling temporal graph data due to their ability to capture complex temporal dependencies but perform poorly on LTF due to the substantial requirement for historical data, which is not practical in most cases. To relieve this problem, a most intuitive way is data augmentation. In this study, we propose \textbf{\underline{U}ncertainty \underline{M}asked \underline{M}ix\underline{U}p (UmmU)}: a plug-and-play module that conducts uncertainty estimation to introduce uncertainty into the embedding of intermediate layer of CTDGNs, and perform masked mixup to further enhance the uncertainty of the embedding to make it generalize to more situations. UmmU can be easily inserted into arbitrary CTDGNs without increasing the number of parameters. We conduct comprehensive experiments on three real-world dynamic graph dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2303.16668</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#25308;&#21344;&#24237;&#23481;&#38169;&#32858;&#21512;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates. (arXiv:2303.16668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#12290;FLANDERS&#23558;&#27599;&#20010;FL&#36718;&#27425;&#20013;&#30001;&#23458;&#25143;&#31471;&#21457;&#36865;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#35270;&#20026;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#23454;&#38469;&#35266;&#27979;&#19982;&#30001;&#30697;&#38453;&#33258;&#22238;&#24402;&#39044;&#27979;&#27169;&#22411;&#20272;&#35745;&#30340;&#35266;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#20316;&#20026;&#36825;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#20540;&#12290;&#22312;&#19981;&#21516;FL&#35774;&#32622;&#19979;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FLANDERS&#22312;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#26041;&#38754;&#19982;&#26368;&#24378;&#22823;&#30340;&#22522;&#32447;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#30340;&#38450;&#24481;&#31574;&#30053;&#30456;&#27604;&#65292; FLANDERS&#21363;&#20351;&#22312;&#26497;&#20854;&#20005;&#37325;&#30340;&#25915;&#20987;&#22330;&#26223;&#19979;&#20173;&#28982;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose FLANDERS, a novel federated learning (FL) aggregation scheme robust to Byzantine attacks. FLANDERS considers the local model updates sent by clients at each FL round as a matrix-valued time series. Then, it identifies malicious clients as outliers of this time series by comparing actual observations with those estimated by a matrix autoregressive forecasting model. Experiments conducted on several datasets under different FL settings demonstrate that FLANDERS matches the robustness of the most powerful baselines against Byzantine clients. Furthermore, FLANDERS remains highly effective even under extremely severe attack scenarios, as opposed to existing defense strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20648;&#22791;&#33410;&#28857;&#30340;&#31070;&#32463;&#20803;&#35270;&#35273;&#24863;&#30693;&#32593;&#32476;&#65288;RN-Net&#65289;&#12290;RN-Net&#21487;&#20197;&#20197;&#20302;&#25104;&#26412;&#26377;&#25928;&#22320;&#22788;&#29702;&#24322;&#27493;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#24182;&#22312;DVS128&#25163;&#21183;&#19978;&#23454;&#29616;&#20102;&#36804;&#20170;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;99.2&#65285;&#65292;&#22312;&#26356;&#23567;&#30340;&#32593;&#32476;&#23610;&#23544;&#19979;&#23454;&#29616;&#20102;DVS Lip&#25968;&#25454;&#38598;&#30340;67.5&#65285;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.10770</link><description>&lt;p&gt;
RN-Net: &#22522;&#20110;&#20648;&#22791;&#33410;&#28857;&#30340;&#31070;&#32463;&#20803;&#35270;&#35273;&#24863;&#30693;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RN-Net: Reservoir Nodes-Enabled Neuromorphic Vision Sensing Network. (arXiv:2303.10770v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20648;&#22791;&#33410;&#28857;&#30340;&#31070;&#32463;&#20803;&#35270;&#35273;&#24863;&#30693;&#32593;&#32476;&#65288;RN-Net&#65289;&#12290;RN-Net&#21487;&#20197;&#20197;&#20302;&#25104;&#26412;&#26377;&#25928;&#22320;&#22788;&#29702;&#24322;&#27493;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#24182;&#22312;DVS128&#25163;&#21183;&#19978;&#23454;&#29616;&#20102;&#36804;&#20170;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;99.2&#65285;&#65292;&#22312;&#26356;&#23567;&#30340;&#32593;&#32476;&#23610;&#23544;&#19979;&#23454;&#29616;&#20102;DVS Lip&#25968;&#25454;&#38598;&#30340;67.5&#65285;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#21463;&#29983;&#29289;&#35270;&#35273;&#31995;&#32479;&#31232;&#30095;&#21644;&#24322;&#27493;&#33033;&#20914;&#34920;&#31034;&#30340;&#21551;&#21457;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#20107;&#20214;&#25968;&#25454;&#35201;&#20040;&#38656;&#35201;&#20351;&#29992;&#26114;&#36149;&#30340;&#29305;&#24449;&#25551;&#36848;&#31526;&#23558;&#33033;&#20914;&#36716;&#25442;&#25104;&#24103;&#65292;&#35201;&#20040;&#20351;&#29992;&#38590;&#20197;&#35757;&#32451;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#21367;&#31215;&#23618;&#21644;&#21160;&#24577;&#26102;&#38388;&#32534;&#30721;&#20648;&#22791;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20855;&#26377;&#20302;&#30828;&#20214;&#21644;&#35757;&#32451;&#25104;&#26412;&#12290;&#20351;&#29992;&#20648;&#22791;&#33410;&#28857;&#30340;&#31070;&#32463;&#20803;&#35270;&#35273;&#24863;&#30693;&#32593;&#32476;&#65288;RN-Net&#65289;&#20351;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#24322;&#27493;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#24182;&#23454;&#29616;&#20102;DVS128&#25163;&#21183;&#30340;&#36804;&#20170;&#26368;&#39640;&#20934;&#30830;&#24230;99.2&#65285;&#65292;&#21516;&#26102;&#22312;&#26356;&#23567;&#30340;&#32593;&#32476;&#23610;&#23544;&#19979;&#23454;&#29616;&#20102;DVS Lip&#25968;&#25454;&#38598;&#30340;67.5&#65285;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#21033;&#29992;&#35760;&#24518;&#30005;&#38459;&#22120;&#30340;&#20869;&#37096;&#21160;&#24577;&#65292;&#21487;&#20197;&#20197;&#38750;&#24120;&#20302;&#30340;&#30828;&#20214;&#25104;&#26412;&#23454;&#29616;&#24322;&#27493;&#26102;&#38388;&#29305;&#24449;&#32534;&#30721;&#65292;&#32780;&#19981;&#38656;&#35201;&#39044;&#22788;&#29702;&#25110;&#19987;&#29992;&#30340;&#23384;&#20648;&#22120;&#21644;&#31639;&#26415;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-based cameras are inspired by the sparse and asynchronous spike representation of the biological visual system. However, processing the even data requires either using expensive feature descriptors to transform spikes into frames, or using spiking neural networks that are difficult to train. In this work, we propose a neural network architecture based on simple convolution layers integrated with dynamic temporal encoding reservoirs with low hardware and training costs. The Reservoir Nodes-enabled neuromorphic vision sensing Network (RN-Net) allows the network to efficiently process asynchronous temporal features, and achieves the highest accuracy of 99.2% for DVS128 Gesture reported to date, and one of the highest accuracy of 67.5% for DVS Lip dataset at a much smaller network size. By leveraging the internal dynamics of memristors, asynchronous temporal feature encoding can be implemented at very low hardware cost without preprocessing or dedicated memory and arithmetic units. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#21644;&#35777;&#26126;&#25968;&#25628;&#32034;&#30456;&#32467;&#21512;&#30340;PN-MCTS&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#28216;&#25103;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#26368;&#32456;&#36208;&#27493;&#36873;&#25321;&#12289;&#35299;&#20915;&#23376;&#26641;&#20197;&#21450;UCT&#20844;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.09449</link><description>&lt;p&gt;
&#22522;&#20110;&#35777;&#26126;&#25968;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Proof Number Based Monte-Carlo Tree Search. (arXiv:2303.09449v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#21644;&#35777;&#26126;&#25968;&#25628;&#32034;&#30456;&#32467;&#21512;&#30340;PN-MCTS&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#28216;&#25103;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#26368;&#32456;&#36208;&#27493;&#36873;&#25321;&#12289;&#35299;&#20915;&#23376;&#26641;&#20197;&#21450;UCT&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28216;&#25103;&#25628;&#32034;&#31639;&#27861;PN-MCTS&#65292;&#23427;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#21644;&#35777;&#26126;&#25968;&#25628;&#32034;&#65288;PNS&#65289;&#30456;&#32467;&#21512;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19977;&#20010;&#39046;&#22495;&#65292;&#21363;&#26368;&#32456;&#36208;&#27493;&#36873;&#25321;&#65292;&#35299;&#20915;&#23376;&#26641;&#20197;&#21450;UCT&#20844;&#24335;&#65292;&#36825;&#20123;&#39046;&#22495;&#21487;&#20197;&#21033;&#29992;&#22312;MCTS&#26641;&#20013;&#25910;&#38598;&#21040;&#30340;&#35777;&#26126;&#25968;&#21644;&#35777;&#20266;&#25968;&#25552;&#20379;&#30340;&#38468;&#21152;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#35774;&#32622;&#19979;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#32452;&#21512;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#22312;&#20960;&#20010;&#28216;&#25103;&#20013;&#19982;vanilla UCT MCTS&#36827;&#34892;&#23545;&#20915;&#65306;&#21160;&#20316;&#32447;&#65288;$7$$\times$$7$&#21644;$8$$\times$$8$&#65289;&#65292;MiniShogi&#65292; Knightthrough&#65292; Awari&#21644;Gomoku&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#25193;&#23637;&#65292;&#20197;&#36866;&#24403;&#22320;&#22788;&#29702;&#20986;&#29616;&#24179;&#23616;&#30340;&#28216;&#25103;&#65292;&#22914;Awari&#65292;&#36890;&#36807;&#22312;MCTS&#26641;&#30340;&#39030;&#37096;&#28155;&#21152;&#19968;&#20010;&#38468;&#21152;&#30340;PNS&#23618;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PN-MCTS&#22312;6&#20010;&#28216;&#25103;&#39046;&#22495;&#20013;&#26377;5&#20010;&#30340;&#32988;&#29575;&#20248;&#20110;MCTS&#65288;&#38500;&#20102;Gomoku&#65289;&#65292;&#20854;&#20013;&#22312;&#21160;&#20316;&#32447;&#28216;&#25103;&#20013;&#33719;&#24471;&#20102;&#39640;&#36798;96.2%&#30340;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new game search algorithm, PN-MCTS, that combines Monte-Carlo Tree Search (MCTS) and Proof-Number Search (PNS). These two algorithms have been successfully applied for decision making in a range of domains. We define three areas where the additional knowledge provided by the proof and disproof numbers gathered in MCTS trees might be used: final move selection, solving subtrees, and the UCT formula. We test all possible combinations on different time settings, playing against vanilla UCT MCTS on several games: Lines of Action ($7$$\times$$7$ and $8$$\times$$8$), MiniShogi, Knightthrough, Awari, and Gomoku. Furthermore, we extend this new algorithm to properly address games with draws, like Awari, by adding an additional layer of PNS on top of the MCTS tree. The experiments show that PN-MCTS confidently outperforms MCTS in 5 out of 6 game domains (all except Gomoku), achieving win rates up to 96.2% for Lines of Action.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#31561;&#38477;&#32500;&#31639;&#27861;&#65292;&#36873;&#25321;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.12012</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#30340;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#30340;&#32463;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection. (arXiv:2302.12012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#31561;&#38477;&#32500;&#31639;&#27861;&#65292;&#36873;&#25321;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#26816;&#26597;&#65292;&#35760;&#24405;&#22823;&#33041;&#30340;&#30005;&#27963;&#21160;&#12290;&#35813;&#26816;&#26597;&#29992;&#20110;&#24110;&#21161;&#35786;&#26029;&#21508;&#31181;&#33041;&#38382;&#39064;&#12290;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#30315;&#30187;&#26816;&#27979;&#12290;&#22312;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#20013;&#65292;&#20027;&#35201;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#32479;&#35745;&#29305;&#24449;&#12290;EEG&#20449;&#21495;&#20013;&#30340;&#38544;&#34255;&#20449;&#24687;&#23545;&#20110;&#26816;&#27979;&#24433;&#21709;&#22823;&#33041;&#30340;&#30142;&#30149;&#24456;&#26377;&#29992;&#12290;&#26377;&#26102;&#65292;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20869;&#35782;&#21035;EEG&#30340;&#26368;&#23567;&#21464;&#21270;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;DWT&#21487;&#20197;&#22312;&#19981;&#21516;&#39057;&#24102;&#36827;&#34892;&#20449;&#21495;&#33391;&#22909;&#30340;&#20998;&#35299;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#38477;&#32500;&#31639;&#27861;&#65306;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#34701;&#21512;&#35268;&#21017;&#36873;&#25321;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Electroencephalogram (EEG) is a non-invasive exam that records the electrical activity of the brain. This exam is used to help diagnose conditions such as different brain problems. EEG signals are taken for the purpose of epilepsy detection and with Discrete Wavelet Transform (DWT) and machine learning classifier, they perform epilepsy detection. In Epilepsy seizure detection, mainly machine learning classifiers and statistical features are used. The hidden information in the EEG signal is useful for detecting diseases affecting the brain. Sometimes it is very difficult to identify the minimum changes in the EEG in the time and frequency domains purpose. The DWT can give a good decomposition of the signals in different frequency bands and feature extraction. We use the tri-dimensionality reduction algorithm.; Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Linear Discriminant Analysis (LDA). Finally, features are selected by using a fusion rule and at t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#35302;&#21457;&#20854;&#20182;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#25915;&#20987;&#26377;&#25928;&#65292;&#21487;&#24212;&#29992;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.16205</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Local Model Reconstruction Attacks in Federated Learning and their Uses. (arXiv:2210.16205v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#35302;&#21457;&#20854;&#20182;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#25915;&#20987;&#26377;&#25928;&#65292;&#21487;&#24212;&#29992;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31363;&#21548;&#30446;&#26631;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#24182;&#37325;&#26500;&#21463;&#23475;&#32773;&#30340;&#26412;&#22320;/&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#35302;&#21457;&#20854;&#20182;&#32463;&#20856;&#25915;&#20987;&#12290;&#26412;&#22320;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#27844;&#28431;&#27604;&#26381;&#21153;&#22120;&#23398;&#20064;&#30340;&#20840;&#23616;&#27169;&#22411;&#26356;&#22810;&#30340;&#31169;&#26377;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#65292;&#21033;&#29992;&#20102;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#20998;&#26512;&#19979;&#30028;&#12290;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26412;&#22320;&#37325;&#26500;&#25915;&#20987;&#23545;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#37117;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#19982;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we initiate the study of local model reconstruction attacks for federated learning, where a honest-but-curious adversary eavesdrops the messages exchanged between a targeted client and the server, and then reconstructs the local/personalized model of the victim. The local model reconstruction attack allows the adversary to trigger other classical attacks in a more effective way, since the local model only depends on the client's data and can leak more private information than the global model learned by the server. Additionally, we propose a novel model-based attribute inference attack in federated learning leveraging the local model reconstruction attack. We provide an analytical lower-bound for this attribute inference attack. Empirical results using real world datasets confirm that our local reconstruction attack works well for both regression and classification tasks. Moreover, we benchmark our novel attribute inference attack against the state-of-the-art attacks in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36827;&#21270;&#35745;&#31639;&#19982;&#31934;&#30830;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;0-1&#22810;&#32500;&#32972;&#21253;&#38382;&#39064;&#65292;&#36890;&#36807;&#25506;&#32034;&#26377;&#21069;&#36884;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#25214;&#21040;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2210.03918</link><description>&lt;p&gt;
&#23547;&#25214;&#21644;&#25506;&#32034;0-1&#22810;&#32500;&#32972;&#21253;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#25628;&#32034;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Finding and Exploring Promising Search Space for the 0-1 Multidimensional Knapsack Problem. (arXiv:2210.03918v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36827;&#21270;&#35745;&#31639;&#19982;&#31934;&#30830;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;0-1&#22810;&#32500;&#32972;&#21253;&#38382;&#39064;&#65292;&#36890;&#36807;&#25506;&#32034;&#26377;&#21069;&#36884;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#25214;&#21040;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
0-1&#22810;&#32500;&#32972;&#21253;&#38382;&#39064;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;NP&#22256;&#38590;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20855;&#26377;&#35768;&#22810;&#24037;&#31243;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23558;&#36827;&#21270;&#35745;&#31639;&#19982;&#31934;&#30830;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;0-1&#22810;&#32500;&#32972;&#21253;&#38382;&#39064;&#12290;&#23427;&#32500;&#25252;&#19968;&#32452;&#35299;&#65292;&#24182;&#21033;&#29992;&#26469;&#33258;&#32676;&#20307;&#30340;&#20449;&#24687;&#25552;&#21462;&#33391;&#22909;&#30340;&#37096;&#20998;&#20998;&#37197;&#12290;&#20026;&#20102;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#65292;&#24212;&#29992;&#31934;&#30830;&#31639;&#27861;&#26469;&#25506;&#32034;&#30001;&#33391;&#22909;&#37096;&#20998;&#20998;&#37197;&#25351;&#23450;&#30340;&#26377;&#21069;&#36884;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#26032;&#30340;&#35299;&#29992;&#20110;&#26356;&#26032;&#32676;&#20307;&#12290;&#22240;&#27492;&#65292;&#38543;&#30528;&#32676;&#20307;&#30340;&#25913;&#36827;&#65292;&#33391;&#22909;&#30340;&#37096;&#20998;&#20998;&#37197;&#26397;&#30528;&#26356;&#22909;&#30340;&#26041;&#21521;&#28436;&#21270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;TPTEA&#21644;DQPSO&#12290;&#23427;&#25214;&#21040;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#35299;&#65292;&#24182;&#20026;8&#20010;&#22823;&#35268;&#27169;&#21644;&#22256;&#38590;&#30340;&#23454;&#20363;&#25552;&#20379;&#20102;&#26032;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 0-1 Multidimensional Knapsack Problem (MKP) is a classical NP-hard combinatorial optimization problem with many engineering applications. In this paper, we propose a novel algorithm combining evolutionary computation with exact algorithm to solve the 0-1 MKP. It maintains a set of solutions and utilizes the information from the population to extract good partial assignments. To find high-quality solutions, an exact algorithm is applied to explore the promising search space specified by the good partial assignments. The new solutions are used to update the population. Thus, the good partial assignments evolve towards a better direction with the improvement of the population. Extensive experimentation with commonly used benchmark sets shows that our algorithm outperforms the state of the art heuristic algorithms, TPTEA and DQPSO. It finds better solutions than the existing algorithms and provides new lower bounds for 8 large and hard instances.
&lt;/p&gt;</description></item><item><title>FALCON&#26159;&#19968;&#20010;&#22522;&#20110;ALC&#26412;&#20307;&#30340;&#27169;&#31946;&#31070;&#32463;&#25512;&#29702;&#22120;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#20010;&#27169;&#22411;&#32467;&#26500;&#35745;&#31639;&#24544;&#23454;&#30340;&#35821;&#20041;&#34164;&#28085;&#65292;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#19990;&#30028;&#27169;&#22411;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#23454;&#29616;&#20102;&#36817;&#20284;&#25512;&#29702;&#12289;&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;ALC&#34920;&#36798;&#30340;&#30693;&#35782;&#25913;&#36827;&#20102;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2208.07628</link><description>&lt;p&gt;
FALCON&#65306;&#22522;&#20110;ALC&#26412;&#20307;&#30340;&#24544;&#23454;&#31070;&#32463;&#35821;&#20041;&#34164;&#28085;
&lt;/p&gt;
&lt;p&gt;
FALCON: Faithful Neural Semantic Entailment over ALC Ontologies. (arXiv:2208.07628v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07628
&lt;/p&gt;
&lt;p&gt;
FALCON&#26159;&#19968;&#20010;&#22522;&#20110;ALC&#26412;&#20307;&#30340;&#27169;&#31946;&#31070;&#32463;&#25512;&#29702;&#22120;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#20010;&#27169;&#22411;&#32467;&#26500;&#35745;&#31639;&#24544;&#23454;&#30340;&#35821;&#20041;&#34164;&#28085;&#65292;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#19990;&#30028;&#27169;&#22411;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#23454;&#29616;&#20102;&#36817;&#20284;&#25512;&#29702;&#12289;&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;ALC&#34920;&#36798;&#30340;&#30693;&#35782;&#25913;&#36827;&#20102;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26412;&#20307;&#65292;&#21363;&#25551;&#36848;&#36923;&#36753;&#65288;DL&#65289;&#30693;&#35782;&#24211;&#65292;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#25552;&#20379;&#20851;&#20110;&#21508;&#31181;&#39046;&#22495;&#30340;&#20016;&#23500;&#30693;&#35782;&#65292;&#24182;&#19988;&#20854;&#20013;&#24456;&#22810;&#22522;&#20110;ALC&#65292;&#21363;&#21407;&#22411;&#21644;&#34920;&#36798;&#21147;&#24378;&#30340;DL&#65292;&#25110;&#20854;&#25193;&#23637;&#12290;&#25506;&#32034;ALC&#26412;&#20307;&#30340;&#20027;&#35201;&#20219;&#21153;&#26159;&#35745;&#31639;&#35821;&#20041;&#34164;&#28085;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;FALCON&#65292;&#19968;&#20010;&#27169;&#31946;&#30340;ALC&#26412;&#20307;&#31070;&#32463;&#25512;&#29702;&#22120;&#65292;&#23427;&#20351;&#29992;&#27169;&#31946;&#36923;&#36753;&#36816;&#31639;&#31526;&#20026;&#20219;&#24847;ALC&#26412;&#20307;&#29983;&#25104;&#27169;&#22411;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#32467;&#26500;&#35745;&#31639;&#24544;&#23454;&#30340;&#35821;&#20041;&#34164;&#28085;&#12290;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;FALCON&#33021;&#22815;&#24544;&#23454;&#22320;&#36817;&#20284;&#35745;&#31639;ALC&#26412;&#20307;&#19978;&#30340;&#35821;&#20041;&#34164;&#28085;&#65292;&#20174;&#32780;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#19990;&#30028;&#27169;&#22411;&#21644;&#23545;&#20854;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FALCON&#33021;&#22815;&#23454;&#29616;&#36817;&#20284;&#25512;&#29702;&#65292;&#36741;&#21161;&#25512;&#29702;&#65288;&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#25512;&#29702;&#65289;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#20197;ALC&#34920;&#36798;&#30340;&#30693;&#35782;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many ontologies, i.e., Description Logic (DL) knowledge bases, have been developed to provide rich knowledge about various domains, and a lot of them are based on ALC, i.e., a prototypical and expressive DL, or its extensions. The main task that explores ALC ontologies is to compute semantic entailment. We developed FALCON, a Fuzzy ALC Ontology Neural reasoner, which uses fuzzy logic operators to generate model structures for arbitrary ALC ontologies, and uses multiple model structures to compute faithful semantic entailments. Theoretical results show that FALCON faithfully approximates semantic entailment over ALC ontologies and therefore endows neural networks with world models and the ability to reason over them. Experimental results show that FALCON enables approximate reasoning, paraconsistent reasoning (reasoning with inconsistencies), and improves machine learning in the biomedical domain by incorporating knowledge expressed in ALC.
&lt;/p&gt;</description></item></channel></rss>