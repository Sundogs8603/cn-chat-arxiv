<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#20351;&#29992;&#27491;&#21017;&#27969;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#20013;&#21382;&#21490;&#38590;&#39064;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>https://rss.arxiv.org/abs/2311.09200</link><description>&lt;p&gt;
&#27491;&#21017;&#27969;&#26159;&#21542;&#26159;&#35299;&#38145;&#25351;&#25968;&#26426;&#21046;&#30340;&#20851;&#38190;&#65311;&#32463;&#36807;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#21452;&#37325;&#32422;&#26463;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#26465;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.09200
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#27491;&#21017;&#27969;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#20013;&#21382;&#21490;&#38590;&#39064;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#26368;&#20808;&#36827;&#19988;&#20107;&#23454;&#26631;&#20934;&#26159;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DPSGD&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26412;&#36136;&#19978;&#26159;&#28010;&#36153;&#30340;&#12290;&#36890;&#36807;&#21521;&#27599;&#20010;&#26799;&#24230;&#28155;&#21152;&#22122;&#22768;&#65292;&#23427;&#20250;&#22312;&#27599;&#20010;&#26799;&#24230;&#27493;&#39588;&#20013;&#38477;&#20302;&#25972;&#20307;&#38544;&#31169;&#12290;&#23613;&#31649;&#32463;&#36807;15&#24180;&#30340;&#20016;&#23500;&#30740;&#31350;&#65292;&#25512;&#36827;&#20102;&#32452;&#21512;&#23450;&#29702;&#12289;&#23376;&#37319;&#26679;&#26041;&#27861;&#21644;&#23454;&#29616;&#25216;&#26415;&#65292;&#20294;&#24403;&#21069;&#30340;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#36798;&#21040;&#36275;&#22815;&#30340;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#20102;&#31169;&#19979;&#20248;&#21270;&#32780;&#35774;&#35745;&#30340;&#25351;&#25968;&#26426;&#21046;&#65288;ExpM&#65289;&#21382;&#26469;&#34987;&#25490;&#38500;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#31169;&#19979;&#35757;&#32451;&#20043;&#22806;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;ExpM&#38656;&#35201;&#20174;&#19968;&#31181;&#21382;&#26469;&#38590;&#20197;&#22788;&#29702;&#30340;&#23494;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23613;&#31649;&#26368;&#36817;&#21457;&#29616;&#20102;&#27491;&#21017;&#27969;&#27169;&#22411;&#65288;NFs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#36924;&#36817;&#38590;&#20197;&#22788;&#29702;&#20998;&#24067;&#30340;&#34920;&#36798;&#28145;&#24230;&#32593;&#32476;&#65292;&#20294;ExpM&#20173;&#28982;&#22788;&#20110;&#32972;&#26223;&#20013;&#12290;&#25105;&#20204;&#30340;&#35266;&#28857;&#26159;&#21033;&#29992;&#27491;&#21017;&#27969;&#26469;&#32469;&#36807;ExpM&#30340;&#21382;&#21490;&#38556;&#30861;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state of the art and de facto standard for differentially private machine learning (ML) is differentially private stochastic gradient descent (DPSGD). Yet, the method is inherently wasteful. By adding noise to every gradient, it diminishes the overall privacy with every gradient step. Despite 15 years of fruitful research advancing the composition theorems, sub-sampling methods, and implementation techniques, adequate accuracy and privacy is often unattainable with current private ML methods. Meanwhile, the Exponential Mechanism (ExpM), designed for private optimization, has been historically sidelined from privately training modern ML algorithms primarily because ExpM requires sampling from a historically intractable density. Despite the recent discovery of Normalizing Flow models (NFs), expressive deep networks for approximating intractable distributions, ExpM remains in the background. Our position is that leveraging NFs to circumvent historic obstructions of ExpM is a potential
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26368;&#26032;&#21457;&#24067;&#30340;Gemma&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;LLaVA&#26694;&#26550;&#20013;&#35757;&#32451;&#20102;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#36830;&#25509;&#22120;&#12289;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#20027;&#24178;&#21644;&#22686;&#21152;&#35821;&#35328;&#20027;&#24178;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.01331</link><description>&lt;p&gt;
LLaVA-Gemma&#65306;&#21033;&#29992;&#32039;&#20945;&#30340;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01331
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26368;&#26032;&#21457;&#24067;&#30340;Gemma&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;LLaVA&#26694;&#26550;&#20013;&#35757;&#32451;&#20102;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#36830;&#25509;&#22120;&#12289;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#20027;&#24178;&#21644;&#22686;&#21152;&#35821;&#35328;&#20027;&#24178;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26368;&#26032;&#21457;&#24067;&#30340;Gemma&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27969;&#34892;&#30340;LLaVA&#26694;&#26550;&#20013;&#35757;&#32451;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65288;MMFM&#65289;&#12290;&#29305;&#21035;&#20540;&#24471;&#20851;&#27880;&#30340;&#26159;2B&#21442;&#25968;&#30340;Gemma&#27169;&#22411;&#65292;&#23427;&#25552;&#20379;&#20102;&#26500;&#24314;&#21151;&#33021;&#24378;&#22823;&#30340;&#23567;&#35268;&#27169;MMFM&#30340;&#26426;&#20250;&#12290;&#19982;&#35813;&#39046;&#22495;&#20854;&#20182;&#35770;&#25991;&#30340;&#21457;&#29616;&#19968;&#33268;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#21435;&#38500;&#19977;&#31181;&#35774;&#35745;&#29305;&#24615;&#30340;&#24433;&#21709;&#65306;&#39044;&#35757;&#32451;&#36830;&#25509;&#22120;&#65292;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#20027;&#24178;&#65292;&#22686;&#21152;&#35821;&#35328;&#20027;&#24178;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;LLaVA-Gemma&#30340;&#32467;&#26524;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20013;&#31561;&#24615;&#33021;&#65292;&#20294;&#26410;&#33021;&#36229;&#36234;&#24403;&#21069;&#30456;&#23545;&#22823;&#23567;&#30340;SOTA&#27169;&#22411;&#12290;&#24615;&#33021;&#30340;&#26356;&#35814;&#32454;&#20998;&#26512;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#25928;&#26524;&#65306;&#36339;&#36807;&#39044;&#35757;&#32451;&#24448;&#24448;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#26356;&#22823;&#30340;&#35270;&#35273;&#27169;&#22411;&#26377;&#26102;&#20250;&#25552;&#39640;&#24615;&#33021;&#65292;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#25928;&#26524;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#35757;&#32451;&#37197;&#26041;&#65292;&#20195;&#30721;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01331v1 Announce Type: cross  Abstract: We train a suite of multimodal foundation models (MMFM) using the popular LLaVA framework with the recently released Gemma family of large language models (LLMs). Of particular interest is the 2B parameter Gemma model, which provides opportunities to construct capable small-scale MMFMs. In line with findings from other papers in this space, we test the effect of ablating three design features: pretraining the connector, utilizing a more powerful image backbone, and increasing the size of the language backbone. The resulting models, which we call LLaVA-Gemma, exhibit moderate performance on an array of evaluations, but fail to improve past the current comparably sized SOTA models. Closer analysis of performance shows mixed effects; skipping pretraining tends to reduce performance, larger vision models sometimes improve performance, and increasing language model size has inconsistent effects. We publicly release training recipes, code an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#26426;&#22120;&#27169;&#22411;&#65292;&#25903;&#25345;&#20102;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#36825;&#19968;&#35770;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.17101</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65306;&#19968;&#20010;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
AI Consciousness is Inevitable: A Theoretical Computer Science Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17101
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#26426;&#22120;&#27169;&#22411;&#65292;&#25903;&#25345;&#20102;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#36825;&#19968;&#35770;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#26469;&#23457;&#35270;&#24847;&#35782;&#65292;&#36825;&#26159;&#25968;&#23398;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#30740;&#31350;&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#30340;&#35745;&#31639;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#20026;&#24847;&#35782;&#24320;&#21457;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#30340;&#26426;&#22120;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#21463;&#21040;&#20102;&#33406;&#20262;&#183;&#22270;&#28789;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#35745;&#31639;&#27169;&#22411;&#21644;&#20271;&#32435;&#24503;&#183;&#24052;&#23572;&#26031;&#24847;&#35782;&#21095;&#22330;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#23613;&#31649;&#38750;&#24120;&#31616;&#21333;&#65292;&#36825;&#20010;&#27169;&#22411;&#22312;&#39640;&#23618;&#27425;&#19978;&#19982;&#35768;&#22810;&#20851;&#20110;&#20154;&#31867;&#21644;&#21160;&#29289;&#24847;&#35782;&#30340;&#20027;&#35201;&#31185;&#23398;&#29702;&#35770;&#30456;&#19968;&#33268;&#65292;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#26029;&#65306;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17101v1 Announce Type: new  Abstract: We look at consciousness through the lens of Theoretical Computer Science, a branch of mathematics that studies computation under resource limitations. From this perspective, we develop a formal machine model for consciousness. The model is inspired by Alan Turing's simple yet powerful model of computation and Bernard Baars' theater model of consciousness. Though extremely simple, the model aligns at a high level with many of the major scientific theories of human and animal consciousness, supporting our claim that machine consciousness is inevitable.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLMs&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#25903;&#25345;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#30340;&#23398;&#20064;&#21644;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#38544;&#34255;&#21327;&#20316;&#32454;&#33410;&#65292;&#23637;&#29616;&#20986;&#27604;&#29616;&#26377;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16854</link><description>&lt;p&gt;
&#19968;&#20010;&#19987;&#23478;&#20215;&#20540;&#19968;&#20010;&#20195;&#24065;&#65306;&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16854
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLMs&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#25903;&#25345;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#30340;&#23398;&#20064;&#21644;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#38544;&#34255;&#21327;&#20316;&#32454;&#33410;&#65292;&#23637;&#29616;&#20986;&#27604;&#29616;&#26377;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#65288;Expert-Token-Routing&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#36890;&#29992;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLM&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#19987;&#23478;LLMs&#34920;&#31034;&#20026;&#20803;LLM&#35789;&#27719;&#20013;&#30340;&#29305;&#27530;&#19987;&#23478;&#20195;&#24065;&#12290;&#20803;LLM&#21487;&#20197;&#36335;&#30001;&#21040;&#19987;&#23478;LLM&#65292;&#23601;&#20687;&#29983;&#25104;&#26032;&#20195;&#24065;&#19968;&#26679;&#12290;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#19981;&#20165;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19987;&#23478;LLMs&#30340;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#12290;&#23427;&#36824;&#21487;&#20197;&#38544;&#34255;&#29992;&#25143;&#35270;&#35282;&#20013;&#30340;&#35814;&#32454;&#21327;&#20316;&#36807;&#31243;&#65292;&#20419;&#36827;&#20132;&#20114;&#23601;&#20687;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;LLM&#19968;&#26679;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#28085;&#30422;&#20845;&#20010;&#19981;&#21516;&#19987;&#23478;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20102;&#21508;&#31181;&#29616;&#26377;&#30340;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#65292;&#23637;&#29616;&#20102;&#36890;&#36807;&#21327;&#21516;&#22810;&#20010;&#19987;&#23478;LLM&#26469;&#26500;&#24314;&#36890;&#29992;&#22411;LLM&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16854v1 Announce Type: cross  Abstract: We present Expert-Token-Routing, a unified generalist framework that facilitates seamless integration of multiple expert LLMs. Our framework represents expert LLMs as special expert tokens within the vocabulary of a meta LLM. The meta LLM can route to an expert LLM like generating new tokens. Expert-Token-Routing not only supports learning the implicit expertise of expert LLMs from existing instruction dataset but also allows for dynamic extension of new expert LLMs in a plug-and-play manner. It also conceals the detailed collaboration process from the user's perspective, facilitating interaction as though it were a singular LLM. Our framework outperforms various existing multi-LLM collaboration paradigms across benchmarks that incorporate six diverse expert domains, demonstrating effectiveness and robustness in building generalist LLM system via synergizing multiple expert LLMs.
&lt;/p&gt;</description></item><item><title>&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#23545;&#35805;&#24335;&#25945;&#23398;&#22330;&#26223;&#20013;&#65292;ChatGPT&#33021;&#22815;&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#65292;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.16687</link><description>&lt;p&gt;
&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16687
&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#23545;&#35805;&#24335;&#25945;&#23398;&#22330;&#26223;&#20013;&#65292;ChatGPT&#33021;&#22815;&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#65292;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#20986;&#29616;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#21069;&#26223;&#12290; LLM&#20855;&#26377;&#35299;&#37322;&#30693;&#35782;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#32771;&#34385;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#20026;&#23398;&#29983;&#25552;&#20379;&#23545;&#35805;&#24335;&#25945;&#23398;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#26816;&#39564;LLM&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#23545;&#35805;&#25945;&#23398;&#22330;&#26223;&#20013;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#25945;&#32946;&#32773;&#65292;&#26159;&#19968;&#20010;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290; &#26412;&#30740;&#31350;&#25307;&#21215;&#20102;34&#21517;&#26412;&#31185;&#29983;&#20316;&#20026;&#21442;&#19982;&#32773;&#65292;&#38543;&#26426;&#20998;&#20026;&#20004;&#32452;&#12290; &#23454;&#39564;&#32452;&#20351;&#29992;ChatGPT&#36827;&#34892;&#23545;&#35805;&#24335;&#25945;&#23398;&#65292;&#32780;&#25511;&#21046;&#32452;&#19982;&#20154;&#31867;&#25945;&#24072;&#20114;&#21160;&#12290; &#20004;&#32452;&#37117;&#23398;&#20064;&#20102;&#20449;&#24687;&#30456;&#20851;&#35838;&#31243;&#8220;&#25968;&#23383;&#22270;&#20687;&#8221;&#30340;&#30452;&#26041;&#22270;&#22343;&#34913;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16687v1 Announce Type: cross  Abstract: In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education. LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students. Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic. This research recruited 34 undergraduate students as participants, who were randomly divided into two groups. The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers. Both groups learned the histogram equalization unit in the information-related course "Digital Ima
&lt;/p&gt;</description></item><item><title>AnyV2V&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#31616;&#21270;&#35270;&#39057;&#32534;&#36753;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#20256;&#32479;&#21644;&#26032;&#39062;&#30340;&#32534;&#36753;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.14468</link><description>&lt;p&gt;
AnyV2V&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14468
&lt;/p&gt;
&lt;p&gt;
AnyV2V&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#31616;&#21270;&#35270;&#39057;&#32534;&#36753;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#20256;&#32479;&#21644;&#26032;&#39062;&#30340;&#32534;&#36753;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14468v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#36234; &#25688;&#35201;: &#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#28041;&#21450;&#32534;&#36753;&#28304;&#35270;&#39057;&#20197;&#21450;&#39069;&#22806;&#30340;&#25511;&#21046;&#65288;&#20363;&#22914;&#25991;&#26412;&#25552;&#31034;&#12289;&#20027;&#39064;&#25110;&#39118;&#26684;&#65289;&#65292;&#20197;&#29983;&#25104;&#19982;&#28304;&#35270;&#39057;&#21644;&#25552;&#20379;&#30340;&#25511;&#21046;&#30456;&#21305;&#37197;&#30340;&#26032;&#35270;&#39057;&#12290;&#20256;&#32479;&#26041;&#27861;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#32534;&#36753;&#31867;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#28385;&#36275;&#24191;&#27867;&#29992;&#25143;&#38656;&#27714;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AnyV2V&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20813;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#35270;&#39057;&#32534;&#36753;&#31616;&#21270;&#20026;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#65288;1&#65289;&#21033;&#29992;&#29616;&#25104;&#30340;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#65288;&#20363;&#22914;InstructPix2Pix&#12289;InstantID&#31561;&#65289;&#20462;&#25913;&#31532;&#19968;&#24103;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29616;&#26377;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;I2VGen-XL&#65289;&#36827;&#34892;DDIM&#36870;&#36716;&#21644;&#29305;&#24449;&#27880;&#20837;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;AnyV2V&#21487;&#20197;&#25554;&#20837;&#20219;&#20309;&#29616;&#26377;&#30340;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;&#65292;&#20197;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#32534;&#36753;&#26041;&#27861;&#65292;AnyV2V&#36824;&#21487;&#20197;&#25903;&#25345;&#26032;&#39062;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#21253;&#25324;&#21442;&#32771;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14468v1 Announce Type: cross  Abstract: Video-to-video editing involves editing a source video along with additional control (such as text prompts, subjects, or styles) to generate a new video that aligns with the source video and the provided control. Traditional methods have been constrained to certain editing types, limiting their ability to meet the wide range of user demands. In this paper, we introduce AnyV2V, a novel training-free framework designed to simplify video editing into two primary steps: (1) employing an off-the-shelf image editing model (e.g. InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion and feature injection. In the first stage, AnyV2V can plug in any existing image editing tools to support an extensive array of video editing tasks. Beyond the traditional prompt-based editing methods, AnyV2V also can support novel video editing tasks, including refe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;DeepFake&#26816;&#27979;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#25581;&#31034;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#23613;&#31649;LLMs&#24182;&#38750;&#19987;&#20026;&#23186;&#20307;&#21462;&#35777;&#20219;&#21153;&#35774;&#35745;&#65292;&#36825;&#19968;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.14077</link><description>&lt;p&gt;
&#32842;&#22825;GPT&#33021;&#22815;&#26816;&#27979;DeepFakes&#21527;&#65311;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23186;&#20307;&#21462;&#35777;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;DeepFake&#26816;&#27979;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#25581;&#31034;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#23613;&#31649;LLMs&#24182;&#38750;&#19987;&#20026;&#23186;&#20307;&#21462;&#35777;&#20219;&#21153;&#35774;&#35745;&#65292;&#36825;&#19968;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DeepFakes&#26159;&#25351;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#23186;&#20307;&#20869;&#23481;&#65292;&#30001;&#20110;&#20854;&#34987;&#29992;&#20316;&#25955;&#24067;&#34394;&#20551;&#20449;&#24687;&#30340;&#25163;&#27573;&#65292;&#24050;&#32463;&#25104;&#20026;&#36234;&#26469;&#36234;&#20196;&#20154;&#25285;&#24551;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#26816;&#27979;DeepFakes&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#32534;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;DeepFake&#26816;&#27979;&#20013;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#21487;&#20197;&#36890;&#36807;&#35880;&#24910;&#30340;&#23454;&#39564;&#35774;&#35745;&#21644;&#21450;&#26102;&#30340;&#24037;&#31243;&#26041;&#27861;&#25581;&#31034;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#32771;&#34385;&#21040;LLMs&#24182;&#19981;&#26159;&#26412;&#36136;&#19978;&#20026;&#23186;&#20307;&#21462;&#35777;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#65292;&#36825;&#19968;&#28857;&#30456;&#24403;&#26377;&#36259;&#65292;&#32780;&#19988;&#36825;&#20010;&#36807;&#31243;&#24182;&#19981;&#38656;&#35201;&#32534;&#31243;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14077v1 Announce Type: new  Abstract: DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation. Detecting DeepFakes is currently solved with programmed machine learning algorithms. In this work, we investigate the capabilities of multimodal large language models (LLMs) in DeepFake detection. We conducted qualitative and quantitative experiments to demonstrate multimodal LLMs and show that they can expose AI-generated images through careful experimental design and prompt engineering. This is interesting, considering that LLMs are not inherently tailored for media forensic tasks, and the process does not require programming. We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.11996</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#30693;&#35782;&#25552;&#21462;&#12289;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#21644;&#22810;&#27169;&#24577;&#26234;&#33021;&#22270;&#25512;&#29702;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11996
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#25105;&#20204;&#23558;&#19968;&#32452;&#28041;&#21450;&#29983;&#29289;&#26448;&#26009;&#39046;&#22495;&#30340;1,000&#31687;&#31185;&#23398;&#35770;&#25991;&#36716;&#21270;&#20026;&#35814;&#32454;&#30340;&#26412;&#20307;&#30693;&#35782;&#22270;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#26080;&#26631;&#24230;&#29305;&#24615;&#12290;&#36890;&#36807;&#22522;&#20110;&#33410;&#28857;&#30456;&#20284;&#24615;&#21644;&#20171;&#25968;&#20013;&#24515;&#24615;&#30340;&#32452;&#21512;&#25490;&#21517;&#65292;&#25506;&#27979;&#19981;&#21516;&#27010;&#24565;&#20043;&#38388;&#30340;&#22270;&#36941;&#21382;&#36335;&#24452;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#20837;&#30340;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#26597;&#35810;&#65292;&#35782;&#21035;&#30693;&#35782;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#21069;&#25152;&#26410;&#35265;&#30340;&#26448;&#26009;&#35774;&#35745;&#21450;&#20854;&#34892;&#20026;&#12290;&#19968;&#39033;&#27604;&#36739;&#25581;&#31034;&#20102;&#29983;&#29289;&#26448;&#26009;&#21644;&#36125;&#22810;&#33452;&#31532;&#20061;&#20132;&#21709;&#26354;&#20043;&#38388;&#30340;&#35814;&#32454;&#32467;&#26500;&#30456;&#20284;&#20043;&#22788;&#65292;&#31361;&#26174;&#20102;&#36890;&#36807;&#21516;&#26500;&#26144;&#23556;&#20849;&#20139;&#22797;&#26434;&#24615;&#27169;&#24335;&#12290;&#35813;&#31639;&#27861;&#36827;&#19968;&#27493;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#20998;&#32423;&#33740;&#19997;&#20307;&#30340;&#22797;&#21512;&#26448;&#26009;&#65292;&#23558;&#22270;&#37319;&#26679;&#30340;&#32852;&#21512;&#21512;&#25104;&#21407;&#29702;&#19982;&#24247;&#23450;&#26031;&#22522;&#12298;&#31532;&#19971;&#32452;&#25104;&#12299;&#20013;&#25552;&#21462;&#30340;&#21407;&#21017;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#36827;&#21270;&#31639;&#27861;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26041;&#27861;&#26469;&#25193;&#23637;&#33258;&#28982;&#34507;&#30333;&#36136;&#8220;&#35789;&#27719;&#37327;&#8221;&#65292;&#20197;&#24320;&#21457;&#20174;&#26410;&#23384;&#22312;&#36807;&#30340;&#20840;&#26032;&#34507;&#30333;&#36136;&#12290;</title><link>https://arxiv.org/abs/2403.08797</link><description>&lt;p&gt;
&#27169;&#25311;&#20998;&#23376;&#36827;&#21270;&#30340;&#36827;&#21270;&#31639;&#27861;&#65306;&#19968;&#20010;&#26032;&#39046;&#22495;&#30340;&#25552;&#35758;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Algorithms Simulating Molecular Evolution: A New Field Proposal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08797
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#36827;&#21270;&#31639;&#27861;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26041;&#27861;&#26469;&#25193;&#23637;&#33258;&#28982;&#34507;&#30333;&#36136;&#8220;&#35789;&#27719;&#37327;&#8221;&#65292;&#20197;&#24320;&#21457;&#20174;&#26410;&#23384;&#22312;&#36807;&#30340;&#20840;&#26032;&#34507;&#30333;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#21629;&#27963;&#21160;&#30340;&#22522;&#26412;&#21151;&#33021;&#22522;&#22240;&#34013;&#22270;&#32534;&#30721;&#22312;DNA&#20013;&#65292;&#36716;&#35793;&#20026;&#34507;&#30333;&#36136;--&#25512;&#21160;&#22823;&#22810;&#25968;&#20195;&#35874;&#36807;&#31243;&#30340;&#24341;&#25806;&#12290;&#26368;&#36817;&#30340;&#22522;&#22240;&#32452;&#27979;&#24207;&#36827;&#23637;&#25581;&#31034;&#20102;&#22823;&#37327;&#30340;&#34507;&#30333;&#36136;&#23478;&#26063;&#65292;&#20294;&#19982;&#25152;&#26377;&#21487;&#33021;&#30340;&#27688;&#22522;&#37240;&#24207;&#21015;&#30340;&#24040;&#22823;&#25628;&#32034;&#31354;&#38388;&#30456;&#27604;&#65292;&#24050;&#30693;&#30340;&#21151;&#33021;&#23478;&#26063;&#38598;&#21512;&#24456;&#23567;&#12290;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#33258;&#28982;&#20855;&#26377;&#26377;&#38480;&#30340;&#34507;&#30333;&#36136;&#8220;&#35789;&#27719;&#37327;&#8221;&#12290;&#22240;&#27492;&#65292;&#35745;&#31639;&#29983;&#29289;&#23398;&#23478;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#36825;&#20010;&#35789;&#27719;&#37327;&#26159;&#21542;&#21487;&#20197;&#25193;&#23637;&#65292;&#20197;&#21253;&#25324;&#24456;&#20037;&#20197;&#21069;&#28781;&#32477;&#25110;&#32773;&#20174;&#26410;&#22312;&#31532;&#19968;&#27425;&#36827;&#21270;&#20013;&#20986;&#29616;&#30340;&#26377;&#29992;&#34507;&#30333;&#36136;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#36890;&#36807;&#34701;&#21512;&#36827;&#21270;&#31639;&#27861;&#12289;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#65292;&#25105;&#20204;&#21487;&#20197;&#20419;&#36827;&#24320;&#21457;&#20174;&#26410;&#23384;&#22312;&#36807;&#30340;&#20840;&#26032;&#34507;&#30333;&#36136;&#12290;&#25105;&#20204;&#35774;&#24819;&#36825;&#39033;&#24037;&#20316;&#23558;&#24418;&#25104;&#35745;&#31639;&#36827;&#21270;&#30340;&#26032;&#30340;&#23376;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08797v1 Announce Type: cross  Abstract: The genetic blueprint for the essential functions of life is encoded in DNA, which is translated into proteins -- the engines driving most of our metabolic processes. Recent advancements in genome sequencing have unveiled a vast diversity of protein families, but compared to the massive search space of all possible amino acid sequences, the set of known functional families is minimal. One could say nature has a limited protein "vocabulary." The major question for computational biologists, therefore, is whether this vocabulary can be expanded to include useful proteins that went extinct long ago, or maybe never evolved in the first place. We outline a computational approach to solving this problem. By merging evolutionary algorithms, machine learning (ML), and bioinformatics, we can facilitate the development of completely novel proteins which have never existed before. We envision this work forming a new sub-field of computational evol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04306</link><description>&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Effectiveness Assessment of Recent Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#20986;&#29616;&#20195;&#34920;&#30528;&#36808;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#31243;&#24230;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#27969;&#34892;&#30340;LVLMs&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#22312;&#19987;&#19994;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#37327;&#36523;&#23450;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#33258;&#28982;&#12289;&#21307;&#30103;&#21644;&#24037;&#19994;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#30340;&#20840;&#38754;&#27979;&#35797;&#24179;&#21488;&#65292;&#28085;&#30422;&#20845;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#26174;&#33879;&#12289;&#20266;&#35013;&#21644;&#36879;&#26126;&#29289;&#20307;&#26816;&#27979;&#65292;&#20197;&#21450;&#24687;&#32905;&#21644;&#30382;&#32932;&#30149;&#21464;&#26816;&#27979;&#65292;&#20197;&#21450;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#26368;&#36817;&#19977;&#31181;&#24320;&#28304;LVLMs--MiniGPT-v2&#12289;LLaVA-1.5&#21644;Shikra--&#22312;&#35270;&#35273;&#35782;&#21035;&#21644;&#23450;&#20301;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#25968;&#23383;&#21462;&#35777;&#35843;&#26597;&#20013;&#26377;&#26395;&#25552;&#21319;&#35843;&#26597;&#25928;&#29575;&#65292;&#25913;&#21892;&#21487;&#36861;&#28335;&#24615;&#65292;&#24182;&#32531;&#35299;&#25191;&#27861;&#26426;&#26500;&#38754;&#20020;&#30340;&#25216;&#26415;&#21644;&#21496;&#27861;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2402.19366</link><description>&lt;p&gt;
SoK: &#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#39640;&#25968;&#23383;&#21462;&#35777;&#35843;&#26597;&#25928;&#29575;&#26041;&#38754;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
SoK: Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19366
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#25968;&#23383;&#21462;&#35777;&#35843;&#26597;&#20013;&#26377;&#26395;&#25552;&#21319;&#35843;&#26597;&#25928;&#29575;&#65292;&#25913;&#21892;&#21487;&#36861;&#28335;&#24615;&#65292;&#24182;&#32531;&#35299;&#25191;&#27861;&#26426;&#26500;&#38754;&#20020;&#30340;&#25216;&#26415;&#21644;&#21496;&#27861;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38656;&#35201;&#25968;&#23383;&#21462;&#35777;&#20998;&#26512;&#30340;&#26696;&#20214;&#25968;&#37327;&#22686;&#38271;&#65292;&#23545;&#25191;&#27861;&#26426;&#26500;&#21450;&#26102;&#36827;&#34892;&#35843;&#26597;&#30340;&#33021;&#21147;&#20135;&#29983;&#20102;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#36825;&#31687;&#31995;&#32479;&#21270;&#30693;&#35782;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#25968;&#23383;&#21462;&#35777;&#35843;&#26597;&#20013;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#28508;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;&#23545;&#29616;&#26377;&#30340;&#25968;&#23383;&#21462;&#35777;&#27169;&#22411;&#12289;&#24037;&#20855;&#12289;LLMs&#12289;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#22312;&#35843;&#26597;&#20013;&#21033;&#29992;LLMs&#30340;&#20840;&#38754;&#25991;&#29486;&#32508;&#36848;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#32508;&#36848;&#30830;&#23450;&#20102;&#29616;&#26377;&#25968;&#23383;&#21462;&#35777;&#27969;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#25972;&#21512;LLMs&#30340;&#38556;&#30861;&#21644;&#21487;&#33021;&#24615;&#12290;&#26368;&#32456;&#65292;&#30740;&#31350;&#26029;&#35328;&#65292;&#22312;&#36866;&#24403;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#25968;&#23383;&#21462;&#35777;&#20013;&#37319;&#29992;LLMs&#26377;&#26395;&#25552;&#21319;&#35843;&#26597;&#25928;&#29575;&#65292;&#25913;&#21892;&#21487;&#36861;&#28335;&#24615;&#65292;&#24182;&#32531;&#35299;&#25191;&#27861;&#26426;&#26500;&#38754;&#20020;&#30340;&#25216;&#26415;&#21644;&#21496;&#27861;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19366v1 Announce Type: cross  Abstract: The growing number of cases requiring digital forensic analysis raises concerns about law enforcement's ability to conduct investigations promptly. Consequently, this systemisation of knowledge paper delves into the potential and effectiveness of integrating Large Language Models (LLMs) into digital forensic investigation to address these challenges. A thorough literature review is undertaken, encompassing existing digital forensic models, tools, LLMs, deep learning techniques, and the utilisation of LLMs in investigations. The review identifies current challenges within existing digital forensic processes and explores both the obstacles and possibilities of incorporating LLMs. In conclusion, the study asserts that the adoption of LLMs in digital forensics, with appropriate constraints, holds the potential to enhance investigation efficiency, improve traceability, and alleviate technical and judicial barriers faced by law enforcement e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#26680;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#20197;&#21450;&#24320;&#21457;&#20102;&#32422;&#26463;&#26465;&#20214;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.18477</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#31614;&#21517;&#26680;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#29992;&#20110;&#38543;&#26426;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#26680;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#20197;&#21450;&#24320;&#21457;&#20102;&#32422;&#26463;&#26465;&#20214;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#22312;&#31185;&#23398;&#12289;&#20581;&#24247;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#31614;&#21517;&#26680;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#30340;&#8220;&#36335;&#24452;&#31354;&#38388;&#8221;&#19978;&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#27979;&#35797;&#65292;&#29992;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#36335;&#24452;&#31354;&#38388;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;CI&#27979;&#35797;&#34920;&#29616;&#20986;&#20005;&#26684;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#38750;&#24490;&#29615;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#24320;&#21457;&#20102;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#26469;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;&#22312;&#20551;&#35774;&#24544;&#23454;&#24615;&#21644;CI&#39044;&#35328;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#23436;&#22791;&#19988;&#27491;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18477v1 Announce Type: cross  Abstract: Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via "which variables enter the differential of which other variables". In this paper, we develop a kernel-based test of conditional independence (CI) on "path-space" -- solutions to SDEs -- by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph. Assuming faithfulness and a CI oracle, our algorithm is sound and complete. We empirical
&lt;/p&gt;</description></item><item><title>reBandit&#26159;&#19968;&#31181;&#22312;&#32447;RL&#31639;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#25928;&#24212;&#21644;&#36125;&#21494;&#26031;&#20808;&#39564;&#24555;&#36895;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#22312;&#31227;&#21160;&#20581;&#24247;&#29615;&#22659;&#20013;&#36890;&#36807;&#20010;&#24615;&#21270;&#24178;&#39044;&#26469;&#20943;&#23569;&#26032;&#20852;&#25104;&#24180;&#20154;&#30340;&#22823;&#40635;&#20351;&#29992;</title><link>https://arxiv.org/abs/2402.17739</link><description>&lt;p&gt;
reBandit&#65306;&#22522;&#20110;&#38543;&#26426;&#25928;&#24212;&#30340;&#22312;&#32447;RL&#31639;&#27861;&#29992;&#20110;&#20943;&#23569;&#22823;&#40635;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17739
&lt;/p&gt;
&lt;p&gt;
reBandit&#26159;&#19968;&#31181;&#22312;&#32447;RL&#31639;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#25928;&#24212;&#21644;&#36125;&#21494;&#26031;&#20808;&#39564;&#24555;&#36895;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#22312;&#31227;&#21160;&#20581;&#24247;&#29615;&#22659;&#20013;&#36890;&#36807;&#20010;&#24615;&#21270;&#24178;&#39044;&#26469;&#20943;&#23569;&#26032;&#20852;&#25104;&#24180;&#20154;&#30340;&#22823;&#40635;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#40635;&#20351;&#29992;&#21450;&#30456;&#20851;&#30340;&#22823;&#40635;&#20351;&#29992;&#38556;&#30861;&#65288;CUD&#65289;&#30340;&#19981;&#26029;&#22686;&#21152;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#30340;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#23588;&#20854;&#26159;&#22312;&#26032;&#20852;&#25104;&#24180;&#20154;&#65288;18-25&#23681;&#65289;&#20013;&#65292;&#23384;&#22312;&#26126;&#26174;&#30340;&#27835;&#30103;&#32570;&#21475;&#65292;&#22240;&#27492;&#35299;&#20915;&#22823;&#40635;&#20351;&#29992;&#21644;CUD&#20173;&#28982;&#26159;2030&#24180;&#32852;&#21512;&#22269;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDG&#65289;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;reBandit&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#31227;&#21160;&#20581;&#24247;&#30740;&#31350;&#20013;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#26469;&#20943;&#23569;&#26032;&#20852;&#25104;&#24180;&#20154;&#30340;&#22823;&#40635;&#20351;&#29992;&#12290;reBandit&#21033;&#29992;&#38543;&#26426;&#25928;&#24212;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#36125;&#21494;&#26031;&#20808;&#39564;&#20197;&#22312;&#22024;&#26434;&#30340;&#31227;&#21160;&#20581;&#24247;&#29615;&#22659;&#20013;&#24555;&#36895;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;reBandit&#37319;&#29992;&#32463;&#39564;&#36125;&#21494;&#26031;&#21644;&#20248;&#21270;&#25216;&#26415;&#26469;&#22312;&#32447;&#33258;&#20027;&#26356;&#26032;&#20854;&#36229;&#21442;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#25311;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17739v1 Announce Type: new  Abstract: The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally. With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG). In this work, we develop an online reinforcement learning (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs. reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments. Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters online. To evaluate the performance of our algorithm, we construct a simulation testbed using data from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#39044;&#31639;&#32422;&#26463;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#20248;&#20808;&#35745;&#21010;&#21644;&#21160;&#24577;&#35268;&#21010;&#21046;&#23450;&#35745;&#21010;&#26469;&#35299;&#20915;&#29992;&#25143;&#26597;&#35810;&#38382;&#39064;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#36807;&#31243;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;</title><link>https://arxiv.org/abs/2402.15960</link><description>&lt;p&gt;
&#20855;&#26377;&#39044;&#31639;&#32422;&#26463;&#30340;&#24037;&#20855;&#23398;&#20064;&#19982;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Budget-Constrained Tool Learning with Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#39044;&#31639;&#32422;&#26463;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#20248;&#20808;&#35745;&#21010;&#21644;&#21160;&#24577;&#35268;&#21010;&#21046;&#23450;&#35745;&#21010;&#26469;&#35299;&#20915;&#29992;&#25143;&#26597;&#35810;&#38382;&#39064;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#36807;&#31243;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#20204;&#22312;&#24037;&#20855;&#23398;&#20064;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#26159;&#20851;&#27880;&#22312;&#29305;&#23450;&#39044;&#31639;&#32422;&#26463;&#19979;&#35299;&#20915;&#29992;&#25143;&#26597;&#35810;&#30340;&#20855;&#26377;&#39044;&#31639;&#32422;&#26463;&#30340;&#24037;&#20855;&#23398;&#20064;&#38382;&#39064;&#21364;&#34987;&#24191;&#27867;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#39044;&#31639;&#32422;&#26463;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#22312;&#39044;&#31639;&#32422;&#26463;&#19979;&#21019;&#24314;&#19968;&#20010;&#20248;&#20808;&#35745;&#21010;&#65292;&#28982;&#21518;&#20877;&#21033;&#29992;&#24037;&#20855;&#12290;&#35813;&#35745;&#21010;&#27010;&#36848;&#20102;&#21487;&#34892;&#30340;&#24037;&#20855;&#21450;&#20854;&#21487;&#34987;&#20351;&#29992;&#30340;&#26368;&#22823;&#27425;&#25968;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#36807;&#31243;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#26356;&#24191;&#27867;&#30340;&#35282;&#24230;&#26469;&#20998;&#37197;&#39044;&#31639;&#12290;&#20026;&#20102;&#22312;&#19981;&#20135;&#29983;&#26174;&#33879;&#39069;&#22806;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#35774;&#35745;&#35745;&#21010;&#65292;&#25105;&#20204;&#24314;&#35758;&#26681;&#25454;&#36807;&#21435;&#30340;&#32463;&#39564;&#39318;&#20808;&#20272;&#35745;&#20505;&#36873;&#24037;&#20855;&#30340;&#26377;&#29992;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#26469;&#21046;&#23450;&#35745;&#21010;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#21508;&#31181;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15960v1 Announce Type: new  Abstract: Despite intensive efforts devoted to tool learning, the problem of budget-constrained tool learning, which focuses on resolving user queries within a specific budget constraint, has been widely overlooked. This paper proposes a novel method for budget-constrained tool learning. Our approach involves creating a preferable plan under the budget constraint before utilizing the tools. This plan outlines the feasible tools and the maximum number of times they can be employed, offering a comprehensive overview of the tool learning process for large language models. This allows them to allocate the budget from a broader perspective. To devise the plan without incurring significant extra costs, we suggest initially estimating the usefulness of the candidate tools based on past experience. Subsequently, we employ dynamic programming to formulate the plan. Experimental results demonstrate that our method can be integrated with various tool learnin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36890;&#36807;&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#65292;&#21033;&#29992;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#65292;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#20248;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#30340;&#22522;&#20934;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14730</link><description>&lt;p&gt;
Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford-Steerable Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14730
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36890;&#36807;&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#65292;&#21033;&#29992;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#65292;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#20248;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#30340;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;$\mathrm{E}(p, q)$&#31561;&#21464;CNN&#31867;&#12290; CS-CNNs&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;$\mathbb{R}^{p,q}$&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#12290; &#23427;&#20204;&#28085;&#30422;&#20102;&#20363;&#22914;$\mathrm{E}(3)$&#22312;$\mathbb{R}^3$&#19978;&#21644;Poincar\'e&#22312;&#38389;&#21487;&#22827;&#26031;&#22522;&#26102;&#31354;$\mathbb{R}^{1,3}$&#19978;&#30340;&#31561;&#21464;&#24615;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36890;&#36807;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#12290; &#22312;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#26041;&#27861;&#19978;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14730v1 Announce Type: cross  Abstract: We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a novel class of $\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector fields on pseudo-Euclidean spaces $\mathbb{R}^{p,q}$. They cover, for instance, $\mathrm{E}(3)$-equivariance on $\mathbb{R}^3$ and Poincar\'e-equivariance on Minkowski spacetime $\mathbb{R}^{1,3}$. Our approach is based on an implicit parametrization of $\mathrm{O}(p,q)$-steerable kernels via Clifford group equivariant neural networks. We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#20171;&#32461;&#20102;&#31038;&#20250;&#29615;&#22659;&#35774;&#35745;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#25919;&#31574;&#21046;&#23450;&#30340;AI&#36890;&#29992;&#26694;&#26550;&#65292;&#26088;&#22312;&#25429;&#25417;&#19968;&#33324;&#32463;&#27982;&#29615;&#22659;&#65292;&#36890;&#36807;AI&#27169;&#25311;&#31995;&#32479;&#20998;&#26512;&#25919;&#24220;&#21644;&#32463;&#27982;&#25919;&#31574;&#65292;&#24182;&#24378;&#35843;&#26410;&#26469;&#22522;&#20110;AI&#30340;&#25919;&#31574;&#21046;&#23450;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.14090</link><description>&lt;p&gt;
&#31038;&#20250;&#29615;&#22659;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Social Environment Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14090
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#20171;&#32461;&#20102;&#31038;&#20250;&#29615;&#22659;&#35774;&#35745;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#25919;&#31574;&#21046;&#23450;&#30340;AI&#36890;&#29992;&#26694;&#26550;&#65292;&#26088;&#22312;&#25429;&#25417;&#19968;&#33324;&#32463;&#27982;&#29615;&#22659;&#65292;&#36890;&#36807;AI&#27169;&#25311;&#31995;&#32479;&#20998;&#26512;&#25919;&#24220;&#21644;&#32463;&#27982;&#25919;&#31574;&#65292;&#24182;&#24378;&#35843;&#26410;&#26469;&#22522;&#20110;AI&#30340;&#25919;&#31574;&#21046;&#23450;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#25919;&#24220;&#21644;&#32463;&#27982;&#25919;&#31574;&#21046;&#23450;&#30340;&#25216;&#26415;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#20171;&#32461;&#20102;&#31038;&#20250;&#29615;&#22659;&#35774;&#35745;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#25919;&#31574;&#21046;&#23450;&#30340;AI&#36890;&#29992;&#26694;&#26550;&#65292;&#19982;&#24378;&#21270;&#23398;&#20064;&#12289;&#32463;&#27982;&#19982;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#31038;&#21306;&#30456;&#36830;&#25509;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#25429;&#25417;&#19968;&#33324;&#32463;&#27982;&#29615;&#22659;&#65292;&#21253;&#25324;&#23545;&#25919;&#31574;&#30446;&#26631;&#30340;&#25237;&#31080;&#65292;&#24182;&#20026;&#36890;&#36807;AI&#27169;&#25311;&#23545;&#25919;&#24220;&#21644;&#32463;&#27982;&#25919;&#31574;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#25552;&#20379;&#25351;&#23548;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#26410;&#26469;&#22522;&#20110;AI&#30340;&#25919;&#31574;&#21046;&#23450;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#24320;&#25918;&#38382;&#39064;&#12290;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24076;&#26395;&#23454;&#29616;&#21508;&#31181;&#31038;&#20250;&#31119;&#21033;&#30446;&#26631;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#20855;&#36947;&#24503;&#21644;&#36127;&#36131;&#20219;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14090v1 Announce Type: new  Abstract: Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing Social Environment Design, a general framework for the use of AI for automated policy-making that connects with the Reinforcement Learning, EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI simulation. We highlight key open problems for future research in AI-based policy-making. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797;&#39537;&#21160;&#24320;&#21457;&#65288;TDD&#65289;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#25551;&#36848;&#21644;&#27979;&#35797;&#20316;&#20026;&#36755;&#20837;&#26159;&#21542;&#20248;&#20110;&#20165;&#23558;&#38382;&#39064;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20195;&#30721;&#29983;&#25104;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.13521</link><description>&lt;p&gt;
&#38754;&#21521;&#20195;&#30721;&#29983;&#25104;&#30340;&#27979;&#35797;&#39537;&#21160;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Test-Driven Development for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797;&#39537;&#21160;&#24320;&#21457;&#65288;TDD&#65289;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#25551;&#36848;&#21644;&#27979;&#35797;&#20316;&#20026;&#36755;&#20837;&#26159;&#21542;&#20248;&#20110;&#20165;&#23558;&#38382;&#39064;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20195;&#30721;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13521v1 &#20844;&#21578;&#31867;&#22411;:&#20132;&#21449;&#25688;&#35201;:&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT4&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#20174;&#38382;&#39064;&#25551;&#36848;&#20013;&#29983;&#25104;&#20195;&#30721;&#29255;&#27573;&#30340;&#33021;&#21147;&#12290;&#20256;&#32479;&#19978;&#65292;&#20154;&#31867;&#32534;&#20889;&#36719;&#20214;&#30340;&#26041;&#27861;&#31867;&#20284;&#20110;&#20174;&#38382;&#39064;&#25551;&#36848;&#25110;&#38656;&#27714;&#20013;&#32534;&#20889;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#26377;&#20960;&#39033;&#30740;&#31350;&#24050;&#32463;&#26174;&#31034;&#20102;&#27979;&#35797;&#39537;&#21160;&#24320;&#21457;&#65288;TDD&#65289;&#30340;&#20215;&#20540;&#65292;&#21363;&#22312;&#32534;&#20889;&#21151;&#33021;&#20195;&#30721;&#20043;&#21069;&#65292;&#20154;&#31867;&#26681;&#25454;&#38382;&#39064;&#25551;&#36848;&#32534;&#20889;&#27979;&#35797;&#12290;&#22312;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#29615;&#22659;&#20013;&#65292;TDD&#30340;&#19968;&#20010;&#26126;&#26174;&#22909;&#22788;&#26159;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#30830;&#20999;&#30693;&#36947;&#29983;&#25104;&#30340;&#20195;&#30721;&#26159;&#21542;&#36890;&#36807;&#20102;&#25152;&#26377;&#32473;&#23450;&#30340;&#27979;&#35797;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#20551;&#35774;&#65306;&#23558;&#38382;&#39064;&#25551;&#36848;&#21644;&#27979;&#35797;&#20316;&#20026;GPT4&#30340;&#36755;&#20837;&#35201;&#20248;&#20110;&#20165;&#23558;&#38382;&#39064;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;TGen&#30340;&#26694;&#26550;&#12290;&#22312;MBPP&#12289;HumanEval&#21644;CodeChef&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13521v1 Announce Type: cross  Abstract: Large language models (LLMs) like GPT4, have shown proficiency in generating code snippets from problem statements. Traditionally software development by humans followed a similar methodology of writing code from problem statements or requirements. However, in the past, there have been several studies that have shown the value of test-driven development (TDD) where humans write tests based on problem statements before the code for the functionality is written. In the context of LLM-based code generation, one obvious benefit of TDD is that the developer then knows for sure if the generated code has passed all the given tests or not. Therefore, in this paper, we want to empirically evaluate the hypothesis: giving the problem statements and tests as input to GPT4 is better than just giving the problem statement as input. To test our hypothesis, we build a framework TGen. In our experiments on the MBPP, HumanEval and CodeChef datasets, we 
&lt;/p&gt;</description></item><item><title>&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09721</link><description>&lt;p&gt;
&#35828;&#26381;&#19968;&#20301;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Persuading a Learning Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09721
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#65288;&#26356;&#19968;&#33324;&#22320;&#65292;&#20219;&#20309;&#20855;&#26377;&#23436;&#20840;&#20449;&#24687;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65289;&#65292;&#20854;&#20013;&#22996;&#25176;&#20154;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#20195;&#29702;&#20154;&#20351;&#29992;&#31639;&#27861;&#26469;&#23398;&#20064;&#22914;&#20309;&#23545;&#22996;&#25176;&#20154;&#30340;&#20449;&#21495;&#20570;&#20986;&#21709;&#24212;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#20010;&#19968;&#27425;&#24615;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#20195;&#29702;&#20154;&#36817;&#20284;&#22320;&#26368;&#20339;&#21709;&#24212;&#12290;&#36890;&#36807;&#36825;&#20010;&#31616;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#65306;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#21487;&#20197;&#20445;&#35777;&#20854;&#25928;&#29992;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#20043;&#38388;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#65307;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#22996;&#25176;&#20154;&#22312;&#23398;&#20064;&#27169;&#22411;&#19982;&#38750;&#23398;&#20064;&#27169;&#22411;&#20013;&#21487;&#20197;&#33719;&#24471;&#30340;&#25928;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#26159;&#26377;&#30028;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09721v1 Announce Type: cross  Abstract: We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bound
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#24341;&#23548;&#27169;&#22411;&#21521;&#23454;&#20107;&#24615;&#38752;&#36817;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#32622;&#20449;&#20272;&#35745;&#21644;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.09267</link><description>&lt;p&gt;
&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#65306;&#36890;&#36807;&#33258;&#35780;&#20943;&#32531;LLMs&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#24341;&#23548;&#27169;&#22411;&#21521;&#23454;&#20107;&#24615;&#38752;&#36817;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#32622;&#20449;&#20272;&#35745;&#21644;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26174;&#31034;&#20986;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#65288;&#21363;&#8220;&#24187;&#35273;&#8221;&#65289;&#24448;&#24448;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#20351;&#23427;&#20204;&#20855;&#26377;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24187;&#35273;&#38382;&#39064;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#20107;&#23454;&#24615;&#27880;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#65292;&#21363;&#21033;&#29992;LLM&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#25552;&#20379;&#35757;&#32451;&#20449;&#21495;&#65292;&#23558;&#27169;&#22411;&#24341;&#23548;&#21521;&#23454;&#20107;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#33258;&#25105;&#35780;&#20272;&#32452;&#20214;Self-Eval&#32435;&#20837;&#21040;LLM&#20013;&#65292;&#20197;&#20165;&#22522;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#39564;&#35777;&#20854;&#33258;&#24049;&#29983;&#25104;&#30340;&#22238;&#22797;&#30340;&#23454;&#20107;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#25105;&#30693;&#35782;&#35843;&#25972;&#65288;SK-Tuning&#65289;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#25913;&#21892;&#27169;&#22411;&#30340;&#32622;&#20449;&#20272;&#35745;&#21644;&#26657;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#33258;&#25105;&#27880;&#37322;&#30340;&#22238;&#22797;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#31639;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09267v1 Announce Type: cross Abstract: Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. "hallucinations", even when they hold relevant knowledge. To address these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's self-evaluation ability by improving the model's confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#26080;&#37197;&#23545;&#30340;&#25513;&#30721;-&#25991;&#26412;&#30417;&#30563;&#26041;&#27861;&#26469;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#30340;&#26694;&#26550;(Uni-OVSeg)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29420;&#31435;&#30340;&#22270;&#20687;-&#25513;&#30721;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25552;&#21462;&#20108;&#36827;&#21046;&#25513;&#30721;&#24182;&#19982;&#23454;&#20307;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.08960</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#37197;&#23545;&#30340;&#25513;&#30721;-&#25991;&#26412;&#30417;&#30563;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#26080;&#37197;&#23545;&#30340;&#25513;&#30721;-&#25991;&#26412;&#30417;&#30563;&#26041;&#27861;&#26469;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#30340;&#26694;&#26550;(Uni-OVSeg)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29420;&#31435;&#30340;&#22270;&#20687;-&#25513;&#30721;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25552;&#21462;&#20108;&#36827;&#21046;&#25513;&#30721;&#24182;&#19982;&#23454;&#20307;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#23574;&#31471;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22270;&#20687;-&#25513;&#30721;-&#25991;&#26412;&#19977;&#20803;&#32452;&#65292;&#28982;&#32780;&#36825;&#31181;&#21463;&#38480;&#30340;&#27880;&#37322;&#22312;&#22797;&#26434;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#65292;&#24182;&#19988;&#36935;&#21040;&#20102;&#21487;&#25193;&#23637;&#24615;&#30340;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#36890;&#36807;&#20165;&#20351;&#29992;&#25991;&#26412;&#30417;&#30563;&#26469;&#20943;&#23569;&#27880;&#37322;&#30340;&#25104;&#26412;&#65292;&#20294;&#30417;&#30563;&#30340;&#19981;&#23436;&#25972;&#24615;&#20005;&#37325;&#38480;&#21046;&#20102;&#22810;&#26679;&#24615;&#21644;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#29420;&#31435;&#30340;&#22270;&#20687;-&#25513;&#30721;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#35299;&#25918;&#20102;&#25513;&#30721;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#20005;&#26684;&#23545;&#24212;&#20851;&#31995;&#65292;&#36825;&#20123;&#23545;&#21487;&#20197;&#20998;&#21035;&#36731;&#26494;&#25910;&#38598;&#12290;&#20511;&#21161;&#36825;&#31181;&#26080;&#37197;&#23545;&#30340;&#25513;&#30721;-&#25991;&#26412;&#30417;&#30563;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#26694;&#26550;(Uni-OVSeg)&#65292;&#23427;&#21033;&#29992;&#33258;&#20449;&#30340;&#25513;&#30721;&#39044;&#27979;&#21644;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#23454;&#20307;&#12290;&#21033;&#29992;&#29420;&#31435;&#30340;&#22270;&#20687;-&#25513;&#30721;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#25105;&#20204;&#39044;&#27979;&#20102;&#19968;&#32452;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;CLIP&#23884;&#20837;&#23558;&#23427;&#20204;&#19982;&#23454;&#20307;&#20851;&#32852;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08960v1 Announce Type: cross Abstract: Contemporary cutting-edge open-vocabulary segmentation approaches commonly rely on image-mask-text triplets, yet this restricted annotation is labour-intensive and encounters scalability hurdles in complex real-world scenarios. Although some methods are proposed to reduce the annotation cost with only text supervision, the incompleteness of supervision severely limits the versatility and performance. In this paper, we liberate the strict correspondence between masks and texts by using independent image-mask and image-text pairs, which can be easily collected respectively. With this unpaired mask-text supervision, we propose a new weakly-supervised open-vocabulary segmentation framework (Uni-OVSeg) that leverages confident pairs of mask predictions and entities in text descriptions. Using the independent image-mask and image-text pairs, we predict a set of binary masks and associate them with entities by resorting to the CLIP embedding s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.05894</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#36935;&#35265;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Meets Graph Neural Network in Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#26399;&#23398;&#26415;&#30028;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#65288;TAG&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#28508;&#21147;&#26377;&#25152;&#25259;&#38706;&#65292;&#20294;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#21463;&#21040;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#39640;&#65292;&#25512;&#29702;&#36807;&#31243;&#20013;&#24310;&#36831;&#38271;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34429;&#28982;&#36731;&#37327;&#19988;&#25797;&#38271;&#23398;&#20064;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#20294;&#23545;&#20110;&#30495;&#23454;&#24212;&#29992;&#20013;TAG&#22797;&#26434;&#35821;&#20041;&#30340;&#25226;&#25569;&#26377;&#25152;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;TAG&#20013;&#33410;&#28857;&#20998;&#31867;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#35328;&#22270;&#30693;&#35782;&#33976;&#39311;&#65288;LinguGKD&#65289;&#65292;&#20351;&#29992;LLMs&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#65292;GNNs&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#20854;&#20013;&#21253;&#25324;&#23545;LLM&#36827;&#34892;TAG&#23450;&#21521;&#25351;&#23548;&#35843;&#25972;&#20197;&#24212;&#23545;&#35774;&#35745;&#30340;&#33410;&#28857;&#20998;&#31867;&#25552;&#31034;&#65292;&#28982;&#21518;&#23545;&#23618;&#27425;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference. Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation. It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#36741;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#24120;&#31867;&#20154;&#30340;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20808;&#36827;&#27169;&#22411;&#22914;GPT-4V&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#31561;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04788</link><description>&lt;p&gt;
MLLM&#20316;&#20026;&#27861;&#23448;&#65306;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#35780;&#20272;&#22810;&#27169;&#24577;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#36741;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#24120;&#31867;&#20154;&#30340;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20808;&#36827;&#27169;&#22411;&#22914;GPT-4V&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#31561;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;MLLM&#30340;&#23454;&#29992;&#24615;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#31526;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#12290;&#26412;&#25991;&#21463;&#21040;LLM&#27169;&#22411;&#20013;LLM&#20316;&#20026;&#27861;&#23448;&#30340;&#21551;&#21457;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#34987;&#31216;&#20026;MLLM&#20316;&#20026;&#27861;&#23448;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#21327;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;&#35780;&#20998;&#35780;&#20272;&#12289;&#23545;&#27604;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#31867;&#20154;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20687;GPT-4V&#36825;&#26679;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22810;&#26679;&#30340;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#30340;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#23545;MLLM&#30340;&#25913;&#36827;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further rese
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Albatross&#30340;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#27169;&#25311;&#33258;&#25105;&#23545;&#24328;&#23398;&#20064;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#21516;&#26102;&#21338;&#24328;&#20013;&#30340;&#26377;&#30028;&#29702;&#24615;&#20195;&#29702;&#34892;&#20026;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#19982;&#20219;&#20309;&#28216;&#25103;&#23454;&#21147;&#30340;&#20195;&#29702;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2402.03136</link><description>&lt;p&gt;
&#35299;&#20915;&#21512;&#20316;&#19982;&#31454;&#20105;&#30340;&#21516;&#26102;&#21338;&#24328;&#30340;&#38646;&#23556;&#20987;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03136
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Albatross&#30340;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#27169;&#25311;&#33258;&#25105;&#23545;&#24328;&#23398;&#20064;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#21516;&#26102;&#21338;&#24328;&#20013;&#30340;&#26377;&#30028;&#29702;&#24615;&#20195;&#29702;&#34892;&#20026;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#19982;&#20219;&#20309;&#28216;&#25103;&#23454;&#21147;&#30340;&#20195;&#29702;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#28216;&#25103;&#20013;&#65292;&#33258;&#25105;&#23545;&#24328;&#21644;&#35268;&#21010;&#30340;&#32452;&#21512;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#22269;&#38469;&#35937;&#26827;&#21644;&#22260;&#26827;&#12290;&#28982;&#32780;&#65292;&#23558;AlphaZero&#31561;&#31639;&#27861;&#35843;&#25972;&#21040;&#21516;&#26102;&#21338;&#24328;&#20013;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#65292;&#23545;&#20854;&#20182;&#20195;&#29702;&#30340;&#21516;&#26102;&#34892;&#21160;&#32570;&#20047;&#20449;&#24687;&#26159;&#19968;&#20010;&#38480;&#21046;&#22240;&#32032;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#36873;&#25321;&#19981;&#21516;&#30340;&#32435;&#20160;&#22343;&#34913;&#25110;&#32773;&#26681;&#26412;&#19981;&#26368;&#20248;&#22320;&#36827;&#34892;&#28216;&#25103;&#12290;&#22240;&#27492;&#65292;&#22312;&#19982;&#20854;&#20182;&#20195;&#29702;&#21338;&#24328;&#26102;&#65292;&#24314;&#27169;&#20854;&#34892;&#20026;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Albatross&#65306;&#20351;&#29992;&#27169;&#25311;&#33258;&#25105;&#23545;&#24328;&#26469;&#23398;&#20064;&#26377;&#30028;&#29702;&#24615;&#20195;&#29702;&#21644;&#22522;&#20110;&#28201;&#24230;&#30340;&#21453;&#24212;&#20248;&#21270;&#30340;AlphaZero&#12290;Albatross&#23398;&#20064;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#34913;&#27010;&#24565;&#65306;&#24179;&#28369;&#26368;&#20339;&#21453;&#24212;&#26497;&#31471;&#22343;&#34913;(SBRLE)&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19982;&#20219;&#20309;&#28216;&#25103;&#23454;&#21147;&#30340;&#20195;&#29702;&#30340;&#21512;&#20316;&#21644;&#31454;&#20105;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#21516;&#26102;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#19978;&#23545;Albatross&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#19982;AlphaZero&#30456;&#27604;&#65292;Albatross&#33021;&#22815;&#21516;&#26102;&#21338;&#24328;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#28216;&#25103;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combination of self-play and planning has achieved great successes in sequential games, for instance in Chess and Go. However, adapting algorithms such as AlphaZero to simultaneous games poses a new challenge. In these games, missing information about concurrent actions of other agents is a limiting factor as they may select different Nash equilibria or do not play optimally at all. Thus, it is vital to model the behavior of the other agents when interacting with them in simultaneous games. To this end, we propose Albatross: AlphaZero for Learning Bounded-rational Agents and Temperature-based Response Optimization using Simulated Self-play. Albatross learns to play the novel equilibrium concept of a Smooth Best Response Logit Equilibrium (SBRLE), which enables cooperation and competition with agents of any playing strength. We perform an extensive evaluation of Albatross on a set of cooperative and competitive simultaneous perfect-information games. In contrast to AlphaZero, Albatr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22238;&#24402;&#20219;&#21153;&#26102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#36890;&#36807;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#26469;&#25512;&#23548;&#32622;&#20449;&#24230;&#24230;&#37327;&#65292;&#24314;&#31435;&#20102;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#19982;&#20010;&#20307;&#34442;&#23376;&#31181;&#32676;&#39044;&#27979;&#30340;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#24847;&#22823;&#21033;&#23041;&#23612;&#25176;&#22320;&#21306;&#21644;&#24503;&#22269;&#19978;&#33713;&#33589;&#27827;&#35895;&#30340;&#22320;&#21306;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.17342</link><description>&lt;p&gt;
&#25552;&#39640;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Latent Space Metric for Enhancing Prediction Confidence in Earth Observation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17342
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22238;&#24402;&#20219;&#21153;&#26102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#36890;&#36807;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#26469;&#25512;&#23548;&#32622;&#20449;&#24230;&#24230;&#37327;&#65292;&#24314;&#31435;&#20102;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#19982;&#20010;&#20307;&#34442;&#23376;&#31181;&#32676;&#39044;&#27979;&#30340;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#24847;&#22823;&#21033;&#23041;&#23612;&#25176;&#22320;&#21306;&#21644;&#24503;&#22269;&#19978;&#33713;&#33589;&#27827;&#35895;&#30340;&#22320;&#21306;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22238;&#24402;&#20219;&#21153;&#26102;&#65292;&#37325;&#28857;&#20851;&#27880;&#34442;&#23376;&#31181;&#32676;&#65288;MA&#65289;&#20272;&#35745;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;EO&#25968;&#25454;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#26469;&#25512;&#23548;&#32622;&#20449;&#24230;&#24230;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#24314;&#31435;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#19982;&#20010;&#20307;MA&#39044;&#27979;&#30340;&#32477;&#23545;&#35823;&#24046;&#65288;AE&#65289;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#20102;&#24847;&#22823;&#21033;&#23041;&#23612;&#25176;&#22320;&#21306;&#21644;&#24503;&#22269;&#19978;&#33713;&#33589;&#27827;&#35895;&#30340;EO&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#22320;&#21306;&#21463;&#34442;&#23376;&#31181;&#32676;&#30340;&#24433;&#21709;&#26174;&#33879;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#21457;&#29616;&#26159;MA&#39044;&#27979;&#30340;AE&#19982;&#25152;&#25552;&#20986;&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#20043;&#38388;&#23384;&#22312;0.46&#30340;&#26174;&#33879;&#30456;&#20851;&#24615;&#12290;&#36825;&#20010;&#30456;&#20851;&#24615;&#24847;&#21619;&#30528;&#36825;&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;&#12289;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;AI&#27169;&#22411;&#22312;&#35813;&#32972;&#26223;&#19979;&#30340;&#39044;&#27979;&#21487;&#38752;&#24615;&#21644;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a new approach for estimating confidence in machine learning model predictions, specifically in regression tasks utilizing Earth Observation (EO) data, with a particular focus on mosquito abundance (MA) estimation. We take advantage of a Variational AutoEncoder architecture, to derive a confidence metric by the latent space representations of EO datasets. This methodology is pivotal in establishing a correlation between the Euclidean distance in latent representations and the Absolute Error (AE) in individual MA predictions. Our research focuses on EO datasets from the Veneto region in Italy and the Upper Rhine Valley in Germany, targeting areas significantly affected by mosquito populations. A key finding is a notable correlation of 0.46 between the AE of MA predictions and the proposed confidence metric. This correlation signifies a robust, new metric for quantifying the reliability and enhancing the trustworthiness of the AI model's predictions in the context of 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#32771;&#34385;&#20102;&#25439;&#22833;&#21644;&#19981;&#30830;&#23450;&#24615;&#22522;&#30784;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.13927</link><description>&lt;p&gt;
&#20851;&#20110;&#25439;&#22833;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the convergence of loss and uncertainty-based active learning algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13927
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#32771;&#34385;&#20102;&#25439;&#22833;&#21644;&#19981;&#30830;&#23450;&#24615;&#22522;&#30784;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#25439;&#22833;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#32452;&#26465;&#20214;&#65292;&#30830;&#20445;&#22312;&#24212;&#29992;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#26102;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#21253;&#25324;&#35777;&#26126;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#22522;&#20110;&#25439;&#22833;&#30340;&#37319;&#26679;&#30340;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24050;&#30693;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#30028;&#38480;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23548;&#20986;&#25439;&#22833;&#37319;&#26679;&#30340;&#25910;&#25947;&#36895;&#29575;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;&#28857;&#37319;&#26679;&#21644;&#38543;&#26426;Polyak&#27493;&#38271;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20851;&#20110;&#37319;&#26679;&#36807;&#31243;&#30340;&#26465;&#20214;&#65292;&#30830;&#20445;&#35813;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#65292;&#29305;&#21035;&#26159;&#22312;&#20809;&#28369;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13927v2 Announce Type: replace-cross  Abstract: We consider the convergence rates of loss and uncertainty-based active learning algorithms under various assumptions. Firstly, we establish a set of conditions that ensure convergence rates when applied to linear classifiers and linearly separable datasets. This includes demonstrating convergence rate guarantees for loss-based sampling with various loss functions. Secondly, we introduce a framework that allows us to derive convergence rate bounds for loss-based sampling by leveraging known convergence rate bounds for stochastic gradient descent algorithms. Lastly, we propose a new algorithm that combines point sampling and stochastic Polyak's step size. We establish a condition on the sampling process, ensuring a convergence rate guarantee for this algorithm, particularly in the case of smooth convex loss functions. Our numerical results showcase the efficiency of the proposed algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;diff&#21382;&#21490;&#30340;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;&#29615;&#22659;&#20013;&#30340;&#35266;&#27979;&#36716;&#25442;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#20415;&#23545;&#20110;&#38271;&#26399;&#25512;&#29702;&#20915;&#31574;&#30340;&#20219;&#21153;&#20013;&#30340;Neural Language Models&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.07540</link><description>&lt;p&gt;
Neural Language Agents&#30340;diff&#21382;&#21490;
&lt;/p&gt;
&lt;p&gt;
diff History for Neural Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;diff&#21382;&#21490;&#30340;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;&#29615;&#22659;&#20013;&#30340;&#35266;&#27979;&#36716;&#25442;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#20415;&#23545;&#20110;&#38271;&#26399;&#25512;&#29702;&#20915;&#31574;&#30340;&#20219;&#21153;&#20013;&#30340;Neural Language Models&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Language Models (LMs)&#20026;&#36890;&#29992;&#30340;&#20855;&#20307;&#25511;&#21046;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#22522;&#20110;LM&#30340;&#25511;&#21046;&#22120;&#26102;&#65292;&#20250;&#20986;&#29616;&#19968;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#38382;&#39064;&#65306;&#29615;&#22659;&#35266;&#27979;&#24517;&#39035;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#36825;&#19982;&#21382;&#21490;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#20887;&#38271;&#32780;&#20887;&#20313;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#22240;&#27492;&#65292;LM&#20195;&#29702;&#30340;&#20808;&#21069;&#24037;&#20316;&#23616;&#38480;&#20110;&#20855;&#26377;&#23567;&#35266;&#27979;&#22823;&#23567;&#20197;&#21450;&#23545;&#20132;&#20114;&#21382;&#21490;&#25110;&#25351;&#31034;&#35843;&#20248;&#38656;&#27714;&#36739;&#23567;&#30340;&#38480;&#21046;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;diff&#21382;&#21490;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#19988;&#38750;&#24120;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#29992;&#20110;&#25552;&#31034;LM&#31574;&#30053;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#30340;&#36830;&#32493;&#25991;&#26412;&#35266;&#27979;&#19978;&#24212;&#29992;Unix diff&#21629;&#20196;&#65292;&#25105;&#20204;&#26082;&#21487;&#20197;&#25688;&#38500;&#20887;&#20313;&#20449;&#24687;&#65292;&#21448;&#21487;&#20197;&#23558;&#25991;&#26412;&#36755;&#20837;&#30340;&#20869;&#23481;&#38598;&#20013;&#22312;&#29615;&#22659;&#20013;&#26174;&#33879;&#21464;&#21270;&#30340;&#26041;&#38754;&#12290;&#22312;&#38656;&#35201;&#38271;&#26399;&#25512;&#29702;&#36827;&#34892;&#20915;&#31574;&#30340;&#26410;&#35299;&#20915;&#30340;&#35270;&#39057;&#28216;&#25103;NetHack&#20013;&#65292;&#20351;&#29992;diff&#21382;&#21490;&#35843;&#20248;&#30340;LM&#19982;&#29366;&#24577;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07540v2 Announce Type: replace Abstract: Neural Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, results in long and verbose textual prompts. As a result, prior work in LM agents is limited to restricted domains with small observation size as well as minimal needs for interaction history or instruction tuning. In this paper, we introduce diff history, a simple and highly effective solution to these issues. By applying the Unix diff command on consecutive text observations in the interaction histories used to prompt LM policies, we can both abstract away redundant information and focus the content of textual inputs on the salient changes in the environment. On NetHack, an unsolved video game that requires long-horizon reasoning for decision-making, LMs tuned with diff history match state-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#25311;&#25509;&#25910;&#32773;&#34892;&#20026;&#30340;&#36125;&#21494;&#26031;&#21149;&#23548;&#38382;&#39064;&#20013;&#65292;&#21457;&#36865;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#20248;&#28040;&#24687;&#31574;&#30053;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#26597;&#35810;&#31639;&#27861;&#65292;&#20197;&#20248;&#21270;&#20854;&#39044;&#26399;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.18138</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#36827;&#34892;&#31639;&#27861;&#24615;&#21149;&#23548;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Persuasion Through Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18138
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#25509;&#25910;&#32773;&#34892;&#20026;&#30340;&#36125;&#21494;&#26031;&#21149;&#23548;&#38382;&#39064;&#20013;&#65292;&#21457;&#36865;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#20248;&#28040;&#24687;&#31574;&#30053;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#26597;&#35810;&#31639;&#27861;&#65292;&#20197;&#20248;&#21270;&#20854;&#39044;&#26399;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#21149;&#23548;&#38382;&#39064;&#65292;&#20854;&#20013;&#21457;&#36865;&#32773;&#24076;&#26395;&#35828;&#26381;&#25509;&#25910;&#32773;&#37319;&#21462;&#20108;&#20803;&#34892;&#20026;&#65292;&#20363;&#22914;&#36141;&#20080;&#20135;&#21697;&#12290;&#21457;&#36865;&#32773;&#20102;&#35299;&#19990;&#30028;&#30340;&#65288;&#20108;&#20803;&#65289;&#29366;&#24577;&#65292;&#27604;&#22914;&#20135;&#21697;&#36136;&#37327;&#26159;&#39640;&#36824;&#26159;&#20302;&#65292;&#20294;&#26159;&#23545;&#25509;&#25910;&#32773;&#30340;&#20449;&#24565;&#21644;&#25928;&#29992;&#21482;&#26377;&#26377;&#38480;&#30340;&#20449;&#24687;&#12290;&#21463;&#21040;&#23458;&#25143;&#35843;&#26597;&#12289;&#29992;&#25143;&#30740;&#31350;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20801;&#35768;&#21457;&#36865;&#32773;&#36890;&#36807;&#26597;&#35810;&#27169;&#25311;&#25509;&#25910;&#32773;&#30340;&#34892;&#20026;&#26469;&#20102;&#35299;&#26356;&#22810;&#20851;&#20110;&#25509;&#25910;&#32773;&#30340;&#20449;&#24687;&#12290;&#22312;&#22266;&#23450;&#25968;&#37327;&#30340;&#26597;&#35810;&#20043;&#21518;&#65292;&#21457;&#36865;&#32773;&#25215;&#35834;&#19968;&#20010;&#28040;&#24687;&#31574;&#30053;&#65292;&#25509;&#25910;&#32773;&#26681;&#25454;&#25910;&#21040;&#30340;&#28040;&#24687;&#26469;&#26368;&#22823;&#21270;&#22905;&#30340;&#39044;&#26399;&#25928;&#29992;&#26469;&#37319;&#21462;&#34892;&#21160;&#12290;&#25105;&#20204;&#23545;&#21457;&#36865;&#32773;&#22312;&#20219;&#20309;&#25509;&#25910;&#32773;&#31867;&#22411;&#20998;&#24067;&#19979;&#30340;&#26368;&#20248;&#28040;&#24687;&#31574;&#30053;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#26597;&#35810;&#31639;&#27861;&#65292;&#20248;&#21270;&#20102;&#36825;&#20010;&#36125;&#21494;&#26031;&#21149;&#23548;&#28216;&#25103;&#20013;&#21457;&#36865;&#32773;&#30340;&#39044;&#26399;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18138v2 Announce Type: replace-cross Abstract: We study a Bayesian persuasion problem where a sender wants to persuade a receiver to take a binary action, such as purchasing a product. The sender is informed about the (binary) state of the world, such as whether the quality of the product is high or low, but only has limited information about the receiver's beliefs and utilities. Motivated by customer surveys, user studies, and recent advances in generative AI, we allow the sender to learn more about the receiver by querying an oracle that simulates the receiver's behavior. After a fixed number of queries, the sender commits to a messaging policy and the receiver takes the action that maximizes her expected utility given the message she receives. We characterize the sender's optimal messaging policy given any distribution over receiver types. We then design a polynomial-time querying algorithm that optimizes the sender's expected utility in this Bayesian persuasion game. We 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25361;&#25112;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#20808;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.12244</link><description>&lt;p&gt;
&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcement Learning from Partial Observability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25361;&#25112;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#20808;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29366;&#24577;&#20449;&#24687;&#21482;&#33021;&#37096;&#20998;&#35266;&#27979;&#21040;&#65292;&#36825;&#30772;&#22351;&#20102;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#23548;&#33268;&#23558;&#35266;&#27979;&#19982;&#29366;&#24577;&#30456;&#28151;&#28102;&#30340;&#31639;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#32780;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#20801;&#35768;&#22312;&#23398;&#20064;&#12289;&#25506;&#32034;&#21644;&#35268;&#21010;&#20013;&#32771;&#34385;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34920;&#31034;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#21644;&#21487;&#34892;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#23454;&#38469;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#35777;&#26126;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#37096;&#20998;&#35266;&#27979;&#19979;&#33021;&#22815;&#36229;&#36234;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#25512;&#21160;&#20102;&#21487;&#38752;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most real-world reinforcement learning applications, state information is only partially observable, which breaks the Markov decision process assumption and leads to inferior performance for algorithms that conflate observations with state. Partially Observable Markov Decision Processes (POMDPs), on the other hand, provide a general framework that allows for partial observability to be accounted for in learning, exploration and planning, but presents significant computational and statistical challenges. To address these difficulties, we develop a representation-based perspective that leads to a coherent framework and tractable algorithmic approach for practical reinforcement learning from partial observations. We provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm, and also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, advancing reliable reinf
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.02174</link><description>&lt;p&gt;
&#35753;&#24490;&#29615;&#30340;&#35810;&#38382;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#20013;&#30340;&#25671;&#25670;
&lt;/p&gt;
&lt;p&gt;
Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02174
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20250;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24448;&#24448;&#22312;&#20854;&#21028;&#26029;&#19978;&#25671;&#25670;&#19981;&#23450;&#65292;&#21363;&#20351;&#21407;&#22987;&#21028;&#26029;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#31181;&#25671;&#25670;&#23545;&#20110;&#29983;&#25104;&#21487;&#38752;&#22238;&#22797;&#21644;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#20197;&#21450;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#30830;&#35748;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#29992;&#20110;&#38381;&#28304;&#27169;&#22411;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#35757;&#32451;&#30340;&#26694;&#26550;Unwavering-FQ&#65292;&#36890;&#36807;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#20445;&#25345;&#20854;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#22686;&#24378;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#32771;&#34385;&#20102;&#26356;&#19968;&#33324;&#30340;&#32047;&#31215;&#30693;&#35782;&#36807;&#31243;&#23478;&#26063;&#65292;&#25506;&#35752;&#20102;&#22312;&#31038;&#20250;&#30693;&#35782;&#31215;&#32047;&#36807;&#31243;&#20013;&#22914;&#20309;&#30830;&#20445;&#19968;&#23450;&#27604;&#20363;&#30340;&#30693;&#35782;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2309.05638</link><description>&lt;p&gt;
&#38169;&#35823;&#22312;&#32047;&#31215;&#30693;&#35782;&#36807;&#31243;&#20013;&#34987;&#26377;&#25928;&#22320;&#39535;&#26381;&#20102;
&lt;/p&gt;
&lt;p&gt;
Errors are Robustly Tamed in Cumulative Knowledge Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05638
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#32771;&#34385;&#20102;&#26356;&#19968;&#33324;&#30340;&#32047;&#31215;&#30693;&#35782;&#36807;&#31243;&#23478;&#26063;&#65292;&#25506;&#35752;&#20102;&#22312;&#31038;&#20250;&#30693;&#35782;&#31215;&#32047;&#36807;&#31243;&#20013;&#22914;&#20309;&#30830;&#20445;&#19968;&#23450;&#27604;&#20363;&#30340;&#30693;&#35782;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31038;&#20250;&#30693;&#35782;&#31215;&#32047;&#36807;&#31243;&#65292;&#20854;&#20013;&#26032;&#30693;&#35782;&#21333;&#20803;&#30340;&#26377;&#25928;&#24615;&#26082;&#21462;&#20915;&#20110;&#20854;&#25512;&#23548;&#30340;&#27491;&#30830;&#24615;&#65292;&#21448;&#21462;&#20915;&#20110;&#23427;&#25152;&#20381;&#36182;&#30340;&#21333;&#20803;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#31181;&#24773;&#22659;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#65306;&#22914;&#26524;&#26032;&#25512;&#23548;&#30340;&#24658;&#23450;&#27604;&#20363;&#26159;&#38169;&#35823;&#30340;&#65292;&#37027;&#20040;&#25237;&#20837;&#19968;&#20010;&#24658;&#23450;&#27604;&#20363;&#24182;&#36828;&#31163;&#19968;&#30340;&#21162;&#21147;&#26159;&#21542;&#33021;&#30830;&#20445;&#31038;&#20250;&#20013;&#30340;&#30693;&#35782;&#30340;&#19968;&#20010;&#24658;&#23450;&#27604;&#20363;&#26159;&#26377;&#25928;&#30340;&#65311;Ben-Eliezer, Mikulincer, Mossel&#21644;Sudan (ITCS 2023)&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#27010;&#29575;&#27169;&#22411;&#26469;&#20998;&#26512;&#36825;&#31867;&#38382;&#39064;&#65292;&#24182;&#23545;&#36825;&#20010;&#38382;&#39064;&#20316;&#20986;&#20102;&#32943;&#23450;&#30340;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#19968;&#20010;&#31616;&#21333;&#24773;&#20917;&#65292;&#21363;&#27599;&#20010;&#26032;&#21333;&#20803;&#21482;&#20381;&#36182;&#20110;&#19968;&#20010;&#29616;&#26377;&#21333;&#20803;&#65292;&#24182;&#19988;&#21333;&#20803;&#26681;&#25454;$\textit{&#20248;&#20808;&#38468;&#21152;&#35268;&#21017;}$&#38468;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26356;&#19968;&#33324;&#30340;&#32047;&#31215;&#30693;&#35782;&#36807;&#31243;&#23478;&#26063;&#65292;&#20854;&#20013;&#26032;&#21333;&#20803;&#21487;&#33021;&#26681;&#25454;&#19981;&#21516;&#30340;&#38468;&#21152;&#26426;&#21046;&#36827;&#34892;&#38468;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05638v2 Announce Type: replace  Abstract: We study processes of societal knowledge accumulation, where the validity of a new unit of knowledge depends both on the correctness of its derivation and on the validity of the units it depends on. A fundamental question in this setting is: If a constant fraction of the new derivations is wrong, can investing a constant fraction, bounded away from one, of effort ensure that a constant fraction of knowledge in society is valid? Ben-Eliezer, Mikulincer, Mossel, and Sudan (ITCS 2023) introduced a concrete probabilistic model to analyze such questions and showed an affirmative answer to this question. Their study, however, focuses on the simple case where each new unit depends on just one existing unit, and units attach according to a $\textit{preferential attachment rule}$.   In this work, we consider much more general families of cumulative knowledge processes, where new units may attach according to varied attachment mechanisms and d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#25197;&#26354;&#20989;&#25968;&#23545;Mixup&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23454;&#29616;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#30456;&#20284;&#25968;&#25454;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01434</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#25197;&#26354;&#20989;&#25968;&#23450;&#21046;Mixup&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Tailoring Mixup to Data using Kernel Warping functions. (arXiv:2311.01434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#25197;&#26354;&#20989;&#25968;&#23545;Mixup&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23454;&#29616;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#30456;&#20284;&#25968;&#25454;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#23398;&#20064;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#22522;&#30784;&#12290;&#22312;&#25152;&#26377;&#25552;&#20986;&#30340;&#22686;&#24378;&#25216;&#26415;&#20013;&#65292;&#32447;&#24615;&#25554;&#20540;&#35757;&#32451;&#25968;&#25454;&#28857;&#65288;&#20063;&#31216;&#20026;Mixup&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#28857;&#36827;&#34892;&#28151;&#21512;&#65292;&#25110;&#32773;&#24212;&#29992;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#25554;&#20540;&#65292;&#32780;&#25105;&#20204;&#21017;&#23545;&#26356;&#30456;&#20284;&#30340;&#28857;&#36827;&#34892;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#24863;&#20852;&#36259;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#25197;&#26354;&#20989;&#25968;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#21462;&#20915;&#20110;&#35201;&#32452;&#21512;&#30340;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#39640;&#25928;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20197;&#36991;&#20813;&#22810;&#26679;&#24615;&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26082;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21448;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/ENSTA-U2IS/torch-uncertainty&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is an essential building block for learning efficient deep learning models. Among all augmentation techniques proposed so far, linear interpolation of training data points, also called mixup, has found to be effective for a large panel of applications. While the majority of works have focused on selecting the right points to mix, or applying complex non-linear interpolation, we are interested in mixing similar points more frequently and strongly than less similar ones. To this end, we propose to dynamically change the underlying distribution of interpolation coefficients through warping functions, depending on the similarity between data points to combine. We define an efficient and flexible framework to do so without losing in diversity. We provide extensive experiments for classification and regression tasks, showing that our proposed method improves both performance and calibration of models. Code available in https://github.com/ENSTA-U2IS/torch-uncertainty
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21253;&#25324;&#25512;&#23548;&#20102;&#25928;&#26524;&#21644;&#25104;&#21151;&#29575;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13786</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limits of Membership Inference Attacks on Machine Learning Models. (arXiv:2310.13786v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21253;&#25324;&#25512;&#23548;&#20102;&#25928;&#26524;&#21644;&#25104;&#21151;&#29575;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#21487;&#20197;&#25581;&#31034;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#26159;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#65292;&#21487;&#33021;&#26292;&#38706;&#20010;&#20154;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;MIA&#30340;&#22522;&#26412;&#32479;&#35745;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;&#32479;&#35745;&#37327;&#65292;&#35813;&#32479;&#35745;&#37327;&#20915;&#23450;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#21644;&#25104;&#21151;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#24773;&#20917;&#65292;&#24182;&#23545;&#36825;&#20010;&#24863;&#20852;&#36259;&#30340;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#30028;&#38480;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#38598;&#20013;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article explores the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. Then, we investigate several situations for which we provide bounds on this quantity of interest. This allows us to infer the accuracy of potential attacks as a function of the number of samples and other structural parameters of learning models, which in some cases can be directly estimated from the dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#36801;&#31227;&#23398;&#20064;&#21551;&#21457;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#35774;&#22791;&#19978;&#30340;&#36164;&#28304;&#21033;&#29992;&#12289;&#20197;&#21450;&#20943;&#23569;&#26381;&#21153;&#22120;&#21644;&#32593;&#32476;&#30340;&#36127;&#36733;&#65292;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#36793;&#32536;&#33410;&#28857;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10367</link><description>&lt;p&gt;
&#36793;&#32536;&#33410;&#28857;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Toward efficient resource utilization at edge nodes in federated learning. (arXiv:2309.10367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#36801;&#31227;&#23398;&#20064;&#21551;&#21457;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#35774;&#22791;&#19978;&#30340;&#36164;&#28304;&#21033;&#29992;&#12289;&#20197;&#21450;&#20943;&#23569;&#26381;&#21153;&#22120;&#21644;&#32593;&#32476;&#30340;&#36127;&#36733;&#65292;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#36793;&#32536;&#33410;&#28857;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#36793;&#32536;&#33410;&#28857;&#33021;&#22815;&#20849;&#21516;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#20182;&#20204;&#30340;&#25968;&#25454;&#12290;&#36825;&#26159;&#36890;&#36807;&#35774;&#22791;&#35745;&#31639;&#26412;&#22320;&#31169;&#26377;&#27169;&#22411;&#26356;&#26032;&#65292;&#28982;&#21518;&#30001;&#26381;&#21153;&#22120;&#36827;&#34892;&#32858;&#21512;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#21644;&#32593;&#32476;&#36890;&#20449;&#23545;&#20110;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#36739;&#22823;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#25104;&#20026;&#20005;&#37325;&#29942;&#39048;&#12290;&#36793;&#32536;&#33410;&#28857;&#24448;&#24448;&#20855;&#26377;&#26377;&#38480;&#30340;&#30828;&#20214;&#36164;&#28304;&#65288;RAM&#12289;CPU&#65289;&#65292;&#32780;&#36793;&#32536;&#30340;&#32593;&#32476;&#24102;&#23485;&#21644;&#21487;&#38752;&#24615;&#23545;&#20110;&#25193;&#23637;&#32852;&#37030;&#36710;&#38431;&#24212;&#29992;&#26469;&#35828;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#21463;&#36801;&#31227;&#23398;&#20064;&#21551;&#21457;&#30340;FL&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#35774;&#22791;&#19978;&#30340;&#36164;&#28304;&#21033;&#29992;&#65292;&#20197;&#21450;&#27599;&#20010;&#20840;&#23616;&#35757;&#32451;&#36718;&#27425;&#20013;&#26381;&#21153;&#22120;&#21644;&#32593;&#32476;&#30340;&#36127;&#36733;&#12290;&#23545;&#20110;&#27599;&#20010;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#35201;&#35757;&#32451;&#30340;&#23618;&#65292;&#20923;&#32467;&#27169;&#22411;&#30340;&#20854;&#20313;&#37096;&#20998;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25490;&#38500;&#25152;&#26377;&#26410;&#35757;&#32451;&#30340;&#37096;&#20998;&#26469;&#20943;&#23569;&#27599;&#36718;&#30340;&#26381;&#21153;&#22120;&#36127;&#36733;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables edge nodes to collaboratively contribute to constructing a global model without sharing their data. This is accomplished by devices computing local, private model updates that are then aggregated by a server. However, computational resource constraints and network communication can become a severe bottleneck for larger model sizes typical for deep learning applications. Edge nodes tend to have limited hardware resources (RAM, CPU), and the network bandwidth and reliability at the edge is a concern for scaling federated fleet applications. In this paper, we propose and evaluate a FL strategy inspired by transfer learning in order to reduce resource utilization on devices, as well as the load on the server and network in each global training round. For each local model update, we randomly select layers to train, freezing the remaining part of the model. In doing so, we can reduce both server load and communication costs per round by excluding all untrained
&lt;/p&gt;</description></item><item><title>LLaMA-E&#26159;&#19968;&#31181;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#12290;</title><link>http://arxiv.org/abs/2308.04913</link><description>&lt;p&gt;
LLaMA-E&#65306;&#22810;&#26041;&#38754;&#25351;&#23548;&#19979;&#30340;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#22686;&#24378;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following. (arXiv:2308.04913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04913
&lt;/p&gt;
&lt;p&gt;
LLaMA-E&#26159;&#19968;&#31181;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#28041;&#21450;&#21019;&#24314;&#21560;&#24341;&#20154;&#12289;&#20016;&#23500;&#19988;&#26377;&#38024;&#23545;&#24615;&#30340;&#20419;&#38144;&#20869;&#23481;&#65292;&#20197;&#25512;&#21160;&#20135;&#21697;&#38144;&#21806;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33539;&#20363;&#65292;&#20026;&#35299;&#20915;&#36825;&#31181;&#24773;&#26223;&#20013;&#30340;&#21508;&#31181;&#21019;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36890;&#29992;&#35821;&#26009;&#24211;&#21644;&#24120;&#35782;&#30693;&#35782;&#35757;&#32451;&#30340;&#20027;&#27969;LLM&#22312;&#36866;&#24212;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#21644;&#23458;&#25143;&#29420;&#29305;&#30340;&#22797;&#26434;&#21644;&#20010;&#24615;&#21270;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#20687;GPT-3.5&#36825;&#26679;&#30340;LLM&#38656;&#35201;&#36827;&#34892;&#36828;&#31243;&#35775;&#38382;&#65292;&#24341;&#21457;&#20102;&#22312;&#20256;&#36755;&#36807;&#31243;&#20013;&#20445;&#25252;&#22823;&#37327;&#23458;&#25143;&#38544;&#31169;&#25968;&#25454;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-E&#65292;&#38024;&#23545;&#22810;&#26679;&#21270;&#30340;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#20219;&#21153;&#30340;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39046;&#22495;&#19987;&#23478;&#20174;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#21019;&#24314;&#20102;&#31181;&#23376;&#25351;&#23548;&#38598;&#21512;&#12290;&#36825;&#20123;&#20219;&#21153;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
E-commerce authoring involves creating attractive, abundant, and targeted promotional content to drive product sales. The emergence of large language models (LLMs) introduces an innovative paradigm, offering a unified solution to address various authoring tasks within this scenario. However, mainstream LLMs trained on general corpora with common sense knowledge reveal limitations in fitting complex and personalized features unique to e-commerce products and customers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility, raising concerns about safeguarding voluminous customer privacy data during transmission. This paper proposes the LLaMA-E, the unified and customized instruction-following language models focusing on diverse e-commerce authoring tasks. Specifically, the domain experts create the seed instruction set from the tasks of ads generation, query-enhanced product title rewriting, product classification, purchase intent speculation, and general Q&amp;A. These tasks enabl
&lt;/p&gt;</description></item><item><title>S-HR-VQVAE&#26159;&#19968;&#31181;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#21644;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#22312;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06701</link><description>&lt;p&gt;
S-HR-VQVAE: &#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction. (arXiv:2307.06701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06701
&lt;/p&gt;
&lt;p&gt;
S-HR-VQVAE&#26159;&#19968;&#31181;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#21644;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#22312;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#23558;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30456;&#32467;&#21512;&#65292;&#29992;&#26469;&#35299;&#20915;&#35270;&#39057;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;S-HR-VQVAE&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;HR-VQVAE&#22312;&#23545;&#38745;&#27490;&#22270;&#20687;&#36827;&#34892;&#24314;&#27169;&#26102;&#30340;&#20869;&#22312;&#33021;&#21147;&#21644;&#32039;&#20945;&#34920;&#31034;&#65292;&#20197;&#21450;ST-PixelCNN&#22788;&#29702;&#26102;&#31354;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292; S-HR-VQVAE&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#23545;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#23398;&#20064;&#26102;&#31354;&#20449;&#24687;&#12289;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#28040;&#38500;&#27169;&#31946;&#39044;&#27979;&#21644;&#38544;&#24335;&#24314;&#27169;&#29289;&#29702;&#29305;&#24615;&#12290;&#23545;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#38754;&#19982;&#39030;&#32423;&#35270;&#39057;&#39044;&#27979;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the video prediction task by putting forth a novel model that combines (i) our recently proposed hierarchical residual vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN (ST-PixelCNN). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging the intrinsic capabilities of HR-VQVAE at modeling still images with a parsimonious representation, combined with the ST-PixelCNN's ability at handling spatiotemporal information, S-HR-VQVAE can better deal with chief challenges in video prediction. These include learning spatiotemporal information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical characteristics. Extensive experimental results on the KTH Human Action and Moving-MNIST tasks demonstrate that our model compares favorably against top video prediction techniques both in quantitative and qualitative evalu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.16533</link><description>&lt;p&gt;
ICSVR: &#22312;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30740;&#31350;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ICSVR: Investigating Compositional and Semantic Understanding in Video Retrieval Models. (arXiv:2306.16533v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16533
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#26816;&#32034;&#65288;VR&#65289;&#28041;&#21450;&#26681;&#25454;&#25991;&#26412;&#26631;&#39064;&#26816;&#32034;&#35270;&#39057;&#25968;&#25454;&#24211;&#20013;&#30340;&#30495;&#23454;&#35270;&#39057;&#65292;&#25110;&#21453;&#20043;&#20134;&#28982;&#12290;&#21512;&#25104;&#24615;&#30340;&#20004;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#23545;&#35937;&#21644;&#23646;&#24615;&#20197;&#21450;&#21160;&#20316;&#65292;&#20351;&#29992;&#27491;&#30830;&#30340;&#35821;&#20041;&#32852;&#32467;&#20197;&#24418;&#25104;&#27491;&#30830;&#30340;&#25991;&#26412;&#26597;&#35810;&#12290;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#65288;&#23545;&#35937;&#21644;&#23646;&#24615;&#12289;&#21160;&#20316;&#21644;&#35821;&#20041;&#65289;&#21508;&#33258;&#22312;&#24110;&#21161;&#21306;&#20998;&#35270;&#39057;&#21644;&#26816;&#32034;&#27491;&#30830;&#30340;&#30495;&#23454;&#35270;&#39057;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#22914;MSRVTT&#12289;MSVD&#21644;DIDEMO&#12290;&#35813;&#30740;&#31350;&#38024;&#23545;&#20004;&#31867;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#20102;&#65292;&#19968;&#31867;&#26159;&#22312;&#35270;&#39057;&#25991;&#26412;&#23545;&#19978;&#39044;&#35757;&#32451;&#24182;&#22312;&#19979;&#28216;&#35270;&#39057;&#26816;&#32034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#65288;&#20363;&#22914;&#65292;Frozen-in-Time&#12289;Violet&#12289;MCQ&#31561;&#65289;&#65292;&#21478;&#19968;&#31867;&#26159;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25991;&#26412;&#34920;&#31034;&#65288;&#22914;CLIP&#65289;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video retrieval (VR) involves retrieving the ground truth video from the video database given a text caption or vice-versa. The two important components of compositionality: objects \&amp; attributes and actions are joined using correct semantics to form a proper text query. These components (objects \&amp; attributes, actions and semantics) each play an important role to help distinguish among videos and retrieve the correct ground truth video. However, it is unclear what is the effect of these components on the video retrieval performance. We therefore, conduct a systematic study to evaluate the compositional and semantic understanding of video retrieval models on standard benchmarks such as MSRVTT, MSVD and DIDEMO. The study is performed on two categories of video retrieval models: (i) which are pre-trained on video-text pairs and fine-tuned on downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.) (ii) which adapt pre-trained image-text representations like CLIP for vid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22788;&#29702;&#20013;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;&#26694;&#26550;&#65288;FFB&#65289;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#35813;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#25552;&#20379;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#26497;&#31616;&#21644;&#38754;&#21521;&#30740;&#31350;&#30340;&#24320;&#28304;&#20195;&#30721;&#65307;&#24314;&#31435;&#32479;&#19968;&#30340;&#20844;&#24179;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#27969;&#27700;&#32447;&#65307;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174; $\mathbf{45,079}$ &#20010;&#23454;&#39564;&#20013;&#33719;&#21462;&#20851;&#38190;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.09468</link><description>&lt;p&gt;
FFB:&#38754;&#21521;&#22788;&#29702;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods. (arXiv:2306.09468v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22788;&#29702;&#20013;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;&#26694;&#26550;&#65288;FFB&#65289;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#35813;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#25552;&#20379;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#26497;&#31616;&#21644;&#38754;&#21521;&#30740;&#31350;&#30340;&#24320;&#28304;&#20195;&#30721;&#65307;&#24314;&#31435;&#32479;&#19968;&#30340;&#20844;&#24179;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#27969;&#27700;&#32447;&#65307;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174; $\mathbf{45,079}$ &#20010;&#23454;&#39564;&#20013;&#33719;&#21462;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;&#65288;FFB&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#22788;&#29702;&#20013;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#22522;&#20934;&#26694;&#26550;&#12290;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#30340;&#20844;&#24179;&#24615;&#23545;&#20110;&#31526;&#21512;&#36947;&#24503;&#21644;&#27861;&#24459;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#39564;&#35774;&#32622;&#30340;&#19981;&#19968;&#33268;&#65292;&#32570;&#20047;&#26131;&#20110;&#35775;&#38382;&#30340;&#31639;&#27861;&#23454;&#29616;&#20197;&#21450;&#24403;&#21069;&#20844;&#24179;&#24230;&#37327;&#24037;&#20855;&#30340;&#26377;&#38480;&#21487;&#25193;&#23637;&#24615;&#65292;&#23384;&#22312;&#27604;&#36739;&#21644;&#24320;&#21457;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#12289;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22788;&#29702;&#20013;&#30340;&#32452;&#20844;&#24179;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#30830;&#20445;&#19981;&#21516;&#27665;&#26063;/&#31181;&#26063;&#32676;&#20307;&#20844;&#24179;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#35813;&#24037;&#20316;&#25552;&#20379;&#20102;&#20197;&#19979;&#20851;&#38190;&#36129;&#29486;&#65306;&#25552;&#20379;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#26497;&#31616;&#21644;&#38754;&#21521;&#30740;&#31350;&#30340;&#24320;&#28304;&#20195;&#30721;&#65307;&#24314;&#31435;&#32479;&#19968;&#30340;&#20844;&#24179;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#27969;&#27700;&#32447;&#65307;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174; $\mathbf{45,079}$ &#20010;&#23454;&#39564;&#20013;&#33719;&#21462;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Fair Fairness Benchmark (\textsf{FFB}), a benchmarking framework for in-processing group fairness methods. Ensuring fairness in machine learning is critical for ethical and legal compliance. However, there exist challenges in comparing and developing of fairness methods due to inconsistencies in experimental settings, lack of accessible algorithmic implementations, and limited extensibility of current fairness packages and tools. To address these issues, we introduce an open-source, standardized benchmark for evaluating in-processing group fairness methods and provide a comprehensive analysis of state-of-the-art methods to ensure different notions of group fairness. This work offers the following key contributions: the provision of flexible, extensible, minimalistic, and research-oriented open-source code; the establishment of unified fairness method benchmarking pipelines; and extensive benchmarking, which yields key insights from $\mathbf{45,079}$ experiment
&lt;/p&gt;</description></item><item><title>ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.06446</link><description>&lt;p&gt;
ShiftAddViT&#65306;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#28151;&#21512;&#23454;&#29616;&#39640;&#25928;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06446
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#30340;&#32479;&#19968;&#39592;&#24178;&#12290;&#20294;&#26159;&#65292;ViTs&#20013;&#30340;&#27880;&#24847;&#21147;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#30001;&#20110;&#23494;&#38598;&#30340;&#20056;&#27861;&#32780;&#19981;&#22815;&#39640;&#25928;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;ViT&#20197;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#65288;&#20363;&#22914;&#20301;&#31227;&#21644;&#21152;&#27861;&#65289;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20840;&#26032;&#31867;&#22411;&#30340;&#20943;&#23569;&#20056;&#27861;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;ShiftAddViT&#65292;&#26088;&#22312;&#23454;&#29616;GPU&#19978;&#30340;&#31471;&#21040;&#31471;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#26597;&#35810;&#21644;&#38190;&#26144;&#23556;&#20026;&#27721;&#26126;&#31354;&#38388;&#20013;&#30340;&#20108;&#36827;&#21046;&#30721;&#20043;&#21518;&#65292;&#37319;&#29992;&#21152;&#27861;&#26680;&#23545;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#20043;&#38388;&#30340;MatMul&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#21097;&#20313;&#30340;MLPs&#25110;&#32447;&#24615;&#23618;&#21017;&#37319;&#29992;&#20301;&#31227;&#26680;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;TVM&#22312;GPU&#19978;&#23454;&#26045;&#24182;&#20248;&#21270;&#36825;&#20123;&#23450;&#21046;&#26680;&#65292;&#20197;&#23454;&#29616;&#23454;&#38469;&#30828;&#20214;&#37096;&#32626;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. But both attention and multi-layer perceptions (MLPs) in ViTs are not efficient enough due to dense multiplications, resulting in costly training and inference. To this end, we propose to reparameterize the pre-trained ViT with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\textbf{ShiftAddViT}$, which aims for end-to-end inference speedups on GPUs without the need of training from scratch. Specifically, all $\texttt{MatMuls}$ among queries, keys, and values are reparameterized by additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized by shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01713</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#35299;&#37322;&#30340;&#38750;&#20132;&#20114;&#35821;&#20041;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks. (arXiv:2305.01713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32454;&#21270;&#36830;&#32493;&#31354;&#38388;&#30340;&#21477;&#23376;&#34920;&#24449;&#19978;&#36827;&#34892;&#35299;&#32806;&#21487;&#20197;&#22312;&#23450;&#20301;&#26126;&#30830;&#21457;&#29983;&#30340;&#29983;&#25104;&#22240;&#32032;&#30340;&#21516;&#26102;&#65292;&#25913;&#36827;&#21487;&#35299;&#37322;&#24615;&#21644;&#35821;&#20041;&#25511;&#21046;&#65292;&#36825;&#20026;&#22522;&#20110;&#31070;&#32463;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#20102;&#19968;&#20123;&#31526;&#21495;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#28789;&#27963;&#24615;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#26469;&#35299;&#38500;&#32534;&#30721;&#30340;&#38544;&#34255;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;INN&#33021;&#22815;&#23558;&#20998;&#24067;&#24335;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#22909;&#30340;&#35821;&#20041;&#19978;&#35299;&#32806;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangling sentence representations over continuous spaces can be a critical process in improving interpretability and semantic control by localising explicit generative factors. Such process confers to neural-based language models some of the advantages that are characteristic of symbolic models, while keeping their flexibility. This work presents a methodology for disentangling the hidden space of a BERT-GPT2 autoencoder by transforming it into a more separable semantic space with the support of a flow-based invertible neural network (INN). Experimental results indicate that the INN can transform the distributed hidden space into a better semantically disentangled latent space, resulting in better interpretability and controllability, when compared to recent state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#32593;&#32476;&#30340;&#34920;&#29616;</title><link>http://arxiv.org/abs/2304.10553</link><description>&lt;p&gt;
&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Sparsity in neural networks can improve their privacy. (arXiv:2304.10553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10553
&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#32593;&#32476;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31232;&#30095;&#24615;&#22914;&#20309;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#32593;&#32476;&#30340;&#38544;&#31169;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22312;&#20219;&#21153;&#19978;&#30340;&#30456;&#20284;&#34920;&#29616;&#12290;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#23436;&#21892;&#21644;&#25193;&#23637;&#20102;&#29616;&#26377;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article measures how sparsity can make neural networks more robust to membership inference attacks. The obtained empirical results show that sparsity improves the privacy of the network, while preserving comparable performances on the task at hand. This empirical study completes and extends existing literature.
&lt;/p&gt;</description></item><item><title>FairShap&#26159;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#37325;&#20272;&#35745;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;Shapley&#20540;&#20272;&#20540;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#31934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.01928</link><description>&lt;p&gt;
&#22522;&#20110;Shapley&#20540;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#25968;&#25454;&#20877;&#21152;&#26435;&#26041;&#27861;FairShap
&lt;/p&gt;
&lt;p&gt;
FairShap: A Data Re-weighting Approach for Algorithmic Fairness based on Shapley Values. (arXiv:2303.01928v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01928
&lt;/p&gt;
&lt;p&gt;
FairShap&#26159;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#37325;&#20272;&#35745;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;Shapley&#20540;&#20272;&#20540;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#31934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#26159;&#26497;&#20854;&#37325;&#35201;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#28982;&#32780;&#24403;&#21069;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#35201;&#27714;&#20351;&#29992;&#36890;&#24120;&#23384;&#22312;&#20559;&#24046;&#30340;&#28023;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19987;&#27880;&#20110;&#24314;&#27169;&#21644;&#32416;&#27491;&#25968;&#25454;&#20559;&#24046;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#25104;&#20026;&#26377;&#20215;&#20540;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Shapley&#20540;&#36827;&#34892;&#25968;&#25454;&#20272;&#20540;&#30340;&#39044;&#22788;&#29702;&#65288;&#20877;&#21152;&#26435;&#65289;&#26041;&#27861;FairShap&#65292;&#29992;&#20110;&#20844;&#24179;&#30340;&#31639;&#27861;&#20915;&#31574;&#21046;&#23450;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#19988;&#26131;&#20110;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#34913;&#37327;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#23545;&#39044;&#23450;&#20041;&#30340;&#20844;&#24179;&#25351;&#26631;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#21516;&#30340;&#24615;&#36136;&#65292;&#26377;&#21508;&#31181;&#22521;&#35757;&#22330;&#26223;&#21644;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20135;&#29983;&#26356;&#20844;&#24179;&#30340;&#27169;&#22411;&#24182;&#19988;&#20934;&#30830;&#24230;&#26356;&#39640;&#25110;&#30456;&#20284;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#30452;&#26041;&#22270;&#21644;&#28508;&#31354;&#38388;&#21487;&#35270;&#21270;&#26469;&#35828;&#26126;FairShap&#30340;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#23545;&#20110;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#30830;&#20445;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic fairness is of utmost societal importance, yet the current trend in large-scale machine learning models requires training with massive datasets that are typically biased. In this context, pre-processing methods that focus on modeling and correcting bias in the data emerge as valuable approaches. In this paper, we propose FairShap, a novel pre-processing (re-weighting) method for fair algorithmic decision-making through data valuation by means of Shapley Values. Our approach is model agnostic and easily interpretable, as it measures the contribution of each training data point to a predefined fairness metric. We empirically validate FairShap on several state-of-the-art datasets of different nature, with a variety of training scenarios and models and show how it outperforms other methods, yielding fairer models with higher or similar levels of accuracy. We also illustrate FairShap's interpretability by means of histograms and latent space visualizations. We believe that this 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#20869;&#22312;&#21160;&#26426;&#31639;&#27861;&#31867;&#21035;SND&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25506;&#32034;&#22256;&#38590;&#29615;&#22659;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2302.11563</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#21033;&#29992;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploration by self-supervised exploitation. (arXiv:2302.11563v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#20869;&#22312;&#21160;&#26426;&#31639;&#27861;&#31867;&#21035;SND&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25506;&#32034;&#22256;&#38590;&#29615;&#22659;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#26681;&#25454;&#39044;&#20808;&#35774;&#35745;&#30340;&#22870;&#21169;&#20989;&#25968;&#22312;&#29615;&#22659;&#20013;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#22870;&#21169;&#36807;&#20110;&#31232;&#30095;&#65292;&#20195;&#29702;&#22312;&#29615;&#22659;&#25506;&#32034;&#20013;&#19981;&#20250;&#36935;&#21040;&#22870;&#21169;&#65292;&#36825;&#31181;&#26041;&#27861;&#23601;&#21464;&#24471;&#38750;&#24120;&#26840;&#25163;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#21487;&#33021;&#26159;&#20026;&#20195;&#29702;&#35013;&#22791;&#20869;&#22312;&#21160;&#26426;&#65292;&#36825;&#26679;&#20195;&#29702;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#20063;&#21487;&#33021;&#36935;&#21040;&#22806;&#37096;&#22870;&#21169;&#12290;&#26032;&#39062;&#24615;&#26816;&#27979;&#26159;&#20869;&#22312;&#21160;&#26426;&#30740;&#31350;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#20998;&#25903;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#35823;&#24046;&#20316;&#20026;&#26032;&#39062;&#24615;&#25351;&#26631;&#30340;&#33258;&#25105;&#30417;&#30563;&#32593;&#32476;&#33976;&#39311;&#65288;SND&#65289;&#31639;&#27861;&#31867;&#21035;&#65292;&#20854;&#20013;&#30446;&#26631;&#27169;&#22411;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#20026;&#27492;&#25913;&#32534;&#20102;&#19977;&#31181;&#29616;&#26377;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#34987;&#35748;&#20026;&#38590;&#20197;&#25506;&#32034;&#30340;&#21313;&#20010;&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning can solve decision-making problems and train an agent to behave in an environment according to a predesigned reward function. However, such an approach becomes very problematic if the reward is too sparse and the agent does not come across the reward during the environmental exploration. The solution to such a problem may be in equipping the agent with an intrinsic motivation, which will provide informed exploration, during which the agent is likely to also encounter external reward. Novelty detection is one of the promising branches of intrinsic motivation research. We present Self-supervised Network Distillation (SND), a class of internal motivation algorithms based on the distillation error as a novelty indicator, where the target model is trained using self-supervised learning. We adapted three existing self-supervised methods for this purpose and experimentally tested them on a set of ten environments that are considered difficult to explore. The results sho
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#22270;&#24418;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#21450;&#20854;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#12289;&#32479;&#19968;&#30340;&#31526;&#21495;&#34920;&#31034;&#12289;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23545;&#21313;&#22235;&#31181;&#26041;&#27861;&#12289;&#20108;&#21313;&#20108;&#20010;&#25968;&#25454;&#38598;&#21644;&#21313;&#20061;&#20010;&#25351;&#26631;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#25972;&#21512;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#24320;&#25918;&#30340;&#25361;&#25112;&#19978;&#12290;</title><link>http://arxiv.org/abs/2210.12089</link><description>&lt;p&gt;
&#22270;&#24418;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#32508;&#36848;: &#23450;&#20041;, &#26041;&#27861;, &#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation. (arXiv:2210.12089v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12089
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#22270;&#24418;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#21450;&#20854;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#12289;&#32479;&#19968;&#30340;&#31526;&#21495;&#34920;&#31034;&#12289;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23545;&#21313;&#22235;&#31181;&#26041;&#27861;&#12289;&#20108;&#21313;&#20108;&#20010;&#25968;&#25454;&#38598;&#21644;&#21313;&#20061;&#20010;&#25351;&#26631;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#25972;&#21512;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#24320;&#25918;&#30340;&#25361;&#25112;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#22312;&#31038;&#21306;&#26816;&#27979;&#21644;&#20998;&#23376;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322; (CE) &#25552;&#20379;&#21453;&#20363;&#26469;&#20811;&#26381;&#40657;&#30418;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#38480;&#21046;&#12290;&#30001;&#20110;&#23545;&#22270;&#23398;&#20064;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#38271;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#20851;&#27880; GNNs &#30340; CE &#27010;&#24565;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#38750;&#24120;&#35268;&#30340;&#25163;&#27573;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#65292;&#32479;&#19968;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#20197;&#21450;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21313;&#22235;&#31181;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#20108;&#21313;&#20108;&#20010;&#25968;&#25454;&#38598;&#21644;&#21313;&#20061;&#20010;&#25351;&#26631;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#22823;&#22810;&#25968;&#26041;&#27861;&#21040; GRETEL &#24211;&#20013;&#65292;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) perform well in community detection and molecule classification. Counterfactual Explanations (CE) provide counter-examples to overcome the transparency limitations of black-box models. Due to the growing attention in graph learning, we focus on the concepts of CE for GNNs. We analysed the SoA to provide a taxonomy, a uniform notation, and the benchmarking datasets and evaluation metrics. We discuss fourteen methods, their evaluation protocols, twenty-two datasets, and nineteen metrics. We integrated the majority of methods into the GRETEL library to conduct an empirical evaluation to understand their strengths and pitfalls. We highlight open challenges and future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#24320;&#25918;&#24335;&#22810;&#27169;&#24577;&#20851;&#31995;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#20132;&#20114;&#22238;&#31572;&#35270;&#39057;&#22330;&#26223;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2012.00822</link><description>&lt;p&gt;
&#24320;&#25918;&#24335;&#22810;&#27169;&#24577;&#20851;&#31995;&#25512;&#29702;&#29992;&#20110;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Open-Ended Multi-Modal Relational Reasoning for Video Question Answering. (arXiv:2012.00822v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.00822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#24320;&#25918;&#24335;&#22810;&#27169;&#24577;&#20851;&#31995;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#20132;&#20114;&#22238;&#31572;&#35270;&#39057;&#22330;&#26223;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#29992;&#20110;&#20998;&#26512;&#22806;&#37096;&#29615;&#22659;&#24182;&#22238;&#31572;&#21442;&#19982;&#32773;&#30340;&#38382;&#39064;&#12290;&#35813;&#20195;&#29702;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#22312;&#22522;&#20110;&#35270;&#39057;&#22330;&#26223;&#30340;&#35821;&#35328;&#20132;&#20114;&#20013;&#21327;&#21161;&#20010;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#35270;&#39057;&#35782;&#21035;&#25216;&#26415;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25972;&#21512;&#21040;&#26426;&#22120;&#20154;&#20195;&#29702;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#21442;&#19982;&#32773;&#21644;&#26426;&#22120;&#20154;&#20195;&#29702;&#20043;&#38388;&#20986;&#29616;&#30340;&#30456;&#20851;&#38382;&#39064;&#65292;&#25506;&#35752;&#24433;&#21709;&#20154;&#26426;&#20132;&#20114;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#26041;&#27861;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20449;&#20219;&#21644;&#20132;&#20114;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#31215;&#26497;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;2%&#33267;3%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a robotic agent specifically designed to analyze external environments and address participants' questions. The primary focus of this agent is to assist individuals using language-based interactions within video-based scenes. Our proposed method integrates video recognition technology and natural language processing models within the robotic agent. We investigate the crucial factors affecting human-robot interactions by examining pertinent issues arising between participants and robot agents. Methodologically, our experimental findings reveal a positive relationship between trust and interaction efficiency. Furthermore, our model demonstrates a 2\% to 3\% performance enhancement in comparison to other benchmark methods.
&lt;/p&gt;</description></item></channel></rss>