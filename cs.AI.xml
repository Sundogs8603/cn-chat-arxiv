<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#31867;&#26080;&#27861;&#36776;&#21035;AI&#29983;&#25104;&#30340;&#20551;&#29031;&#29255;&#21644;&#30495;&#23454;&#29031;&#29255;&#65292;&#36825;&#19968;&#28857;&#21463;&#20010;&#20154;&#32972;&#26223;&#30340;&#24433;&#21709;&#24182;&#19981;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2304.13023</link><description>&lt;p&gt;
&#30524;&#35265;&#19981;&#19968;&#23450;&#20026;&#23454;&#65306;&#20154;&#31867;&#24863;&#30693;AI&#29983;&#25104;&#22270;&#20687;&#30340;&#23450;&#37327;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Seeing is not always believing: A Quantitative Study on Human Perception of AI-Generated Images. (arXiv:2304.13023v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#31867;&#26080;&#27861;&#36776;&#21035;AI&#29983;&#25104;&#30340;&#20551;&#29031;&#29255;&#21644;&#30495;&#23454;&#29031;&#29255;&#65292;&#36825;&#19968;&#28857;&#21463;&#20010;&#20154;&#32972;&#26223;&#30340;&#24433;&#21709;&#24182;&#19981;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29031;&#29255;&#26159;&#20154;&#20204;&#35760;&#24405;&#26085;&#24120;&#29983;&#27963;&#32463;&#21382;&#30340;&#19968;&#31181;&#26041;&#24335;&#65292;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#21487;&#20449;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#25285;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#30340;&#36827;&#27493;&#21487;&#33021;&#20135;&#29983;&#20266;&#36896;&#30340;&#29031;&#29255;&#65292;&#20174;&#32780;&#20135;&#29983;&#22256;&#24785;&#24182;&#38477;&#20302;&#23545;&#29031;&#29255;&#30340;&#20449;&#20219;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;AI&#30340;&#35270;&#35273;&#20869;&#23481;&#29983;&#25104;&#27169;&#22411;&#33021;&#21542;&#25345;&#32493;&#22320;&#27450;&#39575;&#20154;&#31867;&#30340;&#30524;&#30555;&#65292;&#24182;&#20256;&#36798;&#38169;&#35823;&#20449;&#24687;&#12290;&#36890;&#36807;&#23545;50&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#23450;&#37327;&#30740;&#31350;&#65292;&#25105;&#20204;&#39318;&#27425;&#25581;&#31034;&#65292;&#20154;&#31867;&#26080;&#27861;&#26174;&#33879;&#21306;&#20998;&#30495;&#23454;&#29031;&#29255;&#21644;AI&#21019;&#24314;&#30340;&#20266;&#36896;&#29031;&#29255;&#65292;&#36798;&#21040;38.7%&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20010;&#20154;&#30340;&#32972;&#26223;&#65292;&#22914;&#24615;&#21035;&#65292;&#24180;&#40836;&#21644;&#32463;&#39564;&#65292;&#23545;&#21306;&#20998;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#29031;&#29255;&#30340;&#33021;&#21147;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;
&lt;/p&gt;
&lt;p&gt;
Photos serve as a way for humans to record what they experience in their daily lives, and they are often regarded as trustworthy sources of information. However, there is a growing concern that the advancement of artificial intelligence (AI) technology may produce fake photos, which can create confusion and diminish trust in photographs. This study aims to answer the question of whether the current state-of-the-art AI-based visual content generation models can consistently deceive human eyes and convey false information. By conducting a high-quality quantitative study with fifty participants, we reveal, for the first time, that humans cannot distinguish between real photos and AI-created fake photos to a significant degree 38.7%. Our study also finds that an individual's background, such as their gender, age, and experience with AI-generated content (AIGC), does not significantly affect their ability to distinguish AI-generated images from real photographs. However, we do observe that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;Multi-Chain Reasoning (MCR)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26816;&#26597;&#22810;&#20010;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#20174;&#32780;&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65292;&#35299;&#20915;&#22810;&#36339;QA&#38382;&#39064;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MCR&#32988;&#36807;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#35299;&#37322;&#36136;&#37327;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.13007</link><description>&lt;p&gt;
&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65306;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;&#38382;&#39064;&#35299;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Answering Questions by Meta-Reasoning over Multiple Chains of Thought. (arXiv:2304.13007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;Multi-Chain Reasoning (MCR)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26816;&#26597;&#22810;&#20010;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#20174;&#32780;&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65292;&#35299;&#20915;&#22810;&#36339;QA&#38382;&#39064;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MCR&#32988;&#36807;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#35299;&#37322;&#36136;&#37327;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#36339;&#38382;&#39064;&#35299;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#36890;&#24120;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#24605;&#32771;&#27493;&#39588;&#65288;CoT&#65289;&#65292;&#28982;&#21518;&#25165;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#36890;&#24120;&#26469;&#35828;&#65292;&#22810;&#20010;&#38142;&#26465;&#34987;&#25277;&#26679;&#24182;&#36890;&#36807;&#26368;&#32456;&#31572;&#26696;&#30340;&#25237;&#31080;&#26426;&#21046;&#36827;&#34892;&#32858;&#21512;&#65292;&#20294;&#20013;&#38388;&#27493;&#39588;&#26412;&#36523;&#34987;&#20002;&#24323;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#32771;&#34385;&#38142;&#20043;&#38388;&#30340;&#20013;&#38388;&#27493;&#39588;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#19981;&#25552;&#20379;&#39044;&#27979;&#31572;&#26696;&#30340;&#32479;&#19968;&#35299;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340; Multi-Chain Reasoning (MCR) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36229;&#36234;&#22810;&#20010;&#24605;&#32771;&#38142;&#65292;&#32780;&#19981;&#26159;&#32858;&#21512;&#22238;&#31572;&#12290;MCR&#26816;&#26597;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#22312;&#29983;&#25104;&#35299;&#37322;&#21644;&#39044;&#27979;&#31572;&#26696;&#26102;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;MCR&#22312;7&#20010;&#22810;&#36339;QA&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;MCR&#30340;&#35299;&#37322;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;Lux AI v2 Kaggle&#27604;&#36187;&#20013;&#20351;&#29992;&#38598;&#20013;&#24335;&#26041;&#27861;&#26469;&#35757;&#32451;RL&#26234;&#33021;&#20307;&#65292;&#22312;&#22797;&#26434;&#30340;&#23454;&#26102;&#25112;&#30053;&#28216;&#25103;&#20013;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13004</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#23454;&#26102;&#25112;&#30053;&#28216;&#25103;&#20013;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#38598;&#20013;&#21270;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Centralized control for multi-agent RL in a complex Real-Time-Strategy game. (arXiv:2304.13004v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;Lux AI v2 Kaggle&#27604;&#36187;&#20013;&#20351;&#29992;&#38598;&#20013;&#24335;&#26041;&#27861;&#26469;&#35757;&#32451;RL&#26234;&#33021;&#20307;&#65292;&#22312;&#22797;&#26434;&#30340;&#23454;&#26102;&#25112;&#30053;&#28216;&#25103;&#20013;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#30740;&#31350;&#22810;&#20010;&#30456;&#20114;&#20316;&#29992;&#30340;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#20849;&#20139;&#29615;&#22659;&#19979;&#30340;&#34892;&#20026;&#12290;&#19982;&#21333;&#19968;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;MARL&#20855;&#26377;&#26356;&#21152;&#22797;&#26434;&#30340;&#23398;&#20064;&#21160;&#24577;&#65306;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#35266;&#23519;&#21644;&#22870;&#21169;&#37117;&#26159;&#20854;&#20182;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#20989;&#25968;&#12290;&#22312;MARL&#30340;&#32972;&#26223;&#19979;&#65292;&#23454;&#26102;&#25112;&#30053;(RTS)&#28216;&#25103;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#20854;&#20013;&#22810;&#20010;&#29609;&#23478;&#21516;&#26102;&#20114;&#21160;&#24182;&#21516;&#26102;&#25511;&#21046;&#19981;&#21516;&#24615;&#36136;&#30340;&#22810;&#20010;&#21333;&#20301;&#12290;&#23454;&#38469;&#19978;&#65292;&#23545;&#20110;&#24403;&#21069;&#30340;RL&#26041;&#27861;&#26469;&#35828;&#65292;RTS&#28216;&#25103;&#22914;&#27492;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20197;&#33267;&#20110;&#21482;&#26159;&#33021;&#22815;&#29992;RL&#24212;&#23545;&#23427;&#20204;&#24050;&#32463;&#24456;&#26377;&#24847;&#20041;&#20102;&#12290;&#26412;&#39033;&#30446;&#25552;&#20379;&#20102;&#22312;Lux AI v2 Kaggle&#27604;&#36187;&#20013;&#24212;&#29992;RL&#30340;&#31471;&#21040;&#31471;&#20307;&#39564;&#65292;&#21442;&#36187;&#32773;&#35774;&#35745;&#33021;&#22815;&#25511;&#21046;&#21487;&#21464;&#22823;&#23567;&#33328;&#38431;&#21333;&#20301;&#24182;&#22312;1v1&#24773;&#20917;&#19979;&#38754;&#23545;&#20854;&#20182;&#21442;&#36187;&#32773;&#30340;&#22810;&#21464;&#37327;&#20248;&#21270;&#12289;&#36164;&#28304;&#25910;&#38598;&#21644;&#20998;&#37197;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#38598;&#20013;&#24335;&#26041;&#27861;&#26469;&#35757;&#32451;RL&#26234;&#33021;&#20307;&#65292;&#24182;&#20351;&#29992;&#20840;&#23616;&#35270;&#22270;&#34920;&#31034;&#28216;&#25103;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;&#29420;&#31435;&#20195;&#29702;&#21644;&#20998;&#25955;&#35757;&#32451;&#30340;&#26631;&#20934;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement learning (MARL) studies the behaviour of multiple learning agents that coexist in a shared environment. MARL is more challenging than single-agent RL because it involves more complex learning dynamics: the observations and rewards of each agent are functions of all other agents. In the context of MARL, Real-Time Strategy (RTS) games represent very challenging environments where multiple players interact simultaneously and control many units of different natures all at once. In fact, RTS games are so challenging for the current RL methods, that just being able to tackle them with RL is interesting. This project provides the end-to-end experience of applying RL in the Lux AI v2 Kaggle competition, where competitors design agents to control variable-sized fleets of units and tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. We use a centralized approach for training the RL agents, and rep
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#23398;&#20064;&#32467;&#26500;&#21270;&#34920;&#31034;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29289;&#20307;&#26816;&#27979;&#31561;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13001</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#23398;&#20064;&#32467;&#26500;&#21270;&#34920;&#31034;&#30340;&#27867;&#21270;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization of Learned Structured Representations. (arXiv:2304.13001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#23398;&#20064;&#32467;&#26500;&#21270;&#34920;&#31034;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#33021;&#22815;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29289;&#20307;&#26816;&#27979;&#31561;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#36798;&#21040;&#20154;&#31867;&#32423;&#21035;&#30340;&#31995;&#32479;&#27867;&#21270;&#27700;&#24179;&#12290;&#20154;&#20204;&#35748;&#20026;&#26126;&#30830;&#25429;&#33719;&#25968;&#25454;&#30340;&#22522;&#30784;&#32467;&#26500;&#24212;&#35813;&#33021;&#22815;&#35753;&#32852;&#32467;&#20027;&#20041;&#31995;&#32479;&#20197;&#26356;&#21487;&#39044;&#27979;&#21644;&#31995;&#32479;&#30340;&#26041;&#24335;&#36827;&#34892;&#27867;&#21270;&#12290;&#20107;&#23454;&#19978;&#65292;&#20154;&#31867;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#29992;&#31526;&#21495;&#33324;&#30340;&#32452;&#21512;&#23454;&#20307;&#26469;&#35299;&#37322;&#19990;&#30028;&#21487;&#33021;&#23545;&#26234;&#33021;&#34892;&#20026;&#21644;&#39640;&#32423;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#21478;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#24120;&#35265;&#38480;&#21046;&#26159;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#21033;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#23398;&#20064;&#36890;&#29992;&#25968;&#25454;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#29992;&#20110;&#26377;&#25928;&#22320;&#23398;&#20064;&#20219;&#24847;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32467;&#26500;&#21270;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#26410;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#24456;&#23569;&#25110;&#27809;&#26377;&#30417;&#30563;&#65292;&#24182;&#25429;&#33719;&#20854;&#38544;&#34255;&#32467;&#26500;&#12290;&#22312;&#35770;&#25991;&#30340;&#31532;&#19968;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#21487;&#20197;&#25429;&#33719;&#25968;&#25454;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#31532;&#20108;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#22914;&#20309;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29289;&#20307;&#26816;&#27979;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite tremendous progress over the past decade, deep learning methods generally fall short of human-level systematic generalization. It has been argued that explicitly capturing the underlying structure of data should allow connectionist systems to generalize in a more predictable and systematic manner. Indeed, evidence in humans suggests that interpreting the world in terms of symbol-like compositional entities may be crucial for intelligent behavior and high-level reasoning. Another common limitation of deep learning systems is that they require large amounts of training data, which can be expensive to obtain. In representation learning, large datasets are leveraged to learn generic data representations that may be useful for efficient learning of arbitrary downstream tasks.  This thesis is about structured representation learning. We study methods that learn, with little or no supervision, representations of unstructured data that capture its hidden structure. In the first part of
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#24320;&#21457;&#30340;Segment Anything Model&#65288;SAM&#65289;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#31616;&#21333;&#30340;&#36755;&#20837;&#25552;&#31034;&#65288;&#22914;&#19968;&#20010;&#25110;&#22810;&#20010;&#28857;&#12289;&#36793;&#30028;&#26694;&#25110;&#25513;&#30721;&#65289;&#26377;&#25928;&#20998;&#21106;&#33258;&#28982;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#23545;&#35270;&#35273;&#30740;&#31350;&#20154;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#27492;&#39033;&#30740;&#31350;&#25506;&#35752;SAM&#22312;&#31354;&#20013;&#22270;&#20687;&#38382;&#39064;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#39033;&#22522;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.13000</link><description>&lt;p&gt;
&#20174;&#31354;&#38388;&#20013;&#20998;&#21106;&#20219;&#20309;&#29289;&#20307;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Segment anything, from space?. (arXiv:2304.13000v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13000
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;Segment Anything Model&#65288;SAM&#65289;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#31616;&#21333;&#30340;&#36755;&#20837;&#25552;&#31034;&#65288;&#22914;&#19968;&#20010;&#25110;&#22810;&#20010;&#28857;&#12289;&#36793;&#30028;&#26694;&#25110;&#25513;&#30721;&#65289;&#26377;&#25928;&#20998;&#21106;&#33258;&#28982;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#23545;&#35270;&#35273;&#30740;&#31350;&#20154;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#27492;&#39033;&#30740;&#31350;&#25506;&#35752;SAM&#22312;&#31354;&#20013;&#22270;&#20687;&#38382;&#39064;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#39033;&#22522;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20026;&#35270;&#35273;&#20219;&#21153;&#19987;&#38376;&#24320;&#21457;&#30340;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#34987;&#31216;&#20026;&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#12290;SAM&#21487;&#20197;&#26681;&#25454;&#31616;&#21333;&#30340;&#36755;&#20837;&#25552;&#31034;&#65288;&#22914;&#19968;&#20010;&#25110;&#22810;&#20010;&#28857;&#12289;&#36793;&#30028;&#26694;&#25110;&#25513;&#30721;&#65289;&#20998;&#21106;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#12290;&#20316;&#32773;&#20204;&#22312;&#22823;&#37327;&#30340;&#35270;&#35273;&#22522;&#20934;&#20219;&#21153;&#19978;&#30740;&#31350;&#20102;SAM&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#31934;&#24230;&#65292;&#24182;&#21457;&#29616;SAM&#36890;&#24120;&#36798;&#21040;&#20102;&#19982;&#30446;&#26631;&#20219;&#21153;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#30456;&#20284;&#25110;&#26377;&#26102;&#29978;&#33267;&#36229;&#36234;&#20854;&#35782;&#21035;&#31934;&#24230;&#12290;SAM&#22312;&#20998;&#21106;&#26041;&#38754;&#30340;&#21331;&#36234;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#20174;&#20107;&#33258;&#28982;&#22270;&#20687;&#30740;&#31350;&#30340;&#35270;&#35273;&#30740;&#31350;&#20154;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;SAM&#30340;&#21331;&#36234;&#24615;&#33021;&#26159;&#21542;&#25193;&#23637;&#21040;&#31354;&#20013;&#22270;&#20687;&#38382;&#39064;&#65292;&#24182;&#24110;&#21161;&#25351;&#23548;&#31038;&#21306;&#23545;&#20854;&#21457;&#23637;&#30340;&#22238;&#24212;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#22810;&#26679;&#21270;&#21644;&#24191;&#27867;&#30740;&#31350;&#36807;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#30740;&#31350;SAM&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;SAM&#36890;&#24120;&#22312;&#31354;&#20013;&#22270;&#20687;&#19978;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#34920;&#29616;&#65292;&#23613;&#31649;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20250;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the first foundation model developed specifically for vision tasks was developed, termed the "Segment Anything Model" (SAM). SAM can segment objects in input imagery based upon cheap input prompts, such as one (or more) points, a bounding box, or a mask. The authors examined the zero-shot image segmentation accuracy of SAM on a large number of vision benchmark tasks and found that SAM usually achieved recognition accuracy similar to, or sometimes exceeding, vision models that had been trained on the target tasks. The impressive generalization of SAM for segmentation has major implications for vision researchers working on natural imagery. In this work, we examine whether SAM's impressive performance extends to overhead imagery problems, and help guide the community's response to its development. We examine SAM's performance on a set of diverse and widely-studied benchmark tasks. We find that SAM does often generalize well to overhead imagery, although it fails in some cases d
&lt;/p&gt;</description></item><item><title>AudioGPT&#26159;&#19968;&#31181;&#22810;&#27169;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#38899;&#39057;&#20449;&#24687;&#24182;&#25903;&#25345;&#21475;&#35821;&#23545;&#35805;&#65292;&#20854;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#22788;&#29702;&#35821;&#38899;&#12289;&#38899;&#20048;&#12289;&#22768;&#38899;&#21644;&#20154;&#22836;&#20687;&#26041;&#38754;&#26377;&#30528;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.12995</link><description>&lt;p&gt;
AudioGPT&#65306;&#29702;&#35299;&#21644;&#29983;&#25104;&#35821;&#38899;&#12289;&#38899;&#20048;&#12289;&#22768;&#38899;&#21644;&#20154;&#22836;&#20687;
&lt;/p&gt;
&lt;p&gt;
AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head. (arXiv:2304.12995v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12995
&lt;/p&gt;
&lt;p&gt;
AudioGPT&#26159;&#19968;&#31181;&#22810;&#27169;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#38899;&#39057;&#20449;&#24687;&#24182;&#25903;&#25345;&#21475;&#35821;&#23545;&#35805;&#65292;&#20854;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#22788;&#29702;&#35821;&#38899;&#12289;&#38899;&#20048;&#12289;&#22768;&#38899;&#21644;&#20154;&#22836;&#20687;&#26041;&#38754;&#26377;&#30528;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#25361;&#25112;&#20102;&#25105;&#20204;&#23545;&#23398;&#20064;&#21644;&#35748;&#30693;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#24403;&#21069;&#30340;LLM&#26080;&#27861;&#22788;&#29702;&#22797;&#26434;&#30340;&#38899;&#39057;&#20449;&#24687;&#25110;&#36827;&#34892;&#21475;&#35821;&#20132;&#27969;&#65288;&#22914;Siri&#25110;Alexa&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AudioGPT&#30340;&#22810;&#27169;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#34917;&#20805;&#20102;LLM&#65288;&#21363;ChatGPT&#65289;&#65306;1&#65289;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#38899;&#39057;&#20449;&#24687;&#24182;&#35299;&#20915;&#20247;&#22810;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65307;2&#65289;&#25552;&#20379;&#36755;&#20837;/&#36755;&#20986;&#25509;&#21475;&#65288;ASR&#65292;TTS&#65289;&#20197;&#25903;&#25345;&#21475;&#35821;&#23545;&#35805;&#12290;&#38543;&#30528;&#23545;&#20154;&#31867;&#24847;&#22270;&#29702;&#35299;&#21644;&#19982;&#22522;&#30784;&#27169;&#22411;&#21327;&#20316;&#30340;&#22810;&#27169;&#24335;LLM&#30340;&#35780;&#20272;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21407;&#21017;&#21644;&#36807;&#31243;&#65292;&#24182;&#27979;&#35797;&#20102;AudioGPT&#30340;&#19968;&#33268;&#24615;&#12289;&#33021;&#21147;&#21644;&#31283;&#20581;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;AudioGPT&#22312;&#35299;&#20915;&#20855;&#26377;&#35821;&#38899;&#12289;&#38899;&#20048;&#12289;&#22768;&#38899;&#21644;&#20154;&#22836;&#20687;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;AI&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and gene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#27979;&#35797;&#65292;&#20197;&#34913;&#37327;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#27979;&#35797;&#28085;&#30422;&#21307;&#23398;&#12289;&#27861;&#24459;&#12289;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#26377;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#24320;&#21457;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#22343;&#34913;&#30340;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.12986</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Measuring Massive Multitask Chinese Understanding. (arXiv:2304.12986v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#27979;&#35797;&#65292;&#20197;&#34913;&#37327;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#27979;&#35797;&#28085;&#30422;&#21307;&#23398;&#12289;&#27861;&#24459;&#12289;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#26377;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#24320;&#21457;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#22343;&#34913;&#30340;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#21457;&#27491;&#34028;&#21187;&#21457;&#23637;&#65292;&#20294;&#32570;&#20047;&#30456;&#24212;&#30340;&#33021;&#21147;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#65292;&#20197;&#34913;&#37327;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#12290;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#21307;&#23398;&#12289;&#27861;&#24459;&#12289;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#26377;15&#20010;&#23376;&#20219;&#21153;&#65292;&#22312;&#25945;&#32946;&#39046;&#22495;&#26377;8&#20010;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#24179;&#22343;&#27604;&#34920;&#29616;&#26368;&#24046;&#30340;&#27169;&#22411;&#39640;&#20986;&#36817;22&#20010;&#30334;&#20998;&#28857;&#12290;&#22312;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#20013;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24179;&#22343;&#38646;&#26679;&#26412;&#20934;&#30830;&#24230;&#22343;&#26410;&#36229;&#36807;0.5&#12290;&#22312;&#23376;&#39046;&#22495;&#20013;&#65292;&#21482;&#26377;GPT-3.5-turbo&#27169;&#22411;&#22312;&#20020;&#24202;&#21307;&#23398;&#20013;&#23454;&#29616;&#20102;0.703&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24230;&#65292;&#36825;&#26159;&#25152;&#26377;&#27169;&#22411;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;&#25152;&#26377;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#65292;&#26368;&#39640;&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24230;&#20165;&#36798;&#21040;0.259&#12290;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#22810;&#20010;&#23398;&#31185;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#24320;&#21457;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#22343;&#34913;&#30340;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large-scale Chinese language models is flourishing, yet there is a lack of corresponding capability assessments. Therefore, we propose a test to measure the multitask accuracy of large Chinese language models. This test encompasses four major domains, including medicine, law, psychology, and education, with 15 subtasks in medicine and 8 subtasks in education. We found that the best-performing models in the zero-shot setting outperformed the worst-performing models by nearly 22 percentage points on average. Across the four major domains, the average zero-shot accuracy of all models did not exceed 0.5. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot accuracy of 0.703 in clinical medicine, which was the highest accuracy among all models across all subtasks. All models performed poorly in the legal domain, with the highest zero-shot accuracy reaching only 0.259. By comprehensively evaluating the breadth and depth of knowledge across multiple discipli
&lt;/p&gt;</description></item><item><title>RubikONNs&#21033;&#29992;&#20809;&#23398;&#31995;&#32479;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#36890;&#36807;&#26059;&#36716;&#30828;&#20214;&#32534;&#30721;&#22810;&#20010;&#21069;&#39304;&#20989;&#25968;&#65292;&#20026;ONNs&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.12985</link><description>&lt;p&gt;
Rubik&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#65306;&#20855;&#26377;&#29289;&#29702;&#24863;&#30693;&#26059;&#36716;&#32467;&#26500;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rubik's Optical Neural Networks: Multi-task Learning with Physics-aware Rotation Architecture. (arXiv:2304.12985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12985
&lt;/p&gt;
&lt;p&gt;
RubikONNs&#21033;&#29992;&#20809;&#23398;&#31995;&#32479;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#36890;&#36807;&#26059;&#36716;&#30828;&#20214;&#32534;&#30721;&#22810;&#20010;&#21069;&#39304;&#20989;&#25968;&#65292;&#20026;ONNs&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24037;&#20316;&#22312;&#25512;&#36827;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#65288;ONNs&#65289;&#65292;&#22312;&#21151;&#29575;&#25928;&#29575;&#65292;&#24182;&#34892;&#24615;&#21644;&#35745;&#31639;&#36895;&#24230;&#26041;&#38754;&#65292;ONNs&#24102;&#26469;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#38754;&#30340;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there are increasing efforts on advancing optical neural networks (ONNs), which bring significant advantages for machine learning (ML) in terms of power efficiency, parallelism, and computational speed. With the considerable benefits in computation speed and energy efficiency, there are significant interests in leveraging ONNs into medical sensing, security screening, drug detection, and autonomous driving. However, due to the challenge of implementing reconfigurability, deploying multi-task learning (MTL) algorithms on ONNs requires re-building and duplicating the physical diffractive systems, which significantly degrades the energy and cost efficiency in practical application scenarios. This work presents a novel ONNs architecture, namely, \textit{RubikONNs}, which utilizes the physical properties of optical systems to encode multiple feed-forward functions by physically rotating the hardware similarly to rotating a \textit{Rubik's Cube}. To optimize MTL performance on Rubi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22870;&#21169;&#20998;&#35299;&#19982;&#25277;&#35937;&#21160;&#20316;&#31354;&#38388;&#30456;&#32467;&#21512;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#23545;&#35937;&#23646;&#24615;&#30340;&#26126;&#30830;&#39640;&#23618;&#27425;&#35299;&#37322;&#65292;&#36991;&#20813;&#20102;&#35299;&#37322;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12958</link><description>&lt;p&gt;
&#23545;&#39640;&#27700;&#24179;&#26426;&#22120;&#20154;&#35299;&#37322;&#20013;&#30340;&#22870;&#21169;&#20998;&#35299;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Reward Decomposition for High-Level Robotic Explanations. (arXiv:2304.12958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22870;&#21169;&#20998;&#35299;&#19982;&#25277;&#35937;&#21160;&#20316;&#31354;&#38388;&#30456;&#32467;&#21512;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#23545;&#35937;&#23646;&#24615;&#30340;&#26126;&#30830;&#39640;&#23618;&#27425;&#35299;&#37322;&#65292;&#36991;&#20813;&#20102;&#35299;&#37322;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#20154;&#31867;&#35299;&#37322;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#38590;&#20197;&#29702;&#35299;&#30340;&#33258;&#20307;&#24863;&#29366;&#24577;&#12289;&#22810;&#21464;&#30340;&#20013;&#38388;&#30446;&#26631;&#21644;&#20854;&#32467;&#26524;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#19968;&#27493;&#35299;&#37322;&#21487;&#33021;&#26159;&#27169;&#31946;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26410;&#33021;&#22312;&#27599;&#20010;&#36716;&#25442;&#26102;&#32771;&#34385;&#21040;&#20195;&#29702;&#30340;&#26410;&#26469;&#34892;&#20026;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#35299;&#37322;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#26144;&#23556;&#21040;&#20219;&#21153;&#29305;&#23450;&#22522;&#20803;&#30340;&#25277;&#35937;&#21160;&#20316;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#22312;&#31227;&#21160;&#23618;&#38754;&#19978;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#22870;&#21169;&#20998;&#35299;&#65288;RD&#65289;&#19982;&#25277;&#35937;&#21160;&#20316;&#31354;&#38388;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#22522;&#20110;&#20219;&#21153;&#20013;&#30340;&#23545;&#35937;&#23646;&#24615;&#65292;&#23454;&#29616;&#20102;&#26126;&#30830;&#30340;&#39640;&#23618;&#27425;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20004;&#20010;&#26426;&#22120;&#20154;&#22330;&#26223;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#26469;&#35777;&#26126;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;RD&#35299;&#37322;&#30340;&#36755;&#20986;&#25991;&#29289;&#20013;&#30340;&#21487;&#35270;&#21644;&#25991;&#26412;&#35299;&#37322;&#65292;&#26131;&#20110;&#20154;&#31867;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining the behavior of intelligent agents such as robots to humans is challenging due to their incomprehensible proprioceptive states, variational intermediate goals, and resultant unpredictability. Moreover, one-step explanations for reinforcement learning agents can be ambiguous as they fail to account for the agent's future behavior at each transition, adding to the complexity of explaining robot actions. By leveraging abstracted actions that map to task-specific primitives, we avoid explanations on the movement level. Our proposed framework combines reward decomposition (RD) with abstracted action spaces into an explainable learning framework, allowing for non-ambiguous and high-level explanations based on object properties in the task. We demonstrate the effectiveness of our framework through quantitative and qualitative analysis of two robot scenarios, showcasing visual and textual explanations, from output artifacts of RD explanation, that are easy for humans to comprehend. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26694;&#26550;CROCO&#65292;&#33021;&#22815;&#29983;&#25104;&#24378;&#40065;&#26834;&#24615;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#40065;&#26834;&#24615;&#21644;&#35299;&#37322;&#31034;&#20363;&#30340;&#25509;&#36817;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.12943</link><description>&lt;p&gt;
&#29983;&#25104;&#24378;&#40065;&#26834;&#24615;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Generating robust counterfactual explanations. (arXiv:2304.12943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26694;&#26550;CROCO&#65292;&#33021;&#22815;&#29983;&#25104;&#24378;&#40065;&#26834;&#24615;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#40065;&#26834;&#24615;&#21644;&#35299;&#37322;&#31034;&#20363;&#30340;&#25509;&#36817;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#24050;&#32463;&#25104;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20027;&#27969;&#12290;&#36825;&#19968;&#30452;&#35266;&#30340;&#38472;&#36848;&#35753;&#29992;&#25143;&#29702;&#35299;&#22312;&#32473;&#23450;&#24773;&#20917;&#19979;&#65292;&#20026;&#25913;&#21464;&#27169;&#22411;&#39044;&#27979;&#32780;&#24517;&#39035;&#36827;&#34892;&#30340;&#23567;&#20294;&#24517;&#35201;&#30340;&#26356;&#25913;&#12290;&#21453;&#20107;&#23454;&#30340;&#36136;&#37327;&#21462;&#20915;&#20110;&#22810;&#20010;&#26631;&#20934;&#65306;&#30495;&#23454;&#24615;&#12289;&#21487;&#34892;&#24615;&#12289;&#26377;&#25928;&#24615;&#12289;&#40065;&#26834;&#24615;&#31561;&#31561;&#12290;&#26412;&#25991;&#20851;&#27880;&#21453;&#20107;&#23454;&#30340;&#40065;&#26834;&#24615;&#27010;&#24565;&#65292;&#26356;&#20855;&#20307;&#22320;&#65292;&#20851;&#27880;&#21453;&#20107;&#23454;&#36755;&#20837;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#40065;&#26834;&#24615;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#21453;&#20107;&#23454;&#30340;&#40065;&#26834;&#24615;&#21644;&#19982;&#35201;&#35299;&#37322;&#30340;&#31034;&#20363;&#30340;&#25509;&#36817;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;CROCO&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#31649;&#29702;&#36825;&#31181;&#26435;&#34913;&#65292;&#29983;&#25104;&#20855;&#26377;&#24378;&#40065;&#26834;&#24615;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#24182;&#20445;&#35777;&#29992;&#25143;&#20855;&#26377;&#26368;&#23567;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#30830;&#35748;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#30456;&#20851;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations have become a mainstay of the XAI field. This particularly intuitive statement allows the user to understand what small but necessary changes would have to be made to a given situation in order to change a model prediction. The quality of a counterfactual depends on several criteria: realism, actionability, validity, robustness, etc. In this paper, we are interested in the notion of robustness of a counterfactual. More precisely, we focus on robustness to counterfactual input changes. This form of robustness is particularly challenging as it involves a trade-off between the robustness of the counterfactual and the proximity with the example to explain. We propose a new framework, CROCO, that generates robust counterfactuals while managing effectively this trade-off, and guarantees the user a minimal robustness. An empirical evaluation on tabular datasets confirms the relevance and effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>SALSA&#26159;&#19968;&#31181;&#29992;&#20110;DNN&#21152;&#36895;&#22120;&#30340;&#24555;&#36895;&#21452;&#24341;&#25806;&#35843;&#24230;&#22120;&#65292;&#21033;&#29992;&#26032;&#31574;&#30053;&#23558;&#31351;&#20030;&#25628;&#32034;&#21644;&#27169;&#25311;&#36864;&#28779;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#21152;&#36895;&#25628;&#32034;&#30340;&#21516;&#26102;&#25214;&#21040;&#26356;&#20302;&#33021;&#37327;&#35843;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.12931</link><description>&lt;p&gt;
SALSA: &#29992;&#20110;DNN&#21152;&#36895;&#22120;&#30340;&#27169;&#25311;&#36864;&#28779;&#24490;&#29615;&#25490;&#24207;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
SALSA: Simulated Annealing based Loop-Ordering Scheduler for DNN Accelerators. (arXiv:2304.12931v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12931
&lt;/p&gt;
&lt;p&gt;
SALSA&#26159;&#19968;&#31181;&#29992;&#20110;DNN&#21152;&#36895;&#22120;&#30340;&#24555;&#36895;&#21452;&#24341;&#25806;&#35843;&#24230;&#22120;&#65292;&#21033;&#29992;&#26032;&#31574;&#30053;&#23558;&#31351;&#20030;&#25628;&#32034;&#21644;&#27169;&#25311;&#36864;&#28779;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#21152;&#36895;&#25628;&#32034;&#30340;&#21516;&#26102;&#25214;&#21040;&#26356;&#20302;&#33021;&#37327;&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#28385;&#36275;DNN&#35745;&#31639;&#38656;&#27714;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#19987;&#29992;&#30340;&#30828;&#20214;&#20307;&#31995;&#32467;&#26500;&#12290;&#27599;&#20010;DNN&#23618;&#24212;&#35813;&#26144;&#23556;&#21040;&#26368;&#26377;&#25928;&#30340;&#30828;&#20214;&#19978;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#20248;&#21270;&#22120;&#24448;&#24448;&#26080;&#27861;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#20026;&#25152;&#26377;DNN-HW&#32452;&#21512;&#25552;&#20379;&#26368;&#20339;&#30340;&#25191;&#34892;&#35745;&#21010;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SALSA&#65292;&#19968;&#31181;&#24555;&#36895;&#30340;&#21452;&#24341;&#25806;&#35843;&#24230;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#22343;&#21248;&#21644;&#38750;&#22343;&#21248;&#26144;&#23556;&#30340;&#26368;&#20248;&#25191;&#34892;&#35745;&#21010;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#31574;&#30053;&#65292;&#23558;&#31351;&#20030;&#25628;&#32034;&#19982;&#27169;&#25311;&#36864;&#28779;&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#19981;&#21516;&#23618;&#20043;&#38388;&#24490;&#29615;&#25490;&#24207;&#35774;&#35745;&#31354;&#38388;&#22823;&#23567;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;SALSA&#22312;5&#20010;&#19981;&#21516;&#30340;DNN&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24179;&#22343;&#32780;&#35328;&#65292;&#19982;LOMA&#21644;Timeloop&#30456;&#27604;&#65292;SALSA&#22312;&#21152;&#36895;&#25628;&#32034;&#30340;&#21516;&#26102;&#33021;&#22815;&#25214;&#21040;11.9&#65285;&#21644;7.6&#65285;&#26356;&#20302;&#30340;&#33021;&#37327;&#35843;&#24230;&#65292;&#20998;&#21035;&#25552;&#39640;&#20102;1.7&#20493;&#21644;24&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
To meet the growing need for computational power for DNNs, multiple specialized hardware architectures have been proposed. Each DNN layer should be mapped onto the hardware with the most efficient schedule, however, SotA schedulers struggle to consistently provide optimum schedules in a reasonable time across all DNN-HW combinations.  This paper proposes SALSA, a fast dual-engine scheduler to generate optimal execution schedules for both even and uneven mapping. We introduce a new strategy, combining exhaustive search with simulated annealing to address the dynamic nature of the loop ordering design space size across layers. SALSA is extensively benchmarked against two SotA schedulers, LOMA and Timeloop on 5 different DNNs, on average SALSA finds schedules with 11.9% and 7.6% lower energy while speeding up the search by 1.7x and 24x compared to LOMA and Timeloop, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23433;&#20840;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#65292;&#20445;&#35777;&#20840;&#23616;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#36866;&#29992;&#20110;&#24037;&#19994;&#29289;&#32852;&#32593;&#12290;</title><link>http://arxiv.org/abs/2304.12889</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#30340;&#29289;&#32852;&#32593;&#32852;&#37030;&#23398;&#20064;&#19982;&#23433;&#20840;&#38598;&#25104;&#30740;&#31350; (arXiv:2304.12889v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
Blockchain-based Federated Learning with Secure Aggregation in Trusted Execution Environment for Internet-of-Things. (arXiv:2304.12889v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23433;&#20840;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#65292;&#20445;&#35777;&#20840;&#23616;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#36866;&#29992;&#20110;&#24037;&#19994;&#29289;&#32852;&#32593;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#21644;Intel Software Guard Extension&#65288;SGX&#65289;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#65288;TEE&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26694;&#26550;&#65292;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#65288;IIoTs&#65289;&#20013;&#23433;&#20840;&#22320;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#12290;&#22312;FL&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#33021;&#31713;&#25913;&#26412;&#22320;&#27169;&#22411;&#65292;&#22240;&#27492;&#65292;&#20174;&#31713;&#25913;&#30340;&#26412;&#22320;&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#27169;&#22411;&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#12290;&#22240;&#27492;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21306;&#22359;&#38142;&#32593;&#32476;&#36827;&#34892;&#23433;&#20840;&#27169;&#22411;&#32858;&#21512;&#65292;&#27599;&#20010;&#21306;&#22359;&#38142;&#33410;&#28857;&#37117;&#25317;&#26377;&#19968;&#20010;&#21551;&#29992;&#20102;SGX&#30340;&#22788;&#29702;&#22120;&#65292;&#21487;&#20197;&#23433;&#20840;&#22320;&#25191;&#34892;&#22522;&#20110;FL&#30340;&#32858;&#21512;&#20219;&#21153;&#20197;&#29983;&#25104;&#20840;&#23616;&#27169;&#22411;&#65292;&#33410;&#28857;&#21487;&#20197;&#39564;&#35777;&#32858;&#21512;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#36816;&#34892;&#21306;&#22359;&#38142;&#20849;&#35782;&#26426;&#21046;&#20197;&#30830;&#20445;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#65292;&#24182;&#23558;&#20854;&#28155;&#21152;&#21040;&#20998;&#24067;&#24335;&#20998;&#31867;&#24080;&#20013;&#36827;&#34892;&#38450;&#31713;&#25913;&#23384;&#20648;&#12290;&#27599;&#20010;&#38598;&#32676;&#37117;&#21487;&#20197;&#20174;&#21306;&#22359;&#38142;&#33719;&#21462;&#32858;&#21512;&#27169;&#22411;&#24182;&#22312;&#20351;&#29992;&#20043;&#21069;&#39564;&#35777;&#20854;&#23436;&#25972;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;CNN&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20960;&#39033;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#35813;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a blockchain-based Federated Learning (FL) framework with Intel Software Guard Extension (SGX)-based Trusted Execution Environment (TEE) to securely aggregate local models in Industrial Internet-of-Things (IIoTs). In FL, local models can be tampered with by attackers. Hence, a global model generated from the tampered local models can be erroneous. Therefore, the proposed framework leverages a blockchain network for secure model aggregation. Each blockchain node hosts an SGX-enabled processor that securely performs the FL-based aggregation tasks to generate a global model. Blockchain nodes can verify the authenticity of the aggregated model, run a blockchain consensus mechanism to ensure the integrity of the model, and add it to the distributed ledger for tamper-proof storage. Each cluster can obtain the aggregated model from the blockchain and verify its integrity before using it. We conducted several experiments with different CNN models and datasets to evaluate th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#26356;&#22909;&#22320;&#36827;&#34892;&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#26377;&#25928;&#20943;&#36731;&#26032;&#38395;&#21644;&#35777;&#25454;&#20869;&#23481;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.12888</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#37325;&#23545;&#25239;&#21435;&#20559;&#32622;&#23454;&#29616;&#30340;&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution Evidence-aware Fake News Detection via Dual Adversarial Debiasing. (arXiv:2304.12888v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#26356;&#22909;&#22320;&#36827;&#34892;&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#26377;&#25928;&#20943;&#36731;&#26032;&#38395;&#21644;&#35777;&#25454;&#20869;&#23481;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#26088;&#22312;&#23545;&#26032;&#38395;&#21644;&#22522;&#20110;&#26032;&#38395;&#20869;&#23481;&#26816;&#32034;&#30340;&#35777;&#25454;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#26597;&#25214;&#32479;&#19968;&#24615;&#25110;&#19981;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#35777;&#25454;&#24863;&#30693;&#26816;&#27979;&#27169;&#22411;&#20250;&#21463;&#21040;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#21363;&#26032;&#38395;/&#35777;&#25454;&#20869;&#23481;&#21644;&#30495;&#23454;/&#20551;&#26032;&#38395;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#24456;&#38590;&#25512;&#24191;&#21040;&#36234;&#30028;&#24773;&#20917;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;DAL&#20013;&#21152;&#20837;&#20102;&#26032;&#38395;&#26041;&#38754;&#21644;&#35777;&#25454;&#26041;&#38754;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#23427;&#20204;&#30340;&#30446;&#26631;&#37117;&#26159;&#30495;&#20551;&#26032;&#38395;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;DAL&#20250;&#36870;&#21521;&#20248;&#21270;&#26032;&#38395;&#26041;&#38754;&#21644;&#35777;&#25454;&#26041;&#38754;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#20197;&#20943;&#36731;&#26032;&#38395;&#21644;&#35777;&#25454;&#20869;&#23481;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;DAL&#36824;&#20248;&#21270;&#20027;&#35201;&#30340;&#20551;&#26032;&#38395;&#39044;&#27979;&#22120;&#65292;&#35753;&#26032;&#38395;-&#35777;&#25454;&#20132;&#20114;&#27169;&#22359;&#33021;&#22815;&#34987;&#23398;&#20064;&#12290;&#36825;&#20010;&#36807;&#31243;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#25945;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#26356;&#22909;&#22320;&#36827;&#34892;&#26032;&#38395;&#35777;&#25454;&#25512;&#29702;&#65292;&#24182;&#23558;&#26816;&#27979;&#20551;&#26032;&#38395;&#30340;&#36127;&#38754;&#24433;&#21709;&#38477;&#33267;&#26368;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidence-aware fake news detection aims to conduct reasoning between news and evidence, which is retrieved based on news content, to find uniformity or inconsistency. However, we find evidence-aware detection models suffer from biases, i.e., spurious correlations between news/evidence contents and true/fake news labels, and are hard to be generalized to Out-Of-Distribution (OOD) situations. To deal with this, we propose a novel Dual Adversarial Learning (DAL) approach. We incorporate news-aspect and evidence-aspect debiasing discriminators, whose targets are both true/fake news labels, in DAL. Then, DAL reversely optimizes news-aspect and evidence-aspect debiasing discriminators to mitigate the impact of news and evidence content biases. At the same time, DAL also optimizes the main fake news predictor, so that the news-evidence interaction module can be learned. This process allows us to teach evidence-aware fake news detection models to better conduct news-evidence reasoning, and min
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#22312;&#25104;&#23545;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#30340;&#65292;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;LArTPC&#25506;&#27979;&#22120;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12858</link><description>&lt;p&gt;
&#38754;&#21521;&#31185;&#23398;&#30340;&#26080;&#30417;&#30563;&#22495;&#36716;&#31227;&#65306;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20855;&#26377;&#19981;&#21516;&#21709;&#24212;&#27169;&#22411;&#30340;LArTPC&#25506;&#27979;&#22120;&#27169;&#25311;&#20043;&#38388;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Transfer for Science: Exploring Deep Learning Methods for Translation between LArTPC Detector Simulations with Differing Response Models. (arXiv:2304.12858v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#22312;&#25104;&#23545;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#30340;&#65292;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;LArTPC&#25506;&#27979;&#22120;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#23547;&#27714;&#31616;&#21270;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#21644;&#21457;&#29616;&#30340;&#36335;&#24452;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;DL&#27169;&#22411;&#32463;&#24120;&#22312;&#27169;&#25311;&#32467;&#26524;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#30495;&#23454;&#23454;&#39564;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#20219;&#20309;&#31995;&#32479;&#24046;&#24322;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#8220;&#39046;&#22495;&#28418;&#31227;&#8221;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31995;&#32479;&#24046;&#24322;&#30340;&#29609;&#20855;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#26469;&#20943;&#23569;&#20004;&#20010;&#31995;&#32479;&#19981;&#21516;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#22312;&#25104;&#23545;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#20004;&#32452;&#27169;&#25311;&#28082;&#27689;&#26102;&#38388;&#25237;&#24433;&#23460;&#65288;LArTPC&#65289;&#25506;&#27979;&#22120;&#20107;&#20214;&#26679;&#26412;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#36825;&#20123;&#26679;&#26412;&#34987;&#21019;&#24314;&#20197;&#25511;&#21046;&#22320;&#28436;&#31034;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#24120;&#35265;&#31995;&#32479;&#24046;&#24322;&#12290;LArTPC&#25506;&#27979;&#22120;&#20195;&#34920;&#20102;&#19979;&#19968;&#20195;&#31890;&#23376;&#25506;&#27979;&#22120;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) techniques have broad applications in science, especially in seeking to streamline the pathway to potential solutions and discoveries. Frequently, however, DL models are trained on the results of simulation yet applied to real experimental data. As such, any systematic differences between the simulated and real data may degrade the model's performance -- an effect known as "domain shift." This work studies a toy model of the systematic differences between simulated and real data. It presents a fully unsupervised, task-agnostic method to reduce differences between two systematically different samples. The method is based on the recent advances in unpaired image-to-image translation techniques and is validated on two sets of samples of simulated Liquid Argon Time Projection Chamber (LArTPC) detector events, created to illustrate common systematic differences between the simulated and real data in a controlled way. LArTPC-based detectors represent the next-generation pa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#27010;&#24565; troenpy &#26469;&#37327;&#21270;&#24213;&#23618;&#20998;&#24067;&#30340;&#30830;&#23450;&#24615;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#25991;&#26723;&#20998;&#31867;&#21644;&#24207;&#21015;&#25968;&#25454;&#26435;&#37325;&#26041;&#26696;&#65292;&#24182;&#23450;&#20041;&#20102;&#37327;&#23376; troenpy &#37327;&#21270;&#37327;&#23376;&#31995;&#32479;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12833</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#30830;&#23450;&#24615;&#20449;&#24687;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A New Information Theory of Certainty for Machine Learning. (arXiv:2304.12833v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#27010;&#24565; troenpy &#26469;&#37327;&#21270;&#24213;&#23618;&#20998;&#24067;&#30340;&#30830;&#23450;&#24615;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#25991;&#26723;&#20998;&#31867;&#21644;&#24207;&#21015;&#25968;&#25454;&#26435;&#37325;&#26041;&#26696;&#65292;&#24182;&#23450;&#20041;&#20102;&#37327;&#23376; troenpy &#37327;&#21270;&#37327;&#23376;&#31995;&#32479;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20811;&#21171;&#24503;&#183;&#39321;&#20892;&#25552;&#20986;&#20102;&#29109;&#30340;&#27010;&#24565;&#26469;&#37327;&#21270;&#36890;&#20449;&#32534;&#30721;&#29702;&#35770;&#20013;&#38543;&#26426;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29109;&#30340;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#29305;&#24615;&#20063;&#38480;&#21046;&#20102;&#20854;&#22312;&#25968;&#23398;&#24314;&#27169;&#20013;&#30340;&#30452;&#25509;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565; troenpy&#65292;&#20316;&#20026;&#29109;&#30340;&#35268;&#33539;&#23545;&#20598;&#65292;&#26469;&#37327;&#21270;&#24213;&#23618;&#20998;&#24067;&#30340;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#24212;&#29992;&#12290;&#31532;&#19968;&#20010;&#26159;&#29992;&#20110;&#20256;&#32479;&#30340;&#25991;&#26723;&#20998;&#31867;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110; troenpy &#26435;&#37325;&#26041;&#26696;&#26469;&#21033;&#29992;&#25991;&#26723;&#20998;&#31867;&#26631;&#31614;&#12290;&#31532;&#20108;&#20010;&#26159;&#38024;&#23545;&#24207;&#21015;&#25968;&#25454;&#30340;&#33258;&#25105; troenpy &#26435;&#37325;&#26041;&#26696;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#36731;&#26494;&#22320;&#21253;&#21547;&#22312;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#23454;&#29616;&#26174;&#33879;&#30340;&#22256;&#24785;&#24230;&#38477;&#20302;&#12290;&#25105;&#20204;&#36824;&#23450;&#20041;&#20102;&#37327;&#23376; troenpy &#20316;&#20026; Von Neumann &#29109;&#30340;&#23545;&#20598;&#65292;&#20197;&#37327;&#21270;&#37327;&#23376;&#31995;&#32479;&#30340;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Claude Shannon coined entropy to quantify the uncertainty of a random distribution for communication coding theory. We observe that the uncertainty nature of entropy also limits its direct usage in mathematical modeling. Therefore we propose a new concept troenpy,as the canonical dual of entropy, to quantify the certainty of the underlying distribution. We demonstrate two applications in machine learning. The first is for the classical document classification, we develop a troenpy based weighting scheme to leverage the document class label. The second is a self-troenpy weighting scheme for sequential data and show that it can be easily included in neural network based language models and achieve dramatic perplexity reduction. We also define quantum troenpy as the dual of the Von Neumann entropy to quantify the certainty of quantum systems.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#33647;&#22788;&#26041;&#35268;&#21010;&#27169;&#22411; PrescDRL &#20851;&#27880;&#38271;&#26399;&#30103;&#25928;&#65292;&#23545;&#24930;&#24615;&#30142;&#30149;&#27835;&#30103;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#35780;&#20272;&#34920;&#26126;&#20854;&#36739;&#21307;&#29983;&#27835;&#30103;&#25928;&#26524;&#25552;&#39640;&#20102;117%&#21644;153%&#12290;</title><link>http://arxiv.org/abs/2304.12828</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#33647;&#22788;&#26041;&#35268;&#21010;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A optimization framework for herbal prescription planning based on deep reinforcement learning. (arXiv:2304.12828v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12828
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#33647;&#22788;&#26041;&#35268;&#21010;&#27169;&#22411; PrescDRL &#20851;&#27880;&#38271;&#26399;&#30103;&#25928;&#65292;&#23545;&#24930;&#24615;&#30142;&#30149;&#27835;&#30103;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#35780;&#20272;&#34920;&#26126;&#20854;&#36739;&#21307;&#29983;&#27835;&#30103;&#25928;&#26524;&#25552;&#39640;&#20102;117%&#21644;153%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24930;&#24615;&#30149;&#27835;&#30103;&#35268;&#21010;&#26159;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#20013;&#21307;&#20013;&#33647;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20026;&#19981;&#21516;&#20020;&#24202;&#24773;&#20917;&#19979;&#30340;&#24739;&#32773;&#29983;&#25104;&#20248;&#21270;&#30340;&#24207;&#21015;&#27835;&#30103;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#30340;&#38590;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#33647;&#22788;&#26041;&#35268;&#21010;&#26694;&#26550; (PrescDRL)&#65292;&#29992;&#20110;&#24930;&#24615;&#30142;&#30149;&#27835;&#30103;&#12290;PrescDRL&#26159;&#19968;&#20010;&#24207;&#21015;&#20013;&#33647;&#22788;&#26041;&#20248;&#21270;&#27169;&#22411;&#65292;&#23427;&#20851;&#27880;&#30340;&#26159;&#38271;&#26399;&#30340;&#30103;&#25928;&#65292;&#32780;&#19981;&#26159;&#22312;&#27599;&#19968;&#27493;&#36798;&#21040;&#26368;&#22823;&#30340;&#22870;&#21169;&#65292;&#20174;&#32780;&#30830;&#20445;&#26356;&#22909;&#30340;&#24739;&#32773;&#27835;&#30103;&#25928;&#26524;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#29992;&#20110;&#31958;&#23615;&#30149;&#39034;&#24207;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;PrescDRL&#19982;&#35813;&#22522;&#20934;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21307;&#29983;&#30456;&#27604;&#65292;PrescDRL&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#27835;&#30103;&#25928;&#26524;&#65292;&#21333;&#27493;&#22870;&#21169;&#25552;&#39640;&#20102;117%&#21644;153%&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment planning for chronic diseases is a critical task in medical artificial intelligence, particularly in traditional Chinese medicine (TCM). However, generating optimized sequential treatment strategies for patients with chronic diseases in different clinical encounters remains a challenging issue that requires further exploration. In this study, we proposed a TCM herbal prescription planning framework based on deep reinforcement learning for chronic disease treatment (PrescDRL). PrescDRL is a sequential herbal prescription optimization model that focuses on long-term effectiveness rather than achieving maximum reward at every step, thereby ensuring better patient outcomes. We constructed a high-quality benchmark dataset for sequential diagnosis and treatment of diabetes and evaluated PrescDRL against this benchmark. Our results showed that PrescDRL achieved a higher curative effect, with the single-step reward improving by 117% and 153% compared to doctors. Furthermore, PrescDRL
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#24335;&#20027;&#20041;&#26469;&#25805;&#20316;&#21644;&#20998;&#26512;&#35777;&#26126;&#65292;&#29992;&#20110;&#29983;&#25104;&#26356;&#30701;&#30340;&#35777;&#26126;&#21644;&#20943;&#23569;&#25628;&#32034;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.12827</link><description>&lt;p&gt;
&#35777;&#26126;&#32467;&#26500;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigations into Proof Structures. (arXiv:2304.12827v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12827
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#24335;&#20027;&#20041;&#26469;&#25805;&#20316;&#21644;&#20998;&#26512;&#35777;&#26126;&#65292;&#29992;&#20110;&#29983;&#25104;&#26356;&#30701;&#30340;&#35777;&#26126;&#21644;&#20943;&#23569;&#25628;&#32034;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#19968;&#31181;&#26032;&#22411;&#24418;&#24335;&#20027;&#20041;&#26469;&#25805;&#20316;&#21644;&#20998;&#26512;&#35777;&#26126;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#30340;&#23545;&#35937;&#12290;&#22312;&#36825;&#31532;&#19968;&#27425;&#23581;&#35797;&#20013;&#65292;&#36825;&#20010;&#24418;&#24335;&#20027;&#20041;&#20165;&#38480;&#20110;&#30001;&#27987;&#32553;&#25512;&#23548;&#29305;&#24449;&#30340;&#19968;&#38454;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#19968;&#20010;&#20840;&#38754;&#30340;&#24418;&#24335;&#37325;&#26500;&#21644;&#20998;&#26512;&#21382;&#21490;&#19978;{\L}ukasiewicz&#24191;&#27867;&#30740;&#31350;&#36807;&#30340;&#38382;&#39064;&#30340;&#35777;&#26126;&#20026;&#20363;&#36827;&#34892;&#20102;&#38416;&#36848;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#22312;&#35777;&#26126;&#25628;&#32034;&#36807;&#31243;&#20013;&#29983;&#25104;&#24341;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#25628;&#32034;&#24037;&#20316;&#37327;&#24182;&#25214;&#21040;&#26356;&#30701;&#30340;&#35777;&#26126;&#12290;&#22312;&#36825;&#26465;&#36335;&#32447;&#19978;&#25253;&#21578;&#20102;&#35768;&#22810;&#23454;&#39564;&#65292;&#20854;&#20013;&#33258;&#21160;&#21457;&#29616;&#20102;&#19968;&#20010;&#35777;&#26126;{\L}ukasiewicz&#30340;&#38382;&#39064;&#65292;&#23427;&#27604;&#20197;&#21069;&#20219;&#20309;&#30001;&#20154;&#25110;&#26426;&#22120;&#21457;&#29616;&#30340;&#35777;&#26126;&#37117;&#35201;&#30701;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and elaborate a novel formalism for the manipulation and analysis of proofs as objects in a global manner. In this first approach the formalism is restricted to first-order problems characterized by condensed detachment. It is applied in an exemplary manner to a coherent and comprehensive formal reconstruction and analysis of historical proofs of a widely-studied problem due to {\L}ukasiewicz. The underlying approach opens the door towards new systematic ways of generating lemmas in the course of proof search to the effects of reducing the search effort and finding shorter proofs. Among the numerous reported experiments along this line, a proof of {\L}ukasiewicz's problem was automatically discovered that is much shorter than any proof found before by man or machine.
&lt;/p&gt;</description></item><item><title>GraphVF&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#23558;2D&#25299;&#25169;&#21644;3D&#20960;&#20309;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#21487;&#25511;&#29983;&#25104;&#20855;&#26377;&#32467;&#21512;&#33021;&#21147;&#30340;3D&#20998;&#23376;&#65292;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#29983;&#25104;&#32467;&#21512;&#33021;&#21147;&#30340;3D&#20998;&#23376;&#24182;&#23450;&#21046;&#20854;&#29289;&#21270;&#29305;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.12825</link><description>&lt;p&gt;
GraphVF: &#29992;&#21464;&#20998;&#27969;&#25511;&#21046;&#29983;&#25104;&#29305;&#23450;&#34507;&#30333;&#36136;&#30340;&#19977;&#32500;&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
GraphVF: Controllable Protein-Specific 3D Molecule Generation with Variational Flow. (arXiv:2304.12825v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12825
&lt;/p&gt;
&lt;p&gt;
GraphVF&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#23558;2D&#25299;&#25169;&#21644;3D&#20960;&#20309;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#21487;&#25511;&#29983;&#25104;&#20855;&#26377;&#32467;&#21512;&#33021;&#21147;&#30340;3D&#20998;&#23376;&#65292;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#29983;&#25104;&#32467;&#21512;&#33021;&#21147;&#30340;3D&#20998;&#23376;&#24182;&#23450;&#21046;&#20854;&#29289;&#21270;&#29305;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#19982;&#29305;&#23450;&#30446;&#26631;&#34507;&#30333;&#36136;&#32467;&#21512;&#30340;&#20998;&#23376;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#27169;&#22411;&#21033;&#29992;&#20960;&#20309;&#32422;&#26463;&#26465;&#20214;&#29983;&#25104;&#33021;&#22815;&#19982;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#32039;&#23494;&#32467;&#21512;&#30340;&#37197;&#20307;&#20998;&#23376;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#33021;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;2D&#39592;&#26550;&#36864;&#21270;&#21644;&#29289;&#24615;&#38480;&#21046;&#30340;3D&#20998;&#23376;&#65292;&#36825;&#23545;&#20110;&#33647;&#29289;&#30340;&#25928;&#33021;&#21644;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphVF&#65292;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#27969;&#30340;&#26694;&#26550;&#65292;&#23558;2D&#25299;&#25169;&#21644;3D&#20960;&#20309;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#21487;&#25511;&#29983;&#25104;&#20855;&#26377;&#32467;&#21512;&#33021;&#21147;&#30340;3D&#20998;&#23376;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#29305;&#23450;&#34507;&#30333;&#36136;&#29983;&#25104;&#30340;&#23454;&#38469;&#27425;&#32423;&#32467;&#26500;&#24067;&#23616;&#12290;&#23588;&#20854;&#26159;&#65292;GraphVF&#20195;&#34920;&#20102;&#31532;&#19968;&#20010;&#21487;&#25511;&#30340;&#20960;&#20309;&#24847;&#35782;&#65292;&#29305;&#23450;&#34507;&#30333;&#36136;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#23450;&#21046;&#27425;&#32423;&#32467;&#26500;&#21644;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#30340;&#32467;&#21512;3D&#20998;&#23376;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/Franco-Solis/GraphVF-code&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing molecules that bind to specific target proteins is a fundamental task in drug discovery. Recent models leverage geometric constraints to generate ligand molecules that bind cohesively with specific protein pockets. However, these models cannot effectively generate 3D molecules with 2D skeletal curtailments and property constraints, which are pivotal to drug potency and development. To tackle this challenge, we propose GraphVF, a variational flow-based framework that combines 2D topology and 3D geometry, for controllable generation of binding 3D molecules. Empirically, our method achieves state-of-the-art binding affinity and realistic sub-structural layouts for protein-specific generation. In particular, GraphVF represents the first controllable geometry-aware, protein-specific molecule generation method, which can generate binding 3D molecules with tailored sub-structures and physio-chemical properties. Our code is available at https://github.com/Franco-Solis/GraphVF-code.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22768;&#38899;&#26159;&#21542;&#21487;&#20197;&#20855;&#22791;&#21487;&#29233;&#24615;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;&#21487;&#29233;&#30340;&#35821;&#38899;&#29305;&#36136;&#65292;&#21363;&#21487;&#29233;&#30340;&#22768;&#38899;&#23398;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#24180;&#40836;&#27573;&#21644;&#19981;&#21516;&#29305;&#24449;&#30340;&#35821;&#38899;&#24863;&#21463;&#30740;&#31350;&#65292;&#21457;&#29616;&#21487;&#29233;&#24615;&#19982;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#27969;&#21033;&#24230;&#21644;&#20154;&#24037;&#24615;&#31561;&#26041;&#38754;&#26377;&#20851;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29233;&#21971;&#38899;&#30340;&#27169;&#22411;&#65292;&#38656;&#35201;&#36827;&#34892;&#22768;&#38899;&#36136;&#37327;&#12289;&#35748;&#30693;&#35780;&#20272;&#12289;&#34892;&#20026;&#21453;&#24212;&#21644;&#24773;&#24863;&#25253;&#21578;&#30340;&#30740;&#31350;&#26469;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.12809</link><description>&lt;p&gt;
&#35821;&#38899;&#21161;&#25163;&#21487;&#20197;&#21548;&#36215;&#26469;&#21487;&#29233;&#21527;&#65311;&#36808;&#21521;&#21487;&#29233;&#21971;&#38899;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Can Voice Assistants Sound Cute? Towards a Model of Kawaii Vocalics. (arXiv:2304.12809v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22768;&#38899;&#26159;&#21542;&#21487;&#20197;&#20855;&#22791;&#21487;&#29233;&#24615;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;&#21487;&#29233;&#30340;&#35821;&#38899;&#29305;&#36136;&#65292;&#21363;&#21487;&#29233;&#30340;&#22768;&#38899;&#23398;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#24180;&#40836;&#27573;&#21644;&#19981;&#21516;&#29305;&#24449;&#30340;&#35821;&#38899;&#24863;&#21463;&#30740;&#31350;&#65292;&#21457;&#29616;&#21487;&#29233;&#24615;&#19982;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#27969;&#21033;&#24230;&#21644;&#20154;&#24037;&#24615;&#31561;&#26041;&#38754;&#26377;&#20851;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29233;&#21971;&#38899;&#30340;&#27169;&#22411;&#65292;&#38656;&#35201;&#36827;&#34892;&#22768;&#38899;&#36136;&#37327;&#12289;&#35748;&#30693;&#35780;&#20272;&#12289;&#34892;&#20026;&#21453;&#24212;&#21644;&#24773;&#24863;&#25253;&#21578;&#30340;&#30740;&#31350;&#26469;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#26412;&#30340;&#8220;&#21487;&#29233;&#8221;&#27010;&#24565;&#25110;&#34920;&#36798;&#21487;&#29233;&#12289;&#33030;&#24369;&#21644;/&#25110;&#39749;&#21147;&#30340;&#26041;&#24335;&#26159;&#19968;&#31181;&#20840;&#29699;&#25991;&#21270;&#36755;&#20986;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#20154;&#21644;&#34394;&#25311;&#35282;&#33394;&#30340;&#35270;&#35273;&#22806;&#35266;&#12289;&#38750;&#35821;&#35328;&#34892;&#20026;&#21644;&#22768;&#38899;&#20013;&#25506;&#32034;&#21487;&#29233;&#24615;&#20316;&#20026;&#35774;&#35745;&#29305;&#24449;&#21644;&#29992;&#25143;&#20307;&#39564;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#21021;&#27493;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22768;&#38899;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#25506;&#32034;&#35821;&#38899;&#21161;&#25163;&#35821;&#38899;&#30340;&#22768;&#38899;&#29305;&#36136;&#65292;&#21363;&#21487;&#29233;&#30340;&#22768;&#38899;&#23398;&#65292;&#20855;&#22791;&#21487;&#29233;&#24615;&#12290;&#26681;&#25454;&#19968;&#20010;&#21253;&#21547;&#24180;&#40836;&#30340;&#21487;&#29233;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#24180;&#36731;&#21644;&#24180;&#32769;&#30340;&#26085;&#35821;&#30005;&#33041;&#35821;&#38899;&#30340;&#21487;&#29233;&#24230;&#36827;&#34892;&#20102;&#29992;&#25143;&#24863;&#30693;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#29233;&#24615;&#19982;&#24615;&#21035;&#21644;&#24180;&#40836;&#30340;&#24863;&#30693;&#30456;&#20132;&#65292;&#21363;&#24615;&#21035;&#27169;&#31946;&#21644;&#22899;&#23401;&#27668;&#36136;&#65292;&#20197;&#21450;VA&#30340;&#29305;&#24449;&#65292;&#21363;&#27969;&#21033;&#24230;&#21644;&#20154;&#24037;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29233;&#21971;&#38899;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#30740;&#31350;&#22768;&#38899;&#36136;&#37327;&#12289;&#35748;&#30693;&#35780;&#20272;&#12289;&#34892;&#20026;&#21453;&#24212;&#21644;&#24773;&#24863;&#25253;&#21578;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Japanese notion of "kawaii" or expressions of cuteness, vulnerability, and/or charm is a global cultural export. Work has explored kawaii-ness as a design feature and factor of user experience in the visual appearance, nonverbal behaviour, and sound of robots and virtual characters. In this initial work, we consider whether voices can be kawaii by exploring the vocal qualities of voice assistant speech, i.e., kawaii vocalics. Drawing from an age-inclusive model of kawaii, we ran a user perceptions study on the kawaii-ness of younger- and older-sounding Japanese computer voices. We found that kawaii-ness intersected with perceptions of gender and age, i.e., gender ambiguous and girlish, as well as VA features, i.e., fluency and artificiality. We propose an initial model of kawaii vocalics to be validated through the identification and study of vocal qualities, cognitive appraisals, behavioural responses, and affective reports.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22870;&#21169;&#21152;&#26435;&#21644;&#25439;&#22833;&#21152;&#26435;&#26799;&#24230;&#21512;&#24182;&#65292;&#20197;&#26356;&#22909;&#22320;&#25552;&#39640;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12778</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#21644;&#22870;&#21169;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
Loss and Reward Weighing for increased learning in Distributed Reinforcement Learning. (arXiv:2304.12778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22870;&#21169;&#21152;&#26435;&#21644;&#25439;&#22833;&#21152;&#26435;&#26799;&#24230;&#21512;&#24182;&#65292;&#20197;&#26356;&#22909;&#22320;&#25552;&#39640;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#20013;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#21363;&#22870;&#21169;&#21152;&#26435;&#65288;R-Weighted&#65289;&#21644;&#25439;&#22833;&#21152;&#26435;&#65288;L-Weighted&#65289;&#26799;&#24230;&#21512;&#24182;&#12290; R / L &#21152;&#26435;&#26041;&#27861;&#26367;&#25442;&#20102;&#35757;&#32451;&#22810;&#20010;&#20195;&#29702;&#30340;&#26631;&#20934;&#23454;&#36341;&#65292;&#20363;&#22914;&#23545;&#26799;&#24230;&#27714;&#21644;&#25110;&#24179;&#22343;&#12290;&#27599;&#20010;&#20195;&#29702;&#22312;&#19981;&#21516;&#21021;&#22987;&#21270;&#29256;&#26412;&#30340;&#30456;&#21516;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#36825;&#20250;&#20174;&#19981;&#21516;&#30340;actor&#33719;&#24471;&#19981;&#21516;&#30340;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces two learning schemes for distributed agents in Reinforcement Learning (RL) environments, namely Reward-Weighted (R-Weighted) and Loss-Weighted (L-Weighted) gradient merger. The R/L weighted methods replace standard practices for training multiple agents, such as summing or averaging the gradients. The core of our methods is to scale the gradient of each actor based on how high the reward (for R-Weighted) or the loss (for L-Weighted) is compared to the other actors. During training, each agent operates in differently initialized versions of the same environment, which gives different gradients from different actors. In essence, the R-Weights and L-Weights of each agent inform the other agents of its potential, which again reports which environment should be prioritized for learning. This approach of distributed learning is possible because environments that yield higher rewards, or low losses, have more critical information than environments that yield lower reward
&lt;/p&gt;</description></item><item><title>S4&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#19982;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#27604;&#23384;&#22312;&#22235;&#20010;BLEU&#20998;&#25968;&#28857;&#30340;&#24046;&#36317;&#65292;&#38656;&#35201;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#24357;&#34917;&#20854;&#26080;&#27861;&#22312;&#21333;&#20010;&#38544;&#34255;&#29366;&#24577;&#20013;&#24635;&#32467;&#23436;&#25972;&#30340;&#28304;&#21477;&#23376;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2304.12776</link><description>&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#19981;&#22815;&#29992;&#65306;&#26426;&#22120;&#32763;&#35793;&#38656;&#35201;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
State Spaces Aren't Enough: Machine Translation Needs Attention. (arXiv:2304.12776v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12776
&lt;/p&gt;
&lt;p&gt;
S4&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#19982;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#27604;&#23384;&#22312;&#22235;&#20010;BLEU&#20998;&#25968;&#28857;&#30340;&#24046;&#36317;&#65292;&#38656;&#35201;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#24357;&#34917;&#20854;&#26080;&#27861;&#22312;&#21333;&#20010;&#38544;&#34255;&#29366;&#24577;&#20013;&#24635;&#32467;&#23436;&#25972;&#30340;&#28304;&#21477;&#23376;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#30340;&#32467;&#26500;&#29366;&#24577;&#31354;&#38388;&#65288;S4&#65289;&#26159;&#19968;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26377;&#25104;&#21151;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#35270;&#35273;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#38899;&#39057;&#12290;&#30001;&#20110;&#23427;&#30340;&#25968;&#23398;&#20844;&#24335;&#65292;&#23427;&#23558;&#20854;&#36755;&#20837;&#21387;&#32553;&#20026;&#19968;&#20010;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#33021;&#22815;&#25429;&#33719;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38656;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;S4&#24212;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#65292;&#24182;&#22312;WMT'14&#21644;WMT'16&#19978;&#35780;&#20272;&#20102;&#20960;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#20307;&#12290;&#19982;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#25104;&#21151;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;S4&#22312;BLEU&#28857;&#25968;&#19978;&#33853;&#21518;&#20110;&#21464;&#21387;&#22120;&#32422;4&#20010;&#28857;&#65292;&#24182;&#19988;&#20196;&#20154;&#24863;&#21040;&#22256;&#24785;&#30340;&#26159;&#65292;&#23427;&#22312;&#22788;&#29702;&#38271;&#21477;&#26102;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24046;&#36317;&#26159;&#30001;&#20110;S4&#26080;&#27861;&#22312;&#21333;&#20010;&#38544;&#34255;&#29366;&#24577;&#20013;&#24635;&#32467;&#23436;&#25972;&#30340;&#28304;&#21477;&#23376;&#25152;&#33268;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured State Spaces for Sequences (S4) is a recently proposed sequence model with successful applications in various tasks, e.g. vision, language modeling, and audio. Thanks to its mathematical formulation, it compresses its input to a single hidden state, and is able to capture long range dependencies while avoiding the need for an attention mechanism. In this work, we apply S4 to Machine Translation (MT), and evaluate several encoder-decoder variants on WMT'14 and WMT'16. In contrast with the success in language modeling, we find that S4 lags behind the Transformer by approximately 4 BLEU points, and that it counter-intuitively struggles with long sentences. Finally, we show that this gap is caused by S4's inability to summarize the full source sentence in a single hidden state, and show that we can close the gap by introducing an attention mechanism.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#8220;&#29289;&#31181;&#36135;&#24065;&#8221;&#30340;&#25968;&#23383;&#31649;&#29702;&#21644;&#22870;&#21169;&#31995;&#32479;&#65292;&#20197;&#25903;&#25345;&#37326;&#29983;&#21160;&#29289;&#23432;&#25252;&#32773;&#30340;&#20445;&#25252;&#24037;&#20316;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#25968;&#25454;&#26469;&#30417;&#30563;&#21644;&#20445;&#25252;&#23427;&#20204;&#12290;</title><link>http://arxiv.org/abs/2304.12703</link><description>&lt;p&gt;
&#25480;&#26435;&#37326;&#29983;&#21160;&#29289;&#23432;&#25252;&#32773;&#65306;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644; 3/4G &#25668;&#20687;&#26426;&#38519;&#38449;&#30340;&#20844;&#24179;&#25968;&#23383;&#31649;&#29702;&#21644;&#22870;&#21169;&#31995;&#32479;&#36827;&#34892;&#29983;&#29289;&#22810;&#26679;&#24615;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Empowering Wildlife Guardians: An Equitable Digital Stewardship and Reward System for Biodiversity Conservation using Deep Learning and 3/4G Camera Traps. (arXiv:2304.12703v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#8220;&#29289;&#31181;&#36135;&#24065;&#8221;&#30340;&#25968;&#23383;&#31649;&#29702;&#21644;&#22870;&#21169;&#31995;&#32479;&#65292;&#20197;&#25903;&#25345;&#37326;&#29983;&#21160;&#29289;&#23432;&#25252;&#32773;&#30340;&#20445;&#25252;&#24037;&#20316;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#25968;&#25454;&#26469;&#30417;&#30563;&#21644;&#20445;&#25252;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22320;&#29699;&#19978;&#30340;&#29983;&#29289;&#22810;&#26679;&#24615;&#27491;&#38754;&#20020;&#23041;&#32961;&#65292;&#39044;&#35745;&#36817;&#19968;&#30334;&#19975;&#29289;&#31181;&#23558;&#22312;&#20960;&#21313;&#24180;&#20869;&#28781;&#32477;&#12290;&#36825;&#26159;&#30001;&#20110;&#20154;&#31867;&#30340;&#36127;&#38754;&#34892;&#20026;&#65292;&#21253;&#25324;&#29417;&#29454;&#12289;&#36807;&#24230;&#25429;&#25438;&#12289;&#27745;&#26579;&#20197;&#21450;&#20026;&#22478;&#24066;&#21270;&#21644;&#20892;&#19994;&#30446;&#30340;&#32780;&#36716;&#21270;&#22303;&#22320;&#31561;&#12290;&#23613;&#31649;&#24904;&#21892;&#26426;&#26500;&#21644;&#25919;&#24220;&#22312;&#26377;&#30410;&#33258;&#28982;&#30340;&#27963;&#21160;&#19978;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#36164;&#37329;&#65292;&#20840;&#29699;&#37326;&#29983;&#21160;&#29289;&#31181;&#32676;&#20173;&#28982;&#19981;&#26029;&#19979;&#38477;&#12290;&#24403;&#22320;&#30340;&#37326;&#29983;&#21160;&#29289;&#23432;&#25252;&#32773;&#21382;&#21490;&#19978;&#22312;&#20840;&#29699;&#20445;&#25252;&#24037;&#20316;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#20182;&#20204;&#22312;&#21508;&#20010;&#23618;&#27425;&#19978;&#23454;&#29616;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#33021;&#21147;&#12290;2021&#24180;&#65292;&#32852;&#21512;&#22269;&#27668;&#20505;&#21464;&#21270;&#22823;&#20250;&#65288;COP26&#65289;&#35748;&#21487;&#20102;&#20182;&#20204;&#30340;&#36129;&#29486;&#65292;&#24182;&#25215;&#35834;&#27599;&#24180;&#25552;&#20379;17&#20159;&#32654;&#20803;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#20182;&#20204;&#20445;&#25252;&#20102;&#22320;&#29699;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;80&#65285;&#65292;&#36825;&#21482;&#26159;&#20840;&#29699;&#29983;&#29289;&#22810;&#26679;&#24615;&#39044;&#31639;&#65288;&#27599;&#24180;&#32422;1240&#20159;&#33267;1430&#20159;&#32654;&#20803;&#65289;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#29289;&#31181;&#36135;&#24065;&#8221;&#30340;&#28608;&#36827;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#21160;&#29289;&#25317;&#26377;&#33258;&#24049;&#30340;&#36135;&#24065;&#12290;&#36890;&#36807;&#20026;&#37326;&#29983;&#21160;&#29289;&#21019;&#24314;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#25968;&#25454;&#26469;&#30417;&#30563;&#21644;&#20445;&#25252;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
The biodiversity of our planet is under threat, with approximately one million species expected to become extinct within decades. The reason; negative human actions, which include hunting, overfishing, pollution, and the conversion of land for urbanisation and agricultural purposes. Despite significant investment from charities and governments for activities that benefit nature, global wildlife populations continue to decline. Local wildlife guardians have historically played a critical role in global conservation efforts and have shown their ability to achieve sustainability at various levels. In 2021, COP26 recognised their contributions and pledged US$1.7 billion per year; however, this is a fraction of the global biodiversity budget available (between US$124 billion and US$143 billion annually) given they protect 80% of the planets biodiversity. This paper proposes a radical new solution based on "Interspecies Money," where animals own their own money. Creating a digital twin for e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#26800;&#21270;&#35299;&#37322;&#65292;&#28041;&#21450;&#24847;&#20041;&#12289;&#20132;&#27969;&#21644;&#31526;&#21495;&#20986;&#29616;&#30340;&#29616;&#35937;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#24182;&#19981;&#20855;&#22791;&#19982;&#20154;&#31867;&#30456;&#21516;&#30340;&#24847;&#20041;&#29702;&#35299;&#65292;&#20316;&#32773;&#24314;&#35758;&#27169;&#25311;&#20154;&#31867;&#24773;&#24863;&#24182;&#20248;&#21270;&#27169;&#22411;&#20197;&#26500;&#24314;&#24369;&#34920;&#31034;&#24418;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12686</link><description>&lt;p&gt;
&#20851;&#20110;&#24847;&#20041;&#35745;&#31639;&#12289;&#35821;&#35328;&#27169;&#22411;&#19982;&#19981;&#21487;&#29702;&#35299;&#24656;&#24807;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Computation of Meaning, Language Models and Incomprehensible Horrors. (arXiv:2304.12686v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#26800;&#21270;&#35299;&#37322;&#65292;&#28041;&#21450;&#24847;&#20041;&#12289;&#20132;&#27969;&#21644;&#31526;&#21495;&#20986;&#29616;&#30340;&#29616;&#35937;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#24182;&#19981;&#20855;&#22791;&#19982;&#20154;&#31867;&#30456;&#21516;&#30340;&#24847;&#20041;&#29702;&#35299;&#65292;&#20316;&#32773;&#24314;&#35758;&#27169;&#25311;&#20154;&#31867;&#24773;&#24863;&#24182;&#20248;&#21270;&#27169;&#22411;&#20197;&#26500;&#24314;&#24369;&#34920;&#31034;&#24418;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#24847;&#20041;&#30340;&#22522;&#30784;&#29702;&#35770;&#19982;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#25968;&#23398;&#24418;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#26800;&#21270;&#35299;&#37322;&#65292;&#28041;&#21450;&#24847;&#20041;&#12289;&#20132;&#27969;&#21644;&#31526;&#21495;&#20986;&#29616;&#30340;&#29616;&#35937;&#65292;&#36825;&#23545;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#20851;&#20110;&#35821;&#35328;&#26412;&#36136;&#30340;&#36777;&#35770;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#30740;&#31350;&#26426;&#22120;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#33021;&#22815;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#35805;&#35821;&#25110;&#29702;&#35299;&#20154;&#31867;&#30340;&#24847;&#20041;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#24182;&#19981;&#20855;&#22791;&#19982;&#20154;&#31867;&#30456;&#21516;&#30340;&#24847;&#20041;&#29702;&#35299;&#65292;&#20063;&#19981;&#24847;&#21619;&#30528;&#23427;&#20204;&#30340;&#22238;&#31572;&#20855;&#26377;&#25105;&#20204;&#25152;&#24402;&#23646;&#30340;&#20219;&#20309;&#24847;&#20041;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#27169;&#25311;&#20154;&#31867;&#24773;&#24863;&#24182;&#20248;&#21270;&#27169;&#22411;&#20197;&#26500;&#24314;&#24369;&#34920;&#31034;&#24418;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#26524;&#25581;&#31034;&#20102;&#24847;&#20041;&#19982;...
&lt;/p&gt;
&lt;p&gt;
We integrate foundational theories of meaning with a mathematical formalism of artificial general intelligence (AGI) to offer a comprehensive mechanistic explanation of meaning, communication, and symbol emergence. This synthesis holds significance for both AGI and broader debates concerning the nature of language, as it unifies pragmatics, logical truth conditional semantics, Peircean semiotics, and a computable model of enactive cognition, addressing phenomena that have traditionally evaded mechanistic explanation. By examining the conditions under which a machine can generate meaningful utterances or comprehend human meaning, we establish that the current generation of language models do not possess the same understanding of meaning as humans nor intend any meaning that we might attribute to their responses. To address this, we propose simulating human feelings and optimising models to construct weak representations. Our findings shed light on the relationship between meaning and in
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#21333;&#24103;&#28145;&#24230;&#21644;&#22810;&#24103;&#28145;&#24230;&#26041;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20687;&#32032;&#36880;&#20687;&#32032;&#33258;&#36866;&#24212;&#28145;&#24230;&#37319;&#26679;&#27169;&#22359;&#65292;&#20197;&#21333;&#24103;&#28145;&#24230;&#20026;&#25351;&#23548;&#26469;&#35757;&#32451;&#22810;&#24103;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#37325;&#25237;&#24433;&#20026;&#22522;&#30784;&#30340;&#33976;&#39311;&#26041;&#27861;&#26469;&#20248;&#21270;&#21333;&#24103;&#28145;&#24230;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.12685</link><description>&lt;p&gt;
&#25506;&#32034;&#33258;&#30417;&#30563;&#21333;&#24103;&#21644;&#22810;&#24103;&#28145;&#24230;&#20272;&#35745;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Mutual Influence between Self-Supervised Single-Frame and Multi-Frame Depth Estimation. (arXiv:2304.12685v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12685
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#21333;&#24103;&#28145;&#24230;&#21644;&#22810;&#24103;&#28145;&#24230;&#26041;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20687;&#32032;&#36880;&#20687;&#32032;&#33258;&#36866;&#24212;&#28145;&#24230;&#37319;&#26679;&#27169;&#22359;&#65292;&#20197;&#21333;&#24103;&#28145;&#24230;&#20026;&#25351;&#23548;&#26469;&#35757;&#32451;&#22810;&#24103;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#37325;&#25237;&#24433;&#20026;&#22522;&#30784;&#30340;&#33976;&#39311;&#26041;&#27861;&#26469;&#20248;&#21270;&#21333;&#24103;&#28145;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#30417;&#30563;&#21333;&#24103;&#21644;&#22810;&#24103;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#37117;&#21482;&#38656;&#35201;&#26080;&#26631;&#31614;&#21333;&#30446;&#35270;&#39057;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#23427;&#20204;&#25152;&#21033;&#29992;&#30340;&#20449;&#24687;&#19981;&#21516;&#65292;&#21333;&#24103;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;&#22806;&#35980;&#30340;&#29305;&#24449;&#65292;&#32780;&#22810;&#24103;&#26041;&#27861;&#21017;&#19987;&#27880;&#20110;&#20960;&#20309;&#32447;&#32034;&#12290;&#32771;&#34385;&#21040;&#21333;&#24103;&#21644;&#22810;&#24103;&#26041;&#27861;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#21033;&#29992;&#21333;&#24103;&#28145;&#24230;&#26469;&#25913;&#36827;&#22810;&#24103;&#28145;&#24230;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#26082;&#19981;&#33021;&#21033;&#29992;&#21333;&#24103;&#28145;&#24230;&#21644;&#22810;&#24103;&#28145;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25913;&#36827;&#22810;&#24103;&#28145;&#24230;&#65292;&#20063;&#19981;&#33021;&#21033;&#29992;&#22810;&#24103;&#28145;&#24230;&#26469;&#20248;&#21270;&#21333;&#24103;&#28145;&#24230;&#27169;&#22411;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#21333;&#24103;&#21644;&#22810;&#24103;&#26041;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#20010;&#30001;&#21333;&#24103;&#28145;&#24230;&#25351;&#23548;&#30340;&#20687;&#32032;&#36880;&#20687;&#32032;&#33258;&#36866;&#24212;&#28145;&#24230;&#37319;&#26679;&#27169;&#22359;&#26469;&#35757;&#32451;&#22810;&#24103;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#23567;&#37325;&#25237;&#24433;&#20026;&#22522;&#30784;&#30340;&#33976;&#39311;&#26041;&#27861;&#26469;&#21033;&#29992;&#22810;&#24103;&#28145;&#24230;&#20248;&#21270;&#21333;&#24103;&#28145;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although both self-supervised single-frame and multi-frame depth estimation methods only require unlabeled monocular videos for training, the information they leverage varies because single-frame methods mainly rely on appearance-based features while multi-frame methods focus on geometric cues. Considering the complementary information of single-frame and multi-frame methods, some works attempt to leverage single-frame depth to improve multi-frame depth. However, these methods can neither exploit the difference between single-frame depth and multi-frame depth to improve multi-frame depth nor leverage multi-frame depth to optimize single-frame depth models. To fully utilize the mutual influence between single-frame and multi-frame methods, we propose a novel self-supervised training framework. Specifically, we first introduce a pixel-wise adaptive depth sampling module guided by single-frame depth to train the multi-frame model. Then, we leverage the minimum reprojection based distillat
&lt;/p&gt;</description></item><item><title>&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#22810;&#26679;&#24615;&#20250;&#23545;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#20135;&#29983;&#28508;&#22312;&#20998;&#27495;&#38382;&#39064;&#65292;&#24182;&#21487;&#33021;&#25513;&#30422;&#25110;&#28129;&#21270;&#21407;&#22987;&#27169;&#22411;&#20013;&#30340;&#25935;&#24863;&#21644;&#19981;&#20844;&#24179;&#29305;&#24449;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#31649;&#29702;&#36825;&#31181;&#24046;&#24322;&#65292;&#20197;&#38450;&#27490;&#27450;&#39575;&#65292;&#24182;&#30830;&#20445;XAI&#30340;&#36947;&#24503;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.12667</link><description>&lt;p&gt;
&#35770;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20998;&#27495;&#65306;&#36879;&#26126;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disagreement amongst counterfactual explanations: How transparency can be deceptive. (arXiv:2304.12667v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12667
&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#22810;&#26679;&#24615;&#20250;&#23545;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#20135;&#29983;&#28508;&#22312;&#20998;&#27495;&#38382;&#39064;&#65292;&#24182;&#21487;&#33021;&#25513;&#30422;&#25110;&#28129;&#21270;&#21407;&#22987;&#27169;&#22411;&#20013;&#30340;&#25935;&#24863;&#21644;&#19981;&#20844;&#24179;&#29305;&#24449;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#31649;&#29702;&#36825;&#31181;&#24046;&#24322;&#65292;&#20197;&#38450;&#27490;&#27450;&#39575;&#65292;&#24182;&#30830;&#20445;XAI&#30340;&#36947;&#24503;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#20316;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#25216;&#26415;&#20043;&#19968;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#20026;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#35299;&#37322;&#12290;&#23613;&#31649;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#31639;&#27861;&#65292;&#20294;&#24182;&#38750;&#27599;&#20010;&#31639;&#27861;&#37117;&#33021;&#20026;&#21516;&#19968;&#23454;&#20363;&#21019;&#24314;&#19968;&#33268;&#30340;&#35299;&#37322;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22810;&#20010;&#21487;&#33021;&#30340;&#35299;&#37322;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#22312;&#19968;&#20123;&#22330;&#26223;&#19979;&#65292;&#21453;&#20107;&#23454;&#35299;&#37322;&#20043;&#38388;&#30340;&#24046;&#24322;&#20250;&#23548;&#33268;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#28508;&#22312;&#20998;&#27495;&#38382;&#39064;&#12290;&#24403;&#24694;&#24847;&#20027;&#20307;&#21033;&#29992;&#36825;&#31181;&#24046;&#24322;&#26469;&#25513;&#30422;&#25935;&#24863;&#29305;&#24449;&#65292;&#20844;&#27491;&#25513;&#39280;&#19981;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#36947;&#24503;&#38382;&#39064;&#12290;&#38543;&#30528;&#20840;&#29699;&#31435;&#27861;&#32773;&#24320;&#22987;&#23558;&#35299;&#37322;&#25968;&#25454;&#39537;&#21160;&#39640;&#39118;&#38505;&#20915;&#31574;&#30340;&#26435;&#21033;&#32435;&#20837;&#20854;&#25919;&#31574;&#20013;&#65292;&#24212;&#35813;&#20102;&#35299;&#21644;&#35299;&#20915;&#36825;&#20123;&#20262;&#29702;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#21453;&#20107;&#23454;&#35299;&#37322;&#20043;&#38388;&#30340;&#20998;&#27495;&#38382;&#39064;&#36827;&#34892;&#30340;&#25991;&#29486;&#32508;&#36848;&#34920;&#26126;&#65292;&#35299;&#37322;&#21487;&#33021;&#26159;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#65306;&#22810;&#26679;&#21270;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#21487;&#33021;&#20250;&#25513;&#30422;&#25110;&#28129;&#21270;&#21407;&#22987;&#27169;&#22411;&#20013;&#30340;&#25935;&#24863;&#21644;&#19981;&#20844;&#24179;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#31649;&#29702;&#21453;&#20107;&#23454;&#35299;&#37322;&#20043;&#38388;&#24046;&#24322;&#30340;&#35770;&#28857;&#65292;&#20197;&#38450;&#27490;&#27450;&#39575;&#65292;&#24182;&#30830;&#20445;XAI&#30340;&#36947;&#24503;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations are increasingly used as an Explainable Artificial Intelligence (XAI) technique to provide stakeholders of complex machine learning algorithms with explanations for data-driven decisions. The popularity of counterfactual explanations resulted in a boom in the algorithms generating them. However, not every algorithm creates uniform explanations for the same instance. Even though in some contexts multiple possible explanations are beneficial, there are circumstances where diversity amongst counterfactual explanations results in a potential disagreement problem among stakeholders. Ethical issues arise when for example, malicious agents use this diversity to fairwash an unfair machine learning model by hiding sensitive features. As legislators worldwide tend to start including the right to explanations for data-driven, high-stakes decisions in their policies, these ethical issues should be understood and addressed. Our literature review on the disagreement probl
&lt;/p&gt;</description></item><item><title>CoDi &#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#20849;&#21516;&#28436;&#21270;&#30340;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#21333;&#29420;&#22788;&#29702;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#24182;&#30456;&#20114;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#32465;&#23450;&#65292;&#23637;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12654</link><description>&lt;p&gt;
CoDi: &#28151;&#21512;&#31867;&#22411;&#34920;&#26684;&#29983;&#25104;&#30340;&#20849;&#21516;&#28436;&#21270;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis. (arXiv:2304.12654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12654
&lt;/p&gt;
&lt;p&gt;
CoDi &#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#20849;&#21516;&#28436;&#21270;&#30340;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#21333;&#29420;&#22788;&#29702;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#24182;&#30456;&#20114;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#32465;&#23450;&#65292;&#23637;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#27880;&#24847;&#21147;&#34987;&#25918;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#65292;&#23558;&#32508;&#21512;&#34920;&#26684;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;&#23581;&#35797;&#24050;&#32463;&#21521;&#21508;&#31181;&#22330;&#26223;&#25193;&#23637;&#12290;&#30001;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#34920;&#26684;&#25968;&#25454;&#32508;&#21512;&#27169;&#22411;&#29983;&#25104;&#30340;&#34394;&#20551;&#25968;&#25454;&#21464;&#24471;&#22797;&#26434;&#32780;&#30495;&#23454;&#12290;&#20294;&#26159;&#65292;&#24314;&#27169;&#34920;&#26684;&#25968;&#25454;&#30340;&#31163;&#25955;&#21464;&#37327;&#65288;&#21015;&#65289;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20004;&#20010;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#21333;&#29420;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#21464;&#37327;&#65288;&#20294;&#30456;&#20114;&#26465;&#20214;&#21270;&#65289;&#12290;&#20004;&#20010;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#24444;&#27492;&#35835;&#21462;&#26465;&#20214;&#22312;&#35757;&#32451;&#20013;&#20849;&#21516;&#28436;&#21270;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#32465;&#23450;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36127;&#37319;&#26679;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;11&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#21644;8&#20010;&#22522;&#20934;&#26041;&#27861;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861; CoDi &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With growing attention to tabular data these days, the attempt to apply a synthetic table to various tasks has been expanded toward various scenarios. Owing to the recent advances in generative modeling, fake data generated by tabular data synthesis models become sophisticated and realistic. However, there still exists a difficulty in modeling discrete variables (columns) of tabular data. In this work, we propose to process continuous and discrete variables separately (but being conditioned on each other) by two diffusion models. The two diffusion models are co-evolved during training by reading conditions from each other. In order to further bind the diffusion models, moreover, we introduce a contrastive learning method with a negative sampling method. In our experiments with 11 real-world tabular datasets and 8 baseline methods, we prove the efficacy of the proposed method, called CoDi.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#26469;&#25429;&#33719;&#21608;&#22260;&#37051;&#23621;&#26234;&#33021;&#20307;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#37096;&#20998;&#21487;&#35266;&#23519;MARL&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12653</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention. (arXiv:2304.12653v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#26469;&#25429;&#33719;&#21608;&#22260;&#37051;&#23621;&#26234;&#33021;&#20307;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#37096;&#20998;&#21487;&#35266;&#23519;MARL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38590;&#20197;&#22312;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#24212;&#29992;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#22343;&#22330;&#29702;&#35770;&#25552;&#39640;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#35266;&#23519;&#21040;&#22266;&#23450;&#33539;&#22260;&#20869;&#30340;&#20854;&#20182;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#24433;&#21709;&#20102;&#26234;&#33021;&#20307;&#35780;&#20272;&#21608;&#22260;&#26234;&#33021;&#20307;&#34892;&#21160;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#24320;&#21457;&#19968;&#31181;&#20174;&#23616;&#37096;&#35266;&#27979;&#20013;&#33719;&#21462;&#26356;&#26377;&#25928;&#20449;&#24687;&#20197;&#36873;&#25321;&#26356;&#26377;&#25928;&#34892;&#21160;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#20197;&#21069;&#24037;&#20316;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#25110;&#21152;&#26435;&#22343;&#22330;&#26469;&#26356;&#26032;&#37051;&#23621;&#26234;&#33021;&#20307;&#24179;&#22343;&#34892;&#21160;&#65292;&#20294;&#23427;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#21608;&#22260;&#37051;&#23621;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#23548;&#33268;&#20102;&#23616;&#37096;&#26368;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#23427;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#26469;&#25429;&#33719;&#21608;&#22260;&#37051;&#23621;&#26234;&#33021;&#20307;&#30340;&#29305;&#24449;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#37096;&#20998;&#21487;&#35266;&#23519;MARL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional multi-agent reinforcement learning algorithms are difficultly applied in a large-scale multi-agent environment. The introduction of mean field theory has enhanced the scalability of multi-agent reinforcement learning in recent years. This paper considers partially observable multi-agent reinforcement learning (MARL), where each agent can only observe other agents within a fixed range. This partial observability affects the agent's ability to assess the quality of the actions of surrounding agents. This paper focuses on developing a method to capture more effective information from local observations in order to select more effective actions. Previous work in this field employs probability distributions or weighted mean field to update the average actions of neighborhood agents, but it does not fully consider the feature information of surrounding neighbors and leads to a local optimum. In this paper, we propose a novel multi-agent reinforcement learning algorithm, Partially
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Q&#30340;&#31574;&#30053;&#35268;&#21017;&#26063;&#20013;&#30340;&#22343;&#34913;&#20559;&#24046;&#65288;&#25110; Qb-equilibria&#65289;&#65292;&#21363;Q&#20540;&#22312;&#19981;&#21516;&#30417;&#27979;&#25216;&#26415;&#19979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12647</link><description>&lt;p&gt;
&#22522;&#20110;Q&#30340;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Q-based Equilibria. (arXiv:2304.12647v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Q&#30340;&#31574;&#30053;&#35268;&#21017;&#26063;&#20013;&#30340;&#22343;&#34913;&#20559;&#24046;&#65288;&#25110; Qb-equilibria&#65289;&#65292;&#21363;Q&#20540;&#22312;&#19981;&#21516;&#30417;&#27979;&#25216;&#26415;&#19979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;Q&#23398;&#20064;&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#35268;&#21017;&#65292;&#20854;&#20026;&#27599;&#20010;&#26367;&#20195;&#26041;&#26696;&#25552;&#20379;&#20272;&#35745;&#20540;(&#21363;Q&#20540;)&#65292;&#35813;&#20540;&#19982;&#20043;&#21069;&#30340;&#20915;&#31574;&#30456;&#20851;&#12290;&#19968;&#20010;&#26420;&#32032;&#30340;&#31574;&#30053;&#26159;&#22987;&#32456;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;Q&#20540;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#26063;&#22522;&#20110;Q&#30340;&#31574;&#30053;&#35268;&#21017;&#65292;&#36825;&#20123;&#35268;&#21017;&#21487;&#33021;&#31995;&#32479;&#22320;&#25903;&#25345;&#26576;&#20123;&#26367;&#20195;&#26041;&#26696;&#32780;&#19981;&#26159;&#20854;&#20182;&#26367;&#20195;&#26041;&#26696;&#65292;&#20363;&#22914;&#21253;&#21547;&#26377;&#21033;&#21512;&#20316;&#30340;&#23485;&#23481;&#20559;&#24046;&#30340;&#35268;&#21017;&#12290;&#22312; Compte &#21644; Postlewaite [2018] &#30340;&#31934;&#31070;&#19979;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010; Q-based &#35268;&#21017;&#26063;&#20013;&#23547;&#25214;&#22343;&#34913;&#20559;&#24046;&#65288;&#25110; Qb-equilibria&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30417;&#27979;&#25216;&#26415;&#19979;&#30340;&#32463;&#20856;&#21338;&#24328;&#12290;
&lt;/p&gt;
&lt;p&gt;
In dynamic environments, Q-learning is an adaptative rule that provides an estimate (a Q-value) of the continuation value associated with each alternative. A naive policy consists in always choosing the alternative with highest Q-value. We consider a family of Q-based policy rules that may systematically favor some alternatives over others, for example rules that incorporate a leniency bias that favors cooperation. In the spirit of Compte and Postlewaite [2018], we look for equilibrium biases (or Qb-equilibria) within this family of Q-based rules. We examine classic games under various monitoring technologies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;DaeMon&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#26597;&#35810;&#20027;&#39064;&#21644;&#27599;&#20010;&#23545;&#35937;&#20505;&#36873;&#20043;&#38388;&#30340;&#26102;&#38388;&#36335;&#24452;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;TKG&#23454;&#20307;&#25968;&#37327;&#24222;&#22823;&#21644;&#23454;&#26102;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#23454;&#20307;&#34920;&#36798;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#35760;&#24518;&#30340;&#36873;&#25321;&#24615;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.12604</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#36335;&#24452;&#35760;&#24518;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Adaptive Path-Memory Network for Temporal Knowledge Graph Reasoning. (arXiv:2304.12604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12604
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;DaeMon&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#26597;&#35810;&#20027;&#39064;&#21644;&#27599;&#20010;&#23545;&#35937;&#20505;&#36873;&#20043;&#38388;&#30340;&#26102;&#38388;&#36335;&#24452;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;TKG&#23454;&#20307;&#25968;&#37327;&#24222;&#22823;&#21644;&#23454;&#26102;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#23454;&#20307;&#34920;&#36798;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#35760;&#24518;&#30340;&#36873;&#25321;&#24615;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20449;&#24687;&#39044;&#27979;&#26410;&#26469;&#30340;&#20002;&#22833;&#20107;&#23454;&#65292;&#24182;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#22823;&#37327;&#24037;&#20316;&#24050;&#32463;&#33268;&#21147;&#20110;&#27169;&#25311;&#25512;&#29702;&#20219;&#21153;&#30340;&#21382;&#21490;&#32467;&#26500;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#22330;&#26223;&#19979;&#30340;TKG&#23454;&#20307;&#25968;&#37327;&#30456;&#24403;&#21487;&#35266;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#20250;&#20986;&#29616;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#23454;&#20307;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#24314;&#27169;TKG&#30340;&#20851;&#31995;&#29305;&#24449;&#65292;&#21363;aDAptivE path-MemOry Network&#65288;DaeMon&#65289;&#65292;&#23427;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#26597;&#35810;&#20027;&#39064;&#19982;&#27599;&#20010;&#23545;&#35937;&#20505;&#36873;&#20043;&#38388;&#30340;&#26102;&#38388;&#36335;&#24452;&#20449;&#24687;&#12290;&#23427;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#23454;&#20307;&#34920;&#36798;&#30340;&#24773;&#20917;&#19979;&#27169;&#25311;&#21382;&#21490;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DaeMon&#20351;&#29992;&#36335;&#24452;&#20869;&#23384;&#35760;&#24405;&#20174;&#26102;&#38388;&#36724;&#19978;&#30340;&#36335;&#24452;&#27719;&#32858;&#21333;&#20803;&#24471;&#21040;&#30340;&#26102;&#38388;&#36335;&#24452;&#20449;&#24687;&#65292;&#32771;&#34385;&#27599;&#26465;&#36335;&#24452;&#30340;&#39034;&#24207;&#21644;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#35760;&#24518;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal knowledge graph (TKG) reasoning aims to predict the future missing facts based on historical information and has gained increasing research interest recently. Lots of works have been made to model the historical structural and temporal characteristics for the reasoning task. Most existing works model the graph structure mainly depending on entity representation. However, the magnitude of TKG entities in real-world scenarios is considerable, and an increasing number of new entities will arise as time goes on. Therefore, we propose a novel architecture modeling with relation feature of TKG, namely aDAptivE path-MemOry Network (DaeMon), which adaptively models the temporal path information between query subject and each object candidate across history time. It models the historical information without depending on entity representation. Specifically, DaeMon uses path memory to record the temporal path information derived from path aggregation unit across timeline considering the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35270;&#35282;&#29289;&#20307;&#22534;&#21472;&#22330;&#26223;&#20013;&#25805;&#20316;&#20851;&#31995;&#26816;&#27979;&#30340;&#22810;&#35270;&#35282;MRD&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;2D&#21644;3D&#22810;&#35270;&#35282;&#25968;&#25454;&#23398;&#20064;&#19968;&#33268;&#34920;&#31034;&#65292;&#36827;&#32780;&#25351;&#23548;&#26426;&#22120;&#20154;&#25353;&#27491;&#30830;&#30340;&#39034;&#24207;&#25235;&#21462;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2304.12592</link><description>&lt;p&gt;
MMRDN: &#22810;&#35270;&#35282;&#29289;&#20307;&#22534;&#21472;&#22330;&#26223;&#20013;&#22810;&#35270;&#35282;&#25805;&#20316;&#20851;&#31995;&#26816;&#27979;&#30340;&#19968;&#33268;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MMRDN: Consistent Representation for Multi-View Manipulation Relationship Detection in Object-Stacked Scenes. (arXiv:2304.12592v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35270;&#35282;&#29289;&#20307;&#22534;&#21472;&#22330;&#26223;&#20013;&#25805;&#20316;&#20851;&#31995;&#26816;&#27979;&#30340;&#22810;&#35270;&#35282;MRD&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;2D&#21644;3D&#22810;&#35270;&#35282;&#25968;&#25454;&#23398;&#20064;&#19968;&#33268;&#34920;&#31034;&#65292;&#36827;&#32780;&#25351;&#23548;&#26426;&#22120;&#20154;&#25353;&#27491;&#30830;&#30340;&#39034;&#24207;&#25235;&#21462;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25805;&#20316;&#20851;&#31995;&#26816;&#27979;(MRD)&#26088;&#22312;&#25351;&#23548;&#26426;&#22120;&#20154;&#25353;&#27491;&#30830;&#30340;&#39034;&#24207;&#25235;&#21462;&#29289;&#20307;&#65292;&#36825;&#22312;&#29289;&#20307;&#22534;&#21472;&#22330;&#26223;&#20013;&#65292;&#30830;&#20445;&#25235;&#21462;&#30340;&#23433;&#20840;&#21487;&#38752;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#36807;&#29992;&#39044;&#23450;&#20041;&#30340;&#35270;&#35282;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#26029;&#25805;&#20316;&#20851;&#31995;&#65292;&#36825;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#23384;&#22312;&#35270;&#35273;&#38169;&#20301;&#30340;&#23616;&#38480;&#24615;&#12290;&#22810;&#35270;&#35282;&#25968;&#25454;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#20294;&#22810;&#35270;&#35282;MRD&#30340;&#25361;&#25112;&#26159;&#39046;&#22495;&#20559;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#35282;&#34701;&#21512;&#26694;&#26550;&#65292;&#21363;&#22810;&#35270;&#35282;MRD&#32593;&#32476;(MMRDN)&#65292;&#23427;&#26159;&#36890;&#36807;2D&#21644;3D&#22810;&#35270;&#35282;&#25968;&#25454;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#23558;&#19981;&#21516;&#35270;&#35282;&#30340;2D&#25968;&#25454;&#25237;&#24433;&#21040;&#20849;&#21516;&#30340;&#38544;&#34255;&#31354;&#38388;&#20013;&#65292;&#24182;&#29992;&#19968;&#32452;Von-Mises-Fisher&#20998;&#24067;&#25311;&#21512;&#23884;&#20837;&#65292;&#20197;&#23398;&#20064;&#19968;&#33268;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;3D&#25968;&#25454;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#25105;&#20204;&#20174;&#27599;&#20010;&#28857;&#20113;&#20013;&#36873;&#25321;&#19968;&#32452;$K$&#20010;&#26368;&#22823;&#22402;&#30452;&#37051;&#23621;(KMVN)&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulation relationship detection (MRD) aims to guide the robot to grasp objects in the right order, which is important to ensure the safety and reliability of grasping in object stacked scenes. Previous works infer manipulation relationship by deep neural network trained with data collected from a predefined view, which has limitation in visual dislocation in unstructured environments. Multi-view data provide more comprehensive information in space, while a challenge of multi-view MRD is domain shift. In this paper, we propose a novel multi-view fusion framework, namely multi-view MRD network (MMRDN), which is trained by 2D and 3D multi-view data. We project the 2D data from different views into a common hidden space and fit the embeddings with a set of Von-Mises-Fisher distributions to learn the consistent representations. Besides, taking advantage of position information within the 3D data, we select a set of $K$ Maximum Vertical Neighbors (KMVN) points from the point cloud of eac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#33268;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#32422;&#26463;&#26469;&#20943;&#23569;&#21512;&#25104;&#21644;&#32454;&#21270;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#22833;&#30495;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12591</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#33268;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#32422;&#26463;&#36827;&#34892;&#26080;&#30417;&#30563;&#21512;&#25104;&#22270;&#20687;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Synthetic Image Refinement via Contrastive Learning and Consistent Semantic and Structure Constraints. (arXiv:2304.12591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#33268;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#32422;&#26463;&#26469;&#20943;&#23569;&#21512;&#25104;&#21644;&#32454;&#21270;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#22833;&#30495;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#35745;&#31639;&#26426;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#30340;&#30495;&#23454;&#24615;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#30340;&#35821;&#20041;&#20998;&#24067;&#65292;&#22240;&#27492;&#21512;&#25104;&#21644;&#32454;&#21270;&#22270;&#20687;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#36827;&#32780;&#23548;&#33268;&#35821;&#20041;&#22833;&#30495;&#12290;&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#24050;&#25104;&#21151;&#22320;&#29992;&#20110;&#23558;&#30456;&#20851;&#34917;&#19969;&#25289;&#22312;&#19968;&#36215;&#24182;&#23558;&#19981;&#30456;&#20851;&#30340;&#34917;&#19969;&#25512;&#24320;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21512;&#25104;&#21644;&#31934;&#32454;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;CL&#26469;&#20943;&#23569;&#35821;&#20041;&#22833;&#30495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#30828;&#36127;&#37319;&#26679;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#24615;&#21644;&#23450;&#37327;&#25514;&#26045;&#27604;&#36739;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#20960;&#31181;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring the realism of computer-generated synthetic images is crucial to deep neural network (DNN) training. Due to different semantic distributions between synthetic and real-world captured datasets, there exists semantic mismatch between synthetic and refined images, which in turn results in the semantic distortion. Recently, contrastive learning (CL) has been successfully used to pull correlated patches together and push uncorrelated ones apart. In this work, we exploit semantic and structural consistency between synthetic and refined images and adopt CL to reduce the semantic distortion. Besides, we incorporate hard negative mining to improve the performance furthermore. We compare the performance of our method with several other benchmarking methods using qualitative and quantitative measures and show that our method offers the state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#21160;&#24577;&#31995;&#32479;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#12289;&#22312;&#32447;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22312;&#32447;&#36801;&#31227;&#23398;&#20064;&#21644;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.12583</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#21160;&#24577;&#31995;&#32479;&#30340;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#65306;&#26041;&#27861;&#21644;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Real-time Safety Assessment of Dynamic Systems in Non-stationary Environments: A Review of Methods and Techniques. (arXiv:2304.12583v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#21160;&#24577;&#31995;&#32479;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#12289;&#22312;&#32447;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22312;&#32447;&#36801;&#31227;&#23398;&#20064;&#21644;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#30340;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#23545;&#20110;&#24037;&#19994;&#21644;&#20132;&#36890;&#24212;&#29992;&#31561;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#26041;&#27861;&#30340;&#20840;&#38754;&#32508;&#36848;&#38459;&#30861;&#20102;&#30456;&#20851;&#26041;&#27861;&#30340;&#36827;&#23637;&#21644;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#38024;&#23545;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#20219;&#21153;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#31361;&#20986;&#20102;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#26041;&#27861;&#30340;&#32972;&#26223;&#21644;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38382;&#39064;&#25551;&#36848;&#65292;&#21253;&#25324;&#23450;&#20041;&#12289;&#20998;&#31867;&#21644;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#30456;&#20851;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#22914;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#12289;&#22312;&#32447;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22312;&#32447;&#36801;&#31227;&#23398;&#20064;&#21644;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#23637;&#26395;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#26412;&#25991;&#30340;&#32508;&#36848;&#26088;&#22312;&#20026;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#25552;&#20379;&#21442;&#32771;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time safety assessment (RTSA) of dynamic systems is a critical task that has significant implications for various fields such as industrial and transportation applications, especially in non-stationary environments. However, the absence of a comprehensive review of real-time safety assessment methods in non-stationary environments impedes the progress and refinement of related methods. In this paper, a review of methods and techniques for RTSA tasks in non-stationary environments is provided. Specifically, the background and significance of RTSA approaches in non-stationary environments are firstly highlighted. We then present a problem description that covers the definition, classification, and main challenges. We review recent developments in related technologies such as online active learning, online semi-supervised learning, online transfer learning, and online anomaly detection. Finally, we discuss future outlooks and potential directions for further research. Our review aims
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#36866;&#29992;&#20110;&#29616;&#20195;CPU&#20307;&#31995;&#32467;&#26500;&#30340;&#39640;&#25928;&#12289;&#21487;&#31227;&#26893;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#20869;&#26680;&#65292;&#20351;&#29992;&#39640;&#32423;&#24490;&#29615;&#21644;&#24352;&#37327;&#25277;&#35937;&#12290;</title><link>http://arxiv.org/abs/2304.12576</link><description>&lt;p&gt;
&#21033;&#29992;&#39640;&#32423;&#24490;&#29615;&#21644;&#24352;&#37327;&#25277;&#35937;&#22312;CPU&#26550;&#26500;&#19978;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;HPC&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
Harnessing Deep Learning and HPC Kernels via High-Level Loop and Tensor Abstractions on CPU Architectures. (arXiv:2304.12576v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12576
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#36866;&#29992;&#20110;&#29616;&#20195;CPU&#20307;&#31995;&#32467;&#26500;&#30340;&#39640;&#25928;&#12289;&#21487;&#31227;&#26893;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#20869;&#26680;&#65292;&#20351;&#29992;&#39640;&#32423;&#24490;&#29615;&#21644;&#24352;&#37327;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;&#12289;&#32534;&#31243;&#31995;&#32479;&#21644;&#30828;&#20214;&#24050;&#32463;&#19982;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;DL&#21644;HPC&#31995;&#32479;&#30340;&#32534;&#31243;&#26041;&#27861;&#21364;&#20572;&#28382;&#19981;&#21069;&#65292;&#20381;&#36182;&#20110;&#39640;&#24230;&#20248;&#21270;&#12289;&#29305;&#23450;&#20110;&#24179;&#21488;&#12289;&#20725;&#21270;&#30340;&#20379;&#24212;&#21830;&#20248;&#21270;&#24211;&#12290;&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#29616;&#20195;CPU&#26550;&#26500;&#30340;&#39640;&#25928;&#12289;&#21487;&#31227;&#26893;&#30340;DL&#21644;HPC&#20869;&#26680;&#12290;&#25105;&#20204;&#23558;&#20869;&#26680;&#24320;&#21457;&#20998;&#35299;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;1&#65289;&#20351;&#29992;&#24352;&#37327;&#22788;&#29702;&#21407;&#35821;&#65288;TPP&#65289;&#34920;&#36798;&#35745;&#31639;&#26680;&#24515;&#65306;&#19968;&#20010;&#32039;&#20945;&#12289;&#22810;&#21151;&#33021;&#30340;2D&#24352;&#37327;&#36816;&#31639;&#31526;&#65292;2&#65289;&#20197;&#39640;&#32423;&#12289;&#22768;&#26126;&#24615;&#30340;&#26041;&#24335;&#34920;&#36798;TPP&#21608;&#22260;&#30340;&#36923;&#36753;&#24490;&#29615;&#65292;&#32780;&#30830;&#20999;&#30340;&#23454;&#20363;&#21270;&#65288;&#39034;&#24207;&#65292;&#20869;&#23384;&#24067;&#23616;&#65289;&#21017;&#36890;&#36807;&#23558;TPL&#35270;&#20026;&#40657;&#30418;&#65292;&#26681;&#25454;&#20248;&#21270;&#30446;&#26631;&#21644;&#32422;&#26463;&#30001;&#33258;&#21160;&#20248;&#21270;&#22120;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the past decade, Deep Learning (DL) algorithms, programming systems and hardware have converged with the High Performance Computing (HPC) counterparts. Nevertheless, the programming methodology of DL and HPC systems is stagnant, relying on highly-optimized, yet platform-specific and inflexible vendor-optimized libraries. Such libraries provide close-to-peak performance on specific platforms, kernels and shapes thereof that vendors have dedicated optimizations efforts, while they underperform in the remaining use-cases, yielding non-portable codes with performance glass-jaws. This work introduces a framework to develop efficient, portable DL and HPC kernels for modern CPU architectures. We decompose the kernel development in two steps: 1) Expressing the computational core using Tensor Processing Primitives (TPPs): a compact, versatile set of 2D-tensor operators, 2) Expressing the logical loops around TPPs in a high-level, declarative fashion whereas the exact instantiation (order
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#36741;&#21161;&#20219;&#21153;&#30340;&#25968;&#37327;&#21644;&#20195;&#29702;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#25552;&#39640;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#21518;&#32487;&#24230;&#37327;&#30340;&#26032;&#22411;&#36741;&#21161;&#20219;&#21153;&#23478;&#26063; Proto-Value &#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.12567</link><description>&lt;p&gt;
Proto-Value Networks: &#36890;&#36807;&#36741;&#21161;&#20219;&#21153;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks. (arXiv:2304.12567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#36741;&#21161;&#20219;&#21153;&#30340;&#25968;&#37327;&#21644;&#20195;&#29702;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#25552;&#39640;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#21518;&#32487;&#24230;&#37327;&#30340;&#26032;&#22411;&#36741;&#21161;&#20219;&#21153;&#23478;&#26063; Proto-Value &#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36741;&#21161;&#20219;&#21153;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23398;&#21040;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#23613;&#31649;&#20854;&#25928;&#26524;&#24050;&#32463;&#34987;&#30456;&#24403;&#20805;&#20998;&#22320;&#29702;&#35770;&#20998;&#26512;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20204;&#20027;&#35201;&#34987;&#29992;&#20316;&#20027;&#35201;&#23398;&#20064;&#30446;&#26631;&#30340;&#25903;&#25345;&#65292;&#32780;&#19981;&#26159;&#20316;&#20026;&#34920;&#31034;&#23398;&#20064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#36741;&#21161;&#20219;&#21153;&#22312;&#23398;&#20064;&#22797;&#26434;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#21516;&#26102;&#22686;&#21152;&#20219;&#21153;&#25968;&#37327;&#21644;&#20195;&#29702;&#32593;&#32476;&#30340;&#22823;&#23567;&#30340;&#35774;&#32622;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#32487;&#24230;&#37327;&#30340;&#26032;&#22411;&#36741;&#21161;&#20219;&#21153;&#23478;&#26063;&#12290;&#36825;&#20123;&#20219;&#21153;&#26131;&#20110;&#23454;&#29616;&#65292;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#19982;&#21512;&#36866;&#30340;&#31163;&#32447;&#23398;&#20064;&#35268;&#21017;&#32467;&#21512;&#20351;&#29992;&#65292;&#32467;&#26524;&#26159;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026; Proto-Value &#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auxiliary tasks improve the representations learned by deep reinforcement learning agents. Analytically, their effect is reasonably well understood; in practice, however, their primary use remains in support of a main learning objective, rather than as a method for learning representations. This is perhaps surprising given that many auxiliary tasks are defined procedurally, and hence can be treated as an essentially infinite source of information about the environment. Based on this observation, we study the effectiveness of auxiliary tasks for learning rich representations, focusing on the setting where the number of tasks and the size of the agent's network are simultaneously increased. For this purpose, we derive a new family of auxiliary tasks based on the successor measure. These tasks are easy to implement and have appealing theoretical properties. Combined with a suitable off-policy learning rule, the result is a representation learning algorithm that can be understood as extend
&lt;/p&gt;</description></item><item><title>ChatGPT&#20855;&#22791;&#22312;&#38656;&#27714;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#25191;&#34892;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12562</link><description>&lt;p&gt;
&#23545;ChatGPT&#22312;&#38656;&#27714;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#21021;&#27493;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Preliminary Evaluation of ChatGPT in Requirements Information Retrieval. (arXiv:2304.12562v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12562
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20855;&#22791;&#22312;&#38656;&#27714;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#25191;&#34892;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#35768;&#22810;&#35828;&#26126;&#24615;&#23454;&#20363;&#26174;&#31034;&#20102;ChatGPT&#22312;&#25191;&#34892;&#32534;&#31243;&#20219;&#21153;&#21644;&#22238;&#31572;&#19968;&#33324;&#39046;&#22495;&#38382;&#39064;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#30446;&#30340;&#22312;&#20110;&#23454;&#35777;&#35780;&#20215;ChatGPT&#22312;&#38656;&#27714;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20197;&#27492;&#27934;&#23519;&#30001;ChatGPT&#20195;&#34920;&#30340;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#38656;&#27714;&#24037;&#31243;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#26041;&#27861;&#26159;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#27969;&#31243;&#65292;&#21253;&#25324;&#20004;&#20010;&#24120;&#35265;&#30340;&#38656;&#27714;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#12289;&#21253;&#21547;&#20004;&#31181;&#20856;&#22411;&#38656;&#27714;&#24037;&#20214;&#30340;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#12289;&#22266;&#23450;&#20219;&#21153;&#25552;&#31034;&#26597;&#35810;ChatGPT&#65292;&#20197;&#21450;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#20013;&#37117;&#36798;&#21040;&#20102;&#38646;-shot&#35774;&#32622;&#19979;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;$F\beta$&#20540;&#12290;&#23450;&#24615;&#20998;&#26512;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;ChatGPT&#24378;&#22823;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#26377;&#38480;&#30340;&#38656;&#27714;&#24037;&#31243;&#39046;&#22495;&#30693;&#35782;&#12290;&#32467;&#35770;&#26159;&#35780;&#20272;&#20102;ChatGPT&#22312;&#38656;&#27714;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#33719;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#31361;&#26174;&#20102;&#20854;&#25552;&#39640;&#38656;&#27714;&#24037;&#31243;&#20013;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Recently, many illustrative examples have shown ChatGPT's impressive ability to perform programming tasks and answer general domain questions.  Objective: We empirically evaluate how ChatGPT performs on requirements analysis tasks to derive insights into how generative large language model, represented by ChatGPT, influence the research and practice of natural language processing for requirements engineering.  Method: We design an evaluation pipeline including two common requirements information retrieval tasks, four public datasets involving two typical requirements artifacts, querying ChatGPT with fixed task prompts, and quantitative and qualitative results analysis.  Results: Quantitative results show that ChatGPT achieves comparable or better $F\beta$ values in all datasets under a zero-shot setting. Qualitative analysis further illustrates ChatGPT's powerful natural language processing ability and limited requirements engineering domain knowledge.  Conclusion: The evaluat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163;(&#24102;&#26377;&#21453;&#23545;&#25163;&#25200;&#21160;&#30340;&#26679;&#26412;)&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12550</link><description>&lt;p&gt;
&#35757;&#32451;&#20013;&#32467;&#21512;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining Adversaries with Anti-adversaries in Training. (arXiv:2304.12550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12550
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163;(&#24102;&#26377;&#21453;&#23545;&#25163;&#25200;&#21160;&#30340;&#26679;&#26412;)&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20581;&#22766;&#24615;&#30340;&#26377;&#25928;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#22312;&#26356;&#19968;&#33324;&#30340;&#25200;&#21160;&#33539;&#22260;&#19979;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#25506;&#32034;&#34920;&#26126;&#65292;&#23558;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163; (&#24102;&#26377;&#21453;&#23545;&#25163;&#25200;&#21160;&#30340;&#26679;&#26412;) &#32467;&#21512;&#22312;&#35757;&#32451;&#20013;&#65292;&#22312;&#19968;&#20123;&#20856;&#22411;&#30340;&#23398;&#20064;&#22330;&#26223; (&#22914;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#21644;&#19981;&#24179;&#34913;&#23398;&#20064;) &#20013;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23454;&#29616;&#26356;&#22909;&#30340;&#31867;&#21035;&#38388;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#23545;&#25239;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is an effective learning technique to improve the robustness of deep neural networks. In this study, the influence of adversarial training on deep learning models in terms of fairness, robustness, and generalization is theoretically investigated under more general perturbation scope that different samples can have different perturbation directions (the adversarial and anti-adversarial directions) and varied perturbation bounds. Our theoretical explorations suggest that the combination of adversaries and anti-adversaries (samples with anti-adversarial perturbations) in training can be more effective in achieving better fairness between classes and a better tradeoff between robustness and generalization in some typical learning scenarios (e.g., noisy label learning and imbalance learning) compared with standard adversarial training. On the basis of our theoretical findings, a more general learning objective that combines adversaries and anti-adversaries with varied b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#35937;&#26816;&#27979;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#25552;&#39640;&#33322;&#31354;&#28145;&#24230;&#34917;&#20840;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#23558;&#20004;&#20010;&#20219;&#21153;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#25191;&#34892;&#19968;&#27425;&#65292;&#23454;&#29616;&#20102;&#19982;KITTI&#28145;&#24230;&#34917;&#20840;&#22522;&#20934;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12542</link><description>&lt;p&gt;
&#29289;&#20307;&#35821;&#20041;&#20449;&#24687;&#36171;&#20104;&#25105;&#20204;&#25152;&#38656;&#30340;&#28145;&#24230;: &#22810;&#20219;&#21153;&#26041;&#27861;&#35299;&#20915;&#33322;&#31354;&#28145;&#24230;&#34917;&#20840;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Object Semantics Give Us the Depth We Need: Multi-task Approach to Aerial Depth Completion. (arXiv:2304.12542v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12542
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#35937;&#26816;&#27979;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#25552;&#39640;&#33322;&#31354;&#28145;&#24230;&#34917;&#20840;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#23558;&#20004;&#20010;&#20219;&#21153;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#25191;&#34892;&#19968;&#27425;&#65292;&#23454;&#29616;&#20102;&#19982;KITTI&#28145;&#24230;&#34917;&#20840;&#22522;&#20934;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#34917;&#20840;&#21644;&#30446;&#26631;&#26816;&#27979;&#26159;&#33322;&#31354;&#19977;&#32500;&#24314;&#22270;&#12289;&#36335;&#24452;&#35268;&#21010;&#21644;&#26080;&#20154;&#26426;&#36991;&#38556;&#31561;&#39046;&#22495;&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#24120;&#35265;&#26041;&#27861;&#20351;&#29992;&#26469;&#33258;LiDAR&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#25968;&#25454;&#65292;&#20294;&#29983;&#25104;&#30340;&#28857;&#20113;&#36890;&#24120;&#26159;&#31232;&#30095;&#21644;&#19981;&#35268;&#21017;&#30340;&#65292;&#38480;&#21046;&#20102;&#31995;&#32479;&#22312;&#19977;&#32500;&#28210;&#26579;&#21644;&#23433;&#20840;&#20915;&#31574;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#21033;&#29992;&#26080;&#20154;&#26426;&#19978;&#20854;&#20182;&#20256;&#24863;&#22120;&#65288;&#22914;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#25668;&#20687;&#22836;&#65289;&#30340;&#20449;&#24687;&#26469;&#24110;&#21161;&#28145;&#24230;&#34917;&#20840;&#36807;&#31243;&#29983;&#25104;&#26356;&#23494;&#38598;&#30340;&#19977;&#32500;&#27169;&#22411;&#12290;&#21516;&#26102;&#25191;&#34892;&#33322;&#31354;&#28145;&#24230;&#34917;&#20840;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#24182;&#34701;&#21512;&#20004;&#20010;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#23545;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#20004;&#20010;&#20219;&#21153;&#26292;&#38706;&#32473;&#20849;&#21516;&#23398;&#20064;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#30446;&#26631;&#26816;&#27979;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#25913;&#36827;&#33322;&#31354;&#28145;&#24230;&#34917;&#20840;&#32467;&#26524;&#65292;&#22312;&#27969;&#34892;&#30340;KITTI&#28145;&#24230;&#34917;&#20840;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth completion and object detection are two crucial tasks often used for aerial 3D mapping, path planning, and collision avoidance of Uncrewed Aerial Vehicles (UAVs). Common solutions include using measurements from a LiDAR sensor; however, the generated point cloud is often sparse and irregular and limits the system's capabilities in 3D rendering and safety-critical decision-making. To mitigate this challenge, information from other sensors on the UAV (viz., a camera used for object detection) is utilized to help the depth completion process generate denser 3D models. Performing both aerial depth completion and object detection tasks while fusing the data from the two sensors poses a challenge to resource efficiency. We address this challenge by proposing a novel approach to jointly execute the two tasks in a single pass. The proposed method is based on an encoder-focused multi-task learning model that exposes the two tasks to jointly learned features. We demonstrate how semantic ex
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#30340;&#31354;&#38388;&#20449;&#24687;&#25552;&#21462;&#32467;&#26500;&#65292;&#33021;&#26377;&#25928;&#22320;&#20849;&#20139;&#37051;&#22495;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#21487;&#21464;&#20195;&#29702;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#32467;&#26500;&#21487;&#20197;&#25552;&#21319;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12532</link><description>&lt;p&gt;
SEA: &#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#31354;&#38388;&#26174;&#24335;&#20307;&#31995;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
SEA: A Spatially Explicit Architecture for Multi-Agent Reinforcement Learning. (arXiv:2304.12532v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12532
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#30340;&#31354;&#38388;&#20449;&#24687;&#25552;&#21462;&#32467;&#26500;&#65292;&#33021;&#26377;&#25928;&#22320;&#20849;&#20139;&#37051;&#22495;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#21487;&#21464;&#20195;&#29702;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#32467;&#26500;&#21487;&#20197;&#25552;&#21319;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#20449;&#24687;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#21313;&#20998;&#37325;&#35201;&#12290;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#22914;&#20309;&#26681;&#25454;&#20195;&#29702;&#30340;&#31354;&#38388;&#20301;&#32622;&#26174;&#24335;&#24314;&#27169;&#20063;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#24403;&#20195;&#29702;&#25968;&#37327;&#21464;&#21270;&#21644;&#35268;&#27169;&#24040;&#22823;&#26102;&#12290;&#26412;&#25991;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#28857;&#20113;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#31354;&#38388;&#20449;&#24687;&#25552;&#21462;&#32467;&#26500;&#12290;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#31354;&#38388;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#26377;&#25928;&#22320;&#20849;&#20139;&#37051;&#22495;&#21644;&#20840;&#23616;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36981;&#24490;&#20013;&#24515;&#21270;&#35757;&#32451;&#12289;&#21435;&#20013;&#24515;&#21270;&#25191;&#34892;&#65288;CTDE&#65289;&#33539;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26500;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#29616;&#26377;&#30340;&#20027;&#27969;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#21487;&#21464;&#20195;&#29702;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#22312;&#20960;&#20010;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30340;&#23454;&#39564;&#20013;&#65292;&#36890;&#36807;&#28155;&#21152;&#25105;&#20204;&#30340;&#31354;&#38388;&#26174;&#24335;&#20307;&#31995;&#32467;&#26500;&#65292;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#24471;&#21040;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial information is essential in various fields. How to explicitly model according to the spatial location of agents is also very important for the multi-agent problem, especially when the number of agents is changing and the scale is enormous. Inspired by the point cloud task in computer vision, we propose a spatial information extraction structure for multi-agent reinforcement learning in this paper. Agents can effectively share the neighborhood and global information through a spatially encoder-decoder structure. Our method follows the centralized training with decentralized execution (CTDE) paradigm. In addition, our structure can be applied to various existing mainstream reinforcement learning algorithms with minor modifications and can deal with the problem with a variable number of agents. The experiments in several multi-agent scenarios show that the existing methods can get convincing results by adding our spatially explicit architecture.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hint-Aug&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#21069;&#39044;&#35757;&#32451;&#30340;FViTs&#23398;&#21040;&#30340;&#39640;&#24230;&#20195;&#34920;&#24615;&#29305;&#24449;&#26469;&#22686;&#24378;&#35843;&#21442;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;FViTs&#22312;&#23569;&#26679;&#26412;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#8220;&#39269;&#39295;&#8221;&#29305;&#24615;&#65292;&#24182;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;FViT&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#35843;&#21442;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.12520</link><description>&lt;p&gt;
Hint-Aug: &#20174;&#22522;&#30784;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#33719;&#21462;&#25552;&#31034;&#65292;&#23454;&#29616;&#22686;&#24378;&#30340;&#23569;&#26679;&#26412;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Hint-Aug: Drawing Hints from Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning. (arXiv:2304.12520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12520
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hint-Aug&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#21069;&#39044;&#35757;&#32451;&#30340;FViTs&#23398;&#21040;&#30340;&#39640;&#24230;&#20195;&#34920;&#24615;&#29305;&#24449;&#26469;&#22686;&#24378;&#35843;&#21442;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;FViTs&#22312;&#23569;&#26679;&#26412;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#8220;&#39269;&#39295;&#8221;&#29305;&#24615;&#65292;&#24182;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;FViT&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#35843;&#21442;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36234;&#26469;&#36234;&#38656;&#35201;&#35843;&#20248;&#22522;&#30784;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;FViT&#65289;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#22312;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#23569;&#26679;&#26412;&#35843;&#20248;&#65289;&#65292;&#20805;&#20998;&#21457;&#25381;FViTs&#30340;&#28508;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;FViTs&#30340;&#25968;&#25454;&#29305;&#24615;&#26159;&#39269;&#39295;&#30340;&#12290;&#30001;&#20110;&#23569;&#31034;&#20363;&#35843;&#21442;&#25968;&#25454;&#21253;&#21547;&#26377;&#38480;&#30340;&#29305;&#24449;&#65292;&#22240;&#27492;&#24120;&#35265;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#27492;&#24773;&#20917;&#19979;&#26080;&#27861;&#21457;&#25381;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;FViTs&#22312;&#23569;&#26679;&#26412;&#35843;&#20248;&#26041;&#38754;&#30340;&#26426;&#20250;&#65306;&#39044;&#20808;&#35757;&#32451;&#30340;FViTs&#24050;&#32463;&#20174;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#21040;&#20102;&#39640;&#24230;&#20195;&#34920;&#24615;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#36807;&#31243;&#20013;&#23436;&#20840;&#20445;&#30041;&#12290;&#25105;&#20204;&#22240;&#27492;&#20551;&#35774;&#21033;&#29992;&#36825;&#20123;&#24050;&#23398;&#20064;&#30340;&#29305;&#24449;&#26469;&#22686;&#24378;&#35843;&#21442;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#23569;&#26679;&#26412;FViT&#35843;&#20248;&#30340;&#25928;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Hint-based Data Augmentation (Hint-Aug) &#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#35843;&#25972;&#26679;&#26412;&#30340;&#36807;&#24230;&#25311;&#21512;&#37096;&#20998;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;FViTs&#30340;&#23398;&#20064;&#29305;&#24449;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;FViT&#30340;&#23569;&#26679;&#26412;&#35843;&#20248;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Hint-Aug&#26174;&#30528;&#25552;&#39640;&#20102;FViT&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20165;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#21442;&#65292;&#20351;FViTs&#30340;&#24615;&#33021;&#20248;&#20110;&#24403;&#21069;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the growing demand for tuning foundation vision transformers (FViTs) on downstream tasks, fully unleashing FViTs' potential under data-limited scenarios (e.g., few-shot tuning) remains a challenge due to FViTs' data-hungry nature. Common data augmentation techniques fall short in this context due to the limited features contained in the few-shot tuning data. To tackle this challenge, we first identify an opportunity for FViTs in few-shot tuning: pretrained FViTs themselves have already learned highly representative features from large-scale pretraining data, which are fully preserved during widely used parameter-efficient tuning. We thus hypothesize that leveraging those learned features to augment the tuning data can boost the effectiveness of few-shot FViT tuning. To this end, we propose a framework called Hint-based Data Augmentation (Hint-Aug), which aims to boost FViT in few-shot tuning by augmenting the over-fitted parts of tuning samples with the learned features of pret
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#36817;&#20284;&#21387;&#32553;&#30740;&#31350;&#12290;&#20855;&#20307;&#23454;&#39564;&#26159;&#20197;GPT-3.5&#21644;GPT-4&#20026;&#22522;&#30784;&#36827;&#34892;&#30340;&#65292;&#26088;&#22312;&#25506;&#32034;&#36817;&#20284;&#21387;&#32553;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12512</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Semantic Compression With Large Language Models. (arXiv:2304.12512v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#36817;&#20284;&#21387;&#32553;&#30740;&#31350;&#12290;&#20855;&#20307;&#23454;&#39564;&#26159;&#20197;GPT-3.5&#21644;GPT-4&#20026;&#22522;&#30784;&#36827;&#34892;&#30340;&#65292;&#26088;&#22312;&#25506;&#32034;&#36817;&#20284;&#21387;&#32553;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#26377;&#26102;&#20250;&#33258;&#20449;&#22320;&#21576;&#29616;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#20449;&#24687;&#65288;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#65289;&#22806;&#65292;LLMs&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#20196;&#29260;&#25968;&#37327;&#20063;&#22825;&#29983;&#21463;&#38480;&#65292;&#36825;&#20351;&#23427;&#20204;&#22312;&#22788;&#29702;&#22823;&#37327;&#20449;&#24687;&#25110;&#36830;&#32493;&#27969;&#20449;&#24687;&#30340;&#20219;&#21153;&#19978;&#21487;&#33021;&#19981;&#22826;&#26377;&#25928;&#12290;&#20943;&#23567;&#25968;&#25454;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#36890;&#36807;&#26080;&#25439;&#25110;&#26377;&#25439;&#21387;&#32553;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#33021;&#22815;&#20256;&#36798;&#25152;&#38656;&#30340;&#35821;&#20041;&#31934;&#24230;&#25110;&#24847;&#22270;&#65292;&#23601;&#19981;&#19968;&#23450;&#38656;&#35201;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#24674;&#22797;&#27599;&#20010;&#32454;&#33410; to&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;LLMs&#30740;&#31350;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#36817;&#20284;&#21387;&#32553;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#37325;&#28857;&#20851;&#27880;GPT-3.5&#21644;GPT-4&#36890;&#36807;ChatGPT&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of large language models (LLMs) is revolutionizing information retrieval, question answering, summarization, and code generation tasks. However, in addition to confidently presenting factually inaccurate information at times (known as "hallucinations"), LLMs are also inherently limited by the number of input and output tokens that can be processed at once, making them potentially less effective on tasks that require processing a large set or continuous stream of information. A common approach to reducing the size of data is through lossless or lossy compression. Yet, in some cases it may not be strictly necessary to perfectly recover every detail from the original data, as long as a requisite level of semantic precision or intent is conveyed.  This paper presents three contributions to research on LLMs. First, we present the results from experiments exploring the viability of approximate compression using LLMs, focusing specifically on GPT-3.5 and GPT-4 via ChatGPT interfaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; ASAP-Phi&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#27573;&#22870;&#21169;&#20989;&#25968;&#26469;&#20026;&#26410;&#28385;&#36275;&#35268;&#33539;&#30340;&#36712;&#36857;&#20998;&#37197;&#23450;&#37327;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#24182;&#20026;&#20854;&#20313;&#36712;&#36857;&#20998;&#37197;&#39640;&#30340;&#24120;&#25968;&#22870;&#21169;&#65292;&#20351;&#29992;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#31639;&#27861;&#35757;&#32451;&#20195;&#29702;&#26469;&#23613;&#24555;&#23454;&#29616;&#35268;&#33539;&#12290;</title><link>http://arxiv.org/abs/2304.12508</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#23613;&#24555;&#23454;&#29616;&#27491;&#24335;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Fulfilling Formal Specifications ASAP by Model-free Reinforcement Learning. (arXiv:2304.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; ASAP-Phi&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#27573;&#22870;&#21169;&#20989;&#25968;&#26469;&#20026;&#26410;&#28385;&#36275;&#35268;&#33539;&#30340;&#36712;&#36857;&#20998;&#37197;&#23450;&#37327;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#24182;&#20026;&#20854;&#20313;&#36712;&#36857;&#20998;&#37197;&#39640;&#30340;&#24120;&#25968;&#22870;&#21169;&#65292;&#20351;&#29992;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#31639;&#27861;&#35757;&#32451;&#20195;&#29702;&#26469;&#23613;&#24555;&#23454;&#29616;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363; ASAP-Phi&#26694;&#26550;&#65292;&#20197;&#40723;&#21169;&#20195;&#29702;&#23613;&#24555;&#28385;&#36275;&#27491;&#24335;&#35268;&#33539;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#19968;&#20010;&#20998;&#27573;&#22870;&#21169;&#20989;&#25968;&#26469;&#20026;&#26410;&#28385;&#36275;&#35268;&#33539;&#30340;&#36712;&#36857;&#20998;&#37197;&#23450;&#37327;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#24182;&#20026;&#20854;&#20313;&#36712;&#36857;&#20998;&#37197;&#39640;&#30340;&#24120;&#25968;&#22870;&#21169;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#31639;&#27861;&#65288;&#20363;&#22914;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;(SAC)&#25110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#65289;&#35757;&#32451;&#20195;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;ASAP-Phi&#29983;&#25104;&#30340;&#31574;&#30053;&#20248;&#20808;&#32771;&#34385;&#23613;&#24555;&#23454;&#29616;&#35268;&#33539;&#12290;&#23545;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;&#28040;&#34701;&#30740;&#31350;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25104;&#21151;&#22320;&#20026;&#22810;&#36798;97&#65285;&#30340;&#27979;&#35797;&#29992;&#20363;&#25214;&#21040;&#20102;&#36275;&#22815;&#24555;&#30340;&#36712;&#36857;&#65292;&#24182;&#20987;&#36133;&#20102;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model-free reinforcement learning solution, namely the ASAP-Phi framework, to encourage an agent to fulfill a formal specification ASAP. The framework leverages a piece-wise reward function that assigns quantitative semantic reward to traces not satisfying the specification, and a high constant reward to the remaining. Then, it trains an agent with an actor-critic-based algorithm, such as soft actor-critic (SAC), or deep deterministic policy gradient (DDPG). Moreover, we prove that ASAP-Phi produces policies that prioritize fulfilling a specification ASAP. Extensive experiments are run, including ablation studies, on state-of-the-art benchmarks. Results show that our framework succeeds in finding sufficiently fast trajectories for up to 97\% test cases and defeats baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#25991;&#26723;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#36890;&#36807;&#23545;ResNet50&#21644;EfficientNetB0&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#12289;JPEG&#36755;&#20837;&#21387;&#32553;&#21644;&#28784;&#24230;&#36755;&#20837;&#36716;&#25442;&#31561;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12486</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26723;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Adversarial Robustness on Document Image Classification. (arXiv:2304.12486v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#25991;&#26723;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#36890;&#36807;&#23545;ResNet50&#21644;EfficientNetB0&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#12289;JPEG&#36755;&#20837;&#21387;&#32553;&#21644;&#28784;&#24230;&#36755;&#20837;&#36716;&#25442;&#31561;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#19978;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#33267;&#20170;&#22823;&#37096;&#20998;&#30740;&#31350;&#20165;&#38480;&#20110;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23454;&#38469;&#19978;&#22788;&#29702;&#30340;&#26159;&#25991;&#26723;&#25968;&#25454;&#65292;&#36825;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#38750;&#24120;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#23558;&#23545;&#25239;&#25915;&#20987;&#21746;&#23398;&#24212;&#29992;&#20110;&#25991;&#29486;&#21644;&#33258;&#28982;&#25968;&#25454;&#65292;&#24182;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#27492;&#31867;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#26080;&#30446;&#26631;&#22522;&#20110;&#26799;&#24230;&#12289;&#22522;&#20110;&#36716;&#31227;&#21644;&#22522;&#20110;&#20998;&#25968;&#30340;&#25915;&#20987;&#19978;&#65292;&#24182;&#35780;&#20272;&#23545;&#25239;&#35757;&#32451;&#12289;JPEG&#36755;&#20837;&#21387;&#32553;&#21644;&#28784;&#24230;&#36755;&#20837;&#36716;&#25442;&#23545;ResNet50&#21644;EfficientNetB0&#27169;&#22411;&#26550;&#26500;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#31038;&#21306;&#27809;&#26377;&#36827;&#34892;&#36825;&#26679;&#30340;&#30740;&#31350;&#20197;&#30740;&#31350;&#36825;&#20123;&#25915;&#20987;&#23545;&#25991;&#26723;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks and defenses have gained increasing interest on computer vision systems in recent years, but as of today, most investigations are limited to images. However, many artificial intelligence models actually handle documentary data, which is very different from real world images. Hence, in this work, we try to apply the adversarial attack philosophy on documentary and natural data and to protect models against such attacks. We focus our work on untargeted gradient-based, transfer-based and score-based attacks and evaluate the impact of adversarial training, JPEG input compression and grey-scale input transformation on the robustness of ResNet50 and EfficientNetB0 model architectures. To the best of our knowledge, no such work has been conducted by the community in order to study the impact of these attacks on the document image classification task.
&lt;/p&gt;</description></item><item><title>DocParser&#26159;&#19968;&#31181;&#26080;OCR&#30340;&#31471;&#21040;&#31471;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#65292;&#21487;&#26356;&#22909;&#22320;&#25552;&#21462;&#21028;&#21035;&#24615;&#23383;&#31526;&#29305;&#24449;&#65292;&#24182;&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12484</link><description>&lt;p&gt;
DocParser&#65306;&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#20013;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#26080;OCR&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
DocParser: End-to-end OCR-free Information Extraction from Visually Rich Documents. (arXiv:2304.12484v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12484
&lt;/p&gt;
&lt;p&gt;
DocParser&#26159;&#19968;&#31181;&#26080;OCR&#30340;&#31471;&#21040;&#31471;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#65292;&#21487;&#26356;&#22909;&#22320;&#25552;&#21462;&#21028;&#21035;&#24615;&#23383;&#31526;&#29305;&#24449;&#65292;&#24182;&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20854;&#22312;&#20960;&#20010;&#22522;&#20110;&#25991;&#26723;&#25511;&#21046;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#20854;&#24191;&#27867;&#30340;&#21830;&#19994;&#20215;&#20540;&#65292;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#36827;&#34892;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#24037;&#20316;&#37117;&#36981;&#24490;&#20004;&#27493;&#27861;&#12290;&#39318;&#20808;&#65292;&#20182;&#20204;&#20351;&#29992;&#29616;&#25104;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#24341;&#25806;&#35835;&#21462;&#25991;&#26412;&#65292;&#28982;&#21518;&#20174;&#25152;&#33719;&#24471;&#30340;&#25991;&#26412;&#20013;&#25552;&#21462;&#24863;&#20852;&#36259;&#30340;&#23383;&#27573;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#22806;&#37096;OCR&#31995;&#32479;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#24615;&#33021;&#21644;&#35745;&#31639;&#36895;&#24230;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#20123;&#26080;OCR&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#21463;&#21040;&#20854;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DocParser&#30340;OCR-free&#31471;&#21040;&#31471;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#20854;&#26356;&#22909;&#22320;&#25552;&#21462;&#21028;&#21035;&#24615;&#23383;&#31526;&#29305;&#24449;&#30340;&#33021;&#21147;&#19981;&#21516;&#20110;&#20808;&#21069;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#12290;DocParser&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#65292;&#36229;&#36807;&#20102;&#20381;&#36182;&#20110;OCR&#24341;&#25806;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information Extraction from visually rich documents is a challenging task that has gained a lot of attention in recent years due to its importance in several document-control based applications and its widespread commercial value. The majority of the research work conducted on this topic to date follow a two-step pipeline. First, they read the text using an off-the-shelf Optical Character Recognition (OCR) engine, then, they extract the fields of interest from the obtained text. The main drawback of these approaches is their dependence on an external OCR system, which can negatively impact both performance and computational speed. Recent OCR-free methods were proposed to address the previous issues. Inspired by their promising results, we propose in this paper an OCR-free end-to-end information extraction model named DocParser. It differs from prior end-to-end approaches by its ability to better extract discriminative character features. DocParser achieves state-of-the-art results on v
&lt;/p&gt;</description></item><item><title>AGI&#25216;&#26415;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#65292;&#21487;&#20197;&#24314;&#31435;e-learning&#24179;&#21488;&#12289;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#65292;&#24357;&#34917;&#20256;&#32479;AI&#27169;&#22411;&#22240;&#21463;&#38480;&#20110;&#25968;&#25454;&#21644;&#20154;&#38469;&#20132;&#20114;&#38480;&#21046;&#32780;&#26080;&#27861;&#28385;&#36275;&#25945;&#32946;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2304.12479</link><description>&lt;p&gt;
&#29992;&#20110;&#25945;&#32946;&#30340;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;
&lt;/p&gt;
&lt;p&gt;
Artificial General Intelligence (AGI) for Education. (arXiv:2304.12479v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12479
&lt;/p&gt;
&lt;p&gt;
AGI&#25216;&#26415;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#65292;&#21487;&#20197;&#24314;&#31435;e-learning&#24179;&#21488;&#12289;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#65292;&#24357;&#34917;&#20256;&#32479;AI&#27169;&#22411;&#22240;&#21463;&#38480;&#20110;&#25968;&#25454;&#21644;&#20154;&#38469;&#20132;&#20114;&#38480;&#21046;&#32780;&#26080;&#27861;&#28385;&#36275;&#25945;&#32946;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;&#22914;GPT-4&#21644;ChatGPT&#65289;&#30340;&#20986;&#29616;&#65292;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#20316;&#20026;&#26410;&#26469;&#25216;&#26415;&#24050;&#32463;&#24471;&#21040;&#20840;&#29699;&#35748;&#21487;&#12290;AGI&#26088;&#22312;&#36890;&#36807;&#35745;&#31639;&#26426;&#31995;&#32479;&#22797;&#21046;&#20154;&#31867;&#26234;&#33021;&#65292;&#26159;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#12290;&#19982;&#20256;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#38024;&#23545;&#26377;&#38480;&#33539;&#22260;&#30340;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#65292;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#33021;&#26080;&#27861;&#32771;&#34385;&#25945;&#32946;&#20013;&#22797;&#26434;&#30340;&#20154;&#38469;&#21160;&#24577;&#12290;&#21463;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#39537;&#21160;&#65292;AGI&#20195;&#34920;&#20102;&#26426;&#22120;&#22312;&#25191;&#34892;&#38656;&#35201;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#22823;&#39134;&#36291;&#65292;&#20363;&#22914;&#25512;&#29702;&#12289;&#35299;&#20915;&#38382;&#39064;&#12289;&#20570;&#20986;&#20915;&#31574;&#65292;&#29978;&#33267;&#29702;&#35299;&#20154;&#31867;&#24773;&#24863;&#21644;&#31038;&#20132;&#20114;&#21160;&#12290;&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;AGI&#30340;&#20851;&#38190;&#27010;&#24565;&#12289;&#33021;&#21147;&#12289;&#33539;&#22260;&#21644;&#22312;&#26410;&#26469;&#25945;&#32946;&#20013;&#30340;&#28508;&#21147;&#65292;&#21253;&#25324;&#24314;&#31435;e-learning&#24179;&#21488;&#21644;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial general intelligence (AGI) has gained global recognition as a future technology due to the emergence of breakthrough large language models and chatbots such as GPT-4 and ChatGPT, respectively. AGI aims to replicate human intelligence through computer systems, which is one of the critical technologies having the potential to revolutionize the field of education. Compared to conventional AI models, typically designed for a limited range of tasks, demand significant amounts of domain-specific data for training and may not always consider intricate interpersonal dynamics in education. AGI, driven by the recent large pre-trained models, represents a significant leap in the capability of machines to perform tasks that require human-level intelligence, such as reasoning, problem-solving, decision-making, and even understanding human emotions and social interactions. This work reviews AGI's key concepts, capabilities, scope, and potential within future education, including setting e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;CVaR&#21644;EVaR&#39118;&#38505;&#24230;&#37327;&#21160;&#24577;&#20998;&#35299;&#26159;&#30495;&#23454;&#39118;&#38505;&#20540;&#30340;&#20005;&#26684;&#39640;&#20272;&#35745;&#65292;&#28982;&#32780;VaR&#23384;&#22312;&#31934;&#30830;&#30340;&#21160;&#24577;&#20998;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.12477</link><description>&lt;p&gt;
&#20851;&#20110;&#38745;&#24577;&#39118;&#38505;&#24230;&#37327;&#30340;&#21160;&#24577;&#35268;&#21010;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
On Dynamic Program Decompositions of Static Risk Measures. (arXiv:2304.12477v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;CVaR&#21644;EVaR&#39118;&#38505;&#24230;&#37327;&#21160;&#24577;&#20998;&#35299;&#26159;&#30495;&#23454;&#39118;&#38505;&#20540;&#30340;&#20005;&#26684;&#39640;&#20272;&#35745;&#65292;&#28982;&#32780;VaR&#23384;&#22312;&#31934;&#30830;&#30340;&#21160;&#24577;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20248;&#21270;&#38745;&#24577;&#39118;&#38505;&#35268;&#36991;&#30446;&#26631;&#20855;&#26377;&#19968;&#23450;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#23481;&#26131;&#25509;&#21463;&#21160;&#24577;&#35268;&#21010;&#20998;&#35299;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#39118;&#38505;&#24230;&#37327;&#30340;&#21160;&#24577;&#20998;&#35299;&#26469;&#21046;&#23450;&#25193;&#23637;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;&#21160;&#24577;&#35268;&#21010;&#12290;&#26412;&#25991;&#34920;&#26126;&#20960;&#31181;&#29616;&#26377;&#30340;&#20998;&#35299;&#26412;&#36136;&#19978;&#26159;&#19981;&#31934;&#30830;&#30340;&#65292;&#36825;&#19982;&#25991;&#29486;&#20013;&#30340;&#20960;&#20010;&#22768;&#26126;&#30456;&#30683;&#30462;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20030;&#20986;&#20102;&#19968;&#20123;&#20363;&#23376;&#65292;&#35777;&#26126;&#20102;CVaR&#21644;EVaR&#39118;&#38505;&#24230;&#37327;&#30340;&#27969;&#34892;&#20998;&#35299;&#26159;&#30495;&#23454;&#39118;&#38505;&#20540;&#30340;&#20005;&#26684;&#39640;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;VaR&#30830;&#23454;&#23384;&#22312;&#31934;&#30830;&#30340;&#20998;&#35299;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#31616;&#21333;&#30340;&#35777;&#26126;&#65292;&#38416;&#26126;&#20102;VaR&#21644;CVaR&#21160;&#24577;&#35268;&#21010;&#23646;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing static risk-averse objectives in Markov decision processes is challenging because they do not readily admit dynamic programming decompositions. Prior work has proposed to use a dynamic decomposition of risk measures that help to formulate dynamic programs on an augmented state space. This paper shows that several existing decompositions are inherently inexact, contradicting several claims in the literature. In particular, we give examples that show that popular decompositions for CVaR and EVaR risk measures are strict overestimates of the true risk values. However, an exact decomposition is possible for VaR, and we give a simple proof that illustrates the fundamental difference between VaR and CVaR dynamic programming properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;MDP&#20013;&#22522;&#20110;&#27010;&#29575;&#20195;&#29702;&#25481;&#32447;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#25481;&#32447;&#24773;&#20917;&#38656;&#35201;&#26522;&#20030;&#35745;&#31639;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.12458</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;MDP&#20013;&#22522;&#20110;&#27010;&#29575;&#20195;&#29702;&#25481;&#32447;&#30340;&#26080;&#27169;&#22411;&#23398;&#20064;&#21644;&#26368;&#20248;&#31574;&#30053;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Model-Free Learning and Optimal Policy Design in Multi-Agent MDPs Under Probabilistic Agent Dropout. (arXiv:2304.12458v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;MDP&#20013;&#22522;&#20110;&#27010;&#29575;&#20195;&#29702;&#25481;&#32447;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#25481;&#32447;&#24773;&#20917;&#38656;&#35201;&#26522;&#20030;&#35745;&#31639;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#35813;&#36807;&#31243;&#21487;&#20197;&#32463;&#21382;&#20195;&#29702;&#25481;&#32447;&#65292;&#24182;&#22522;&#20110;&#23545;&#20110;&#31574;&#30053;&#30340;&#25511;&#21046;&#21644;&#39044;&#20195;&#29702;&#36807;&#31243;&#30340;&#37319;&#26679;&#26469;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#31574;&#30053;&#12290;&#25511;&#21046;&#22120;&#30340;&#30446;&#26631;&#26159;&#23547;&#25214;&#19968;&#20010;&#26368;&#20248;&#31574;&#30053;&#65292;&#20351;&#24471;&#22312;&#24050;&#30693;&#20195;&#29702;&#25481;&#20986;&#27010;&#29575;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#26399;&#26395;&#31995;&#32479;&#30340;&#20215;&#20540;&#26368;&#22823;&#21270;&#12290;&#23545;&#20110;&#20219;&#20309;&#29305;&#23450;&#30340;&#25481;&#32447;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#31574;&#30053;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#29305;&#20363;&#12290;&#23545;&#20110;&#20855;&#26377;&#29305;&#23450;&#36716;&#25442;&#29420;&#31435;&#24615;&#21644;&#22870;&#21169;&#21487;&#20998;&#24615;&#32467;&#26500;&#30340;MDPs&#65292;&#25105;&#20204;&#20551;&#35774;&#20174;&#31995;&#32479;&#20013;&#31227;&#38500;&#20195;&#29702;&#32452;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;MDP&#65292;&#30001;&#21097;&#20313;&#20195;&#29702;&#32452;&#25104;&#20855;&#26377;&#26032;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;MDP&#65292;&#36716;&#25442;&#21160;&#24577;&#28040;&#38500;&#24050;&#21024;&#38500;&#30340;&#20195;&#29702;&#65292;&#22870;&#21169;&#19982;&#24050;&#21024;&#38500;&#30340;&#20195;&#29702;&#26080;&#20851;&#12290;&#39318;&#20808;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#39044;&#25481;&#20986;&#31995;&#32479;&#26399;&#26395;&#20540;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#21333;&#19968;&#30340;MDP&#26469;&#34920;&#31034;&#65307;&#36825;&#20010;&#8220;&#40065;&#26834;MDP&#8221;&#33021;&#22815;&#28040;&#38500;&#22312;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#26102;&#35201;&#35780;&#20272;&#25152;&#26377;$2^N$&#31181;&#20195;&#29702;&#25481;&#32447;&#24773;&#20917;&#30340;&#38656;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#21644;&#37325;&#35201;&#24615;&#37319;&#26679;&#26469;&#23398;&#20064;&#40065;&#26834;MDP&#65292;&#20174;&#32780;&#33021;&#22815;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#20223;&#30495;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies a multi-agent Markov decision process (MDP) that can undergo agent dropout and the computation of policies for the post-dropout system based on control and sampling of the pre-dropout system. The controller's objective is to find an optimal policy that maximizes the value of the expected system given a priori knowledge of the agents' dropout probabilities. Finding an optimal policy for any specific dropout realization is a special case of this problem. For MDPs with a certain transition independence and reward separability structure, we assume that removing agents from the system forms a new MDP comprised of the remaining agents with new state and action spaces, transition dynamics that marginalize the removed agents, and rewards that are independent of the removed agents. We first show that under these assumptions, the value of the expected post-dropout system can be represented by a single MDP; this "robust MDP" eliminates the need to evaluate all $2^N$ realizations
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#26679;&#26412;&#38598;&#20248;&#21270;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#27700;&#19979;&#33322;&#34892;&#22120;&#33337;&#20307;&#65292;&#20854;&#20013;&#20195;&#29702;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#20248;&#21270;&#26356;&#21152;&#24555;&#36895;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2304.12420</link><description>&lt;p&gt;
&#27700;&#19979;&#33322;&#34892;&#22120;&#33337;&#20307;&#30340;&#26679;&#26412;&#39640;&#25928;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient and Surrogate-Based Design Optimization of Underwater Vehicle Hulls. (arXiv:2304.12420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12420
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#26679;&#26412;&#38598;&#20248;&#21270;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#27700;&#19979;&#33322;&#34892;&#22120;&#33337;&#20307;&#65292;&#20854;&#20013;&#20195;&#29702;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#20248;&#21270;&#26356;&#21152;&#24555;&#36895;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#27169;&#25311;&#26159;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;(CAD)&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#35745;&#31639;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20351;&#31934;&#30830;(&#35745;&#31639;&#26114;&#36149;)&#30340;&#27169;&#25311;&#21487;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#20013;&#65292;&#38656;&#35201;&#19968;&#20010;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#20248;&#21270;&#26694;&#26550;&#25110;&#24555;&#36895;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;(&#20195;&#29702;&#27169;&#22411;)&#26469;&#20195;&#26367;&#38271;&#26102;&#38388;&#36816;&#34892;&#30340;&#27169;&#25311;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#20248;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#36827;&#23637;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35774;&#35745;&#19968;&#20010;&#26368;&#20339;&#30340;&#26080;&#20154;&#27700;&#19979;&#33322;&#34892;&#22120;(UUV)&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#20248;&#21270;&#25216;&#26415;&#22312;&#20248;&#21270;&#24490;&#29615;&#20013;&#19982;&#26631;&#20934;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;(CFD)&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#26102;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25910;&#25947;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#36924;&#36817;&#21542;&#21017;&#36890;&#36807;CFD&#27714;&#35299;&#22120;&#36827;&#34892;&#35745;&#31639;&#30340;&#38459;&#21147;&#12290;&#20195;&#29702;&#27169;&#22411;&#36827;&#32780;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#20248;&#21270;&#26694;&#26550;&#20013;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#26631;&#20934;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics simulations are a computational bottleneck in computer-aided design (CAD) optimization processes. Hence, in order to make accurate (computationally expensive) simulations feasible for use in design optimization, one requires either an optimization framework that is highly sample-efficient or fast data-driven proxies (surrogate models) for long running simulations. In this work, we leverage recent advances in optimization and artificial intelligence (AI) to address both of these potential solutions, in the context of designing an optimal unmanned underwater vehicle (UUV). We first investigate and compare the sample efficiency and convergence behavior of different optimization techniques with a standard computational fluid dynamics (CFD) solver in the optimization loop. We then develop a deep neural network (DNN) based surrogate model to approximate drag forces that would otherwise be computed via direct numerical simulation with the CFD solver. The surrogate model is in turn use
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CaLiCa&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#33258;&#26631;&#23450;&#32593;&#32476;&#65292;&#29992;&#20110;&#32852;&#21512;&#33258;&#21160;&#26657;&#20934;&#38024;&#23380;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#22266;&#26377;&#21644;&#22806;&#21442;&#21442;&#25968;&#20197;&#30830;&#20445;&#36710;&#36742;&#22810;&#27169;&#24335;&#24863;&#30693;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#36136;&#37327;&#65292;&#21516;&#26102;&#37319;&#29992;&#23402;&#29983;&#32467;&#26500;&#20197;&#36798;&#21040;&#39046;&#22495;&#20849;&#20139;&#29305;&#24449;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.12412</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#28608;&#20809;&#38647;&#36798;-&#30456;&#26426;&#31471;&#21040;&#31471;&#33258;&#26631;&#23450;
&lt;/p&gt;
&lt;p&gt;
End-to-End Lidar-Camera Self-Calibration for Autonomous Vehicles. (arXiv:2304.12412v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CaLiCa&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#33258;&#26631;&#23450;&#32593;&#32476;&#65292;&#29992;&#20110;&#32852;&#21512;&#33258;&#21160;&#26657;&#20934;&#38024;&#23380;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#22266;&#26377;&#21644;&#22806;&#21442;&#21442;&#25968;&#20197;&#30830;&#20445;&#36710;&#36742;&#22810;&#27169;&#24335;&#24863;&#30693;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#36136;&#37327;&#65292;&#21516;&#26102;&#37319;&#29992;&#23402;&#29983;&#32467;&#26500;&#20197;&#36798;&#21040;&#39046;&#22495;&#20849;&#20139;&#29305;&#24449;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#37197;&#22791;&#20102;&#22810;&#27169;&#24335;&#24863;&#30693;&#20256;&#24863;&#22120;&#65292;&#20197;&#30830;&#20445;&#27773;&#36710;&#23433;&#20840;&#34892;&#39542;&#12290;&#20294;&#26159;&#22914;&#20309;&#22312;&#27773;&#36710;&#36816;&#34892;&#26399;&#38388;&#20445;&#25345;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#36136;&#37327;&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#22914;&#20309;&#32852;&#21512;&#26657;&#20934;&#22810;&#20010;&#20256;&#24863;&#22120;&#20197;&#30830;&#20445;&#31995;&#32479;&#35823;&#24046;&#19981;&#20250;&#20256;&#25773;&#20063;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;CaLiCa&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#33258;&#26631;&#23450;&#32593;&#32476;&#65292;&#38024;&#23545;&#38024;&#23380;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#33258;&#21160;&#26657;&#20934;&#38382;&#39064;&#20570;&#20986;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#36890;&#36807;&#22238;&#24402;&#30456;&#26426;&#22270;&#20687;&#21644;&#28608;&#20809;&#28857;&#20113;&#20043;&#38388;&#30340;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#32852;&#21512;&#39044;&#27979;&#30456;&#26426;&#22266;&#26377;&#21442;&#25968;(&#28966;&#36317;&#21644;&#30072;&#21464;)&#20197;&#21450;&#28608;&#20809;&#38647;&#36798;-&#30456;&#26426;&#22806;&#21442;&#21442;&#25968;(&#26059;&#36716;&#21644;&#24179;&#31227;)&#12290;&#32593;&#32476;&#37319;&#29992;&#23402;&#29983;&#32467;&#26500;&#23433;&#25490;&#20197;&#23558;&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#32422;&#26463;&#22312;&#28857;&#20113;&#21644;&#30456;&#26426;&#22270;&#20687;&#39046;&#22495;&#30340;&#20849;&#20139;&#29305;&#24449;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles are equipped with a multi-modal sensor setup to enable the car to drive safely. The initial calibration of such perception sensors is a highly matured topic and is routinely done in an automated factory environment. However, an intriguing question arises on how to maintain the calibration quality throughout the vehicle's operating duration. Another challenge is to calibrate multiple sensors jointly to ensure no propagation of systemic errors. In this paper, we propose CaLiCa, an end-to-end deep self-calibration network which addresses the automatic calibration problem for pinhole camera and Lidar. We jointly predict the camera intrinsic parameters (focal length and distortion) as well as Lidar-Camera extrinsic parameters (rotation and translation), by regressing feature correlation between the camera image and the Lidar point cloud. The network is arranged in a Siamese-twin structure to constrain the network features learning to a mutually shared feature in both poi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.12410</link><description>&lt;p&gt;
PEFT-Ref: &#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#21442;&#32771;&#26550;&#26500;&#21644;&#31867;&#22411;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques. (arXiv:2304.12410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;(PEFT)&#25216;&#26415;&#26088;&#22312;&#25913;&#21892;&#23436;&#20840;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#38543;&#30528;&#19981;&#21516;&#30340;PEFT&#25216;&#26415;&#19981;&#26029;&#20986;&#29616;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#27604;&#36739;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#19979;&#26041;&#38754;&#65306;(i)&#23427;&#20204;&#28155;&#21152;&#21040;PLM&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;(ii)&#19981;&#21516;&#31867;&#22411;&#21644;&#31243;&#24230;&#30340;&#25928;&#29575;&#25913;&#36827;&#65292;(iii)&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;(iv)&#32467;&#26500;&#21644;&#21151;&#33021;&#24046;&#24322;&#22914;&#20309;&#19982;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#26679;&#30340;&#27604;&#36739;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#21516;&#26102;&#23558;&#24046;&#24322;&#38548;&#31163;&#21040;&#19982;&#26631;&#20934;&#32452;&#20214;&#30340;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#12290;&#36890;&#36807;&#36825;&#20010;&#26631;&#20934;&#21270;&#21644;&#38548;&#31163;&#24046;&#24322;&#30340;&#36807;&#31243;&#65292;&#20986;&#29616;&#20102;PEFT&#25216;&#26415;&#30340;&#27169;&#22359;&#21270;&#35270;&#22270;&#65292;&#19981;&#20165;&#25903;&#25345;&#30452;&#25509;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#31216;&#20026;PEFT-Ref&#65292;&#21253;&#25324;&#19971;&#20010;&#26680;&#24515;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#22788;&#29702;PEFT&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#24182;&#21487;&#29992;&#20316;&#24320;&#21457;&#26032;PEFT&#25216;&#26415;&#21644;&#27604;&#36739;&#29616;&#26377;&#25216;&#26415;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent parameter-efficient finetuning (PEFT) techniques aim to improve over the considerable cost of fully finetuning large pretrained language models (PLM). As different PEFT techniques proliferate, it is becoming difficult to compare them, in particular in terms of (i) the structure and functionality they add to the PLM, (ii) the different types and degrees of efficiency improvements achieved, (iii) performance at different downstream tasks, and (iv) how differences in structure and functionality relate to efficiency and task performance. To facilitate such comparisons, this paper presents a reference framework which standardises aspects shared by different PEFT techniques, while isolating differences to specific locations and interactions with the standard components. Through this process of standardising and isolating differences, a modular view of PEFT techniques emerges, supporting not only direct comparison of different techniques and their efficiency and task performance, but a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20351;&#29992;&#40657;&#30418;API&#36827;&#34892;&#27602;&#24615;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#20381;&#36182;&#32487;&#25215;&#30340;&#33258;&#21160;&#27602;&#24615;&#35780;&#20998;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#24314;&#35758;&#37319;&#29992;&#26356;&#21152;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#35780;&#20272;&#27602;&#24615;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.12397</link><description>&lt;p&gt;
&#35770;&#20351;&#29992;&#40657;&#30418;API&#36827;&#34892;&#27602;&#24615;&#35780;&#20272;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research. (arXiv:2304.12397v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20351;&#29992;&#40657;&#30418;API&#36827;&#34892;&#27602;&#24615;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#20381;&#36182;&#32487;&#25215;&#30340;&#33258;&#21160;&#27602;&#24615;&#35780;&#20998;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#24314;&#35758;&#37319;&#29992;&#26356;&#21152;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#35780;&#20272;&#27602;&#24615;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27602;&#24615;&#30340;&#24863;&#30693;&#38543;&#26102;&#38388;&#25512;&#31227;&#32780;&#19981;&#26029;&#28436;&#21464;&#65292;&#32780;&#19988;&#22312;&#19981;&#21516;&#30340;&#22320;&#29702;&#21644;&#25991;&#21270;&#32972;&#26223;&#20013;&#24448;&#24448;&#23384;&#22312;&#24046;&#24322;&#12290;&#21516;&#26679;&#65292;&#29992;&#20110;&#26816;&#27979;&#27602;&#24615;&#30340;&#21830;&#19994;&#40657;&#30418;API&#65288;&#20363;&#22914;Perspective API&#65289;&#20063;&#19981;&#26159;&#38745;&#24577;&#30340;&#65292;&#32780;&#32463;&#24120;&#37325;&#26032;&#35757;&#32451;&#20197;&#35299;&#20915;&#20219;&#20309;&#26410;&#34987;&#20851;&#27880;&#30340;&#24369;&#28857;&#21644;&#20559;&#35265;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#21464;&#21270;&#23545;&#27604;&#36739;&#26088;&#22312;&#36943;&#21046;&#27602;&#24615;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#30340;&#30456;&#23545;&#20248;&#21155;&#30340;&#30740;&#31350;&#21457;&#29616;&#30340;&#21487;&#37325;&#22797;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20381;&#36182;&#32487;&#25215;&#30340;&#33258;&#21160;&#27602;&#24615;&#35780;&#20998;&#26469;&#27604;&#36739;&#27169;&#22411;&#21644;&#25216;&#26415;&#30340;&#30740;&#31350;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#37325;&#26032;&#23545;HELM&#30340;&#25152;&#26377;&#27169;&#22411;&#36827;&#34892;&#26368;&#26032;&#29256;&#26412;API&#30340;&#27602;&#24615;&#35780;&#20998;&#65292;&#23548;&#33268;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#19981;&#21516;&#25490;&#21517;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#23558;&#30740;&#31350;&#20043;&#38388;&#36827;&#34892;&#39532;&#34562;&#25340;&#25509;&#22411;&#27604;&#36739;&#26102;&#35201;&#35880;&#24910;&#65292;&#24182;&#20026;&#35780;&#20272;&#27602;&#24615;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#26356;&#21152;&#26377;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#25552;&#20986;&#24314;&#35758;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/X/XXX&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. Our findings suggest that research that relied on inherited automatic toxicity scores to compare models and techniques may have resulted in inaccurate findings. Rescoring all models from HELM, a widely respected living benchmark, for toxicity with the recent version of the API led to a different ranking of widely used foundation models. We suggest caution in applying apples-to-apples comparisons between studies and lay recommendations for a more structured approach to evaluating toxicity over time. Code and data are available at https
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#20013;&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12395</link><description>&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#30340;&#26497;&#38480;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Extreme Classification for Answer Type Prediction in Question Answering. (arXiv:2304.12395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#20013;&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#30340;&#26377;&#29992;&#27493;&#39588;&#12290; SMART&#20219;&#21153;&#28041;&#21450;&#39044;&#27979;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#21069;k&#20010;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#31867;&#22411;&#12290;&#30001;&#20110;KG&#20013;&#23384;&#22312;&#22823;&#37327;&#31867;&#22411;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#20855;&#20307;&#22320;&#25913;&#21892;&#20102;XBERT&#27969;&#31243;&#30340;&#32858;&#31867;&#38454;&#27573;&#65292;&#21033;&#29992;&#20174;KG&#20013;&#27966;&#29983;&#30340;&#25991;&#26412;&#21644;&#32467;&#26500;&#29305;&#24449;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;SMART&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#65292;&#24182;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic answer type prediction (SMART) is known to be a useful step towards effective question answering (QA) systems. The SMART task involves predicting the top-$k$ knowledge graph (KG) types for a given natural language question. This is challenging due to the large number of types in KGs. In this paper, we propose use of extreme multi-label classification using Transformer models (XBERT) by clustering KG types using structural and semantic features based on question text. We specifically improve the clustering stage of the XBERT pipeline using textual and structural features derived from KGs. We show that these features can improve end-to-end performance for the SMART task, and yield state-of-the-art results.
&lt;/p&gt;</description></item><item><title>Virus2Vec &#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#29305;&#24449;&#21521;&#37327;&#34920;&#31034;&#30149;&#27602;&#24207;&#21015;&#65292;&#33021;&#22815;&#35782;&#21035;&#30149;&#27602;&#23487;&#20027;&#65292;&#20854;&#22312;&#30149;&#27602;&#23487;&#20027;&#39044;&#27979;&#19978;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#36798;16&#65285;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.12328</link><description>&lt;p&gt;
Virus2Vec: &#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#30149;&#27602;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Virus2Vec: Viral Sequence Classification Using Machine Learning. (arXiv:2304.12328v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12328
&lt;/p&gt;
&lt;p&gt;
Virus2Vec &#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#29305;&#24449;&#21521;&#37327;&#34920;&#31034;&#30149;&#27602;&#24207;&#21015;&#65292;&#33021;&#22815;&#35782;&#21035;&#30149;&#27602;&#23487;&#20027;&#65292;&#20854;&#22312;&#30149;&#27602;&#23487;&#20027;&#39044;&#27979;&#19978;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;&#22810;&#36798;16&#65285;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#19981;&#21516;&#30149;&#27602;&#23478;&#26063;&#30340;&#23487;&#20027;&#29305;&#24322;&#24615;&#21487;&#20197;&#25581;&#31034; SARS-CoV-2&#12289;&#29378;&#29356;&#30149;&#31561;&#21160;&#29289;&#28304;&#24615;&#30149;&#21407;&#20307;&#22312;&#20154;&#31867;&#20013;&#30340;&#36215;&#28304;&#12290;&#36825;&#26377;&#21161;&#20110;&#27969;&#34892;&#30149;&#23398;&#23478;&#12289;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#21450;&#26102;&#36943;&#21046;&#29616;&#26377;&#30340;&#27969;&#34892;&#30149;&#24182;&#39044;&#38450;&#26410;&#26469;&#30340;&#27969;&#34892;&#30149;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Virus2Vec&#65292;&#23427;&#26159;&#30149;&#27602;&#65288;&#26680;&#33527;&#37240;&#25110;&#27688;&#22522;&#37240;&#65289;&#24207;&#21015;&#30340;&#29305;&#24449;&#21521;&#37327;&#34920;&#31034;&#65292;&#21487;&#20197;&#35753;&#22522;&#20110;&#21521;&#37327;&#31354;&#38388;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#30149;&#27602;&#23487;&#20027;&#12290;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20896;&#29366;&#30149;&#27602;&#31185;&#21644;&#29378;&#29356;&#30149;&#27602;&#30340;&#23487;&#20027;&#39044;&#27979;&#19978;&#23558;&#31934;&#24230;&#25552;&#39640;&#20102;&#22810;&#36798;16&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the host-specificity of different families of viruses sheds light on the origin of, e.g., SARS-CoV-2, rabies, and other such zoonotic pathogens in humans. It enables epidemiologists, medical professionals, and policymakers to curb existing epidemics and prevent future ones promptly. In the family Coronaviridae (of which SARS-CoV-2 is a member), it is well-known that the spike protein is the point of contact between the virus and the host cell membrane. On the other hand, the two traditional mammalian orders, Carnivora (carnivores) and Chiroptera (bats) are recognized to be responsible for maintaining and spreading the Rabies Lyssavirus (RABV). We propose Virus2Vec, a feature-vector representation for viral (nucleotide or amino acid) sequences that enable vector-space-based machine learning models to identify viral hosts. Virus2Vec generates numerical feature vectors for unaligned sequences, allowing us to forego the computationally expensive sequence alignment step from t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#28023;&#27915;&#20013;&#21494;&#32511;&#32032;&#30340;&#29983;&#38271;&#19982;&#29289;&#29702;&#21270;&#23398;&#25104;&#20998;&#22914;&#38081;&#12289;&#30813;&#37240;&#30416;&#12289;&#30967;&#37240;&#30416;&#12289;pH&#20540;&#12289;&#30416;&#24230;&#31561;&#30340;&#26368;&#20339;&#27987;&#24230;&#26377;&#20851;&#65292;&#21487;&#20197;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#28023;&#27915;&#21494;&#32511;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.12325</link><description>&lt;p&gt;
&#23398;&#20064;&#25216;&#26415;&#22312;&#28023;&#27915;&#21494;&#32511;&#32032;&#20998;&#26512;&#20013;&#30340;&#29289;&#29702;&#21270;&#23398;&#29305;&#24615;&#20381;&#36182;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dependence of Physiochemical Features on Marine Chlorophyll Analysis with Learning Techniques. (arXiv:2304.12325v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12325
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#28023;&#27915;&#20013;&#21494;&#32511;&#32032;&#30340;&#29983;&#38271;&#19982;&#29289;&#29702;&#21270;&#23398;&#25104;&#20998;&#22914;&#38081;&#12289;&#30813;&#37240;&#30416;&#12289;&#30967;&#37240;&#30416;&#12289;pH&#20540;&#12289;&#30416;&#24230;&#31561;&#30340;&#26368;&#20339;&#27987;&#24230;&#26377;&#20851;&#65292;&#21487;&#20197;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#28023;&#27915;&#21494;&#32511;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#28014;&#28216;&#26893;&#29289;&#20013;&#23384;&#22312;&#30340;&#21494;&#32511;&#32032;&#26159;&#20809;&#21512;&#20316;&#29992;&#30340;&#22522;&#30784;&#65292;&#23545;&#32500;&#25345;&#29983;&#24577;&#24179;&#34913;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#23545;&#20840;&#29699;&#21021;&#32423;&#29983;&#20135;&#21147;&#20316;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#65292;&#24182;&#22788;&#20110;&#35768;&#22810;&#28023;&#27915;&#29983;&#29289;&#30340;&#39135;&#29289;&#38142;&#20013;&#12290;&#28014;&#28216;&#26893;&#29289;&#27987;&#24230;&#30340;&#19981;&#24179;&#34913;&#21487;&#33021;&#20250;&#30772;&#22351;&#29983;&#24577;&#24179;&#34913;&#12290;&#28014;&#28216;&#26893;&#29289;&#30340;&#29983;&#38271;&#21462;&#20915;&#20110;&#29289;&#29702;&#21270;&#23398;&#25104;&#20998;&#30340;&#26368;&#20339;&#27987;&#24230;&#65292;&#22914;&#38081;&#65292;&#30813;&#37240;&#30416;&#65292;&#30967;&#37240;&#30416;&#65292;pH&#20540;&#65292;&#30416;&#24230;&#31561;&#30340;&#20559;&#31163;&#29702;&#24819;&#27987;&#24230;&#21487;&#33021;&#20250;&#24433;&#21709;&#28014;&#28216;&#26893;&#29289;&#30340;&#29983;&#38271;&#65292;&#20174;&#32780;&#26368;&#32456;&#30772;&#22351;&#29983;&#24577;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#20998;&#26512;&#36825;&#20123;&#25104;&#20998;&#20855;&#26377;&#26497;&#39640;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#20272;&#35745;&#28023;&#27915;&#28014;&#28216;&#26893;&#29289;&#30340;&#21487;&#33021;&#29983;&#38271;&#12290;&#36965;&#24863;&#25216;&#26415;&#30340;&#36827;&#27493;&#25913;&#21892;&#20102;&#20840;&#29699;&#33539;&#22260;&#20869;&#36828;&#31243;&#30740;&#31350;&#29289;&#29702;&#21270;&#23398;&#25104;&#20998;&#30340;&#21487;&#33021;&#24615;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20351;&#24471;&#39044;&#27979;&#28023;&#27915;&#21494;&#32511;&#32032;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Marine chlorophyll which is present within phytoplankton are the basis of photosynthesis and they have a high significance in sustaining ecological balance as they highly contribute toward global primary productivity and comes under the food chain of many marine organisms. Imbalance in the concentrations of phytoplankton can disrupt the ecological balance. The growth of phytoplankton depends upon the optimum concentrations of physiochemical constituents like iron, nitrates, phosphates, pH level, salinity, etc. and deviations from an ideal concentration can affect the growth of phytoplankton which can ultimately disrupt the ecosystem at a large scale. Thus the analysis of such constituents has high significance to estimate the probable growth of marine phytoplankton. The advancements of remote sensing technologies have improved the scope to remotely study the physiochemical constituents on a global scale. The machine learning techniques have made it possible to predict the marine chloro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;USA-Net&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#21487;&#24494;&#20998;&#22320;&#22270;&#30340;&#19990;&#30028;&#34920;&#31034;&#65292;&#32534;&#30721;&#20102;&#22330;&#26223;&#30340;&#35821;&#20041;&#21644;&#31354;&#38388;&#20316;&#29992;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#25353;&#29031;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#31227;&#21160;&#12290;&#35813;&#31995;&#32479;&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#30340;&#35745;&#21010;&#31995;&#32479;&#65292;&#21487;&#20197;&#22788;&#29702;&#24320;&#25918;&#24335;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#25163;&#21160;&#24037;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.12164</link><description>&lt;p&gt;
USA-Net&#65306;&#26426;&#22120;&#20154;&#35760;&#24518;&#30340;&#32479;&#19968;&#35821;&#20041;&#21644;&#20316;&#29992;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
USA-Net: Unified Semantic and Affordance Representations for Robot Memory. (arXiv:2304.12164v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;USA-Net&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#21487;&#24494;&#20998;&#22320;&#22270;&#30340;&#19990;&#30028;&#34920;&#31034;&#65292;&#32534;&#30721;&#20102;&#22330;&#26223;&#30340;&#35821;&#20041;&#21644;&#31354;&#38388;&#20316;&#29992;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#25353;&#29031;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#31227;&#21160;&#12290;&#35813;&#31995;&#32479;&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#30340;&#35745;&#21010;&#31995;&#32479;&#65292;&#21487;&#20197;&#22788;&#29702;&#24320;&#25918;&#24335;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#25163;&#21160;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35753;&#26426;&#22120;&#20154;&#33021;&#22815;&#25353;&#29031;&#8220;&#21435;&#25171;&#24320;&#27700;&#27133;&#19978;&#26041;&#30340;&#26837;&#33394;&#27249;&#26588;&#8221;&#31561;&#24320;&#25918;&#24335;&#25351;&#20196;&#34892;&#21160;&#65292;&#23427;&#20204;&#38656;&#35201;&#29702;&#35299;&#22330;&#26223;&#20960;&#20309;&#24418;&#29366;&#21644;&#29615;&#22659;&#35821;&#20041;&#12290;&#26426;&#22120;&#20154;&#31995;&#32479;&#36890;&#24120;&#36890;&#36807;&#20998;&#24320;&#19981;&#21516;&#30340;&#27969;&#31243;&#26469;&#22788;&#29702;&#36825;&#20123;&#25351;&#20196;&#65292;&#26377;&#26102;&#20351;&#29992;&#38750;&#24120;&#19981;&#21516;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#24403;&#20004;&#20010;&#30446;&#26631;&#20914;&#31361;&#26102;&#36825;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;USA-Net&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#21487;&#24494;&#20998;&#22320;&#22270;&#30340;&#19990;&#30028;&#34920;&#31034;&#65292;&#35813;&#22320;&#22270;&#32534;&#30721;&#20102;&#22330;&#26223;&#30340;&#35821;&#20041;&#21644;&#31354;&#38388;&#20316;&#29992;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#22522;&#20110;&#26799;&#24230;&#30340;&#35268;&#21010;&#22120;&#65292;&#35813;&#35268;&#21010;&#22120;&#21487;&#20197;&#23548;&#33322;&#21040;&#20351;&#29992;&#24320;&#25918;&#24335;&#35789;&#27719;&#25351;&#23450;&#30340;&#22330;&#26223;&#20301;&#32622;&#12290;&#22312;CLIP&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#35268;&#21010;&#22120;&#19968;&#36143;&#29983;&#25104;&#36739;&#30701;&#65288;5-10%&#65289;&#21644;&#26356;&#25509;&#36817;&#30446;&#26631;&#26597;&#35810;&#65288;10-30%&#65289;&#30340;&#36335;&#24452;&#65292;&#32780;&#36825;&#20123;&#36335;&#24452;&#26469;&#33258;&#19981;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#30340;&#21487;&#27604;&#36739;&#22522;&#20110;&#32593;&#26684;&#30340;&#35268;&#21010;&#22120;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#30340;&#35745;&#21010;&#31995;&#32479;&#65292;&#21487;&#20197;&#22788;&#29702;&#24320;&#25918;&#24335;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#25163;&#21160;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order for robots to follow open-ended instructions like "go open the brown cabinet over the sink", they require an understanding of both the scene geometry and the semantics of their environment. Robotic systems often handle these through separate pipelines, sometimes using very different representation spaces, which can be suboptimal when the two objectives conflict. In this work, we present USA-Net, a simple method for constructing a world representation that encodes both the semantics and spatial affordances of a scene in a differentiable map. This allows us to build a gradient-based planner which can navigate to locations in the scene specified using open-ended vocabulary. We use this planner to consistently generate trajectories which are both shorter 5-10% shorter and 10-30% closer to our goal query in CLIP embedding space than paths from comparable grid-based planners which don't leverage gradient information. To our knowledge, this is the first end-to-end differentiable plan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#36328;&#25991;&#21270;&#20262;&#29702;&#23454;&#36341;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#25991;&#21270;&#24046;&#24322;&#22914;&#20309;&#24433;&#21709;&#25216;&#26415;&#30340;&#37319;&#32435;&#21644;&#20351;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#25216;&#26415;&#27010;&#24565;&#22914;&#20309;&#24110;&#21161;&#37027;&#20123;&#22312;&#31038;&#20250;&#21644;&#25991;&#21270;&#22810;&#26679;&#24615;&#22320;&#21306;&#20013;&#20855;&#26377;&#19981;&#21516;&#38656;&#27714;&#30340;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2304.11861</link><description>&lt;p&gt;
&#35770;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#36328;&#25991;&#21270;&#20262;&#29702;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Towards a Praxis for Intercultural Ethics in Explainable AI. (arXiv:2304.11861v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11861
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#36328;&#25991;&#21270;&#20262;&#29702;&#23454;&#36341;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#25991;&#21270;&#24046;&#24322;&#22914;&#20309;&#24433;&#21709;&#25216;&#26415;&#30340;&#37319;&#32435;&#21644;&#20351;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#25216;&#26415;&#27010;&#24565;&#22914;&#20309;&#24110;&#21161;&#37027;&#20123;&#22312;&#31038;&#20250;&#21644;&#25991;&#21270;&#22810;&#26679;&#24615;&#22320;&#21306;&#20013;&#20855;&#26377;&#19981;&#21516;&#38656;&#27714;&#30340;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#36890;&#24120;&#34987;&#23459;&#20256;&#20026;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21151;&#33021;&#21644;&#39044;&#27979;&#20135;&#29983;&#30340;&#21407;&#22240;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#22909;&#22788;&#22823;&#22810;&#20026;&#37027;&#20123;&#20855;&#26377;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#30340;&#20154;&#25152;&#20445;&#30041;&#65292;&#27604;&#22914;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20154;&#21592;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#35748;&#20026;&#65292;&#20351;AI&#21487;&#35299;&#37322;&#21487;&#33021;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;AI&#26356;&#26377;&#29992;&#30340;&#19968;&#31181;&#21487;&#34892;&#26041;&#24335;&#65292;&#23588;&#20854;&#26159;&#22312;&#20840;&#29699;&#21335;&#26041;&#20302;&#36164;&#28304;&#39046;&#22495;&#20869;&#12290;&#23613;&#31649;AI&#24050;&#32463;&#36328;&#36234;&#20102;&#22269;&#30028;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#23558;&#35299;&#37322;AI&#27010;&#24565;&#27665;&#20027;&#21270;&#21040;&#8220;&#22823;&#22810;&#25968;&#19990;&#30028;&#8221;&#20869;&#65292;&#36825;&#30041;&#32473;&#25105;&#20204;&#22312;&#36825;&#20010;&#39046;&#22495;&#25506;&#32034;&#21644;&#24320;&#21457;&#26032;&#26041;&#27861;&#30340;&#26426;&#20250;&#65292;&#20197;&#28385;&#36275;&#22312;&#25991;&#21270;&#21644;&#31038;&#20132;&#22810;&#26679;&#21270;&#30340;&#22320;&#21306;&#20013;&#20855;&#26377;&#19981;&#21516;&#38656;&#27714;&#30340;&#29992;&#25143;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#25991;&#21270;&#20262;&#29702;&#23454;&#36341;&#30340;AI&#21487;&#35299;&#37322;&#26041;&#27861;&#12290;&#23427;&#30740;&#31350;&#20102;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#22914;&#20309;&#24433;&#21709;&#25216;&#26415;&#37319;&#32435;&#21644;&#20351;&#29992;&#65292;&#20197;&#21450;&#38459;&#30861;&#35299;&#37322;&#25216;&#26415;&#27010;&#24565;&#22914;AI&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) is often promoted with the idea of helping users understand how machine learning models function and produce predictions. Still, most of these benefits are reserved for those with specialized domain knowledge, such as machine learning developers. Recent research has argued that making AI explainable can be a viable way of making AI more useful in real-world contexts, especially within low-resource domains in the Global South. While AI has transcended borders, a limited amount of work focuses on democratizing the concept of explainable AI to the "majority world", leaving much room to explore and develop new approaches within this space that cater to the distinct needs of users within culturally and socially-diverse regions. This article introduces the concept of an intercultural ethics approach to AI explainability. It examines how cultural nuances impact the adoption and use of technology, the factors that impede how technical concepts such as AI are explained, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#32423;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120; (HDAE) &#65292;&#21033;&#29992;&#32454;&#31890;&#24230;&#21040;&#25277;&#35937;&#21644;&#20302;&#32423;&#21040;&#39640;&#32423;&#30340;&#29305;&#24449;&#23618;&#27425;&#32467;&#26500;&#26469;&#26500;&#24314;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#28508;&#22312;&#31354;&#38388;&#12290;&#35813;&#26041;&#27861;&#26356;&#20840;&#38754;&#22320;&#32534;&#30721;&#20102;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#35821;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#25130;&#26029;&#29305;&#24449;&#30340;&#20998;&#35299;&#22270;&#20687;&#25805;&#20316;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.11829</link><description>&lt;p&gt;
&#20998;&#32423;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#21644;&#20998;&#35299;&#22270;&#20687;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation. (arXiv:2304.11829v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#32423;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120; (HDAE) &#65292;&#21033;&#29992;&#32454;&#31890;&#24230;&#21040;&#25277;&#35937;&#21644;&#20302;&#32423;&#21040;&#39640;&#32423;&#30340;&#29305;&#24449;&#23618;&#27425;&#32467;&#26500;&#26469;&#26500;&#24314;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#28508;&#22312;&#31354;&#38388;&#12290;&#35813;&#26041;&#27861;&#26356;&#20840;&#38754;&#22320;&#32534;&#30721;&#20102;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#35821;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#25130;&#26029;&#29305;&#24449;&#30340;&#20998;&#35299;&#22270;&#20687;&#25805;&#20316;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35299;&#37322;&#21644;&#25805;&#20316;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#24182;&#27809;&#26377;&#34987;&#24191;&#27867;&#25506;&#32034;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#23558;&#35821;&#20041;&#34920;&#31034;&#32534;&#30721;&#21040;&#19968;&#20010;&#35821;&#20041;&#28508;&#22312;&#20195;&#30721;&#20013;&#65292;&#20294;&#19981;&#33021;&#21453;&#26144;&#32454;&#33410;&#21644;&#20869;&#22312;&#29305;&#24449;&#23618;&#27425;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#32423;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120; (HDAE)&#65292;&#21033;&#29992;&#32454;&#31890;&#24230;&#21040;&#25277;&#35937;&#21644;&#20302;&#32423;&#21040;&#39640;&#32423;&#30340;&#29305;&#24449;&#23618;&#27425;&#32467;&#26500;&#26469;&#26500;&#24314;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290; HDAE &#30340;&#20998;&#23618;&#28508;&#22312;&#31354;&#38388;&#22266;&#26377;&#22320;&#32534;&#30721;&#20102;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#35821;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25130;&#26029;&#29305;&#24449;&#30340;&#20998;&#35299;&#22270;&#20687;&#25805;&#20316;&#26041;&#27861;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#21644;&#24212;&#29992;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22270;&#29255;&#37325;&#24314;&#12289;&#39118;&#26684;&#28151;&#21512;&#12289;&#21487;&#25511;&#29983;&#25104;&#21644;&#22270;&#20687;&#32534;&#36753;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have attained impressive visual quality for image synthesis. However, how to interpret and manipulate the latent space of diffusion models has not been extensively explored. Prior work diffusion autoencoders encode the semantic representations into a semantic latent code, which fails to reflect the rich information of details and the intrinsic feature hierarchy. To mitigate those limitations, we propose Hierarchical Diffusion Autoencoders (HDAE) that exploit the fine-grained-to-abstract and lowlevel-to-high-level feature hierarchy for the latent space of diffusion models. The hierarchical latent space of HDAE inherently encodes different abstract levels of semantics and provides more comprehensive semantic representations. In addition, we propose a truncated-feature-based approach for disentangled image manipulation. We demonstrate the effectiveness of our proposed approach with extensive experiments and applications on image reconstruction, style mixing, controllable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#21644;&#32771;&#34385;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#24615;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20013;&#24102;&#26469;&#20102;&#26356;&#22810;&#25361;&#25112;&#65292;&#24517;&#39035;&#35299;&#20915;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#12289;&#33258;&#20027;&#26435;&#12289;&#36131;&#20219;&#21644;&#38382;&#36131;&#21046;&#31561;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#21150;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.11530</link><description>&lt;p&gt;
&#36890;&#36807;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#30830;&#20445;&#21487;&#20449;&#36182;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Ensuring Trustworthy Medical Artificial Intelligencethrough Ethical and Philosophical Principles. (arXiv:2304.11530v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#21644;&#32771;&#34385;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#24615;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20013;&#24102;&#26469;&#20102;&#26356;&#22810;&#25361;&#25112;&#65292;&#24517;&#39035;&#35299;&#20915;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#12289;&#33258;&#20027;&#26435;&#12289;&#36131;&#20219;&#21644;&#38382;&#36131;&#21046;&#31561;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#21150;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#21307;&#30103;&#25252;&#29702;&#26041;&#38754;&#20855;&#26377;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#39640;&#21307;&#30103;&#19987;&#23478;&#21644;&#24739;&#32773;&#30340;&#20307;&#39564;&#26469;&#24443;&#24213;&#25913;&#21464;&#20247;&#22810;&#21307;&#30103;&#25252;&#29702;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#24037;&#20855;&#22914;&#26524;&#33021;&#22815;&#34920;&#29616;&#20986;&#33394;&#29978;&#33267;&#19982;&#20020;&#24202;&#19987;&#23478;&#30340;&#27700;&#24179;&#30456;&#24403;&#65292;&#23601;&#21487;&#20197;&#20135;&#29983;&#24040;&#22823;&#30340;&#25928;&#30410;&#12290;&#22240;&#27492;&#65292;&#21457;&#23637;&#20013;&#22269;&#23478;&#21487;&#20197;&#25552;&#20379;&#20808;&#36827;&#30340;&#21307;&#30103;&#25252;&#29702;&#26381;&#21153;&#65292;&#24182;&#35299;&#20915;&#32570;&#20047;&#19987;&#19994;&#21307;&#30103;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#21487;&#20197;&#33410;&#30465;&#26102;&#38388;&#12289;&#36164;&#28304;&#21644;&#25972;&#20307;&#27835;&#30103;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#19982;&#20154;&#31867;&#30456;&#27604;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#25581;&#31034;&#22823;&#37327;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#29978;&#33267;&#21487;&#20197;&#20026;&#21307;&#23398;&#25552;&#20379;&#26032;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#30103;&#25252;&#29702;&#20013;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#20063;&#24102;&#26469;&#20102;&#20960;&#20010;&#20262;&#29702;&#21644;&#21746;&#23398;&#19978;&#30340;&#38382;&#39064;&#65292;&#22914;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#12289;&#33258;&#20027;&#26435;&#12289;&#36131;&#20219;&#21644;&#38382;&#36131;&#21046;&#65292;&#36825;&#20123;&#38382;&#39064;&#24517;&#39035;&#22312;&#23558;&#36825;&#20123;&#24037;&#20855;&#25972;&#21512;&#21040;&#20020;&#24202;&#29615;&#22659;&#20043;&#21069;&#24471;&#21040;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#25252;&#29702;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#20197;&#21450;&#32771;&#34385;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#20197;&#30830;&#20445;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#21307;&#30103;&#25252;&#29702;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#30340;&#38656;&#35201;&#12289;&#33258;&#20027;&#20915;&#31574;&#30340;&#38382;&#39064;&#20197;&#21450;&#38382;&#36131;&#21046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#28508;&#22312;&#26041;&#26696;&#65292;&#21253;&#25324;&#30830;&#20445;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;&#30340;&#26694;&#26550;&#20197;&#21450;&#25351;&#23548;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#32771;&#34385;&#20262;&#29702;&#21407;&#21017;&#30340;&#25351;&#21335;&#12290;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#24182;&#23454;&#26045;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#20445;&#24320;&#21457;&#20986;&#31526;&#21512;&#35786;&#25152;&#35774;&#32622;&#30340;&#21463;&#20449;&#20219;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) methods have great potential to revolutionize numerous medical care by enhancing the experience of medical experts and patients. AI based computer-assisted diagnosis tools can have a tremendous benefit if they can outperform or perform similarly to the level of a clinical expert. As a result, advanced healthcare services can be affordable in developing nations, and the problem of a lack of expert medical practitioners can be addressed. AI based tools can save time, resources, and overall cost for patient treatment. Furthermore, in contrast to humans, AI can uncover complex relations in the data from a large set of inputs and even lead to new evidence-based knowledge in medicine. However, integrating AI in healthcare raises several ethical and philosophical concerns, such as bias, transparency, autonomy, responsibility and accountability, which must be addressed before integrating such tools into clinical settings. In this article, we emphasize recent advanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#25552;&#21319;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#29305;&#21035;&#26159;ToM&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11490</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#26234;&#29702;&#35770;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Boosting Theory-of-Mind Performance in Large Language Models via Prompting. (arXiv:2304.11490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#25552;&#21319;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#29305;&#21035;&#26159;ToM&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22797;&#26434;&#25512;&#29702;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#38656;&#35201;&#29702;&#35299;&#20195;&#29702;&#20154;&#30340;&#20449;&#24565;&#12289;&#30446;&#26631;&#21644;&#24515;&#29702;&#29366;&#24577;&#65292;&#23545;&#20110;&#28041;&#21450;&#20154;&#31867;&#30340;&#24120;&#35782;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#25552;&#39640;LLM&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#27979;&#37327;&#20102;GPT-4&#21644;&#19977;&#20010;GPT-3.5&#21464;&#20307;&#65288;Davinci-2&#12289;Davinci-3&#12289;GPT-3.5-Turbo&#65289;&#30340;ToM&#34920;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#23427;&#20204;&#30340;ToM&#29702;&#35299;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21253;&#21547;&#20004;&#27493;&#24605;&#32500;&#25512;&#29702;&#21644;&#36880;&#27493;&#24605;&#32771;&#35828;&#26126;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#35757;&#32451;&#30340;LLMs&#65288;&#38500;Davinci-2&#22806;&#30340;&#25152;&#26377;&#27169;&#22411;&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;ToM&#20934;&#30830;&#24615;&#12290;GPT-4&#22312;&#38646;&#36718;&#24773;&#20917;&#19979;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;&#36817;80%&#30340;ToM&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#19981;&#36275;&#27979;&#35797;&#38598;&#19978;87%&#30340;&#20154;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#25552;&#20379;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25552;&#31034;&#26102;&#65292;GPT-4&#21644;&#19977;&#20010;GPT-3.5&#21464;&#20307;&#30340;ToM&#20934;&#30830;&#24615;&#26174;&#33879;&#39640;&#20110;&#26080;&#25552;&#31034;&#26102;&#65292;&#20854;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#65288;GPT-3.5-Turbo&#65289;&#36798;&#21040;&#20102;92%&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#21319;LLM&#22312;&#22797;&#26434;&#25512;&#29702;&#23588;&#20854;&#26159;ToM&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11171</link><description>&lt;p&gt;
&#39063;&#31890;&#29699;&#35745;&#31639;&#65306;&#19968;&#31181;&#39640;&#25928;&#12289;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35748;&#30693;&#20855;&#26377;&#8220;&#20808;&#22823;&#21518;&#23567;&#8221;&#30340;&#35748;&#30693;&#26426;&#21046;&#65292;&#22240;&#27492;&#20855;&#26377;&#33258;&#36866;&#24212;&#30340;&#22810;&#31890;&#24230;&#25551;&#36848;&#33021;&#21147;&#12290;&#36825;&#23548;&#33268;&#20102;&#26377;&#25928;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#35745;&#31639;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#12290;&#20182;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human cognition has a ``large-scale first'' cognitive mechanism, therefore possesses adaptive multi-granularity description capabilities. This results in computational characteristics such as efficiency, robustness, and interpretability. Although most existing artificial intelligence learning methods have certain multi-granularity features, they do not fully align with the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball computing is an important model method developed in recent years. This method can use granular-balls of different sizes to adaptively represent and cover the sample space, and perform learning based on granular-balls. Since the number of coarse-grained "granular-ball" is smaller than the number of sample points, granular-ball computing is more efficient; the coarse-grained characteristics of granular-balls are less likely to be affected by fine-grained sample points, making them more robust; the multi-granularity structure of granular-balls ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20132;&#36890;&#36335;&#32593;&#20013;&#32570;&#23569;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#34917;&#20805;&#27969;&#37327;&#29366;&#24577;&#25110;&#29366;&#24577;&#21644;&#21160;&#20316;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#26465;&#20214;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.10722</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Approaches for Traffic Signal Control under Missing Data. (arXiv:2304.10722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20132;&#36890;&#36335;&#32593;&#20013;&#32570;&#23569;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#34917;&#20805;&#27969;&#37327;&#29366;&#24577;&#25110;&#29366;&#24577;&#21644;&#21160;&#20316;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#26465;&#20214;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#20132;&#36890;&#29366;&#24577;&#30340;&#32570;&#22833;&#21487;&#33021;&#32463;&#24120;&#21457;&#29983;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#32570;&#23569;&#20256;&#24863;&#22120;&#30340;&#36335;&#32593;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#25511;&#21046;&#20132;&#36890;&#20449;&#21495;&#22312;&#29616;&#23454;&#29615;&#22659;&#19979;&#30340;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#19968;&#20123;&#36335;&#21475;&#27809;&#26377;&#23433;&#35013;&#20256;&#24863;&#22120;&#65292;&#22240;&#27492;&#21608;&#22260;&#27809;&#26377;&#30452;&#25509;&#35266;&#23519;&#25968;&#25454;&#12290;&#22312;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65306;&#31532;&#19968;&#31181;&#26041;&#26696;&#34917;&#20805;&#27969;&#37327;&#29366;&#24577;&#20197;&#23454;&#29616;&#33258;&#36866;&#24212;&#25511;&#21046;&#65292;&#31532;&#20108;&#31181;&#26041;&#26696;&#34917;&#20805;&#29366;&#24577;&#21644;&#21160;&#20316;&#20197;&#36827;&#34892;&#26465;&#20214;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of reinforcement learning (RL) methods in traffic signal control tasks has achieved better performance than conventional rule-based approaches. Most RL approaches require the observation of the environment for the agent to decide which action is optimal for a long-term reward. However, in real-world urban scenarios, missing observation of traffic states may frequently occur due to the lack of sensors, which makes existing RL methods inapplicable on road networks with missing observation. In this work, we aim to control the traffic signals in a real-world setting, where some of the intersections in the road network are not installed with sensors and thus with no direct observations around them. To the best of our knowledge, we are the first to use RL methods to tackle the traffic signal control problem in this real-world setting. Specifically, we propose two solutions: the first one imputes the traffic states to enable adaptive control, and the second one imputes both stat
&lt;/p&gt;</description></item><item><title>Adam&#20248;&#21270;&#31639;&#27861;&#22312;&#22823;&#25209;&#37327;&#30340;&#35757;&#32451;&#19979;&#23481;&#26131;&#20986;&#29616;&#19981;&#31283;&#23450;&#29616;&#35937;&#65292;&#24182;&#23548;&#33268;&#35757;&#32451;&#24322;&#24120;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35813;&#29616;&#35937;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.09871</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20013;Adam&#19981;&#31283;&#23450;&#24615;&#30340;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Theory on Adam Instability in Large-Scale Machine Learning. (arXiv:2304.09871v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09871
&lt;/p&gt;
&lt;p&gt;
Adam&#20248;&#21270;&#31639;&#27861;&#22312;&#22823;&#25209;&#37327;&#30340;&#35757;&#32451;&#19979;&#23481;&#26131;&#20986;&#29616;&#19981;&#31283;&#23450;&#29616;&#35937;&#65292;&#24182;&#23548;&#33268;&#35757;&#32451;&#24322;&#24120;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35813;&#29616;&#35937;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20043;&#21069;&#26410;&#34987;&#35299;&#37322;&#30340;&#29616;&#35937;&#30340;&#29702;&#35770;&#65292;&#35813;&#29616;&#35937;&#20986;&#29616;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#21457;&#25955;&#34892;&#20026;&#20013;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#20027;&#27969;&#30340;&#20248;&#21270;&#31639;&#27861; Adam &#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040; Adam &#21487;&#33021;&#20250;&#36827;&#20837;&#19968;&#31181;&#29366;&#24577;&#65292;&#20854;&#20013;&#21442;&#25968;&#26356;&#26032;&#21521;&#37327;&#26377;&#27604;&#36739;&#22823;&#30340;&#33539;&#25968;&#65292;&#24182;&#19988;&#19982;&#35757;&#32451;&#25439;&#22833;&#26223;&#35266;&#19979;&#30340;&#19979;&#38477;&#26041;&#21521;&#22522;&#26412;&#26080;&#20851;&#65292;&#20174;&#32780;&#23548;&#33268;&#21457;&#25955;&#12290;&#36825;&#31181;&#29616;&#35937;&#26356;&#23481;&#26131;&#22312;&#22823;&#25209;&#37327;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#36825;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#20856;&#22411;&#35774;&#32622;&#12290;&#20026;&#20102;&#35777;&#26126;&#35813;&#29702;&#35770;&#65292;&#25105;&#20204;&#23545;&#35268;&#27169;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;70&#20159;&#65292;300&#20159;&#65292;650&#20159;&#21644;5460&#20159;&#21442;&#25968;&#65289;&#36827;&#34892;&#20102;&#35757;&#32451;&#36816;&#34892;&#30340;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a theory for the previously unexplained divergent behavior noticed in the training of large language models. We argue that the phenomenon is an artifact of the dominant optimization algorithm used for training, called Adam. We observe that Adam can enter a state in which the parameter update vector has a relatively large norm and is essentially uncorrelated with the direction of descent on the training loss landscape, leading to divergence. This artifact is more likely to be observed in the training of a deep model with a large batch size, which is the typical setting of large-scale language model training. To argue the theory, we present observations from the training runs of the language models of different scales: 7 billion, 30 billion, 65 billion, and 546 billion parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;MLA&#65289;&#20197;&#26368;&#22823;&#21270;&#25512;&#29702;&#36136;&#37327;&#24182;&#23454;&#29616;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#38899;&#21040;&#25991;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;S2T NMT&#65289;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#21516;&#22768;&#20256;&#35793;&#65288;SimulST&#65289;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#39057;&#35889;&#22270;&#20219;&#21153;&#65292;&#22312;TTS&#19978;&#20855;&#26377;&#39640;&#25928;&#29575;&#25910;&#30410;&#65292;&#22312;NMT&#21644;SimulST&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08453</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#25913;&#36827;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Improving Autoregressive NLP Tasks via Modular Linearized Attention. (arXiv:2304.08453v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;MLA&#65289;&#20197;&#26368;&#22823;&#21270;&#25512;&#29702;&#36136;&#37327;&#24182;&#23454;&#29616;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#38899;&#21040;&#25991;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;S2T NMT&#65289;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#21516;&#22768;&#20256;&#35793;&#65288;SimulST&#65289;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#39057;&#35889;&#22270;&#20219;&#21153;&#65292;&#22312;TTS&#19978;&#20855;&#26377;&#39640;&#25928;&#29575;&#25910;&#30410;&#65292;&#22312;NMT&#21644;SimulST&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#30340;&#27169;&#22411;&#24517;&#39035;&#22312;&#26368;&#32456;&#24212;&#29992;&#20110;&#36793;&#32536;&#25110;&#20854;&#20182;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#23567;&#22411;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#23558;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#23567;&#20943;&#23567;&#65292;&#20294;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#33258;&#22238;&#24402;&#20219;&#21153;&#32780;&#35328;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;MLA&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#20010;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21253;&#25324;cosFormer&#65292;&#20197;&#26368;&#22823;&#21270;&#25512;&#29702;&#36136;&#37327;&#24182;&#23454;&#29616;&#26174;&#30528;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#38899;&#21040;&#25991;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;S2T NMT&#65289;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#21516;&#22768;&#20256;&#35793;&#65288;SimulST&#65289;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#39057;&#35889;&#22270;&#20219;&#21153;&#65292;&#22312;TTS&#19978;&#20855;&#26377;&#39640;&#25928;&#29575;&#25910;&#30410;&#65292;&#22312;NMT&#21644;SimulST&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various natural language processing (NLP) tasks necessitate models that are efficient and small based on their ultimate application at the edge or in other resource-constrained environments. While prior research has reduced the size of these models, increasing computational efficiency without considerable performance impacts remains difficult, especially for autoregressive tasks. This paper proposes {modular linearized attention (MLA), which combines multiple efficient attention mechanisms, including cosFormer, to maximize inference quality while achieving notable speedups. We validate this approach on several autoregressive NLP tasks, including speech-to-text neural machine translation (S2T NMT), speech-to-text simultaneous translation (SimulST), and autoregressive text-to-spectrogram, noting efficiency gains on TTS and competitive performance for NMT and SimulST during training and inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#22312;&#20013;&#25991;&#35821;&#35328;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2304.07987</link><description>&lt;p&gt;
&#20013;&#25991;&#24320;&#25918;&#24335;&#25351;&#20196;&#24191;&#20041;&#35821;&#35328;&#27169;&#22411;&#65306;&#21021;&#27493;&#21457;&#24067;
&lt;/p&gt;
&lt;p&gt;
Chinese Open Instruction Generalist: A Preliminary Release. (arXiv:2304.07987v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#22312;&#20013;&#25991;&#35821;&#35328;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#26500;&#24314;&#24191;&#20041;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#38543;&#30528;InstructGPT&#21644;ChatGPT&#30340;&#21457;&#24067;&#65292;&#23427;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#22522;&#30784;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#36824;&#26410;&#25506;&#32034;&#33521;&#35821;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#20219;&#21153;&#19978;&#26159;&#21542;&#21487;&#20197;&#20687;&#33521;&#35821;&#20219;&#21153;&#37027;&#26679;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25351;&#20196;&#35843;&#25972;&#26469;&#25191;&#34892;&#65292;&#20197;&#21450;&#25105;&#20204;&#22914;&#20309;&#26500;&#24314;&#25152;&#38656;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35843;&#25972;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39033;&#30446;&#65292;&#35797;&#22270;&#36890;&#36807;&#36866;&#24212;4&#20010;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#37319;&#29992;&#21508;&#31181;&#26041;&#27861;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#32422;20&#19975;&#20010;&#20013;&#25991;&#25351;&#20196;&#35843;&#25972;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#26816;&#26597;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#33521;&#25991;&#21644;&#20013;&#25991;&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#24182;&#23545;&#19968;&#20123;&#28508;&#22312;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31616;&#35201;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of InstructGPT~\citep{ouyang2022training} and ChatGPT\footnote{\url{https://chat.openai.com/}}. Despite impressive progress in English-oriented large-scale language models (LLMs), it is still under-explored whether English-based foundation LLMs can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning.  To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#26681;&#25454;&#29992;&#20110;&#23450;&#20041;&#23427;&#20204;&#30340;&#23646;&#24615;&#26469;&#35782;&#21035;&#23454;&#20307;&#31867;&#22411;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#19968;&#32452;&#22522;&#20110;&#23646;&#24615;&#30340;&#24230;&#37327;&#21644;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#20307;&#31867;&#22411;&#35782;&#21035;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07910</link><description>&lt;p&gt;
&#22522;&#20110;&#23646;&#24615;&#35782;&#21035;&#23454;&#20307;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
Recognizing Entity Types via Properties. (arXiv:2304.07910v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#26681;&#25454;&#29992;&#20110;&#23450;&#20041;&#23427;&#20204;&#30340;&#23646;&#24615;&#26469;&#35782;&#21035;&#23454;&#20307;&#31867;&#22411;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#19968;&#32452;&#22522;&#20110;&#23646;&#24615;&#30340;&#24230;&#37327;&#21644;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#20307;&#31867;&#22411;&#35782;&#21035;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#26412;&#20307;&#30340;&#20027;&#27969;&#26041;&#27861;&#26159;&#21512;&#24182;&#32534;&#30721;&#19981;&#21516;&#20449;&#24687;&#30340;&#26412;&#20307;&#65292;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#22256;&#38590;&#26159;&#24322;&#26500;&#24615;&#20419;&#20351;&#26412;&#20307;&#21512;&#24182;&#20294;&#20063;&#38480;&#21046;&#39640;&#36136;&#37327;&#30340;&#21512;&#24182;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#23454;&#20307;&#31867;&#22411;&#35782;&#21035;&#20219;&#21153;&#20197;&#22788;&#29702;&#36825;&#31181;&#24322;&#26500;&#24615;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26412;&#20307;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#25512;&#26029;&#23454;&#20307;&#21644;&#23454;&#20307;&#31867;&#22411;&#30340;&#31867;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#26681;&#25454;&#29992;&#20110;&#23450;&#20041;&#23427;&#20204;&#30340;&#23646;&#24615;&#26469;&#35782;&#21035;&#23454;&#20307;&#31867;&#22411;&#12290;&#20174;&#35748;&#35782;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23454;&#38469;&#19978;&#26159;&#23646;&#24615;&#34920;&#24449;&#23454;&#20307;&#21644;&#23454;&#20307;&#31867;&#22411;&#65292;&#36825;&#20010;&#23450;&#20041;&#29420;&#31435;&#20110;&#29992;&#20110;&#23450;&#20041;&#23427;&#20204;&#30340;&#20855;&#20307;&#26631;&#31614;&#21644;&#23618;&#27425;&#32467;&#26500;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#19968;&#32452;&#22522;&#20110;&#23646;&#24615;&#30340;&#24230;&#37327;&#65292;&#20197;&#34913;&#37327;&#23454;&#20307;&#31867;&#22411;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#20307;&#31867;&#22411;&#35782;&#21035;&#31639;&#27861;exploiting p&#12290;
&lt;/p&gt;
&lt;p&gt;
The mainstream approach to the development of ontologies is merging ontologies encoding different information, where one of the major difficulties is that the heterogeneity motivates the ontology merging but also limits high-quality merging performance. Thus, the entity type (etype) recognition task is proposed to deal with such heterogeneity, aiming to infer the class of entities and etypes by exploiting the information encoded in ontologies. In this paper, we introduce a property-based approach that allows recognizing etypes on the basis of the properties used to define them. From an epistemological point of view, it is in fact properties that characterize entities and etypes, and this definition is independent of the specific labels and hierarchical schemas used to define them. The main contribution consists of a set of property-based metrics for measuring the contextual similarity between etypes and entities, and a machine learning-based etype recognition algorithm exploiting the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36739;&#23569;&#30340;&#28436;&#31034;&#25968;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#20219;&#21153;&#65292;&#22312;&#27979;&#35797;&#26597;&#35810;&#19978;&#21482;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#26102;&#24182;&#27809;&#26377;&#26126;&#26174;&#24615;&#33021;&#19979;&#38477;&#65292;&#32780;&#21482;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#28436;&#31034;&#30340;ICL&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20840;&#28436;&#31034;ICL&#12290;</title><link>http://arxiv.org/abs/2303.08119</link><description>&lt;p&gt;
&#19968;&#20154;&#29420;&#33310;&#22909;&#36824;&#26159;&#20154;&#22810;&#38393;&#24515;&#65311;&#19981;&#21516;&#28436;&#31034;&#27425;&#25968;&#19979;&#30340;&#19978;&#19979;&#25991;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
It Takes One to Tango but More Make Trouble? In-Context Training with Different Number of Demonstrations. (arXiv:2303.08119v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36739;&#23569;&#30340;&#28436;&#31034;&#25968;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#20219;&#21153;&#65292;&#22312;&#27979;&#35797;&#26597;&#35810;&#19978;&#21482;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#26102;&#24182;&#27809;&#26377;&#26126;&#26174;&#24615;&#33021;&#19979;&#38477;&#65292;&#32780;&#21482;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#28436;&#31034;&#30340;ICL&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20840;&#28436;&#31034;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#20102;&#19968;&#20123;&#36755;&#20837;&#36755;&#20986;&#28436;&#31034;&#65288;demos&#65289;&#24182;&#32473;&#20986;&#26356;&#22810;&#28436;&#31034;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65288;&#8220;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#8221;&#65289;&#26102;&#65292;&#33021;&#22815;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27599;&#20010;&#27979;&#35797;&#26597;&#35810;&#19978;&#20351;&#29992;&#36739;&#23569;&#30340;&#28436;&#31034;&#26469;&#36827;&#34892;ICL&#30340;&#20219;&#21153;~\cite{wei2022chain}&#12290;&#24778;&#20154;&#22320;&#65292;&#24403;&#21482;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#26102;&#65292;&#25105;&#20204;&#24182;&#27809;&#26377;&#35266;&#23519;&#21040;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#23545;&#20110;&#27599;&#20010;&#27979;&#35797;&#26597;&#35810;&#65292;&#25105;&#20204;&#23558;&#28436;&#31034;&#20998;&#31867;&#20026;&#8220;&#27491;&#30830;&#28436;&#31034;&#8221;&#21644;&#8220;&#38169;&#35823;&#28436;&#31034;&#8221;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#24191;&#27867;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#20559;&#24046;&#65306;&#22823;&#22810;&#25968;&#27979;&#35797;&#26597;&#35810;&#30340;&#22823;&#22810;&#25968;&#28436;&#31034;&#37117;&#26159;&#27491;&#30830;&#30340;&#65292;&#36825;&#35299;&#37322;&#20102;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#28436;&#31034;&#26102;&#34920;&#29616;&#33391;&#22909;&#30340;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#28436;&#31034;&#30340;ICL&#65288;&#24102;&#21644;&#19981;&#24102;CoT&#65289;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#22823;&#22810;&#25968;&#20808;&#21069;&#24037;&#20316;&#37319;&#29992;&#30340;&#20840;&#28436;&#31034;ICL&#65292;&#34920;&#26126;&#28436;&#31034;&#25968;&#37327;&#24182;&#19981;&#24635;&#26159;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are capable to perform complex reasoning by in-context learning (ICL) when provided with a few input-output demonstrations (demos) and more powerful when intermediate reasoning steps ("chain of thoughts (CoT)") of the demos are given. Is it necessary to use multi-demo in ICL? In this paper, we study ICL using fewer demos for each test query on the tasks in~\cite{wei2022chain}. Surprisingly, we do not observe significant degradation when using only one randomly chosen demo. To study this phenomenon, for each test query, we categorize demos into "correct demos" leading to the correct answer, and "wrong demos" resulting in wrong answers. Our analysis reveals an inherent bias in those widely studied datasets: most demos are correct for a majority of test queries, which explains the good performance of using one random demo. Moreover, ICL (with and w/o CoT) using only one correct demo significantly outperforms all-demo ICL adopted by most previous works, indicat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#21644;&#26041;&#38754;&#29305;&#23450;&#65292;&#25105;&#20204;&#22312;&#20116;&#20010;NLG&#20803;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#34920;&#26126;ChatGPT&#20316;&#20026;NLG&#35780;&#20272;&#25351;&#26631;&#24182;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#19968;&#33268;&#65292;&#23588;&#20854;&#26159;&#22312;&#27969;&#30021;&#24230;&#26041;&#38754;&#12290;&#36825;&#25552;&#37266;&#20154;&#20204;&#22312;&#20351;&#29992;ChatGPT&#20316;&#20026;&#21807;&#19968;&#30340;&#33258;&#21160;NLG&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2303.04048</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#35780;&#20215;&#25351;&#26631;&#21487;&#38752;&#21527;&#65311;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good NLG Evaluator? A Preliminary Study. (arXiv:2303.04048v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04048
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#21644;&#26041;&#38754;&#29305;&#23450;&#65292;&#25105;&#20204;&#22312;&#20116;&#20010;NLG&#20803;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#34920;&#26126;ChatGPT&#20316;&#20026;NLG&#35780;&#20272;&#25351;&#26631;&#24182;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#19968;&#33268;&#65292;&#23588;&#20854;&#26159;&#22312;&#27969;&#30021;&#24230;&#26041;&#38754;&#12290;&#36825;&#25552;&#37266;&#20154;&#20204;&#22312;&#20351;&#29992;ChatGPT&#20316;&#20026;&#21807;&#19968;&#30340;&#33258;&#21160;NLG&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#20197;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#20026;&#22522;&#30784;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;ChatGPT&#20316;&#20026;&#19968;&#31181;&#35780;&#20272;&#25351;&#26631;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#32771;&#34385;&#21040;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#27169;&#22411;&#30340;&#36136;&#37327;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;NLG&#25351;&#26631;&#20197;&#20854;&#31967;&#31957;&#30340;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#32780;&#38395;&#21517;&#65292;&#22240;&#27492;&#25105;&#20204;&#26159;&#21542;&#20250;&#35748;&#20026;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;NLG&#35780;&#20272;&#25351;&#26631;&#12290;&#22312;&#36825;&#31687;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#20803;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;ChatGPT&#20316;&#20026;NLG&#25351;&#26631;&#30340;&#21487;&#38752;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;ChatGPT&#35270;&#20026;&#20154;&#31867;&#35780;&#20272;&#22120;&#65292;&#24182;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#65288;&#20363;&#22914;&#25688;&#35201;&#65289;&#21644;&#26041;&#38754;&#29305;&#23450;&#65288;&#20363;&#22914;&#30456;&#20851;&#24615;&#65289;&#36827;&#34892;&#35828;&#26126;&#65292;&#20197;&#20419;&#20351;ChatGPT&#35780;&#20272;NLG&#27169;&#22411;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#25688;&#35201;&#12289;&#25925;&#20107;&#29983;&#25104;&#21644;&#32763;&#35793;&#22312;&#20869;&#30340;&#20116;&#20010;NLG&#20803;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#26041;&#38754;&#65288;&#20363;&#22914;&#27969;&#30021;&#24230;&#65289;&#65292;ChatGPT&#24182;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#19968;&#33268;&#12290;&#36825;&#25552;&#37266;&#20154;&#20204;&#22312;&#20351;&#29992;ChatGPT&#20316;&#20026;&#21807;&#19968;&#30340;&#33258;&#21160;NLG&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992; COIL &#26041;&#27861;&#30740;&#31350;&#20102;&#26377;&#25928;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#37197;&#36865;&#38382;&#39064;&#65292;&#30830;&#20445;&#22312;&#20445;&#35777;&#23433;&#20840;&#30417;&#25511;&#30340;&#21069;&#25552;&#19979;&#26368;&#22823;&#21270;&#26426;&#22120;&#20154;&#37197;&#36865;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.03211</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#33258;&#20027;&#26426;&#22120;&#20154;&#26368;&#21518;&#19968;&#33521;&#37324;&#21487;&#38752;&#30417;&#25511;&#30340;&#26377;&#25928;&#25628;&#32034;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Using a Variational Autoencoder to Learn Valid Search Spaces of Safely Monitored Autonomous Robots for Last-Mile Delivery. (arXiv:2303.03211v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992; COIL &#26041;&#27861;&#30740;&#31350;&#20102;&#26377;&#25928;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#37197;&#36865;&#38382;&#39064;&#65292;&#30830;&#20445;&#22312;&#20445;&#35777;&#23433;&#20840;&#30417;&#25511;&#30340;&#21069;&#25552;&#19979;&#26368;&#22823;&#21270;&#26426;&#22120;&#20154;&#37197;&#36865;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#20027;&#26426;&#22120;&#20154;&#36865;&#36135;&#32473;&#23458;&#25143;&#26159;&#19968;&#31181;&#21487;&#38752;&#19988;&#21487;&#25345;&#32493;&#30340;&#26381;&#21153;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#23433;&#20840;&#36215;&#35265;&#65292;&#33258;&#20027;&#26426;&#22120;&#20154;&#20173;&#38656;&#35201;&#20154;&#31867;&#30417;&#30563;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#20445;&#35777;&#23433;&#20840;&#30417;&#25511;&#30340;&#21069;&#25552;&#19979;&#65292;&#26368;&#22823;&#21270;&#33258;&#20027;&#26426;&#22120;&#20154;&#37197;&#36865;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#30340;&#26041;&#27861; COIL&#65288;&#22312;&#23398;&#20064;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#32422;&#26463;&#20248;&#21270;&#65289;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20934;&#36951;&#20256;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25913;&#21892; COIL &#30340;&#36895;&#24230;&#21644;&#25928;&#29575;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377; COIL &#33021;&#22815;&#25214;&#21040;&#25152;&#26377;&#38382;&#39064;&#21464;&#21270;&#27979;&#35797;&#20013;&#36866;&#24403;&#25968;&#37327;&#30340;&#26426;&#22120;&#20154;&#21516;&#26102;&#36816;&#34892;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#24403; COIL &#23398;&#20064;&#20854;&#28508;&#22312;&#34920;&#31034;&#26102;&#65292;&#21487;&#20197;&#27604; G&#65288;&#36951;&#20256;&#31639;&#27861;&#65289;&#26356;&#24555;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of autonomous robots for delivery of goods to customers is an exciting new way to provide a reliable and sustainable service. However, in the real world, autonomous robots still require human supervision for safety reasons. We tackle the realworld problem of optimizing autonomous robot timings to maximize deliveries, while ensuring that there are never too many robots running simultaneously so that they can be monitored safely. We assess the use of a recent hybrid machine-learningoptimization approach COIL (constrained optimization in learned latent space) and compare it with a baseline genetic algorithm for the purposes of exploring variations of this problem. We also investigate new methods for improving the speed and efficiency of COIL. We show that only COIL can find valid solutions where appropriate numbers of robots run simultaneously for all problem variations tested. We also show that when COIL has learned its latent representation, it can optimize 10% faster than the G
&lt;/p&gt;</description></item><item><title>Hitachi&#22242;&#38431;&#21442;&#21152;SemEval-2023&#31532;3&#39033;&#20219;&#21153;&#65292;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#31574;&#30053;&#65292;&#34920;&#26126;&#36328;&#35821;&#35328;/&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#25910;&#38598;&#22806;&#37096;&#24179;&#34913;&#25968;&#25454;&#38598;&#21487;&#20197;&#26377;&#30410;&#20110;&#27969;&#27966;&#21644;&#26694;&#26550;&#26816;&#27979;&#65292;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#20420;&#35821;&#27969;&#27966;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.01794</link><description>&lt;p&gt;
Hitachi&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#30340;&#34920;&#29616;&#65306;&#25506;&#32034;&#36328;&#35821;&#35328;&#22810;&#20219;&#21153;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#32447;&#26032;&#38395;&#20013;&#30340;&#27969;&#27966;&#21644;&#26694;&#26550;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hitachi at SemEval-2023 Task 3: Exploring Cross-lingual Multi-task Strategies for Genre and Framing Detection in Online News. (arXiv:2303.01794v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01794
&lt;/p&gt;
&lt;p&gt;
Hitachi&#22242;&#38431;&#21442;&#21152;SemEval-2023&#31532;3&#39033;&#20219;&#21153;&#65292;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#31574;&#30053;&#65292;&#34920;&#26126;&#36328;&#35821;&#35328;/&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#25910;&#38598;&#22806;&#37096;&#24179;&#34913;&#25968;&#25454;&#38598;&#21487;&#20197;&#26377;&#30410;&#20110;&#27969;&#27966;&#21644;&#26694;&#26550;&#26816;&#27979;&#65292;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#20420;&#35821;&#27969;&#27966;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Hitachi&#22242;&#38431;&#21442;&#21152;SemEval-2023&#31532;3&#39033;&#20219;&#21153;&#8220;&#22312;&#22810;&#35821;&#35328;&#35774;&#32622;&#20013;&#26816;&#27979;&#22312;&#32447;&#26032;&#38395;&#20013;&#30340;&#27969;&#27966;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#8221;&#30340;&#24773;&#20917;&#12290;&#37492;&#20110;&#20219;&#21153;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#20219;&#21153;&#24615;&#36136;&#21644;&#20302;&#36164;&#28304;&#29615;&#22659;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#36328;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#31574;&#30053;&#65292;&#20197;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;(a)&#36328;&#35821;&#35328;/&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20197;&#21450;(b)&#25910;&#38598;&#22806;&#37096;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26377;&#30410;&#20110;&#27969;&#27966;&#21644;&#26694;&#26550;&#26816;&#27979;&#12290;&#25105;&#20204;&#20174;&#32467;&#26524;&#26500;&#24314;&#20102;&#38598;&#25104;&#27169;&#22411;&#65292;&#24182;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#20420;&#35821;&#27969;&#27966;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explains the participation of team Hitachi to SemEval-2023 Task 3 "Detecting the genre, the framing, and the persuasion techniques in online news in a multi-lingual setup.'' Based on the multilingual, multi-task nature of the task and the low-resource setting, we investigated different cross-lingual and multi-task strategies for training the pretrained language models. Through extensive experiments, we found that (a) cross-lingual/multi-task training, and (b) collecting an external balanced dataset, can benefit the genre and framing detection. We constructed ensemble models from the results and achieved the highest macro-averaged F1 scores in Italian and Russian genre categorization subtasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#36827;&#21270;&#31639;&#27861;&#22312;&#39640;&#21407;&#19978;&#28040;&#32791;&#30340;&#26102;&#38388;&#65292;&#36890;&#36807;&#31639;&#27861;&#22312;&#38024;&#23574;&#38382;&#39064;&#21644;&#26032;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#26368;&#20339;&#21464;&#24322;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;$(1+1)$&#36827;&#21270;&#31639;&#27861;&#30456;&#23545;&#20110;&#19968;&#33324;&#38543;&#26426;&#25628;&#32034;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#39640;&#21407;&#38382;&#39064;&#19978;&#20855;&#26377;&#24778;&#20154;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.08021</link><description>&lt;p&gt;
Fourier&#20998;&#26512;&#19982;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#30456;&#36935;&#65306;&#20851;&#20110;&#8220;&#39640;&#21407;&#8221;&#38382;&#39064;&#30340;&#31934;&#20934;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fourier Analysis Meets Runtime Analysis: Precise Runtimes on Plateaus. (arXiv:2302.08021v2 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#36827;&#21270;&#31639;&#27861;&#22312;&#39640;&#21407;&#19978;&#28040;&#32791;&#30340;&#26102;&#38388;&#65292;&#36890;&#36807;&#31639;&#27861;&#22312;&#38024;&#23574;&#38382;&#39064;&#21644;&#26032;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#26368;&#20339;&#21464;&#24322;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;$(1+1)$&#36827;&#21270;&#31639;&#27861;&#30456;&#23545;&#20110;&#19968;&#33324;&#38543;&#26426;&#25628;&#32034;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#39640;&#21407;&#38382;&#39064;&#19978;&#20855;&#26377;&#24778;&#20154;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#36827;&#21270;&#31639;&#27861;&#22312;&#39640;&#21407;&#19978;&#28040;&#32791;&#30340;&#26102;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#31435;&#21363;&#35777;&#26126;&#20102;&#30001;Garnier&#65292;Kallel&#65292;&#21644;Schoenauer&#65288;1999&#65289;&#25552;&#20986;&#30340;&#20851;&#20110;&#38024;&#23574;&#38382;&#39064;&#19978;&#39044;&#26399;&#36816;&#34892;&#26102;&#38388;&#30340;&#32463;&#20856;&#20272;&#35745;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#38382;&#39064;&#65306;&#30001;$n/\ell$&#20010;&#26377;&#25928;&#22823;&#23567;&#20026;$2^\ell-1$&#30340;&#39640;&#21407;&#32452;&#25104;&#65292;&#38656;&#35201;&#20197;LeadingOnes&#30340;&#26041;&#24335;&#39034;&#24207;&#22320;&#23545;&#20854;&#36827;&#34892;&#20248;&#21270;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#38745;&#24577;&#21644;&#19982;&#36866;&#24212;&#24230;&#30456;&#20851;&#30340;&#21464;&#24322;&#29575;&#30340;&#31934;&#30830;&#39044;&#26399;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#28176;&#36827;&#26368;&#20248;&#30340;&#38745;&#24577;&#21644;&#19982;&#36866;&#24212;&#24230;&#30456;&#20851;&#30340;&#21464;&#24322;&#29575;&#12290;&#23545;&#20110;$\ell = o(n)$&#65292;&#26368;&#20248;&#30340;&#38745;&#24577;&#21464;&#24322;&#29575;&#36817;&#20284;&#20026;$1.59/n$&#12290;&#24403;&#25214;&#21040;&#21069;$k$&#20010;&#19982;&#36866;&#24212;&#24230;&#30456;&#20851;&#30340;&#20108;&#36827;&#21046;&#20301;&#26102;&#65292;&#26368;&#20248;&#30340;&#36866;&#24212;&#24230;&#30456;&#20851;&#21464;&#24322;&#29575;&#28176;&#36827;&#20026;$1/(k+1)$&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20123;&#32467;&#26524;&#20165;&#35777;&#26126;&#20102;&#39044;&#26399;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#34920;&#26126;&#35813;&#26032;&#22522;&#20934;&#38382;&#39064;&#38750;&#24120;&#22256;&#38590;&#65292;&#32780;$(1+1)$&#36827;&#21270;&#31639;&#27861;&#22312;&#39640;&#21407;&#38382;&#39064;&#19978;&#19982;&#19968;&#33324;&#30340;&#38543;&#26426;&#25628;&#32034;&#21551;&#21457;&#24335;&#31639;&#27861;&#30456;&#27604;&#25928;&#29575;&#24778;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method based on discrete Fourier analysis to analyze the time evolutionary algorithms spend on plateaus. This immediately gives a concise proof of the classic estimate of the expected runtime of the $(1+1)$ evolutionary algorithm on the Needle problem due to Garnier, Kallel, and Schoenauer (1999).  We also use this method to analyze the runtime of the $(1+1)$ evolutionary algorithm on a new benchmark consisting of $n/\ell$ plateaus of effective size $2^\ell-1$ which have to be optimized sequentially in a LeadingOnes fashion.  Using our new method, we determine the precise expected runtime both for static and fitness-dependent mutation rates. We also determine the asymptotically optimal static and fitness-dependent mutation rates. For $\ell = o(n)$, the optimal static mutation rate is approximately $1.59/n$. The optimal fitness dependent mutation rate, when the first $k$ fitness-relevant bits have been found, is asymptotically $1/(k+1)$. These results, so far only prove
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#26234;&#33021;&#25163;&#34920;&#25968;&#25454;&#36827;&#34892;&#35748;&#30693;&#36127;&#33655;&#20272;&#35745;&#65292;&#30740;&#31350;&#21457;&#29616;&#28216;&#25103;&#21270;&#30340;&#33258;&#25105;&#25253;&#21578;&#26041;&#24335;&#19982;&#20256;&#32479;&#26041;&#24335;&#30340;&#35748;&#30693;&#36127;&#33655;&#27809;&#26377;&#24046;&#24322;&#65292;&#20294;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#28216;&#25103;&#21270;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2302.03616</link><description>&lt;p&gt;
&#28216;&#25103;&#21270;&#33021;&#21542;&#20943;&#36731;mHealth&#24212;&#29992;&#20013;&#33258;&#25105;&#25253;&#21578;&#30340;&#36127;&#25285;&#65311;&#21033;&#29992;&#26234;&#33021;&#25163;&#34920;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#35748;&#30693;&#36127;&#33655;&#20272;&#35745;&#30340;&#21487;&#34892;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can gamification reduce the burden of self-reporting in mHealth applications? A feasibility study using machine learning from smartwatch data to estimate cognitive load. (arXiv:2302.03616v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03616
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#26234;&#33021;&#25163;&#34920;&#25968;&#25454;&#36827;&#34892;&#35748;&#30693;&#36127;&#33655;&#20272;&#35745;&#65292;&#30740;&#31350;&#21457;&#29616;&#28216;&#25103;&#21270;&#30340;&#33258;&#25105;&#25253;&#21578;&#26041;&#24335;&#19982;&#20256;&#32479;&#26041;&#24335;&#30340;&#35748;&#30693;&#36127;&#33655;&#27809;&#26377;&#24046;&#24322;&#65292;&#20294;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#28216;&#25103;&#21270;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#27835;&#30103;&#30340;&#26377;&#25928;&#24615;&#21487;&#20197;&#36890;&#36807;&#35201;&#27714;&#24739;&#32773;&#36890;&#36807;&#24212;&#29992;&#31243;&#24207;&#33258;&#25105;&#25253;&#21578;&#20854;&#29366;&#24577;&#26469;&#34913;&#37327;&#65292;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#20196;&#20154;&#19981;&#30693;&#25152;&#25514;&#24182;&#23548;&#33268;&#22833;&#21435;&#21442;&#19982;&#24230;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#25506;&#35752;&#28216;&#25103;&#21270;&#23545;&#33258;&#25105;&#25253;&#21578;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21019;&#24314;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#20998;&#26512;&#20809;-&#34880;&#23481;&#31215;&#21464;&#21270;&#20449;&#21495;&#26469;&#35780;&#20272;&#35748;&#30693;&#36127;&#33655;&#65288;CL&#65289;&#12290;&#21033;&#29992;11&#21517;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;CL&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20004;&#20010;&#29256;&#26412;&#30340;&#35843;&#26597;&#38382;&#21367;&#65306;&#19968;&#20010;&#26159;&#28216;&#25103;&#21270;&#29256;&#26412;&#65292;&#19968;&#20010;&#26159;&#20256;&#32479;&#29256;&#26412;&#12290;&#25105;&#20204;&#20272;&#35745;&#20854;&#20182;&#21442;&#19982;&#32773;&#65288;13&#21517;&#65289;&#22312;&#23436;&#25104;&#35843;&#26597;&#38382;&#21367;&#26102;&#32463;&#21382;&#30340;CL&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#39044;&#20808;&#22312;&#24212;&#28608;&#26816;&#27979;&#20219;&#21153;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#22686;&#24378;CL&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;13&#21517;&#21442;&#19982;&#32773;&#20013;&#30340;10&#21517;&#65292;&#20010;&#24615;&#21270;CL&#26816;&#27979;&#22120;&#21487;&#20197;&#23454;&#29616;&#39640;&#20110;0.7&#30340;F1&#24471;&#20998;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;CL&#26041;&#38754;&#65292;&#28216;&#25103;&#21270;&#21644;&#38750;&#28216;&#25103;&#21270;&#30340;&#35843;&#26597;&#38382;&#21367;&#27809;&#26377;&#21306;&#21035;&#65292;&#20294;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#28216;&#25103;&#21270;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of digital treatments can be measured by requiring patients to self-report their state through applications, however, it can be overwhelming and causes disengagement. We conduct a study to explore the impact of gamification on self-reporting. Our approach involves the creation of a system to assess cognitive load (CL) through the analysis of photoplethysmography (PPG) signals. The data from 11 participants is utilized to train a machine learning model to detect CL. Subsequently, we create two versions of surveys: a gamified and a traditional one. We estimate the CL experienced by other participants (13) while completing surveys. We find that CL detector performance can be enhanced via pre-training on stress detection tasks. For 10 out of 13 participants, a personalized CL detector can achieve an F1 score above 0.7. We find no difference between the gamified and non-gamified surveys in terms of CL but participants prefer the gamified version.
&lt;/p&gt;</description></item><item><title>&#35752;&#35770;&#20102;&#19981;&#38656;&#35201;&#26174;&#24335;&#23454;&#29616;&#24178;&#39044;&#34920;&#31034;&#30340; Pareto &#26368;&#20248;&#25968;&#23398;&#24418;&#24335;&#65292;&#36890;&#36807;&#24402;&#32435;&#24615;&#22320;&#20986;&#29616;&#30456;&#20851;&#22240;&#26524;&#24178;&#39044;&#30340;&#34920;&#31034;&#65292;&#20316;&#20026;&#33258;&#25105;&#21644;&#20219;&#20309;&#20854;&#20182;&#23545;&#35937;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.03189</link><description>&lt;p&gt;
&#26032;&#20852;&#22240;&#26524;&#24615;&#19982;&#24847;&#35782;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Emergent Causality &amp; the Foundation of Consciousness. (arXiv:2302.03189v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03189
&lt;/p&gt;
&lt;p&gt;
&#35752;&#35770;&#20102;&#19981;&#38656;&#35201;&#26174;&#24335;&#23454;&#29616;&#24178;&#39044;&#34920;&#31034;&#30340; Pareto &#26368;&#20248;&#25968;&#23398;&#24418;&#24335;&#65292;&#36890;&#36807;&#24402;&#32435;&#24615;&#22320;&#20986;&#29616;&#30456;&#20851;&#22240;&#26524;&#24178;&#39044;&#30340;&#34920;&#31034;&#65292;&#20316;&#20026;&#33258;&#25105;&#21644;&#20219;&#20309;&#20854;&#20182;&#23545;&#35937;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#20570;&#20986;&#20934;&#30830;&#30340;&#25512;&#26029;&#65292;&#26234;&#33021;&#20307;&#19981;&#33021;&#25226;&#34987;&#21160;&#35266;&#23519;&#20107;&#20214;&#19982;&#24178;&#39044;&#24341;&#36215;&#23427;&#20204;&#28151;&#28102;&#12290;$do$ &#25805;&#20316;&#31526;&#23558;&#24178;&#39044;&#24418;&#24335;&#21270;&#65292;&#20197;&#20415;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#20854;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#23384;&#22312;&#26080;&#38656;&#26174;&#24335;&#23454;&#29616;&#24178;&#39044;&#34920;&#31034;&#30340; Pareto &#26368;&#20248;&#25968;&#23398;&#24418;&#24335;&#65292;&#21487;&#20197;&#20570;&#20986;&#26368;&#22823;&#31243;&#24230;&#30340;&#20934;&#30830;&#25512;&#26029;&#12290;&#26412;&#25991;&#32771;&#23519;&#20102;&#20854;&#20013;&#19968;&#20010;&#36825;&#26679;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#23384;&#22312; $do$ &#25805;&#20316;&#31526;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#21464;&#37327;&#34920;&#31034;&#24178;&#39044;&#12290;&#28982;&#21518;&#25105;&#20204;&#35748;&#20026;&#21464;&#37327;&#26159;&#25277;&#35937;&#30340;&#65292;&#38656;&#35201;&#26174;&#24335;&#39044;&#20808;&#34920;&#31034;&#24178;&#39044;&#21482;&#26159;&#22240;&#20026;&#25105;&#20204;&#39044;&#35774;&#20102;&#36825;&#31867;&#25277;&#35937;&#27010;&#24565;&#12290;&#21069;&#36848;&#30340;&#24418;&#24335;&#36991;&#20813;&#20102;&#36825;&#31181;&#22256;&#25200;&#65292;&#22240;&#27492;&#65292;&#22914;&#26524;&#26377;&#36866;&#24403;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#21017;&#30456;&#20851;&#22240;&#26524;&#24178;&#39044;&#30340;&#34920;&#31034;&#23558;&#36890;&#36807;&#24402;&#32435;&#24615;&#22320;&#20986;&#29616;&#12290;&#36825;&#20123;&#26032;&#20852;&#30340;&#25277;&#35937;&#20316;&#20026;&#33258;&#25105;&#21644;&#20219;&#20309;&#20854;&#20182;&#23545;&#35937;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make accurate inferences in an interactive setting, an agent must not confuse passive observation of events with having intervened to cause them. The $do$ operator formalises interventions so that we may reason about their effect. Yet there exist pareto optimal mathematical formalisms of general intelligence in an interactive setting which, presupposing no explicit representation of intervention, make maximally accurate inferences. We examine one such formalism. We show that in the absence of a $do$ operator, an intervention can be represented by a variable. We then argue that variables are abstractions, and that need to explicitly represent interventions in advance arises only because we presuppose these sorts of abstractions. The aforementioned formalism avoids this and so, initial conditions permitting, representations of relevant causal interventions will emerge through induction. These emergent abstractions function as representations of one`s self and of any other object, inas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26500;&#24314;&#20551;&#35774;&#30340;&#36807;&#31243;&#20013;&#65292;&#36873;&#25321;&#26368;&#30701;&#30340;&#20551;&#35774;&#19981;&#22914;&#36873;&#25321;&#26368;&#24369;&#30340;&#20551;&#35774;&#65292;&#32780;&#24369;&#28857;&#26159;&#27604;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26356;&#22909;&#30340;&#25512;&#24191;&#34920;&#29616;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2301.12987</link><description>&lt;p&gt;
&#20551;&#35774;&#30340;&#26368;&#20339;&#36873;&#25321;&#26159;&#26368;&#24369;&#30340;&#32780;&#19981;&#26159;&#26368;&#30701;&#30340;
&lt;/p&gt;
&lt;p&gt;
The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest. (arXiv:2301.12987v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26500;&#24314;&#20551;&#35774;&#30340;&#36807;&#31243;&#20013;&#65292;&#36873;&#25321;&#26368;&#30701;&#30340;&#20551;&#35774;&#19981;&#22914;&#36873;&#25321;&#26368;&#24369;&#30340;&#20551;&#35774;&#65292;&#32780;&#24369;&#28857;&#26159;&#27604;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26356;&#22909;&#30340;&#25512;&#24191;&#34920;&#29616;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;$A$&#21644;$B$&#26159;&#36825;&#26679;&#30340;&#38598;&#21512;&#65292;&#21363;$A \subset B$&#65292;&#37027;&#20040;&#19968;&#33324;&#21270;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#20174;$A$&#25512;&#26029;&#20986;&#19968;&#20010;&#36275;&#20197;&#26500;&#24314;$B$&#30340;&#20551;&#35774;&#12290;&#21487;&#20197;&#20174;$A$&#25512;&#26029;&#20986;&#20219;&#24847;&#25968;&#37327;&#30340;&#20551;&#35774;&#65292;&#20294;&#21482;&#26377;&#20854;&#20013;&#30340;&#19968;&#20123;&#21487;&#20197;&#25512;&#24191;&#21040;$B$&#12290;&#24590;&#26679;&#30693;&#36947;&#21738;&#20123;&#20551;&#35774;&#21487;&#33021;&#25512;&#24191;&#65311;&#19968;&#31181;&#31574;&#30053;&#26159;&#36873;&#25321;&#26368;&#30701;&#30340;&#65292;&#23558;&#21387;&#32553;&#20449;&#24687;&#30340;&#33021;&#21147;&#19982;&#25512;&#24191;&#30340;&#33021;&#21147;&#65288;&#26234;&#33021;&#30340;&#20195;&#29702;&#65289;&#31561;&#21516;&#36215;&#26469;&#12290;&#25105;&#20204;&#22312;&#20027;&#21160;&#35748;&#30693;&#30340;&#25968;&#23398;&#24418;&#24335;&#20027;&#20041;&#32972;&#26223;&#19979;&#30740;&#31350;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#21387;&#32553;&#26082;&#19981;&#26159;&#26368;&#22823;&#21270;&#34920;&#29616;&#65288;&#29992;&#20551;&#35774;&#25512;&#24191;&#30340;&#27010;&#29575;&#34913;&#37327;&#65289;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20063;&#19981;&#26159;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#19982;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26080;&#20851;&#30340;&#20195;&#29702;&#65292;&#31216;&#20026;&#24369;&#28857;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#20219;&#21153;&#26159;&#22343;&#21248;&#20998;&#24067;&#30340;&#65292;&#21017;&#19981;&#23384;&#22312;&#20219;&#20309;&#20195;&#29702;&#30340;&#36873;&#25321;&#65292;&#21487;&#20197;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#33267;&#23569;&#19982;&#24369;&#28857;&#26368;&#22823;&#21270;&#30340;&#34920;&#29616;&#30456;&#21516;&#65292;&#21516;&#26102;&#22312;&#33267;&#23569;&#19968;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#27604;&#36739;&#26368;&#22823;&#24369;&#28857;&#21644;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;(MDL)&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#24369;&#28857;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;MDL&#12290;&#25105;&#20204;&#35748;&#20026;&#24369;&#28857;&#26159;&#27604;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26356;&#22909;&#30340;&#25512;&#24191;&#34920;&#29616;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
If $A$ and $B$ are sets such that $A \subset B$, generalisation may be understood as the inference from $A$ of a hypothesis sufficient to construct $B$. One might infer any number of hypotheses from $A$, yet only some of those may generalise to $B$. How can one know which are likely to generalise? One strategy is to choose the shortest, equating the ability to compress information with the ability to generalise (a proxy for intelligence). We examine this in the context of a mathematical formalism of enactive cognition. We show that compression is neither necessary nor sufficient to maximise performance (measured in terms of the probability of a hypothesis generalising). We formulate a proxy unrelated to length or simplicity, called weakness. We show that if tasks are uniformly distributed, then there is no choice of proxy that performs at least as well as weakness maximisation in all tasks while performing strictly better in at least one. In experiments comparing maximum weakness and m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#35270;&#35273;Transformer&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;3D&#35821;&#20041;&#20998;&#21106;&#20013;&#65292;&#36890;&#36807;&#20445;&#30041;&#19982;RGB&#22270;&#20687;&#30456;&#21516;&#30340;&#39592;&#24178;&#32467;&#26500;&#65292;&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;ViTs&#22312;&#32467;&#21512;&#25237;&#24433;&#26041;&#27861;&#65292;&#22823;&#25968;&#25454;&#35757;&#32451;&#21644;&#20855;&#26377;&#22122;&#22768;&#40065;&#26834;&#24615;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#21518;&#21487;&#20197;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.10222</link><description>&lt;p&gt;
RangeViT&#65306;&#38754;&#21521;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#30340;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving. (arXiv:2301.10222v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#31350;&#35270;&#35273;Transformer&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;3D&#35821;&#20041;&#20998;&#21106;&#20013;&#65292;&#36890;&#36807;&#20445;&#30041;&#19982;RGB&#22270;&#20687;&#30456;&#21516;&#30340;&#39592;&#24178;&#32467;&#26500;&#65292;&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;ViTs&#22312;&#32467;&#21512;&#25237;&#24433;&#26041;&#27861;&#65292;&#22823;&#25968;&#25454;&#35757;&#32451;&#21644;&#20855;&#26377;&#22122;&#22768;&#40065;&#26834;&#24615;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#21518;&#21487;&#20197;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23460;&#22806;LiDAR&#28857;&#20113;&#30340;&#35821;&#20041;&#20998;&#21106;&#35270;&#20026;&#20108;&#32500;&#38382;&#39064;&#65288;&#20363;&#22914;&#36890;&#36807;&#36317;&#31163;&#25237;&#24433;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#21644;&#27969;&#34892;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#22522;&#20110;&#25237;&#24433;&#30340;&#26041;&#27861;&#36890;&#24120;&#21463;&#30410;&#20110;&#24555;&#36895;&#35745;&#31639;&#65292;&#24182;&#19988;&#19982;&#20351;&#29992;&#20854;&#20182;&#28857;&#20113;&#34920;&#31034;&#30340;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#30446;&#21069;&#65292;&#25237;&#24433;&#26041;&#27861;&#21033;&#29992;2D CNNs&#65292;&#20294;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22312;&#35768;&#22810;&#22522;&#20110;&#22270;&#20687;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;ViTs&#30340;&#26368;&#26032;&#25913;&#36827;&#26469;&#25913;&#36827;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#30340;&#25237;&#24433;&#26041;&#27861;&#12290;&#25105;&#20204;&#22238;&#31572;&#26159;&#32943;&#23450;&#30340;&#65292;&#20294;&#21482;&#26377;&#22312;&#32467;&#21512;&#20102;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#20043;&#21518;&#25165;&#33021;&#23454;&#29616;&#65306;&#65288;a&#65289;ViTs&#38590;&#20197;&#35757;&#32451;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#23398;&#20064;&#24378;&#22823;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#20445;&#30041;&#19982;RGB&#22270;&#20687;&#30456;&#21516;&#30340;&#39592;&#24178;&#32467;&#26500;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23545;&#22823;&#22270;&#20687;&#38598;&#21512;&#30340;&#38271;&#26102;&#38388;&#35757;&#32451;&#30340;&#30693;&#35782;&#65292;&#36825;&#20123;&#38598;&#21512;&#27604;&#30456;&#24212;&#30340;&#28857;&#20113;&#25968;&#25454;&#38598;&#35201;&#23567;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Casting semantic segmentation of outdoor LiDAR point clouds as a 2D problem, e.g., via range projection, is an effective and popular approach. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud representations, achieve state-of-the-art results. Today, projection-based methods leverage 2D CNNs but recent advances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many image-based benchmarks. In this work, we question if projection-based methods for 3D semantic segmentation can benefit from these latest improvements on ViTs. We answer positively but only after combining them with three key ingredients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations. By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22823;&#35268;&#27169;&#35757;&#32451;&#19979;&#25513;&#27169;&#33258;&#32534;&#30721;&#21644;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23567;&#35268;&#27169;&#35757;&#32451;&#20013;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#32467;&#21512;&#20351;&#29992;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#20013;&#27809;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2301.07836</link><description>&lt;p&gt;
&#25513;&#27169;&#33258;&#32534;&#30721;&#22312;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#20013;&#27809;&#26377;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoding Does Not Help Natural Language Supervision at Scale. (arXiv:2301.07836v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22823;&#35268;&#27169;&#35757;&#32451;&#19979;&#25513;&#27169;&#33258;&#32534;&#30721;&#21644;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23567;&#35268;&#27169;&#35757;&#32451;&#20013;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#32467;&#21512;&#20351;&#29992;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#20013;&#27809;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#21644;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#24050;&#25104;&#20026;&#35757;&#32451;&#36890;&#29992;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#20004;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#32467;&#21512;&#20351;&#29992;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#20027;&#35201;&#20351;&#29992;&#20102;&#23567;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&lt;50M&#26679;&#26412;&#65289;&#65292;&#24182;&#19988;&#27809;&#26377;&#26377;&#25928;&#22320;&#21453;&#26144;&#20986;&#24120;&#29992;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&gt;100M&#26679;&#26412;&#65289;&#30340;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#31867;&#20284;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#26356;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;11.3M&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25513;&#27169;&#33258;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#32452;&#21512;&#21487;&#20197;&#27604;&#21482;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#26356;&#22909;&#65292;&#20294;&#22312;&#23545;1.4B&#20010;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#23427;&#19982;&#20165;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30456;&#27604;&#65292;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#20248;&#21183;&#65288;&#22312;&#19968;&#22871;&#24120;&#35265;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#35780;&#20272;&#65289;&#12290;&#26412;&#30740;&#31350;&#20026;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#19968;&#20123;&#38656;&#35201;&#30340;&#28165;&#26224;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self supervision and natural language supervision have emerged as two exciting ways to train general purpose image encoders which excel at a variety of downstream tasks. Recent works such as M3AE and SLIP have suggested that these approaches can be effectively combined, but most notably their results use small pre-training datasets (&lt;50M samples) and don't effectively reflect the large-scale regime (&gt;100M examples) that is commonly used for these approaches. Here we investigate whether a similar approach can be effective when trained with a much larger amount of data. We find that a combination of two state of the art approaches: masked auto-encoders, MAE and contrastive language image pre-training, CLIP provides a benefit over CLIP when trained on a corpus of 11.3M image-text pairs, but little to no benefit (as evaluated on a suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B images. Our work provides some much needed clarity into the effectiveness (or lack
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;VISEM-Tracking&#65292;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#19987;&#23478;&#20998;&#26512;&#30340;&#31934;&#23376;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#20197;&#20379;&#26131;&#20110;&#35775;&#38382;&#21644;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.02842</link><description>&lt;p&gt;
VISEM-Tracking&#65292;&#19968;&#20221;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VISEM-Tracking, a human spermatozoa tracking dataset. (arXiv:2212.02842v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;VISEM-Tracking&#65292;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#19987;&#23478;&#20998;&#26512;&#30340;&#31934;&#23376;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#20197;&#20379;&#26131;&#20110;&#35775;&#38382;&#21644;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#23376;&#36816;&#21160;&#30340;&#25163;&#21160;&#35780;&#20272;&#38656;&#35201;&#26174;&#24494;&#38236;&#35266;&#23519;&#65292;&#30001;&#20110;&#25152;&#35266;&#23519;&#30340;&#31934;&#23376;&#22312;&#35270;&#37326;&#20013;&#30340;&#24555;&#36895;&#31227;&#21160;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#27491;&#30830;&#30340;&#32467;&#26524;&#65292;&#25163;&#21160;&#35780;&#20272;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#22521;&#35757;&#12290;&#22240;&#27492;&#65292;&#22312;&#35786;&#25152;&#20013;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#31934;&#23376;&#20998;&#26512;&#65288;CASA&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#29992;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#26469;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;VISEM-Tracking&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20&#20010;30&#31186;&#30340;&#35270;&#39057;&#35760;&#24405;&#65288;&#21253;&#25324;29,196&#24103;&#65289;&#30340;&#28287;&#24615;&#31934;&#23376;&#21046;&#22791;&#29289;&#65292;&#20855;&#22791;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#35813;&#39046;&#22495;&#30340;&#19987;&#23478;&#20998;&#26512;&#30340;&#19968;&#32452;&#31934;&#23376;&#29305;&#24449;&#12290;&#38500;&#20102;&#24050;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#21098;&#36753;&#65292;&#20197;&#20415;&#36890;&#36807;&#33258;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#36731;&#26494;&#35775;&#38382;&#21644;&#20998;&#26512;&#25968;&#25454;&#12290;&#20316;&#20026;&#26412;&#25991;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#32447;&#31934;&#23376;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A manual assessment of sperm motility requires microscopy observation, which is challenging due to the fast-moving spermatozoa in the field of view. To obtain correct results, manual evaluation requires extensive training. Therefore, computer-assisted sperm analysis (CASA) has become increasingly used in clinics. Despite this, more data is needed to train supervised machine learning approaches in order to improve accuracy and reliability in the assessment of sperm motility and kinematics. In this regard, we provide a dataset called VISEM-Tracking with 20 video recordings of 30 seconds (comprising 29,196 frames) of wet sperm preparations with manually annotated bounding-box coordinates and a set of sperm characteristics analyzed by experts in the domain. In addition to the annotated data, we provide unlabeled video clips for easy-to-use access and analysis of the data via methods such as selfor unsupervised learning. As part of this paper, we present baseline sperm detection performan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#31995;&#24863;&#30693;&#35821;&#35328;&#22270;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#32852;&#21512;&#25512;&#29702;&#35821;&#35328;&#21644;&#22270;&#20851;&#20110;&#23454;&#20307;&#20851;&#31995;&#65292;&#24182;&#22312;&#22810;&#20010;QA&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.00975</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#31995;&#24863;&#30693;&#30340;&#35821;&#35328;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Relation-Aware Language-Graph Transformer for Question Answering. (arXiv:2212.00975v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#31995;&#24863;&#30693;&#35821;&#35328;&#22270;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#32852;&#21512;&#25512;&#29702;&#35821;&#35328;&#21644;&#22270;&#20851;&#20110;&#23454;&#20307;&#20851;&#31995;&#65292;&#24182;&#22312;&#22810;&#20010;QA&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#26159;&#19968;&#39033;&#38656;&#35201;&#25512;&#29702;&#33258;&#28982;&#35821;&#35328;&#29615;&#22659;&#30340;&#20219;&#21153;&#65292;&#35768;&#22810;&#30456;&#20851;&#24037;&#20316;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;(LM)&#65292;&#20197;&#23545;&#30693;&#35782;&#22270;&#35889;(KG)&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38754;&#21521;&#38382;&#31572;&#30340;&#22522;&#20110;GNN&#30340;&#27169;&#22359;&#24182;&#26410;&#21033;&#29992;KG&#30340;&#20016;&#23500;&#20851;&#31995;&#20449;&#24687;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;LM&#21644;KG&#20043;&#38388;&#30340;&#26377;&#38480;&#20449;&#24687;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Question Answering Transformer(QAT)&#65292;&#23427;&#26088;&#22312;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#32852;&#21512;&#25512;&#29702;&#35821;&#35328;&#21644;&#22270;&#20851;&#20110;&#23454;&#20307;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;QAT&#26500;&#24314;&#20102;&#20803;&#36335;&#24452;&#20196;&#29260;&#65292;&#36825;&#20123;&#20196;&#29260;&#23398;&#20064;&#22522;&#20110;&#19981;&#21516;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#20851;&#31995;&#30340;&#20851;&#31995;&#20013;&#24515;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#20851;&#31995;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#36890;&#36807;&#36328;&#27169;&#24577;&#30456;&#20851;&#20301;&#32622;&#20559;&#24046;&#20840;&#38754;&#25972;&#21512;&#20102;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#20197;&#25351;&#23548;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30456;&#20851;&#23454;&#20307;&#30340;&#20449;&#24687;&#20132;&#25442;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;QAT&#22312;&#22810;&#20010;QA&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question Answering (QA) is a task that entails reasoning over natural language contexts, and many relevant works augment language models (LMs) with graph neural networks (GNNs) to encode the Knowledge Graph (KG) information. However, most existing GNN-based modules for QA do not take advantage of rich relational information of KGs and depend on limited information interaction between the LM and the KG. To address these issues, we propose Question Answering Transformer (QAT), which is designed to jointly reason over language and graphs with respect to entity relations in a unified manner. Specifically, QAT constructs Meta-Path tokens, which learn relation-centric embeddings based on diverse structural and semantic relations. Then, our Relation-Aware Self-Attention module comprehensively integrates different modalities via the Cross-Modal Relative Position Bias, which guides information exchange between relevant entites of different modalities. We validate the effectiveness of QAT on com
&lt;/p&gt;</description></item><item><title>xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.00735</link><description>&lt;p&gt;
xTrimoABFold&#65306;&#26080;&#22810;&#24207;&#21015;&#27604;&#23545;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v2 [q-bio.QM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00735
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25239;&#20307;&#24037;&#31243;&#39046;&#22495;&#65292;&#35774;&#35745;&#19968;&#20010;&#26032;&#22411;&#25239;&#20307;&#20197;&#27491;&#30830;&#22320;&#32467;&#21512;&#29305;&#23450;&#25239;&#21407;&#30340;&#34920;&#20301;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20102;&#35299;&#25239;&#20307;&#32467;&#26500;&#21644;&#20854;&#34920;&#20301;&#21487;&#20197;&#20419;&#36827;&#23545;&#20854;&#21151;&#33021;&#30340;&#26426;&#21046;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#20174;&#20854;&#24207;&#21015;&#39044;&#27979;&#25239;&#20307;&#32467;&#26500;&#19968;&#30452;&#26159;&#19968;&#39033;&#39640;&#24230;&#26377;&#20215;&#20540;&#30340;&#20219;&#21153;&#65292;&#32780;AlphaFold2&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#34507;&#30333;&#36136;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23545;&#20110;&#25239;&#20307;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25239;&#20307;&#30340;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDRs&#65289;&#65292;&#20854;&#39044;&#27979;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of antibody engineering, an essential task is to design a novel antibody whose paratopes bind to a specific antigen with correct epitopes. Understanding antibody structure and its paratope can facilitate a mechanistic understanding of its function. Therefore, antibody structure prediction from its sequence alone has always been a highly valuable problem for de novo antibody design. AlphaFold2, a breakthrough in the field of structural biology, provides a solution to predict protein structure based on protein sequences and computationally expensive coevolutionary multiple sequence alignments (MSAs). However, the computational efficiency and undesirable prediction accuracy of antibodies, especially on the complementarity-determining regions (CDRs) of antibodies limit their applications in the industrially high-throughput drug design. To learn an informative representation of antibodies, we employed a deep antibody language model (ALM) on curated sequences from the observed a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#27169;&#25311;&#39030;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#34987;&#31216;&#20026;&#20998;&#31163;&#31209;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#20027;&#35201;&#21462;&#20915;&#20110;&#20998;&#21306;&#30340;&#34892;&#36208;&#25351;&#25968;&#65292;&#21363;&#20174;&#20998;&#30028;&#32447;&#24320;&#22987;&#30340;&#34892;&#36208;&#25968;&#37327;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;WISA&#30340;&#36793;&#31232;&#30095;&#21270;&#31639;&#27861;&#20197;&#25552;&#39640;GNNs&#30340;&#22788;&#29702;&#25928;&#29575;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.16494</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#39030;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Ability of Graph Neural Networks to Model Interactions Between Vertices. (arXiv:2211.16494v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#27169;&#25311;&#39030;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#34987;&#31216;&#20026;&#20998;&#31163;&#31209;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#20027;&#35201;&#21462;&#20915;&#20110;&#20998;&#21306;&#30340;&#34892;&#36208;&#25351;&#25968;&#65292;&#21363;&#20174;&#20998;&#30028;&#32447;&#24320;&#22987;&#30340;&#34892;&#36208;&#25968;&#37327;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;WISA&#30340;&#36793;&#31232;&#30095;&#21270;&#31639;&#27861;&#20197;&#25552;&#39640;GNNs&#30340;&#22788;&#29702;&#25928;&#29575;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#34987;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#30001;&#22270;&#20013;&#39030;&#28857;&#34920;&#31034;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#19968;&#20123;&#29702;&#35770;&#20998;&#26512;GNNs&#34920;&#36798;&#33021;&#21147;&#30340;&#21162;&#21147;&#65292;&#20294;&#23545;&#20854;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#32570;&#20047;&#19968;&#20010;&#27491;&#24335;&#30340;&#25551;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#36890;&#36807;&#19968;&#20010;&#24050;&#30693;&#30340;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#20998;&#31163;&#31209;(separation rank)&#26469;&#35268;&#33539;&#21270;&#30456;&#20114;&#20316;&#29992;&#30340;&#24378;&#24230;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#26576;&#20123;GNNs&#27169;&#25311;&#32473;&#23450;&#39030;&#28857;&#23376;&#38598;&#21450;&#20854;&#34917;&#38598;&#20043;&#38388;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#21363;&#36755;&#20837;&#39030;&#28857;&#32452;&#25104;&#30340;&#32473;&#23450;&#20998;&#21306;&#30340;&#20004;&#20391;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#20027;&#35201;&#21462;&#20915;&#20110;&#20998;&#21306;&#30340;&#34892;&#36208;&#25351;&#25968;(walk index)&#8212;&#8212;&#19968;&#20010;&#30001;&#20998;&#30028;&#32447;&#24320;&#22987;&#30340;&#34892;&#36208;&#25968;&#37327;&#23450;&#20041;&#30340;&#22270;&#24418;&#29305;&#24449;&#12290;&#24120;&#35265;GNN&#26550;&#26500;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#21457;&#29616;&#12290;&#20316;&#20026;&#25105;&#20204;&#29702;&#35770;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Walk Indexed Sparsification Algorithm (WISA)&#30340;&#36793;&#31232;&#30095;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25552;&#39640;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#24418;&#30340;GNNs&#25928;&#29575;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are widely used for modeling complex interactions between entities represented as vertices of a graph. Despite recent efforts to theoretically analyze the expressive power of GNNs, a formal characterization of their ability to model interactions is lacking. The current paper aims to address this gap. Formalizing strength of interactions through an established measure known as separation rank, we quantify the ability of certain GNNs to model interaction between a given subset of vertices and its complement, i.e. between the sides of a given partition of input vertices. Our results reveal that the ability to model interaction is primarily determined by the partition's walk index -- a graph-theoretical characteristic defined by the number of walks originating from the boundary of the partition. Experiments with common GNN architectures corroborate this finding. As a practical application of our theory, we design an edge sparsification algorithm named Walk Inde
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#31227;&#38500;&#21644;&#25233;&#21046;&#19981;&#24403;&#30340;&#22270;&#20687;&#37096;&#20998;&#65292;&#20174;&#32780;&#32531;&#35299;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22240;&#19981;&#24403;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2211.05105</link><description>&lt;p&gt;
&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#65306;&#32531;&#35299;&#25193;&#25955;&#27169;&#22411;&#20013;&#19981;&#24403;&#36864;&#21270;
&lt;/p&gt;
&lt;p&gt;
Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models. (arXiv:2211.05105v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#31227;&#38500;&#21644;&#25233;&#21046;&#19981;&#24403;&#30340;&#22270;&#20687;&#37096;&#20998;&#65292;&#20174;&#32780;&#32531;&#35299;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22240;&#19981;&#24403;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#26524;&#65292;&#24182;&#22240;&#27492;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#30001;&#20110;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#38543;&#26426;&#20174;&#20114;&#32852;&#32593;&#19978;&#25235;&#21462;&#30340;&#25968;&#21313;&#20159;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#23427;&#20204;&#20063;&#38754;&#20020;&#26469;&#33258;&#36864;&#21270;&#21644;&#20559;&#35265;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#19981;&#33391;&#24433;&#21709;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#12290;&#21453;&#36807;&#26469;&#65292;&#23427;&#20204;&#29978;&#33267;&#21487;&#33021;&#24378;&#21270;&#36825;&#20123;&#20559;&#35265;&#12290;&#20026;&#20102;&#24110;&#21161;&#24212;&#23545;&#36825;&#20123;&#19981;&#33391;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23433;&#20840;&#28508;&#21521;&#25193;&#25955;&#65288;SLD&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#34913;&#37327;&#30001;&#20110;&#26410;&#36807;&#28388;&#21644;&#19981;&#24179;&#34913;&#30340;&#35757;&#32451;&#38598;&#32780;&#24341;&#36215;&#30340;&#19981;&#24403;&#36864;&#21270;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#20687;&#29983;&#25104;&#27979;&#35797;&#24179;&#21488;&#8212;&#8212;&#21253;&#21547;&#19987;&#38376;&#30340;&#12289;&#35206;&#30422;&#35064;&#38706;&#21644;&#26292;&#21147;&#31561;&#27010;&#24565;&#30340;&#23454;&#38469;&#22270;&#20687;&#21040;&#25991;&#26412;&#25552;&#31034;&#30340;&#19981;&#24403;&#22270;&#20687;&#25552;&#31034;&#65288;I2P&#65289;&#12290;&#27491;&#22914;&#25105;&#20204;&#35814;&#23613;&#30340;&#23454;&#35777;&#35780;&#20272;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;&#24341;&#20837;&#30340;SLD&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#31227;&#38500;&#21644;&#25233;&#21046;&#20102;&#19981;&#24403;&#30340;&#22270;&#20687;&#37096;&#20998;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#24182;&#19988;&#23545;&#22270;&#20687;&#36136;&#37327;&#27809;&#26377;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#36716;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#23618;&#35821;&#35328;&#30446;&#26631;&#21644;&#23616;&#37096;&#35270;&#28857;&#20113;&#26500;&#24314;&#29289;&#29702;&#26377;&#25928;&#30340;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27493;&#39588;3D&#35268;&#21010;&#20219;&#21153;&#65292;&#21363;&#20351;&#20351;&#29992;&#26410;&#30693;&#23545;&#35937;&#20173;&#33021;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.04604</link><description>&lt;p&gt;
StructDiffusion&#65306;&#20351;&#29992;&#26410;&#30693;&#23545;&#35937;&#36827;&#34892;&#29289;&#29702;&#26377;&#25928;&#32467;&#26500;&#30340;&#35821;&#35328;&#25351;&#23548;&#21019;&#24314;
&lt;/p&gt;
&lt;p&gt;
StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects. (arXiv:2211.04604v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#36716;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#23618;&#35821;&#35328;&#30446;&#26631;&#21644;&#23616;&#37096;&#35270;&#28857;&#20113;&#26500;&#24314;&#29289;&#29702;&#26377;&#25928;&#30340;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27493;&#39588;3D&#35268;&#21010;&#20219;&#21153;&#65292;&#21363;&#20351;&#20351;&#29992;&#26410;&#30693;&#23545;&#35937;&#20173;&#33021;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#29615;&#22659;&#20013;&#36816;&#20316;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#33021;&#22815;&#23558;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#25104;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#37197;&#32622;&#65292;&#21363;&#20351;&#36825;&#20123;&#29289;&#20307;&#20197;&#21069;&#27809;&#35265;&#36807;&#12290;&#26412;&#25991;&#20851;&#27880;&#22914;&#20309;&#22312;&#26080;&#38656;&#36880;&#27493;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#29289;&#29702;&#26377;&#25928;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;StructDiffusion&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25193;&#25955;&#27169;&#22411;&#21644;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#36716;&#25442;&#22120;&#65292;&#26681;&#25454;&#23616;&#37096;&#35270;&#28857;&#20113;&#21644;&#39640;&#32423;&#35821;&#35328;&#30446;&#26631;&#65288;&#22914;&#8220;&#25670;&#26700;&#23376;&#8221;&#65289;&#65292;&#26500;&#24314;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#25191;&#34892;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35821;&#35328;&#26465;&#20214;&#30340;&#22810;&#27493;&#39588;3D&#35268;&#21010;&#20219;&#21153;&#12290;&#19982;&#35757;&#32451;&#22312;&#29305;&#23450;&#32467;&#26500;&#19978;&#30340;&#29616;&#26377;&#22810;&#27169;&#24577;&#36716;&#25442;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;StructDiffusion&#29978;&#33267;&#25552;&#39640;&#20102;&#23558;&#26410;&#30693;&#23545;&#35937;&#32452;&#35013;&#25104;&#29289;&#29702;&#26377;&#25928;&#32467;&#26500;&#30340;&#25104;&#21151;&#29575;&#65292;&#24179;&#22343;&#21487;&#25552;&#39640;16&#65285;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#25311;&#21644;&#23454;&#38469;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#20013;&#20351;&#29992;&#20445;&#30041;&#23545;&#35937;&#30340;&#23454;&#39564;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25193;&#25955;&#27169;&#22411;&#21644;&#30896;&#25758;&#37492;&#21035;&#22120;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#23454;&#29616;&#23545;&#25311;&#21512;&#24615;&#20197;&#21450;&#23545;&#35821;&#35328;&#25351;&#23548;&#29289;&#29702;&#26377;&#25928;&#32467;&#26500;&#26500;&#24314;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots operating in human environments must be able to rearrange objects into semantically-meaningful configurations, even if these objects are previously unseen. In this work, we focus on the problem of building physically-valid structures without step-by-step instructions. We propose StructDiffusion, which combines a diffusion model and an object-centric transformer to construct structures given partial-view point clouds and high-level language goals, such as "set the table". Our method can perform multiple challenging language-conditioned multi-step 3D planning tasks using one model. StructDiffusion even improves the success rate of assembling physically-valid structures out of unseen objects by on average 16% over an existing multi-modal transformer model trained on specific structures. We show experiments on held-out objects in both simulation and on real-world rearrangement tasks. Importantly, we show how integrating both a diffusion model and a collision-discriminator model allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Aug-imodels &#26694;&#26550;&#65292;&#21033;&#29992; LLMs &#30340;&#30693;&#35782;&#22312;&#25311;&#21512;&#36807;&#31243;&#20013;&#26500;&#24314;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20351;&#29992; LLMs&#65292;&#20855;&#22791;&#23436;&#20840;&#30340;&#36879;&#26126;&#24615;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#19981;&#21516;&#26041;&#24335;&#30340;&#23454;&#29616;&#65292;&#24182;&#22312;&#22810;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.11799</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#26469;&#22686;&#24378;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Augmenting Interpretable Models with LLMs during Training. (arXiv:2209.11799v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Aug-imodels &#26694;&#26550;&#65292;&#21033;&#29992; LLMs &#30340;&#30693;&#35782;&#22312;&#25311;&#21512;&#36807;&#31243;&#20013;&#26500;&#24314;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20351;&#29992; LLMs&#65292;&#20855;&#22791;&#23436;&#20840;&#30340;&#36879;&#26126;&#24615;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#19981;&#21516;&#26041;&#24335;&#30340;&#23454;&#29616;&#65292;&#24182;&#22312;&#22810;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36827;&#20837;&#39640;&#39118;&#38505;&#39046;&#22495;&#65288;&#20363;&#22914;&#21307;&#23398;&#65289;&#21644;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#65292;&#23545;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#21487;&#35299;&#37322;&#27169;&#22411;&#65288;Aug-imodels&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#25152;&#23398;&#20064;&#30340;&#30693;&#35782;&#24314;&#31435;&#26497;&#20854;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;Aug-imodels&#22312;&#25311;&#21512;&#36807;&#31243;&#20013;&#20351;&#29992;LLMs&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20351;&#29992;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23436;&#20840;&#30340;&#36879;&#26126;&#24615;&#65292;&#24182;&#19988;&#19982;LLMs&#30456;&#27604;&#65292;&#25512;&#29702;&#36895;&#24230;&#21644;&#20869;&#23384;&#24615;&#33021;&#26377;&#20102;&#22823;&#20110;1000&#20493;&#30340;&#25552;&#39640;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20004;&#31181;Aug-imodels&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20855;&#20307;&#23454;&#20363;&#65306;&#65288;i&#65289;Aug-GAM&#65292;&#23427;&#20351;&#29992;&#26469;&#33258;LLM&#30340;&#35299;&#32806;&#23884;&#20837;&#26469;&#22686;&#24378;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#65307;&#65288;ii&#65289;Aug-Tree&#65292;&#23427;&#36890;&#36807;LLM&#29305;&#24449;&#25193;&#23637;&#26469;&#22686;&#24378;&#20915;&#31574;&#26641;&#12290;&#22312;&#21508;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20248;&#20110;&#20854;&#26410;&#22686;&#24378;&#30340;&#23545;&#29031;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their proliferation into high-stakes domains (e.g. medicine) and compute-limited settings has created a burgeoning need for interpretability and efficiency. We address this need by proposing Augmented Interpretable Models (Aug-imodels), a framework for leveraging the knowledge learned by LLMs to build extremely efficient and interpretable models. Aug-imodels use LLMs during fitting but not during inference, allowing complete transparency and often a speed/memory improvement of greater than 1,000x for inference compared to LLMs. We explore two instantiations of Aug-imodels in natural-language processing: (i) Aug-GAM, which augments a generalized additive model with decoupled embeddings from an LLM and (ii) Aug-Tree, which augments a decision tree with LLM feature expansions. Across a variety of text-classification datasets, both outperform their non-augmented co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#26469;&#26356;&#31995;&#32479;&#22320;&#35780;&#20272;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#20013;&#24213;&#23618;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.07881</link><description>&lt;p&gt;
&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#35859;&#35789;&#30340;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Model Predictive Robustness of Signal Temporal Logic Predicates. (arXiv:2209.07881v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#26469;&#26356;&#31995;&#32479;&#22320;&#35780;&#20272;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#20013;&#24213;&#23618;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#30340;&#40065;&#26834;&#24615;&#19981;&#20165;&#35780;&#20272;&#20102;&#19968;&#20010;&#20449;&#21495;&#26159;&#21542;&#31526;&#21512;&#35268;&#33539;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#34913;&#37327;&#20844;&#24335;&#34987;&#28385;&#36275;&#25110;&#36829;&#21453;&#30340;&#31243;&#24230;&#30340;&#25351;&#26631;&#12290;&#40065;&#26834;&#24615;&#30340;&#35745;&#31639;&#22522;&#20110;&#23545;&#24213;&#23618;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#36890;&#24120;&#20197;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#26041;&#24335;&#23450;&#20041;&#65292;&#21363;&#19981;&#21253;&#25324;&#31995;&#32479;&#21160;&#24577;&#12290;&#32780;&#19988;&#65292;&#31934;&#30830;&#23450;&#20041;&#22797;&#26434;&#35859;&#35789;&#30340;&#40065;&#26834;&#24615;&#36890;&#24120;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#39044;&#27979;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32771;&#34385;&#22522;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#25552;&#20379;&#20102;&#27604;&#20043;&#21069;&#26041;&#27861;&#26356;&#31995;&#32479;&#30340;&#35780;&#20272;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26469;&#23398;&#20064;&#22522;&#20110;&#39044;&#20808;&#35745;&#31639;&#30340;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#20415;&#21487;&#20197;&#22312;&#32447;&#39640;&#25928;&#22320;&#35745;&#31639;&#40065;&#26834;&#24615;&#20540;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22312;&#35760;&#24405;&#30340;&#25968;&#25454;&#19978;&#20351;&#29992;&#22312;&#24418;&#24335;&#21270;&#20132;&#36890;&#35268;&#21017;&#20013;&#20351;&#29992;&#30340;&#35859;&#35789;&#30340;&#33258;&#21160;&#39550;&#39542;&#29992;&#20363;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of signal temporal logic not only assesses whether a signal adheres to a specification but also provides a measure of how much a formula is fulfilled or violated. The calculation of robustness is based on evaluating the robustness of underlying predicates. However, the robustness of predicates is usually defined in a model-free way, i.e., without including the system dynamics. Moreover, it is often nontrivial to define the robustness of complicated predicates precisely. To address these issues, we propose a notion of model predictive robustness, which provides a more systematic way of evaluating robustness compared to previous approaches by considering model-based predictions. In particular, we use Gaussian process regression to learn the robustness based on precomputed predictions so that robustness values can be efficiently computed online. We evaluate our approach for the use case of autonomous driving with predicates used in formalized traffic rules on a recorded dat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#29992;&#19987;&#23478;&#30693;&#35782;&#23450;&#20041;&#25928;&#29992;&#20989;&#25968;&#22256;&#38590;&#19988;&#19982;&#19987;&#23478;&#21453;&#22797;&#20114;&#21160;&#26114;&#36149;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#24456;&#23569;&#32467;&#26524;&#26102;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#20256;&#36882;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.10300</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#21442;&#25968;&#20248;&#21270;&#20013;&#30340;&#26377;&#25928;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#19982;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Efficient Utility Function Learning for Multi-Objective Parameter Optimization with Prior Knowledge. (arXiv:2208.10300v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10300
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#29992;&#19987;&#23478;&#30693;&#35782;&#23450;&#20041;&#25928;&#29992;&#20989;&#25968;&#22256;&#38590;&#19988;&#19982;&#19987;&#23478;&#21453;&#22797;&#20114;&#21160;&#26114;&#36149;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#24456;&#23569;&#32467;&#26524;&#26102;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#20256;&#36882;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#36890;&#24120;&#20551;&#23450;&#24050;&#26377;&#25928;&#29992;&#20989;&#25968;&#12289;&#36890;&#36807;&#20114;&#21160;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#25110;&#23581;&#35797;&#30830;&#23450;&#23436;&#25972;&#30340;Pareto&#21069;&#27839;&#26469;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#20013;&#65292;&#32467;&#26524;&#24448;&#24448;&#22522;&#20110;&#38544;&#21547;&#21644;&#26174;&#24615;&#30340;&#19987;&#23478;&#30693;&#35782;&#65292;&#38590;&#20197;&#23450;&#20041;&#19968;&#20010;&#25928;&#29992;&#20989;&#25968;&#65292;&#32780;&#20114;&#21160;&#23398;&#20064;&#25110;&#21518;&#32493;&#21551;&#21457;&#24335;&#38656;&#35201;&#21453;&#22797;&#24182;&#19988;&#26114;&#36149;&#22320;&#19987;&#23478;&#21442;&#19982;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#20351;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#65292;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#20165;&#20351;&#29992;&#65288;&#25104;&#23545;&#30340;&#65289;&#32467;&#26524;&#20559;&#22909;&#65292;&#32780;&#19988;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#24456;&#23569;&#30340;&#32467;&#26524;&#26102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#21040;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current state-of-the-art in multi-objective optimization assumes either a given utility function, learns a utility function interactively or tries to determine the complete Pareto front, requiring a post elicitation of the preferred result. However, result elicitation in real world problems is often based on implicit and explicit expert knowledge, making it difficult to define a utility function, whereas interactive learning or post elicitation requires repeated and expensive expert involvement. To mitigate this, we learn a utility function offline, using expert knowledge by means of preference learning. In contrast to other works, we do not only use (pairwise) result preferences, but also coarse information about the utility function space. This enables us to improve the utility function estimate, especially when using very few results. Additionally, we model the occurring uncertainties in the utility function learning task and propagate them through the whole optimization chain. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21516;&#19968;&#22522;&#22240;&#20301;&#28857;&#19978;&#19981;&#21516;mRNA&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.03325</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21516;&#19968;&#22522;&#22240;&#20301;&#28857;&#19978;&#30340;&#19981;&#21516;mRNA&#30340;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
Isoform Function Prediction Using a Deep Neural Network. (arXiv:2208.03325v3 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21516;&#19968;&#22522;&#22240;&#20301;&#28857;&#19978;&#19981;&#21516;mRNA&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#21098;&#20999;&#26159;&#20174;&#21516;&#19968;&#22522;&#22240;&#20301;&#28857;&#20135;&#29983;&#22810;&#20010;mRNA&#30340;&#29616;&#35937;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36229;&#36807;95%&#30340;&#20154;&#31867;&#22810;&#22806;&#26174;&#23376;&#22522;&#22240;&#32463;&#21382;&#20102;&#24322;&#21098;&#20999;&#12290;&#34429;&#28982;mRNA&#24207;&#21015;&#30340;&#21464;&#21270;&#24456;&#23567;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#23545;&#32454;&#32990;&#21151;&#33021;&#21644;&#35843;&#33410;&#20135;&#29983;&#31995;&#32479;&#24615;&#24433;&#21709;&#12290;&#25253;&#36947;&#31216;&#65292;&#21516;&#19968;&#22522;&#22240;&#30340;&#19981;&#21516;&#21098;&#25509;&#24418;&#24335;&#20855;&#26377;&#19981;&#21516;&#29978;&#33267;&#23545;&#31435;&#30340;&#21151;&#33021;&#12290;&#34429;&#28982;&#22522;&#22240;&#30340;&#21151;&#33021;&#30740;&#31350;&#33539;&#22260;&#24456;&#24191;&#65292;&#20294;&#23545;&#20110;&#21516;&#19968;&#22522;&#22240;&#20301;&#28857;&#19978;&#19981;&#21516;mRNA&#30340;&#21151;&#33021;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#22522;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35745;&#31639;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#39044;&#27979;&#21151;&#33021;&#65292;&#24182;&#32467;&#21512;&#20102;&#22522;&#22240;&#21151;&#33021;&#21644;&#22522;&#22240;&#34920;&#36798;&#35889;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#12290;&#27492;&#22806;&#65292;&#27010;&#29575;&#27169;&#22411;&#65292;&#22914;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRF&#65289;&#65292;&#20063;&#34987;&#29992;&#20110;&#24314;&#27169;&#24322;&#26500;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Isoforms are mRNAs produced from the same gene site in the phenomenon called Alternative Splicing. Studies have shown that more than 95% of human multi-exon genes have undergone alternative splicing. Although there are few changes in mRNA sequence, They may have a systematic effect on cell function and regulation. It is widely reported that isoforms of a gene have distinct or even contrasting functions. Most studies have shown that alternative splicing plays a significant role in human health and disease. Despite the wide range of gene function studies, there is little information about isoforms' functionalities. Recently, some computational methods based on Multiple Instance Learning have been proposed to predict isoform function using gene function and gene expression profile. However, their performance is not desirable due to the lack of labeled training data. In addition, probabilistic models such as Conditional Random Field (CRF) have been used to model the relation between isofor
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;&#8212;&#8212;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65288;CMCIR&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#65292;&#21457;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#30495;&#27491;&#22240;&#26524;&#32467;&#26500;&#65292;&#23454;&#29616;&#24378;&#20581;&#30340;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#38382;&#31572;&#12290;</title><link>http://arxiv.org/abs/2207.12647</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#22312;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering. (arXiv:2207.12647v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;&#8212;&#8212;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65288;CMCIR&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#65292;&#21457;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#30495;&#27491;&#22240;&#26524;&#32467;&#26500;&#65292;&#23454;&#29616;&#24378;&#20581;&#30340;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#24448;&#24448;&#25429;&#25417;&#36328;&#27169;&#24577;&#30340;&#20266;&#30456;&#20851;&#24615;&#65292;&#32780;&#26410;&#33021;&#21457;&#29616;&#30495;&#27491;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#20197;&#30495;&#23454;&#22320;&#22522;&#20110;&#20027;&#23548;&#35270;&#35273;&#35777;&#25454;&#21644;&#38382;&#39064;&#24847;&#22270;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#36328;&#27169;&#24577;&#20107;&#20214;&#32423;&#29702;&#35299;&#65292;&#38656;&#35201;&#32852;&#21512;&#24314;&#27169;&#20107;&#20214;&#30340;&#26102;&#38388;&#24615;&#12289;&#22240;&#26524;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#26032;&#30340;&#35282;&#24230;&#65292;&#21363;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65292;&#32858;&#28966;&#20110;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#65292;&#24341;&#20837;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#26469;&#21457;&#29616;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#30495;&#27491;&#22240;&#26524;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36328;&#27169;&#24577;&#22240;&#26524;&#20851;&#31995;&#25512;&#29702;&#65288;CMCIR&#65289;&#30340;&#26032;&#22411;&#20107;&#20214;&#32423;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24378;&#20581;&#30340;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#38382;&#31572;&#12290;&#20026;&#20102;&#21457;&#29616;&#36328;&#27169;&#24577;&#22240;&#26524;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#65288;CVLR&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#20849;&#21516;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing visual question answering methods tend to capture the cross-modal spurious correlations and fail to discover the true causal mechanism that facilitates reasoning truthfully based on the dominant visual evidence and the question intention. Additionally, the existing methods usually ignore the cross-modal event-level understanding that requires to jointly model event temporality, causality, and dynamics. In this work, we focus on event-level visual question answering from a new perspective, i.e., cross-modal causal relational reasoning, by introducing causal intervention methods to discover the true causal structures for visual and linguistic modalities. Specifically, we propose a novel event-level visual question answering framework named Cross-Modal Causal RelatIonal Reasoning (CMCIR), to achieve robust causality-aware visual-linguistic question answering. To discover cross-modal causal structures, the Causality-aware Visual-Linguistic Reasoning (CVLR) module is proposed to co
&lt;/p&gt;</description></item><item><title>BiometricBlender&#26159;&#19968;&#20010;&#39640;&#32500;&#24230;&#22810;&#31867;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#24037;&#20855;&#65292;&#21487;&#20197;&#27169;&#25311;&#30495;&#23454;&#29983;&#29289;&#29305;&#24449;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26377;&#21161;&#20110;&#22312;&#29305;&#24449;&#31579;&#36873;&#39046;&#22495;&#36827;&#34892;&#24555;&#36895;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2206.10747</link><description>&lt;p&gt;
BiometricBlender&#65306;&#39640;&#32500;&#24230;&#22810;&#31867;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
BiometricBlender: Ultra-high dimensional, multi-class synthetic data generator to imitate biometric feature space. (arXiv:2206.10747v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10747
&lt;/p&gt;
&lt;p&gt;
BiometricBlender&#26159;&#19968;&#20010;&#39640;&#32500;&#24230;&#22810;&#31867;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#24037;&#20855;&#65292;&#21487;&#20197;&#27169;&#25311;&#30495;&#23454;&#29983;&#29289;&#29305;&#24449;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26377;&#21161;&#20110;&#22312;&#29305;&#24449;&#31579;&#36873;&#39046;&#22495;&#36827;&#34892;&#24555;&#36895;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#33258;&#30001;&#21487;&#24471;&#30340;&#39640;&#32500;&#24230;&#22810;&#31867;&#30495;&#23454;&#25110;&#21512;&#25104;&#25968;&#25454;&#38598;&#24120;&#24120;&#38480;&#21046;&#20102;&#29305;&#24449;&#31579;&#36873;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BiometricBlender&#30340;Python&#21253;&#65292;&#23427;&#26159;&#19968;&#20010;&#36229;&#39640;&#32500;&#24230;&#22810;&#31867;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#24037;&#20855;&#65292;&#29992;&#20110;&#27979;&#35797;&#21508;&#31181;&#29305;&#24449;&#31579;&#36873;&#26041;&#27861;&#12290;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#29305;&#24449;&#28151;&#21512;&#30340;&#24635;&#20307;&#26377;&#29992;&#24615;&#21644;&#20114;&#30456;&#20851;&#31995;&#25968;&#65292;&#22240;&#27492;&#21512;&#25104;&#29305;&#24449;&#31354;&#38388;&#33021;&#22815;&#27169;&#25311;&#30495;&#23454;&#29983;&#29289;&#29305;&#24449;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of freely available (real-life or synthetic) high or ultra-high dimensional, multi-class datasets may hamper the rapidly growing research on feature screening, especially in the field of biometrics, where the usage of such datasets is common. This paper reports a Python package called BiometricBlender, which is an ultra-high dimensional, multi-class synthetic data generator to benchmark a wide range of feature screening methods. During the data generation process, the overall usefulness and the intercorrelations of blended features can be controlled by the user, thus the synthetic feature space is able to imitate the key properties of a real biometric dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DRAGNET++, &#36890;&#36807;&#32771;&#34385;&#23545;&#35805;&#32447;&#31243;&#30340;&#35821;&#20041;&#12289;&#20256;&#25773;&#32467;&#26500;&#21644;&#29992;&#25143;&#20132;&#20114;&#65292;&#20197;&#39044;&#27979;&#25512;&#25991;&#30340;&#22238;&#22797;&#38142;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20167;&#24680;&#31243;&#24230;&#65292;&#24182;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#21487;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25552;&#20379;&#22312;&#24694;&#24847;&#23545;&#35805;&#21319;&#32423;&#20043;&#21069;&#35782;&#21035;&#21644;&#31649;&#29702;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2206.08406</link><description>&lt;p&gt;
&#39044;&#27979;Twitter&#23545;&#35805;&#32447;&#31243;&#20013;&#30340;&#20167;&#24680;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Predicting Hate Intensity of Twitter Conversation Threads. (arXiv:2206.08406v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DRAGNET++, &#36890;&#36807;&#32771;&#34385;&#23545;&#35805;&#32447;&#31243;&#30340;&#35821;&#20041;&#12289;&#20256;&#25773;&#32467;&#26500;&#21644;&#29992;&#25143;&#20132;&#20114;&#65292;&#20197;&#39044;&#27979;&#25512;&#25991;&#30340;&#22238;&#22797;&#38142;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20167;&#24680;&#31243;&#24230;&#65292;&#24182;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#21487;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25552;&#20379;&#22312;&#24694;&#24847;&#23545;&#35805;&#21319;&#32423;&#20043;&#21069;&#35782;&#21035;&#21644;&#31649;&#29702;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#25991;&#26159;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#20013;&#26368;&#31616;&#27905;&#30340;&#20132;&#27969;&#24418;&#24335;&#65292;&#19968;&#26465;&#25512;&#25991;&#26377;&#21487;&#33021;&#26159;&#23545;&#35805;&#20013;&#25171;&#36896;&#25110;&#30772;&#22351;&#35752;&#35770;&#30340;&#28508;&#22312;&#23186;&#20171;&#12290;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#23481;&#26131;&#33719;&#24471;&#65292;&#38459;&#27490;&#20854;&#20256;&#25773;&#23545;&#20110;&#31038;&#20132;&#23186;&#20307;&#20844;&#21496;&#21644;&#29992;&#25143;&#26469;&#35828;&#26159;&#26497;&#20026;&#37325;&#35201;&#30340;&#65292;&#21487;&#20197;&#25512;&#36827;&#33391;&#22909;&#30340;&#20132;&#27969;&#26041;&#24335;&#12290;&#30446;&#21069;&#38500;&#20102;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#22806;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#19987;&#27880;&#20110;&#20998;&#31867;&#21333;&#20010;&#25512;&#25991;&#65292;&#32780;&#24573;&#30053;&#20102;&#25512;&#25991;&#20043;&#38388;&#30340;&#23545;&#35805;&#32447;&#31243;/&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DRAGNET++&#65292;&#26088;&#22312;&#36890;&#36807;&#25512;&#25991;&#30340;&#22238;&#22797;&#38142;&#39044;&#27979;&#23427;&#21487;&#33021;&#24102;&#26469;&#30340;&#20167;&#24680;&#31243;&#24230;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#23545;&#35805;&#32447;&#31243;&#30340;&#35821;&#20041;&#21644;&#20256;&#25773;&#32467;&#26500;&#20197;&#21450;&#32447;&#31243;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;DRAGNET++&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35748;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21487;&#20197;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#39044;&#27979;&#21644;&#31649;&#29702;&#24694;&#24847;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tweets are the most concise form of communication in online social media, wherein a single tweet has the potential to make or break the discourse of the conversation. Online hate speech is more accessible than ever, and stifling its propagation is of utmost importance for social media companies and users for congenial communication. Most of the research barring a recent few has focused on classifying an individual tweet regardless of the tweet thread/context leading up to that point. One of the classical approaches to curb hate speech is to adopt a reactive strategy after the hate speech postage. The ex-post facto strategy results in neglecting subtle posts that do not show the potential to instigate hate speech on their own but may portend in the subsequent discussion ensuing in the post's replies. In this paper, we propose DRAGNET++, which aims to predict the intensity of hatred that a tweet can bring in through its reply chain in the future. It uses the semantic and propagating stru
&lt;/p&gt;</description></item><item><title>BRExIt&#31639;&#27861;&#20351;&#29992;&#23545;&#25163;&#27169;&#22411;&#21152;&#36895;&#28216;&#25103;&#23398;&#20064;&#65292;&#25552;&#39640;&#23398;&#24466;&#30340;&#29305;&#24449;&#22609;&#36896;&#21644;&#35268;&#21010;&#20559;&#21521;&#20110;&#23545;&#25163;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;BRExIt&#27604;ExIt&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2206.00113</link><description>&lt;p&gt;
BRExIt&#65306;&#35770;&#23545;&#25163;&#24314;&#27169;&#22312;&#19987;&#23478;&#36845;&#20195;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
BRExIt: On Opponent Modelling in Expert Iteration. (arXiv:2206.00113v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00113
&lt;/p&gt;
&lt;p&gt;
BRExIt&#31639;&#27861;&#20351;&#29992;&#23545;&#25163;&#27169;&#22411;&#21152;&#36895;&#28216;&#25103;&#23398;&#20064;&#65292;&#25552;&#39640;&#23398;&#24466;&#30340;&#29305;&#24449;&#22609;&#36896;&#21644;&#35268;&#21010;&#20559;&#21521;&#20110;&#23545;&#25163;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;BRExIt&#27604;ExIt&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21338;&#24328;&#35770;&#21644;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#65292;&#23547;&#25214;&#26368;&#20339;&#21453;&#24212;&#31574;&#30053;&#26159;&#19968;&#20010;&#26680;&#24515;&#30446;&#26631;&#12290;&#29616;&#20195;&#22522;&#20110;&#32676;&#20307;&#30340;&#35757;&#32451;&#26041;&#27861;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20316;&#20026;&#26368;&#20339;&#21453;&#24212;&#39044;&#27979;&#22120;&#65292;&#20197;&#25913;&#21892;&#29609;&#23478;&#23545;&#20505;&#36873;&#23545;&#25163;&#65288;&#36890;&#24120;&#26159;&#20808;&#21069;&#23398;&#20064;&#30340;&#31574;&#30053;&#65289;&#30340;&#28216;&#25103;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#20339;&#21453;&#24212;&#19987;&#23478;&#36845;&#20195;&#65288;BRExIt&#65289;&#31639;&#27861;&#65292;&#23427;&#23558;&#23545;&#25163;&#27169;&#22411;&#34701;&#20837;&#21040;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#31639;&#27861;Expert Iteration&#65288;ExIt&#65289;&#20013;&#20197;&#21152;&#36895;&#28216;&#25103;&#23398;&#20064;&#12290;BRExIt&#26088;&#22312;&#65288;1&#65289;&#25913;&#36827;&#23398;&#24466;&#20013;&#30340;&#29305;&#24449;&#22609;&#36896;&#65292;&#36890;&#36807;&#31574;&#30053;&#22836;&#39044;&#27979;&#23545;&#25163;&#31574;&#30053;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65307;&#65288;2&#65289;&#23558;&#23545;&#25163;&#31227;&#21160;&#30340;&#35268;&#21010;&#20559;&#21521;&#20110;&#32473;&#23450;&#25110;&#24050;&#23398;&#20064;&#30340;&#23545;&#25163;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#26356;&#22909;&#22320;&#36924;&#36817;&#26368;&#20339;&#21453;&#24212;&#30340;&#23398;&#24466;&#30446;&#26631;&#12290;&#25105;&#20204;&#23545;BRExIt&#30340;&#31639;&#27861;&#21464;&#20307;&#36827;&#34892;&#20102;&#23454;&#35777;&#28040;&#34701;&#65292;&#38024;&#23545;&#19968;&#32452;&#22266;&#23450;&#27979;&#35797;&#20195;&#29702;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;BRExIt&#27604;ExIt&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a best response policy is a central objective in game theory and multi-agent learning, with modern population-based training approaches employing reinforcement learning algorithms as best-response oracles to improve play against candidate opponents (typically previously learnt policies). We propose Best Response Expert Iteration (BRExIt), which accelerates learning in games by incorporating opponent models into the state-of-the-art learning algorithm Expert Iteration (ExIt). BRExIt aims to (1) improve feature shaping in the apprentice, with a policy head predicting opponent policies as an auxiliary task, and (2) bias opponent moves in planning towards the given or learnt opponent model, to generate apprentice targets that better approximate a best response. In an empirical ablation on BRExIt's algorithmic variants against a set of fixed test agents, we provide statistical evidence that BRExIt learns better performing policies than ExIt.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Noisier2Noise&#26694;&#26550;&#30340;&#33258;&#30417;&#30563;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;SSDU&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20462;&#25913;&#12290;</title><link>http://arxiv.org/abs/2205.10278</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21464;&#23494;&#24230;Noisier2Noise&#30340;&#33258;&#30417;&#30563;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#30340;&#29702;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A theoretical framework for self-supervised MR image reconstruction using sub-sampling via variable density Noisier2Noise. (arXiv:2205.10278v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Noisier2Noise&#26694;&#26550;&#30340;&#33258;&#30417;&#30563;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;SSDU&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#24314;&#27169;&#33021;&#21147;&#37325;&#24314;&#23376;&#37319;&#26679;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#25552;&#20986;&#30340;&#26041;&#27861;&#20551;&#35774;&#23384;&#22312;&#20195;&#34920;&#24615;&#30340;&#23436;&#20840;&#37319;&#26679;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#23436;&#20840;&#30563;&#23548;&#24335;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#65292;&#23436;&#20840;&#37319;&#26679;&#30340;&#35757;&#32451;&#25968;&#25454;&#26159;&#19981;&#21487;&#29992;&#30340;&#65292;&#32780;&#19988;&#21487;&#33021;&#38750;&#24120;&#38590;&#20197;&#33719;&#21462;&#12290;&#22240;&#27492;&#65292;&#21457;&#23637;&#21644;&#29702;&#35299;&#20165;&#20351;&#29992;&#23376;&#37319;&#26679;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#23558;&#26368;&#21021;&#29992;&#20110;&#33258;&#30417;&#30563;&#21435;&#22122;&#20219;&#21153;&#30340;Noisier2Noise&#26694;&#26550;&#25193;&#23637;&#21040;&#21464;&#23494;&#24230;&#23376;&#37319;&#26679;MRI&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;Noisier2Noise&#26694;&#26550;&#26469;&#35299;&#37322;&#36817;&#26399;&#25552;&#20986;&#30340;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#20294;&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064; via &#25968;&#25454;&#27424;&#37319;&#26679;&#65288;SSDU&#65289;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20004;&#31181;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been attention on leveraging the statistical modeling capabilities of neural networks for reconstructing sub-sampled Magnetic Resonance Imaging (MRI) data. Most proposed methods assume the existence of a representative fully-sampled dataset and use fully-supervised training. However, for many applications, fully sampled training data is not available, and may be highly impractical to acquire. The development and understanding of self-supervised methods, which use only sub-sampled data for training, are therefore highly desirable. This work extends the Noisier2Noise framework, which was originally constructed for self-supervised denoising tasks, to variable density sub-sampled MRI data. We use the Noisier2Noise framework to analytically explain the performance of Self-Supervised Learning via Data Undersampling (SSDU), a recently proposed method that performs well in practice but until now lacked theoretical justification. Further, we propose two modifications 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;&#20154;&#32676;&#22330;&#26223;&#20013;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#19981;&#24178;&#25200;&#30340;&#23548;&#33322;&#65292;&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#25512;&#26029;&#20986;&#21160;&#24577;&#20195;&#29702;&#30340;&#24847;&#22270;&#65292;&#36991;&#20813;&#20405;&#20837;&#20182;&#20154;&#39044;&#26399;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2203.01821</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#20132;&#20114;&#22270;&#20013;&#30340;&#24847;&#22270;&#24863;&#30693;&#26426;&#22120;&#20154;&#32676;&#20307;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Intention Aware Robot Crowd Navigation with Attention-Based Interaction Graph. (arXiv:2203.01821v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01821
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;&#20154;&#32676;&#22330;&#26223;&#20013;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#19981;&#24178;&#25200;&#30340;&#23548;&#33322;&#65292;&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#25512;&#26029;&#20986;&#21160;&#24577;&#20195;&#29702;&#30340;&#24847;&#22270;&#65292;&#36991;&#20813;&#20405;&#20837;&#20182;&#20154;&#39044;&#26399;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#22312;&#23494;&#38598;&#19988;&#20114;&#21160;&#30340;&#20154;&#32676;&#20013;&#36827;&#34892;&#23433;&#20840;&#21644;&#24847;&#22270;&#24863;&#30693;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#19982;&#22823;&#22810;&#25968;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24490;&#29615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20195;&#29702;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#20132;&#20114;&#65292;&#20197;&#20415;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30830;&#23450;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#26681;&#25454;&#26410;&#26469;&#22810;&#20010;&#26102;&#38388;&#27493;&#38271;&#30340;&#36712;&#36857;&#39044;&#27979;&#26469;&#25512;&#26029;&#21160;&#24577;&#20195;&#29702;&#30340;&#24847;&#22270;&#65292;&#24182;&#23558;&#36825;&#20123;&#39044;&#27979;&#32467;&#26524;&#32435;&#20837;&#19968;&#20010;&#26080;&#27169;&#22411;RL&#26694;&#26550;&#20013;&#65292;&#20197;&#36991;&#20813;&#26426;&#22120;&#20154;&#20405;&#20837;&#20182;&#20154;&#30340;&#39044;&#26399;&#36335;&#24452;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#26426;&#22120;&#20154;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#32676;&#23548;&#33322;&#22330;&#26223;&#19979;&#33021;&#22815;&#20197;&#39640;&#25928;&#12289;&#23433;&#20840;&#21644;&#19981;&#24178;&#25200;&#30340;&#26041;&#24335;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#22312;&#20223;&#30495;&#20013;&#23398;&#20064;&#30340;&#31574;&#30053;&#36716;&#31227;&#21040;&#20102;&#30495;&#23454;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of safe and intention-aware robot navigation in dense and interactive crowds. Most previous reinforcement learning (RL) based methods fail to consider different types of interactions among all agents or ignore the intentions of people, which results in performance degradation. To learn a safe and efficient robot policy, we propose a novel recurrent graph neural network with attention mechanisms to capture heterogeneous interactions among agents through space and time. To encourage longsighted robot behaviors, we infer the intentions of dynamic agents by predicting their future trajectories for several timesteps. The predictions are incorporated into a model-free RL framework to prevent the robot from intruding into the intended paths of other agents. We demonstrate that our method enables the robot to achieve good navigation performance and non-invasiveness in challenging crowd navigation scenarios. We successfully transfer the policy learned in simulation to a rea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;APEx&#8221;&#30340;&#25216;&#33021;&#21457;&#29616;&#21644;&#36716;&#31227;&#31639;&#27861;&#65292;&#23427;&#20197;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#23398;&#20064;&#30340;&#26041;&#24335;&#21033;&#29992;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#25928;&#29575;&#12289;&#20219;&#21153;&#21487;&#36716;&#31227;&#24615;&#21644;&#27010;&#25324;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#38750;&#20998;&#23618;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.08115</link><description>&lt;p&gt;
&#20808;&#39564;&#12289;&#23618;&#27425;&#21644;&#20449;&#24687;&#19981;&#23545;&#31216;&#22312;&#24378;&#21270;&#23398;&#20064;&#25216;&#33021;&#36716;&#31227;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning. (arXiv:2201.08115v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;APEx&#8221;&#30340;&#25216;&#33021;&#21457;&#29616;&#21644;&#36716;&#31227;&#31639;&#27861;&#65292;&#23427;&#20197;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#23398;&#20064;&#30340;&#26041;&#24335;&#21033;&#29992;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#25928;&#29575;&#12289;&#20219;&#21153;&#21487;&#36716;&#31227;&#24615;&#21644;&#27010;&#25324;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#38750;&#20998;&#23618;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#21457;&#29616;&#34892;&#20026;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#26032;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#26159;&#26234;&#33021;&#20195;&#29702;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#39640;&#25928;&#37319;&#26679;&#30340;&#26631;&#24535;&#12290;&#20026;&#26234;&#33021;&#24378;&#21270;&#23398;&#20064;&#32773;&#35013;&#22791;&#30456;&#21516;&#30340;&#33021;&#21147;&#21487;&#33021;&#23545;&#23427;&#20204;&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#19978;&#30340;&#25104;&#21151;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20998;&#23618;&#21644;KL-&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#21508;&#33258;&#22312;&#36825;&#26041;&#38754;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#28151;&#21512;&#26041;&#27861;&#21487;&#33021;&#32467;&#21512;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#28857;&#12290;&#36825;&#20123;&#39046;&#22495;&#30340;&#20851;&#38190;&#22312;&#20110;&#21033;&#29992;&#26550;&#26500;&#27169;&#22359;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#26469;&#20559;&#25191;&#23398;&#20064;&#30340;&#25216;&#33021;&#12290;&#34429;&#28982;&#19981;&#23545;&#31216;&#24615;&#36873;&#25321;&#23545;&#21487;&#36716;&#31227;&#24615;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#30452;&#35273;&#65292;&#20197;&#19968;&#20010;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#21487;&#33021;&#27425;&#20248;&#30340;&#26041;&#24335;&#36827;&#34892;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#24207;&#21015;&#20219;&#21153;&#20013;&#25216;&#33021;&#30340;&#20851;&#38190;&#34920;&#36798;&#33021;&#21147;-&#21487;&#36716;&#31227;&#24615;&#30340;&#24179;&#34913;&#65292;&#30001;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#25511;&#21046;&#12290;&#22312;&#33719;&#24471;&#36825;&#19968;&#27934;&#35265;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;APEx&#8221;&#30340;&#25216;&#33021;&#21457;&#29616;&#21644;&#36716;&#31227;&#31639;&#27861;&#65292;&#23427;&#20197;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#23398;&#20064;&#30340;&#26041;&#24335;&#21033;&#29992;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#12290;APEx&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#25928;&#29575;&#12289;&#20219;&#21153;&#21487;&#36716;&#31227;&#24615;&#21644;&#27010;&#25324;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#38750;&#20998;&#23618;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to discover behaviours from past experience and transfer them to new tasks is a hallmark of intelligent agents acting sample-efficiently in the real world. Equipping embodied reinforcement learners with the same ability may be crucial for their successful deployment in robotics. While hierarchical and KL-regularized reinforcement learning individually hold promise here, arguably a hybrid approach could combine their respective benefits. Key to these fields is the use of information asymmetry across architectural modules to bias which skills are learnt. While asymmetry choice has a large influence on transferability, existing methods base their choice primarily on intuition in a domain-independent, potentially sub-optimal, manner. In this paper, we theoretically and empirically show the crucial expressivity-transferability trade-off of skills across sequential tasks, controlled by information asymmetry. Given this insight, we introduce Attentive Priors for Expressive and Tra
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#27599;&#20010;&#31639;&#27861;&#37117;&#23384;&#22312;&#19968;&#20010;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#36229;&#36807;&#35813;&#22823;&#23567;&#65292;&#26080;&#27861;&#39044;&#27979;&#27450;&#39575;&#32773;&#30340;&#31639;&#27861;&#27010;&#29575;&#19978;&#30028;&#26159;&#35813;&#31639;&#27861;&#30340;&#31639;&#27861;&#27010;&#29575;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2112.12275</link><description>&lt;p&gt;
&#24418;&#24335;&#29702;&#35770;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#31616;&#21333;&#24615;&#27873;&#27819;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Simplicity Bubble Problem in Formal-Theoretic Learning Systems. (arXiv:2112.12275v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12275
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#27599;&#20010;&#31639;&#27861;&#37117;&#23384;&#22312;&#19968;&#20010;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#36229;&#36807;&#35813;&#22823;&#23567;&#65292;&#26080;&#27861;&#39044;&#27979;&#27450;&#39575;&#32773;&#30340;&#31639;&#27861;&#27010;&#29575;&#19978;&#30028;&#26159;&#35813;&#31639;&#27861;&#30340;&#31639;&#27861;&#27010;&#29575;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25366;&#25496;&#22823;&#22411;&#25968;&#25454;&#38598;&#20197;&#39044;&#27979;&#26032;&#25968;&#25454;&#26102;&#65292;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#32972;&#21518;&#30340;&#21407;&#21017;&#38480;&#21046;&#19981;&#20165;&#23545;&#22823;&#25968;&#25454;&#27946;&#27969;&#26500;&#25104;&#20005;&#37325;&#25361;&#25112;&#65292;&#36824;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20559;&#21521;&#20302;&#31639;&#27861;&#22797;&#26434;&#24230;&#30340;&#20256;&#32479;&#20551;&#35774;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#21363;&#20351;&#20551;&#35774;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#23384;&#22312;&#24213;&#23618;&#30340;&#31639;&#27861;&#20449;&#24687;&#20559;&#22909;&#65292;&#25105;&#20204;&#20063;&#35777;&#26126;&#20102;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#25110;&#20219;&#20309;&#33258;&#19978;&#32780;&#19979;&#20154;&#24037;&#26234;&#33021;&#21644;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#28151;&#21512;&#65289;&#24635;&#26159;&#21487;&#20197;&#34987;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#33258;&#28982;&#22320;&#25110;&#20154;&#20026;&#22320;&#27450;&#39575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#27599;&#20010;&#23398;&#20064;&#31639;&#27861;&#65288;&#26080;&#35770;&#26159;&#21542;&#26377;&#24418;&#24335;&#21270;&#29702;&#35770;&#30340;&#25903;&#25345;&#65289;&#65292;&#37117;&#23384;&#22312;&#19968;&#20010;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#36229;&#36807;&#35813;&#22823;&#23567;&#65292;&#26080;&#27861;&#39044;&#27979;&#27450;&#39575;&#32773;&#30340;&#31639;&#27861;&#27010;&#29575;&#19978;&#30028;&#26159;&#35813;&#31639;&#27861;&#30340;&#31639;&#27861;&#27010;&#29575;&#19978;&#30028;&#65288;&#20056;&#20197;&#20165;&#20381;&#36182;&#20110;&#23398;&#20064;&#31639;&#27861;&#30340;&#19968;&#20010;&#20056;&#27861;&#24120;&#25968;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
When mining large datasets in order to predict new data, limitations of the principles behind statistical machine learning pose a serious challenge not only to the Big Data deluge, but also to the traditional assumptions that data generating processes are biased toward low algorithmic complexity. Even when one assumes an underlying algorithmic-informational bias toward simplicity in finite dataset generators, we show that current approaches to machine learning (including deep learning, or any formal-theoretic hybrid mix of top-down AI and statistical machine learning approaches), can always be deceived, naturally or artificially, by sufficiently large datasets. In particular, we demonstrate that, for every learning algorithm (with or without access to a formal theory), there is a sufficiently large dataset size above which the algorithmic probability of an unpredictable deceiver is an upper bound (up to a multiplicative constant that only depends on the learning algorithm) for the algo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23884;&#20837;&#20132;&#20114;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;&#21644;&#27604;&#36739;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;CP&#65292;DistMult&#21644;ComplEx&#31561;&#19977;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;ComplEx&#24615;&#33021;&#26356;&#22909;&#30340;&#21407;&#22240;&#65292;&#24182;&#20026;&#24320;&#21457;&#26032;&#27169;&#22411;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/1903.11406</link><description>&lt;p&gt;
&#20174;&#22810;&#23884;&#20837;&#20132;&#20114;&#30340;&#35282;&#24230;&#20998;&#26512;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective. (arXiv:1903.11406v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1903.11406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23884;&#20837;&#20132;&#20114;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;&#21644;&#27604;&#36739;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;CP&#65292;DistMult&#21644;ComplEx&#31561;&#19977;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;ComplEx&#24615;&#33021;&#26356;&#22909;&#30340;&#21407;&#22240;&#65292;&#24182;&#20026;&#24320;&#21457;&#26032;&#27169;&#22411;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#34920;&#31034;&#30693;&#35782;&#30340;&#26684;&#24335;&#65292;&#19988;&#22312;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;&#12289;&#38382;&#31572;&#31995;&#32479;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#26041;&#27861;&#65292;&#22914;CP&#12289;DistMult&#21644;ComplEx&#31561;&#65292;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#39044;&#27979;&#23427;&#20204;&#20043;&#38388;&#30340;&#38142;&#25509;&#12290;&#23884;&#20837;&#21521;&#37327;&#26412;&#36523;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#20854;&#20182;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#25968;&#25454;&#20998;&#26512;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#26426;&#21046;&#21644;&#23884;&#20837;&#21521;&#37327;&#26412;&#36523;&#21464;&#21270;&#24456;&#22823;&#65292;&#20351;&#20854;&#38590;&#20197;&#29702;&#35299;&#21644;&#27604;&#36739;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#20197;&#22810;&#23884;&#20837;&#20132;&#20114;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;&#21644;&#27604;&#36739;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65288;CP&#12289;DistMult&#21644;ComplEx&#65289;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#30340;&#19981;&#21516;&#23884;&#20837;&#21521;&#37327;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;ComplEx&#24615;&#33021;&#26356;&#22909;&#30340;&#21407;&#22240;&#65292;&#24182;&#20026;&#24320;&#21457;&#26032;&#27169;&#22411;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph is a popular format for representing knowledge, with many applications to semantic search engines, question-answering systems, and recommender systems. Real-world knowledge graphs are usually incomplete, so knowledge graph embedding methods, such as Canonical decomposition/Parallel factorization (CP), DistMult, and ComplEx, have been proposed to address this issue. These methods represent entities and relations as embedding vectors in semantic space and predict the links between them. The embedding vectors themselves contain rich semantic information and can be used in other applications such as data analysis. However, mechanisms in these models and the embedding vectors themselves vary greatly, making it difficult to understand and compare them. Given this lack of understanding, we risk using them ineffectively or incorrectly, particularly for complicated models, such as CP, with two role-based embedding vectors, or the state-of-the-art ComplEx model, with complex-valu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38598;&#25104;&#37319;&#26679;&#65292;&#20197;&#36817;&#20284;Thompson&#37319;&#26679;&#24182;&#22312;&#22797;&#26434;&#27169;&#22411;&#19979;&#20445;&#25345;&#21487;&#34892;&#24615;&#65292;&#23558;&#22823;&#22823;&#25193;&#23637;Thompson&#37319;&#26679;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/1705.07347</link><description>&lt;p&gt;
&#38598;&#25104;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Ensemble Sampling. (arXiv:1705.07347v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1705.07347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38598;&#25104;&#37319;&#26679;&#65292;&#20197;&#36817;&#20284;Thompson&#37319;&#26679;&#24182;&#22312;&#22797;&#26434;&#27169;&#22411;&#19979;&#20445;&#25345;&#21487;&#34892;&#24615;&#65292;&#23558;&#22823;&#22823;&#25193;&#23637;Thompson&#37319;&#26679;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Thompson&#37319;&#26679;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#35299;&#20915;&#22810;&#31181;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#30340;&#26377;&#25928;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#22312;&#20854;&#22522;&#26412;&#24418;&#24335;&#20013;&#65292;&#35813;&#31639;&#27861;&#38656;&#35201;&#35745;&#31639;&#24182;&#20174;&#27169;&#22411;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#20165;&#22312;&#31616;&#21333;&#29305;&#27530;&#24773;&#20917;&#19979;&#25165;&#21487;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#38598;&#25104;&#37319;&#26679;&#65292;&#26088;&#22312;&#36817;&#20284;Thompson&#37319;&#26679;&#65292;&#21516;&#26102;&#22312;&#22797;&#26434;&#27169;&#22411;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#38754;&#21069;&#20445;&#25345;&#21487;&#34892;&#24615;&#12290;&#38598;&#25104;&#37319;&#26679;&#22823;&#22823;&#25193;&#23637;&#20102;&#36866;&#29992;Thompson&#37319;&#26679;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#25903;&#25345;&#35813;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#35745;&#31639;&#32467;&#26524;&#26469;&#25552;&#20379;&#26356;&#22810;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thompson sampling has emerged as an effective heuristic for a broad range of online decision problems. In its basic form, the algorithm requires computing and sampling from a posterior distribution over models, which is tractable only for simple special cases. This paper develops ensemble sampling, which aims to approximate Thompson sampling while maintaining tractability even in the face of complex models such as neural networks. Ensemble sampling dramatically expands on the range of applications for which Thompson sampling is viable. We establish a theoretical basis that supports the approach and present computational results that offer further insight.
&lt;/p&gt;</description></item></channel></rss>