<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.01335</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#21487;&#20197;&#23558;&#20854;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#32454;&#35843;&#65288;SFT&#65289;&#21033;&#29992;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#21147;&#37327;&#23545;&#20110;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#38656;&#35201;&#33719;&#21462;&#39069;&#22806;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#25104;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26032;&#30340;&#32454;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#32454;&#35843;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;SPIN&#30340;&#26680;&#24515;&#26159;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#20854;&#20013;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#23454;&#20363;&#23545;&#24328;&#26469;&#25552;&#21319;&#33258;&#24049;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#33258;&#24049;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20248;&#21270;&#33258;&#36523;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#33258;&#25105;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#26469;&#33258;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#22238;&#24212;&#26469;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20026;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20805;&#20998;&#21457;&#25496;&#20154;&#31867;&#26631;&#27880;&#31034;&#33539;&#25968;&#25454;&#22312;SFT&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#26159;&#21487;&#20197;&#36798;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achiev
&lt;/p&gt;</description></item><item><title>TREC iKAT 2023&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65292;&#26088;&#22312;&#24320;&#21457;&#36866;&#24212;&#29992;&#25143;&#20132;&#20114;&#21644;&#19978;&#19979;&#25991;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#35813;&#20219;&#21153;&#36824;&#24378;&#35843;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#29992;&#25143;&#36890;&#36807;&#31579;&#36873;&#25968;&#25454;&#21644;&#20449;&#24687;&#26469;&#36827;&#34892;&#20915;&#31574;&#21644;&#25191;&#34892;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.01330</link><description>&lt;p&gt;
TREC iKAT 2023: &#20132;&#20114;&#24335;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview. (arXiv:2401.01330v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01330
&lt;/p&gt;
&lt;p&gt;
TREC iKAT 2023&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65292;&#26088;&#22312;&#24320;&#21457;&#36866;&#24212;&#29992;&#25143;&#20132;&#20114;&#21644;&#19978;&#19979;&#25991;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#35813;&#20219;&#21153;&#36824;&#24378;&#35843;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#29992;&#25143;&#36890;&#36807;&#31579;&#36873;&#25968;&#25454;&#21644;&#20449;&#24687;&#26469;&#36827;&#34892;&#20915;&#31574;&#21644;&#25191;&#34892;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20449;&#24687;&#26597;&#35810;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#20063;&#26377;&#24456;&#22823;&#30340;&#36129;&#29486;&#12290;TREC&#20132;&#20114;&#24335;&#30693;&#35782;&#36741;&#21161;&#20219;&#21153;&#65288;iKAT&#65289;&#24314;&#31435;&#22312;TREC&#20250;&#35805;&#36741;&#21161;&#20219;&#21153;&#65288;CAsT&#65289;&#30340;&#22522;&#30784;&#19978;&#12290;&#28982;&#32780;&#65292;iKAT&#30528;&#37325;&#20110;&#21019;&#24314;&#21644;&#30740;&#31350;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#20043;&#21069;&#30340;&#20132;&#20114;&#21644;&#24403;&#21069;&#24773;&#22659;&#33258;&#36866;&#24212;&#21709;&#24212;&#30340;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#12290;&#25361;&#25112;&#22312;&#20110;&#20351;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#33021;&#22815;&#23558;&#20010;&#24615;&#21270;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#34701;&#20837;&#21040;&#30456;&#24212;&#20013;&#65292;&#20197;&#39640;&#25928;&#22320;&#24341;&#23548;&#29992;&#25143;&#33719;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;iKAT&#36824;&#30528;&#37325;&#20110;&#20915;&#31574;&#25628;&#32034;&#20219;&#21153;&#65292;&#21363;&#29992;&#25143;&#36890;&#36807;&#25968;&#25454;&#21644;&#20449;&#24687;&#31579;&#36873;&#26469;&#34913;&#37327;&#21508;&#31181;&#36873;&#25321;&#65292;&#20197;&#36798;&#21040;&#32467;&#35770;&#25110;&#25191;&#34892;&#21160;&#20316;&#12290;&#36825;&#20123;&#20219;&#21153;&#22312;&#26085;&#24120;&#20449;&#24687;&#25628;&#32034;&#20915;&#31574;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#26080;&#35770;&#26159;&#26053;&#28216;&#12289;&#20581;&#24247;&#36824;&#26159;&#36141;&#29289;&#31561;&#65292;&#36890;&#24120;&#28041;&#21450;&#19968;&#32452;&#39640;&#32423;&#20449;&#24687;&#25805;&#20316;&#31526;&#65292;&#20854;&#20013;&#26597;&#35810;&#25110;&#38382;&#39064;&#21487;&#33021;&#20250;
&lt;/p&gt;
&lt;p&gt;
Conversational Information Seeking stands as a pivotal research area with significant contributions from previous works. The TREC Interactive Knowledge Assistance Track (iKAT) builds on the foundational work of the TREC Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes the creation and research of conversational search agents that adapt responses based on user's prior interactions and present context. The challenge lies in enabling Conversational Search Agents (CSA) to incorporate this personalized context to efficiency and effectively guide users through the relevant information to them. iKAT also emphasizes decisional search tasks, where users sift through data and information to weigh up options in order to reach a conclusion or perform an action. These tasks, prevalent in everyday information-seeking decisions -- be it related to travel, health, or shopping -- often revolve around a subset of high-level information operators where queries or questions a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#22270;&#29983;&#25104;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#23545;&#40784;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01326</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#22270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction. (arXiv:2401.01326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01326
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#22270;&#29983;&#25104;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#23545;&#40784;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#30340;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#19982;&#20256;&#32479;&#30340;&#29983;&#25104;&#24335;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#36328;&#24230;&#30340;&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#32447;&#24615;&#21270;&#30340;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#25991;&#26412;&#36328;&#24230;&#65292;&#36793;&#34920;&#31034;&#20851;&#31995;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#25351;&#21521;&#26426;&#21046;&#30340;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#21160;&#24577;&#35789;&#27719;&#34920;&#26469;&#34920;&#31034;&#36328;&#24230;&#21644;&#20851;&#31995;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36328;&#24230;&#34920;&#31034;&#25429;&#25417;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#32467;&#26500;&#29305;&#24449;&#21644;&#36793;&#30028;&#65292;&#21516;&#26102;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/urchade/ATG&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method for joint entity and relation extraction from unstructured text by framing it as a conditional sequence generation problem. In contrast to conventional generative information extraction models that are left-to-right token-level generators, our approach is \textit{span-based}. It generates a linearized graph where nodes represent text spans and edges represent relation triplets. Our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types. Our model can capture the structural characteristics and boundaries of entities and relations through span representations while simultaneously grounding the generated output in the original text thanks to the pointing mechanism. Evaluation on benchmark datasets validates the effectiveness of our approach, demonstrating competitive results. Code is available at https://github.com/urchade/ATG.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01325</link><description>&lt;p&gt;
&#33258;&#25193;&#23637;LLM:&#26080;&#38656;&#35843;&#25972;&#30340;LLM&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. (arXiv:2401.01325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26102;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#31934;&#35843;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35757;&#32451;&#24207;&#21015;&#30340;&#26377;&#38480;&#38271;&#24230;&#21487;&#33021;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;LLMs&#26412;&#36523;&#20855;&#26377;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20854;&#22266;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Self-Extend&#26041;&#27861;&#26469;&#28608;&#21457;LLMs&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#28508;&#21147;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#26500;&#24314;&#21452;&#23618;&#27880;&#24847;&#20449;&#24687;&#65306;&#32676;&#32452;&#32423;&#21644;&#37051;&#23621;&#32423;&#12290;&#36825;&#20004;&#20010;&#32423;&#21035;&#36890;&#36807;&#21407;&#22987;&#27169;&#22411;&#30340;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#36825;&#24847;&#21619;&#30528;&#25152;&#25552;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#12290;&#21482;&#38656;&#20462;&#25913;&#22235;&#34892;&#20195;&#30721;&#65292;&#25152;&#25552;&#26041;&#27861;&#23601;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#31934;&#35843;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;+&#25688;&#35201;&#20943;&#25481;&#25991;&#31456;&#26368;&#21518;&#19968;&#21477;&#35441;
&lt;/p&gt;
&lt;p&gt;
This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39564;&#35777;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;GNSS&#27450;&#39575;&#25915;&#20987;&#26816;&#27979;&#26694;&#26550;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20004;&#31181;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;&#27604;&#36739;&#39044;&#27979;&#20301;&#32622;&#20559;&#31227;&#21644;&#24815;&#24615;&#20256;&#24863;&#22120;&#20301;&#32622;&#20559;&#31227;&#20197;&#21450;&#21033;&#29992;&#36716;&#21521;&#35282;&#20256;&#24863;&#22120;&#26469;&#26816;&#27979;&#21644;&#20998;&#31867;&#36716;&#24367;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01304</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#22522;&#20110;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;GNSS&#27450;&#39575;&#25915;&#20987;&#26816;&#27979;&#26694;&#26550;&#30340;&#23454;&#39564;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Experimental Validation of Sensor Fusion-based GNSS Spoofing Attack Detection Framework for Autonomous Vehicles. (arXiv:2401.01304v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39564;&#35777;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;GNSS&#27450;&#39575;&#25915;&#20987;&#26816;&#27979;&#26694;&#26550;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20004;&#31181;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;&#27604;&#36739;&#39044;&#27979;&#20301;&#32622;&#20559;&#31227;&#21644;&#24815;&#24615;&#20256;&#24863;&#22120;&#20301;&#32622;&#20559;&#31227;&#20197;&#21450;&#21033;&#29992;&#36716;&#21521;&#35282;&#20256;&#24863;&#22120;&#26469;&#26816;&#27979;&#21644;&#20998;&#31867;&#36716;&#24367;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39564;&#35777;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;&#20840;&#29699;&#23548;&#33322;&#21355;&#26143;&#31995;&#32479;&#65288;GNSS&#65289;&#27450;&#39575;&#25915;&#20987;&#26816;&#27979;&#26694;&#26550;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#37197;&#22791;GNSS&#25509;&#25910;&#22120;&#21644;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#30340;&#36710;&#36742;&#25910;&#38598;&#25968;&#25454;&#12290;&#35813;&#26816;&#27979;&#26694;&#26550;&#37319;&#29992;&#20004;&#31181;&#31574;&#30053;&#65306;&#31532;&#19968;&#31181;&#31574;&#30053;&#36890;&#36807;&#27604;&#36739;&#39044;&#27979;&#30340;&#20301;&#32622;&#20559;&#31227;&#65288;&#21363;&#20004;&#20010;&#36830;&#32493;&#26102;&#38388;&#25139;&#20043;&#38388;&#30340;&#34892;&#39542;&#36317;&#31163;&#65289;&#19982;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#20301;&#32622;&#20559;&#31227;&#26469;&#36827;&#34892;&#12290;&#20026;&#27492;&#65292;&#23558;&#26469;&#33258;&#20302;&#25104;&#26412;&#36710;&#36742;&#20869;&#30340;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#34701;&#21512;&#24182;&#36755;&#20837;&#21040;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#31532;&#20108;&#31181;&#31574;&#30053;&#37319;&#29992;&#38543;&#26426;&#26862;&#26519;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#36716;&#21521;&#35282;&#20256;&#24863;&#22120;&#30340;&#36755;&#20986;&#26469;&#26816;&#27979;&#21644;&#20998;&#31867;&#36716;&#24367;&#65292;&#24182;&#21306;&#20998;&#24038;&#36716;&#21644;&#21491;&#36716;&#12290;&#22312;&#23454;&#39564;&#20013;&#36824;&#27169;&#25311;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#27450;&#39575;&#25915;&#20987;&#27169;&#22411;&#65306;&#19968;&#27493;&#27493;&#36716;&#24367;&#21644;&#38169;&#35823;&#36716;&#24367;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we validate the performance of the a sensor fusion-based Global Navigation Satellite System (GNSS) spoofing attack detection framework for Autonomous Vehicles (AVs). To collect data, a vehicle equipped with a GNSS receiver, along with Inertial Measurement Unit (IMU) is used. The detection framework incorporates two strategies: The first strategy involves comparing the predicted location shift, which is the distance traveled between two consecutive timestamps, with the inertial sensor-based location shift. For this purpose, data from low-cost in-vehicle inertial sensors such as the accelerometer and gyroscope sensor are fused and fed into a long short-term memory (LSTM) neural network. The second strategy employs a Random-Forest supervised machine learning model to detect and classify turns, distinguishing between left and right turns using the output from the steering angle sensor. In experiments, two types of spoofing attack models: turn-by-turn and wrong turn are simul
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#27861;&#24459;&#24187;&#35273;&#65292;&#19981;&#19968;&#33268;&#27861;&#24459;&#20107;&#23454;&#65292;&#24187;&#35273;&#26222;&#36941;&#23384;&#22312;&#39640;&#36798;69%&#33267;88%&#30340;&#24773;&#20917;&#65292;&#26080;&#27861;&#32416;&#27491;&#29992;&#25143;&#38169;&#35823;&#27861;&#24459;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2401.01301</link><description>&lt;p&gt;
&#22823;&#22411;&#27861;&#24459;&#34394;&#26500;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27861;&#24459;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models. (arXiv:2401.01301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01301
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#27861;&#24459;&#24187;&#35273;&#65292;&#19981;&#19968;&#33268;&#27861;&#24459;&#20107;&#23454;&#65292;&#24187;&#35273;&#26222;&#36941;&#23384;&#22312;&#39640;&#36798;69%&#33267;88%&#30340;&#24773;&#20917;&#65292;&#26080;&#27861;&#32416;&#27491;&#29992;&#25143;&#38169;&#35823;&#27861;&#24459;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#21487;&#33021;&#25913;&#21464;&#27861;&#24459;&#23454;&#36341;&#65292;&#20294;&#20854;&#28508;&#21147;&#21463;&#21040;&#27861;&#24459;&#24187;&#35273;&#30340;&#23041;&#32961;&#65292;&#21363;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#19982;&#27861;&#24459;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#22871;&#21407;&#21019;&#30340;&#27861;&#24459;&#26597;&#35810;&#26469;&#35843;&#26597;&#36825;&#20123;&#24187;&#35273;&#30340;&#31243;&#24230;&#65292;&#23558;LLMs&#30340;&#22238;&#31572;&#19982;&#32467;&#26500;&#21270;&#30340;&#27861;&#24459;&#20803;&#25968;&#25454;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#26816;&#26597;&#20854;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26377;&#22235;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;&#65288;1&#65289;&#25105;&#20204;&#24314;&#31435;&#20102;&#27861;&#24459;&#24187;&#35273;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#20170;&#21518;&#22312;&#36825;&#19968;&#39046;&#22495;&#36827;&#34892;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#27010;&#24565;&#26694;&#26550;&#12290;&#65288;2&#65289;&#25105;&#20204;&#21457;&#29616;&#65292;&#27861;&#24459;&#24187;&#35273;&#30340;&#26222;&#36941;&#24615;&#20196;&#20154;&#25285;&#24551;&#65292;&#22312;&#23545;&#38543;&#26426;&#32852;&#37030;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20855;&#20307;&#12289;&#21487;&#39564;&#35777;&#30340;&#38382;&#39064;&#26102;&#65292;ChatGPT 3.5&#20135;&#29983;&#30340;&#24187;&#35273;&#21457;&#29983;&#29575;&#20026;69&#65285;&#65292;&#32780;Llama 2&#20026;88&#65285;&#12290;&#65288;3&#65289;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#22312;&#36870;&#21521;&#38382;&#39064;&#35774;&#32622;&#20013;&#24448;&#24448;&#26080;&#27861;&#32416;&#27491;&#29992;&#25143;&#30340;&#38169;&#35823;&#27861;&#24459;&#20551;&#35774;&#12290;&#65288;4&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;LLMs&#24182;&#19981;&#24635;&#33021;&#39044;&#27979;&#25110;&#24182;&#19981;&#24635;&#30693;&#36947;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have the potential to transform the practice of law, but this potential is threatened by the presence of legal hallucinations -- responses from these models that are not consistent with legal facts. We investigate the extent of these hallucinations using an original suite of legal queries, comparing LLMs' responses to structured legal metadata and examining their consistency. Our work makes four key contributions: (1) We develop a typology of legal hallucinations, providing a conceptual framework for future research in this area. (2) We find that legal hallucinations are alarmingly prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with Llama 2, when these models are asked specific, verifiable questions about random federal court cases. (3) We illustrate that LLMs often fail to correct a user's incorrect legal assumptions in a contra-factual question setup. (4) We provide evidence that LLMs cannot always predict, or do not always know, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30340;&#26080;&#32447;&#20449;&#36947;&#24314;&#27169;&#26041;&#27861;&#65292;&#24635;&#32467;&#20102;&#20854;&#20855;&#26377;&#36890;&#29992;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20248;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;PINN&#26041;&#27861;&#20307;&#31995;&#32467;&#26500;&#65292;&#20026;&#26410;&#26469;&#27169;&#22411;&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#19982;&#21551;&#31034;&#12290;&#21478;&#22806;&#65292;&#36824;&#38024;&#23545;&#23460;&#20869;&#20449;&#36947;&#39044;&#27979;&#25552;&#20986;&#20102;&#31934;&#30830;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.01288</link><description>&lt;p&gt;
&#37319;&#29992;&#20998;&#21106;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#29702;&#20449;&#24687;&#36890;&#29992;&#21270;&#26080;&#32447;&#20449;&#36947;&#24314;&#27169;&#65306;&#22522;&#30784;&#12289;&#26041;&#27861;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Generalizable Wireless Channel Modeling with Segmentation and Deep Learning: Fundamentals, Methodologies, and Challenges. (arXiv:2401.01288v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30340;&#26080;&#32447;&#20449;&#36947;&#24314;&#27169;&#26041;&#27861;&#65292;&#24635;&#32467;&#20102;&#20854;&#20855;&#26377;&#36890;&#29992;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20248;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;PINN&#26041;&#27861;&#20307;&#31995;&#32467;&#26500;&#65292;&#20026;&#26410;&#26469;&#27169;&#22411;&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#19982;&#21551;&#31034;&#12290;&#21478;&#22806;&#65292;&#36824;&#38024;&#23545;&#23460;&#20869;&#20449;&#36947;&#39044;&#27979;&#25552;&#20986;&#20102;&#31934;&#30830;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#36947;&#24314;&#27169;&#26159;&#25512;&#21160;&#26080;&#32447;&#31995;&#32479;&#21457;&#23637;&#30340;&#22522;&#30784;&#65292;&#24182;&#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#36235;&#21183;&#26159;&#36234;&#26469;&#36234;&#20381;&#36182;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#26469;&#20419;&#36827;&#24314;&#27169;&#36807;&#31243;&#65292;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#20449;&#36947;&#39044;&#27979;&#12290;&#26412;&#25991;&#39318;&#20808;&#23545;&#25968;&#25454;&#39537;&#21160;&#30340;&#20449;&#36947;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#31616;&#35201;&#27010;&#36848;&#65292;&#24182;&#24378;&#35843;&#20854;&#23616;&#38480;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#24314;&#27169;&#30340;&#27010;&#24565;&#21644;&#20248;&#21183;&#65292;&#24182;&#24635;&#32467;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;PINN&#30340;&#20449;&#36947;&#24314;&#27169;&#26041;&#27861;&#20855;&#26377;&#24456;&#22909;&#30340;&#36890;&#29992;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;PINN&#26041;&#27861;&#20307;&#31995;&#32467;&#26500;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#27169;&#22411;&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#19982;&#21551;&#31034;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#25105;&#20204;&#26368;&#36817;&#30340;&#20851;&#20110;&#31934;&#30830;&#30340;&#23460;&#20869;&#20449;&#36947;&#39044;&#27979;&#19982;&#35821;&#20041;&#20998;&#21106;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Channel modeling is fundamental in advancing wireless systems and has thus attracted considerable research focus. Recent trends have seen a growing reliance on data-driven techniques to facilitate the modeling process and yield accurate channel predictions. In this work, we first provide a concise overview of data-driven channel modeling methods, highlighting their limitations. Subsequently, we introduce the concept and advantages of physics-informed neural network (PINN)-based modeling and a summary of recent contributions in this area. Our findings demonstrate that PINN-based approaches in channel modeling exhibit promising attributes such as generalizability, interpretability, and robustness. We offer a comprehensive architecture for PINN methodology, designed to inform and inspire future model development. A case-study of our recent work on precise indoor channel prediction with semantic segmentation and deep learning is presented. The study concludes by addressing the challenges f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>LLbezpeky&#26159;&#19968;&#39033;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#29702;&#35299;&#20154;&#31867;&#21644;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#35821;&#20041;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;AI&#39537;&#21160;&#30340;&#24037;&#20316;&#27969;&#31243;&#26469;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#35782;&#21035;&#21644;&#20462;&#22797;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2401.01269</link><description>&lt;p&gt;
LLbezpeky: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LLbezpeky: Leveraging Large Language Models for Vulnerability Detection. (arXiv:2401.01269v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01269
&lt;/p&gt;
&lt;p&gt;
LLbezpeky&#26159;&#19968;&#39033;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#29702;&#35299;&#20154;&#31867;&#21644;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#35821;&#20041;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;AI&#39537;&#21160;&#30340;&#24037;&#20316;&#27969;&#31243;&#26469;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#35782;&#21035;&#21644;&#20462;&#22797;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#26500;&#24314;&#23433;&#20840;&#31995;&#32479;&#26041;&#38754;&#36827;&#34892;&#20102;&#25345;&#32493;&#30340;&#30740;&#31350;&#21644;&#36827;&#23637;&#65292;&#20294;&#23433;&#21331;&#24212;&#29992;&#31243;&#24207;&#20173;&#28982;&#23384;&#22312;&#28431;&#27934;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#20998;&#26512;&#24037;&#20855;&#31574;&#30053;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#22823;&#37327;&#30340;&#35823;&#25253;&#21644;&#26377;&#38480;&#30340;&#20998;&#26512;&#33539;&#22260;&#65292;&#20351;&#24471;&#38590;&#20197;&#37319;&#29992;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#25968;&#25454;&#38656;&#27714;&#21644;&#29305;&#24449;&#24037;&#31243;&#25361;&#25112;&#30340;&#38480;&#21046;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20973;&#20511;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#65292;&#22312;&#29702;&#35299;&#20154;&#31867;&#21644;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#35821;&#20041;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22312;&#23433;&#21331;&#23433;&#20840;&#30340;&#32972;&#26223;&#19979;&#65292;LLMs&#29992;&#20110;&#26816;&#27979;&#28431;&#27934;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#35782;&#21035;&#21644;&#20462;&#22797;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#33021;&#22815;&#26377;&#25928;&#26816;&#27979;&#20986;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the continued research and progress in building secure systems, Android applications continue to be ridden with vulnerabilities, necessitating effective detection methods. Current strategies involving static and dynamic analysis tools come with limitations like overwhelming number of false positives and limited scope of analysis which make either difficult to adopt. Over the past years, machine learning based approaches have been extensively explored for vulnerability detection, but its real-world applicability is constrained by data requirements and feature engineering challenges. Large Language Models (LLMs), with their vast parameters, have shown tremendous potential in understanding semnatics in human as well as programming languages. We dive into the efficacy of LLMs for detecting vulnerabilities in the context of Android security. We focus on building an AI-driven workflow to assist developers in identifying and rectifying vulnerabilities. Our experiments show that LLMs o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#32508;&#21512;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#26368;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#38376;&#25968;&#26469;&#38477;&#20302;&#33455;&#29255;&#38754;&#31215;&#21644;&#30005;&#36335;&#25104;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;&#20943;&#23569;&#20102;&#36817;30&#65285;&#30340;&#38376;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.01265</link><description>&lt;p&gt;
&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#32508;&#21512;&#20855;&#26377;&#36890;&#29992;&#38376;&#30340;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#26368;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Synthesis of Finite State Machines with Universal Gates using Evolutionary Algorithm. (arXiv:2401.01265v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#32508;&#21512;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#26368;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#38376;&#25968;&#26469;&#38477;&#20302;&#33455;&#29255;&#38754;&#31215;&#21644;&#30005;&#36335;&#25104;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;&#20943;&#23569;&#20102;&#36817;30&#65285;&#30340;&#38376;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32508;&#21512;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#20943;&#23569;&#33455;&#29255;&#38754;&#31215;&#21644;&#30005;&#36335;&#25104;&#26412;&#12290;&#21033;&#29992;&#31515;&#21345;&#23572;&#36951;&#20256;&#32534;&#31243;&#28436;&#21270;&#20102;MCNC91&#22522;&#20934;&#30005;&#36335;&#20013;&#30340;&#19968;&#31995;&#21015;&#26377;&#38480;&#29366;&#24577;&#26426;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#24635;&#35745;&#20943;&#23569;&#20102;&#36817;30&#65285;&#30340;&#38376;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#19968;&#20123;&#21442;&#25968;&#23545;&#36827;&#21270;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents an optimization method for the synthesis of finite state machines. The focus is on the reduction in the on-chip area and the cost of the circuit. A list of finite state machines from MCNC91 benchmark circuits have been evolved using Cartesian Genetic Programming. On the average, almost 30% of reduction in the total number of gates has been achieved. The effects of some parameters on the evolutionary process have also been discussed in the paper.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#21457;&#20844;&#24179;&#24615;&#35748;&#35777;&#26041;&#27861;&#12290;&#36890;&#36807;&#32508;&#36848;&#22823;&#37327;&#25991;&#29486;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#20026;&#25805;&#20316;&#21270;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2401.01262</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20844;&#24179;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Fairness Certification for Natural Language Processing and Large Language Models. (arXiv:2401.01262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01262
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#21457;&#20844;&#24179;&#24615;&#35748;&#35777;&#26041;&#27861;&#12290;&#36890;&#36807;&#32508;&#36848;&#22823;&#37327;&#25991;&#29486;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#20026;&#25805;&#20316;&#21270;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;NLP&#22312;&#25307;&#32856;&#31561;&#20844;&#24179;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#20013;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#20363;&#22914;&#20316;&#20026;&#19987;&#23478;&#31995;&#32479;&#25110;&#22522;&#20110;LLM&#30340;&#25945;&#32946;&#23548;&#24072;&#12290;&#30001;&#20110;NLP&#22522;&#20110;&#20154;&#31867;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#26377;&#23475;&#20559;&#35265;&#28183;&#20837;NLP&#31995;&#32479;&#65292;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#27495;&#35270;&#23569;&#25968;&#32676;&#20307;&#25110;&#24341;&#21457;&#27861;&#24459;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24320;&#23637;NLP&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#37319;&#29992;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#22823;&#37327;&#25991;&#29486;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#19982;&#35813;&#39046;&#22495;&#30340;&#22810;&#20301;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#30340;&#19987;&#23478;&#35775;&#35848;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25552;&#20986;&#20102;NLP&#30340;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#32454;&#21270;&#20026;18&#20010;&#23376;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#26631;&#20934;&#20026;&#23454;&#26045;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) plays an important role in our daily lives, particularly due to the enormous progress of Large Language Models (LLM). However, NLP has many fairness-critical use cases, e.g., as an expert system in recruitment or as an LLM-based tutor in education. Since NLP is based on human language, potentially harmful biases can diffuse into NLP systems and produce unfair results, discriminate against minorities or generate legal issues. Hence, it is important to develop a fairness certification for NLP approaches. We follow a qualitative research approach towards a fairness certification for NLP. In particular, we have reviewed a large body of literature on algorithmic fairness, and we have conducted semi-structured expert interviews with a wide range of experts from that area. We have systematically devised six fairness criteria for NLP, which can be further refined into 18 sub-categories. Our criteria offer a foundation for operationalizing and testing processes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.01259</link><description>&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#26159;&#21542;&#36981;&#24490;&#23616;&#37096;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Concept Bottleneck Models Obey Locality?. (arXiv:2401.01259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#22522;&#30784;&#23398;&#20064;&#36890;&#36807;&#35299;&#37322;&#20854;&#39044;&#27979;&#32467;&#26524;&#20351;&#29992;&#20154;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#25913;&#21892;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#31181;&#33539;&#24335;&#19979;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#29420;&#31435;&#20110;&#20854;&#20182;&#27010;&#24565;&#30340;&#32473;&#23450;&#27010;&#24565;&#30340;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#28872;&#26263;&#31034;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#22312;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#36825;&#19968;&#20856;&#22411;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#26550;&#26500;&#20013;&#19981;&#33021;&#25104;&#31435;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#36825;&#20123;&#27010;&#24565;&#26082;&#22312;&#31354;&#38388;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#23436;&#20840;&#30001;&#22266;&#23450;&#23376;&#38598;&#30340;&#29305;&#24449;&#23450;&#20041;&#65289;&#21448;&#22312;&#35821;&#20041;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#20165;&#19982;&#39044;&#23450;&#20041;&#30340;&#22266;&#23450;&#23376;&#38598;&#30340;&#27010;&#24565;&#30456;&#20851;&#32852;&#65289;&#23450;&#20301;&#26102;&#65292;CBMs&#26159;&#21542;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#12290;&#20026;&#20102;&#29702;&#35299;&#23616;&#37096;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27010;&#24565;&#20043;&#22806;&#30340;&#29305;&#24449;&#21464;&#21270;&#23545;&#27010;&#24565;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept-based learning improves a deep learning model's interpretability by explaining its predictions via human-understandable concepts. Deep learning models trained under this paradigm heavily rely on the assumption that neural networks can learn to predict the presence or absence of a given concept independently of other concepts. Recent work, however, strongly suggests that this assumption may fail to hold in Concept Bottleneck Models (CBMs), a quintessential family of concept-based interpretable architectures. In this paper, we investigate whether CBMs correctly capture the degree of conditional independence across concepts when such concepts are localised both spatially, by having their values entirely defined by a fixed subset of features, and semantically, by having their values correlated with only a fixed subset of predefined concepts. To understand locality, we analyse how changes to features outside of a concept's spatial or semantic locality impact concept predictions. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20108;&#36827;&#21046;&#20107;&#20214;&#32534;&#30721;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20855;&#26377;&#25512;&#26029;&#26412;&#22320;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01242</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#26681;&#26641;&#20013;&#23545;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#30340;&#20108;&#36827;&#21046;&#20107;&#20214;&#36827;&#34892;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Encoding Binary Events from Continuous Time Series in Rooted Trees using Contrastive Learning. (arXiv:2401.01242v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20108;&#36827;&#21046;&#20107;&#20214;&#32534;&#30721;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20855;&#26377;&#25512;&#26029;&#26412;&#22320;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#22495;&#22522;&#30784;&#35774;&#26045;&#25152;&#26377;&#32773;&#36890;&#24120;&#19981;&#30693;&#36947;&#20182;&#20204;&#30340;&#23458;&#25143;&#22312;&#26412;&#22320;&#32593;&#32476;&#20013;&#26159;&#22914;&#20309;&#36830;&#25509;&#30340;&#65292;&#36825;&#20123;&#32593;&#32476;&#20197;&#26681;&#26641;&#32467;&#26500;&#32452;&#32455;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#20351;&#29992;&#26469;&#33258;&#26641;&#21494;&#65288;&#23458;&#25143;&#65289;&#30340;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25512;&#26029;&#26412;&#22320;&#32593;&#32476;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20108;&#36827;&#21046;&#20107;&#20214;&#32534;&#30721;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#20316;&#20026;&#21021;&#27493;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#26377;&#20215;&#20540;&#30340;&#32534;&#30721;&#22120;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Broadband infrastructure owners do not always know how their customers are connected in the local networks, which are structured as rooted trees. A recent study is able to infer the topology of a local network using discrete time series data from the leaves of the tree (customers). In this study we propose a contrastive approach for learning a binary event encoder from continuous time series data. As a preliminary result, we show that our approach has some potential in learning a valuable encoder.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;VGG&#30340;&#22810;&#27169;&#24577;&#20154;&#33080;&#29983;&#29289;&#29305;&#24449;&#31995;&#32479;"IdentiFace"&#65292;&#36890;&#36807;&#23558;&#20154;&#33080;&#35782;&#21035;&#19982;&#24615;&#21035;&#12289;&#33080;&#22411;&#21644;&#24773;&#24863;&#31561;&#36719;&#29983;&#29289;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#26377;&#24847;&#20041;&#30340;&#32467;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#26550;&#26500;&#21644;&#23545;&#23398;&#21040;&#30340;&#29305;&#24449;&#36827;&#34892;&#35299;&#37322;&#65292;&#35813;&#31995;&#32479;&#22312;&#39640;&#20869;&#31867;&#21035;&#21464;&#24322;&#19979;&#21462;&#24471;&#20102;99.2%&#30340;&#27979;&#35797;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.01227</link><description>&lt;p&gt;
IdentiFace&#65306;&#22522;&#20110;VGG&#30340;&#22810;&#27169;&#24577;&#20154;&#33080;&#29983;&#29289;&#29305;&#24449;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
IdentiFace : A VGG Based Multimodal Facial Biometric System. (arXiv:2401.01227v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;VGG&#30340;&#22810;&#27169;&#24577;&#20154;&#33080;&#29983;&#29289;&#29305;&#24449;&#31995;&#32479;"IdentiFace"&#65292;&#36890;&#36807;&#23558;&#20154;&#33080;&#35782;&#21035;&#19982;&#24615;&#21035;&#12289;&#33080;&#22411;&#21644;&#24773;&#24863;&#31561;&#36719;&#29983;&#29289;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#26377;&#24847;&#20041;&#30340;&#32467;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#26550;&#26500;&#21644;&#23545;&#23398;&#21040;&#30340;&#29305;&#24449;&#36827;&#34892;&#35299;&#37322;&#65292;&#35813;&#31995;&#32479;&#22312;&#39640;&#20869;&#31867;&#21035;&#21464;&#24322;&#19979;&#21462;&#24471;&#20102;99.2%&#30340;&#27979;&#35797;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#29983;&#29289;&#29305;&#24449;&#31995;&#32479;&#30340;&#21457;&#23637;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#24040;&#22823;&#36129;&#29486;&#12290;&#29616;&#20170;&#65292;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#20197;&#39640;&#25928;&#12289;&#26377;&#24847;&#20041;&#30340;&#26041;&#24335;&#32467;&#21512;&#22810;&#31181;&#29983;&#29289;&#29305;&#24449;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;IdentiFace&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#20154;&#33080;&#29983;&#29289;&#29305;&#24449;&#31995;&#32479;&#65292;&#23558;&#20154;&#33080;&#35782;&#21035;&#30340;&#26680;&#24515;&#19982;&#24615;&#21035;&#12289;&#33080;&#22411;&#21644;&#24773;&#24863;&#31561;&#19968;&#20123;&#37325;&#35201;&#30340;&#36719;&#29983;&#29289;&#29305;&#24449;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#30528;&#37325;&#20351;&#29992;&#21482;&#26377;VGG-16&#21463;&#21040;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#22312;&#19981;&#21516;&#23376;&#31995;&#32479;&#20013;&#36827;&#34892;&#20102;&#19968;&#20123;&#32454;&#24494;&#30340;&#26356;&#25913;&#12290;&#36825;&#31181;&#32479;&#19968;&#24615;&#20351;&#24471;&#36328;&#27169;&#24577;&#30340;&#38598;&#25104;&#26356;&#21152;&#31616;&#21333;&#12290;&#23427;&#26356;&#23481;&#26131;&#35299;&#37322;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#23398;&#21040;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#23545;&#20110;&#20915;&#31574;&#36807;&#31243;&#21644;&#20154;&#33080;&#27169;&#24577;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#32473;&#20986;&#20102;&#24456;&#22909;&#30340;&#25351;&#31034;&#12290;&#23545;&#20110;&#35782;&#21035;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#22312;&#20116;&#20010;&#31867;&#21035;&#30340;&#39640;&#20869;&#31867;&#21035;&#21464;&#24322;&#19979;&#33719;&#24471;&#20102;99.2%&#30340;&#27979;&#35797;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of facial biometric systems has contributed greatly to the development of the computer vision field. Nowadays, there's always a need to develop a multimodal system that combines multiple biometric traits in an efficient, meaningful way. In this paper, we introduce "IdentiFace" which is a multimodal facial biometric system that combines the core of facial recognition with some of the most important soft biometric traits such as gender, face shape, and emotion. We also focused on developing the system using only VGG-16 inspired architecture with minor changes across different subsystems. This unification allows for simpler integration across modalities. It makes it easier to interpret the learned features between the tasks which gives a good indication about the decision-making process across the facial modalities and potential connection. For the recognition problem, we acquired a 99.2% test accuracy for five classes with high intra-class variations using data collected 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;&#65288;ZOE&#65289;&#26469;&#38477;&#20302;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#12290;&#23454;&#39564;&#35777;&#23454;ZOE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01218</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Position Debiasing for Large Language Models. (arXiv:2401.01218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;&#65288;ZOE&#65289;&#26469;&#38477;&#20302;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#12290;&#23454;&#39564;&#35777;&#23454;ZOE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#24615;&#33021;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;LLMs&#21487;&#33021;&#36866;&#24212;&#25968;&#25454;&#38598;&#20559;&#35265;&#21644;&#39044;&#27979;&#30340;&#25463;&#24452;&#65292;&#23548;&#33268;&#29983;&#25104;&#24615;&#33021;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#23481;&#26131;&#34920;&#29616;&#20986;&#20301;&#32622;&#20559;&#24046;&#65292;&#21363;&#21033;&#29992;&#20301;&#20110;&#24320;&#22836;&#25110;&#26411;&#23614;&#25110;&#36755;&#20837;&#20013;&#29305;&#23450;&#20301;&#32622;&#32447;&#32034;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#20943;&#36731;&#20301;&#32622;&#20559;&#24046;&#30340;&#24037;&#20316;&#38656;&#35201;&#22806;&#37096;&#20559;&#24046;&#30693;&#35782;&#25110;&#24102;&#27880;&#37322;&#30340;&#38750;&#20559;&#20506;&#26679;&#26412;&#65292;&#22312;&#23454;&#38469;&#20013;&#19981;&#22826;&#23454;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#65288;ZOE&#65289;&#26694;&#26550;&#23545;LLMs&#36827;&#34892;&#20301;&#32622;&#21435;&#20559;&#12290;ZOE&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#20219;&#20309;&#22806;&#37096;&#30693;&#35782;&#25110;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25552;&#39640;&#26080;&#30417;&#30563;&#21709;&#24212;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#20174;&#23545;&#40784;&#65288;MSA&#65289;&#27169;&#22359;&#26469;&#20462;&#21098;&#36825;&#20123;&#21709;&#24212;&#12290;&#23545;&#20843;&#20010;&#25968;&#25454;&#38598;&#21644;&#20116;&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ZOE&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Experimental result shows that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing works on mitigating position bias require external bias knowledge or annotated non-biased samples, which is unpractical in reality. In this work, we propose a zero-shot position debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages unsupervised responses from pre-trained LLMs for debiasing, thus without any external knowledge or datasets. To improve the quality of unsupervised responses, we propose a master-slave alignment (MSA) module to prune these responses. Experiments on eight datasets and five tasks show that ZOE consistently outperform
&lt;/p&gt;</description></item><item><title>PPBFL&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#21306;&#22359;&#38142;&#21644;&#33258;&#36866;&#24212;&#24046;&#20998;&#38544;&#31169;&#28155;&#21152;&#31639;&#27861;&#65292;&#22686;&#24378;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;&#21644;&#33410;&#28857;&#30340;&#31215;&#26497;&#21442;&#19982;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#20132;&#26131;&#28151;&#21512;&#26426;&#21046;&#65292;&#26356;&#22909;&#22320;&#20445;&#25252;&#26412;&#22320;&#35757;&#32451;&#30340;&#36523;&#20221;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.01204</link><description>&lt;p&gt;
PPBFL: &#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PPBFL: A Privacy Protected Blockchain-based Federated Learning Model. (arXiv:2401.01204v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01204
&lt;/p&gt;
&lt;p&gt;
PPBFL&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#21306;&#22359;&#38142;&#21644;&#33258;&#36866;&#24212;&#24046;&#20998;&#38544;&#31169;&#28155;&#21152;&#31639;&#27861;&#65292;&#22686;&#24378;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;&#21644;&#33410;&#28857;&#30340;&#31215;&#26497;&#21442;&#19982;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#20132;&#26131;&#28151;&#21512;&#26426;&#21046;&#65292;&#26356;&#22909;&#22320;&#20445;&#25252;&#26412;&#22320;&#35757;&#32451;&#30340;&#36523;&#20221;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#32852;&#37030;&#23398;&#20064;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#31361;&#20986;&#30340;&#28966;&#28857;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#21442;&#25968;&#25915;&#20987;&#21644;&#32570;&#20047;&#28608;&#21169;&#26426;&#21046;&#31561;&#25361;&#25112;&#38459;&#30861;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65288;PPBFL&#65289;&#65292;&#20197;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;&#24182;&#20419;&#36827;&#33410;&#28857;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#31215;&#26497;&#21442;&#19982;&#12290;&#21306;&#22359;&#38142;&#30830;&#20445;&#20102;&#23384;&#20648;&#22312;&#26143;&#38469;&#25991;&#20214;&#31995;&#32479;&#65288;IPFS&#65289;&#20013;&#30340;&#27169;&#22411;&#21442;&#25968;&#19981;&#34987;&#31713;&#25913;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#24046;&#20998;&#38544;&#31169;&#28155;&#21152;&#31639;&#27861;&#65292;&#21516;&#26102;&#24212;&#29992;&#20110;&#26412;&#22320;&#27169;&#22411;&#21644;&#20840;&#23616;&#27169;&#22411;&#65292;&#20445;&#25252;&#26412;&#22320;&#27169;&#22411;&#30340;&#38544;&#31169;&#21516;&#26102;&#38450;&#27490;&#22240;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#22823;&#37327;&#26412;&#22320;&#27169;&#22411;&#32780;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20132;&#26131;&#28151;&#21512;&#26426;&#21046;&#65292;&#20197;&#26356;&#22909;&#22320;&#20445;&#25252;&#26412;&#22320;&#35757;&#32451;&#30340;&#36523;&#20221;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of machine learning and growing concerns about data privacy, federated learning has become an increasingly prominent focus. However, challenges such as attacks on model parameters and the lack of incentive mechanisms hinder the effectiveness of federated learning. Therefore, we propose a Privacy Protected Blockchain-based Federated Learning Model (PPBFL) to enhance the security of federated learning and promote the active participation of nodes in model training. Blockchain ensures that model parameters stored in the InterPlanetary File System (IPFS) remain unaltered. A novel adaptive differential privacy addition algorithm is simultaneously applied to local and global models, preserving the privacy of local models and preventing a decrease in the security of the global model due to the presence of numerous local models in federated learning. Additionally, we introduce a new mix transactions mechanism to better protect the identity privacy of local training c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#20351;&#29992;&#36817;&#32418;&#22806;&#20809;&#35889;&#25968;&#25454;&#36827;&#34892;&#30382;&#32932;&#30149;&#21464;&#35786;&#26029;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#36817;&#32418;&#22806;&#20809;&#35889;&#21487;&#20197;&#25552;&#20379;&#30149;&#21464;&#20998;&#23376;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#20026;&#33258;&#21160;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#25552;&#20379;&#20102;&#26367;&#20195;&#20449;&#24687;&#26469;&#28304;&#12290;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#20809;&#35889;&#23398;&#30340;&#24212;&#29992;&#20063;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2401.01200</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#30382;&#32932;&#30149;&#21464;&#35786;&#26029;&#30340;&#27963;&#20307;&#36817;&#32418;&#22806;&#20809;&#35889;&#25968;&#25454;&#12290; (arXiv:2401.01200v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
Skin cancer diagnosis using NIR spectroscopy data of skin lesions in vivo using machine learning algorithms. (arXiv:2401.01200v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#20351;&#29992;&#36817;&#32418;&#22806;&#20809;&#35889;&#25968;&#25454;&#36827;&#34892;&#30382;&#32932;&#30149;&#21464;&#35786;&#26029;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#36817;&#32418;&#22806;&#20809;&#35889;&#21487;&#20197;&#25552;&#20379;&#30149;&#21464;&#20998;&#23376;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#20026;&#33258;&#21160;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#25552;&#20379;&#20102;&#26367;&#20195;&#20449;&#24687;&#26469;&#28304;&#12290;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#20809;&#35889;&#23398;&#30340;&#24212;&#29992;&#20063;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30382;&#32932;&#30149;&#21464;&#21487;&#20197;&#20998;&#20026;&#33391;&#24615;&#21644;&#24694;&#24615;&#12290;&#22312;&#24694;&#24615;&#30149;&#21464;&#20013;&#65292;&#40657;&#33394;&#32032;&#30244;&#26159;&#19968;&#31181;&#38750;&#24120;&#20405;&#34989;&#24615;&#30340;&#30284;&#30151;&#65292;&#20063;&#26159;&#23548;&#33268;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#26089;&#26399;&#35786;&#26029;&#30382;&#32932;&#30284;&#38750;&#24120;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992;&#22270;&#20687;&#21644;&#20020;&#24202;&#25968;&#25454;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#65288;CAD&#65289;&#20013;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#22823;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20449;&#24687;&#26469;&#28304;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#33021;&#25552;&#20379;&#30149;&#21464;&#20998;&#23376;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#36817;&#32418;&#22806;&#20809;&#35889;&#21487;&#20197;&#20026;&#30382;&#32932;&#30149;&#21464;&#30340;&#33258;&#21160;CAD&#25552;&#20379;&#26367;&#20195;&#20449;&#24687;&#26469;&#28304;&#12290;&#20809;&#35889;&#23398;&#20013;&#26368;&#24120;&#29992;&#30340;&#25216;&#26415;&#21644;&#20998;&#31867;&#31639;&#27861;&#26159;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#12289;&#20559;&#26368;&#23567;&#20108;&#20056;-&#21028;&#21035;&#20998;&#26512;&#65288;PLS-DA&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#23558;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;MDL&#65289;&#24212;&#29992;&#20110;&#20809;&#35889;&#23398;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#32780;&#23558;MDL&#24212;&#29992;&#20110;&#20809;&#35889;&#23398;&#30340;&#20027;&#35201;&#38480;&#21046;&#20043;&#19968;&#26159;&#32570;&#20047;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin lesions are classified in benign or malignant. Among the malignant, melanoma is a very aggressive cancer and the major cause of deaths. So, early diagnosis of skin cancer is very desired. In the last few years, there is a growing interest in computer aided diagnostic (CAD) using most image and clinical data of the lesion. These sources of information present limitations due to their inability to provide information of the molecular structure of the lesion. NIR spectroscopy may provide an alternative source of information to automated CAD of skin lesions. The most commonly used techniques and classification algorithms used in spectroscopy are Principal Component Analysis (PCA), Partial Least Squares - Discriminant Analysis (PLS-DA), and Support Vector Machines (SVM). Nonetheless, there is a growing interest in applying the modern techniques of machine and deep learning (MDL) to spectroscopy. One of the main limitations to apply MDL to spectroscopy is the lack of public datasets. Si
&lt;/p&gt;</description></item><item><title>JMA&#26159;&#19968;&#31181;&#36890;&#29992;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20960;&#20046;&#26368;&#20248;&#30340;&#23450;&#21521;&#23545;&#25239;&#26679;&#26412;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;Jacobian&#24341;&#36215;&#30340;&#39532;&#27663;&#36317;&#31163;&#65292;&#32771;&#34385;&#20102;&#23558;&#36755;&#20837;&#26679;&#26412;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#22312;&#32473;&#23450;&#26041;&#21521;&#19978;&#31227;&#21160;&#25152;&#38656;&#30340;&#25237;&#20837;&#12290;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#23545;&#25239;&#26679;&#26412;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.01199</link><description>&lt;p&gt;
JMA:&#19968;&#31181;&#24555;&#36895;&#29983;&#25104;&#20960;&#20046;&#26368;&#20248;&#23450;&#21521;&#23545;&#25239;&#26679;&#26412;&#30340;&#36890;&#29992;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial Example. (arXiv:2401.01199v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01199
&lt;/p&gt;
&lt;p&gt;
JMA&#26159;&#19968;&#31181;&#36890;&#29992;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20960;&#20046;&#26368;&#20248;&#30340;&#23450;&#21521;&#23545;&#25239;&#26679;&#26412;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;Jacobian&#24341;&#36215;&#30340;&#39532;&#27663;&#36317;&#31163;&#65292;&#32771;&#34385;&#20102;&#23558;&#36755;&#20837;&#26679;&#26412;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#22312;&#32473;&#23450;&#26041;&#21521;&#19978;&#31227;&#21160;&#25152;&#38656;&#30340;&#25237;&#20837;&#12290;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#23545;&#25239;&#26679;&#26412;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#29992;&#20110;&#29983;&#25104;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#23450;&#21521;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#37117;&#26159;&#39640;&#24230;&#27425;&#20248;&#30340;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#22686;&#21152;&#30446;&#26631;&#31867;&#21035;&#30340;&#21487;&#33021;&#24615;&#65292;&#22240;&#27492;&#38544;&#21547;&#22320;&#19987;&#27880;&#20110;&#19968;&#28909;&#32534;&#30721;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#36890;&#29992;&#30340;&#12289;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#23450;&#21521;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26368;&#23567;&#21270;&#38597;&#21487;&#27604;&#24341;&#36215;&#30340;&#39532;&#27663;&#36317;&#31163;&#65288;JMA&#65289;&#39033;&#65292;&#32771;&#34385;&#23558;&#36755;&#20837;&#26679;&#26412;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#22312;&#32473;&#23450;&#26041;&#21521;&#19978;&#31227;&#21160;&#25152;&#38656;&#30340;&#25237;&#20837;&#65288;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#27779;&#23572;&#22827;&#20108;&#37325;&#24615;&#23450;&#29702;&#27714;&#35299;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#35299;&#38750;&#36127;&#26368;&#23567;&#20108;&#20056;&#65288;NNLS&#65289;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20026;Szegedy&#31561;&#20154;&#26368;&#21021;&#24341;&#20837;&#30340;&#23545;&#25239;&#26679;&#26412;&#38382;&#39064;&#30340;&#32447;&#24615;&#21270;&#29256;&#26412;&#25552;&#20379;&#20102;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#30340;&#24191;&#27867;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the approaches proposed so far to craft targeted adversarial examples against Deep Learning classifiers are highly suboptimal and typically rely on increasing the likelihood of the target class, thus implicitly focusing on one-hot encoding settings. In this paper, we propose a more general, theoretically sound, targeted attack that resorts to the minimization of a Jacobian-induced MAhalanobis distance (JMA) term, taking into account the effort (in the input space) required to move the latent space representation of the input sample in a given direction. The minimization is solved by exploiting the Wolfe duality theorem, reducing the problem to the solution of a Non-Negative Least Square (NNLS) problem. The proposed algorithm provides an optimal solution to a linearized version of the adversarial example problem originally introduced by Szegedy et al. \cite{szegedy2013intriguing}. The experiments we carried out confirm the generality of the proposed attack which is proven to be 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#35823;&#35299;&#20449;&#24687;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20998;&#31867;&#26694;&#26550;&#21644;&#29983;&#25104;&#26377;&#25928;&#30340;&#29992;&#25143;&#26597;&#35810;&#26469;&#35299;&#20915;&#32570;&#22833;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35823;&#35299;&#20449;&#24687;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01197</link><description>&lt;p&gt;
&#35823;&#35299;&#20449;&#24687;&#26816;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Resolution in Misinformation Detection. (arXiv:2401.01197v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#35823;&#35299;&#20449;&#24687;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20998;&#31867;&#26694;&#26550;&#21644;&#29983;&#25104;&#26377;&#25928;&#30340;&#29992;&#25143;&#26597;&#35810;&#26469;&#35299;&#20915;&#32570;&#22833;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35823;&#35299;&#20449;&#24687;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#35299;&#20449;&#24687;&#23384;&#22312;&#21508;&#31181;&#39118;&#38505;&#65292;&#22914;&#30772;&#22351;&#20844;&#20247;&#20449;&#20219;&#21644;&#25197;&#26354;&#20107;&#23454;&#35328;&#35770;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#24050;&#34987;&#35777;&#26126;&#22312;&#20943;&#36731;&#35823;&#35299;&#20449;&#24687;&#26041;&#38754;&#24456;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#25552;&#20379;&#36275;&#22815;&#19978;&#19979;&#25991;&#30340;&#38472;&#36848;&#26102;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24456;&#38590;&#20934;&#30830;&#35780;&#20272;&#27169;&#31946;&#25110;&#32570;&#20047;&#19978;&#19979;&#25991;&#30340;&#38472;&#36848;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#27492;&#31867;&#38472;&#36848;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23545;&#32570;&#22833;&#20449;&#24687;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20026;LIAR-New&#25968;&#25454;&#38598;&#25552;&#20379;&#31867;&#21035;&#26631;&#31614;&#65292;&#35813;&#25968;&#25454;&#38598;&#36866;&#29992;&#20110;&#20855;&#26377;&#32570;&#22833;&#20449;&#24687;&#30340;&#36328;&#22495;&#20869;&#23481;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#29983;&#25104;&#32570;&#22833;&#19978;&#19979;&#25991;&#30340;&#26377;&#25928;&#29992;&#25143;&#26597;&#35810;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#29992;&#25143;&#21487;&#22238;&#31572;&#38382;&#39064;&#30340;&#27604;&#20363;38&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#19988;&#20998;&#31867;&#24615;&#33021;&#25552;&#39640;&#20102;&#36229;&#36807;10&#20010;&#30334;&#20998;&#28857;&#30340;&#23439;F1&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#26410;&#26469;&#30340;&#35823;&#35299;&#20449;&#24687;&#32531;&#35299;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation poses a variety of risks, such as undermining public trust and distorting factual discourse. Large Language Models (LLMs) like GPT-4 have been shown effective in mitigating misinformation, particularly in handling statements where enough context is provided. However, they struggle to assess ambiguous or context-deficient statements accurately. This work introduces a new method to resolve uncertainty in such statements. We propose a framework to categorize missing information and publish category labels for the LIAR-New dataset, which is adaptable to cross-domain content with missing information. We then leverage this framework to generate effective user queries for missing context. Compared to baselines, our method improves the rate at which generated questions are answerable by the user by 38 percentage points and classification performance by over 10 percentage points macro F1. Thus, this approach may provide a valuable component for future misinformation mitigation pi
&lt;/p&gt;</description></item><item><title>NID-SLAM&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#30340;RGB-D SLAM&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#25552;&#21319;&#35821;&#20041;&#25513;&#33180;&#20013;&#30340;&#19981;&#20934;&#30830;&#21306;&#22495;&#21644;&#24341;&#20837;&#20851;&#38190;&#24103;&#36873;&#25321;&#31574;&#30053;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#36319;&#36394;&#31934;&#24230;&#21644;&#24314;&#22270;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.01189</link><description>&lt;p&gt;
NID-SLAM: &#22522;&#20110;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#30340;&#21160;&#24577;&#29615;&#22659;RGB-D SLAM
&lt;/p&gt;
&lt;p&gt;
NID-SLAM: Neural Implicit Representation-based RGB-D SLAM in dynamic environments. (arXiv:2401.01189v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01189
&lt;/p&gt;
&lt;p&gt;
NID-SLAM&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#30340;RGB-D SLAM&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#25552;&#21319;&#35821;&#20041;&#25513;&#33180;&#20013;&#30340;&#19981;&#20934;&#30830;&#21306;&#22495;&#21644;&#24341;&#20837;&#20851;&#38190;&#24103;&#36873;&#25321;&#31574;&#30053;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#36319;&#36394;&#31934;&#24230;&#21644;&#24314;&#22270;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#24050;&#34987;&#29992;&#20110;&#22686;&#24378;&#35270;&#35273;SLAM&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#25552;&#20379;&#39640;&#20445;&#30495;&#23494;&#38598;&#22320;&#22270;&#26041;&#38754;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#38745;&#24577;&#22330;&#26223;&#19979;&#36816;&#20316;&#33391;&#22909;&#65292;&#20294;&#22312;&#31227;&#21160;&#29289;&#20307;&#36896;&#25104;&#30340;&#24178;&#25200;&#19979;&#22256;&#38590;&#37325;&#37325;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NID-SLAM&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#31070;&#32463;SLAM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#35821;&#20041;&#25513;&#33180;&#20013;&#30340;&#19981;&#20934;&#30830;&#21306;&#22495;&#65292;&#29305;&#21035;&#26159;&#36793;&#32536;&#21306;&#22495;&#12290;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#20013;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#31934;&#30830;&#22320;&#21435;&#38500;&#21160;&#24577;&#29289;&#20307;&#65292;&#20174;&#32780;&#38477;&#20302;&#30456;&#26426;&#28418;&#31227;&#30340;&#27010;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21160;&#24577;&#22330;&#26223;&#30340;&#20851;&#38190;&#24103;&#36873;&#25321;&#31574;&#30053;&#65292;&#22686;&#24378;&#20102;&#30456;&#26426;&#36319;&#36394;&#23545;&#22823;&#23610;&#24230;&#29289;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#39640;&#20102;&#24314;&#22270;&#30340;&#25928;&#29575;&#12290;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;RGB-D&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36319;&#36394;&#31934;&#24230;&#21644;&#24314;&#22270;&#36136;&#37327;&#19978;&#20248;&#20110;&#31454;&#20105;&#24615;&#30340;&#31070;&#32463;SLAM&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural implicit representations have been explored to enhance visual SLAM algorithms, especially in providing high-fidelity dense map. Existing methods operate robustly in static scenes but struggle with the disruption caused by moving objects. In this paper we present NID-SLAM, which significantly improves the performance of neural SLAM in dynamic environments. We propose a new approach to enhance inaccurate regions in semantic masks, particularly in marginal areas. Utilizing the geometric information present in depth images, this method enables accurate removal of dynamic objects, thereby reducing the probability of camera drift. Additionally, we introduce a keyframe selection strategy for dynamic scenes, which enhances camera tracking robustness against large-scale objects and improves the efficiency of mapping. Experiments on publicly available RGB-D datasets demonstrate that our method outperforms competitive neural SLAM approaches in tracking accuracy and mapping quality in dynam
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#25163;&#26426;&#24433;&#20687;&#36827;&#34892;&#22478;&#24066;&#34903;&#36947;&#26641;&#26408;&#28165;&#26597;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#20998;&#21106;&#26641;&#24178;&#24182;&#35745;&#31639;&#33016;&#24452;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#20934;&#30830;&#24615;&#12289;&#26356;&#23569;&#23545;&#19987;&#19994;&#35774;&#22791;&#30340;&#20381;&#36182;&#65292;&#24182;&#36866;&#29992;&#20110;&#38590;&#20197;&#25509;&#35302;&#30340;&#22320;&#21306;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#33016;&#24452;&#20272;&#31639;&#19978;&#30340;&#20934;&#30830;&#29575;&#35823;&#24046;&#29575;&#23567;&#20110;2.5%&#65292;&#26377;&#26395;&#22312;&#22478;&#24066;&#34903;&#36947;&#26641;&#26408;&#28165;&#26597;&#26041;&#38754;&#20135;&#29983;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01180</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#31227;&#21160;&#25163;&#26426;&#22270;&#20687;&#19978;&#36827;&#34892;&#31934;&#30830;&#39640;&#25928;&#30340;&#22478;&#24066;&#34903;&#36947;&#26641;&#26408;&#28165;&#26597;
&lt;/p&gt;
&lt;p&gt;
Accurate and Efficient Urban Street Tree Inventory with Deep Learning on Mobile Phone Imagery. (arXiv:2401.01180v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#25163;&#26426;&#24433;&#20687;&#36827;&#34892;&#22478;&#24066;&#34903;&#36947;&#26641;&#26408;&#28165;&#26597;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#20998;&#21106;&#26641;&#24178;&#24182;&#35745;&#31639;&#33016;&#24452;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#20934;&#30830;&#24615;&#12289;&#26356;&#23569;&#23545;&#19987;&#19994;&#35774;&#22791;&#30340;&#20381;&#36182;&#65292;&#24182;&#36866;&#29992;&#20110;&#38590;&#20197;&#25509;&#35302;&#30340;&#22320;&#21306;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#33016;&#24452;&#20272;&#31639;&#19978;&#30340;&#20934;&#30830;&#29575;&#35823;&#24046;&#29575;&#23567;&#20110;2.5%&#65292;&#26377;&#26395;&#22312;&#22478;&#24066;&#34903;&#36947;&#26641;&#26408;&#28165;&#26597;&#26041;&#38754;&#20135;&#29983;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26862;&#26519;&#30733;&#20240;&#26159;&#27668;&#20505;&#21464;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#20250;&#23548;&#33268;&#20892;&#19994;&#37096;&#38376;&#30340;&#20013;&#26029;&#12289;&#20840;&#29699;&#21464;&#26262;&#12289;&#23665;&#27946;&#26292;&#21457;&#21644;&#28369;&#22369;&#31561;&#19981;&#21033;&#21518;&#26524;&#12290;&#20256;&#32479;&#30340;&#22478;&#24066;&#34903;&#36947;&#26641;&#26408;&#28165;&#26597;&#26041;&#27861;&#23384;&#22312;&#31934;&#24230;&#19981;&#20934;&#30830;&#21644;&#23545;&#19987;&#19994;&#35774;&#22791;&#30340;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#25163;&#26426;&#24433;&#20687;&#36827;&#34892;&#22478;&#24066;&#34903;&#36947;&#26641;&#26408;&#28165;&#26597;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26234;&#33021;&#25163;&#26426;&#25668;&#20687;&#22836;&#25293;&#25668;&#30340;&#19968;&#23545;&#22270;&#20687;&#26469;&#31934;&#30830;&#22320;&#20998;&#21106;&#26641;&#24178;&#24182;&#35745;&#31639;&#33016;&#24452;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20197;&#19979;&#20960;&#20010;&#20248;&#28857;&#65292;&#21253;&#25324;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#23545;&#19987;&#19994;&#35774;&#22791;&#30340;&#20381;&#36182;&#36739;&#20302;&#21644;&#36866;&#29992;&#20110;&#38590;&#20197;&#25509;&#35302;&#30340;&#22320;&#21306;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;400&#39063;&#26641;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#33016;&#24452;&#20272;&#31639;&#20934;&#30830;&#29575;&#26041;&#38754;&#36798;&#21040;&#20102;&#23567;&#20110;2.5%&#30340;&#35823;&#24046;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#21487;&#20197;&#22823;&#24133;&#24230;&#25913;&#21892;&#22478;&#24066;&#34903;&#36947;&#26641;&#26408;&#28165;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deforestation, a major contributor to climate change, poses detrimental consequences such as agricultural sector disruption, global warming, flash floods, and landslides. Conventional approaches to urban street tree inventory suffer from inaccuracies and necessitate specialised equipment. To overcome these challenges, this paper proposes an innovative method that leverages deep learning techniques and mobile phone imaging for urban street tree inventory. Our approach utilises a pair of images captured by smartphone cameras to accurately segment tree trunks and compute the diameter at breast height (DBH). Compared to traditional methods, our approach exhibits several advantages, including superior accuracy, reduced dependency on specialised equipment, and applicability in hard-to-reach areas. We evaluated our method on a comprehensive dataset of 400 trees and achieved a DBH estimation accuracy with an error rate of less than 2.5%. Our method holds significant potential for substantially
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20923;&#32467;&#20027;&#24178;&#30340;&#36866;&#37197;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#25239;&#24178;&#25200;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#20445;&#30041;&#20449;&#24687;&#30340;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#24182;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01179</link><description>&lt;p&gt;
&#20923;&#32467;&#20027;&#24178;&#65306;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#25239;&#24178;&#25200;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#27604;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Freeze the backbones: A Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-training. (arXiv:2401.01179v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01179
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20923;&#32467;&#20027;&#24178;&#30340;&#36866;&#37197;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#25239;&#24178;&#25200;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#20445;&#30041;&#20449;&#24687;&#30340;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#24182;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#21307;&#30103;&#24120;&#24120;&#22312;&#35786;&#26029;&#20013;&#21516;&#26102;&#20351;&#29992;&#25918;&#23556;&#22270;&#20687;&#21644;&#25991;&#23383;&#25253;&#21578;&#65292;&#40723;&#21169;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#35270;&#35273;-&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;(VL-SSL)&#20197;&#23398;&#20064;&#22810;&#21151;&#33021;&#30340;&#21307;&#23398;&#35270;&#35273;&#34920;&#24449;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;VL-SSL&#26694;&#26550;&#26159;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#65292;&#36825;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#20002;&#22833;&#23884;&#20837;&#22312;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#30340;&#37325;&#35201;&#20808;&#39564;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26080;&#20027;&#24178;&#36866;&#37197;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#20445;&#25345;&#39044;&#35757;&#32451;&#22270;&#20687;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#20923;&#32467;&#29366;&#24577;&#65292;&#20445;&#30041;&#21307;&#23398;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#27169;&#22359;&#36827;&#34892;&#36328;&#27169;&#24577;&#23398;&#20064;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20445;&#30041;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#21487;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#20943;&#23569;&#36229;&#36807;90%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#21482;&#20351;&#29992;1%&#30340;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#36866;&#37197;&#22120;&#30340;&#24615;&#33021;&#20248;&#20110;&#20960;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern healthcare often utilises radiographic images alongside textual reports for diagnostics, encouraging the use of Vision-Language Self-Supervised Learning (VL-SSL) with large pre-trained models to learn versatile medical vision representations. However, most existing VL-SSL frameworks are trained end-to-end, which is computation-heavy and can lose vital prior information embedded in pre-trained encoders. To address both issues, we introduce the backbone-agnostic Adaptor framework, which preserves medical knowledge in pre-trained image and text encoders by keeping them frozen, and employs a lightweight Adaptor module for cross-modal learning. Experiments on medical image classification and segmentation tasks across three datasets reveal that our framework delivers competitive performance while cutting trainable parameters by over 90% compared to current pre-training approaches. Notably, when fine-tuned with just 1% of data, Adaptor outperforms several Transformer-based methods trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#35786;&#26029;&#24102;&#26377;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#36724;&#25215;&#25925;&#38556;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.01172</link><description>&lt;p&gt;
&#25391;&#21160;&#20449;&#21495;&#30340;&#20108;&#27425;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#29992;&#20110;&#35786;&#26029;&#36724;&#25215;&#25925;&#38556;
&lt;/p&gt;
&lt;p&gt;
Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing Bearing Faults. (arXiv:2401.01172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#35786;&#26029;&#24102;&#26377;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#36724;&#25215;&#25925;&#38556;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36724;&#25215;&#25925;&#38556;&#30340;&#35786;&#26029;&#23545;&#20110;&#38477;&#20302;&#32500;&#20462;&#25104;&#26412;&#21644;&#35774;&#22791;&#20572;&#26426;&#33267;&#20851;&#37325;&#35201;&#12290;&#36724;&#25215;&#25925;&#38556;&#26159;&#26426;&#22120;&#25391;&#21160;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#20998;&#26512;&#20854;&#20449;&#21495;&#24418;&#24577;&#21487;&#20197;&#25581;&#31034;&#20854;&#20581;&#24247;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#25511;&#21046;&#29615;&#22659;&#36827;&#34892;&#20248;&#21270;&#65292;&#24573;&#30053;&#20102;&#23454;&#38469;&#26465;&#20214;&#19979;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#36716;&#36895;&#21644;&#25391;&#21160;&#30340;&#38750;&#24179;&#31283;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#19979;&#35786;&#26029;&#36724;&#25215;&#25925;&#38556;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#36724;&#25215;&#25925;&#38556;&#24341;&#36215;&#30340;&#25391;&#21160;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#38750;&#24179;&#31283;&#24615;&#19982;&#36724;&#25215;&#22266;&#26377;&#21644;&#25805;&#20316;&#21442;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#38416;&#36848;&#20102;&#20108;&#27425;&#26102;&#38388;&#39057;&#29575;&#20998;&#24067;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38388;&#39057;&#29575;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosis of bearing faults is paramount to reducing maintenance costs and operational breakdowns. Bearing faults are primary contributors to machine vibrations, and analyzing their signal morphology offers insights into their health status. Unfortunately, existing approaches are optimized for controlled environments, neglecting realistic conditions such as time-varying rotational speeds and the vibration's non-stationary nature. This paper presents a fusion of time-frequency analysis and deep learning techniques to diagnose bearing faults under time-varying speeds and varying noise levels. First, we formulate the bearing fault-induced vibrations and discuss the link between their non-stationarity and the bearing's inherent and operational parameters. We also elucidate quadratic time-frequency distributions and validate their effectiveness in resolving distinctive dynamic patterns associated with different bearing faults. Based on this, we design a time-frequency convolutional neural n
&lt;/p&gt;</description></item><item><title>Spiker+&#26159;&#19968;&#20010;&#22312;&#36793;&#32536;&#36827;&#34892;&#25512;&#29702;&#30340;&#39640;&#25928;Spiking&#31070;&#32463;&#32593;&#32476;FPGA&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;&#65292;&#20855;&#26377;&#20302;&#36164;&#28304;&#28040;&#32791;&#21644;&#20302;&#21151;&#32791;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.01141</link><description>&lt;p&gt;
Spiker&#65291;&#65306;&#19968;&#31181;&#29992;&#20110;&#22312;&#36793;&#32536;&#36827;&#34892;&#25512;&#29702;&#30340;&#39640;&#25928;Spiking&#31070;&#32463;&#32593;&#32476;FPGA&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Spiker+: a framework for the generation of efficient Spiking Neural Networks FPGA accelerators for inference at the edge. (arXiv:2401.01141v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01141
&lt;/p&gt;
&lt;p&gt;
Spiker+&#26159;&#19968;&#20010;&#22312;&#36793;&#32536;&#36827;&#34892;&#25512;&#29702;&#30340;&#39640;&#25928;Spiking&#31070;&#32463;&#32593;&#32476;FPGA&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;&#65292;&#20855;&#26377;&#20302;&#36164;&#28304;&#28040;&#32791;&#21644;&#20302;&#21151;&#32791;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#21253;&#21547;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20351;&#24212;&#29992;&#31243;&#24207;&#22312;&#32593;&#32476;&#36793;&#32536;&#35774;&#22791;&#20013;&#30452;&#25509;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Spiker&#65291;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#36793;&#32536;&#36827;&#34892;&#25512;&#29702;&#30340;&#39640;&#25928;&#12289;&#20302;&#21151;&#32791;&#12289;&#20302;&#38754;&#31215;&#30340;&#23450;&#21046;Spiking&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;FPGA&#21152;&#36895;&#22120;&#30340;&#32508;&#21512;&#26694;&#26550;&#12290;Spiker&#65291;&#25552;&#20379;&#20102;&#21487;&#37197;&#32622;&#30340;&#22810;&#23618;&#30828;&#20214;SNN&#12289;&#39640;&#25928;&#30340;&#31070;&#32463;&#20803;&#26550;&#26500;&#24211;&#20197;&#21450;&#19968;&#20010;&#35774;&#35745;&#26694;&#26550;&#65292;&#20351;&#24471;&#21487;&#20197;&#29992;&#23569;&#37327;Python&#20195;&#30721;&#24320;&#21457;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#12290;Spiker&#65291;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;MNIST&#21644;Spiking Heidelberg Digits&#65288;SHD&#65289;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#22312;MNIST&#19978;&#65292;&#23427;&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#30340;SNN&#21152;&#36895;&#22120;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#23427;&#22312;&#36164;&#28304;&#20998;&#37197;&#26041;&#38754;&#20248;&#20110;&#23427;&#20204;&#65292;&#38656;&#35201;7612&#20010;&#36923;&#36753;&#21333;&#20803;&#21644;18&#20010;Block RAM&#65288;BRAM&#65289;&#65292;&#21487;&#20197;&#36866;&#24212;&#38750;&#24120;&#23567;&#30340;FPGA&#65292;&#24182;&#19988;&#21151;&#32791;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Including Artificial Neural Networks in embedded systems at the edge allows applications to exploit Artificial Intelligence capabilities directly within devices operating at the network periphery. This paper introduces Spiker+, a comprehensive framework for generating efficient, low-power, and low-area customized Spiking Neural Networks (SNN) accelerators on FPGA for inference at the edge. Spiker+ presents a configurable multi-layer hardware SNN, a library of highly efficient neuron architectures, and a design framework, enabling the development of complex neural network accelerators with few lines of Python code. Spiker+ is tested on two benchmark datasets, the MNIST and the Spiking Heidelberg Digits (SHD). On the MNIST, it demonstrates competitive performance compared to state-of-the-art SNN accelerators. It outperforms them in terms of resource allocation, with a requirement of 7,612 logic cells and 18 Block RAMs (BRAMs), which makes it fit in very small FPGA, and power consumption,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22312;&#32447;&#36873;&#25321;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;TreeSHAP&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#30340;&#19987;&#38376;&#21270;&#65292;&#20197;&#24212;&#23545;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#23454;&#38469;&#20915;&#31574;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01124</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#33258;&#36866;&#24212;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#36873;&#25321;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Adaptive Tree-based Model Selection for Time Series Forecasting. (arXiv:2401.01124v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01124
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22312;&#32447;&#36873;&#25321;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;TreeSHAP&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#30340;&#19987;&#38376;&#21270;&#65292;&#20197;&#24212;&#23545;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#23454;&#38469;&#20915;&#31574;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#30001;&#20110;&#20854;&#30456;&#23545;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#23427;&#20204;&#22312;&#38656;&#27714;&#21644;&#24191;&#27867;&#35748;&#21487;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27169;&#22411;&#37117;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22312;&#32447;&#39044;&#27979;&#35774;&#32622;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#26102;&#38388;&#24207;&#21015;&#35266;&#27979;&#19981;&#26029;&#22686;&#21152;&#65292;&#32780;&#23427;&#20204;&#25152;&#32472;&#21046;&#30340;&#20998;&#24067;&#21487;&#33021;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;TreeSHAP&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#32447;&#36873;&#25321;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#26469;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#20174;&#20219;&#24847;&#19968;&#32452;&#19981;&#21516;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;&#24615;&#33021;&#30340;&#25490;&#21517;&#65292;&#24182;&#20855;&#26377;&#19968;&#33268;&#30340;&#35774;&#35745;&#65292;&#20351;&#24471;TreeSHAP&#33021;&#22815;&#26681;&#25454;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#30340;&#19981;&#21516;&#21306;&#22495;&#26469;&#19987;&#38376;&#21270;&#22522;&#20110;&#26641;&#30340;&#39044;&#27979;&#22120;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#36866;&#24403;&#30340;&#27169;&#24335;...
&lt;/p&gt;
&lt;p&gt;
Tree-based models have been successfully applied to a wide variety of tasks, including time series forecasting. They are increasingly in demand and widely accepted because of their comparatively high level of interpretability. However, many of them suffer from the overfitting problem, which limits their application in real-world decision-making. This problem becomes even more severe in online-forecasting settings where time series observations are incrementally acquired, and the distributions from which they are drawn may keep changing over time. In this context, we propose a novel method for the online selection of tree-based models using the TreeSHAP explainability method in the task of time series forecasting. We start with an arbitrary set of different tree-based models. Then, we outline a performance-based ranking with a coherent design to make TreeSHAP able to specialize the tree-based forecasters across different regions in the input time series. In this framework, adequate mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;CVGAN&#27169;&#22411;&#29983;&#25104;&#28378;&#21160;&#36724;&#25215;&#25391;&#21160;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#21382;&#21490;&#25391;&#21160;&#25968;&#25454;&#21644;&#21097;&#20313;&#23551;&#21629;&#26465;&#20214;&#29983;&#25104;&#19968;&#32500;&#25391;&#21160;&#20449;&#21495;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#26469;&#25351;&#23548;&#20449;&#21495;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CVGAN&#27169;&#22411;&#22312;MMD&#21644;FID&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#39640;&#32423;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01119</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#22238;&#24402;&#32593;&#32476;&#36827;&#34892;&#28378;&#21160;&#36724;&#25215;&#20840;&#29983;&#21629;&#21608;&#26399;&#25968;&#25454;&#29983;&#25104;&#29992;&#20110;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Utilizing Autoregressive Networks for Full Lifecycle Data Generation of Rolling Bearings for RUL Prediction. (arXiv:2401.01119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;CVGAN&#27169;&#22411;&#29983;&#25104;&#28378;&#21160;&#36724;&#25215;&#25391;&#21160;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#21382;&#21490;&#25391;&#21160;&#25968;&#25454;&#21644;&#21097;&#20313;&#23551;&#21629;&#26465;&#20214;&#29983;&#25104;&#19968;&#32500;&#25391;&#21160;&#20449;&#21495;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#26469;&#25351;&#23548;&#20449;&#21495;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CVGAN&#27169;&#22411;&#22312;MMD&#21644;FID&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#39640;&#32423;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#29983;&#20135;&#20013;&#65292;&#28378;&#21160;&#36724;&#25215;&#23551;&#21629;&#30340;&#39044;&#27979;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#39640;&#36136;&#37327;&#30340;&#28378;&#21160;&#36724;&#25215;&#20840;&#29983;&#21629;&#21608;&#26399;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#19968;&#30452;&#26159;&#31934;&#30830;&#39044;&#27979;&#30340;&#20027;&#35201;&#21046;&#32422;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;CVGAN&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#27700;&#24179;&#21644;&#22402;&#30452;&#26041;&#21521;&#19978;&#30340;&#19968;&#32500;&#25391;&#21160;&#20449;&#21495;&#30340;&#26032;&#26694;&#26550;&#65292;&#20854;&#21463;&#21382;&#21490;&#25391;&#21160;&#25968;&#25454;&#21644;&#21097;&#20313;&#23551;&#21629;&#30340;&#26465;&#20214;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#36845;&#20195;&#22320;&#21033;&#29992;&#20043;&#21069;&#29983;&#25104;&#30340;&#25391;&#21160;&#20449;&#24687;&#26469;&#25351;&#23548;&#24403;&#21069;&#20449;&#21495;&#30340;&#29983;&#25104;&#12290;&#36890;&#36807;&#22312;PHM 2012&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;CVGAN&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CVGAN&#27169;&#22411;&#22312;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#24335;&#19979;&#65292;&#22312;MMD&#21644;FID&#25351;&#26631;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#35768;&#22810;&#39640;&#32423;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35757;&#32451;&#20351;&#29992;&#20102;&#30001;CVGAN&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#29983;&#21629;&#21608;&#26399;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of rolling bearing lifespan is of significant importance in industrial production. However, the scarcity of high-quality, full lifecycle data has been a major constraint in achieving precise predictions. To address this challenge, this paper introduces the CVGAN model, a novel framework capable of generating one-dimensional vibration signals in both horizontal and vertical directions, conditioned on historical vibration data and remaining useful life. In addition, we propose an autoregressive generation method that can iteratively utilize previously generated vibration information to guide the generation of current signals. The effectiveness of the CVGAN model is validated through experiments conducted on the PHM 2012 dataset. Our findings demonstrate that the CVGAN model, in terms of both MMD and FID metrics, outperforms many advanced methods in both autoregressive and non-autoregressive generation modes. Notably, training using the full lifecycle data generated by the 
&lt;/p&gt;</description></item><item><title>AI-FLARES&#26159;&#19968;&#39033;&#30740;&#31350;&#39033;&#30446;&#65292;&#36890;&#36807;&#24320;&#21457;&#21644;&#20351;&#29992;&#35745;&#31639;&#26041;&#27861;&#20998;&#26512;&#22826;&#38451;&#32768;&#26001;&#25968;&#25454;&#65292;&#25104;&#26524;&#21253;&#25324;&#32768;&#26001;&#39044;&#27979;&#12289;&#32768;&#26001;&#28304;&#24418;&#24577;&#37325;&#24314;&#21644;&#22826;&#38451;&#32768;&#26001;&#21152;&#36895;&#26426;&#21046;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.01104</link><description>&lt;p&gt;
AI-FLARES: &#29992;&#20110;&#22826;&#38451;&#32768;&#26001;&#25968;&#25454;&#20998;&#26512;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
AI-FLARES: Artificial Intelligence for the Analysis of Solar Flares Data. (arXiv:2401.01104v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01104
&lt;/p&gt;
&lt;p&gt;
AI-FLARES&#26159;&#19968;&#39033;&#30740;&#31350;&#39033;&#30446;&#65292;&#36890;&#36807;&#24320;&#21457;&#21644;&#20351;&#29992;&#35745;&#31639;&#26041;&#27861;&#20998;&#26512;&#22826;&#38451;&#32768;&#26001;&#25968;&#25454;&#65292;&#25104;&#26524;&#21253;&#25324;&#32768;&#26001;&#39044;&#27979;&#12289;&#32768;&#26001;&#28304;&#24418;&#24577;&#37325;&#24314;&#21644;&#22826;&#38451;&#32768;&#26001;&#21152;&#36895;&#26426;&#21046;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI-FLARES&#65288;&#29992;&#20110;&#22826;&#38451;&#32768;&#26001;&#25968;&#25454;&#20998;&#26512;&#30340;&#20154;&#24037;&#26234;&#33021;&#65289;&#26159;&#30001;&#24847;&#22823;&#21033;&#22826;&#31354;&#23616;&#21644;&#24847;&#22823;&#21033;&#22269;&#23478;&#22825;&#25991;&#29289;&#29702;&#30740;&#31350;&#25152;&#36164;&#21161;&#30340;&#30740;&#31350;&#39033;&#30446;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#20351;&#29992;&#35745;&#31639;&#26041;&#27861;&#20998;&#26512;&#19982;&#22826;&#38451;&#32768;&#26001;&#21457;&#23556;&#30456;&#20851;&#30340;&#36828;&#31243;&#24863;&#27979;&#31354;&#38388;&#25968;&#25454;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#35813;&#39033;&#30446;&#21462;&#24471;&#30340;&#20027;&#35201;&#25104;&#26524;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#12289;&#32768;&#26001;&#28304;&#24418;&#24577;&#37325;&#24314;&#21644;&#22826;&#38451;&#32768;&#26001;&#35302;&#21457;&#30340;&#21152;&#36895;&#26426;&#21046;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-FLARES (Artificial Intelligence for the Analysis of Solar Flares Data) is a research project funded by the Agenzia Spaziale Italiana and by the Istituto Nazionale di Astrofisica within the framework of the ``Attivit\`a di Studio per la Comunit\`a Scientifica Nazionale Sole, Sistema Solare ed Esopianeti'' program. The topic addressed by this project was the development and use of computational methods for the analysis of remote sensing space data associated to solar flare emission. This paper overviews the main results obtained by the project, with specific focus on solar flare forecasting, reconstruction of morphologies of the flaring sources, and interpretation of acceleration mechanisms triggered by solar flares.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32452;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21644;&#32452;&#36845;&#20195;&#24182;&#34892;&#35299;&#30721;&#65292;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#65292;&#24182;&#25104;&#21151;&#25429;&#25417;&#25552;&#31034;&#35821;&#38899;&#30340;&#35828;&#35805;&#20154;&#39118;&#26684;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01099</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#26041;&#27861;&#65306;&#20351;&#29992;&#32452;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Parallel Audio Generation using Group Masked Language Modeling. (arXiv:2401.01099v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01099
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32452;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21644;&#32452;&#36845;&#20195;&#24182;&#34892;&#35299;&#30721;&#65292;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#65292;&#24182;&#25104;&#21151;&#25429;&#25417;&#25552;&#31034;&#35821;&#38899;&#30340;&#35828;&#35805;&#20154;&#39118;&#26684;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#32534;&#35299;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#12290;&#34429;&#28982;SoundStorm&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#30456;&#27604;&#33258;&#22238;&#24402;&#27169;&#22411;&#21152;&#36895;&#20102;&#25512;&#29702;&#36895;&#24230;&#65292;&#20294;&#30001;&#20110;&#36845;&#20195;&#37319;&#26679;&#65292;&#23427;&#20173;&#28982;&#21463;&#21040;&#25512;&#29702;&#36895;&#24230;&#24930;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32452;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;G-MLM&#65289;&#21644;&#32452;&#36845;&#20195;&#24182;&#34892;&#35299;&#30721;&#65288;G-IPD&#65289;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#12290;&#35757;&#32451;&#21644;&#37319;&#26679;&#26041;&#26696;&#37117;&#33021;&#22815;&#36890;&#36807;&#26377;&#25928;&#24314;&#27169;&#32452;&#38388;&#26465;&#20214;&#20381;&#36182;&#26469;&#22312;&#23569;&#25968;&#36845;&#20195;&#27425;&#25968;&#20869;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#20197;&#25429;&#25417;&#25552;&#31034;&#35821;&#38899;&#30340;&#35828;&#35805;&#20154;&#39118;&#26684;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a fast and high-quality codec language model for parallel audio generation. While SoundStorm, a state-of-the-art parallel audio generation model, accelerates inference speed compared to autoregressive models, it still suffers from slow inference due to iterative sampling. To resolve this problem, we propose Group-Masked Language Modeling~(G-MLM) and Group Iterative Parallel Decoding~(G-IPD) for efficient parallel audio generation. Both the training and sampling schemes enable the model to synthesize high-quality audio with a small number of iterations by effectively modeling the group-wise conditional dependencies. In addition, our model employs a cross-attention-based architecture to capture the speaker style of the prompt voice and improves computational efficiency. Experimental results demonstrate that our proposed model outperforms the baselines in prompt-based audio generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Quokka&#8212;&#8212;&#19968;&#20010;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#23545;&#36229;&#36807;&#19968;&#30334;&#19975;&#31687;&#39046;&#22495;&#29305;&#23450;&#30340;&#35770;&#25991;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#26597;&#35810;&#20013;&#25552;&#20379;&#21363;&#26102;&#30340;&#19978;&#19979;&#25991;&#24847;&#35782;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2401.01089</link><description>&lt;p&gt;
Quokka: &#19968;&#20010;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Quokka: An Open-source Large Language Model ChatBot for Material Science. (arXiv:2401.01089v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Quokka&#8212;&#8212;&#19968;&#20010;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#23545;&#36229;&#36807;&#19968;&#30334;&#19975;&#31687;&#39046;&#22495;&#29305;&#23450;&#30340;&#35770;&#25991;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#26597;&#35810;&#20013;&#25552;&#20379;&#21363;&#26102;&#30340;&#19978;&#19979;&#25991;&#24847;&#35782;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26448;&#26009;&#31185;&#23398;&#30340;&#19987;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24320;&#21457;&#65292;&#21033;&#29992;&#20102;Llama-2&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;S2ORC&#25968;&#25454;&#38598;&#20013;&#30340;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#24191;&#27867;&#30740;&#31350;&#25991;&#31456;&#19978;&#36827;&#34892;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#12290;&#26041;&#27861;&#21253;&#25324;&#39318;&#20808;&#22312;&#36229;&#36807;&#19968;&#30334;&#19975;&#31687;&#39046;&#22495;&#29305;&#23450;&#30340;&#35770;&#25991;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#36807;&#31243;&#26469;&#25913;&#21892;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#12290;&#35813;&#32842;&#22825;&#26426;&#22120;&#20154;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#21363;&#26102;&#30340;&#12289;&#20855;&#26377;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#26597;&#35810;&#21709;&#24212;&#65292;&#26469;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#12289;&#25945;&#24072;&#21644;&#23398;&#29983;&#12290;&#25105;&#20204;&#23558;&#22235;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#26816;&#26597;&#28857;&#65288;7B&#12289;13B&#65292;&#24102;&#25110;&#19981;&#24102;&#32842;&#22825;&#21151;&#33021;&#65289;&#20813;&#36153;&#25552;&#20379;&#32473;&#30740;&#31350;&#30028;&#65292;&#32593;&#22336;&#20026;https://github.com/Xianjun-Yang/Quokka&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the development of a specialized chatbot for materials science, leveraging the Llama-2 language model, and continuing pre-training on the expansive research articles in the materials science domain from the S2ORC dataset. The methodology involves an initial pretraining phase on over one million domain-specific papers, followed by an instruction-tuning process to refine the chatbot's capabilities. The chatbot is designed to assist researchers, educators, and students by providing instant, context-aware responses to queries in the field of materials science. We make the four trained checkpoints (7B, 13B, with or without chat ability) freely available to the research community at https://github.com/Xianjun-Yang/Quokka.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.01078</link><description>&lt;p&gt;
&#36234;&#21335;&#35799;&#27468;&#29983;&#25104;&#19982;&#36328;&#35821;&#35328;&#35799;&#27468;&#32763;&#35793;&#30340;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Vietnamese Poem Generation &amp; The Prospect Of Cross-Language Poem-To-Poem Translation. (arXiv:2401.01078v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35799;&#27468;&#29983;&#25104;&#19968;&#30452;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#39033;&#25361;&#25112;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#27169;&#22411;&#29702;&#35299;&#35821;&#35328;&#12289;&#24773;&#24863;&#21644;&#39118;&#26684;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#65292;&#20174;&#32780;&#23454;&#29616;&#30452;&#35266;&#30340;&#36807;&#31243;&#21644;&#22686;&#24378;&#30340;&#20869;&#23481;&#25511;&#21046;&#12290;&#25105;&#20204;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;GPT-3 Babbage&#21464;&#31181;&#65292;&#22312;&#36234;&#21335;&#35799;&#27468;&#30340;&#8220;&#20845;&#20843;&#35789;&#8221;&#31867;&#22411;&#20013;&#23454;&#29616;&#20102;0.8&#30340;&#33258;&#23450;&#20041;&#35780;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#25913;&#20889;&#25104;&#27491;&#24120;&#25991;&#26412;&#25552;&#31034;&#30340;&#24819;&#27861;&#65292;&#24182;&#22312;&#8220;&#20845;&#20843;&#35789;&#8221;&#31867;&#22411;&#20013;&#33719;&#24471;&#20102;&#30456;&#23545;&#36739;&#39640;&#30340;0.718&#20998;&#25968;&#12290;&#36825;&#20010;&#23454;&#39564;&#23637;&#31034;&#20102;&#20197;&#32763;&#35793;&#21518;&#30340;&#35799;&#27468;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#36328;&#35821;&#35328;&#35799;&#27468;&#32763;&#35793;&#30340;&#28508;&#21147;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poetry generation has been a challenging task in the field of Natural Language Processing, as it requires the model to understand the nuances of language, sentiment, and style. In this paper, we propose using Large Language Models to generate Vietnamese poems from natural language prompts, thereby facilitating an intuitive process with enhanced content control. Our most efficacious model, the GPT-3 Babbage variant, achieves a custom evaluation score of 0.8, specifically tailored to the "luc bat" genre of Vietnamese poetry. Furthermore, we also explore the idea of paraphrasing poems into normal text prompts and yield a relatively high score of 0.718 in the "luc bat" genre. This experiment presents the potential for cross-Language poem-to-poem translation with translated poems as the inputs while concurrently maintaining complete control over the generated content.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#36890;&#36807;&#20027;&#39064;&#27169;&#22411;&#21644;&#32479;&#35745;&#20998;&#26512;&#26041;&#27861;&#20174;&#27861;&#24459;&#35009;&#20915;&#25991;&#26412;&#20013;&#21457;&#29616;&#37325;&#35201;&#20027;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#19982;&#32467;&#26524;&#30456;&#20851;&#30340;&#26696;&#20363;&#20027;&#39064;&#65292;&#24182;&#36890;&#36807;&#20027;&#39064;-&#35789;&#20998;&#24067;&#21644;&#26696;&#20363;-&#20027;&#39064;&#26435;&#37325;&#26469;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01068</link><description>&lt;p&gt;
&#20174;&#27861;&#24459;&#35009;&#20915;&#20013;&#21457;&#29616;&#37325;&#35201;&#20027;&#39064;&#30340;&#36873;&#25321;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discovering Significant Topics from Legal Decisions with Selective Inference. (arXiv:2401.01068v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#36890;&#36807;&#20027;&#39064;&#27169;&#22411;&#21644;&#32479;&#35745;&#20998;&#26512;&#26041;&#27861;&#20174;&#27861;&#24459;&#35009;&#20915;&#25991;&#26412;&#20013;&#21457;&#29616;&#37325;&#35201;&#20027;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#19982;&#32467;&#26524;&#30456;&#20851;&#30340;&#26696;&#20363;&#20027;&#39064;&#65292;&#24182;&#36890;&#36807;&#20027;&#39064;-&#35789;&#20998;&#24067;&#21644;&#26696;&#20363;-&#20027;&#39064;&#26435;&#37325;&#26469;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#36890;&#36807;&#23558;&#20027;&#39064;&#27169;&#22411;&#21512;&#25104;&#30340;&#29305;&#24449;&#36890;&#36807;&#21463;&#24809;&#32602;&#30340;&#22238;&#24402;&#21644;&#21518;&#36873;&#25321;&#26174;&#33879;&#24615;&#26816;&#39564;&#26469;&#21457;&#29616;&#27861;&#24459;&#35009;&#20915;&#25991;&#26412;&#20013;&#30340;&#37325;&#35201;&#20027;&#39064;&#12290;&#35813;&#26041;&#27861;&#35782;&#21035;&#20986;&#19982;&#32467;&#26524;&#26174;&#33879;&#30456;&#20851;&#30340;&#26696;&#20363;&#20027;&#39064;&#65292;&#21487;&#20197;&#25163;&#21160;&#35299;&#37322;&#20197;&#33719;&#21462;&#26377;&#20851;&#37325;&#35201;&#20027;&#39064;&#30340;&#35265;&#35299;&#30340;&#20027;&#39064;-&#35789;&#20998;&#24067;&#65292;&#20197;&#21450;&#21487;&#20197;&#29992;&#20110;&#26631;&#35782;&#27599;&#20010;&#20027;&#39064;&#30340;&#20195;&#34920;&#24615;&#26696;&#20363;&#30340;&#26696;&#20363;-&#20027;&#39064;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22495;&#21517;&#20105;&#35758;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#36829;&#35268;&#26696;&#20363;&#30340;&#32463;&#20856;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#12290;&#22522;&#20110;&#28508;&#22312;&#35821;&#20041;&#20998;&#26512;&#21644;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#30340;&#20027;&#39064;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#27969;&#31243;&#25512;&#23548;&#30340;&#20027;&#39064;&#22312;&#20004;&#20010;&#39046;&#22495;&#20013;&#19982;&#27861;&#24459;&#25945;&#26465;&#19968;&#33268;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20854;&#20182;&#30456;&#20851;&#27861;&#24459;&#20998;&#26512;&#20219;&#21153;&#20013;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose and evaluate an automated pipeline for discovering significant topics from legal decision texts by passing features synthesized with topic models through penalised regressions and post-selection significance tests. The method identifies case topics significantly correlated with outcomes, topic-word distributions which can be manually-interpreted to gain insights about significant topics, and case-topic weights which can be used to identify representative cases for each topic. We demonstrate the method on a new dataset of domain name disputes and a canonical dataset of European Court of Human Rights violation cases. Topic models based on latent semantic analysis as well as language model embeddings are evaluated. We show that topics derived by the pipeline are consistent with legal doctrines in both areas and can be useful in other related legal analysis tasks.
&lt;/p&gt;</description></item><item><title>BEV-CLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#22797;&#26434;&#22330;&#26223;&#30340;&#22810;&#27169;&#24577;BEV&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25551;&#36848;&#24615;&#25991;&#26412;&#36827;&#34892;&#26816;&#32034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340; semantic feature extraction &#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#21322;&#32467;&#26500;&#21270;&#20449;&#24687;&#26469;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01065</link><description>&lt;p&gt;
BEV-CLIP&#65306;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#22797;&#26434;&#22330;&#26223;&#30340;&#22810;&#27169;&#24577;BEV&#26816;&#32034;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving. (arXiv:2401.01065v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01065
&lt;/p&gt;
&lt;p&gt;
BEV-CLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#22797;&#26434;&#22330;&#26223;&#30340;&#22810;&#27169;&#24577;BEV&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25551;&#36848;&#24615;&#25991;&#26412;&#36827;&#34892;&#26816;&#32034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340; semantic feature extraction &#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#21322;&#32467;&#26500;&#21270;&#20449;&#24687;&#26469;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20056;&#29992;&#36710;&#36742;&#20855;&#22791;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#33021;&#21147;&#65292;&#33258;&#21160;&#39550;&#39542;&#20013;&#23545;&#22797;&#26434;&#22330;&#26223;&#25968;&#25454;&#30340;&#26816;&#32034;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#38271;&#23614;&#24773;&#26223;&#26102;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#30340;&#20108;&#32500;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#19979;&#65292;&#23384;&#22312;&#19968;&#20123;&#22330;&#26223;&#26816;&#32034;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#20840;&#23616;&#29305;&#24449;&#34920;&#24449;&#19981;&#36275;&#21644;&#25991;&#26412;&#26816;&#32034;&#33021;&#21147;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BEV-CLIP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#25551;&#36848;&#24615;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#26469;&#26816;&#32034;&#30456;&#24212;&#22330;&#26223;&#30340;&#22810;&#27169;&#24577;&#40479;&#30640;&#22270;&#26816;&#32034;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23545;&#24191;&#27867;&#25991;&#26412;&#25551;&#36848;&#30340;&#38646;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#30340;&#21322;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#23884;&#20837;&#30340;&#35821;&#20041;&#20016;&#23500;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;NuScenes&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;87.66%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for the retrieval of complex scene data in autonomous driving is increasing, especially as passenger vehicles have been equipped with the ability to navigate urban settings, with the imperative to address long-tail scenarios. Meanwhile, under the pre-existing two dimensional image retrieval method, some problems may arise with scene retrieval, such as lack of global feature representation and subpar text retrieval ability. To address these issues, we have proposed \textbf{BEV-CLIP}, the first multimodal Bird's-Eye View(BEV) retrieval methodology that utilizes descriptive text as an input to retrieve corresponding scenes. This methodology applies the semantic feature extraction abilities of a large language model (LLM) to facilitate zero-shot retrieval of extensive text descriptions, and incorporates semi-structured information from a knowledge graph to improve the semantic richness and variety of the language embedding. Our experiments result in 87.66% accuracy on NuScenes d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TLDNN&#30340;&#28151;&#21512;&#28145;&#24230;&#26694;&#26550;&#65292;&#23558;Transformer&#21644;LSTM&#30340;&#32467;&#26500;&#32467;&#21512;&#65292;&#36890;&#36807;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#21644;&#25429;&#25417;&#26102;&#22495;&#20381;&#36182;&#24615;&#30340;&#22686;&#24378;&#65292;&#20026;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#24102;&#26469;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.01056</link><description>&lt;p&gt;
&#36890;&#36807;&#40065;&#26834;&#30340;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#22686;&#24378;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Enhancing Automatic Modulation Recognition through Robust Global Feature Extraction. (arXiv:2401.01056v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01056
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TLDNN&#30340;&#28151;&#21512;&#28145;&#24230;&#26694;&#26550;&#65292;&#23558;Transformer&#21644;LSTM&#30340;&#32467;&#26500;&#32467;&#21512;&#65292;&#36890;&#36807;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#21644;&#25429;&#25417;&#26102;&#22495;&#20381;&#36182;&#24615;&#30340;&#22686;&#24378;&#65292;&#20026;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#24102;&#26469;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#31574;&#30053;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#35843;&#21046;&#20449;&#21495;&#23637;&#31034;&#20102;&#38271;&#26399;&#30340;&#26102;&#22495;&#20381;&#36182;&#24615;&#65292;&#25552;&#21462;&#20840;&#23616;&#29305;&#24449;&#23545;&#20110;&#35782;&#21035;&#35843;&#21046;&#26041;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#19978;&#65292;&#20154;&#24037;&#19987;&#23478;&#20998;&#26512;&#26143;&#24231;&#22270;&#20013;&#30340;&#27169;&#24335;&#26469;&#20998;&#31867;&#35843;&#21046;&#26041;&#26696;&#12290;&#30001;&#20110;&#26377;&#38480;&#30340;&#24863;&#21463;&#37326;&#65292;&#20256;&#32479;&#30340;&#21367;&#31215;&#32593;&#32476;&#25797;&#38271;&#25552;&#21462;&#23616;&#37096;&#29305;&#24449;&#65292;&#20294;&#38590;&#20197;&#25429;&#25417;&#20840;&#23616;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;TLDNN&#30340;&#26032;&#22411;&#28151;&#21512;&#28145;&#24230;&#26694;&#26550;&#65292;&#23427;&#23558;Transformer&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#32467;&#26500;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#21033;&#29992;Transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#27169;&#25311;&#20449;&#21495;&#24207;&#21015;&#20013;&#30340;&#20840;&#23616;&#20851;&#32852;&#65292;&#21516;&#26102;&#21033;&#29992;LSTM&#22686;&#24378;&#26102;&#22495;&#20381;&#36182;&#24615;&#30340;&#25429;&#25417;&#12290;&#20026;&#20102;&#20943;&#36731;&#23556;&#39057;&#25351;&#32441;&#29305;&#24449;&#21644;&#20449;&#36947;&#29305;&#24615;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Modulation Recognition (AMR) plays a crucial role in wireless communication systems. Deep learning AMR strategies have achieved tremendous success in recent years. Modulated signals exhibit long temporal dependencies, and extracting global features is crucial in identifying modulation schemes. Traditionally, human experts analyze patterns in constellation diagrams to classify modulation schemes. Classical convolutional-based networks, due to their limited receptive fields, excel at extracting local features but struggle to capture global relationships. To address this limitation, we introduce a novel hybrid deep framework named TLDNN, which incorporates the architectures of the transformer and long short-term memory (LSTM). We utilize the self-attention mechanism of the transformer to model the global correlations in signal sequences while employing LSTM to enhance the capture of temporal dependencies. To mitigate the impact like RF fingerprint features and channel characteri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA&#36229;&#36234;&#33521;&#35821;&#65306;&#35821;&#35328;&#33021;&#21147;&#36716;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;LLaMA&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#20998;&#26512;&#20102;&#35789;&#27719;&#25193;&#23637;&#12289;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#31561;&#20851;&#38190;&#22240;&#32032;&#23545;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#33021;&#21147;&#36716;&#31227;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22235;&#20010;&#26631;&#20934;&#21270;&#27979;&#35797;&#22522;&#20934;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#21644;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.01055</link><description>&lt;p&gt;
LLaMA&#36229;&#36234;&#33521;&#35821;&#65306;&#35821;&#35328;&#33021;&#21147;&#36716;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLaMA Beyond English: An Empirical Study on Language Capability Transfer. (arXiv:2401.01055v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA&#36229;&#36234;&#33521;&#35821;&#65306;&#35821;&#35328;&#33021;&#21147;&#36716;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;LLaMA&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#20998;&#26512;&#20102;&#35789;&#27719;&#25193;&#23637;&#12289;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#31561;&#20851;&#38190;&#22240;&#32032;&#23545;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#33021;&#21147;&#36716;&#31227;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22235;&#20010;&#26631;&#20934;&#21270;&#27979;&#35797;&#22522;&#20934;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#21644;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#29087;&#32451;&#24230;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20027;&#27969;&#30340;LLM&#65288;&#22914;LLaMA&#65289;&#26159;&#22522;&#20110;&#20197;&#33521;&#35821;&#20026;&#20027;&#23548;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20854;&#20182;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#35821;&#35328;&#29983;&#25104;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#33021;&#21147;&#36716;&#31227;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;LLaMA&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#24635;&#35745;&#32791;&#36153;&#20102;1440&#20010;GPU&#23567;&#26102;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35832;&#22914;&#35789;&#27719;&#25193;&#23637;&#12289;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#31561;&#20851;&#38190;&#22240;&#32032;&#23545;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22235;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#26631;&#20934;&#21270;&#27979;&#35797;&#22522;&#20934;&#65306;C-Eval&#12289;MMLU&#12289;AGI-Eval&#21644;GAOKAO-Bench&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#27169;&#22411;&#30340;&#21709;&#24212;&#36136;&#37327;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#35832;&#22914;...
&lt;/p&gt;
&lt;p&gt;
In recent times, substantial advancements have been witnessed in large language models (LLMs), exemplified by ChatGPT, showcasing remarkable proficiency across a range of complex tasks. However, many mainstream LLMs (e.g. LLaMA) are pretrained on English-dominant corpus, which limits their performance in other non-English languages. In this paper, we focus on how to effectively transfer the capabilities of language generation and following instructions to a non-English language. To answer this question, we conduct an extensive empirical investigation based on LLaMA, accumulating over 1440 GPU hours. We analyze the impact of key factors such as vocabulary extension, further pretraining, and instruction tuning on transfer. To accurately assess the model's level of knowledge, we employ four widely used standardized testing benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a comprehensive evaluation of the model's response quality is conducted, considering aspects such as 
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#35770;&#25991;&#65292;&#20171;&#32461;&#20102;&#22312;&#21160;&#24577;&#22810;&#20219;&#21153;&#22330;&#26223;&#19979;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#24377;&#24615;&#22240;&#23376;&#65292;&#21487;&#20197;&#35299;&#20915;&#26799;&#24230;&#24046;&#24322;&#21644;&#36127;&#36801;&#31227;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01054</link><description>&lt;p&gt;
&#24377;&#24615;&#22810;&#26799;&#24230;&#19979;&#38477;&#29992;&#20110;&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Elastic Multi-Gradient Descent for Parallel Continual Learning. (arXiv:2401.01054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01054
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#35770;&#25991;&#65292;&#20171;&#32461;&#20102;&#22312;&#21160;&#24577;&#22810;&#20219;&#21153;&#22330;&#26223;&#19979;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#24377;&#24615;&#22240;&#23376;&#65292;&#21487;&#20197;&#35299;&#20915;&#26799;&#24230;&#24046;&#24322;&#21644;&#36127;&#36801;&#31227;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#30446;&#26631;&#26159;&#20174;&#26032;&#30340;&#25968;&#25454;&#27969;&#20013;&#25345;&#32493;&#23398;&#20064;&#24182;&#23436;&#25104;&#30456;&#24212;&#30340;&#20219;&#21153;&#12290;&#36807;&#21435;&#30740;&#31350;&#30340;CL&#20551;&#35774;&#25968;&#25454;&#25353;&#20219;&#21153;&#30340;&#39034;&#24207;&#32473;&#20986;&#65292;&#22240;&#27492;&#23646;&#20110;&#20018;&#34892;&#36830;&#32493;&#23398;&#20064;&#65288;SCL&#65289;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#22810;&#20219;&#21153;&#22330;&#26223;&#19979;&#30340;&#26032;&#20852;&#33539;&#24335;&#8212;&#8212;&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;&#65288;PCL&#65289;&#65292;&#20854;&#20013;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#36935;&#21040;&#20102;&#22810;&#26679;&#30340;&#20219;&#21153;&#12290;PCL&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#35757;&#32451;&#25968;&#37327;&#19981;&#30830;&#23450;&#19988;&#23398;&#20064;&#36827;&#24230;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#23548;&#33268;&#24456;&#38590;&#20445;&#35777;&#25152;&#26377;&#36935;&#21040;&#30340;&#20219;&#21153;&#37117;&#33021;&#24471;&#21040;&#26377;&#25928;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#20250;&#35758;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#27979;&#37327;&#21644;&#20943;&#23567;&#26799;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#28982;&#32780;&#65292;&#27599;&#27425;&#27169;&#22411;&#26356;&#26032;&#20173;&#28982;&#21487;&#33021;&#23384;&#22312;&#36127;&#36801;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#21160;&#24577;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24377;&#24615;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Continual Learning (CL) is to continuously learn from new data streams and accomplish the corresponding tasks. Previously studied CL assumes that data are given in sequence nose-to-tail for different tasks, thus indeed belonging to Serial Continual Learning (SCL). This paper studies the novel paradigm of Parallel Continual Learning (PCL) in dynamic multi-task scenarios, where a diverse set of tasks is encountered at different time points. PCL presents challenges due to the training of an unspecified number of tasks with varying learning progress, leading to the difficulty of guaranteeing effective model updates for all encountered tasks. In our previous conference work, we focused on measuring and reducing the discrepancy among gradients in a multi-objective optimization problem, which, however, may still contain negative transfers in every model update. To address this issue, in the dynamic multi-objective optimization problem, we introduce task-specific elastic factors to
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Auffusion&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#25193;&#25955;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#21644;&#25991;&#26412;-&#38899;&#39057;&#23545;&#40784;&#12290;&#35813;&#31995;&#32479;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#19979;&#32988;&#36807;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#20851;&#27880;&#20102;&#32534;&#30721;&#22120;&#36873;&#25321;&#23545;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01044</link><description>&lt;p&gt;
Auffusion: &#21033;&#29992;&#25193;&#25955;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Auffusion: Leveraging the Power of Diffusion and Large Language Models for Text-to-Audio Generation. (arXiv:2401.01044v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01044
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Auffusion&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#25193;&#25955;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#21644;&#25991;&#26412;-&#38899;&#39057;&#23545;&#40784;&#12290;&#35813;&#31995;&#32479;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#19979;&#32988;&#36807;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#20851;&#27880;&#20102;&#32534;&#30721;&#22120;&#36873;&#25321;&#23545;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#26174;&#33879;&#25512;&#21160;&#20102;AIGC&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#25991;&#26412;&#21040;&#38899;&#39057;&#65288;TTA&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;AIGC&#24212;&#29992;&#65292;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#38899;&#39057;&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;TTA&#30740;&#31350;&#24448;&#24448;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#25991;&#26412;-&#38899;&#39057;&#23545;&#40784;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22797;&#26434;&#30340;&#25991;&#26412;&#36755;&#20837;&#12290;&#21463;&#21040;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Auffusion&#65292;&#19968;&#31181;&#23558;T2I&#27169;&#22411;&#26694;&#26550;&#36866;&#24212;TTA&#20219;&#21153;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#20854;&#22266;&#26377;&#30340;&#29983;&#25104;&#20248;&#21183;&#21644;&#31934;&#30830;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#34920;&#26126;&#65292;Auffusion&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;TTA&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20043;&#21069;&#30340;T2I&#30740;&#31350;&#35748;&#35782;&#21040;&#32534;&#30721;&#22120;&#36873;&#25321;&#23545;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#20363;&#22914;&#32454;&#33410;&#21644;&#29289;&#20307;&#32465;&#23450;&#65292;&#32780;&#31867;&#20284;&#30340;&#35780;&#20272;&#21017;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in diffusion models and large language models (LLMs) have significantly propelled the field of AIGC. Text-to-Audio (TTA), a burgeoning AIGC application designed to generate audio from natural language prompts, is attracting increasing attention. However, existing TTA studies often struggle with generation quality and text-audio alignment, especially for complex textual inputs. Drawing inspiration from state-of-the-art Text-to-Image (T2I) diffusion models, we introduce Auffusion, a TTA system adapting T2I model frameworks to TTA task, by effectively leveraging their inherent generative strengths and precise cross-modal alignment. Our objective and subjective evaluations demonstrate that Auffusion surpasses previous TTA approaches using limited data and computational resource. Furthermore, previous studies in T2I recognizes the significant impact of encoder choice on cross-modal alignment, like fine-grained details and object bindings, while similar evaluation is lack
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20998;&#26512;&#20102;&#20854;&#27169;&#22411;&#30340;&#24615;&#33021;&#29305;&#24449;&#21644;&#35745;&#31639;&#36816;&#31639;&#31526;&#65292;&#24182;&#35752;&#35770;&#20102;&#20174;&#31995;&#32479;&#21644;&#26550;&#26500;&#35282;&#24230;&#19978;NSAI&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.01040</link><description>&lt;p&gt;
&#36208;&#21521;&#35748;&#30693;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65306;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#26597;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI. (arXiv:2401.01040v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20998;&#26512;&#20102;&#20854;&#27169;&#22411;&#30340;&#24615;&#33021;&#29305;&#24449;&#21644;&#35745;&#31639;&#36816;&#31639;&#31526;&#65292;&#24182;&#35752;&#35770;&#20102;&#20174;&#31995;&#32479;&#21644;&#26550;&#26500;&#35282;&#24230;&#19978;NSAI&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#20027;&#35201;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#65292;&#24050;&#32463;&#22312;&#25105;&#20204;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21487;&#25345;&#32493;&#35745;&#31639;&#36712;&#36857;&#30340;&#25361;&#25112;&#12289;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#21644;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#35201;&#27714;&#24320;&#21457;&#19979;&#19968;&#20195;AI&#31995;&#32479;&#12290;&#31070;&#32463;&#31526;&#21495;AI&#65288;NSAI&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#28508;&#21147;&#30340;&#33539;&#24335;&#20986;&#29616;&#65292;&#34701;&#21512;&#20102;&#31070;&#32463;&#12289;&#31526;&#21495;&#21644;&#27010;&#29575;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#35299;&#37322;&#33021;&#21147;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#23545;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#26368;&#36817;&#30340;NSAI&#31995;&#32479;&#22312;&#20855;&#26377;&#25512;&#29702;&#21644;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#22330;&#26223;&#20013;&#23637;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;NSAI&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#65292;&#24182;&#20998;&#26512;&#20102;NSAI&#27169;&#22411;&#30340;&#24615;&#33021;&#29305;&#24449;&#21644;&#35745;&#31639;&#36816;&#31639;&#31526;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#31995;&#32479;&#21644;&#26550;&#26500;&#30340;&#35282;&#24230;&#35752;&#35770;&#20102;NSAI&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable advancements in artificial intelligence (AI), primarily driven by deep neural networks, have significantly impacted various aspects of our lives. However, the current challenges surrounding unsustainable computational trajectories, limited robustness, and a lack of explainability call for the development of next-generation AI systems. Neuro-symbolic AI (NSAI) emerges as a promising paradigm, fusing neural, symbolic, and probabilistic approaches to enhance interpretability, robustness, and trustworthiness while facilitating learning from much less data. Recent NSAI systems have demonstrated great potential in collaborative human-AI scenarios with reasoning and cognitive capabilities. In this paper, we provide a systematic review of recent progress in NSAI and analyze the performance characteristics and computational operators of NSAI models. Furthermore, we discuss the challenges and potential future directions of NSAI from both system and architectural perspectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#23454;&#29616;Text-to-image diffusion models&#20013;&#30340;&#24555;&#36895;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01008</link><description>&lt;p&gt;
Text-to-image diffusion models&#20013;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Fast Inference Through The Reuse Of Attention Maps In Diffusion Models. (arXiv:2401.01008v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#23454;&#29616;Text-to-image diffusion models&#20013;&#30340;&#24555;&#36895;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#28789;&#27963;&#21644;&#36924;&#30495;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#21333;&#20010;&#22270;&#20687;&#25152;&#38656;&#30340;&#36845;&#20195;&#36807;&#31243;&#26082;&#26114;&#36149;&#21448;&#20855;&#26377;&#36739;&#39640;&#30340;&#24310;&#36831;&#65292;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#30740;&#31350;&#20854;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#37319;&#26679;&#27493;&#38271;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#37325;&#22797;&#35745;&#31639;&#27880;&#24847;&#21147;&#26144;&#23556;&#26082;&#32791;&#26102;&#21448;&#20887;&#20313;&#65292;&#22240;&#27492;&#25105;&#20204;&#24314;&#35758;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#32467;&#26500;&#21270;&#22320;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#37325;&#29992;&#31574;&#30053;&#21463;&#21040;&#21021;&#32423;ODE&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#35813;&#29702;&#35770;&#35748;&#20026;&#22312;&#37319;&#26679;&#36807;&#31243;&#30340;&#21518;&#26399;&#37325;&#29992;&#26368;&#21512;&#36866;&#12290;&#22312;&#27880;&#24847;&#21040;&#36825;&#31181;&#29702;&#35770;&#26041;&#27861;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models have demonstrated unprecedented abilities at flexible and realistic image synthesis. However, the iterative process required to produce a single image is costly and incurs a high latency, prompting researchers to further investigate its efficiency. Typically, improvements in latency have been achieved in two ways: (1) training smaller models through knowledge distillation (KD); and (2) adopting techniques from ODE-theory to facilitate larger step sizes. In contrast, we propose a training-free approach that does not alter the step-size of the sampler. Specifically, we find the repeated calculation of attention maps to be both costly and redundant; therefore, we propose a structured reuse of attention maps during sampling. Our initial reuse policy is motivated by rudimentary ODE-theory, which suggests that reuse is most suitable late in the sampling procedure. After noting a number of limitations in this theoretical approach, we empirically search for a bet
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#23454;&#29616;6G&#21450;&#20197;&#21518;&#32593;&#32476;AI&#30340;&#38646;&#25490;&#25918;&#30899;&#30446;&#26631;&#12290;&#36890;&#36807;&#30830;&#23450;&#25490;&#25918;&#28304;&#24182;&#24341;&#20837;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#21160;&#24577;&#33021;&#28304;&#20132;&#26131;&#21644;&#20219;&#21153;&#20998;&#37197;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#31227;&#21160;&#32593;&#32476;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#25490;&#25918;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01007</link><description>&lt;p&gt;
&#26397;&#30528;&#23454;&#29616;6G&#21450;&#20197;&#21518;&#32593;&#32476;AI&#38646;&#25490;&#25918;&#30899;&#30446;&#26631;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Net-Zero Carbon Emissions in Network AI for 6G and Beyond. (arXiv:2401.01007v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#23454;&#29616;6G&#21450;&#20197;&#21518;&#32593;&#32476;AI&#30340;&#38646;&#25490;&#25918;&#30899;&#30446;&#26631;&#12290;&#36890;&#36807;&#30830;&#23450;&#25490;&#25918;&#28304;&#24182;&#24341;&#20837;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#21160;&#24577;&#33021;&#28304;&#20132;&#26131;&#21644;&#20219;&#21153;&#20998;&#37197;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#31227;&#21160;&#32593;&#32476;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#25490;&#25918;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#27491;&#22312;&#21162;&#21147;&#20943;&#23569;&#20840;&#29699;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#65288;&#20027;&#35201;&#26159;&#30899;&#25490;&#25918;&#65289;&#21040;2030&#24180;&#20943;&#21322;&#65292;&#24182;&#22312;2050&#24180;&#23454;&#29616;&#38646;&#25490;&#25918;&#12290;&#21457;&#23637;6G&#25216;&#26415;&#20063;&#24517;&#39035;&#31526;&#21512;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#21487;&#25345;&#32493;&#19988;&#38646;&#25490;&#25918;&#31995;&#32479;&#20197;&#28385;&#36275;&#29992;&#25143;&#23545;&#31227;&#21160;&#26381;&#21153;&#65288;&#29305;&#21035;&#26159;&#26234;&#33021;&#26381;&#21153;&#21644;&#24212;&#29992;&#65289;&#19981;&#26029;&#22686;&#38271;&#30340;&#38656;&#27714;&#65292;&#21487;&#33021;&#27604;&#39044;&#26399;&#30340;&#35201;&#20855;&#26377;&#26356;&#22823;&#30340;&#25361;&#25112;&#24615;&#12290;&#23588;&#20854;&#26159;&#23613;&#31649;&#30828;&#20214;&#21644;&#36719;&#20214;&#35774;&#35745;&#30340;&#33021;&#25928;&#25913;&#36827;&#65292;&#31227;&#21160;&#32593;&#32476;&#30340;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#25490;&#25918;&#20173;&#22312;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#22686;&#38271;&#12290;&#36164;&#28304;&#38656;&#27714;&#20005;&#37325;&#30340;AI&#31639;&#27861;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#19981;&#26029;&#26222;&#21450;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20027;&#35201;&#30340;&#25490;&#25918;&#28304;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20998;&#26512;&#32593;&#32476;AI&#23454;&#29616;&#29983;&#21629;&#21608;&#26399;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#21512;&#21160;&#24577;&#33021;&#28304;&#20132;&#26131;&#21644;&#20219;&#21153;&#20998;&#37197;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A global effort has been initiated to reduce the worldwide greenhouse gas (GHG) emissions, primarily carbon emissions, by half by 2030 and reach net-zero by 2050. The development of 6G must also be compliant with this goal. Unfortunately, developing a sustainable and net-zero emission systems to meet the users' fast growing demands on mobile services, especially smart services and applications, may be much more challenging than expected. Particularly, despite the energy efficiency improvement in both hardware and software designs, the overall energy consumption and carbon emission of mobile networks are still increasing at a tremendous speed. The growing penetration of resource-demanding AI algorithms and solutions further exacerbate this challenge. In this article, we identify the major emission sources and introduce an evaluation framework for analyzing the lifecycle of network AI implementations. A novel joint dynamic energy trading and task allocation optimization framework, called
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#37322;&#20102;&#20799;&#31461;&#26159;&#22914;&#20309;&#36890;&#36807;&#22235;&#20010;&#35748;&#30693;&#26426;&#21046;&#23454;&#29616;&#20803;&#23398;&#20064;&#31574;&#30053;&#30340;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#27169;&#22411;&#65292;&#20026;&#22522;&#20110;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#30340;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.01001</link><description>&lt;p&gt;
&#20799;&#31461;&#20803;&#23398;&#20064;&#8212;&#8212;&#23545;&#36127;&#36131;&#20219;&#30340;&#33041;&#21551;&#21457;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
Metalearning-Informed Competence in Children: Implications for Responsible Brain-Inspired Artificial Intelligence. (arXiv:2401.01001v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#37322;&#20102;&#20799;&#31461;&#26159;&#22914;&#20309;&#36890;&#36807;&#22235;&#20010;&#35748;&#30693;&#26426;&#21046;&#23454;&#29616;&#20803;&#23398;&#20064;&#31574;&#30053;&#30340;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#27169;&#22411;&#65292;&#20026;&#22522;&#20110;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#30340;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#21253;&#25324;&#22235;&#20010;&#22522;&#26412;&#30340;&#35748;&#30693;&#26426;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#21516;&#26102;&#24182;&#21327;&#21516;&#22320;&#36816;&#20316;&#65292;&#20197;&#23454;&#29616;&#20799;&#31461;&#20803;&#23398;&#20064;&#65288;&#23398;&#20064;&#30340;&#30693;&#35782;&#21644;&#35843;&#33410;&#65289;&#31574;&#30053;&#30340;&#23454;&#26045;&#12290;&#36890;&#36807;&#21576;&#29616;&#19968;&#20010;&#21253;&#21547;&#26680;&#24515;&#26426;&#21046;&#21644;&#30456;&#20851;&#31574;&#30053;&#30340;&#36335;&#32447;&#22270;&#65292;&#26412;&#25991;&#35299;&#37322;&#20102;&#21457;&#23637;&#20013;&#30340;&#22823;&#33041;&#22312;&#36328;&#29615;&#22659;&#23398;&#20064;&#19978;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#36873;&#25321;&#22235;&#20010;&#22522;&#26412;&#20114;&#34917;&#24615;&#36807;&#31243;&#30340;&#32452;&#21512;&#26469;&#20849;&#21516;&#20195;&#34920;&#31934;&#31616;&#30340;&#20803;&#23398;&#20064;&#26550;&#26500;&#65292;&#36825;&#21487;&#20197;&#25193;&#23637;&#21040;&#27169;&#20223;&#22823;&#33041;&#23398;&#20064;&#21644;&#35299;&#20915;&#38382;&#39064;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#12290;&#36890;&#36807;&#23558;&#20855;&#26377;&#20803;&#23398;&#20064;&#33021;&#21147;&#30340;&#24180;&#36731;&#24605;&#32500;&#20316;&#20026;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#30340;&#27169;&#22411;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#23545;&#36947;&#24503;&#31435;&#22330;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper offers a novel conceptual framework comprising four essential cognitive mechanisms that operate concurrently and collaboratively to enable metalearning (knowledge and regulation of learning) strategy implementation in young children. A roadmap incorporating the core mechanisms and the associated strategies is presented as an explanation of the developing brain's remarkable cross-context learning competence. The tetrad of fundamental complementary processes is chosen to collectively represent the bare-bones metalearning architecture that can be extended to artificial intelligence (AI) systems emulating brain-like learning and problem-solving skills. Utilizing the metalearning-enabled young mind as a model for brain-inspired computing, this work further discusses important implications for morally grounded AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SafeCompress&#30340;&#27979;&#35797;&#39537;&#21160;&#31232;&#30095;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#36719;&#20214;&#20013;&#30340;&#23433;&#20840;&#27169;&#22411;&#21387;&#32553;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00996</link><description>&lt;p&gt;
&#23433;&#20840;&#19982;&#24615;&#33021;&#65292;&#20026;&#20160;&#20040;&#19981;&#20004;&#32773;&#20860;&#24471;&#65311;&#38024;&#23545;&#20154;&#24037;&#26234;&#33021;&#36719;&#20214;&#37096;&#32626;&#30340;&#24322;&#26500;&#25915;&#20987;&#30340;&#21452;&#30446;&#26631;&#20248;&#21270;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Safety and Performance, Why Not Both? Bi-Objective Optimized Model Compression against Heterogeneous Attacks Toward AI Software Deployment. (arXiv:2401.00996v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SafeCompress&#30340;&#27979;&#35797;&#39537;&#21160;&#31232;&#30095;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#36719;&#20214;&#20013;&#30340;&#23433;&#20840;&#27169;&#22411;&#21387;&#32553;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36719;&#20214;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#23567;&#27491;&#22312;&#36805;&#36895;&#22686;&#21152;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#65288;&#22914;&#26234;&#33021;&#25163;&#26426;&#65289;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#26500;&#25104;&#20102;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;AI&#36719;&#20214;&#21387;&#32553;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#26088;&#22312;&#22312;&#20445;&#25345;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#21387;&#32553;&#27169;&#22411;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#27169;&#22411;&#20013;&#30340;&#22266;&#26377;&#32570;&#38519;&#21487;&#33021;&#20250;&#34987;&#21387;&#32553;&#27169;&#22411;&#32487;&#25215;&#12290;&#30001;&#20110;&#21387;&#32553;&#27169;&#22411;&#36890;&#24120;&#22312;&#22823;&#37327;&#35774;&#22791;&#19978;&#37096;&#32626;&#19988;&#27809;&#26377;&#36275;&#22815;&#30340;&#20445;&#25252;&#65292;&#36825;&#20123;&#32570;&#38519;&#21487;&#33021;&#26131;&#34987;&#23545;&#25163;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#23433;&#20840;&#24615;&#33021;&#21327;&#35843;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#23433;&#20840;&#27169;&#22411;&#21387;&#32553;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21463;&#36719;&#20214;&#24037;&#31243;&#20013;&#27979;&#35797;&#39537;&#21160;&#24320;&#21457;&#65288;TDD&#65289;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SafeCompress&#30340;&#27979;&#35797;&#39537;&#21160;&#31232;&#30095;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#25915;&#20987;&#26426;&#21046;&#27169;&#25311;&#20026;&#23433;&#20840;&#27979;&#35797;&#65292;SafeCompress&#21487;&#20197;&#33258;&#21160;&#23558;&#22823;&#22411;&#27169;&#22411;&#21387;&#32553;&#20026;&#23567;&#22411;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The size of deep learning models in artificial intelligence (AI) software is increasing rapidly, hindering the large-scale deployment on resource-restricted devices (e.g., smartphones). To mitigate this issue, AI software compression plays a crucial role, which aims to compress model size while keeping high performance. However, the intrinsic defects in a big model may be inherited by the compressed one. Such defects may be easily leveraged by adversaries, since a compressed model is usually deployed in a large number of devices without adequate protection. In this article, we aim to address the safe model compression problem from the perspective of safety-performance co-optimization. Specifically, inspired by the test-driven development (TDD) paradigm in software engineering, we propose a test-driven sparse training framework called SafeCompress. By simulating the attack mechanism as safety testing, SafeCompress can automatically compress a big model to a small one following the dynam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38598;&#20013;&#30740;&#31350;&#20102;&#22312;&#26377;&#32972;&#26223;&#24178;&#25200;&#25928;&#26524;&#21644;&#36974;&#25377;&#29615;&#22659;&#20013;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24212;&#29992;SSD&#21644;YOLO&#31639;&#27861;&#65292;&#24182;&#25913;&#21892;&#26816;&#27979;&#31934;&#24230;&#65292;&#20943;&#23569;&#38382;&#39064;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#24103;&#29575;&#30340;SSD-Mobilenet v2&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.00986</link><description>&lt;p&gt;
&#22312;&#26377;&#32972;&#26223;&#24178;&#25200;&#25928;&#26524;&#30340;&#36974;&#25377;&#29615;&#22659;&#20013;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Real-Time Object Detection in Occluded Environment with Background Cluttering Effects Using Deep Learning. (arXiv:2401.00986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38598;&#20013;&#30740;&#31350;&#20102;&#22312;&#26377;&#32972;&#26223;&#24178;&#25200;&#25928;&#26524;&#21644;&#36974;&#25377;&#29615;&#22659;&#20013;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24212;&#29992;SSD&#21644;YOLO&#31639;&#27861;&#65292;&#24182;&#25913;&#21892;&#26816;&#27979;&#31934;&#24230;&#65292;&#20943;&#23569;&#38382;&#39064;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#24103;&#29575;&#30340;SSD-Mobilenet v2&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#26816;&#27979;&#23567;&#22411;&#12289;&#19981;&#30830;&#23450;&#30340;&#31227;&#21160;&#29289;&#20307;&#25110;&#22312;&#36974;&#25377;&#29615;&#22659;&#20013;&#20855;&#26377;&#26434;&#20081;&#32972;&#26223;&#30340;&#29289;&#20307;&#12290;&#36825;&#20005;&#37325;&#24433;&#21709;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#38598;&#20013;&#30740;&#31350;&#20102;&#22312;&#36974;&#25377;&#29615;&#22659;&#20013;&#21033;&#29992;SSD&#21644;YOLO&#31639;&#27861;&#36827;&#34892;&#23454;&#26102;&#27773;&#36710;&#21644;&#22374;&#20811;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25913;&#21892;&#20102;&#26816;&#27979;&#31934;&#24230;&#65292;&#20943;&#23569;&#20102;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25152;&#24320;&#21457;&#30340;&#26041;&#27861;&#26500;&#24314;&#20102;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#39044;&#22788;&#29702;&#25216;&#26415;&#28165;&#38500;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35757;&#32451;&#25152;&#24320;&#21457;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#24179;&#34913;&#21644;&#20016;&#23500;&#25968;&#25454;&#12290;&#36890;&#36807;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#24182;&#31361;&#20986;&#23637;&#31034;&#25105;&#20204;&#24471;&#21040;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#23545;&#24050;&#24314;&#31435;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#27604;&#19981;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#26102;&#24471;&#21040;&#20102;&#26356;&#20934;&#30830;&#30340;&#31934;&#24230;&#21644;&#27599;&#31186;&#24103;&#25968;&#12290;SSD-Mobilenet v2&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#24103;&#29575;&#22343;&#39640;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of small, undetermined moving objects or objects in an occluded environment with a cluttered background is the main problem of computer vision. This greatly affects the detection accuracy of deep learning models. To overcome these problems, we concentrate on deep learning models for real-time detection of cars and tanks in an occluded environment with a cluttered background employing SSD and YOLO algorithms and improved precision of detection and reduce problems faced by these models. The developed method makes the custom dataset and employs a preprocessing technique to clean the noisy dataset. For training the developed model we apply the data augmentation technique to balance and diversify the data. We fine-tuned, trained, and evaluated these models on the established dataset by applying these techniques and highlighting the results we got more accurately than without applying these techniques. The accuracy and frame per second of the SSD-Mobilenet v2 model are higher than 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#20248;&#21270;&#12289;&#33258;&#28982;&#21551;&#21457;&#24335;&#31639;&#27861;&#20197;&#21450;&#28151;&#21512;&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#31639;&#27861;&#28151;&#21512;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00976</link><description>&lt;p&gt;
&#36816;&#29992;&#33258;&#28982;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#20248;&#21270;&#65306;&#20171;&#32461;&#12289;&#28151;&#21512;&#21644;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Nature-Inspired Algorithms in Optimization: Introduction, Hybridization and Insights. (arXiv:2401.00976v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#20248;&#21270;&#12289;&#33258;&#28982;&#21551;&#21457;&#24335;&#31639;&#27861;&#20197;&#21450;&#28151;&#21512;&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#31639;&#27861;&#28151;&#21512;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#37117;&#26159;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#33021;&#38656;&#35201;&#22797;&#26434;&#30340;&#20248;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#12290;&#33258;&#28982;&#21551;&#21457;&#24335;&#31639;&#27861;&#26159;&#19968;&#31867;&#29992;&#20110;&#20248;&#21270;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#19968;&#20123;&#31639;&#27861;&#25110;&#21464;&#31181;&#32463;&#24120;&#36890;&#36807;&#28151;&#21512;&#26041;&#27861;&#26469;&#24320;&#21457;&#12290;&#22522;&#20934;&#27979;&#35797;&#22312;&#35780;&#20272;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#26041;&#38754;&#20063;&#24456;&#37325;&#35201;&#12290;&#26412;&#31456;&#20027;&#35201;&#20171;&#32461;&#20248;&#21270;&#12289;&#33258;&#28982;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#28151;&#21512;&#26041;&#27861;&#30340;&#27010;&#36848;&#12290;&#25105;&#20204;&#36824;&#23558;&#37325;&#28857;&#20171;&#32461;&#19968;&#20123;&#31639;&#27861;&#28151;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many problems in science and engineering are optimization problems, which may require sophisticated optimization techniques to solve. Nature-inspired algorithms are a class of metaheuristic algorithms for optimization, and some algorithms or variants are often developed by hybridization. Benchmarking is also important in evaluating the performance of optimization algorithms. This chapter focuses on the overview of optimization, nature-inspired algorithms and the role of hybridization. We will also highlight some issues with hybridization of algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#35757;&#32451;&#20013;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#22312;&#19981;&#21516;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#35757;&#32451;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#26102;&#65292;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BN&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#20248;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.00974</link><description>&lt;p&gt;
&#22312;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#20013;&#65292;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#30340;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models. (arXiv:2401.00974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#35757;&#32451;&#20013;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#22312;&#19981;&#21516;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#35757;&#32451;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#26102;&#65292;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BN&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#20248;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#27969;&#31243;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#20294;&#20855;&#26377;&#37325;&#35201;&#23454;&#38469;&#24847;&#20041;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#31867;&#22411;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#23545;&#20110;&#21512;&#25104;&#35757;&#32451;&#20219;&#21153;&#20013;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#65292;&#32473;&#23450;&#29305;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31867;&#21035;&#21644;&#24615;&#33021;&#24230;&#37327;&#65292;&#25552;&#20379;&#30340;&#35265;&#35299;&#26377;&#38480;&#12290;&#26412;&#25991;&#38024;&#23545;&#35757;&#32451;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#23548;&#21521;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#35843;&#26597;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#32422;&#26463;&#19981;&#21516;&#26102;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25903;&#25345;&#20197;&#19979;&#35266;&#28857;&#65306;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#32422;&#26463;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BN&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#22343;&#33021;&#24456;&#22909;&#23436;&#25104;&#21512;&#25104;&#35757;&#32451;&#20219;&#21153;&#65292;&#20294;&#22312;&#21512;&#25104;&#35757;&#32451;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#26102;&#65292;BN-based&#29983;&#25104;&#27169;&#22411;&#27604;NN-based&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Devising procedures for downstream task-oriented generative model selections is an unresolved problem of practical importance. Existing studies focused on the utility of a single family of generative models. They provided limited insights on how synthetic data practitioners select the best family generative models for synthetic training tasks given a specific combination of machine learning model class and performance metric. In this paper, we approach the downstream task-oriented generative model selections problem in the case of training fraud detection models and investigate the best practice given different combinations of model interpretability and model performance constraints. Our investigation supports that, while both Neural Network(NN)-based and Bayesian Network(BN)-based generative models are both good to complete synthetic training task under loose model interpretability constrain, the BN-based generative models is better than NN-based when synthetic training fraud detectio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24212;&#29992;&#22522;&#20110;&#22270;&#20687;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20110;WiFi CSI&#65292;&#26088;&#22312;&#35299;&#20915;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36328;&#22330;&#26223;&#21644;&#36328;&#31995;&#32479;&#30340;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#32447;&#24615;&#35270;&#32447;&#65288;LOS&#65289;&#21644;&#38750;&#32447;&#24615;&#35270;&#32447;&#65288;NLOS&#65289;&#31359;&#22681;&#22330;&#26223;&#20043;&#38388;&#20197;&#21450;&#19981;&#21516;&#22825;&#32447;&#31995;&#32479;&#20043;&#38388;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;EfficientNetV2&#26550;&#26500;&#30340;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#24182;&#36827;&#34892;&#28040;&#34701;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.00964</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;WiFi CSI&#22522;&#20110;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human Activity Recognition. (arXiv:2401.00964v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24212;&#29992;&#22522;&#20110;&#22270;&#20687;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20110;WiFi CSI&#65292;&#26088;&#22312;&#35299;&#20915;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36328;&#22330;&#26223;&#21644;&#36328;&#31995;&#32479;&#30340;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#32447;&#24615;&#35270;&#32447;&#65288;LOS&#65289;&#21644;&#38750;&#32447;&#24615;&#35270;&#32447;&#65288;NLOS&#65289;&#31359;&#22681;&#22330;&#26223;&#20043;&#38388;&#20197;&#21450;&#19981;&#21516;&#22825;&#32447;&#31995;&#32479;&#20043;&#38388;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;EfficientNetV2&#26550;&#26500;&#30340;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#24182;&#36827;&#34892;&#28040;&#34701;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#33021;&#22815;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#23454;&#29616;&#26080;&#25509;&#35302;&#21644;&#35270;&#35273;&#20445;&#25252;&#38544;&#31169;&#30340;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#26465;&#20214;&#21644;&#24863;&#30693;&#30828;&#20214;&#30340;&#24046;&#24322;&#65292;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#26159;&#36825;&#20010;&#39046;&#22495;&#20013;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#23558;&#24120;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#24212;&#29992;&#20110;WiFi CSI&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#23545;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#22312;&#36328;&#22330;&#26223;&#21644;&#36328;&#31995;&#32479;&#35774;&#32622;&#20013;&#30340;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32447;&#24615;&#35270;&#32447;&#65288;LOS&#65289;&#21644;&#38750;&#32447;&#24615;&#35270;&#32447;&#65288;NLOS&#65289;&#31359;&#22681;&#22330;&#26223;&#20043;&#38388;&#30340;&#27867;&#21270;&#65292;&#20197;&#21450;&#19981;&#21516;&#22825;&#32447;&#31995;&#32479;&#20043;&#38388;&#30340;&#27867;&#21270;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#32463;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#20844;&#24320;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#20307;&#27963;&#21160;CSI&#24133;&#24230;&#35889;&#22270;&#30340;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#36827;&#34892;&#20102;&#19968;&#20010;&#28040;&#34701;&#30740;&#31350;&#65292;&#22522;&#20110;EfficientNetV2&#26550;&#26500;&#26500;&#24314;&#20102;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recognition of human activities based on WiFi Channel State Information (CSI) enables contactless and visual privacy-preserving sensing in indoor environments. However, poor model generalization, due to varying environmental conditions and sensing hardware, is a well-known problem in this space. To address this issue, in this work, data augmentation techniques commonly used in image-based learning are applied to WiFi CSI to investigate their effects on model generalization performance in cross-scenario and cross-system settings. In particular, we focus on the generalization between line-of-sight (LOS) and non-line-of-sight (NLOS) through-wall scenarios, as well as on the generalization between different antenna systems, which remains under-explored. We collect and make publicly available a dataset of CSI amplitude spectrograms of human activities. Utilizing this data, an ablation study is conducted in which activity recognition models based on the EfficientNetV2 architecture are tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#39044;&#27979;&#12290;&#35813;&#31639;&#27861;&#32771;&#34385;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#21253;&#21547;&#20102;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#38543;&#26426;&#32593;&#26684;&#25628;&#32034;&#21644;&#36138;&#23146;&#25628;&#32034;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.00961</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Automated Model Selection for Tabular Data. (arXiv:2401.00961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#39044;&#27979;&#12290;&#35813;&#31639;&#27861;&#32771;&#34385;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#21253;&#21547;&#20102;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#38543;&#26426;&#32593;&#26684;&#25628;&#32034;&#21644;&#36138;&#23146;&#25628;&#32034;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#21253;&#21547;&#29420;&#29305;&#19988;&#31163;&#25955;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#23545;&#30446;&#26631;&#30340;&#37325;&#35201;&#24615;&#21508;&#19981;&#30456;&#21516;&#12290;&#21333;&#20010;&#29305;&#24449;&#30340;&#32452;&#21512;&#21487;&#33021;&#27604;&#31616;&#21333;&#30340;&#21333;&#20010;&#29305;&#24449;&#36129;&#29486;&#26356;&#20855;&#39044;&#27979;&#24615;&#21644;&#24847;&#20041;&#12290;R&#30340;&#28151;&#21512;&#25928;&#24212;&#32447;&#24615;&#27169;&#22411;&#24211;&#20801;&#35768;&#29992;&#25143;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#25552;&#20379;&#36825;&#31181;&#20132;&#20114;&#24335;&#29305;&#24449;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#26377;&#35768;&#22810;&#29305;&#24449;&#21644;&#21487;&#33021;&#30340;&#20132;&#20114;&#36873;&#25321;&#65292;&#27169;&#22411;&#36873;&#25321;&#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#36739;&#23567;&#65292;&#33258;&#21160;&#21270;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#65292;&#24182;&#21516;&#26102;&#32771;&#34385;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65306;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#38543;&#26426;&#32593;&#26684;&#25628;&#32034;&#21644;&#36138;&#23146;&#25628;&#32034;&#26041;&#27861;&#12290;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#39564;&#27010;&#29575;&#26469;&#24341;&#23548;&#25628;&#32034;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#29305;&#24449;&#32452;&#21512;&#12290;&#36138;&#23146;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#22320;&#28155;&#21152;&#29305;&#24449;&#26500;&#24314;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured data in the form of tabular datasets contain features that are distinct and discrete, with varying individual and relative importances to the target. Combinations of one or more features may be more predictive and meaningful than simple individual feature contributions. R's mixed effect linear models library allows users to provide such interactive feature combinations in the model design. However, given many features and possible interactions to select from, model selection becomes an exponentially difficult task. We aim to automate the model selection process for predictions on tabular datasets incorporating feature interactions while keeping computational costs small. The framework includes two distinct approaches for feature selection: a Priority-based Random Grid Search and a Greedy Search method. The Priority-based approach efficiently explores feature combinations using prior probabilities to guide the search. The Greedy method builds the solution iteratively by addin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#21644;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#65292;&#36890;&#36807;&#35299;&#20915;&#30333;&#32454;&#32990;&#23610;&#24230;&#24046;&#24322;&#38382;&#39064;&#21644;&#25552;&#39640;&#26816;&#27979;&#31934;&#24230;&#65292;&#20197;&#25913;&#21892;&#20256;&#32479;&#34880;&#28082;&#26816;&#27979;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.00926</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#21464;&#24418;DETR&#21644;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#29992;&#20110;&#36741;&#21161;&#34880;&#28082;&#30142;&#30149;&#35786;&#26029;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases. (arXiv:2401.00926v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#21644;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#65292;&#36890;&#36807;&#35299;&#20915;&#30333;&#32454;&#32990;&#23610;&#24230;&#24046;&#24322;&#38382;&#39064;&#21644;&#25552;&#39640;&#26816;&#27979;&#31934;&#24230;&#65292;&#20197;&#25913;&#21892;&#20256;&#32479;&#34880;&#28082;&#26816;&#27979;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#21307;&#38498;&#34880;&#28082;&#26816;&#27979;&#20013;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#38656;&#35201;&#21307;&#29983;&#20351;&#29992;&#26174;&#24494;&#38236;&#20174;&#24739;&#32773;&#30340;&#34880;&#28082;&#26174;&#24494;&#22270;&#20687;&#20013;&#25163;&#21160;&#20998;&#31163;&#30333;&#32454;&#32990;&#12290;&#28982;&#21518;&#36890;&#36807;&#33258;&#21160;&#30333;&#32454;&#32990;&#20998;&#31867;&#22120;&#23545;&#36825;&#20123;&#20998;&#31163;&#30340;&#30333;&#32454;&#32990;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#30830;&#23450;&#34880;&#26679;&#20013;&#19981;&#21516;&#31867;&#22411;&#30333;&#32454;&#32990;&#30340;&#27604;&#20363;&#21644;&#20307;&#31215;&#65292;&#20174;&#32780;&#21327;&#21161;&#30142;&#30149;&#35786;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#32791;&#26102;&#12289;&#32791;&#21147;&#65292;&#32780;&#19988;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#65292;&#22240;&#20026;&#22270;&#20687;&#36136;&#37327;&#21644;&#29615;&#22659;&#26465;&#20214;&#31561;&#22240;&#32032;&#65292;&#21487;&#33021;&#23548;&#33268;&#21518;&#32493;&#20998;&#31867;&#38169;&#35823;&#21644;&#35823;&#35786;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#26041;&#27861;&#65306;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#21644;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#65288;MFDS-DETR&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#30333;&#32454;&#32990;&#23610;&#24230;&#24046;&#24322;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#39640;&#32423;&#31579;&#36873;&#29305;&#24449;&#34701;&#21512;&#37329;&#23383;&#22612;&#65288;HS-FPN&#65289;&#65292;&#23454;&#29616;&#20102;&#22810;&#32423;&#34701;&#21512;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#39640;&#32423;&#29305;&#24449;&#20316;&#20026;&#29305;&#24449;&#34701;&#21512;&#30340;&#36755;&#20837;&#65292;&#21516;&#26102;&#37319;&#29992;&#21464;&#24418;&#33258;&#27880;&#24847;DETR&#23454;&#29616;&#31934;&#30830;&#30340;&#30333;&#32454;&#32990;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In standard hospital blood tests, the traditional process requires doctors to manually isolate leukocytes from microscopic images of patients' blood using microscopes. These isolated leukocytes are then categorized via automatic leukocyte classifiers to determine the proportion and volume of different types of leukocytes present in the blood samples, aiding disease diagnosis. This methodology is not only time-consuming and labor-intensive, but it also has a high propensity for errors due to factors such as image quality and environmental conditions, which could potentially lead to incorrect subsequent classifications and misdiagnosis. To address these issues, this paper proposes an innovative method of leukocyte detection: the Multi-level Feature Fusion and Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte scale disparity, we designed the High-level Screening-feature Fusion Pyramid (HS-FPN), enabling multi-level fusion. This model uses high-level features as 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#28151;&#27788;&#31995;&#32479;&#20013;&#36827;&#34892;&#25968;&#25454;&#21516;&#21270;&#30340;&#26032;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23436;&#25972;&#25110;&#37096;&#20998;&#35266;&#27979;&#30340;&#29366;&#24577;&#21464;&#37327;&#36827;&#34892;&#29366;&#24577;&#26657;&#27491;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#35266;&#27979;&#21644;&#39044;&#27979;&#29366;&#24577;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.00916</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#28151;&#27788;&#31995;&#32479;&#20013;&#36827;&#34892;&#25968;&#25454;&#21516;&#21270;
&lt;/p&gt;
&lt;p&gt;
Data Assimilation in Chaotic Systems Using Deep Reinforcement Learning. (arXiv:2401.00916v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#28151;&#27788;&#31995;&#32479;&#20013;&#36827;&#34892;&#25968;&#25454;&#21516;&#21270;&#30340;&#26032;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23436;&#25972;&#25110;&#37096;&#20998;&#35266;&#27979;&#30340;&#29366;&#24577;&#21464;&#37327;&#36827;&#34892;&#29366;&#24577;&#26657;&#27491;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#35266;&#27979;&#21644;&#39044;&#27979;&#29366;&#24577;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21516;&#21270;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#27668;&#20505;&#39044;&#27979;&#21644;&#22825;&#27668;&#39044;&#25253;&#21040;&#33258;&#20027;&#36710;&#36742;&#30340;&#36712;&#36857;&#35268;&#21010;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;EnKF&#65289;&#65292;&#23427;&#20381;&#36182;&#20110;&#32447;&#24615;&#26356;&#26032;&#26469;&#26368;&#23567;&#21270;&#39044;&#27979;&#29366;&#24577;&#38598;&#21512;&#30340;&#26041;&#24046;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#30475;&#21040;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#20027;&#35201;&#26159;&#22312;&#26377;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#20869;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26410;&#32463;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#30340;&#36866;&#24212;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#21516;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#20351;&#29992;&#23436;&#25972;&#25110;&#37096;&#20998;&#35266;&#27979;&#30340;&#29366;&#24577;&#21464;&#37327;&#36827;&#34892;&#29366;&#24577;&#26657;&#27491;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#22312;&#28151;&#27788;&#30340;Lorenz '63&#31995;&#32479;&#19978;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#20195;&#29702;&#30340;&#30446;&#26631;&#26159;&#23558;&#35266;&#27979;&#21644;&#30456;&#24212;&#30340;&#39044;&#27979;&#29366;&#24577;&#20043;&#38388;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#26657;&#27491;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data assimilation (DA) plays a pivotal role in diverse applications, ranging from climate predictions and weather forecasts to trajectory planning for autonomous vehicles. A prime example is the widely used ensemble Kalman filter (EnKF), which relies on linear updates to minimize variance among the ensemble of forecast states. Recent advancements have seen the emergence of deep learning approaches in this domain, primarily within a supervised learning framework. However, the adaptability of such models to untrained scenarios remains a challenge. In this study, we introduce a novel DA strategy that utilizes reinforcement learning (RL) to apply state corrections using full or partial observations of the state variables. Our investigation focuses on demonstrating this approach to the chaotic Lorenz '63 system, where the agent's objective is to minimize the root-mean-squared error between the observations and corresponding forecast states. Consequently, the agent develops a correction stra
&lt;/p&gt;</description></item><item><title>LaFFi&#26159;&#19968;&#31181;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#35201;&#27714;&#27169;&#22411;&#39044;&#27979;&#26631;&#27880;&#32773;&#23558;&#20250;&#32473;&#20986;&#30340;&#21453;&#39304;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.00907</link><description>&lt;p&gt;
LaFFi: &#21033;&#29992;&#28151;&#21512;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models. (arXiv:2401.00907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00907
&lt;/p&gt;
&lt;p&gt;
LaFFi&#26159;&#19968;&#31181;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#35201;&#27714;&#27169;&#22411;&#39044;&#27979;&#26631;&#27880;&#32773;&#23558;&#20250;&#32473;&#20986;&#30340;&#21453;&#39304;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24494;&#35843;&#21487;&#20197;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#12290;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;LLM&#34987;&#35757;&#32451;&#25104;&#20135;&#29983;&#26399;&#26395;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;SFT&#35757;&#32451;&#30340;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#38382;&#31572;&#65289;&#20013;&#26377;&#26102;&#20250;&#20986;&#29616;&#31616;&#21333;&#38169;&#35823;&#21644;&#24187;&#35273;&#12290;&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;SFT&#24456;&#38590;&#23398;&#20064;&#21040;&#38382;&#39064;&#21644;&#26399;&#26395;&#31572;&#26696;&#20043;&#38388;&#30340;&#33391;&#22909;&#26144;&#23556;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#24494;&#35843;LLM&#65288;LaFFi&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;LaFFi&#35201;&#27714;LLM&#30452;&#25509;&#39044;&#27979;&#26631;&#27880;&#32773;&#23558;&#20250;&#32473;&#20986;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#26679;&#30340;&#21453;&#24605;&#35201;&#27714;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#39046;&#22495;&#20869;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#22312;SFT LLM&#39046;&#22495;&#20013;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#39069;&#22806;&#30340;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#37096;&#20998;&#21487;&#20197;&#34987;&#26367;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models (LLMs) adapts a trained model to specific downstream tasks, significantly improving task-specific performance. Supervised Fine-Tuning (SFT) is a common approach, where an LLM is trained to produce desired answers. However, LLMs trained with SFT sometimes make simple mistakes and result in hallucinations on reasoning tasks such as question-answering. Without external feedback, it is difficult for SFT to learn a good mapping between the question and the desired answer, especially with a small dataset. This paper introduces an alternative to SFT called Natural Language Feedback for Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they will receive from an annotator. We find that requiring such reflection can significantly improve the accuracy in in-domain question-answering tasks, providing a promising direction for the application of natural language feedback in the realm of SFT LLMs. Additional ablation studies show that the portion
&lt;/p&gt;</description></item><item><title>&#33945;&#38754;&#24314;&#27169;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#34987;&#33945;&#38754;&#37096;&#20998;&#23454;&#29616;&#28145;&#24230;&#27169;&#22411;&#23398;&#20064;&#31283;&#20581;&#30340;&#34920;&#31034;&#65292;&#24050;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.00897</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;&#20013;&#30340;&#33945;&#38754;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Masked Modeling for Self-supervised Representation Learning on Vision and Beyond. (arXiv:2401.00897v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00897
&lt;/p&gt;
&lt;p&gt;
&#33945;&#38754;&#24314;&#27169;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#34987;&#33945;&#38754;&#37096;&#20998;&#23454;&#29616;&#28145;&#24230;&#27169;&#22411;&#23398;&#20064;&#31283;&#20581;&#30340;&#34920;&#31034;&#65292;&#24050;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#38761;&#21629;&#19981;&#26029;&#21521;&#21069;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22240;&#20854;&#20986;&#33394;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20302;&#20381;&#36182;&#24615;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#20123;&#22810;&#26679;&#30340;&#33258;&#30417;&#30563;&#25216;&#26415;&#20013;&#65292;&#33945;&#38754;&#24314;&#27169;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30456;&#24212;&#27604;&#20363;&#30340;&#21407;&#22987;&#25968;&#25454;&#20250;&#34987;&#33945;&#38754;&#65292;&#27169;&#22411;&#38656;&#35201;&#39044;&#27979;&#20986;&#36825;&#20123;&#33945;&#38754;&#30340;&#37096;&#20998;&#12290;&#36825;&#31181;&#33539;&#24335;&#20351;&#28145;&#24230;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#31283;&#20581;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20854;&#20182;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#33945;&#38754;&#24314;&#27169;&#26694;&#26550;&#21450;&#20854;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#33945;&#38754;&#24314;&#27169;&#20013;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#21253;&#25324;&#22810;&#26679;&#30340;&#33945;&#38754;&#31574;&#30053;&#12289;&#24674;&#22797;&#30446;&#26631;&#12289;&#32593;&#32476;&#26550;&#26500;&#31561;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#23427;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20854;&#20849;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the deep learning revolution marches on, self-supervised learning has garnered increasing attention in recent years thanks to its remarkable representation learning ability and the low dependence on labeled data. Among these varied self-supervised techniques, masked modeling has emerged as a distinctive approach that involves predicting parts of the original data that are proportionally masked during training. This paradigm enables deep models to learn robust representations and has demonstrated exceptional performance in the context of computer vision, natural language processing, and other modalities. In this survey, we present a comprehensive review of the masked modeling framework and its methodology. We elaborate on the details of techniques within masked modeling, including diverse masking strategies, recovering targets, network architectures, and more. Then, we systematically investigate its wide-ranging applications across domains. Furthermore, we also explore the commonalit
&lt;/p&gt;</description></item><item><title>Social-LLM&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#65292;&#36890;&#36807;&#32467;&#21512;&#26412;&#22320;&#21270;&#31038;&#20132;&#32593;&#32476;&#20114;&#21160;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#23545;&#29992;&#25143;&#34892;&#20026;&#30340;&#35268;&#27169;&#21270;&#24314;&#27169;&#12290;&#22522;&#20110;&#31038;&#20132;&#32593;&#32476;&#21516;&#36136;&#24615;&#30340;&#21069;&#25552;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#29992;&#25143;&#26816;&#27979;&#20219;&#21153;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.00893</link><description>&lt;p&gt;
Social-LLM: &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#23545;&#35268;&#27169;&#21270;&#29992;&#25143;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Social-LLM: Modeling User Behavior at Scale using Language Models and Social Network Data. (arXiv:2401.00893v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00893
&lt;/p&gt;
&lt;p&gt;
Social-LLM&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#65292;&#36890;&#36807;&#32467;&#21512;&#26412;&#22320;&#21270;&#31038;&#20132;&#32593;&#32476;&#20114;&#21160;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#23545;&#29992;&#25143;&#34892;&#20026;&#30340;&#35268;&#27169;&#21270;&#24314;&#27169;&#12290;&#22522;&#20110;&#31038;&#20132;&#32593;&#32476;&#21516;&#36136;&#24615;&#30340;&#21069;&#25552;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#29992;&#25143;&#26816;&#27979;&#20219;&#21153;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#30340;&#22823;&#37327;&#22686;&#21152;&#20026;&#23545;&#20154;&#31867;&#34892;&#20026;&#36827;&#34892;&#24191;&#27867;&#30340;&#25968;&#25454;&#39537;&#21160;&#25506;&#32034;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#31038;&#20132;&#32593;&#32476;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#20026;&#21508;&#31181;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#38382;&#39064;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#65292;&#29305;&#21035;&#26159;&#31038;&#20132;&#24433;&#21709;&#21644;&#20449;&#24687;&#20256;&#25773;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#23545;&#22823;&#35268;&#27169;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#38754;&#20020;&#30528;&#35745;&#31639;&#25361;&#25112;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#24471;&#23545;&#25991;&#26412;&#20869;&#23481;&#36827;&#34892;&#24314;&#27169;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#65292;&#20294;&#20219;&#20309;&#20808;&#36827;&#30340;&#32593;&#32476;&#34920;&#31034;&#26041;&#27861;&#37117;&#38590;&#20197;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#37096;&#32626;&#21040;&#26410;&#30693;&#29992;&#25143;&#30340;&#38382;&#39064;&#19978;&#28385;&#36275;&#35201;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29992;&#25143;&#26816;&#27979;&#20219;&#21153;&#20013;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#23558;&#26412;&#22320;&#21270;&#31038;&#20132;&#32593;&#32476;&#20114;&#21160;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#22522;&#20110;&#31038;&#20132;&#32593;&#32476;&#21516;&#36136;&#24615;&#30340;&#21069;&#25552;&#65292;&#21363;&#31038;&#20132;&#36830;&#25509;&#30340;&#29992;&#25143;&#20855;&#26377;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of social network data has unlocked unprecedented opportunities for extensive, data-driven exploration of human behavior. The structural intricacies of social networks offer insights into various computational social science issues, particularly concerning social influence and information diffusion. However, modeling large-scale social network data comes with computational challenges. Though large language models make it easier than ever to model textual content, any advanced network representation methods struggle with scalability and efficient deployment to out-of-sample users. In response, we introduce a novel approach tailored for modeling social network data in user detection tasks. This innovative method integrates localized social network interactions with the capabilities of large language models. Operating under the premise of social network homophily, which posits that socially connected users share similarities, our approach is designed to address these cha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#24320;&#21457;&#20986;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#30333;&#34880;&#30149;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#31934;&#30830;&#24230;&#21644;F1-score&#25351;&#26631;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.00883</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#33258;&#21160;&#21270;&#30333;&#34880;&#30149;&#35786;&#26029;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Automating Leukemia Diagnosis with Autoencoders: A Comparative Study. (arXiv:2401.00883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#24320;&#21457;&#20986;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#30333;&#34880;&#30149;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#31934;&#30830;&#24230;&#21644;F1-score&#25351;&#26631;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30333;&#34880;&#30149;&#26159;&#23041;&#32961;&#20154;&#31867;&#29983;&#21629;&#30340;&#26368;&#24120;&#35265;&#21644;&#33268;&#21629;&#30340;&#30284;&#30151;&#20043;&#19968;&#12290;&#26469;&#33258;&#24739;&#32773;&#20851;&#38190;&#21442;&#25968;&#30340;&#21307;&#30103;&#25968;&#25454;&#20013;&#38544;&#34255;&#30528;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#35838;&#39064;&#19978;&#65292;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#29992;&#26469;&#25552;&#21462;&#36825;&#20123;&#20449;&#24687;&#12290;&#26412;&#25991;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#24320;&#21457;&#20102;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#65292;&#20197;&#24110;&#21161;&#25552;&#39640;&#30333;&#34880;&#30149;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23581;&#35797;&#25214;&#21040;&#26368;&#20339;&#30340;&#28608;&#27963;&#20989;&#25968;&#21644;&#20248;&#21270;&#22120;&#26469;&#22312;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#20351;&#29992;&#65292;&#24182;&#35774;&#35745;&#20102;&#26368;&#20339;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#19982;&#35813;&#39046;&#22495;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#21644;F1-score&#25351;&#26631;&#19978;&#27604;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#21319;&#20102;&#36229;&#36807;11%&#12290;
&lt;/p&gt;
&lt;p&gt;
Leukemia is one of the most common and death-threatening types of cancer that threaten human life. Medical data from some of the patient's critical parameters contain valuable information hidden among these data. On this subject, deep learning can be used to extract this information. In this paper, AutoEncoders have been used to develop valuable features to help the precision of leukemia diagnosis. It has been attempted to get the best activation function and optimizer to use in AutoEncoder and designed the best architecture for this neural network. The proposed architecture is compared with this area's classical machine learning models. Our proposed method performs better than other machine learning in precision and f1-score metrics by more than 11%.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#39046;&#22495;&#39640;&#32423;&#25512;&#29702;&#21644;&#25191;&#34892;&#20043;&#38388;&#30340;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00880</link><description>&lt;p&gt;
&#20026;&#24357;&#21512;&#26426;&#22120;&#20154;&#39640;&#32423;&#25512;&#29702;&#21644;&#25191;&#34892;&#20043;&#38388;&#30340;&#24046;&#36317;&#32780;&#21162;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Bridging the Gap between High-Level Reasoning and Execution on Robots. (arXiv:2401.00880v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#39046;&#22495;&#39640;&#32423;&#25512;&#29702;&#21644;&#25191;&#34892;&#20043;&#38388;&#30340;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#36827;&#34892;&#21160;&#20316;&#25512;&#29702;&#26102;&#65292;&#20363;&#22914;&#36890;&#36807;&#20219;&#21153;&#35268;&#21010;&#25110;&#20351;&#29992;Golog&#36827;&#34892;&#20195;&#29702;&#32534;&#31243;&#26102;&#65292;&#36890;&#24120;&#20250;&#23558;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#24314;&#27169;&#20026;&#25277;&#35937;&#23618;&#27425;&#65292;&#20854;&#20013;&#22797;&#26434;&#30340;&#21160;&#20316;&#65288;&#22914;&#25342;&#21462;&#29289;&#20307;&#65289;&#34987;&#35270;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#25928;&#26524;&#21644;&#20165;&#20381;&#36182;&#20110;&#24403;&#21069;&#29366;&#24577;&#30340;&#21069;&#25552;&#26465;&#20214;&#30340;&#21407;&#23376;&#22522;&#20803;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#26426;&#22120;&#20154;&#19978;&#25191;&#34892;&#27492;&#31867;&#21160;&#20316;&#26102;&#65292;&#23427;&#19981;&#20877;&#21487;&#20197;&#34987;&#35270;&#20026;&#21407;&#23376;&#22522;&#20803;&#12290;&#30456;&#21453;&#65292;&#21160;&#20316;&#25191;&#34892;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#22810;&#20010;&#27493;&#39588;&#65292;&#20855;&#26377;&#39069;&#22806;&#30340;&#26102;&#38388;&#21069;&#25552;&#26465;&#20214;&#21644;&#26102;&#38388;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#21160;&#20316;&#21487;&#33021;&#20250;&#20135;&#29983;&#22122;&#22768;&#65292;&#20363;&#22914;&#20135;&#29983;&#38169;&#35823;&#30340;&#24863;&#30693;&#32467;&#26524;&#65292;&#24182;&#19988;&#19981;&#33021;&#24635;&#26159;&#36798;&#21040;&#26399;&#26395;&#30340;&#25928;&#26524;&#12290;&#23613;&#31649;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#36890;&#24120;&#24573;&#30053;&#36825;&#20123;&#26041;&#38754;&#65292;&#20294;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#38656;&#35201;&#22788;&#29702;&#23427;&#20204;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
When reasoning about actions, e.g., by means of task planning or agent programming with Golog, the robot's actions are typically modeled on an abstract level, where complex actions such as picking up an object are treated as atomic primitives with deterministic effects and preconditions that only depend on the current state. However, when executing such an action on a robot it can no longer be seen as a primitive. Instead, action execution is a complex task involving multiple steps with additional temporal preconditions and timing constraints. Furthermore, the action may be noisy, e.g., producing erroneous sensing results and not always having the desired effects. While these aspects are typically ignored in reasoning tasks, they need to be dealt with during execution. In this thesis, we propose several approaches towards closing this gap.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bargrain&#30340;&#24179;&#34913;&#33041;&#22270;&#32467;&#26500;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#27169;&#25311;&#32463;&#36807;&#28388;&#27874;&#30340;&#30456;&#20851;&#30697;&#38453;&#21644;&#26368;&#20248;&#26679;&#26412;&#22270;&#26469;&#25913;&#36827;&#33041;&#30142;&#30149;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20165;&#20381;&#36182;&#21333;&#19968;&#31867;&#22411;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.00876</link><description>&lt;p&gt;
&#24179;&#34913;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#29992;&#20110;&#33041;&#30142;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Balanced Graph Structure Information for Brain Disease Detection. (arXiv:2401.00876v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00876
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bargrain&#30340;&#24179;&#34913;&#33041;&#22270;&#32467;&#26500;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#27169;&#25311;&#32463;&#36807;&#28388;&#27874;&#30340;&#30456;&#20851;&#30697;&#38453;&#21644;&#26368;&#20248;&#26679;&#26412;&#22270;&#26469;&#25913;&#36827;&#33041;&#30142;&#30149;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20165;&#20381;&#36182;&#21333;&#19968;&#31867;&#22411;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#33041;&#21306;&#38388;&#30340;&#36830;&#25509;&#23545;&#20110;&#26816;&#27979;&#33258;&#38381;&#30151;&#25110;&#31934;&#31070;&#20998;&#35010;&#31561;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26469;&#21033;&#29992;&#33041;&#20013;&#30340;&#22270;&#32467;&#26500;&#65292;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;ROI&#30340;&#34880;&#27687;&#27700;&#24179;&#20381;&#36182;&#24615;(BOLD)&#20449;&#21495;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#29983;&#25104;&#22270;&#32467;&#26500;&#12290;&#20854;&#20182;&#26041;&#27861;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#20351;&#29992;&#35757;&#32451;&#26679;&#26412;&#26469;&#23398;&#20064;&#26368;&#20248;&#30340;&#22270;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#29420;&#31435;&#23454;&#26045;&#36825;&#20123;&#26041;&#27861;&#20250;&#23548;&#33268;&#30456;&#20851;&#24615;&#22270;&#20013;&#30340;&#22122;&#38899;&#25968;&#25454;&#38382;&#39064;&#20197;&#21450;&#26368;&#20248;&#22270;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bargrain(&#24179;&#34913;&#30340;&#33041;&#22270;&#32467;&#26500;)&#65292;&#23427;&#27169;&#25311;&#20102;&#20004;&#31181;&#22270;&#32467;&#26500;&#65306;&#32463;&#36807;&#28388;&#27874;&#30340;&#30456;&#20851;&#30697;&#38453;&#21644;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;(GCNs)&#29983;&#25104;&#30340;&#26368;&#20248;&#26679;&#26412;&#22270;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#20004;&#31181;&#22270;&#30340;&#20248;&#28857;&#65292;&#24182;&#35299;&#20915;&#20165;&#20381;&#36182;&#21333;&#19968;&#31867;&#22411;&#32467;&#26500;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing connections between brain regions of interest (ROI) is vital to detect neurological disorders such as autism or schizophrenia. Recent advancements employ graph neural networks (GNNs) to utilize graph structures in brains, improving detection performances. Current methods use correlation measures between ROI's blood-oxygen-level-dependent (BOLD) signals to generate the graph structure. Other methods use the training samples to learn the optimal graph structure through end-to-end learning. However, implementing those methods independently leads to some issues with noisy data for the correlation graphs and overfitting problems for the optimal graph. In this work, we proposed Bargrain (balanced graph structure for brains), which models two graph structures: filtered correlation matrix and optimal sample graph using graph convolution networks (GCNs). This approach aims to get advantages from both graphs and address the limitations of only relying on a single type of structure. Bas
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Prompt2Forget&#65288;P2F&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24536;&#35760;&#38544;&#31169;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;LLM&#26412;&#22320;&#38544;&#31169;&#25361;&#25112;&#12290;P2F&#26041;&#27861;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#29255;&#27573;&#24182;&#29983;&#25104;&#34394;&#26500;&#31572;&#26696;&#65292;&#27169;&#31946;&#21270;&#27169;&#22411;&#23545;&#21407;&#22987;&#36755;&#20837;&#30340;&#35760;&#24518;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;P2F&#20855;&#26377;&#24456;&#24378;&#30340;&#27169;&#31946;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#19979;&#33258;&#36866;&#24212;&#20351;&#29992;&#65292;&#26080;&#38656;&#25163;&#21160;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2401.00870</link><description>&lt;p&gt;
&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24536;&#35760;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Teach Large Language Models to Forget Privacy. (arXiv:2401.00870v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Prompt2Forget&#65288;P2F&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24536;&#35760;&#38544;&#31169;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;LLM&#26412;&#22320;&#38544;&#31169;&#25361;&#25112;&#12290;P2F&#26041;&#27861;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#29255;&#27573;&#24182;&#29983;&#25104;&#34394;&#26500;&#31572;&#26696;&#65292;&#27169;&#31946;&#21270;&#27169;&#22411;&#23545;&#21407;&#22987;&#36755;&#20837;&#30340;&#35760;&#24518;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;P2F&#20855;&#26377;&#24456;&#24378;&#30340;&#27169;&#31946;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#19979;&#33258;&#36866;&#24212;&#20351;&#29992;&#65292;&#26080;&#38656;&#25163;&#21160;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20445;&#25252;&#38544;&#31169;&#26041;&#27861;&#65292;&#22914;&#24046;&#20998;&#38544;&#31169;&#21644;&#21516;&#24577;&#21152;&#23494;&#65292;&#22312;&#21482;&#26377;&#40657;&#30418;API&#30340;&#29615;&#22659;&#19979;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#35201;&#27714;&#27169;&#22411;&#36879;&#26126;&#24615;&#25110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt2Forget&#65288;P2F&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;LLM&#26412;&#22320;&#38544;&#31169;&#25361;&#25112;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#23548;LLM&#24536;&#35760;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#23436;&#25972;&#38382;&#39064;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#29255;&#27573;&#65292;&#29983;&#25104;&#34394;&#26500;&#30340;&#31572;&#26696;&#65292;&#24182;&#20351;&#27169;&#22411;&#23545;&#21407;&#22987;&#36755;&#20837;&#30340;&#35760;&#24518;&#27169;&#31946;&#21270;&#12290;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#39046;&#22495;&#30340;&#21253;&#21547;&#38544;&#31169;&#25935;&#24863;&#20449;&#24687;&#30340;&#38382;&#39064;&#21019;&#24314;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;P2F&#23454;&#29616;&#20102;&#38646;-shot&#27867;&#21270;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#19979;&#33258;&#36866;&#24212;&#65292;&#26080;&#38656;&#25163;&#21160;&#35843;&#25972;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;P2F&#20855;&#26377;&#24456;&#24378;&#30340;&#27169;&#31946;&#21270;LLM&#35760;&#24518;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#20219;&#20309;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven powerful, but the risk of privacy leakage remains a significant concern. Traditional privacy-preserving methods, such as Differential Privacy and Homomorphic Encryption, are inadequate for black-box API-only settings, demanding either model transparency or heavy computational resources. We propose Prompt2Forget (P2F), the first framework designed to tackle the LLM local privacy challenge by teaching LLM to forget. The method involves decomposing full questions into smaller segments, generating fabricated answers, and obfuscating the model's memory of the original input. A benchmark dataset was crafted with questions containing privacy-sensitive information from diverse fields. P2F achieves zero-shot generalization, allowing adaptability across a wide range of use cases without manual adjustments. Experimental results indicate P2F's robust capability to obfuscate LLM's memory, attaining a forgetfulness score of around 90\% without any utility los
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#32593;&#32476;&#21487;&#20197;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#65292;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65292;&#22312;&#24615;&#33021;&#19978;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#25552;&#21462;&#29305;&#24449;&#27010;&#29575;&#12289;&#29109;&#21644;&#20114;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#24322;&#24120;&#30340;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2401.00867</link><description>&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#22312;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tensor Networks for Explainable Machine Learning in Cybersecurity. (arXiv:2401.00867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00867
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#21487;&#20197;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#65292;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65292;&#22312;&#24615;&#33021;&#19978;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#25552;&#21462;&#29305;&#24449;&#27010;&#29575;&#12289;&#29109;&#21644;&#20114;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#24322;&#24120;&#30340;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#22914;&#20309;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65288;MPS&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#23545;&#25163;&#29983;&#25104;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;MPS&#22312;&#24615;&#33021;&#26041;&#38754;&#21487;&#20197;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#28982;&#22320;&#20419;&#36827;&#20102;&#29305;&#24449;&#27010;&#29575;&#12289;&#20911;&#183;&#35834;&#20234;&#26364;&#29109;&#21644;&#20114;&#20449;&#24687;&#30340;&#25552;&#21462;&#65292;&#20026;&#24322;&#24120;&#20998;&#31867;&#25552;&#20379;&#20102;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#20419;&#36827;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#22522;&#26412;&#21407;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we show how tensor networks help in developing explainability of machine learning algorithms. Specifically, we develop an unsupervised clustering algorithm based on Matrix Product States (MPS) and apply it in the context of a real use-case of adversary-generated threat intelligence. Our investigation proves that MPS rival traditional deep learning models such as autoencoders and GANs in terms of performance, while providing much richer model interpretability. Our approach naturally facilitates the extraction of feature-wise probabilities, Von Neumann Entropy, and mutual information, offering a compelling narrative for classification of anomalies and fostering an unprecedented level of transparency and interpretability, something fundamental to understand the rationale behind artificial intelligence decisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#30693;&#35782;&#22788;&#29702;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#29123;&#28903;&#31185;&#23398;&#20013;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;RAG&#26694;&#26550;&#65292;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#29123;&#28903;&#30740;&#31350;&#25968;&#25454;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#35745;&#31639;&#21644;&#32463;&#27982;&#24320;&#38144;&#65292;&#21516;&#26102;&#20248;&#21270;&#25968;&#25454;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.00544</link><description>&lt;p&gt;
&#28779;&#29123;&#31185;&#23398;&#20013;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#21487;&#38752;&#30693;&#35782;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Reliable Knowledge Processing Framework for Combustion Science using Foundation Models. (arXiv:2401.00544v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#30693;&#35782;&#22788;&#29702;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#29123;&#28903;&#31185;&#23398;&#20013;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;RAG&#26694;&#26550;&#65292;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#29123;&#28903;&#30740;&#31350;&#25968;&#25454;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#35745;&#31639;&#21644;&#32463;&#27982;&#24320;&#38144;&#65292;&#21516;&#26102;&#20248;&#21270;&#25968;&#25454;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#31185;&#23398;&#25968;&#25454;&#34701;&#21512;&#20013;&#65292;&#20197;&#29123;&#28903;&#31185;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#36890;&#36807;&#25972;&#21512;&#22522;&#30784;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#26679;&#21270;&#29123;&#28903;&#30740;&#31350;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#23454;&#39564;&#30740;&#31350;&#12289;&#27169;&#25311;&#21644;&#25991;&#29486;&#31561;&#26041;&#38754;&#12290;&#29123;&#28903;&#30740;&#31350;&#30340;&#22810;&#26041;&#38754;&#24615;&#24378;&#35843;&#20102;&#30693;&#35782;&#22788;&#29702;&#22312;&#20174;&#20016;&#23500;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#26469;&#28304;&#20013;&#23548;&#33322;&#21644;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25152;&#24320;&#21457;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#25968;&#25454;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#35745;&#31639;&#21644;&#32463;&#27982;&#24320;&#38144;&#12290;&#23427;&#21253;&#25324;&#25552;&#31034;&#24037;&#31243;&#21644;&#31163;&#32447;&#24320;&#28304;LLMs&#65292;&#20026;&#29992;&#25143;&#36873;&#25321;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#33258;&#20027;&#24615;&#12290;&#26412;&#30740;&#31350;&#23545;&#25991;&#26412;&#20998;&#21106;&#31574;&#30053;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#36827;&#34892;&#20102;LLMs&#20043;&#38388;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#24182;&#25506;&#32034;&#20102;&#21508;&#31181;&#20248;&#21270;&#30340;&#25552;&#31034;&#26041;&#24335;&#65292;&#20197;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research explores the integration of large language models (LLMs) into scientific data assimilation, focusing on combustion science as a case study. Leveraging foundational models integrated with Retrieval-Augmented Generation (RAG) framework, the study introduces an approach to process diverse combustion research data, spanning experimental studies, simulations, and literature. The multifaceted nature of combustion research emphasizes the critical role of knowledge processing in navigating and extracting valuable information from a vast and diverse pool of sources. The developed approach minimizes computational and economic expenses while optimizing data privacy and accuracy. It incorporates prompt engineering and offline open-source LLMs, offering user autonomy in selecting base models. The study provides a thorough examination of text segmentation strategies, conducts comparative studies between LLMs, and explores various optimized prompts to demonstrate the effectiveness of th
&lt;/p&gt;</description></item><item><title>AVRE&#26159;&#19968;&#31181;&#29992;&#20110;&#24418;&#24335;&#39564;&#35777;NextG&#36890;&#20449;&#21327;&#35758;&#30340;&#33258;&#21160;&#24314;&#27169;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#21327;&#35758;&#25551;&#36848;&#20026;&#20381;&#36182;&#22270;&#21644;&#24418;&#24335;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#24314;&#31435;&#21487;&#37327;&#21270;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#22797;&#26434;&#36890;&#20449;&#21327;&#35758;&#30340;&#39564;&#35777;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.17353</link><description>&lt;p&gt;
&#36808;&#21521;NextG&#21327;&#35758;&#24418;&#24335;&#39564;&#35777;&#30340;&#33258;&#21160;&#24314;&#27169;&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Auto-Modeling of Formal Verification for NextG Protocols: A Multimodal cross- and self-attention Large Language Model Approach. (arXiv:2312.17353v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17353
&lt;/p&gt;
&lt;p&gt;
AVRE&#26159;&#19968;&#31181;&#29992;&#20110;&#24418;&#24335;&#39564;&#35777;NextG&#36890;&#20449;&#21327;&#35758;&#30340;&#33258;&#21160;&#24314;&#27169;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#21327;&#35758;&#25551;&#36848;&#20026;&#20381;&#36182;&#22270;&#21644;&#24418;&#24335;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#24314;&#31435;&#21487;&#37327;&#21270;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#22797;&#26434;&#36890;&#20449;&#21327;&#35758;&#30340;&#39564;&#35777;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Auto-modeling of Formal Verification with Real-world Prompting for 5G and NextG protocols&#65288;AVRE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#35774;&#35745;&#29992;&#20110;&#24418;&#24335;&#39564;&#35777;Next Generation&#65288;NextG&#65289;&#36890;&#20449;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#21327;&#35758;&#35774;&#35745;&#21644;&#39564;&#35777;&#20013;&#26085;&#30410;&#22797;&#26434;&#21644;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;AVRE&#23558;&#21327;&#35758;&#25551;&#36848;&#36716;&#21270;&#20026;&#20381;&#36182;&#22270;&#21644;&#24418;&#24335;&#27169;&#22411;&#65292;&#39640;&#25928;&#22320;&#35299;&#20915;&#20102;&#27495;&#20041;&#38382;&#39064;&#24182;&#25429;&#25417;&#35774;&#35745;&#24847;&#22270;&#12290;&#31995;&#32479;&#36890;&#36807;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;&#21464;&#21387;&#22120;&#27169;&#22411;&#19982;LLM&#38598;&#25104;&#65292;&#33258;&#20027;&#24314;&#31435;&#21487;&#37327;&#21270;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#32463;&#36807;HyFuzz&#23454;&#39564;&#24179;&#21488;&#30340;&#36845;&#20195;&#21453;&#39304;&#65292;AVRE&#26174;&#33879;&#25552;&#39640;&#20102;&#22797;&#26434;&#36890;&#20449;&#21327;&#35758;&#27491;&#24335;&#39564;&#35777;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20026;&#39564;&#35777;&#22797;&#26434;&#30340;&#36890;&#20449;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;CAL&#30340;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;LL&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Auto-modeling of Formal Verification with Real-world Prompting for 5G and NextG protocols (AVRE), a novel system designed for the formal verification of Next Generation (NextG) communication protocols, addressing the increasing complexity and scalability challenges in network protocol design and verification. Utilizing Large Language Models (LLMs), AVRE transforms protocol descriptions into dependency graphs and formal models, efficiently resolving ambiguities and capturing design intent. The system integrates a transformer model with LLMs to autonomously establish quantifiable dependency relationships through cross- and self-attention mechanisms. Enhanced by iterative feedback from the HyFuzz experimental platform, AVRE significantly advances the accuracy and relevance of formal verification in complex communication protocols, offering a groundbreaking approach to validating sophisticated communication systems. We compare CAL's performance with state-of-the-art L
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20869;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#26426;&#22120;&#20154;&#30340;&#21709;&#24212;&#26469;&#20272;&#35745;&#22806;&#37096;&#29366;&#24577;&#65292;&#36825;&#23545;&#20110;&#20581;&#22766;&#30340;&#36816;&#21160;&#25511;&#21046;&#38750;&#24120;&#37325;&#35201;&#12290;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#20248;&#21270;&#23884;&#20837;&#34920;&#31034;&#65292;&#20351;&#20854;&#25509;&#36817;&#26426;&#22120;&#20154;&#30340;&#21518;&#32487;&#29366;&#24577;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#26426;&#22120;&#20154;&#30340;&#22266;&#26377;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2312.11460</link><description>&lt;p&gt;
&#28151;&#21512;&#20869;&#27169;&#65306;&#36890;&#36807;&#27169;&#25311;&#26426;&#22120;&#20154;&#21709;&#24212;&#23398;&#20064;&#25935;&#25463;&#33151;&#37096;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;
Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated Robot Response. (arXiv:2312.11460v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20869;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#26426;&#22120;&#20154;&#30340;&#21709;&#24212;&#26469;&#20272;&#35745;&#22806;&#37096;&#29366;&#24577;&#65292;&#36825;&#23545;&#20110;&#20581;&#22766;&#30340;&#36816;&#21160;&#25511;&#21046;&#38750;&#24120;&#37325;&#35201;&#12290;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#20248;&#21270;&#23884;&#20837;&#34920;&#31034;&#65292;&#20351;&#20854;&#25509;&#36817;&#26426;&#22120;&#20154;&#30340;&#21518;&#32487;&#29366;&#24577;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#26426;&#22120;&#20154;&#30340;&#22266;&#26377;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#22766;&#30340;&#36816;&#21160;&#25511;&#21046;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#29366;&#24577;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#20256;&#24863;&#22120;&#21482;&#33021;&#25552;&#20379;&#37096;&#20998;&#21644;&#22024;&#26434;&#30340;&#35266;&#27979;&#65292;&#20351;&#24471;&#20272;&#35745;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22806;&#37096;&#29366;&#24577;&#65292;&#22914;&#22320;&#24418;&#25705;&#25830;&#21644;&#39640;&#31243;&#22270;&#12290;&#21463;&#32463;&#20856;&#30340;&#20869;&#27169;&#25511;&#21046;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#22806;&#37096;&#29366;&#24577;&#35270;&#20026;&#24178;&#25200;&#65292;&#24182;&#24341;&#20837;&#28151;&#21512;&#20869;&#27169;&#65288;HIM&#65289;&#26469;&#26681;&#25454;&#26426;&#22120;&#20154;&#30340;&#21709;&#24212;&#26469;&#20272;&#35745;&#23427;&#20204;&#12290;&#25105;&#20204;&#23558;&#21709;&#24212;&#31216;&#20026;&#28151;&#21512;&#20869;&#23884;&#34920;&#31034;&#65292;&#23427;&#21253;&#21547;&#20102;&#26426;&#22120;&#20154;&#30340;&#26174;&#24335;&#36895;&#24230;&#21644;&#38544;&#24335;&#31283;&#23450;&#24615;&#34920;&#31034;&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#36816;&#21160;&#20219;&#21153;&#30340;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#65306;&#26174;&#24335;&#36861;&#36394;&#36895;&#24230;&#21644;&#38544;&#24335;&#32500;&#25345;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#20248;&#21270;&#23884;&#20837;&#24335;&#34920;&#31034;&#65292;&#20351;&#20854;&#25509;&#36817;&#26426;&#22120;&#20154;&#30340;&#21518;&#32487;&#29366;&#24577;&#65292;&#20854;&#20013;&#33258;&#28982;&#23884;&#20837;&#20102;&#21709;&#24212;&#12290;HIM&#20855;&#26377;&#20960;&#20010;&#21560;&#24341;&#20154;&#30340;&#22909;&#22788;&#65306;&#23427;&#21482;&#38656;&#35201;&#26426;&#22120;&#20154;&#30340;&#22266;&#26377;&#24863;&#30693;&#65292;&#21363;&#20851;&#33410;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust locomotion control depends on accurate state estimations. However, the sensors of most legged robots can only provide partial and noisy observations, making the estimation particularly challenging, especially for external states like terrain frictions and elevation maps. Inspired by the classical Internal Model Control principle, we consider these external states as disturbances and introduce Hybrid Internal Model (HIM) to estimate them according to the response of the robot. The response, which we refer to as the hybrid internal embedding, contains the robot's explicit velocity and implicit stability representation, corresponding to two primary goals for locomotion tasks: explicitly tracking velocity and implicitly maintaining stability. We use contrastive learning to optimize the embedding to be close to the robot's successor state, in which the response is naturally embedded. HIM has several appealing benefits: It only needs the robot's proprioceptions, i.e., those from joint
&lt;/p&gt;</description></item><item><title>&#20840;&#23616;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476; (GFPNet) &#26159;&#19968;&#20010;&#22686;&#24378;&#29256;&#26412;&#30340; PAFPN&#65292;&#36890;&#36807;&#25972;&#21512;&#20840;&#23616;&#20449;&#24687;&#26469;&#22686;&#24378;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#25429;&#33719;&#20840;&#23616;&#29305;&#24449;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;VNC&#32534;&#30721;&#22120;&#22788;&#29702;&#36825;&#20123;&#29305;&#24449;&#65292;GFPNet&#20855;&#26377;&#26174;&#30528;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#20943;&#23569;&#35823;&#26816;&#21644;&#28431;&#26816;&#12290;</title><link>http://arxiv.org/abs/2312.11231</link><description>&lt;p&gt;
&#20840;&#23616;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Global Feature Pyramid Network. (arXiv:2312.11231v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11231
&lt;/p&gt;
&lt;p&gt;
&#20840;&#23616;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476; (GFPNet) &#26159;&#19968;&#20010;&#22686;&#24378;&#29256;&#26412;&#30340; PAFPN&#65292;&#36890;&#36807;&#25972;&#21512;&#20840;&#23616;&#20449;&#24687;&#26469;&#22686;&#24378;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#25429;&#33719;&#20840;&#23616;&#29305;&#24449;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;VNC&#32534;&#30721;&#22120;&#22788;&#29702;&#36825;&#20123;&#29305;&#24449;&#65292;GFPNet&#20855;&#26377;&#26174;&#30528;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#20943;&#23569;&#35823;&#26816;&#21644;&#28431;&#26816;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#29305;&#24449;&#37329;&#23383;&#22612;&#22312;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#36807;&#20998;&#24378;&#35843;&#23618;&#38388;&#29305;&#24449;&#20132;&#20114;&#65292;&#24573;&#35270;&#20102;&#23618;&#20869;&#29305;&#24449;&#35843;&#25972;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#32463;&#39564;&#34920;&#26126;&#65292;&#23618;&#20869;&#29305;&#24449;&#20132;&#20114;&#22312;&#22686;&#24378;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#25110;&#35270;&#35273;&#21464;&#25442;&#22120;&#23398;&#20064;&#21387;&#32553;&#30340;&#23618;&#20869;&#29305;&#24449;&#34920;&#31034;&#65292;&#20294;&#21364;&#24573;&#35270;&#20102;&#20840;&#23616;&#20449;&#24687;&#20132;&#20114;&#30340;&#25972;&#21512;&#12290;&#36825;&#20010;&#30095;&#24573;&#23548;&#33268;&#20102;&#35823;&#26816;&#21644;&#28431;&#26816;&#30340;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#20840;&#23616;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;GFPNet&#65289;&#65292;&#23427;&#26159;PAFPN&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#36890;&#36807;&#25972;&#21512;&#20840;&#23616;&#20449;&#24687;&#26469;&#22686;&#24378;&#30446;&#26631;&#26816;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#36731;&#37327;&#32423;MLP&#26469;&#25429;&#33719;&#20840;&#23616;&#29305;&#24449;&#20449;&#24687;&#65292;&#21033;&#29992;VNC&#32534;&#30721;&#22120;&#22788;&#29702;&#36825;&#20123;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#24182;&#34892;lea
&lt;/p&gt;
&lt;p&gt;
The visual feature pyramid has proven its effectiveness and efficiency in target detection tasks. Yet, current methodologies tend to overly emphasize inter-layer feature interaction, neglecting the crucial aspect of intra-layer feature adjustment. Experience underscores the significant advantages of intra-layer feature interaction in enhancing target detection tasks. While some approaches endeavor to learn condensed intra-layer feature representations using attention mechanisms or visual transformers, they overlook the incorporation of global information interaction. This oversight results in increased false detections and missed targets.To address this critical issue, this paper introduces the Global Feature Pyramid Network (GFPNet), an augmented version of PAFPN that integrates global information for enhanced target detection. Specifically, we leverage a lightweight MLP to capture global feature information, utilize the VNC encoder to process these features, and employ a parallel lea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MusER&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#38899;&#20048;&#20803;&#32032;&#30340;&#27491;&#21017;&#21270;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#38899;&#20048;&#20803;&#32032;&#22312;&#24773;&#24863;&#20013;&#30340;&#20316;&#29992;&#30340;&#21078;&#26512;&#65292;&#24182;&#36827;&#19968;&#27493;&#25805;&#32437;&#20803;&#32032;&#26469;&#25913;&#21464;&#38899;&#20048;&#30340;&#24773;&#24863;&#12290;&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#23545;&#24773;&#24863;&#38899;&#20048;&#29983;&#25104;&#20013;&#38899;&#20048;&#20803;&#32032;&#36129;&#29486;&#30340;&#19981;&#36275;&#65292;&#24182;&#20026;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#25511;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.10307</link><description>&lt;p&gt;
MusER&#65306;&#22522;&#20110;&#38899;&#20048;&#20803;&#32032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#29992;&#20110;&#20135;&#29983;&#20855;&#26377;&#24773;&#24863;&#30340;&#31526;&#21495;&#38899;&#20048;
&lt;/p&gt;
&lt;p&gt;
MusER: Musical Element-Based Regularization for Generating Symbolic Music with Emotion. (arXiv:2312.10307v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MusER&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#38899;&#20048;&#20803;&#32032;&#30340;&#27491;&#21017;&#21270;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#38899;&#20048;&#20803;&#32032;&#22312;&#24773;&#24863;&#20013;&#30340;&#20316;&#29992;&#30340;&#21078;&#26512;&#65292;&#24182;&#36827;&#19968;&#27493;&#25805;&#32437;&#20803;&#32032;&#26469;&#25913;&#21464;&#38899;&#20048;&#30340;&#24773;&#24863;&#12290;&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#23545;&#24773;&#24863;&#38899;&#20048;&#29983;&#25104;&#20013;&#38899;&#20048;&#20803;&#32032;&#36129;&#29486;&#30340;&#19981;&#36275;&#65292;&#24182;&#20026;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#25511;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#29983;&#20855;&#26377;&#24773;&#24863;&#30340;&#38899;&#20048;&#26159;&#33258;&#21160;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#24773;&#24863;&#36890;&#36807;&#21508;&#31181;&#38543;&#26102;&#38388;&#21464;&#21270;&#24182;&#30456;&#20114;&#21327;&#20316;&#30340;&#38899;&#20048;&#20803;&#32032;&#65288;&#22914;&#38899;&#39640;&#21644;&#25345;&#32493;&#26102;&#38388;&#65289;&#26469;&#21796;&#36215;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24773;&#24863;&#38899;&#20048;&#29983;&#25104;&#30740;&#31350;&#24456;&#23569;&#25506;&#32034;&#19981;&#21516;&#38899;&#20048;&#20803;&#32032;&#23545;&#24773;&#24863;&#30340;&#36129;&#29486;&#65292;&#26356;&#19981;&#29992;&#35828;&#26377;&#24847;&#35782;&#22320;&#25805;&#32437;&#36825;&#20123;&#20803;&#32032;&#26469;&#25913;&#21464;&#38899;&#20048;&#30340;&#24773;&#24863;&#20102;&#65292;&#36825;&#19981;&#21033;&#20110;&#23545;&#24773;&#24863;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#20803;&#32032;&#32423;&#25511;&#21046;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#22522;&#20110;&#38899;&#20048;&#20803;&#32032;&#30340;&#27491;&#21017;&#21270;&#22788;&#29702;&#22312;&#28508;&#21464;&#37327;&#31354;&#38388;&#20013;&#35299;&#32806;&#19981;&#21516;&#30340;&#20803;&#32032;&#65292;&#25506;&#31350;&#23427;&#20204;&#22312;&#21306;&#20998;&#24773;&#24863;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#36827;&#19968;&#27493;&#25805;&#32437;&#20803;&#32032;&#26469;&#25913;&#21464;&#38899;&#20048;&#30340;&#24773;&#24863;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MusER&#30340;&#22522;&#20110;VQ-VAE&#30340;&#27169;&#22411;&#12290;MusER&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#25439;&#22833;&#65292;&#26469;&#32422;&#26463;&#38899;&#20048;&#20803;&#32032;&#24207;&#21015;&#19982;&#29305;&#23450;&#24773;&#24863;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating music with emotion is an important task in automatic music generation, in which emotion is evoked through a variety of musical elements (such as pitch and duration) that change over time and collaborate with each other. However, prior research on deep learning-based emotional music generation has rarely explored the contribution of different musical elements to emotions, let alone the deliberate manipulation of these elements to alter the emotion of music, which is not conducive to fine-grained element-level control over emotions. To address this gap, we present a novel approach employing musical element-based regularization in the latent space to disentangle distinct elements, investigate their roles in distinguishing emotions, and further manipulate elements to alter musical emotions. Specifically, we propose a novel VQ-VAE-based model named MusER. MusER incorporates a regularization loss to enforce the correspondence between the musical element sequences and the specific 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#19968;GPU&#19978;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#19988;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2312.10144</link><description>&lt;p&gt;
&#21333;&#19968;GPU&#19978;&#30340;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Multimodal Fusion on a Single GPU. (arXiv:2312.10144v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#19968;GPU&#19978;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#19988;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#20849;&#20139;&#22810;&#27169;&#24577;&#36755;&#20837;&#20043;&#38388;&#30340;&#21333;&#19968;&#28508;&#22312;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#29616;&#26377;&#30340;&#22312;&#22823;&#37327;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#24212;&#35813;&#33021;&#22815;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#20174;&#21333;&#27169;&#24577;&#27169;&#22411;&#20013;&#21019;&#24314;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FuseMix&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#22686;&#24378;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#20219;&#24847;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25805;&#20316;&#12290;&#36890;&#36807;&#20351;&#29992;FuseMix&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#40784;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21644;&#38899;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#22312;Flickr30K&#30340;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#20013;&#27604;CLIP&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#32422;600&#20493;&#65292;&#32780;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of multimodal alignment is to learn a single latent space that is shared between multimodal inputs. The most powerful models in this space have been trained using massive datasets of paired inputs and large-scale computational resources, making them prohibitively expensive to train in many practical scenarios. We surmise that existing unimodal encoders pre-trained on large amounts of unimodal data should provide an effective bootstrap to create multimodal models from unimodal ones at much lower costs. We therefore propose FuseMix, a multimodal augmentation scheme that operates on the latent spaces of arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal alignment, we achieve competitive performance -- and in certain cases outperform state-of-the art methods -- in both image-text and audio-text retrieval, with orders of magnitude less compute and data: for example, we outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \! 600\times$ fewer GP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20197;&#33258;&#25105;&#23637;&#31034;&#29702;&#35770;&#20026;&#25351;&#23548;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#21382;&#21490;&#23545;&#35805;&#20998;&#25104;&#21512;&#29702;&#21644;&#29702;&#24615;&#30340;&#21477;&#23376;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#38416;&#26126;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#22686;&#24378;&#21516;&#29702;&#24515;&#22238;&#24212;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.08702</link><description>&lt;p&gt;
&#29702;&#24615;&#24773;&#24863;&#65306;&#20197;&#33258;&#25105;&#23637;&#31034;&#29702;&#35770;&#20026;&#25351;&#23548;&#30340;&#22686;&#24378;&#22411;&#21516;&#29702;&#24515;&#22238;&#24212;&#29983;&#25104;&#30340;LLM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rational Sensibility: LLM Enhanced Empathetic Response Generation Guided by Self-presentation Theory. (arXiv:2312.08702v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20197;&#33258;&#25105;&#23637;&#31034;&#29702;&#35770;&#20026;&#25351;&#23548;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#21382;&#21490;&#23545;&#35805;&#20998;&#25104;&#21512;&#29702;&#21644;&#29702;&#24615;&#30340;&#21477;&#23376;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#38416;&#26126;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#22686;&#24378;&#21516;&#29702;&#24515;&#22238;&#24212;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#20934;&#30830;&#34920;&#36798;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#23545;&#20110;&#21516;&#29702;&#24515;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#23545;&#35805;&#26412;&#36523;&#30340;&#29702;&#24615;&#34920;&#36798;&#21644;&#21512;&#29702;&#30340;&#34920;&#29616;&#26041;&#38754;&#65292;&#21364;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#65292;&#32780;&#36825;&#20123;&#26159;&#35748;&#30693;&#21516;&#29702;&#24515;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#31038;&#20250;&#23398;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#33258;&#25105;&#23637;&#31034;&#29702;&#35770;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#21382;&#21490;&#23545;&#35805;&#20998;&#25104;&#21512;&#29702;&#21644;&#29702;&#24615;&#30340;&#21477;&#23376;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#38416;&#26126;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#23545;&#35805;&#20013;&#30340;&#29702;&#24615;&#20449;&#24687;&#21463;&#21040;&#38480;&#21046;&#65292;&#24182;&#19988;&#20808;&#21069;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#22806;&#37096;&#30693;&#35782;&#23384;&#22312;&#35821;&#20041;&#30683;&#30462;&#21644;&#29421;&#31364;&#35270;&#37326;&#30340;&#38480;&#21046;&#12290;&#32771;&#34385;&#21040;LLM&#22312;&#26234;&#33021;&#20195;&#29702;&#39046;&#22495;&#30340;&#21331;&#36234;&#34920;&#29616;&#65292;&#25105;&#20204;&#37319;&#29992;LLaMA2-70b&#20316;&#20026;&#29702;&#24615;&#22823;&#33041;&#26469;&#20998;&#26512;&#28145;&#36828;&#30340;&#36923;&#36753;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Having the ability to empathize is crucial for accurately representing human behavior during conversations. Despite numerous research aim to improve the cognitive capability of models by incorporating external knowledge, there has been limited attention on the sensible and rational expression of the conversation itself, which are crucial components of the cognitive empathy. Guided by self-presentation theory in sociology, we have designed an innovative categorical approach that segregates historical dialogues into sensible and rational sentences and subsequently elucidate the context through the designed attention mechanism. However, the rational information within the conversation is restricted and the external knowledge used in previous methods have limitations of semantic contradiction and narrow vision field. Considering the impressive performance of LLM in the domain of intelligent agent. We employ LLaMA2-70b as a rational brain to analyze the profound logical information maintain
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#35270;&#35273;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#25216;&#26415;&#65292;&#25351;&#20986;&#20854;&#23545;&#26080;&#20154;&#26426;&#25805;&#20316;&#33021;&#21147;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#23398;&#20064;&#30340;&#25511;&#21046;&#26041;&#27861;&#21644;&#21508;&#31181;&#24212;&#29992;&#30340;&#25361;&#25112;&#19982;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2312.05019</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Vision-based Learning for Drones: A Survey. (arXiv:2312.05019v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#35270;&#35273;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#25216;&#26415;&#65292;&#25351;&#20986;&#20854;&#23545;&#26080;&#20154;&#26426;&#25805;&#20316;&#33021;&#21147;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#23398;&#20064;&#30340;&#25511;&#21046;&#26041;&#27861;&#21644;&#21508;&#31181;&#24212;&#29992;&#30340;&#25361;&#25112;&#19982;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20808;&#36827;&#30340;&#26234;&#33021;&#29289;&#29702;&#31995;&#32479;&#65292;&#26080;&#20154;&#26426;&#27491;&#22312;&#32463;&#21382;&#19968;&#27425;&#36716;&#22411;&#21464;&#38761;&#65292;&#37319;&#29992;&#22522;&#20110;&#35270;&#35273;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#36825;&#20010;&#39046;&#22495;&#22240;&#20854;&#23545;&#26080;&#20154;&#26426;&#33258;&#20027;&#24615;&#21644;&#21151;&#33021;&#30340;&#28145;&#36828;&#24433;&#21709;&#32780;&#36805;&#36895;&#23835;&#36215;&#12290;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#29305;&#23450;&#20219;&#21153;&#35843;&#26597;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#27010;&#36848;&#20102;&#26080;&#20154;&#26426;&#20013;&#22522;&#20110;&#35270;&#35273;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#20854;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#25552;&#39640;&#26080;&#20154;&#26426;&#25805;&#20316;&#33021;&#21147;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#38416;&#26126;&#20102;&#22522;&#20110;&#35270;&#35273;&#23398;&#20064;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#31361;&#20986;&#20102;&#23427;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#26080;&#20154;&#26426;&#30340;&#35270;&#35273;&#24863;&#30693;&#21644;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#20174;&#24863;&#30693;-&#25511;&#21046;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#35270;&#35273;&#30340;&#25511;&#21046;&#26041;&#27861;&#20998;&#20026;&#38388;&#25509;&#12289;&#21322;&#30452;&#25509;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#20855;&#26377;&#35270;&#35273;&#23398;&#20064;&#33021;&#21147;&#30340;&#26080;&#20154;&#26426;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#20174;&#21333;&#20010;&#26234;&#33021;&#20307;&#31995;&#32479;&#21040;&#26356;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#21644;&#24322;&#26500;&#31995;&#32479;&#30340;&#22330;&#26223;&#65292;&#24182;&#24378;&#35843;&#20102;&#25361;&#25112;&#19982;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drones as advanced cyber-physical systems are undergoing a transformative shift with the advent of vision-based learning, a field that is rapidly gaining prominence due to its profound impact on drone autonomy and functionality. Different from existing task-specific surveys, this review offers a comprehensive overview of vision-based learning in drones, emphasizing its pivotal role in enhancing their operational capabilities under various scenarios. We start by elucidating the fundamental principles of vision-based learning, highlighting how it significantly improves drones' visual perception and decision-making processes. We then categorize vision-based control methods into indirect, semi-direct, and end-to-end approaches from the perception-control perspective. We further explore various applications of vision-based drones with learning capabilities, ranging from single-agent systems to more complex multi-agent and heterogeneous system scenarios, and underscore the challenges and inn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#30340;&#26657;&#20934;&#20250;&#20808;&#22686;&#21152;&#32780;&#21518;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#20027;&#35201;&#20986;&#29616;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#31561;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#25552;&#31034;&#38656;&#35201;&#38024;&#23545;&#21487;&#38752;&#24615;&#22330;&#26223;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.04021</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26657;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Calibration of In-context Learning. (arXiv:2312.04021v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#30340;&#26657;&#20934;&#20250;&#20808;&#22686;&#21152;&#32780;&#21518;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#20027;&#35201;&#20986;&#29616;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#31561;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#25552;&#31034;&#38656;&#35201;&#38024;&#23545;&#21487;&#38752;&#24615;&#22330;&#26223;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#23433;&#20840;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#29616;&#20195;LMs&#26657;&#20934;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;LMs&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#22312;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#26368;&#21021;&#20250;&#20986;&#29616;&#22686;&#21152;&#30340;&#26657;&#20934;&#35823;&#24046;&#65292;&#28982;&#21518;&#25165;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#26657;&#20934;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#24448;&#24448;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20197;&#25552;&#39640;&#21487;&#29992;&#24615;&#20026;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#22914;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#65292;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#36825;&#34920;&#26126;&#22312;&#26399;&#26395;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#22330;&#26223;&#20013;&#21487;&#33021;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty quantification is crucial for the safe deployment of language models (LMs), and prior research has demonstrated improvements in the calibration of modern LMs. Our study focuses on in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examines the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations, suggesting that new methods may be required for scenarios where models are expected to be reliable.
&lt;/p&gt;</description></item><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;AI&#23545;&#40784;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;&#30740;&#31350;&#22260;&#32469;&#22235;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#23637;&#24320;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#20004;&#20010;&#37096;&#20998;&#12290; AI&#23545;&#40784;&#26159;&#20026;&#20102;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#65292;&#24182;&#20943;&#36731;&#30001;&#20110;&#31995;&#32479;&#19981;&#23545;&#40784;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.19852</link><description>&lt;p&gt;
AI&#23545;&#40784;: &#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
AI Alignment: A Comprehensive Survey. (arXiv:2310.19852v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;AI&#23545;&#40784;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;&#30740;&#31350;&#22260;&#32469;&#22235;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#23637;&#24320;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#20004;&#20010;&#37096;&#20998;&#12290; AI&#23545;&#40784;&#26159;&#20026;&#20102;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#65292;&#24182;&#20943;&#36731;&#30001;&#20110;&#31995;&#32479;&#19981;&#23545;&#40784;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#40784;&#26088;&#22312;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#12290;&#38543;&#30528;&#25317;&#26377;&#36229;&#20154;&#31867;&#33021;&#21147;&#30340;AI&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#38169;&#35823;&#23545;&#40784;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#28508;&#22312;&#22823;&#35268;&#27169;&#39118;&#38505;&#21464;&#24471;&#26126;&#26174;&#12290;&#25968;&#30334;&#21517;AI&#19987;&#23478;&#21644;&#20844;&#20247;&#20154;&#29289;&#37117;&#23545;AI&#39118;&#38505;&#34920;&#36798;&#20102;&#20851;&#27880;&#65292;&#35748;&#20026;&#20943;&#36731;AI&#24102;&#26469;&#30340;&#28781;&#32477;&#39118;&#38505;&#24212;&#35813;&#25104;&#20026;&#20840;&#29699;&#30340;&#20248;&#20808;&#20107;&#39033;&#65292;&#19982;&#22823;&#35268;&#27169;&#31038;&#20250;&#39118;&#38505;&#22914;&#22823;&#27969;&#34892;&#30149;&#21644;&#26680;&#25112;&#20105;&#24182;&#21015;&#12290;&#37492;&#20110;AI&#23545;&#40784;&#39046;&#22495;&#32570;&#20047;&#26368;&#26032;&#30340;&#31995;&#32479;&#35843;&#26597;&#65292;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#23545;&#40784;&#30740;&#31350;&#30340;&#26680;&#24515;&#27010;&#24565;&#12289;&#26041;&#27861;&#35770;&#21644;&#23454;&#36341;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#30446;&#26631;&#21407;&#21017;&#20316;&#20026;AI&#23545;&#40784;&#30340;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#65288;RICE&#65289;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#24403;&#21069;&#23545;&#40784;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#12290;&#21069;&#32773;&#26088;&#22312;&#20351;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI alignment aims to build AI systems that are in accordance with human intentions and values. With the emergence of AI systems possessing superhuman capabilities, the potential large-scale risks associated with misaligned systems become apparent. Hundreds of AI experts and public figures have expressed their concerns about AI risks, arguing that mitigating the risk of extinction from AI should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war. Motivated by the lack of an up-to-date systematic survey on AI alignment, in this paper, we delve into the core concepts, methodology, and practice of alignment research. To begin with, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). We outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned v
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#24180;&#40836;&#20272;&#35745;&#20013;&#30340;&#20869;&#23481;&#20559;&#24046;&#65292;&#24182;&#39564;&#35777;&#20102;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#20110;&#22270;&#20687;&#20869;&#23481;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#20943;&#36731;&#22270;&#20687;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#30340;&#23545;&#31574;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02067</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24180;&#40836;&#20272;&#35745;&#20013;&#30340;&#20869;&#23481;&#20559;&#24046;&#65306;&#26397;&#30528;&#26356;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Content Bias in Deep Learning Age Approximation: A new Approach Towards more Explainability. (arXiv:2310.02067v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#24180;&#40836;&#20272;&#35745;&#20013;&#30340;&#20869;&#23481;&#20559;&#24046;&#65292;&#24182;&#39564;&#35777;&#20102;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#20110;&#22270;&#20687;&#20869;&#23481;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#20943;&#36731;&#22270;&#20687;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#28508;&#22312;&#30340;&#23545;&#31574;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#22270;&#20687;&#21462;&#35777;&#30340;&#32972;&#26223;&#19979;&#65292;&#24456;&#38590;&#30830;&#23450;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20165;&#20165;&#21033;&#29992;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#36890;&#24120;&#65292;&#26102;&#38388;&#30456;&#36817;&#30340;&#22270;&#20687;&#65288;&#20363;&#22914;&#23646;&#20110;&#21516;&#19968;&#24180;&#40836;&#31867;&#21035;&#30340;&#65289;&#20855;&#26377;&#19968;&#20123;&#20849;&#21516;&#30340;&#20869;&#23481;&#23646;&#24615;&#12290;&#36825;&#31181;&#20869;&#23481;&#20559;&#24046;&#21487;&#20197;&#34987;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22270;&#20687;&#20869;&#23481;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#24102;&#26377;&#23884;&#20837;&#24335;&#24180;&#40836;&#20449;&#21495;&#30340;&#21512;&#25104;&#22270;&#20687;&#36827;&#34892;&#39564;&#35777;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#34920;&#26126;&#65292;&#22312;&#24180;&#40836;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#20013;&#35757;&#32451;&#30340;&#8220;&#26631;&#20934;&#8221;&#31070;&#32463;&#32593;&#32476;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22270;&#20687;&#20869;&#23481;&#12290;&#20316;&#20026;&#28508;&#22312;&#30340;&#23545;&#31574;&#65292;&#26412;&#25991;&#24212;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#20943;&#36731;&#35757;&#32451;&#36807;&#31243;&#20013;&#22270;&#20687;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of temporal image forensics, it is not evident that a neural network, trained on images from different time-slots (classes), exploit solely age related features. Usually, images taken in close temporal proximity (e.g., belonging to the same age class) share some common content properties. Such content bias can be exploited by a neural network. In this work, a novel approach that evaluates the influence of image content is proposed. This approach is verified using synthetic images (where content bias can be ruled out) with an age signal embedded. Based on the proposed approach, it is shown that a `standard' neural network trained in the context of age classification is strongly dependent on image content. As a potential countermeasure, two different techniques are applied to mitigate the influence of the image content during training, and they are also evaluated by the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.16741</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#36890;&#36807;&#28508;&#31354;&#38388;&#25237;&#24433;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections. (arXiv:2309.16741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20844;&#21496;&#36890;&#24120;&#22788;&#29702;&#21644;&#23384;&#20648;&#20135;&#29983;&#36830;&#32493;&#19988;&#39640;&#39057;&#30340;&#25968;&#21313;&#20159;&#26465;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20026;&#20102;&#25903;&#25345;&#39640;&#25928;&#30340;&#25968;&#25454;&#23384;&#20648;&#21644;&#26816;&#32034;&#65292;&#20986;&#29616;&#20102;&#19987;&#38376;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24211;&#21644;&#31995;&#32479;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#25903;&#25345;&#36890;&#36807;&#31867;&#20284;&#20110;&#32422;&#26463;&#21270;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#30340;&#26684;&#24335;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32034;&#24341;&#21644;&#26597;&#35810;&#65292;&#20197;&#23454;&#29616;&#20687;&#8220;&#26376;&#24230;&#20215;&#26684;&#22238;&#25253;&#22823;&#20110;5%&#30340;&#32929;&#31080;&#8221;&#36825;&#26679;&#30340;&#26597;&#35810;&#65292;&#24182;&#20197;&#20005;&#26684;&#30340;&#26684;&#24335;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26597;&#35810;&#19981;&#33021;&#25429;&#25417;&#21040;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#24448;&#24448;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#25110;&#35821;&#35328;&#65288;&#20363;&#22914;&#8220;&#22788;&#20110;&#20302;&#27874;&#21160;&#24615;&#29366;&#24577;&#30340;&#32929;&#31080;&#8221;&#65289;&#26356;&#22909;&#22320;&#25551;&#36848;&#12290;&#32780;&#19988;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#31354;&#38388;&#20013;&#36827;&#34892;&#25628;&#32034;&#25152;&#38656;&#30340;&#23384;&#20648;&#12289;&#35745;&#31639;&#26102;&#38388;&#21644;&#26816;&#32034;&#22797;&#26434;&#24230;&#24448;&#24448;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#28436;&#31034;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#28508;&#31354;&#38388;&#25237;&#24433;&#21487;&#20197;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial firms commonly process and store billions of time-series data, generated continuously and at a high frequency. To support efficient data storage and retrieval, specialized time-series databases and systems have emerged. These databases support indexing and querying of time-series by a constrained Structured Query Language(SQL)-like format to enable queries like "Stocks with monthly price returns greater than 5%", and expressed in rigid formats. However, such queries do not capture the intrinsic complexity of high dimensional time-series data, which can often be better described by images or language (e.g., "A stock in low volatility regime"). Moreover, the required storage, computational time, and retrieval complexity to search in the time-series space are often non-trivial. In this paper, we propose and demonstrate a framework to store multi-modal data for financial time-series in a lower-dimensional latent space using deep encoders, such that the latent space projections ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#29616;&#26377;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26377;&#25928;&#26816;&#27979;&#21644;&#27700;&#21360;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.15224</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Collaborative Watermarking for Adversarial Speech Synthesis. (arXiv:2309.15224v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#29616;&#26377;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26377;&#25928;&#26816;&#27979;&#21644;&#27700;&#21360;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#38899;&#21512;&#25104;&#30340;&#36827;&#23637;&#20351;&#24471;&#25216;&#26415;&#19981;&#20165;&#25509;&#36817;&#20154;&#31867;&#30340;&#33258;&#28982;&#24230;&#65292;&#32780;&#19988;&#33021;&#22815;&#20197;&#23569;&#37327;&#25968;&#25454;&#36827;&#34892;&#21363;&#26102;&#35821;&#38899;&#20811;&#38534;&#65292;&#24182;&#19988;&#20511;&#21161;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#39640;&#24230;&#21487;&#35775;&#38382;&#24615;&#12290;&#24403;&#28982;&#65292;&#29983;&#25104;&#20869;&#23481;&#30340;&#28508;&#22312;&#27867;&#28389;&#24341;&#36215;&#20102;&#23545;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#21644;&#27700;&#21360;&#25216;&#26415;&#30340;&#38656;&#27714;&#12290;&#26368;&#36817;&#65292;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#30340;&#30740;&#31350;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;&#21644;&#27450;&#39575;&#23545;&#31574;&#25361;&#25112;&#65288;ASVspoof&#65289;&#19978;&#65292;&#35813;&#25361;&#25112;&#19987;&#27880;&#20110;&#34987;&#21160;&#23545;&#31574;&#12290;&#26412;&#25991;&#20174;&#21478;&#19968;&#35282;&#24230;&#20986;&#21457;&#65292;&#38024;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#22312;&#19981;&#24178;&#25200;&#20154;&#31867;&#21548;&#20247;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#36890;&#36807;&#21327;&#21516;&#26426;&#22120;&#26816;&#27979;&#21040;&#29983;&#25104;&#35821;&#38899;&#30340;&#27700;&#21360;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;ASVspoof 2021&#22522;&#32447;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#30340;HiFi-GAN&#31070;&#32463;&#22768;&#30721;&#22120;&#30340;&#21512;&#20316;&#35757;&#32451;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in neural speech synthesis have brought us technology that is not only close to human naturalness, but is also capable of instant voice cloning with little data, and is highly accessible with pre-trained models available. Naturally, the potential flood of generated content raises the need for synthetic speech detection and watermarking. Recently, considerable research effort in synthetic speech detection has been related to the Automatic Speaker Verification and Spoofing Countermeasure Challenge (ASVspoof), which focuses on passive countermeasures. This paper takes a complementary view to generated speech detection: a synthesis system should make an active effort to watermark the generated speech in a way that aids detection by another machine, but remains transparent to a human listener. We propose a collaborative training scheme for synthetic speech watermarking and show that a HiFi-GAN neural vocoder collaborating with the ASVspoof 2021 baseline countermeasure models consis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14496</link><description>&lt;p&gt;
Era Splitting.&#65288;arXiv:2309.14496v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Era Splitting. (arXiv:2309.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20250;&#21576;&#29616;&#20986;&#25968;&#25454;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#34892;&#20026;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#33539;&#24335;&#30340;&#33539;&#22260;&#65292;&#35813;&#33539;&#24335;&#20551;&#35774;&#25968;&#25454;&#22312;&#26102;&#38388;&#21644;&#22320;&#28857;&#19978;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#26032;&#20852;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#39046;&#22495;&#36890;&#36807;&#23558;&#29615;&#22659;&#25110;&#26102;&#20195;&#20449;&#24687;&#34701;&#20837;&#31639;&#27861;&#20013;&#65292;&#26469;&#24212;&#23545;&#36825;&#20010;&#29616;&#23454;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;/&#25110;&#31070;&#32463;&#32593;&#32476;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#19982;&#27599;&#20010;&#25968;&#25454;&#28857;&#30456;&#20851;&#30340;&#26102;&#20195;&#20449;&#24687;&#65292;&#26469;&#25214;&#21040;&#22312;&#25968;&#25454;&#30340;&#25152;&#26377;&#19981;&#30456;&#20132;&#26102;&#20195;&#20013;&#37117;&#26159;&#26368;&#20248;&#30340;&#20999;&#20998;&#28857;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#22240;&#26524;&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#21270;&#22270;&#20687;&#20013;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#22806;&#35266;&#65292;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#24182;&#20135;&#29983;&#20102;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#65292;&#32858;&#28966;&#20110;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.10399</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#21033;&#29992;&#22240;&#26524;&#20449;&#21495;&#30340;&#30740;&#31350;&#65306;&#19968;&#39033;&#24102;&#26377;&#23454;&#35777;&#32467;&#26524;&#30340;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploiting Causality Signals in Medical Images: A Pilot Study with Empirical Results. (arXiv:2309.10399v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#22240;&#26524;&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#21270;&#22270;&#20687;&#20013;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#22806;&#35266;&#65292;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#24182;&#20135;&#29983;&#20102;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#65292;&#32858;&#28966;&#20110;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#21307;&#23398;&#22270;&#20687;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22330;&#26223;&#20013;&#30340;&#24369;&#22240;&#26524;&#20449;&#21495;&#26469;&#24314;&#27169;&#22270;&#20687;&#20013;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;&#21478;&#19968;&#20010;&#37096;&#20998;&#29305;&#24449;&#30340;&#22806;&#35266;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#21644;&#22240;&#26524;&#22240;&#23376;&#25552;&#21462;&#27169;&#22359;&#12290;&#21518;&#32773;&#35745;&#31639;&#29305;&#24449;&#22270;&#30340;&#26435;&#37325;&#65292;&#26681;&#25454;&#20854;&#23545;&#22270;&#20687;&#22330;&#26223;&#30340;&#22240;&#26524;&#24433;&#21709;&#22686;&#24378;&#27599;&#20010;&#29305;&#24449;&#22270;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#22806;&#37096;&#20449;&#21495;&#26469;&#20462;&#25913;&#22240;&#26524;&#27169;&#22359;&#30340;&#21151;&#33021;&#65292;&#20174;&#32780;&#33719;&#24471;&#25105;&#20204;&#26041;&#27861;&#30340;&#19981;&#21516;&#21464;&#20307;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#23454;&#39564;&#12289;&#23450;&#24615;&#35780;&#20272;&#21644;&#21066;&#24369;&#23454;&#39564;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23545;&#21069;&#21015;&#33146;MRI&#22270;&#20687;&#36827;&#34892;&#21069;&#21015;&#33146;&#30284;&#35786;&#26029;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#20135;&#29983;&#20102;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#65292;&#32858;&#28966;&#20110;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#36825;&#22312;&#21307;&#30103;&#22270;&#20687;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new method for automatically classifying medical images that uses weak causal signals in the scene to model how the presence of a feature in one part of the image affects the appearance of another feature in a different part of the image. Our method consists of two components: a convolutional neural network backbone and a causality-factors extractor module. The latter computes weights for the feature maps to enhance each feature map according to its causal influence in the image's scene. We can modify the functioning of the causality module by using two external signals, thus obtaining different variants of our method. We evaluate our method on a public dataset of prostate MRI images for prostate cancer diagnosis, using quantitative experiments, qualitative assessment, and ablation studies. Our results show that our method improves classification performance and produces more robust predictions, focusing on relevant parts of the image. That is especially important in medic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#25991;&#23383;&#21270;&#20154;&#29289;&#26816;&#32034;&#30340;&#21327;&#21516;&#30693;&#35782;&#20256;&#36882;&#65288;CSKT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#25552;&#31034;&#20256;&#36882;&#65288;BPT&#65289;&#21644;&#21452;&#36866;&#37197;&#22120;&#20256;&#36882;&#65288;DAT&#65289;&#23454;&#29616;&#20102;&#36755;&#20837;&#31471;&#21644;&#36755;&#20986;&#31471;&#30340;&#30693;&#35782;&#20256;&#36882;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09496</link><description>&lt;p&gt;
&#22522;&#20110;CLIP&#30340;&#25991;&#23383;&#21270;&#20154;&#29289;&#26816;&#32034;&#30340;&#21327;&#21516;&#30693;&#35782;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
CLIP-based Synergistic Knowledge Transfer for Text-based Person Retrieval. (arXiv:2309.09496v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#25991;&#23383;&#21270;&#20154;&#29289;&#26816;&#32034;&#30340;&#21327;&#21516;&#30693;&#35782;&#20256;&#36882;&#65288;CSKT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#25552;&#31034;&#20256;&#36882;&#65288;BPT&#65289;&#21644;&#21452;&#36866;&#37197;&#22120;&#20256;&#36882;&#65288;DAT&#65289;&#23454;&#29616;&#20102;&#36755;&#20837;&#31471;&#21644;&#36755;&#20986;&#31471;&#30340;&#30693;&#35782;&#20256;&#36882;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21270;&#20154;&#29289;&#26816;&#32034;&#65288;TPR&#65289;&#26088;&#22312;&#26681;&#25454;&#25991;&#26412;&#26597;&#35810;&#26816;&#32034;&#30446;&#26631;&#20154;&#29289;&#22270;&#20687;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#24357;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#26377;&#38480;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#25991;&#23383;&#21270;&#20154;&#29289;&#26816;&#32034;&#30340;&#21327;&#21516;&#30693;&#35782;&#20256;&#36882;&#65288;CSKT&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#25506;&#32034;CLIP&#22312;&#36755;&#20837;&#31471;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;&#21452;&#21521;&#25552;&#31034;&#21644;&#32806;&#21512;&#25237;&#24433;&#26500;&#24314;&#30340;&#21452;&#21521;&#25552;&#31034;&#20256;&#36882;&#65288;BPT&#65289;&#27169;&#22359;&#12290;&#20854;&#27425;&#65292;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65288;MHA&#65289;&#30340;&#36755;&#20986;&#31471;&#65292;&#35774;&#35745;&#20102;&#21452;&#36866;&#37197;&#22120;&#20256;&#36882;&#65288;DAT&#65289;&#20197;&#20256;&#36882;&#30693;&#35782;&#12290;&#36825;&#31181;&#21327;&#21516;&#30340;&#21452;&#21521;&#21512;&#20316;&#26426;&#21046;&#20419;&#36827;&#20102;&#26089;&#26399;&#29305;&#24449;&#34701;&#21512;&#65292;&#24182;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;CLIP&#30340;&#29616;&#26377;&#30693;&#35782;&#12290;&#24403;&#35757;&#32451;&#21442;&#25968;&#20165;&#21344;7%&#26102;&#65292;CSKT&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based Person Retrieval (TPR) aims to retrieve the target person images given a textual query. The primary challenge lies in bridging the substantial gap between vision and language modalities, especially when dealing with limited large-scale datasets. In this paper, we introduce a CLIP-based Synergistic Knowledge Transfer (CSKT) approach for TPR. Specifically, to explore the CLIP's knowledge on input side, we first propose a Bidirectional Prompts Transferring (BPT) module constructed by text-to-image and image-to-text bidirectional prompts and coupling projections. Secondly, Dual Adapters Transferring (DAT) is designed to transfer knowledge on output side of Multi-Head Attention (MHA) in vision and language. This synergistic two-way collaborative mechanism promotes the early-stage feature fusion and efficiently exploits the existing knowledge of CLIP. CSKT outperforms the state-of-the-art approaches across three benchmark datasets when the training parameters merely account for 7.
&lt;/p&gt;</description></item><item><title>SayCanPay&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#21551;&#21457;&#24335;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#21407;&#21017;&#65292;&#29983;&#25104;&#21487;&#34892;&#30340;&#26368;&#20248;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2308.12682</link><description>&lt;p&gt;
SayCanPay: &#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21551;&#21457;&#24335;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge. (arXiv:2308.12682v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12682
&lt;/p&gt;
&lt;p&gt;
SayCanPay&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#21551;&#21457;&#24335;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#21407;&#21017;&#65292;&#29983;&#25104;&#21487;&#34892;&#30340;&#26368;&#20248;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#20854;&#24222;&#22823;&#30340;"&#19990;&#30028;&#30693;&#35782;"&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35268;&#21010;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#33719;&#24471;&#26082;&#21487;&#34892;&#65288;&#22522;&#20110;&#21487;&#29992;&#24615;&#65289;&#21448;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#65288;&#35745;&#21010;&#38271;&#24230;&#26041;&#38754;&#65289;&#30340;&#35745;&#21010;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#19982;&#21551;&#21457;&#24335;&#35268;&#21010;&#26041;&#27861;&#24418;&#25104;&#21453;&#24046;&#65292;&#21551;&#21457;&#24335;&#35268;&#21010;&#26041;&#27861;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;(&#22312;&#21160;&#20316;&#27169;&#22411;&#22914;PDDL&#20013;&#24418;&#24335;&#21270;)&#21644;&#21551;&#21457;&#24335;&#25628;&#32034;&#26469;&#29983;&#25104;&#21487;&#34892;&#30340;&#26368;&#20248;&#35745;&#21010;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#21644;&#21551;&#21457;&#24335;&#35268;&#21010;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#21407;&#21017;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;SayCanPay&#21033;&#29992;LLMs&#26469;&#29983;&#25104;&#30001;&#21487;&#23398;&#20064;&#30340;&#39046;&#22495;&#30693;&#35782;&#24341;&#23548;&#30340;&#21160;&#20316;(Say)&#65292;&#35780;&#20272;&#21160;&#20316;&#30340;&#21487;&#34892;&#24615;(Can)&#21644;&#38271;&#26399;&#22238;&#25253;/&#25910;&#30410;(Pay)&#65292;&#21033;&#29992;&#21551;&#21457;&#24335;&#25628;&#32034;&#26469;&#36873;&#25321;&#26368;&#20339;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;(1)&#22312;&#21551;&#21457;&#24335;&#35268;&#21010;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;LLM&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#26500;&#24314;&#65292;(2)&#25972;&#21512;&#20102;&#21487;&#29992;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive planning abilities due to their vast "world knowledge". Yet, obtaining plans that are both feasible (grounded in affordances) and cost-effective (in plan length), remains a challenge, despite recent progress. This contrasts with heuristic planning methods that employ domain knowledge (formalized in action models such as PDDL) and heuristic search to generate feasible, optimal plans. Inspired by this, we propose to combine the power of LLMs and heuristic planning by leveraging the world knowledge of LLMs and the principles of heuristic search. Our approach, SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain knowledge, that evaluates actions' feasibility (Can) and long-term reward/payoff (Pay), and heuristic search to select the best sequence of actions. Our contributions are (1) a novel framing of the LLM planning problem in the context of heuristic planning, (2) integrating grounding and cost-effective 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06035</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#26399;&#38388;&#34920;&#29616;&#20986;&#20154;&#31867;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Evidence of Human-Like Visual-Linguistic Integration in Multimodal Large Language Models During Predictive Language Processing. (arXiv:2308.06035v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;mLLMs&#65289;&#22312;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#30340;&#35270;&#35273;-&#35821;&#35328;&#38598;&#25104;&#33021;&#21147;&#26159;&#21542;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;mLLMs&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#22797;&#21046;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#30340;&#20105;&#35758;&#12290;LLMs&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#19968;&#20010;&#21306;&#21035;&#22312;&#20110;&#65292;&#35821;&#35328;&#36755;&#20837;&#36890;&#24120;&#24314;&#31435;&#22312;&#22810;&#20010;&#30693;&#35273;&#27169;&#24577;&#19978;&#65292;&#32780;&#22823;&#22810;&#25968;LLMs&#20165;&#22788;&#29702;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#20351;&#20154;&#31867;&#33021;&#22815;&#25972;&#21512;&#35270;&#35273;&#32972;&#26223;&#19982;&#35821;&#35328;&#20449;&#24687;&#65292;&#20174;&#32780;&#23545;&#21363;&#23558;&#20986;&#29616;&#30340;&#21333;&#35789;&#30340;&#31354;&#38388;&#26045;&#21152;&#38480;&#21046;&#65292;&#20943;&#23569;&#35748;&#30693;&#36127;&#33655;&#65292;&#25552;&#39640;&#24863;&#30693;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;LLMs&#65288;mLLMs&#65289;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#21464;&#21387;&#22120;&#31867;&#22411;&#30340;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#19979;&#19968;&#20010;&#21333;&#35789;&#30340;&#39044;&#27979;&#12290;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#65292;&#22522;&#20110;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#39044;&#27979;&#35821;&#35328;&#22788;&#29702;&#22312;mLLMs&#21644;&#20154;&#31867;&#20013;&#21563;&#21512;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;200&#21517;&#34987;&#35797;&#35266;&#30475;&#20102;&#30701;&#30340;&#35270;&#21548;&#21098;&#36753;&#65292;&#24182;&#20272;&#35745;&#20102;&#21363;&#23558;&#20986;&#29616;&#30340;&#21160;&#35789;&#25110;&#21517;&#35789;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advanced language processing abilities of large language models (LLMs) have stimulated debate over their capacity to replicate human-like cognitive processes. One differentiating factor between language processing in LLMs and humans is that language input is often grounded in more than one perceptual modality, whereas most LLMs process solely text-based information. Multimodal grounding allows humans to integrate - e.g. visual context with linguistic information and thereby place constraints on the space of upcoming words, reducing cognitive load and improving perception and comprehension. Recent multimodal LLMs (mLLMs) combine visual and linguistic embedding spaces with a transformer type attention mechanism for next-word prediction. To what extent does predictive language processing based on multimodal input align in mLLMs and humans? To answer this question, 200 human participants watched short audio-visual clips and estimated the predictability of an upcoming verb or noun. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21367;&#31215;&#21464;&#25442;&#22120;&#26550;&#26500;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#20809;&#29031;&#12289;&#36974;&#25377;&#21644;&#25104;&#29087;&#24230;&#26465;&#20214;&#19979;&#33258;&#20027;&#35782;&#21035;&#21644;&#20998;&#32423;&#35199;&#32418;&#26623;&#12290;</title><link>http://arxiv.org/abs/2307.01530</link><description>&lt;p&gt;
&#24212;&#23545;&#19981;&#21516;&#20809;&#29031;&#12289;&#36974;&#25377;&#21644;&#25104;&#29087;&#24230;&#26465;&#20214;&#19979;&#30340;&#35199;&#32418;&#26623;&#33258;&#20027;&#35782;&#21035;&#21644;&#20998;&#32423;&#30340;&#21367;&#31215;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions. (arXiv:2307.01530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21367;&#31215;&#21464;&#25442;&#22120;&#26550;&#26500;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#20809;&#29031;&#12289;&#36974;&#25377;&#21644;&#25104;&#29087;&#24230;&#26465;&#20214;&#19979;&#33258;&#20027;&#35782;&#21035;&#21644;&#20998;&#32423;&#35199;&#32418;&#26623;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#29992;&#31227;&#21160;&#26426;&#22120;&#20154;&#37319;&#25688;&#23436;&#20840;&#25104;&#29087;&#30340;&#35199;&#32418;&#26623;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#26469;&#33258;&#20110;&#21494;&#23376;&#21644;&#26641;&#26525;&#36896;&#25104;&#30340;&#36974;&#25377;&#65292;&#20197;&#21450;&#22312;&#26524;&#23454;&#21457;&#32946;&#38454;&#27573;&#65292;&#35199;&#32418;&#26623;&#21644;&#21608;&#22260;&#26893;&#34987;&#20043;&#38388;&#30340;&#39068;&#33394;&#30456;&#20284;&#24615;&#12290;&#33258;&#28982;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#20809;&#29031;&#26465;&#20214;&#12289;&#35270;&#35282;&#12289;&#36974;&#25377;&#22240;&#32032;&#21644;&#19981;&#21516;&#30340;&#25104;&#29087;&#24230;&#27700;&#24179;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21367;&#31215;&#21464;&#25442;&#22120;&#26550;&#26500;&#33258;&#20027;&#35782;&#21035;&#21644;&#20998;&#32423;&#35199;&#32418;&#26623;&#65292;&#26080;&#35770;&#20854;&#36974;&#25377;&#27700;&#24179;&#12289;&#20809;&#29031;&#26465;&#20214;&#21644;&#25104;&#29087;&#24230;&#22914;&#20309;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32463;&#36807;&#29305;&#21035;&#20026;&#27492;&#30446;&#30340;&#31934;&#24515;&#27880;&#37322;&#30340;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#30340;&#20809;&#29031;&#26465;&#20214;&#12289;&#35270;&#35282;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#31227;&#21160;&#30456;&#26426;&#20256;&#24863;&#22120;&#19979;&#20934;&#22791;&#65292;&#19982;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;Laboro&#65289;&#26377;&#25152;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harvesting fully ripe tomatoes with mobile robots presents significant challenges in real-world scenarios. These challenges arise from factors such as occlusion caused by leaves and branches, as well as the color similarity between tomatoes and the surrounding foliage during the fruit development stage. The natural environment further compounds these issues with varying light conditions, viewing angles, occlusion factors, and different maturity levels. To overcome these obstacles, this research introduces a novel framework that leverages a convolutional transformer architecture to autonomously recognize and grade tomatoes, irrespective of their occlusion level, lighting conditions, and ripeness. The proposed model is trained and tested using carefully annotated images curated specifically for this purpose. The dataset is prepared under various lighting conditions, viewing perspectives, and employs different mobile camera sensors, distinguishing it from existing datasets such as Laboro 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11300</link><description>&lt;p&gt;
RS5M&#65306;&#29992;&#20110;&#36965;&#24863;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model. (arXiv:2306.11300v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#21033;&#29992;&#24050;&#26377;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#22495;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#36801;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#24357;&#21512;&#20102;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#65288;RS&#65289;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;500&#19975;&#24352;&#24102;&#26377;&#33521;&#25991;&#25551;&#36848;&#30340;RS&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#20165;&#24102;&#26631;&#31614;&#30340;RS&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Vision-Language Foundation Models utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain Foundation Model (DFM), bridging the gap between the General Foundation Model (GFM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Essential Element Network (EEN)&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#24182;&#36827;&#34892;&#30456;&#20851;&#24615;&#35745;&#31639;&#21644;&#20248;&#21270;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#38899;&#20048;&#30340;&#28145;&#23618;&#32467;&#26500;&#20449;&#24687;&#65292;&#20026;&#21400;&#28165;&#38899;&#20048;&#32467;&#26500;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13631</link><description>&lt;p&gt;
&#38899;&#20048;&#32467;&#26500;&#30340;&#33258;&#32452;&#32455;&#32593;&#32476;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
In-depth analysis of music structure as a self-organized network. (arXiv:2303.13631v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Essential Element Network (EEN)&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#24182;&#36827;&#34892;&#30456;&#20851;&#24615;&#35745;&#31639;&#21644;&#20248;&#21270;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#38899;&#20048;&#30340;&#28145;&#23618;&#32467;&#26500;&#20449;&#24687;&#65292;&#20026;&#21400;&#28165;&#38899;&#20048;&#32467;&#26500;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#35789;&#27719;&#19981;&#20165;&#20256;&#36882;&#20449;&#24687;&#65292;&#36824;&#38543;&#30528;&#25991;&#26126;&#21644;&#20154;&#31867;&#36801;&#31227;&#32780;&#28436;&#21464;&#12290;&#38899;&#20048;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#29702;&#35299;&#38899;&#20048;&#32972;&#21518;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21483;&#20570;Essential Element Network (EEN)&#30340;&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#35745;&#31639;&#38899;&#35843;&#12289;&#26102;&#38388;&#21644;&#38899;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24471;&#21040;&#65292;&#36890;&#36807;&#20248;&#21270;EEN&#31639;&#27861;&#20197;&#29983;&#25104;Zipf&#23450;&#24459;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#35821;&#20041;&#20851;&#31995;&#35270;&#20026;&#35789;&#27719;&#24182;&#29983;&#25104;&#23427;&#20204;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32534;&#30721;&#21518;&#30340;&#35789;&#27719;&#26144;&#23556;&#21040;&#38899;&#35843;-&#26102;&#38388;&#31354;&#38388;&#20013;&#65292;&#26377;&#21161;&#20110;&#25105;&#20204;&#31995;&#32479;&#22320;&#32452;&#32455;&#38899;&#20048;&#28145;&#23618;&#32467;&#26500;&#20013;&#30340;&#21477;&#27861;&#12290;&#30456;&#27604;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#40657;&#30418;&#23376;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#23545;&#38899;&#20048;&#32972;&#21518;&#22797;&#26434;&#32593;&#32476;&#30340;&#31934;&#30830;&#25551;&#36848;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#36807;&#31243;&#31215;&#32047;&#30340;&#32463;&#39564;&#21644;&#23646;&#24615;&#19981;&#20165;&#20026;&#27492;&#31867;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#20026;&#35768;&#22810;&#20854;&#20182;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25506;&#32034;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Words in a natural language not only transmit information but also evolve with the development of civilization and human migration. The same is true for music. To understand the complex structure behind the music, we introduced an algorithm called the Essential Element Network (EEN) to encode the audio into text. The network is obtained by calculating the correlations between scales, time, and volume. Optimizing EEN to generate Zipfs law for the frequency and rank of the clustering coefficient enables us to generate and regard the semantic relationships as words. We map these encoded words into the scale-temporal space, which helps us organize systematically the syntax in the deep structure of music. Our algorithm provides precise descriptions of the complex network behind the music, as opposed to the black-box nature of other deep learning approaches. As a result, the experience and properties accumulated through these processes can offer not only a new approach to the applications of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#26144;&#23556;&#24212;&#29992;&#20110;&#23454;&#38469;&#26426;&#22120;&#20154;&#22330;&#26223;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#38480;&#21046;&#38382;&#39064;&#32500;&#24230;&#24182;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#21487;&#20197;&#23454;&#29616;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#20960;&#21400;&#31859;&#32423;&#21035;&#30340;&#23454;&#26102;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.04718</link><description>&lt;p&gt;
&#20851;&#20110;&#39640;&#25928;&#31070;&#32463;&#26144;&#23556;&#22312;&#26080;&#20154;&#22320;&#38754;&#36710;&#36742;&#23454;&#26102;&#23460;&#20869;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Application of Efficient Neural Mapping to Real-Time Indoor Localisation for Unmanned Ground Vehicles. (arXiv:2211.04718v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#26144;&#23556;&#24212;&#29992;&#20110;&#23454;&#38469;&#26426;&#22120;&#20154;&#22330;&#26223;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#38480;&#21046;&#38382;&#39064;&#32500;&#24230;&#24182;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#21487;&#20197;&#23454;&#29616;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#20960;&#21400;&#31859;&#32423;&#21035;&#30340;&#23454;&#26102;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#35273;&#25968;&#25454;&#20013;&#36827;&#34892;&#20840;&#23616;&#23450;&#20301;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36866;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#20154;&#39046;&#22495;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#23558;&#29615;&#22659;&#30340;&#22270;&#20687;&#26144;&#23556;&#21040;&#35813;&#29615;&#22659;&#19979;&#30340;&#32477;&#23545;&#30456;&#26426;&#23039;&#24577;&#65292;&#20174;&#32780;&#22312;&#27492;&#36807;&#31243;&#20013;&#23398;&#20064;&#38544;&#24335;&#31070;&#32463;&#26144;&#23556;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#35777;&#26126;&#36890;&#36807;&#23558;&#38382;&#39064;&#38480;&#21046;&#22312;&#20108;&#32500;&#31354;&#38388;&#65292;&#24182;&#26174;&#33879;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#21487;&#20197;&#20351;&#29992;&#32039;&#20945;&#30340;&#27169;&#22411;&#22312;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#23454;&#26102;&#25512;&#29702;&#65292;&#23454;&#29616;&#20960;&#21400;&#31859;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;&#25105;&#20204;&#22312;&#22320;&#38754;&#36710;&#36742;&#24179;&#21488;&#19978;&#37096;&#32626;&#20102;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#65292;&#22312;&#33322;&#28857;&#23548;&#33322;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#22312;&#22320;&#38754;&#36710;&#36742;&#30340;&#23884;&#20837;&#24335;CPU&#19978;&#20197;6fps&#30340;&#36895;&#29575;&#21644;&#24179;&#22343;&#31934;&#24230;&#20026;9cm&#36827;&#34892;&#23450;&#20301;&#65292;&#22312;&#23884;&#20837;&#24335;GPU&#19978;&#20197;35fps&#30340;&#36895;&#29575;&#36827;&#34892;&#23450;&#20301;&#65292;&#22312;&#26700;&#38754;GPU&#19978;&#20197;220fps&#30340;&#36895;&#29575;&#36827;&#34892;&#23450;&#20301;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#21457;&#24067;&#19968;&#20010;&#26032;&#39062;&#30340;&#23450;&#20301;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Global localisation from visual data is a challenging problem applicable to many robotics domains. Prior works have shown that neural networks can be trained to map images of an environment to absolute camera pose within that environment, learning an implicit neural mapping in the process. In this work we evaluate the applicability of such an approach to real-world robotics scenarios, demonstrating that by constraining the problem to 2-dimensions and significantly increasing the quantity of training data, a compact model capable of real-time inference on embedded platforms can be used to achieve localisation accuracy of several centimetres. We deploy our trained model onboard a UGV platform, demonstrating its effectiveness in a waypoint navigation task, wherein it is able to localise with a mean accuracy of 9cm at a rate of 6fps running on the UGV onboard CPU, 35fps on an embedded GPU, or 220fps on a desktop GPU. Along with this work we will release a novel localisation dataset compris
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#36827;&#34892;&#20102;&#36817;&#20284;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#30456;&#23545;&#20110;&#20256;&#32479;&#32447;&#24615;&#21464;&#25442;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#36890;&#36807;&#21019;&#36896;&#24615;&#30340;&#26500;&#36896;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#19988;&#25506;&#31350;&#20102;&#28145;&#23618;&#32593;&#32476;&#30340;&#20989;&#25968;&#36924;&#36817;&#36895;&#24230;&#12290;&#32447;&#24615;&#29305;&#24449;&#30340;&#22810;&#20998;&#36776;&#21367;&#31215;&#20998;&#35299;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.09041</link><description>&lt;p&gt;
&#20174;&#29305;&#24449;&#25552;&#21462;&#35282;&#24230;&#30340;CNN&#36817;&#20284;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Approximation analysis of CNNs from a feature extraction view. (arXiv:2210.09041v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#36827;&#34892;&#20102;&#36817;&#20284;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#30456;&#23545;&#20110;&#20256;&#32479;&#32447;&#24615;&#21464;&#25442;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#36890;&#36807;&#21019;&#36896;&#24615;&#30340;&#26500;&#36896;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#19988;&#25506;&#31350;&#20102;&#28145;&#23618;&#32593;&#32476;&#30340;&#20989;&#25968;&#36924;&#36817;&#36895;&#24230;&#12290;&#32447;&#24615;&#29305;&#24449;&#30340;&#22810;&#20998;&#36776;&#21367;&#31215;&#20998;&#35299;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#25104;&#21151;&#30340;&#32467;&#26524;&#65292;&#20294;&#30001;&#20110;&#32593;&#32476;&#26550;&#26500;&#21644;&#32467;&#26500;&#30340;&#38480;&#21046;&#65292;&#23427;&#32570;&#20047;&#36275;&#22815;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#24230;&#22810;&#36890;&#36947;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#24314;&#31435;&#20102;&#19968;&#20123;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#30340;&#20998;&#26512;&#65292;&#36825;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20256;&#32479;&#32447;&#24615;&#21464;&#25442;&#65288;&#22914;&#20613;&#37324;&#21494;&#21464;&#25442;&#12289;&#23567;&#27874;&#21464;&#25442;&#12289;&#20887;&#20313;&#23383;&#20856;&#32534;&#30721;&#26041;&#27861;&#65289;&#19978;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#26500;&#36896;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22810;&#36890;&#36947;CNNs&#39640;&#25928;&#22320;&#36827;&#34892;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#38477;&#20302;&#36924;&#36817;&#39640;&#32500;&#20989;&#25968;&#25152;&#38656;&#30340;&#22522;&#26412;&#32500;&#24230;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#36890;&#36947;&#23454;&#29616;&#30340;&#28145;&#23618;&#32593;&#32476;&#21644;&#20840;&#36830;&#25509;&#23618;&#30340;&#20989;&#25968;&#36924;&#36817;&#36895;&#29575;&#12290;&#23558;&#32447;&#24615;&#29305;&#24449;&#22240;&#23376;&#20998;&#35299;&#20026;&#22810;&#20998;&#36776;&#21367;&#31215;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning based on deep neural networks has been very successful in many practical applications, but it lacks enough theoretical understanding due to the network architectures and structures. In this paper we establish some analysis for linear feature extraction by a deep multi-channel convolutional neural networks (CNNs), which demonstrates the power of deep learning over traditional linear transformations, like Fourier, wavelets, redundant dictionary coding methods. Moreover, we give an exact construction presenting how linear features extraction can be conducted efficiently with multi-channel CNNs. It can be applied to lower the essential dimension for approximating a high dimensional function. Rates of function approximation by such deep networks implemented with channels and followed by fully-connected layers are investigated as well. Harmonic analysis for factorizing linear features into multi-resolution convolutions plays an essential role in our work. Nevertheless, a dedica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#21462;&#36135;&#21644;&#36865;&#36135;&#23545;&#36335;&#36793;&#20132;&#36890;&#30340;&#25317;&#22581;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#21644;&#20998;&#31163;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2206.02164</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#20943;&#36731;&#36335;&#36793;&#21462;&#36135;&#21644;&#36865;&#36135;&#30340;&#25317;&#22581;&#24433;&#21709;&#65306;&#19968;&#31181;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Estimating and Mitigating the Congestion Effect of Curbside Pick-ups and Drop-offs: A Causal Inference Approach. (arXiv:2206.02164v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#21462;&#36135;&#21644;&#36865;&#36135;&#23545;&#36335;&#36793;&#20132;&#36890;&#30340;&#25317;&#22581;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#21644;&#20998;&#31163;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36335;&#36793;&#31354;&#38388;&#26159;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#26368;&#32321;&#24537;&#30340;&#21306;&#22495;&#20043;&#19968;&#12290;&#29305;&#21035;&#26159;&#36817;&#24180;&#26469;&#65292;&#32593;&#32422;&#36710;&#21644;&#21830;&#19994;&#37197;&#36865;&#30340;&#24555;&#36895;&#22686;&#38271;&#23548;&#33268;&#20102;&#22823;&#37327;&#30340;&#21462;&#36135;&#21644;&#36865;&#36135;&#65288;PUDOs&#65289;&#65292;&#36825;&#20123;&#21344;&#25454;&#20102;&#20960;&#21313;&#24180;&#21069;&#35774;&#35745;&#24314;&#36896;&#30340;&#26377;&#38480;&#36335;&#36793;&#31354;&#38388;&#12290;&#36825;&#20123;PUDOs&#21487;&#33021;&#23548;&#33268;&#36335;&#36793;&#21033;&#29992;&#29575;&#30340;&#25317;&#22581;&#21644;&#24178;&#25200;&#20027;&#32447;&#20132;&#36890;&#27969;&#65292;&#26126;&#26174;&#24102;&#26469;&#26174;&#33879;&#30340;&#36127;&#38754;&#31038;&#20250;&#22806;&#37096;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#20005;&#26684;&#37327;&#21270;&#21644;&#20943;&#36731;PUDOs&#25317;&#22581;&#24433;&#21709;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#23588;&#20854;&#26159;&#32570;&#20047;&#25968;&#25454;&#25903;&#25345;&#21644;&#28151;&#28102;&#25928;&#24212;&#30340;&#21442;&#19982;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#25991;&#37319;&#29992;&#20005;&#26684;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#35780;&#20272;PUDOs&#23545;&#19968;&#33324;&#21306;&#22495;&#32593;&#32476;&#30340;&#25317;&#22581;&#24433;&#21709;&#12290;&#24314;&#31435;&#20102;&#19968;&#20010;&#22240;&#26524;&#22270;&#26469;&#34920;&#31034;PUDOs&#21644;&#20132;&#36890;&#36895;&#24230;&#20043;&#38388;&#30340;&#26102;&#31354;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#21644;&#20998;&#31163;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;DSML&#65289;&#26041;&#27861;&#26469;&#37327;&#21270;PUDOs&#23545;&#20132;&#36890;&#36895;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curb space is one of the busiest areas in urban road networks. Especially in recent years, the rapid increase of ride-hailing trips and commercial deliveries has induced massive pick-ups/drop-offs (PUDOs), which occupy the limited curb space that was designed and built decades ago. These PUDOs could jam curbside utilization and disturb the mainline traffic flow, evidently leading to significant negative societal externalities. However, there is a lack of an analytical framework that rigorously quantifies and mitigates the congestion effect of PUDOs in the system view, particularly with little data support and involvement of confounding effects. To bridge this research gap, this paper develops a rigorous causal inference approach to estimate the congestion effect of PUDOs on general regional networks. A causal graph is set to represent the spatio-temporal relationship between PUDOs and traffic speed, and a double and separated machine learning (DSML) method is proposed to quantify how P
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#32508;&#36848;&#20102;NeSy&#21644;StarAI&#20004;&#31181;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#23398;&#20064;&#21644;&#25512;&#29702;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#20849;&#26377;&#19971;&#20010;&#32500;&#24230;&#29992;&#20110;&#23545;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2108.11451</link><description>&lt;p&gt;
&#20174;&#32479;&#35745;&#20851;&#31995;&#21040;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Statistical Relational to Neural Symbolic Artificial Intelligence: a Survey. (arXiv:2108.11451v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11451
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#32508;&#36848;&#20102;NeSy&#21644;StarAI&#20004;&#31181;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#23398;&#20064;&#21644;&#25512;&#29702;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#20849;&#26377;&#19971;&#20010;&#32500;&#24230;&#29992;&#20110;&#23545;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#20004;&#20010;&#19981;&#21516;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#23398;&#20064;&#21644;&#25512;&#29702;&#30340;&#38598;&#25104;&#26041;&#27861;&#65306;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#65288;NeSy&#65289;&#21644;&#32479;&#35745;&#20851;&#31995;&#20154;&#24037;&#26234;&#33021;&#65288;StarAI&#65289;&#12290;NeSy&#26088;&#22312;&#23558;&#31526;&#21495;&#25512;&#29702;&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#32780;StarAI&#21017;&#19987;&#27880;&#20110;&#23558;&#36923;&#36753;&#19982;&#27010;&#29575;&#22270;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#35813;&#35843;&#26597;&#20851;&#27880;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#19971;&#20010;&#20849;&#21516;&#32500;&#24230;&#12290;&#36825;&#20123;&#32500;&#24230;&#29992;&#20110;&#23545;&#20004;&#31181;&#39046;&#22495;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#22522;&#20110;&#27169;&#22411;&#36824;&#26159;&#22522;&#20110;&#35777;&#26126;&#65307;&#65288;2&#65289;&#36923;&#36753;&#29702;&#35770;&#30340;&#35821;&#27861;&#65307;&#65288;3&#65289;&#31995;&#32479;&#30340;&#36923;&#36753;&#35821;&#20041;&#21450;&#20854;&#25193;&#23637;&#20197;&#20419;&#36827;&#23398;&#20064;&#65307;&#65288;4&#65289;&#23398;&#20064;&#30340;&#33539;&#22260;&#65292;&#21253;&#25324;&#20165;&#28041;&#21450;&#21442;&#25968;&#36824;&#26159;&#28041;&#21450;&#25972;&#20010;&#36923;&#36753;&#29702;&#35770;&#65307;&#65288;5&#65289;&#34920;&#31034;&#27861;&#20013;&#31526;&#21495;&#21644;&#23376;&#31526;&#21495;&#25104;&#20998;&#30340;&#23384;&#22312;&#65307;&#65288;6&#65289;&#31995;&#32479;&#25429;&#25417;&#21407;&#22987;&#36923;&#36753;&#12289;&#27010;&#29575;&#21644;&#31070;&#32463;&#33539;&#20363;&#30340;&#31243;&#24230;&#65307;&#21644;&#65288;7&#65289;&#20219;&#21153;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neural-symbolic computation (NeSy) and statistical relational artificial intelligence (StarAI). NeSy aims to integrate symbolic reasoning and neural networks while StarAI focuses on integrating logic with probabilistic graphical models. The survey brings attention to seven shared dimensions between the two approaches. These dimensions are employed to categorize both fields and include: (1) the approach to logic inference, whether model or proof-based; (2) the syntax of logical theories; (3) the logic semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either the parameters alone or the entire logic theory; (5) the presence of symbolic and subsymbolic components in representations; (6) the degree to which the systems can capture the original logic, probabilistic, and neural paradigms; and (7) the classes of tasks the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CRUMB&#30340;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#25918;&#36890;&#36807;&#37325;&#26032;&#32452;&#21512;&#29305;&#24449;&#22270;&#26469;&#32531;&#35299;&#36951;&#24536;&#38382;&#39064;&#12290;CRUMB&#36890;&#36807;&#23384;&#20648;&#20869;&#23384;&#22359;&#30340;&#32034;&#24341;&#26469;&#20351;&#24471;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#33021;&#22815;&#22238;&#25918;&#29305;&#23450;&#30340;&#35760;&#24518;&#65292;&#36825;&#31181;&#37325;&#24314;&#26426;&#21046;&#36824;&#21487;&#20197;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#26368;&#23567;&#21270;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2104.02206</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#27969;&#23398;&#20064;&#30340;&#35843;&#20248;&#32452;&#21512;&#29305;&#24449;&#22238;&#25918;
&lt;/p&gt;
&lt;p&gt;
Tuned Compositional Feature Replays for Efficient Stream Learning. (arXiv:2104.02206v7 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.02206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CRUMB&#30340;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#25918;&#36890;&#36807;&#37325;&#26032;&#32452;&#21512;&#29305;&#24449;&#22270;&#26469;&#32531;&#35299;&#36951;&#24536;&#38382;&#39064;&#12290;CRUMB&#36890;&#36807;&#23384;&#20648;&#20869;&#23384;&#22359;&#30340;&#32034;&#24341;&#26469;&#20351;&#24471;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#33021;&#22815;&#22238;&#25918;&#29305;&#23450;&#30340;&#35760;&#24518;&#65292;&#36825;&#31181;&#37325;&#24314;&#26426;&#21046;&#36824;&#21487;&#20197;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#26368;&#23567;&#21270;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#22823;&#33041;&#20174;&#30636;&#26102;&#30340;&#19990;&#30028;&#32463;&#39564;&#20013;&#25552;&#21462;&#20986;&#25345;&#20037;&#30340;&#12289;&#21487;&#25512;&#24191;&#30340;&#30693;&#35782;&#12290;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36828;&#36828;&#19981;&#33021;&#36798;&#21040;&#30456;&#21516;&#30340;&#27700;&#24179;&#65306;&#24403;&#34987;&#35201;&#27714;&#36890;&#36807;&#25353;&#29031;&#26102;&#38388;&#39034;&#24207;&#35757;&#32451;&#38750;&#37325;&#22797;&#35270;&#39057;&#24103;&#26469;&#23398;&#20064;&#23545;&#35937;&#20998;&#31867;&#26102;&#65288;&#22312;&#32447;&#27969;&#23398;&#20064;&#65289;&#65292;&#37027;&#20123;&#33021;&#22815;&#20174;&#37325;&#26032;&#25490;&#21015;&#30340;&#25968;&#25454;&#38598;&#20013;&#33391;&#22909;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#23398;&#20064;&#26032;&#30340;&#21050;&#28608;&#26102;&#20250;&#28798;&#38590;&#24615;&#22320;&#36951;&#24536;&#26087;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;Compositional Replay Using Memory Blocks (CRUMB)&#65292;&#36890;&#36807;&#37325;&#25918;&#36890;&#36807;&#37325;&#26032;&#32452;&#21512;&#36890;&#29992;&#37096;&#20998;&#37325;&#24314;&#30340;&#29305;&#24449;&#22270;&#26469;&#32531;&#35299;&#36951;&#24536;&#38382;&#39064;&#12290;CRUMB&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#20018;&#32852;&#21487;&#35757;&#32451;&#21644;&#21487;&#37325;&#29992;&#30340;&#8220;&#20869;&#23384;&#22359;&#8221;&#21521;&#37327;&#65292;&#20197;&#32452;&#21512;&#26041;&#24335;&#37325;&#24314;&#29305;&#24449;&#22270;&#24352;&#37327;&#65292;&#23601;&#20687;&#38754;&#21253;&#23633;&#32452;&#21512;&#25104;&#19968;&#20010;&#38754;&#21253;&#19968;&#26679;&#12290;CRUMB&#23384;&#20648;&#29992;&#20110;&#37325;&#24314;&#26032;&#21050;&#28608;&#30340;&#20869;&#23384;&#22359;&#32034;&#24341;&#65292;&#20174;&#32780;&#20351;&#24471;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#33021;&#22815;&#22238;&#25918;&#29305;&#23450;&#30340;&#35760;&#24518;&#12290;&#36825;&#31181;&#37325;&#24314;&#26426;&#21046;&#36824;&#21487;&#20197;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#26368;&#23567;&#21270;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our brains extract durable, generalizable knowledge from transient experiences of the world. Artificial neural networks come nowhere close: when tasked with learning to classify objects by training on non-repeating video frames in temporal order (online stream learning), models that learn well from shuffled datasets catastrophically forget old knowledge upon learning new stimuli. We propose a new continual learning algorithm, Compositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting by replaying feature maps reconstructed by recombining generic parts. CRUMB concatenates trainable and re-usable "memory block" vectors to compositionally reconstruct feature map tensors in convolutional neural networks, like crumbs forming a loaf of bread. CRUMB stores the indices of memory blocks used to reconstruct new stimuli, enabling replay of specific memories during later tasks. This reconstruction mechanism also primes the neural network to minimize catastrophic forgetting by for
&lt;/p&gt;</description></item></channel></rss>