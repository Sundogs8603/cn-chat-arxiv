<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>DATT&#26159;&#19968;&#31181;&#29992;&#20110;&#22235;&#26059;&#32764;&#25511;&#21046;&#30340;&#28145;&#24230;&#33258;&#36866;&#24212;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#31934;&#30830;&#36319;&#36394;&#20219;&#24847;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#24182;&#33021;&#22815;&#22312;&#23384;&#22312;&#22823;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;L1&#33258;&#36866;&#24212;&#25511;&#21046;&#36827;&#34892;&#22686;&#24378;&#65292;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09053</link><description>&lt;p&gt;
DATT&#65306;&#29992;&#20110;&#22235;&#26059;&#32764;&#25511;&#21046;&#30340;&#28145;&#24230;&#33258;&#36866;&#24212;&#36712;&#36857;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
DATT: Deep Adaptive Trajectory Tracking for Quadrotor Control. (arXiv:2310.09053v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09053
&lt;/p&gt;
&lt;p&gt;
DATT&#26159;&#19968;&#31181;&#29992;&#20110;&#22235;&#26059;&#32764;&#25511;&#21046;&#30340;&#28145;&#24230;&#33258;&#36866;&#24212;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#31934;&#30830;&#36319;&#36394;&#20219;&#24847;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#24182;&#33021;&#22815;&#22312;&#23384;&#22312;&#22823;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;L1&#33258;&#36866;&#24212;&#25511;&#21046;&#36827;&#34892;&#22686;&#24378;&#65292;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26410;&#30693;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#12289;&#36712;&#36857;&#19981;&#21487;&#34892;&#24615;&#21644;&#25191;&#34892;&#38480;&#21046;&#65292;&#23545;&#20110;&#22235;&#26059;&#32764;&#30340;&#31934;&#30830;&#20219;&#24847;&#36712;&#36857;&#36319;&#36394;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#33258;&#36866;&#24212;&#36712;&#36857;&#36319;&#36394;&#65288;DATT&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#31934;&#30830;&#22320;&#36319;&#36394;&#20219;&#24847;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#24182;&#22312;&#23384;&#22312;&#22823;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25511;&#21046;&#12290;DATT&#22522;&#20110;&#19968;&#31181;&#22312;&#20223;&#30495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#26032;&#39062;&#21069;&#39304;-&#21453;&#39304;&#33258;&#36866;&#24212;&#25511;&#21046;&#32467;&#26500;&#12290;&#24403;&#22312;&#30495;&#23454;&#30828;&#20214;&#19978;&#37096;&#32626;&#26102;&#65292;DATT&#36890;&#36807;&#22312;&#38381;&#29615;&#20013;&#20351;&#29992;L1&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#24178;&#25200;&#20272;&#35745;&#22120;&#36827;&#34892;&#22686;&#24378;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;DATT&#22312;&#20855;&#26377;&#19981;&#31283;&#23450;&#39118;&#22330;&#30340;&#21487;&#34892;&#24179;&#28369;&#21644;&#19981;&#21487;&#34892;&#36712;&#36857;&#19978;&#65292;&#26126;&#26174;&#20248;&#20110;&#31454;&#20105;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#21644;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#21253;&#25324;&#22522;&#32447;&#23436;&#20840;&#22833;&#25928;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;DATT&#21487;&#20197;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#39640;&#25928;&#36816;&#34892;&#65292;&#25512;&#29702;&#26102;&#38388;&#19981;&#21040;3.2&#27627;&#31186;&#65292;&#20165;&#20026;&#19968;/&#22235;&#20998;&#20043;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise arbitrary trajectory tracking for quadrotors is challenging due to unknown nonlinear dynamics, trajectory infeasibility, and actuation limits. To tackle these challenges, we present Deep Adaptive Trajectory Tracking (DATT), a learning-based approach that can precisely track arbitrary, potentially infeasible trajectories in the presence of large disturbances in the real world. DATT builds on a novel feedforward-feedback-adaptive control structure trained in simulation using reinforcement learning. When deployed on real hardware, DATT is augmented with a disturbance estimator using L1 adaptive control in closed-loop, without any fine-tuning. DATT significantly outperforms competitive adaptive nonlinear and model predictive controllers for both feasible smooth and infeasible trajectories in unsteady wind fields, including challenging scenarios where baselines completely fail. Moreover, DATT can efficiently run online with an inference time less than 3.2 ms, less than 1/4 of the ad
&lt;/p&gt;</description></item><item><title>COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04353</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04353
&lt;/p&gt;
&lt;p&gt;
COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#19982;&#22806;&#37096;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25511;&#21046;&#20219;&#21153;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have recently emerged as a promising approach to control tasks. We present the first language-agent approach to formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries.  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.03149</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03149
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#27010;&#24565;&#25506;&#27979;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#27010;&#24565;&#30340;&#20301;&#32622;&#21644;&#31232;&#30095;&#24615;&#24182;&#19981;&#23436;&#20840;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#22823;&#37327;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#26576;&#20123;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#29305;&#24449;&#65292;&#20316;&#20026;&#20854;&#23545;&#25968;&#25454;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#19968;&#37096;&#20998;&#12290;&#30001;&#20110;&#25317;&#26377;&#27491;&#30830;&#65288;&#25110;&#38169;&#35823;&#65289;&#30340;&#27010;&#24565;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#25105;&#20204;&#24819;&#35201;&#30693;&#36947;&#22312;&#32473;&#23450;&#23618;&#27425;&#19978;&#65292;&#27169;&#22411;&#21407;&#22987;&#35757;&#32451;&#38598;&#20013;&#30340;&#21738;&#20123;&#36755;&#20837;&#23545;&#20110;&#23398;&#20064;&#19968;&#20010;&#27010;&#24565;&#26368;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#19982;&#25506;&#27979;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#32593;&#32476;&#23618;&#27425;&#19978;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24402;&#22240;&#30340;TRAK&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#27010;&#24565;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#32593;&#32476;&#21644;&#25506;&#27979;&#27169;&#22411;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#36890;&#36807;&#31227;&#38500;&#23545;&#19968;&#20010;&#27010;&#24565;&#20855;&#26377;&#26368;&#39640;&#24402;&#22240;&#30340;&#21069;10000&#24352;&#22270;&#20687;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#27010;&#24565;&#22312;&#32593;&#32476;&#20013;&#30340;&#20301;&#32622;&#20197;&#21450;&#27010;&#24565;&#30340;&#25506;&#27979;&#31232;&#30095;&#24615;&#24182;&#27809;&#26377;&#21457;&#29983;&#25913;&#21464;&#12290;&#36825;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#20110;&#23569;&#37327;&#29305;&#23450;&#31034;&#20363;&#19981;&#21516;&#65292;&#29992;&#20110;&#30830;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#20855;&#26377;&#36739;&#39640;&#30340;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
By now there is substantial evidence that deep learning models learn certain human-interpretable features as part of their internal representations of data. As having the right (or wrong) concepts is critical to trustworthy machine learning systems, it is natural to ask which inputs from the model's original training set were most important for learning a concept at a given layer. To answer this, we combine data attribution methods with methods for probing the concepts learned by a model. Training network and probe ensembles for two concept datasets on a range of network layers, we use the recently developed TRAK method for large-scale data attribution. We find some evidence for convergence, where removing the 10,000 top attributing images for a concept and retraining the model does not change the location of the concept in the network nor the probing sparsity of the concept. This suggests that rather than being highly dependent on a few specific examples, the features that inform the 
&lt;/p&gt;</description></item><item><title>AmbientFlow&#26159;&#19968;&#20010;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22270;&#20687;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04856</link><description>&lt;p&gt;
AmbientFlow: &#26469;&#33258;&#19981;&#23436;&#25972;&#12289;&#22122;&#22768;&#27979;&#37327;&#30340;&#21487;&#36870;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AmbientFlow: Invertible generative models from incomplete, noisy measurements. (arXiv:2309.04856v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04856
&lt;/p&gt;
&lt;p&gt;
AmbientFlow&#26159;&#19968;&#20010;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22270;&#20687;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#31185;&#23398;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#37325;&#24314;&#12289;&#21518;&#39564;&#37319;&#26679;&#21644;&#25968;&#25454;&#20849;&#20139;&#12290;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#20197;&#21487;&#34892;&#30340;&#26041;&#24335;&#25552;&#20379;&#31934;&#30830;&#30340;&#23494;&#24230;&#20272;&#35745;&#20197;&#21450;&#24555;&#36895;&#12289;&#24265;&#20215;&#21644;&#22810;&#26679;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#19968;&#20010;&#22823;&#22411;&#12289;&#39640;&#36136;&#37327;&#30340;&#23545;&#35937;&#25968;&#25454;&#38598;&#12290;&#22312;&#35745;&#31639;&#25104;&#20687;&#31561;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#38656;&#35201;&#38271;&#26102;&#38388;&#33719;&#21462;&#25110;&#39640;&#36752;&#23556;&#21058;&#37327;&#65292;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#23545;&#35937;&#30340;&#22122;&#22768;&#25110;&#37096;&#20998;&#35266;&#27979;&#27979;&#37327;&#26356;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AmbientFlow&#65292;&#19968;&#20010;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#30452;&#25509;&#23398;&#20064;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#22122;&#22768;&#12289;&#19981;&#23436;&#25972;&#25968;&#25454;&#24314;&#31435;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have gained popularity for their potential applications in imaging science, such as image reconstruction, posterior sampling and data sharing. Flow-based generative models are particularly attractive due to their ability to tractably provide exact density estimates along with fast, inexpensive and diverse samples. Training such models, however, requires a large, high quality dataset of objects. In applications such as computed imaging, it is often difficult to acquire such data due to requirements such as long acquisition time or high radiation dose, while acquiring noisy or partially observed measurements of these objects is more feasible. In this work, we propose AmbientFlow, a framework for learning flow-based generative models directly from noisy and incomplete data. Using variational Bayesian methods, a novel framework for establishing flow-based generative models from noisy, incomplete data is proposed. Extensive numerical studies demonstrate the effectiveness o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#26469;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02784</link><description>&lt;p&gt;
Norm&#35843;&#25972;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#24615;&#33021;&#20302;&#27604;&#29305;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Norm Tweaking: High-performance Low-bit Quantization of Large Language Models. (arXiv:2309.02784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#26469;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23610;&#23544;&#19981;&#26029;&#22686;&#22823;&#65292;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#21069;&#25552;&#19979;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#24050;&#25104;&#20026;&#37096;&#32626;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#34429;&#28982;&#19968;&#20123;&#37327;&#21270;&#26041;&#27861;&#65292;&#22914;GPTQ&#65292;&#22312;&#23454;&#29616;&#21487;&#25509;&#21463;&#30340;4&#27604;&#29305;&#26435;&#37325;&#37327;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23581;&#35797;&#26356;&#20302;&#20301;&#30340;&#37327;&#21270;&#24448;&#24448;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;norm tweaking&#8221;&#30340;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#24403;&#21069;PTQ&#26041;&#27861;&#30340;&#25554;&#20214;&#65292;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#25104;&#26412;&#39640;&#25928;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#19968;&#39033;&#35266;&#23519;&#30340;&#21551;&#31034;&#65292;&#21363;&#20351;&#35843;&#25972;&#37327;&#21270;&#30340;&#28608;&#27963;&#20998;&#24067;&#20197;&#19982;&#20854;&#28014;&#28857;&#23545;&#24212;&#29289;&#21305;&#37197;&#65292;&#20063;&#21487;&#20197;&#24674;&#22797;LLMs&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#35843;&#25972;&#31574;&#30053;&#65292;&#21253;&#25324;&#29983;&#25104;&#26657;&#20934;&#25968;&#25454;&#21644;&#36890;&#36947;&#36317;&#31163;&#32422;&#26463;&#65292;&#20197;&#26356;&#26032;&#24402;&#19968;&#21270;&#23618;&#30340;&#26435;&#37325;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20960;&#20010;&#24320;&#28304;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of large language models (LLMs) continues to grow, model compression without sacrificing accuracy has become a crucial challenge for deployment. While some quantization methods, such as GPTQ, have made progress in achieving acceptable 4-bit weight-only quantization, attempts at lower bit quantization often result in severe performance degradation. In this paper, we introduce a technique called norm tweaking, which can be used as a plugin in current PTQ methods to achieve high precision while being cost-efficient. Our approach is inspired by the observation that rectifying the quantized activation distribution to match its float counterpart can readily restore accuracy for LLMs. To achieve this, we carefully design a tweaking strategy that includes calibration data generation and channel-wise distance constraint to update the weights of normalization layers for better generalization. We conduct extensive experiments on various datasets using several open-sourced LLMs. Our me
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.08742</link><description>&lt;p&gt;
PMET: &#22312;Transformer&#20013;&#30340;&#31934;&#30830;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#30693;&#35782;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26159;&#21069;&#39304;&#32593;&#32476;&#30340;&#38190;&#20540;&#20869;&#23384;&#30340;&#20540;&#12290;&#23427;&#20204;&#36890;&#24120;&#20248;&#21270;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26469;&#35760;&#24518;&#30446;&#26631;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#30340;&#20449;&#24687;&#27969;&#26469;&#33258;&#19977;&#20010;&#37096;&#20998;&#65306;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#21253;&#21547;&#20102;&#21069;&#39304;&#32593;&#32476;&#29305;&#21035;&#38656;&#35201;&#30340;&#20449;&#24687;&#36825;&#19968;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#32534;&#36753;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#12290;&#36825;&#24847;&#21619;&#30528;&#24403;&#24341;&#20837;&#26032;&#30693;&#35782;&#26102;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#19981;&#38656;&#35201;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;FPGA&#36164;&#28304;&#24863;&#30693;&#30340;&#23454;&#26102;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20462;&#21098;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#36164;&#28304;&#24863;&#30693;&#24352;&#37327;&#32467;&#26500;&#30340;&#32972;&#21253;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20462;&#21098;&#36807;&#31243;&#20013;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#21644;&#36127;&#36733;&#24179;&#34913;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05170</link><description>&lt;p&gt;
FPGA&#36164;&#28304;&#24863;&#30693;&#30340;&#23454;&#26102;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
FPGA Resource-aware Structured Pruning for Real-Time Neural Networks. (arXiv:2308.05170v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;FPGA&#36164;&#28304;&#24863;&#30693;&#30340;&#23454;&#26102;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20462;&#21098;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#36164;&#28304;&#24863;&#30693;&#24352;&#37327;&#32467;&#26500;&#30340;&#32972;&#21253;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20462;&#21098;&#36807;&#31243;&#20013;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#21644;&#36127;&#36733;&#24179;&#34913;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#38899;&#35782;&#21035;&#12289;&#31185;&#23398;&#20998;&#26512;&#31561;&#20247;&#22810;&#24212;&#29992;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#23454;&#26102;&#31995;&#32479;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#23545;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#21644;&#26356;&#20302;&#30340;&#21151;&#32791;&#38656;&#27714;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;FPGA&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#25512;&#26029;&#30340;&#21512;&#36866;&#35774;&#22791;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#21387;&#32553;&#25216;&#26415;&#65292;&#22914;&#20462;&#21098;&#12289;&#37327;&#21270;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#20462;&#21098;&#31232;&#30095;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20943;&#23569;&#20102;&#20056;&#27861;&#21644;&#20869;&#23384;&#30340;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#20462;&#21098;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#24213;&#23618;&#30828;&#20214;&#30340;&#29305;&#24615;&#65292;&#23548;&#33268;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#21644;&#36127;&#36733;&#24179;&#34913;&#25928;&#29575;&#20302;&#19979;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#36164;&#28304;&#25913;&#36827;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#30828;&#20214;&#20026;&#20013;&#24515;&#30340;&#20462;&#21098;&#20844;&#24335;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#36164;&#28304;&#24863;&#30693;&#24352;&#37327;&#32467;&#26500;&#30340;&#32972;&#21253;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks achieve state-of-the-art performance in image classification, speech recognition, scientific analysis and many more application areas. With the ever-increasing need for faster computation and lower power consumption, driven by real-time systems and Internet-of-Things (IoT) devices, FPGAs have emerged as suitable devices for deep learning inference. Due to the high computational complexity and memory footprint of neural networks, various compression techniques, such as pruning, quantization and knowledge distillation, have been proposed in literature. Pruning sparsifies a neural network, reducing the number of multiplications and memory. However, pruning often fails to capture properties of the underlying hardware, causing unstructured sparsity and load-balance inefficiency, thus bottlenecking resource improvements. We propose a hardware-centric formulation of pruning, by formulating it as a knapsack problem with resource-aware tensor structures. The primary emphasis is 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.00031</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00031
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;&#36817;&#21313;&#24180;&#26469;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#26368;&#20196;&#20154;&#20852;&#22859;&#30340;&#21457;&#23637;&#20043;&#19968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#32463;&#25104;&#20026;&#38750;&#24120;&#25104;&#21151;&#30340;&#33539;&#24335;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;RL&#24212;&#29992;&#20110;&#29983;&#25104;AI&#20013;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35752;&#35770;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65292;&#21363;&#20316;&#20026;&#19968;&#31181;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#65292;&#20316;&#20026;&#19968;&#31181;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#65292;&#20197;&#21450;&#20316;&#20026;&#19968;&#31181;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#36731;&#26494;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#35843;&#26597;&#32467;&#26524;&#20013;&#23545;&#36825;&#20010;&#36855;&#20154;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#21644;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#21457;&#29616;&#65292;Bellman &#35823;&#24046;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02345</link><description>&lt;p&gt;
LLQL: &#36923;&#36753;&#20284;&#28982; Q-Learning &#29992;&#20110;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning. (arXiv:2307.02345v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#21644;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#21457;&#29616;&#65292;Bellman &#35823;&#24046;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#20998;&#20026;&#22312;&#32447;&#21644;&#31163;&#32447;&#20004;&#31181;&#21464;&#20307;&#12290;&#20316;&#20026;&#22312;&#32447;&#21644;&#31163;&#32447; RL &#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#24403;&#21069;&#23545; Bellman &#26041;&#31243;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#25216;&#26415;&#21644;&#24615;&#33021;&#22686;&#24378;&#19978;&#65292;&#32780;&#19981;&#26159;&#25506;&#32034; Bellman &#35823;&#24046;&#30340;&#22266;&#26377;&#32467;&#26500;&#29305;&#24615;&#65292;&#22914;&#20854;&#20998;&#24067;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545; Bellman &#26041;&#31243;&#36827;&#34892;&#36845;&#20195;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#22312;&#32447; RL &#21644;&#31163;&#32447; RL &#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#24773;&#20917;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26080;&#35770;&#26159;&#22312;&#32447; RL &#36824;&#26159;&#31163;&#32447; RL&#65292;Bellman &#35823;&#24046;&#37117;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#65288;LLoss&#65289;&#20316;&#20026;&#24120;&#29992;&#30340; MSE Loss &#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20551;&#35774; Bellman &#35823;&#24046;&#26381;&#20174;&#27491;&#24577;&#20998;&#24067;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#22312;&#19981;&#21516;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern reinforcement learning (RL) can be categorized into online and offline variants. As a pivotal aspect of both online and offline RL, current research on the Bellman equation revolves primarily around optimization techniques and performance enhancement rather than exploring the inherent structural properties of the Bellman error, such as its distribution characteristics. This study investigates the distribution of the Bellman approximation error in both online and offline settings through iterative exploration of the Bellman equation. We observed that both in online RL and offline RL, the Bellman error conforms to a Logistic distribution. Building upon this discovery, this study employed the Logistics maximum likelihood function (LLoss) as an alternative to the commonly used MSE Loss, assuming that Bellman errors adhere to a normal distribution. We validated our hypotheses through extensive numerical experiments across diverse online and offline environments. In particular, we app
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;15.5B&#21442;&#25968;&#21644;8K&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;StarCoder&#65292;&#20854;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#22823;&#25209;&#37327;&#25512;&#29702;&#12290;&#32463;&#35780;&#20272;&#35777;&#26126;&#65292;&#22312;Python&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#33719;&#24471;40\%&#30340;pass@1&#30340;&#24471;&#20998;&#65292;&#19988;&#22312;&#20854;&#20182;&#31243;&#24207;&#20013;&#20063;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06161</link><description>&lt;p&gt;
StarCoder: &#28304;&#20195;&#30721;&#19982;&#20320;&#21516;&#22312;&#65281;
&lt;/p&gt;
&lt;p&gt;
StarCoder: may the source be with you!. (arXiv:2305.06161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;15.5B&#21442;&#25968;&#21644;8K&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;StarCoder&#65292;&#20854;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#22823;&#25209;&#37327;&#25512;&#29702;&#12290;&#32463;&#35780;&#20272;&#35777;&#26126;&#65292;&#22312;Python&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#33719;&#24471;40\%&#30340;pass@1&#30340;&#24471;&#20998;&#65292;&#19988;&#22312;&#20854;&#20182;&#31243;&#24207;&#20013;&#20063;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BigCode&#31038;&#21306;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#31185;&#23398;&#21512;&#20316;&#32452;&#32455;&#65292;&#33268;&#21147;&#20110;&#24320;&#21457;&#20195;&#34920;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Code LLMs&#65289;&#30340;&#36127;&#36131;&#20219;&#21457;&#23637;&#12290;&#35813;&#25991;&#20171;&#32461;&#20102;StarCoder&#21644;StarCoderBase&#65292;&#36825;&#26159;&#20855;&#26377;15.5B&#21442;&#25968;&#27169;&#22411;&#21644;8K&#19978;&#19979;&#25991;&#38271;&#24230;&#12289;&#22635;&#20805;&#33021;&#21147;&#20197;&#21450;&#22810;&#31181;&#26597;&#35810;&#27880;&#24847;&#21147;&#23454;&#29616;&#30340;&#24555;&#36895;&#22823;&#25209;&#37327;&#25512;&#29702;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;StarCoderBase&#30340;1&#19975;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892; fine-tuning&#65292;&#21019;&#24314;&#20102;StarCoder&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;Code LLMs&#35780;&#20272;&#65292;&#24182;&#34920;&#26126;StarCoderBase&#20248;&#20110;&#25903;&#25345;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#27599;&#20010;&#24320;&#25918;Code LLM&#65292;&#24182;&#19982;OpenAI code-cushman-001&#27169;&#22411;&#30456;&#21305;&#37197;&#25110;&#20248;&#20110;&#35813;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;StarCoder&#22312;Python&#19978;&#20063;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#65292;&#33021;&#22815;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#33719;&#24471;40\%&#30340;pass@1&#30340;&#24471;&#20998;&#65292;&#24182;&#20173;&#28982;&#20445;&#25345;&#20854;&#22312;&#20854;&#20182;&#31243;&#24207;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other program
&lt;/p&gt;</description></item><item><title>Structure-CLIP&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20351;&#29992;&#22330;&#26223;&#22270;&#24378;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06152</link><description>&lt;p&gt;
Structure-CLIP: &#32467;&#21512;&#32467;&#26500;&#30693;&#35782;&#20248;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Structure-CLIP: Enhance Multi-modal Language Representations with Structure Knowledge. (arXiv:2305.06152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06152
&lt;/p&gt;
&lt;p&gt;
Structure-CLIP&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20351;&#29992;&#22330;&#26223;&#22270;&#24378;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#38656;&#35201;&#23545;&#25991;&#26412;&#36827;&#34892;&#35814;&#32454;&#35821;&#20041;&#29702;&#35299;&#30340;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#19978;&#36890;&#24120;&#34920;&#29616;&#36739;&#24046;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#19968;&#20123;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21477;&#23376;&#20013;&#23384;&#22312;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;Structure-CLIP&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#38544;&#24335;&#35814;&#32454;&#35821;&#20041;&#65292;&#20197;&#22686;&#24378;&#31934;&#32454;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;(1)&#25105;&#20204;&#20351;&#29992;&#22330;&#26223;&#22270;&#26469;&#26356;&#21152;&#20851;&#27880;&#25991;&#26412;&#20013;&#30340;&#35814;&#32454;&#35821;&#20041;&#23398;&#20064;&#65292;&#24182;&#20805;&#20998;&#25506;&#32034;&#32454;&#31890;&#24230;&#35821;&#20041;&#20043;&#38388;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;(2)&#25105;&#20204;&#32467;&#21512;&#22330;&#26223;&#22270;&#30340;&#30693;&#35782;&#24378;&#21270;&#26694;&#26550;&#26469;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale vision-language pre-training has shown promising advances on various downstream tasks and achieved significant performance in multi-modal understanding and generation tasks. However, existing methods often perform poorly on image-text matching tasks that require a detailed semantics understanding of the text. Although there have been some works on this problem, they do not sufficiently exploit the structural knowledge present in sentences to enhance multi-modal language representations, which leads to poor performance. In this paper, we present an end-to-end framework Structure-CLIP, which integrates latent detailed semantics from the text to enhance fine-grained semantic representations. Specifically, (1) we use scene graphs in order to pay more attention to the detailed semantic learning in the text and fully explore structured knowledge between fine-grained semantics, and (2) we utilize the knowledge-enhanced framework with the help of the scene graph to make full use of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;CVaR&#21644;EVaR&#39118;&#38505;&#24230;&#37327;&#21160;&#24577;&#20998;&#35299;&#26159;&#30495;&#23454;&#39118;&#38505;&#20540;&#30340;&#20005;&#26684;&#39640;&#20272;&#35745;&#65292;&#28982;&#32780;VaR&#23384;&#22312;&#31934;&#30830;&#30340;&#21160;&#24577;&#20998;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.12477</link><description>&lt;p&gt;
&#20851;&#20110;&#38745;&#24577;&#39118;&#38505;&#24230;&#37327;&#30340;&#21160;&#24577;&#35268;&#21010;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
On Dynamic Program Decompositions of Static Risk Measures. (arXiv:2304.12477v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;CVaR&#21644;EVaR&#39118;&#38505;&#24230;&#37327;&#21160;&#24577;&#20998;&#35299;&#26159;&#30495;&#23454;&#39118;&#38505;&#20540;&#30340;&#20005;&#26684;&#39640;&#20272;&#35745;&#65292;&#28982;&#32780;VaR&#23384;&#22312;&#31934;&#30830;&#30340;&#21160;&#24577;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20248;&#21270;&#38745;&#24577;&#39118;&#38505;&#35268;&#36991;&#30446;&#26631;&#20855;&#26377;&#19968;&#23450;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#23481;&#26131;&#25509;&#21463;&#21160;&#24577;&#35268;&#21010;&#20998;&#35299;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#39118;&#38505;&#24230;&#37327;&#30340;&#21160;&#24577;&#20998;&#35299;&#26469;&#21046;&#23450;&#25193;&#23637;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;&#21160;&#24577;&#35268;&#21010;&#12290;&#26412;&#25991;&#34920;&#26126;&#20960;&#31181;&#29616;&#26377;&#30340;&#20998;&#35299;&#26412;&#36136;&#19978;&#26159;&#19981;&#31934;&#30830;&#30340;&#65292;&#36825;&#19982;&#25991;&#29486;&#20013;&#30340;&#20960;&#20010;&#22768;&#26126;&#30456;&#30683;&#30462;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20030;&#20986;&#20102;&#19968;&#20123;&#20363;&#23376;&#65292;&#35777;&#26126;&#20102;CVaR&#21644;EVaR&#39118;&#38505;&#24230;&#37327;&#30340;&#27969;&#34892;&#20998;&#35299;&#26159;&#30495;&#23454;&#39118;&#38505;&#20540;&#30340;&#20005;&#26684;&#39640;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;VaR&#30830;&#23454;&#23384;&#22312;&#31934;&#30830;&#30340;&#20998;&#35299;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#31616;&#21333;&#30340;&#35777;&#26126;&#65292;&#38416;&#26126;&#20102;VaR&#21644;CVaR&#21160;&#24577;&#35268;&#21010;&#23646;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing static risk-averse objectives in Markov decision processes is challenging because they do not readily admit dynamic programming decompositions. Prior work has proposed to use a dynamic decomposition of risk measures that help to formulate dynamic programs on an augmented state space. This paper shows that several existing decompositions are inherently inexact, contradicting several claims in the literature. In particular, we give examples that show that popular decompositions for CVaR and EVaR risk measures are strict overestimates of the true risk values. However, an exact decomposition is possible for VaR, and we give a simple proof that illustrates the fundamental difference between VaR and CVaR dynamic programming properties.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(GPT&#21644;BERT)&#35782;&#21035;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#24615;&#33021;, &#32467;&#26524;&#26174;&#31034;BERT&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#20854;&#20013;PubMedBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#31934;&#24230;&#21644;F1&#20998;&#25968;&#65292;BioM-ALBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.17728</link><description>&lt;p&gt;
&#22522;&#20110;GPT&#21644;BERT&#30340;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#37492;&#23450;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text. (arXiv:2303.17728v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17728
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(GPT&#21644;BERT)&#35782;&#21035;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#24615;&#33021;, &#32467;&#26524;&#26174;&#31034;BERT&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#20854;&#20013;PubMedBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#31934;&#24230;&#21644;F1&#20998;&#25968;&#65292;BioM-ALBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;(PPIs)&#23545;&#20110;&#29702;&#35299;&#36951;&#20256;&#26426;&#21046;&#12289;&#30142;&#30149;&#21457;&#30149;&#26426;&#29702;&#21644;&#33647;&#29289;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#21644;&#20934;&#30830;&#25552;&#21462;PPIs&#20197;&#20419;&#36827;&#31185;&#23398;&#30693;&#35782;&#30340;&#21457;&#25496;&#12290;&#24050;&#32463;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;(GPT)&#21644;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#21464;&#21387;&#22120;(BERT)&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#25163;&#21160;&#32534;&#21046;&#30340;LLL&#22522;&#20934;&#35821;&#26009;&#24211;&#35780;&#20272;&#20102;&#21508;&#31181;GPT&#21644;BERT&#27169;&#22411;&#30340;PPI&#35782;&#21035;&#24615;&#33021;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;77&#20010;&#21477;&#23376;&#20013;&#30340;164&#20010;PPIs&#12290;BERT&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;PubMedBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#31934;&#24230;(85.17%)&#21644;F1&#20998;&#25968;(86.47%)&#65292;BioM-ALBERT&#20855;&#26377;&#26368;&#39640;&#30340;&#21484;&#22238;&#29575;(93.83%)&#12290;&#23613;&#31649;GPT-4&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#20854;&#24615;&#33021;&#21487;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting protein-protein interactions (PPIs) is crucial for understanding genetic mechanisms, disease pathogenesis, and drug design. However, with the fast-paced growth of biomedical literature, there is a growing need for automated and accurate extraction of PPIs to facilitate scientific knowledge discovery. Pre-trained language models, such as generative pre-trained transformer (GPT) and bidirectional encoder representations from transformers (BERT), have shown promising results in natural language processing (NLP) tasks. We evaluated the PPI identification performance of various GPT and BERT models using a manually curated benchmark corpus of 164 PPIs in 77 sentences from learning language in logic (LLL). BERT-based models achieved the best overall performance, with PubMedBERT achieving the highest precision (85.17%) and F1-score (86.47%) and BioM-ALBERT achieving the highest recall (93.83%). Despite not being explicitly trained for biomedical texts, GPT-4 achieved comparable perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21407;&#22987;&#38647;&#36798;&#27169;&#25311;&#25968;&#23383;&#65288;ADC&#65289;&#25968;&#25454;&#19978;&#25191;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20449;&#21495;&#22788;&#29702;&#27169;&#22359;&#34987;&#23884;&#20837;&#32593;&#32476;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11420</link><description>&lt;p&gt;
ADCNet&#65306;&#24102;&#21407;&#22987;&#38647;&#36798;ADC&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
ADCNet: End-to-end perception with raw radar ADC data. (arXiv:2303.11420v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21407;&#22987;&#38647;&#36798;&#27169;&#25311;&#25968;&#23383;&#65288;ADC&#65289;&#25968;&#25454;&#19978;&#25191;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20449;&#21495;&#22788;&#29702;&#27169;&#22359;&#34987;&#23884;&#20837;&#32593;&#32476;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#34892;&#19994;&#23545;&#38647;&#36798;&#20256;&#24863;&#22120;&#30340;&#20852;&#36259;&#37325;&#26032;&#28608;&#21457;&#12290;&#38647;&#36798;&#20316;&#20026;&#19968;&#31181;&#30456;&#23545;&#25104;&#29087;&#30340;&#25216;&#26415;&#65292;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#24471;&#21040;&#20102;&#31283;&#23450;&#30340;&#25913;&#36827;&#65292;&#20351;&#20854;&#25104;&#20026;&#24120;&#29992;&#30340;LiDAR&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#21697;&#25110;&#34917;&#20805;&#12290;&#19968;&#31181;&#26032;&#20852;&#30340;&#36235;&#21183;&#26159;&#21033;&#29992;&#20016;&#23500;&#30340;&#20302;&#32423;&#21035;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#24863;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36235;&#21183;&#25512;&#21521;&#20102;&#26497;&#31471;--&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21407;&#22987;&#38647;&#36798;&#27169;&#25311;&#25968;&#23383;&#65288;ADC&#65289;&#25968;&#25454;&#19978;&#25191;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20449;&#21495;&#22788;&#29702;&#27169;&#22359;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001;&#20256;&#32479;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#24341;&#23548;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#30340;&#25972;&#20307;&#26377;&#25928;&#24615;&#65292;&#32780;&#28040;&#34701;&#30740;&#31350;&#39564;&#35777;&#20102;&#25105;&#20204;&#20010;&#20307;&#21019;&#26032;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a renewed interest in radar sensors in the autonomous driving industry. As a relatively mature technology, radars have seen steady improvement over the last few years, making them an appealing alternative or complement to the commonly used LiDARs. An emerging trend is to leverage rich, low-level radar data for perception. In this work we push this trend to the extreme -- we propose a method to perform end-to-end learning on the raw radar analog-to-digital (ADC) data. Specifically, we design a learnable signal processing module inside the neural network, and a pre-training method guided by traditional signal processing algorithms. Experiment results corroborate the overall efficacy of the end-to-end learning method, while an ablation study validates the effectiveness of our individual innovations.
&lt;/p&gt;</description></item><item><title>&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#26159;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19982;&#38598;&#25104;&#23398;&#20064;&#65288;EL&#65289;&#30456;&#32467;&#21512;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#27169;&#22411;&#25110;&#22521;&#35757;&#31639;&#27861;&#20840;&#38754;&#25506;&#32034;&#38382;&#39064;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.02618</link><description>&lt;p&gt;
&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Ensemble Reinforcement Learning: A Survey. (arXiv:2303.02618v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02618
&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#26159;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19982;&#38598;&#25104;&#23398;&#20064;&#65288;EL&#65289;&#30456;&#32467;&#21512;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#27169;&#22411;&#25110;&#22521;&#35757;&#31639;&#27861;&#20840;&#38754;&#25506;&#32034;&#38382;&#39064;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#21508;&#31181;&#31185;&#23398;&#21644;&#24212;&#29992;&#38382;&#39064;&#30340;&#39640;&#25928;&#25216;&#26415;&#12290;&#23613;&#31649;&#20854;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26576;&#20123;&#22797;&#26434;&#20219;&#21153;&#20173;&#38590;&#20197;&#20165;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#21644;&#31639;&#27861;&#35299;&#20915;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;RL&#21644;&#38598;&#25104;&#23398;&#20064;&#65288;EL&#65289;&#30340;&#20248;&#28857;&#65292;&#24050;&#32463;&#24191;&#27867;&#21463;&#21040;&#27426;&#36814;&#12290;ERL&#21033;&#29992;&#22810;&#20010;&#27169;&#22411;&#25110;&#22521;&#35757;&#31639;&#27861;&#20840;&#38754;&#25506;&#32034;&#38382;&#39064;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;ERL&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#20197;&#20415;&#20026;&#35835;&#32773;&#25552;&#20379;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#27010;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;ERL&#30340;&#32972;&#26223;&#21644;&#21160;&#26426;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#25104;&#21151;&#24212;&#29992;&#20110;ERL&#20013;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#27169;&#22411;&#24179;&#22343;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#22411;&#32452;&#21512;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#24182;&#20998;&#26512;&#20102;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has emerged as a highly effective technique for addressing various scientific and applied problems. Despite its success, certain complex tasks remain challenging to be addressed solely with a single model and algorithm. In response, ensemble reinforcement learning (ERL), a promising approach that combines the benefits of both RL and ensemble learning (EL), has gained widespread popularity. ERL leverages multiple models or training algorithms to comprehensively explore the problem space and possesses strong generalization capabilities. In this study, we present a comprehensive survey on ERL to provide readers with an overview of recent advances and challenges in the field. First, we introduce the background and motivation for ERL. Second, we analyze in detail the strategies that have been successfully applied in ERL, including model averaging, model selection, and model combination. Subsequently, we summarize the datasets and analyze algorithms used in releva
&lt;/p&gt;</description></item><item><title>&#20174;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#21306;&#20998;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21387;&#32553;&#34920;&#31034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Q-Score&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2203.01881</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#20998;&#29305;&#24449;&#24230;&#37327;&#19979;&#28216;&#20998;&#31867;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features. (arXiv:2203.01881v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01881
&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#21306;&#20998;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21387;&#32553;&#34920;&#31034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Q-Score&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23427;&#20204;&#30340;&#22833;&#36133;&#27169;&#24335;&#21644;&#23398;&#20064;&#34920;&#31034;&#30340;&#35299;&#37322;&#65292;&#23384;&#22312;&#30528;&#26377;&#38480;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102; SimCLR&#12289;SwaV&#12289;MoCo&#12289;BYOL&#12289;DINO&#12289;SimSiam&#12289;VICReg &#21644; Barlow Twins &#31561;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#22312;&#19981;&#20351;&#29992;&#31867;&#26631;&#31614;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#23545;&#24212;&#20110;&#22270;&#20687;&#20013;&#29420;&#29305;&#29289;&#29702;&#23646;&#24615;&#30340;&#21306;&#20998;&#29305;&#24449;&#65292;&#36825;&#20123;&#21306;&#20998;&#29305;&#24449;&#20027;&#35201;&#23384;&#22312;&#20110;&#27491;&#30830;&#20998;&#31867;&#30340;&#34920;&#31034;&#20013;&#12290;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#34920;&#31034;&#31354;&#38388;&#21387;&#32553;&#22810;&#36798; 40%&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#32447;&#24615;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65288;&#25110; Q-Score&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#12289;&#26080;&#30417;&#30563;&#30340;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#19968;&#20010;&#32473;&#23450;&#26679;&#26412;&#22312;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#26159;&#21542;&#21487;&#33021;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#22312; ImageNet-100 &#21644; ImageNet-1K &#19978;&#23454;&#29616;&#20102; AUPRC &#20998;&#21035;&#20026; 91.45 &#21644; 78.78&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to $40\%$ without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), a model-agnostic, unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24310;&#36831;&#29615;&#22659;&#20013;&#65292;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#24310;&#36831;&#25191;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#21407;&#22987;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#38750;&#22266;&#23450;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#26368;&#22823;&#22870;&#21169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24310;&#36831;&#25191;&#34892;&#20219;&#21153;&#30340;&#38750;&#22266;&#23450;Q-learning&#39118;&#26684;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2101.11992</link><description>&lt;p&gt;
&#22312;&#24310;&#36831;&#29615;&#22659;&#20013;&#20197;&#38750;&#22266;&#23450;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Acting in Delayed Environments with Non-Stationary Markov Policies. (arXiv:2101.11992v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.11992
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24310;&#36831;&#29615;&#22659;&#20013;&#65292;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#24310;&#36831;&#25191;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#21407;&#22987;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#38750;&#22266;&#23450;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#26368;&#22823;&#22870;&#21169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24310;&#36831;&#25191;&#34892;&#20219;&#21153;&#30340;&#38750;&#22266;&#23450;Q-learning&#39118;&#26684;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#20551;&#35774;&#22312;&#36873;&#25321;&#21160;&#20316;&#21518;&#31435;&#21363;&#25191;&#34892;&#65292;&#20294;&#36825;&#31181;&#20551;&#35774;&#24120;&#24120;&#19981;&#20999;&#23454;&#38469;&#65292;&#20250;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#12289;&#20113;&#35745;&#31639;&#21644;&#37329;&#34701;&#31561;&#24212;&#29992;&#20013;&#23548;&#33268;&#28798;&#38590;&#24615;&#25925;&#38556;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#35745;&#21010;&#30340;MDP&#26694;&#26550;&#65292;&#20854;&#20013;&#20915;&#31574;&#32773;&#36873;&#25321;&#30340;&#21160;&#20316;&#38656;&#35201;&#24310;&#36831;$m$&#27493;&#25165;&#33021;&#25191;&#34892;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24310;&#36831;&#25191;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#21407;&#22987;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#36275;&#20197;&#23454;&#29616;&#26368;&#22823;&#22870;&#21169;&#65292;&#20294;&#38656;&#35201;&#26159;&#38750;&#22266;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22266;&#23450;&#30340;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#27425;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#22266;&#23450;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;Q-learning&#39118;&#26684;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#24310;&#36831;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard Markov Decision Process (MDP) formulation hinges on the assumption that an action is executed immediately after it was chosen. However, assuming it is often unrealistic and can lead to catastrophic failures in applications such as robotic manipulation, cloud computing, and finance. We introduce a framework for learning and planning in MDPs where the decision-maker commits actions that are executed with a delay of $m$ steps. The brute-force state augmentation baseline where the state is concatenated to the last $m$ committed actions suffers from an exponential complexity in $m$, as we show for policy iteration. We then prove that with execution delay, deterministic Markov policies in the original state-space are sufficient for attaining maximal reward, but need to be non-stationary. As for stationary Markov policies, we show they are sub-optimal in general. Consequently, we devise a non-stationary Q-learning style model-based algorithm that solves delayed execution tasks wi
&lt;/p&gt;</description></item></channel></rss>