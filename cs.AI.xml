<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#23450;&#27169;&#24577;&#30340;Transformer&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#32452;&#20214;&#36830;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.14405</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#36335;&#24452;&#65306;&#36890;&#36807;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;Transformer
&lt;/p&gt;
&lt;p&gt;
Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities. (arXiv:2401.14405v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#23450;&#27169;&#24577;&#30340;Transformer&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#32452;&#20214;&#36830;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26469;&#33258;&#20854;&#20182;&#27169;&#24577;&#30340;&#26080;&#20851;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#23450;&#27169;&#24577;&#30340;Transformer&#65292;&#20363;&#22914;&#65292;&#20351;&#29992;&#38899;&#39057;&#25110;&#28857;&#20113;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;ImageNet&#27169;&#22411;&#12290;&#25105;&#20204;&#24378;&#35843;&#30446;&#26631;&#27169;&#24577;&#30340;&#25968;&#25454;&#26679;&#26412;&#19982;&#20854;&#20182;&#27169;&#24577;&#26080;&#20851;&#65292;&#36825;&#19982;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#37197;&#23545;&#25968;&#25454;&#65288;&#22914;CLIP&#65289;&#25110;&#20132;&#38169;&#25968;&#25454;&#30340;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#36335;&#24452;&#30340;&#26041;&#27861;-&#32473;&#23450;&#30446;&#26631;&#27169;&#24577;&#21644;&#35774;&#35745;&#29992;&#20110;&#35813;&#27169;&#24577;&#30340;Transformer&#65292;&#25105;&#20204;&#20351;&#29992;&#20351;&#29992;&#21478;&#19968;&#20010;&#27169;&#24577;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#36741;&#21161;Transformer&#65292;&#24182;&#26500;&#24314;&#36335;&#24452;&#26469;&#36830;&#25509;&#20004;&#20010;&#27169;&#22411;&#30340;&#32452;&#20214;&#65292;&#20197;&#20415;&#30446;&#26631;&#27169;&#24577;&#30340;&#25968;&#25454;&#21487;&#20197;&#34987;&#20004;&#20010;&#27169;&#22411;&#22788;&#29702;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20174;&#20004;&#20010;&#27169;&#24577;&#33719;&#24471;&#30340;Transformer&#30340;&#36890;&#29992;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#12290;&#20316;&#20026;&#20855;&#20307;&#23454;&#29616;&#65292;&#25105;&#20204;&#36890;&#24120;&#20351;&#29992;&#29305;&#23450;&#27169;&#24577;&#30340;tokenizer&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;head&#65292;&#20294;&#26159;&#21033;&#29992;&#36741;&#21161;&#27169;&#22411;&#30340;Transformer block&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#29615;&#22659;&#20013;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#30340;&#20840;&#26632;&#26041;&#27861;&#65292;&#26426;&#22120;&#20154;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26694;&#26550;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#23454;&#36341;&#23398;&#20064;&#36866;&#24212;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26032;&#23545;&#35937;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20302;&#25104;&#26412;&#30340;&#31227;&#21160;&#25805;&#20316;&#30828;&#20214;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2401.14403</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#31227;&#21160;&#25805;&#20316;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#30340;&#21487;&#20851;&#33410;&#29289;&#20307;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adaptive Mobile Manipulation for Articulated Objects In the Open World. (arXiv:2401.14403v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#29615;&#22659;&#20013;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#30340;&#20840;&#26632;&#26041;&#27861;&#65292;&#26426;&#22120;&#20154;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#26694;&#26550;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#23454;&#36341;&#23398;&#20064;&#36866;&#24212;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26032;&#23545;&#35937;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20302;&#25104;&#26412;&#30340;&#31227;&#21160;&#25805;&#20316;&#30828;&#20214;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#30340;&#26080;&#32467;&#26500;&#29615;&#22659;&#20013;&#37096;&#32626;&#26426;&#22120;&#20154;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#36890;&#24120;&#21482;&#22312;&#23553;&#38381;&#30340;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#36827;&#34892;&#30740;&#31350;&#65292;&#20043;&#21069;&#30340;&#31227;&#21160;&#25805;&#20316;&#24037;&#20316;&#20063;&#20165;&#38480;&#20110;&#25342;&#21462;&#12289;&#31227;&#21160;&#12289;&#25918;&#32622;&#65292;&#36825;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#21482;&#26159;&#20912;&#23665;&#19968;&#35282;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24320;&#25918;&#19990;&#30028;&#31227;&#21160;&#25805;&#20316;&#31995;&#32479;&#65292;&#37319;&#29992;&#20840;&#26632;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#20851;&#33410;&#29289;&#20307;&#30340;&#25805;&#20316;&#65292;&#20363;&#22914;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#38376;&#12289;&#26588;&#23376;&#12289;&#25277;&#23625;&#21644;&#20912;&#31665;&#12290;&#26426;&#22120;&#20154;&#21033;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#20808;&#20174;&#19968;&#23567;&#32452;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#28982;&#21518;&#36890;&#36807;&#22312;&#32447;&#23454;&#36341;&#23398;&#20064;&#26469;&#22788;&#29702;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#26032;&#23545;&#35937;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#31227;&#21160;&#25805;&#20316;&#30828;&#20214;&#24179;&#21488;&#65292;&#33021;&#22815;&#22312;&#26080;&#32467;&#26500;&#29615;&#22659;&#20013;&#36827;&#34892;&#23433;&#20840;&#21644;&#33258;&#20027;&#30340;&#22312;&#32447;&#36866;&#24212;&#65292;&#25104;&#26412;&#32422;&#20026;20,000&#32654;&#20803;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;20&#20010;&#21487;&#20851;&#33410;&#30340;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate obje
&lt;/p&gt;</description></item><item><title>TURNA&#26159;&#19968;&#31181;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#33021;&#21147;&#12290;TURNA&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#33021;&#19982;&#22303;&#32819;&#20854;&#35821;&#21333;&#35821;&#27169;&#22411;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2401.14373</link><description>&lt;p&gt;
TURNA: &#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22303;&#32819;&#20854;&#32534;&#30721;-&#35299;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation. (arXiv:2401.14373v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14373
&lt;/p&gt;
&lt;p&gt;
TURNA&#26159;&#19968;&#31181;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#33021;&#21147;&#12290;TURNA&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#33021;&#19982;&#22303;&#32819;&#20854;&#35821;&#21333;&#35821;&#27169;&#22411;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#20559;&#21521;&#20110;&#36164;&#28304;&#20016;&#23500;&#19988;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#65292;&#36825;&#23548;&#33268;&#20102;&#19982;&#36164;&#28304;&#31232;&#32570;&#30340;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TURNA&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#36164;&#28304;&#31232;&#32570;&#30340;&#22303;&#32819;&#20854;&#35821;&#24320;&#21457;&#65292;&#33021;&#22815;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;TURNA&#20351;&#29992;&#22522;&#20110;&#32479;&#19968;&#26694;&#26550;UL2&#30340;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#25105;&#20204;&#19987;&#38376;&#20026;&#27492;&#30446;&#30340;&#31579;&#36873;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#23545;TURNA&#22312;&#22303;&#32819;&#20854;&#35821;&#30340;&#19977;&#20010;&#29983;&#25104;&#20219;&#21153;&#21644;&#20116;&#20010;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TURNA&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#20248;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19982;&#22303;&#32819;&#20854;&#35821;&#21333;&#35821;&#27169;&#22411;&#22312;&#29702;&#35299;&#20219;&#21153;&#19978;&#31454;&#20105;&#12290;TURNA&#24050;&#22312;https://huggingface.co/boun-tabi-LMG/TURNA &#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in natural language processing have predominantly favored well-resourced English-centric models, resulting in a significant gap with low-resource languages. In this work, we introduce the language model TURNA, which is developed for the low-resource language Turkish and is capable of both natural language understanding and generation tasks. TURNA is pretrained with an encoder-decoder architecture based on the unified framework UL2 with a diverse corpus that we specifically curated for this purpose. We evaluated TURNA with three generation tasks and five understanding tasks for Turkish. The results show that TURNA outperforms several multilingual models in both understanding and generation tasks, and competes with monolingual Turkish models in understanding tasks. TURNA is made available at https://huggingface.co/boun-tabi-LMG/TURNA .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#24310;&#36831;&#36755;&#20837;&#20449;&#21495;&#26469;&#23454;&#29616;&#20648;&#22791;&#35745;&#31639;&#26426;&#26368;&#20248;&#25805;&#20316;&#21306;&#22495;&#35782;&#21035;&#30340;&#39640;&#25928;&#20248;&#21270;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25805;&#20316;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14371</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#24310;&#36831;&#36755;&#20837;&#30340;&#29289;&#29702;&#20648;&#22791;&#35745;&#31639;&#26426;&#30340;&#39640;&#25928;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Optimisation of Physical Reservoir Computers using only a Delayed Input. (arXiv:2401.14371v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#24310;&#36831;&#36755;&#20837;&#20449;&#21495;&#26469;&#23454;&#29616;&#20648;&#22791;&#35745;&#31639;&#26426;&#26368;&#20248;&#25805;&#20316;&#21306;&#22495;&#35782;&#21035;&#30340;&#39640;&#25928;&#20248;&#21270;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25805;&#20316;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#20648;&#22791;&#35745;&#31639;&#30340;&#20248;&#21270;&#25216;&#26415;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#20351;&#29992;&#20102;&#20809;&#30005;&#23376;&#35774;&#32622;&#12290;&#20648;&#22791;&#35745;&#31639;&#26159;&#19968;&#31181;&#29992;&#20110;&#20449;&#21495;&#22788;&#29702;&#24212;&#29992;&#30340;&#31283;&#20581;&#26694;&#26550;&#65292;&#39640;&#25928;&#20248;&#21270;&#26041;&#27861;&#30340;&#24320;&#21457;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#20165;&#21033;&#29992;&#24310;&#36831;&#29256;&#26412;&#30340;&#36755;&#20837;&#20449;&#21495;&#26469;&#35782;&#21035;&#20648;&#22791;&#30340;&#26368;&#20339;&#25805;&#20316;&#21306;&#22495;&#65292;&#31616;&#21270;&#20102;&#20256;&#32479;&#19978;&#32791;&#26102;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#20219;&#21153;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#22522;&#20934;&#20219;&#21153;&#21644;&#20648;&#22791;&#25805;&#20316;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an experimental validation of a recently proposed optimization technique for reservoir computing, using an optoelectronic setup. Reservoir computing is a robust framework for signal processing applications, and the development of efficient optimization approaches remains a key challenge. The technique we address leverages solely a delayed version of the input signal to identify the optimal operational region of the reservoir, simplifying the traditionally time-consuming task of hyperparameter tuning. We verify the effectiveness of this approach on different benchmark tasks and reservoir operating conditions.
&lt;/p&gt;</description></item><item><title>Genie&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#23548;&#21521;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#23454;&#29616;&#65306;&#20869;&#23481;&#20934;&#22791;&#12289;&#29983;&#25104;&#21644;&#36807;&#28388;&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#21457;&#29616;&#26159;&#33258;&#28982;&#19988;&#39640;&#36136;&#37327;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#20351;&#29992;&#20154;&#24037;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.14367</link><description>&lt;p&gt;
Genie&#65306;&#23454;&#29616;&#20869;&#23481;&#23548;&#21521;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#20154;&#31867;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Genie: Achieving Human Parity in Content-Grounded Datasets Generation. (arXiv:2401.14367v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14367
&lt;/p&gt;
&lt;p&gt;
Genie&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#23548;&#21521;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#23454;&#29616;&#65306;&#20869;&#23481;&#20934;&#22791;&#12289;&#29983;&#25104;&#21644;&#36807;&#28388;&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#21457;&#29616;&#26159;&#33258;&#28982;&#19988;&#39640;&#36136;&#37327;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#20351;&#29992;&#20154;&#24037;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20869;&#23481;&#23548;&#21521;&#29983;&#25104;&#20219;&#21153;&#65292;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#25512;&#21160;&#36825;&#20123;&#20219;&#21153;&#21457;&#23637;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Genie&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#23548;&#21521;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#65288;a&#65289;&#20869;&#23481;&#20934;&#22791;&#65292;&#65288;b&#65289;&#29983;&#25104;&#65306;&#20174;&#20869;&#23481;&#20013;&#21019;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#31034;&#20363;&#65288;&#20363;&#22914;&#38382;&#39064;-&#31572;&#26696;&#23545;&#25110;&#25688;&#35201;&#65289;&#65292;&#65288;c&#65289;&#36807;&#28388;&#26426;&#21046;&#65292;&#26088;&#22312;&#30830;&#20445;&#29983;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#19977;&#20010;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#65306;&#38271;&#22411;&#38382;&#39064;&#22238;&#31572;&#65288;LFQA&#65289;&#12289;&#25688;&#35201;&#21644;&#20449;&#24687;&#25552;&#21462;&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#21457;&#29616;&#26159;&#33258;&#28982;&#19988;&#39640;&#36136;&#37327;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#20351;&#29992;&#20154;&#24037;&#32534;&#20889;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739; - &#23545;&#20110;LFQA&#65292;&#25105;&#20204;&#19982;ELI5&#21644;ASQA&#36827;&#34892;&#27604;&#36739;&#65292;&#23545;&#20110;&#25688;&#35201;&#65292;&#25105;&#20204;&#19982;CNN-DailyMail&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#25110;&#36229;&#36807;&#20351;&#29992;&#20154;&#24037;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks. To address this gap, we propose Genie, a novel method for automatically generating high-quality content-grounded data. It consists of three stages: (a) Content Preparation, (b) Generation: creating task-specific examples from the content (e.g., question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure the quality and faithfulness of the generated data. We showcase this methodology by generating three large-scale synthetic data, making wishes, for Long-Form Question-Answering (LFQA), summarization, and information extraction. In a human evaluation, our generated data was found to be natural and of high quality. Furthermore, we compare models trained on our data with models trained on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for Summarization. We show that our models are on par with or outperforming models trained 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#20154;&#30340;&#32463;&#21382;&#65292;&#20998;&#26512;&#20102;&#29992;&#25143;&#22914;&#20309;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#21019;&#24314;&#29420;&#29305;&#30340;&#25903;&#25345;&#35282;&#33394;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#24515;&#29702;&#20581;&#24247;&#32972;&#26223;&#19979;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#27835;&#30103;&#20215;&#20540;&#35266;&#30456;&#21305;&#37197;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#35774;&#35745;&#24072;&#22788;&#29702;&#20262;&#29702;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.14362</link><description>&lt;p&gt;
&#25171;&#23383;&#30103;&#27861;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#20013;&#30340;&#24212;&#29992;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support. (arXiv:2401.14362v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#20154;&#30340;&#32463;&#21382;&#65292;&#20998;&#26512;&#20102;&#29992;&#25143;&#22914;&#20309;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#21019;&#24314;&#29420;&#29305;&#30340;&#25903;&#25345;&#35282;&#33394;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#24515;&#29702;&#20581;&#24247;&#32972;&#26223;&#19979;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#27835;&#30103;&#20215;&#20540;&#35266;&#30456;&#21305;&#37197;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#35774;&#35745;&#24072;&#22788;&#29702;&#20262;&#29702;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#24037;&#20855;&#65292;&#20294;&#26377;&#35777;&#25454;&#34920;&#26126;&#36890;&#29992;&#22411;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#20063;&#23384;&#22312;&#19968;&#23450;&#39118;&#38505;&#65292;&#22914;&#26524;&#35774;&#35745;&#19981;&#36127;&#36131;&#20219;&#21487;&#33021;&#20250;&#21361;&#21450;&#29992;&#25143;&#30340;&#31119;&#31049;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#20154;&#30340;&#30495;&#23454;&#32463;&#21382;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#32972;&#26223;&#30340;21&#20010;&#20010;&#20154;&#36827;&#34892;&#35775;&#35848;&#65292;&#20998;&#26512;&#20102;&#29992;&#25143;&#22914;&#20309;&#20026;&#20182;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21019;&#24314;&#29420;&#29305;&#30340;&#25903;&#25345;&#35282;&#33394;&#65292;&#22635;&#34917;&#26085;&#24120;&#25252;&#29702;&#30340;&#31354;&#30333;&#65292;&#24182;&#22312;&#23547;&#27714;&#26469;&#33258;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25903;&#25345;&#26102;&#22914;&#20309;&#23548;&#33322;&#30456;&#20851;&#30340;&#25991;&#21270;&#38480;&#21046;&#12290;&#25105;&#20204;&#23558;&#20998;&#26512;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#25991;&#29486;&#20013;&#26377;&#25928;&#25903;&#25345;&#30340;&#27010;&#24565;&#65292;&#24182;&#24341;&#20837;&#20102;AI&#19982;&#24515;&#29702;&#20581;&#24247;&#32972;&#26223;&#19979;&#30340;&#27835;&#30103;&#20215;&#20540;&#35266;&#23545;&#20854;&#36827;&#34892;&#21305;&#37197;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35774;&#35745;&#24072;&#22914;&#20309;&#22788;&#29702;&#20262;&#29702;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
People experiencing severe distress increasingly use Large Language Model (LLM) chatbots as mental health support tools. Discussions on social media have described how engagements were lifesaving for some, but evidence suggests that general-purpose LLM chatbots also have notable risks that could endanger the welfare of users if not designed responsibly. In this study, we investigate the lived experiences of people who have used LLM chatbots for mental health support. We build on interviews with 21 individuals from globally diverse backgrounds to analyze how users create unique support roles for their chatbots, fill in gaps in everyday care, and navigate associated cultural limitations when seeking support from chatbots. We ground our analysis in psychotherapy literature around effective support, and introduce the concept of therapeutic alignment, or aligning AI with therapeutic values for mental health contexts. Our study offers recommendations for how designers can approach the ethica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28176;&#36827;&#22810;&#20219;&#21153;&#25239;&#22122;&#22768;&#23398;&#20064;&#65288;PMAL&#65289;&#26694;&#26550;&#21644;&#28176;&#36827;&#22810;&#20219;&#21153;&#25552;&#21462;&#65288;PMD&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32454;&#31890;&#24230;&#36710;&#36742;&#35782;&#21035;&#20013;&#30001;&#22270;&#20687;&#22122;&#22768;&#24341;&#36215;&#30340;&#31867;&#20869;&#21464;&#21270;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14336</link><description>&lt;p&gt;
&#28176;&#36827;&#22810;&#20219;&#21153;&#25239;&#22122;&#22768;&#23398;&#20064;&#21644;&#25552;&#21462;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#36710;&#36742;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for Fine-grained Vehicle Recognition. (arXiv:2401.14336v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28176;&#36827;&#22810;&#20219;&#21153;&#25239;&#22122;&#22768;&#23398;&#20064;&#65288;PMAL&#65289;&#26694;&#26550;&#21644;&#28176;&#36827;&#22810;&#20219;&#21153;&#25552;&#21462;&#65288;PMD&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32454;&#31890;&#24230;&#36710;&#36742;&#35782;&#21035;&#20013;&#30001;&#22270;&#20687;&#22122;&#22768;&#24341;&#36215;&#30340;&#31867;&#20869;&#21464;&#21270;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#31890;&#24230;&#36710;&#36742;&#35782;&#21035;&#65288;FGVR&#65289;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#22522;&#30784;&#25216;&#26415;&#65292;&#20294;&#30001;&#20110;&#20869;&#22312;&#30340;&#31867;&#20869;&#21464;&#21270;&#38750;&#24120;&#22256;&#38590;&#12290;&#20043;&#21069;&#30340;FGVR&#30740;&#31350;&#22823;&#22810;&#21482;&#20851;&#27880;&#30001;&#19981;&#21516;&#25293;&#25668;&#35282;&#24230;&#12289;&#20301;&#32622;&#31561;&#24341;&#36215;&#30340;&#31867;&#20869;&#21464;&#21270;&#65292;&#32780;&#30001;&#22270;&#20687;&#22122;&#22768;&#24341;&#36215;&#30340;&#31867;&#20869;&#21464;&#21270;&#24456;&#23569;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#22810;&#20219;&#21153;&#25239;&#22122;&#22768;&#23398;&#20064;&#65288;PMAL&#65289;&#26694;&#26550;&#21644;&#28176;&#36827;&#22810;&#20219;&#21153;&#25552;&#21462;&#65288;PMD&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#30001;&#20110;&#22270;&#20687;&#22122;&#22768;&#24341;&#36215;&#30340;FGVR&#20013;&#30340;&#31867;&#20869;&#21464;&#21270;&#38382;&#39064;&#12290;PMAL&#26694;&#26550;&#36890;&#36807;&#23558;&#22270;&#20687;&#21435;&#22122;&#35270;&#20026;&#22270;&#20687;&#35782;&#21035;&#20013;&#30340;&#38468;&#21152;&#20219;&#21153;&#65292;&#24182;&#36880;&#27493;&#24378;&#21046;&#27169;&#22411;&#23398;&#20064;&#22122;&#22768;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;PMD&#26694;&#26550;&#23558;PMAL&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#21407;&#22987;&#39592;&#24178;&#32593;&#32476;&#20013;&#65292;&#20135;&#29983;&#19968;&#20010;&#20855;&#26377;&#19982;PMAL&#35757;&#32451;&#27169;&#22411;&#30456;&#21516;&#35782;&#21035;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#20294;&#19981;&#38656;&#35201;&#39069;&#22806;&#28155;&#21152;&#20219;&#20309;&#19996;&#35199;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained vehicle recognition (FGVR) is an essential fundamental technology for intelligent transportation systems, but very difficult because of its inherent intra-class variation. Most previous FGVR studies only focus on the intra-class variation caused by different shooting angles, positions, etc., while the intra-class variation caused by image noise has received little attention. This paper proposes a progressive multi-task anti-noise learning (PMAL) framework and a progressive multi-task distilling (PMD) framework to solve the intra-class variation problem in FGVR due to image noise. The PMAL framework achieves high recognition accuracy by treating image denoising as an additional task in image recognition and progressively forcing a model to learn noise invariance. The PMD framework transfers the knowledge of the PMAL-trained model into the original backbone network, which produces a model with about the same recognition accuracy as the PMAL-trained model, but without any add
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.14295</link><description>&lt;p&gt;
&#25512;&#29702;&#30340;&#25299;&#25169;&#23398;&#65306;&#25581;&#31192;&#24605;&#32500;&#38142;&#12289;&#26641;&#21644;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#26041;&#38754;&#12290;&#20854;&#20013;&#65292;&#19982;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#24037;&#31243;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20854;&#35774;&#35745;&#22914;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#31561;&#65292;&#36890;&#36807;&#32467;&#26500;&#25351;&#23548;&#25972;&#20307;LLM&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#30340;&#35828;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#36923;&#36753;&#25110;&#25968;&#23398;&#25512;&#29702;&#12289;&#35268;&#21010;&#25110;&#21019;&#36896;&#24615;&#20889;&#20316;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#26041;&#20415;&#29702;&#35299;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#26041;&#26696;&#30340;&#36890;&#29992;&#34013;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#25191;&#34892;&#27969;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#28548;&#28165;&#24182;&#26126;&#30830;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#31532;&#19968;&#20010;&#20998;&#31867;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#19988;&#25104;&#26412;&#25928;&#30410;&#30340;&#35774;&#35745;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20108;&#32500;&#35302;&#35273;&#29305;&#24449;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#22768;&#23398;&#33021;&#37327;&#21644;&#20998;&#26512;&#24133;&#24230;&#35843;&#21046;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#35302;&#35273;&#29305;&#24449;&#20272;&#35745;&#12290;&#23454;&#38469;&#27979;&#35797;&#35777;&#26126;&#20102;&#35813;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.14292</link><description>&lt;p&gt;
AST-2:&#21333;&#23618;&#21644;&#21452;&#23618;&#20108;&#32500;&#22768;&#23398;&#36719;&#35302;&#35273;&#30382;&#32932;
&lt;/p&gt;
&lt;p&gt;
AST-2: Single and bi-layered 2-D acoustic soft tactile skin. (arXiv:2401.14292v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#19988;&#25104;&#26412;&#25928;&#30410;&#30340;&#35774;&#35745;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20108;&#32500;&#35302;&#35273;&#29305;&#24449;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#22768;&#23398;&#33021;&#37327;&#21644;&#20998;&#26512;&#24133;&#24230;&#35843;&#21046;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#35302;&#35273;&#29305;&#24449;&#20272;&#35745;&#12290;&#23454;&#38469;&#27979;&#35797;&#35777;&#26126;&#20102;&#35813;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#21019;&#26032;&#19988;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#22768;&#23398;&#36719;&#35302;&#35273;(AST)&#30382;&#32932;&#35774;&#35745;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#26174;&#33879;&#25552;&#39640;&#20108;&#32500;&#35302;&#35273;&#29305;&#24449;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30340;&#25361;&#25112;&#22312;&#20110;&#20351;&#29992;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#23454;&#29616;&#31934;&#30830;&#30340;&#35302;&#35273;&#29305;&#24449;&#20272;&#35745;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#25509;&#35302;&#20960;&#20309;&#29305;&#24449;&#12290;&#25105;&#20204;&#20551;&#35774;&#36890;&#36807;&#22312;&#24863;&#27979;&#34920;&#38754;&#19979;&#30340;&#20004;&#23618;&#19987;&#29992;&#22768;&#23398;&#36890;&#36947;&#20013;&#21033;&#29992;&#22768;&#23398;&#33021;&#37327;&#65292;&#24182;&#20998;&#26512;&#24133;&#24230;&#35843;&#21046;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#30721;&#24863;&#27979;&#34920;&#38754;&#19978;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#35302;&#35273;&#29305;&#24449;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#30828;&#20214;&#32452;&#20214;&#26126;&#30830;&#20998;&#31163;&#65292;&#36127;&#36131;&#21457;&#23556;&#21644;&#25509;&#25910;&#22768;&#23398;&#20449;&#21495;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22359;&#21270;&#21644;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#30382;&#32932;&#35774;&#35745;&#12290;&#23454;&#38469;&#27979;&#35797;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#39062;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#36798;&#21040;&#20102;&#22312;&#20272;&#35745;&#25509;&#35302;&#27861;&#21521;&#21147;(MAE &lt;0.8 N)&#21644;&#20108;&#32500;&#25509;&#35302;&#23450;&#20301;&#26041;&#38754;&#30340;&#26174;&#33879;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
This paper aims to present an innovative and cost-effective design for Acoustic Soft Tactile (AST) Skin, with the primary goal of significantly enhancing the accuracy of 2-D tactile feature estimation. The existing challenge lies in achieving precise tactile feature estimation, especially concerning contact geometry characteristics, using cost-effective solutions. We hypothesise that by harnessing acoustic energy through dedicated acoustic channels in 2 layers beneath the sensing surface and analysing amplitude modulation, we can effectively decode interactions on the sensory surface, thereby improving tactile feature estimation. Our approach involves the distinct separation of hardware components responsible for emitting and receiving acoustic signals, resulting in a modular and highly customizable skin design. Practical tests demonstrate the effectiveness of this novel design, achieving remarkable precision in estimating contact normal forces (MAE &lt; 0.8 N), 2D contact localisation (M
&lt;/p&gt;</description></item><item><title>POUR-Net&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#32676;&#20808;&#39564;&#30340;&#36807;&#20559;&#24046;&#20302;&#35745;&#25968;PET&#34928;&#20943;&#26144;&#23556;&#29983;&#25104;&#32593;&#32476;&#65292;&#26088;&#22312;&#36890;&#36807;&#26377;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#20840;&#38754;&#30340;&#20808;&#39564;&#20449;&#24687;&#26469;&#25552;&#39640;&#20302;&#21058;&#37327;PET&#30340;&#34928;&#20943;&#26144;&#23556;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.14285</link><description>&lt;p&gt;
POUR-Net:&#19968;&#31181;&#22522;&#20110;&#20154;&#32676;&#20808;&#39564;&#30340;&#36807;&#20559;&#24046;&#20302;&#35745;&#25968;PET&#34928;&#20943;&#26144;&#23556;&#29983;&#25104;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
POUR-Net: A Population-Prior-Aided Over-Under-Representation Network for Low-Count PET Attenuation Map Generation. (arXiv:2401.14285v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14285
&lt;/p&gt;
&lt;p&gt;
POUR-Net&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#32676;&#20808;&#39564;&#30340;&#36807;&#20559;&#24046;&#20302;&#35745;&#25968;PET&#34928;&#20943;&#26144;&#23556;&#29983;&#25104;&#32593;&#32476;&#65292;&#26088;&#22312;&#36890;&#36807;&#26377;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#20840;&#38754;&#30340;&#20808;&#39564;&#20449;&#24687;&#26469;&#25552;&#39640;&#20302;&#21058;&#37327;PET&#30340;&#34928;&#20943;&#26144;&#23556;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#21058;&#37327;PET&#20026;PET&#25104;&#20687;&#20013;&#20943;&#23569;&#36752;&#23556;&#26292;&#38706;&#25552;&#20379;&#20102;&#19968;&#31181;&#23453;&#36149;&#30340;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36890;&#34892;&#30340;&#20351;&#29992;&#39069;&#22806;&#30340;CT&#25195;&#25551;&#26469;&#29983;&#25104;PET&#34928;&#20943;&#26144;&#23556;&#65288;u-map&#65289;&#30340;&#20570;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36752;&#23556;&#21058;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#36827;&#19968;&#27493;&#20943;&#36731;&#20302;&#21058;&#37327;PET&#26816;&#26597;&#20013;&#30340;&#36752;&#23556;&#26292;&#38706;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;POUR-Net&#8212;&#8212;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#20154;&#32676;&#20808;&#39564;&#30340;&#36807;&#20559;&#24046;&#20302;&#35745;&#25968;PET&#34928;&#20943;&#26144;&#23556;&#29983;&#25104;&#32593;&#32476;&#12290;&#39318;&#20808;&#65292;POUR-Net&#38598;&#25104;&#20102;&#19968;&#31181;&#36807;&#20559;&#24046;&#20302;&#35745;&#25968;&#32593;&#32476;&#65288;OUR-Net&#65289;&#65292;&#20197;&#20419;&#36827;&#26377;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#65292;&#21253;&#25324;&#20302;&#20998;&#36776;&#29575;&#30340;&#25277;&#35937;&#29305;&#24449;&#21644;&#32454;&#33410;&#29305;&#24449;&#65292;&#20197;&#36741;&#21161;&#20840;&#20998;&#36776;&#29575;&#32423;&#21035;&#19978;&#30340;&#28145;&#24230;&#29983;&#25104;&#12290;&#20854;&#27425;&#65292;&#20316;&#20026;OUR-Net&#30340;&#34917;&#20805;&#65292;&#19968;&#20010;&#21033;&#29992;&#20840;&#38754;&#30340;CT&#34893;&#29983;&#30340;u-map&#25968;&#25454;&#38598;&#30340;&#20154;&#32676;&#20808;&#39564;&#29983;&#25104;&#26426;&#65288;PPGM&#65289;&#25552;&#20379;&#39069;&#22806;&#30340;&#20808;&#39564;&#20449;&#24687;&#26469;&#36741;&#21161;OUR-Net&#30340;&#29983;&#25104;&#12290;OUR-Net&#21644;PPGM&#30340;&#38598;&#25104;&#22312;&#20302;&#21058;&#37327;PET&#34928;&#20943;&#26144;&#23556;&#29983;&#25104;&#20013;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-dose PET offers a valuable means of minimizing radiation exposure in PET imaging. However, the prevalent practice of employing additional CT scans for generating attenuation maps (u-map) for PET attenuation correction significantly elevates radiation doses. To address this concern and further mitigate radiation exposure in low-dose PET exams, we propose POUR-Net - an innovative population-prior-aided over-under-representation network that aims for high-quality attenuation map generation from low-dose PET. First, POUR-Net incorporates an over-under-representation network (OUR-Net) to facilitate efficient feature extraction, encompassing both low-resolution abstracted and fine-detail features, for assisting deep generation on the full-resolution level. Second, complementing OUR-Net, a population prior generation machine (PPGM) utilizing a comprehensive CT-derived u-map dataset, provides additional prior information to aid OUR-Net generation. The integration of OUR-Net and PPGM within
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#25509;&#21475;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#21360;&#22320;&#35821;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#26377;&#28508;&#21147;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2401.14280</link><description>&lt;p&gt;
RomanSetu: &#36890;&#36807;&#32599;&#39532;&#21270;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization. (arXiv:2401.14280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#25509;&#21475;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#21360;&#22320;&#35821;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#26377;&#28508;&#21147;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#65288;&#29305;&#21035;&#26159;&#20351;&#29992;&#38750;&#25289;&#19969;&#23383;&#27597;&#34920;&#30340;&#35821;&#35328;&#65289;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25509;&#21475;&#65292;&#20551;&#35774;&#39057;&#32321;&#30340;&#38750;&#27491;&#24335;&#20351;&#29992;&#21644;&#19982;&#33521;&#35821;&#20849;&#20139;&#30340;&#26631;&#35760;&#26377;&#21161;&#20110;&#36328;&#35821;&#35328;&#23545;&#40784;&#12290;&#25105;&#20204;&#20197;&#21360;&#22320;&#35821;&#20026;&#37325;&#28857;&#65292;&#36890;&#36807;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#35777;&#26126;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#30001;&#20110;&#20854;&#36739;&#20302;&#30340;&#29983;&#20135;&#21147;&#32780;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#22810;&#33050;&#26412;&#25552;&#31034;&#26041;&#27861;&#32467;&#21512;&#20102;&#32599;&#39532;&#21270;&#21644;&#21407;&#29983;&#25991;&#26412;&#65292;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#22312;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#26410;&#26469;&#30340;&#24037;&#20316;&#23558;&#33268;&#21147;&#20110;&#23558;&#27492;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#22810;&#30340;&#35821;&#35328;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Latin scripts. We propose an innovative approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Focusing on Hindi, we demonstrate through Hindi-to-English translation and sentiment analysis tasks that romanized text not only significantly improves inference efficiency due to its lower fertility compared to native text but also achieves competitive performance with limited pre-training. Additionally, our novel multi-script prompting approach, which combines romanized and native texts, shows promise in further enhancing task performance. These findings suggest the potential of romanization in bridging the language gap for LLM applications, with future work aimed at expanding this approach to more languages and tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;transformer&#32593;&#32476;&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#30382;&#23618;&#27874;&#22312;&#25552;&#21462;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.14267</link><description>&lt;p&gt;
Transformers&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#65306;&#22312;&#26102;&#38388;&#19978;&#20256;&#36882;&#19978;&#19979;&#25991;&#30340;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers and Cortical Waves: Encoders for Pulling In Context Across Time. (arXiv:2401.14267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;transformer&#32593;&#32476;&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#30382;&#23618;&#27874;&#22312;&#25552;&#21462;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;ChatGPT&#21644;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;transformer&#32593;&#32476;&#30340;&#33021;&#21147;&#24050;&#32463;&#24341;&#36215;&#20102;&#19990;&#30028;&#30340;&#20851;&#27880;&#12290;&#23427;&#20204;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#23558;&#23436;&#25972;&#30340;&#36755;&#20837;&#24207;&#21015;&#65288;&#20363;&#22914;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#21333;&#35789;&#65289;&#36716;&#21270;&#20026;&#19968;&#20010;&#38271;&#30340;&#8220;&#32534;&#30721;&#21521;&#37327;&#8221;&#65292;&#20351;&#24471;transformer&#33021;&#22815;&#23398;&#20064;&#33258;&#28982;&#24207;&#21015;&#20013;&#30340;&#38271;&#31243;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#8220;&#33258;&#27880;&#24847;&#21147;&#8221;&#24212;&#29992;&#20110;&#36825;&#20010;&#32534;&#30721;&#21521;&#37327;&#65292;&#36890;&#36807;&#35745;&#31639;&#36755;&#20837;&#24207;&#21015;&#20013;&#21333;&#35789;&#23545;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#22686;&#24378;&#20102;transformer&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#35748;&#20026;&#31070;&#32463;&#27963;&#21160;&#22312;&#21333;&#20010;&#30382;&#23618;&#21306;&#22495;&#20869;&#25110;&#25972;&#20010;&#22823;&#33041;&#33539;&#22260;&#20869;&#20256;&#25773;&#30340;&#27874;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#32534;&#30721;&#21407;&#29702;&#12290;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#21051;&#23558;&#26368;&#36817;&#30340;&#36755;&#20837;&#21382;&#21490;&#23553;&#35013;&#20026;&#21333;&#20010;&#31354;&#38388;&#27169;&#24335;&#65292;&#30382;&#23618;&#27874;&#21487;&#20197;&#20174;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#25552;&#21462;&#26102;&#38388;&#19978;&#19979;&#25991;&#65292;&#36825;&#19982;&#35745;&#31639;&#21407;&#29702;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities of transformer networks such as ChatGPT and other Large Language Models (LLMs) have captured the world's attention. The crucial computational mechanism underlying their performance relies on transforming a complete input sequence - for example, all the words in a sentence into a long "encoding vector" - that allows transformers to learn long-range temporal dependencies in naturalistic sequences. Specifically, "self-attention" applied to this encoding vector enhances temporal context in transformers by computing associations between pairs of words in the input sequence. We suggest that waves of neural activity, traveling across single cortical regions or across multiple regions at the whole-brain scale, could implement a similar encoding principle. By encapsulating recent input history into a single spatial pattern at each moment in time, cortical waves may enable temporal context to be extracted from sequences of sensory inputs, the same computational principle used in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35270;&#35282;&#33609;&#22270;&#24341;&#23548;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26694;&#26550;(Sketch2NeRF)&#65292;&#20197;&#22686;&#21152;&#23545;3D&#29983;&#25104;&#30340;&#33609;&#22270;&#25511;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#20248;&#21270;3D&#22330;&#26223;&#23454;&#29616;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21512;&#25104;&#19968;&#33268;&#30340;3D&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2401.14257</link><description>&lt;p&gt;
Sketch2NeRF: &#22810;&#35270;&#35282;&#33609;&#22270;&#24341;&#23548;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation. (arXiv:2401.14257v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35270;&#35282;&#33609;&#22270;&#24341;&#23548;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26694;&#26550;(Sketch2NeRF)&#65292;&#20197;&#22686;&#21152;&#23545;3D&#29983;&#25104;&#30340;&#33609;&#22270;&#25511;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#20248;&#21270;3D&#22330;&#26223;&#23454;&#29616;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21512;&#25104;&#19968;&#33268;&#30340;3D&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;3D&#30340;&#26041;&#27861;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;3D&#20869;&#23481;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#23545;&#35937;&#26159;&#38543;&#26426;&#30340;&#65292;&#24182;&#19988;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#25511;&#21046;&#12290;&#33609;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#24265;&#20215;&#30340;&#26041;&#27861;&#26469;&#24341;&#20837;&#36825;&#31181;&#32454;&#31890;&#24230;&#30340;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33609;&#22270;&#30340;&#25277;&#35937;&#21644;&#27169;&#31946;&#24615;&#65292;&#23454;&#29616;&#20174;&#36825;&#20123;&#33609;&#22270;&#20013;&#33719;&#24471;&#28789;&#27963;&#30340;&#25511;&#21046;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35270;&#35282;&#33609;&#22270;&#24341;&#23548;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26694;&#26550;&#65288;&#21363;Sketch2NeRF&#65289;&#65292;&#20197;&#22686;&#21152;&#23545;3D&#29983;&#25104;&#30340;&#33609;&#22270;&#25511;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#65288;&#20363;&#22914;Stable Diffusion&#21644;ControlNet&#65289;&#26469;&#30417;&#30563;&#30001;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#34920;&#31034;&#30340;3D&#22330;&#26223;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#27493;&#29983;&#25104;&#21644;&#37325;&#24314;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#20248;&#21270;NeRF&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20004;&#31181;&#22810;&#35270;&#35282;&#33609;&#22270;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21512;&#25104;&#19968;&#33268;&#30340;3D&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#38450;&#24481;&#26041;&#27861;AR-GAN&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#20132;&#36890;&#26631;&#24535;&#20998;&#31867;&#31995;&#32479;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#19979;&#20445;&#25345;&#39640;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.14232</link><description>&lt;p&gt;
AR-GAN: &#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20132;&#36890;&#26631;&#24535;&#20998;&#31867;&#31995;&#32479;&#30340;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AR-GAN: Generative Adversarial Network-Based Defense Method Against Adversarial Attacks on the Traffic Sign Classification System of Autonomous Vehicles. (arXiv:2401.14232v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14232
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#38450;&#24481;&#26041;&#27861;AR-GAN&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#20132;&#36890;&#26631;&#24535;&#20998;&#31867;&#31995;&#32479;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#19979;&#20445;&#25345;&#39640;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#20132;&#36890;&#26631;&#24535;&#20998;&#31867;&#65292;&#31216;&#20026;&#25915;&#20987;&#40065;&#26834;&#30340;GAN&#65288;AR-GAN&#65289;&#12290;AR-GAN&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65288;i&#65289;&#20551;&#35774;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#21644;&#26679;&#26412;&#19968;&#26080;&#25152;&#30693;&#65292;&#65288;ii&#65289;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#31867;&#22411;&#19979;&#22987;&#32456;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#20132;&#36890;&#26631;&#24535;&#20998;&#31867;&#24615;&#33021;&#12290;AR-GAN&#20998;&#31867;&#31995;&#32479;&#30001;&#19968;&#20010;&#36890;&#36807;&#37325;&#24314;&#21435;&#22122;&#22270;&#20687;&#30340;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#23545;&#37325;&#24314;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#30340;&#20998;&#31867;&#22120;&#32452;&#25104;&#12290;&#20316;&#32773;&#22312;&#27809;&#26377;&#25915;&#20987;&#21644;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#65288;&#22914;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#27861;&#65288;FGSM&#65289;&#65292;DeepFool&#65292;Carlini&#21644;Wagner&#65288;C&amp;W&#65289;&#65292;&#20197;&#21450;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;PGD&#65289;&#65289;&#30340;&#24773;&#20917;&#19979;&#27979;&#35797;&#20102;AR-GAN&#12290;&#20316;&#32773;&#32771;&#34385;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#20004;&#31181;&#24418;&#24335;&#65292;&#21363;&#65288;i&#65289;&#40657;&#30418;&#25915;&#20987;&#65288;&#20551;&#35774;&#25915;&#20987;&#32773;&#23545;&#20998;&#31867;&#22120;&#27809;&#26377;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#65289;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#30333;&#30418;&#25915;&#20987;&#65288;&#20551;&#35774;&#25915;&#20987;&#32773;&#25317;&#26377;&#23436;&#20840;&#30693;&#35782;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study developed a generative adversarial network (GAN)-based defense method for traffic sign classification in an autonomous vehicle (AV), referred to as the attack-resilient GAN (AR-GAN). The novelty of the AR-GAN lies in (i) assuming zero knowledge of adversarial attack models and samples and (ii) providing consistently high traffic sign classification performance under various adversarial attack types. The AR-GAN classification system consists of a generator that denoises an image by reconstruction, and a classifier that classifies the reconstructed image. The authors have tested the AR-GAN under no-attack and under various adversarial attacks, such as Fast Gradient Sign Method (FGSM), DeepFool, Carlini and Wagner (C&amp;W), and Projected Gradient Descent (PGD). The authors considered two forms of these attacks, i.e., (i) black-box attacks (assuming the attackers possess no prior knowledge of the classifier), and (ii) white-box attacks (assuming the attackers possess full knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22359;&#30340;&#21487;&#31227;&#26893;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#31227;&#26893;&#30340;&#27169;&#22359;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#37325;&#22797;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2401.14228</link><description>&lt;p&gt;
&#35780;&#20272;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#35757;&#32451;&#30340;&#21442;&#25968;&#30697;&#38453;&#30340;&#21487;&#31227;&#26893;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods. (arXiv:2401.14228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22359;&#30340;&#21487;&#31227;&#26893;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#31227;&#26893;&#30340;&#27169;&#22359;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#37325;&#22797;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35757;&#32451;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#26412;&#22686;&#21152;&#65292;&#23545;&#37325;&#22797;&#21033;&#29992;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20852;&#36259;&#20063;&#22312;&#22686;&#21152;&#12290;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#34920;&#26126;&#65292;&#37325;&#22797;&#21033;&#29992;&#38750;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21487;&#20197;&#24110;&#21161;&#21518;&#32493;&#29305;&#23450;&#20219;&#21153;&#23398;&#20064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#21453;&#30340;&#24773;&#20917;&#65306;&#23558;&#20174;&#19968;&#20010;&#27169;&#22411;&#31227;&#26893;&#32534;&#30721;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#30340;&#23436;&#25972;&#21151;&#33021;&#27169;&#22359;&#21040;&#21478;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#39033;&#21253;&#25324;1,440&#20010;&#35757;&#32451;/&#27979;&#35797;&#36816;&#34892;&#30340;&#30740;&#31350;&#65292;&#20197;&#27979;&#35797;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;(PEFT)&#25216;&#26415;&#35757;&#32451;&#30340;&#27169;&#22359;&#30340;&#21487;&#31227;&#26893;&#24615;&#65292;&#20197;&#24773;&#24863;&#20998;&#26512;&#20026;&#31034;&#20363;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#27979;&#35797;&#20102;&#21487;&#31227;&#26893;&#24615;&#65292;&#28041;&#21450;&#19981;&#21516;&#30340;PEFT&#25216;&#26415;&#21644;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#20027;&#26426;&#27169;&#22411;&#65292;&#31561;&#31561;&#12290;&#25105;&#20204;&#23558;&#31227;&#26893;&#30340;&#27169;&#22359;&#30340;&#24615;&#33021;&#19982;(i)&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#30456;&#31561;&#27169;&#22359;&#30340;&#24615;&#33021;&#21644;(ii)&#20174;&#19982;&#31227;&#26893;&#30340;&#27169;&#22359;&#30456;&#21516;&#20998;&#24067;&#30340;&#21442;&#25968;&#20013;&#37319;&#26679;&#35757;&#32451;&#30340;&#27169;&#22359;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31227;&#26893;&#30340;&#27169;&#22359;&#30340;&#24615;&#33021;&#36828;&#36828;&#36229;&#36807;&#25152;&#27979;&#35797;&#30340;&#20004;&#31181;&#26367;&#20195;&#26041;&#26696;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#27880;&#24847;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the cost of training ever larger language models has grown, so has the interest in reusing previously learnt knowledge. Transfer learning methods have shown how reusing non-task-specific knowledge can help in subsequent task-specific learning. In this paper, we investigate the inverse: porting whole functional modules that encode task-specific knowledge from one model to another. We designed a study comprising 1,440 training/testing runs to test the portability of modules trained by parameter-efficient finetuning (PEFT) techniques, using sentiment analysis as an example task. We test portability in a wide range of scenarios, involving different PEFT techniques and different pretrained host models, among other dimensions. We compare the performance of ported modules with that of equivalent modules trained (i) from scratch, and (ii) from parameters sampled from the same distribution as the ported module. We find that the ported modules far outperform the two alternatives tested, but t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#35282;&#33394;&#21477;&#23376;&#19981;&#20855;&#20449;&#24687;&#24615;&#30340;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#65292;&#24182;&#35774;&#35745;&#31574;&#30053;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#25552;&#39640;&#22238;&#24212;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.14215</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#20010;&#24615;&#21270;&#32454;&#21270;&#65292;&#22686;&#24378;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#24120;&#35782;&#22686;&#24378;&#24615;&#20869;&#23384;&#26500;&#24314;&#21644;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#35282;&#33394;&#21477;&#23376;&#19981;&#20855;&#20449;&#24687;&#24615;&#30340;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#65292;&#24182;&#35774;&#35745;&#31574;&#30053;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#25552;&#39640;&#22238;&#24212;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#23545;&#35805;&#20013;&#65292;&#35760;&#24518;&#21644;&#21033;&#29992;&#35828;&#35805;&#32773;&#30340;&#35282;&#33394;&#26159;&#29983;&#25104;&#22238;&#24212;&#30340;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#32534;&#20889;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#25552;&#20379;&#26080;&#20449;&#24687;&#30340;&#35282;&#33394;&#21477;&#23376;&#65292;&#36825;&#22952;&#30861;&#20102;&#22238;&#24212;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#26469;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#36825;&#20123;&#38382;&#39064;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#19981;&#20135;&#29983;&#19982;&#20854;&#20182;&#35282;&#33394;&#30456;&#30683;&#30462;&#30340;&#35282;&#33394;&#65292;&#25105;&#20204;&#20391;&#37325;&#20110;&#26681;&#25454;&#35774;&#35745;&#30340;&#31574;&#30053;&#65292;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#27492;&#26469;&#32454;&#21270;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#32972;&#26223;&#12290;&#20316;&#20026;&#22810;&#20250;&#35805;&#24773;&#22659;&#20013;&#35282;&#33394;&#25193;&#23637;&#30340;&#20808;&#39537;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#31867;&#20154;&#20010;&#24615;&#32454;&#21270;&#20419;&#36827;&#20102;&#26356;&#22909;&#30340;&#22238;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memorizing and utilizing speakers' personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation. While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32925;&#33039;CT&#25195;&#25551;&#23545;&#32467;&#30452;&#32928;&#30284;&#22522;&#22240;&#31361;&#21464;&#36827;&#34892;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;CT&#22270;&#20687;&#20013;&#35782;&#21035;CRC RAS&#31361;&#21464;&#23478;&#26063;&#12290;</title><link>http://arxiv.org/abs/2401.14206</link><description>&lt;p&gt;
&#21033;&#29992;&#32925;&#33039;CT&#25195;&#25551;&#36827;&#34892;&#32467;&#30452;&#32928;&#30284;&#22522;&#22240;&#31361;&#21464;&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploiting Liver CT scans in Colorectal Carcinoma genomics mutation classification. (arXiv:2401.14206v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32925;&#33039;CT&#25195;&#25551;&#23545;&#32467;&#30452;&#32928;&#30284;&#22522;&#22240;&#31361;&#21464;&#36827;&#34892;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;CT&#22270;&#20687;&#20013;&#35782;&#21035;CRC RAS&#31361;&#21464;&#23478;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32925;&#33039;&#26159;&#32467;&#30452;&#32928;&#30284;&#65288;CRC&#65289;&#24739;&#32773;&#36828;&#31471;&#36716;&#31227;&#26368;&#24120;&#35265;&#30340;&#22120;&#23448;&#65292;&#20102;&#35299;&#30149;&#21464;&#30340;&#31361;&#21464;&#29366;&#24577;&#23545;&#27491;&#30830;&#35774;&#35745;&#26368;&#20339;&#20010;&#20307;&#21270;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20154;&#20204;&#20026;&#20102;&#24320;&#21457;&#38750;&#20405;&#20837;&#24615;&#21644;&#23454;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#20998;&#26512;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#25152;&#24471;&#21040;&#30340;&#32959;&#30244;&#24433;&#20687;&#65292;&#20197;&#20415;&#20998;&#26512;&#25972;&#20010;&#32959;&#30244;&#12290;&#20026;&#20102;&#35299;&#20915;&#30446;&#21069;&#22522;&#20110;&#27963;&#26816;&#20998;&#26512;&#30340;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24739;&#32773;&#21307;&#23398;&#24433;&#20687;&#36827;&#34892;&#31361;&#21464;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;i&#65289;&#19968;&#20010;&#31649;&#29702;&#21487;&#29992;CT&#25195;&#25551;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#27969;&#31243;&#21644;ii&#65289;&#22522;&#22240;&#31361;&#21464;&#35786;&#26029;&#25903;&#25345;&#30340;&#22522;&#32447;&#30740;&#31350;&#65292;&#20197;&#36827;&#34892;&#39044;&#38450;&#24615;&#24739;&#32773;&#38543;&#35775;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;CT&#22270;&#20687;&#20013;&#20197;0.73&#30340;F1&#24471;&#20998;&#35782;&#21035;CRC RAS&#31361;&#21464;&#23478;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;
The liver is the most involved organ by distant metastasis in colon-rectal cancer (CRC) patients and it comes necessary to be aware of the mutational status of the lesions to correctly design the best individual treatment. So far, efforts have been made in order to develop non-invasive and real-time methods that permit the analysis of the whole tumor, using new artificial intelligence tools to analyze the tumor's image obtained by Computed Tomography (CT) scan. In order to address the current medical workflow, that is biopsy analysis-based, we propose the first DeepLearning-based exploration, to our knowledge, of such classification approach from the patient medical imaging. We propose i) a solid pipeline for managing undersized datasets of available CT scans and ii) a baseline study for genomics mutation diagnosis support for preemptive patient follow-up. Our method is able to identify CRC RAS mutation family from CT images with 0.73 F1 score.
&lt;/p&gt;</description></item><item><title>TDFNet&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#65292;&#22522;&#20110;TDANet&#26550;&#26500;&#65292;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;10%&#12290;</title><link>http://arxiv.org/abs/2401.14185</link><description>&lt;p&gt;
TDFNet: &#19968;&#31181;&#39640;&#25928;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#65292;&#20855;&#26377;&#33258;&#39030;&#21521;&#19979;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down Fusion. (arXiv:2401.14185v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14185
&lt;/p&gt;
&lt;p&gt;
TDFNet&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#65292;&#22522;&#20110;TDANet&#26550;&#26500;&#65292;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#20998;&#31163;&#36817;&#24180;&#26469;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#26085;&#31243;&#20998;&#26512;&#12289;&#22330;&#26223;&#20998;&#26512;&#21644;&#36741;&#21161;&#25216;&#26415;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#35774;&#35745;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#20998;&#31163;&#32593;&#32476;&#23545;&#20110;&#20302;&#24310;&#36831;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#26159;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#26356;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#26356;&#22810;&#30340;&#21442;&#25968;&#25165;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TDFNet&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#19968;&#31181;&#22522;&#20110;TDANet&#26550;&#26500;&#30340;&#26368;&#26032;&#25216;&#26415;&#65288;SOTA&#65289;&#27169;&#22411;&#65292;TDANet&#26159;&#19968;&#31181;&#20165;&#29992;&#20110;&#38899;&#39057;&#30340;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#12290;TDANet&#22312;TDFNet&#20869;&#37096;&#26500;&#24314;&#20102;&#21548;&#35273;&#21644;&#35270;&#35273;&#32593;&#32476;&#30340;&#26550;&#26500;&#22522;&#30784;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;&#39640;&#25928;&#27169;&#22411;&#12290;&#22312;LRS2-2Mix&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#21069;&#19968;&#26368;&#20339;&#26041;&#27861;&#30456;&#27604;&#65292;TDFNet&#22312;&#25152;&#26377;&#24615;&#33021;&#25351;&#26631;&#19978;&#30340;&#34920;&#29616;&#25552;&#39640;&#20102;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio-visual speech separation has gained significant traction in recent years due to its potential applications in various fields such as speech recognition, diarization, scene analysis and assistive technologies. Designing a lightweight audio-visual speech separation network is important for low-latency applications, but existing methods often require higher computational costs and more parameters to achieve better separation performance. In this paper, we present an audio-visual speech separation model called Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for audio-visual speech separation, which builds upon the architecture of TDANet, an audio-only speech separation method. TDANet serves as the architectural foundation for the auditory and visual networks within TDFNet, offering an efficient model with fewer parameters. On the LRS2-2Mix dataset, TDFNet achieves a performance increase of up to 10\% across all performance metrics compared with the previous SOTA method 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#20027;&#20379;&#24212;&#38142;&#65288;ASC&#65289;&#30340;&#27491;&#24335;&#23450;&#20041;&#12289;&#29305;&#24449;&#21644;&#36741;&#21161;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#27010;&#24565;&#26694;&#26550;&#21644;&#20379;&#24212;&#38142;&#33258;&#27835;&#21442;&#32771;&#27169;&#22411;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#36825;&#20123;&#27010;&#24565;&#30340;&#21021;&#22987;ASC&#23454;&#26045;&#12290;</title><link>http://arxiv.org/abs/2401.14183</link><description>&lt;p&gt;
&#24378;&#35843;&#33258;&#20027;&#20379;&#24212;&#38142;&#65306;&#23450;&#20041;&#12289;&#29305;&#24449;&#12289;&#27010;&#24565;&#26694;&#26550;&#21644;&#33258;&#20027;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Towards Autonomous Supply Chains: Definition, Characteristics, Conceptual Framework, and Autonomy Levels. (arXiv:2401.14183v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#20027;&#20379;&#24212;&#38142;&#65288;ASC&#65289;&#30340;&#27491;&#24335;&#23450;&#20041;&#12289;&#29305;&#24449;&#21644;&#36741;&#21161;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#27010;&#24565;&#26694;&#26550;&#21644;&#20379;&#24212;&#38142;&#33258;&#27835;&#21442;&#32771;&#27169;&#22411;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#36825;&#20123;&#27010;&#24565;&#30340;&#21021;&#22987;ASC&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20840;&#29699;&#24615;&#24178;&#25200;&#65292;&#22914;&#22823;&#27969;&#34892;&#21644;&#22320;&#32536;&#25919;&#27835;&#20914;&#31361;&#65292;&#28145;&#21051;&#26292;&#38706;&#20102;&#20256;&#32479;&#20379;&#24212;&#38142;&#30340;&#33030;&#24369;&#24615;&#65292;&#38656;&#35201;&#25506;&#32034;&#26356;&#26377;&#24377;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#33258;&#20027;&#20379;&#24212;&#38142;&#65288;ASC&#65289;&#20316;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#25552;&#20379;&#20102;&#22312;&#21160;&#33633;&#30340;&#36152;&#26131;&#29615;&#22659;&#20013;&#22686;&#21152;&#21487;&#35265;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#24377;&#24615;&#30340;&#20248;&#21183;&#12290;&#23613;&#31649;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#22810;&#24180;&#26469;&#19968;&#30452;&#22312;&#35752;&#35770;ASC&#65292;&#20294;&#32570;&#20047;&#33391;&#22909;&#24314;&#31435;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;ASC&#30340;&#27491;&#24335;&#23450;&#20041;&#20197;&#21450;&#20854;&#23450;&#20041;&#29305;&#24449;&#21644;&#36741;&#21161;&#27010;&#24565;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MIISI&#27169;&#22411;&#30340;&#20998;&#23618;&#27010;&#24565;&#26694;&#26550;&#12290;&#36890;&#36807;&#19968;&#20010;&#37325;&#28857;&#20851;&#27880;&#32905;&#31867;&#20379;&#24212;&#38142;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;&#36825;&#19968;&#27010;&#24565;&#27169;&#22411;&#30340;&#21021;&#22987;ASC&#23454;&#26045;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#19971;&#32423;&#20379;&#24212;&#38142;&#33258;&#27835;&#21442;&#32771;&#27169;&#22411;&#65292;&#25551;&#32472;&#20102;&#23454;&#29616;&#23436;&#20840;&#20379;&#24212;&#38142;&#33258;&#27835;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent global disruptions, such as the pandemic and geopolitical conflicts, have profoundly exposed vulnerabilities in traditional supply chains, requiring exploration of more resilient alternatives. Autonomous supply chains (ASCs) have emerged as a potential solution, offering increased visibility, flexibility, and resilience in turbulent trade environments. Despite discussions in industry and academia over several years, ASCs lack well-established theoretical foundations. This paper addresses this research gap by presenting a formal definition of ASC along with its defining characteristics and auxiliary concepts. We propose a layered conceptual framework called the MIISI model. An illustrative case study focusing on the meat supply chain demonstrates an initial ASC implementation based on this conceptual model. Additionally, we introduce a seven-level supply chain autonomy reference model, delineating a trajectory towards achieving a full supply chain autonomy. Recognising that this 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;Copilot&#29983;&#25104;&#30340;Python&#20195;&#30721;&#20013;&#30340;&#20195;&#30721;&#24322;&#21619;&#65292;&#35780;&#20272;Copilot&#20462;&#22797;&#36825;&#20123;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26377;8&#31181;Python&#20195;&#30721;&#24322;&#21619;&#21487;&#20197;&#22312;Copilot&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#26816;&#27979;&#21040;&#12290;</title><link>http://arxiv.org/abs/2401.14176</link><description>&lt;p&gt;
Copilot&#32454;&#21270;&#65306;&#35299;&#20915;Copilot&#29983;&#25104;&#30340;Python&#20195;&#30721;&#20013;&#30340;&#20195;&#30721;&#24322;&#21619;
&lt;/p&gt;
&lt;p&gt;
Copilot Refinement: Addressing Code Smells in Copilot-Generated Python Code. (arXiv:2401.14176v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;Copilot&#29983;&#25104;&#30340;Python&#20195;&#30721;&#20013;&#30340;&#20195;&#30721;&#24322;&#21619;&#65292;&#35780;&#20272;Copilot&#20462;&#22797;&#36825;&#20123;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26377;8&#31181;Python&#20195;&#30721;&#24322;&#21619;&#21487;&#20197;&#22312;Copilot&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26368;&#27969;&#34892;&#30340;&#21160;&#24577;&#35821;&#35328;&#20043;&#19968;&#65292;Python&#22312;&#23384;&#22312;&#20195;&#30721;&#24322;&#21619;&#26102;&#21487;&#35835;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#20250;&#19979;&#38477;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;AI&#25903;&#25345;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#37325;&#26500;&#24037;&#20855;&#30340;&#26085;&#30410;&#20851;&#27880;&#12290;GitHub Copilot&#26159;&#20854;&#20013;&#19968;&#31181;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#12290;Copilot Chat&#26159;&#22312;2023&#24180;9&#26376;&#21457;&#24067;&#30340;&#19968;&#31181;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#26088;&#22312;&#20026;&#33258;&#28982;&#35821;&#35328;&#39537;&#21160;&#30340;&#32534;&#30721;&#25552;&#20379;&#20415;&#21033;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29702;&#35299;Copilot&#29983;&#25104;&#30340;Python&#20195;&#30721;&#20013;&#30340;&#20195;&#30721;&#24322;&#21619;&#20197;&#21450;Copilot&#20462;&#22797;&#20854;&#29983;&#25104;&#30340;&#20195;&#30721;&#24322;&#21619;&#30340;&#33021;&#21147;&#65292;&#20154;&#20204;&#24182;&#27809;&#26377;&#32473;&#20104;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;102&#20010;Copilot&#29983;&#25104;&#30340;Python&#20195;&#30721;&#20013;&#30340;&#20195;&#30721;&#24322;&#21619;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39318;&#20808;&#25506;&#32034;Copilot&#29983;&#25104;&#30340;Python&#20195;&#30721;&#20013;&#20195;&#30721;&#24322;&#21619;&#30340;&#21457;&#29983;&#24773;&#20917;&#65292;&#28982;&#21518;&#35780;&#20272;Copilot&#22312;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#20462;&#22797;&#36825;&#20123;&#20195;&#30721;&#24322;&#21619;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;10&#31181;Python&#20195;&#30721;&#24322;&#21619;&#20013;&#26377;8&#31181;&#21487;&#20197;&#22312;Copilot&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
As one of the most popular dynamic languages, Python experiences a decrease in readability and maintainability when code smells are present. Recent advancements in Large Language Models have sparked growing interest in AI-enabled tools for both code generation and refactoring. GitHub Copilot is one such tool that has gained widespread usage. Copilot Chat, released on September 2023, functions as an interactive tool aims at facilitating natural language-powered coding. However, limited attention has been given to understanding code smells in Copilot-generated Python code and Copilot's ability to fix the code smells it generates. To this end, we built a dataset comprising 102 code smells in Copilot-generated Python code. Our aim is to first explore the occurrence of code smells in Copilot-generated Python code and then evaluate the effectiveness of Copilot in fixing these code smells employing different prompts. The results show that 8 out of 10 types of Python smells can be detected in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23618;&#27425;&#21270;&#20219;&#21153;&#32593;&#32476;&#35268;&#21010;&#20013;&#19977;&#20010;&#32463;&#20856;&#38382;&#39064;&#30340;&#21487;&#35745;&#31639;&#36793;&#30028;&#65292;&#25552;&#20379;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#23558;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#32467;&#26524;&#20174;&#21407;&#22987;&#20219;&#21153;&#32593;&#32476;&#25512;&#24191;&#21040;&#19968;&#33324;&#20219;&#21153;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#36825;&#19977;&#20010;&#38382;&#39064;&#30340;&#21442;&#25968;&#21270;&#22797;&#26434;&#24230;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.14174</link><description>&lt;p&gt;
&#21010;&#20998;&#22312;&#23618;&#27425;&#21270;&#20219;&#21153;&#32593;&#32476;&#35268;&#21010;&#20013;&#30340;&#21487;&#35745;&#31639;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
The Boundaries of Tractability in Hierarchical Task Network Planning. (arXiv:2401.14174v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23618;&#27425;&#21270;&#20219;&#21153;&#32593;&#32476;&#35268;&#21010;&#20013;&#19977;&#20010;&#32463;&#20856;&#38382;&#39064;&#30340;&#21487;&#35745;&#31639;&#36793;&#30028;&#65292;&#25552;&#20379;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#23558;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#32467;&#26524;&#20174;&#21407;&#22987;&#20219;&#21153;&#32593;&#32476;&#25512;&#24191;&#21040;&#19968;&#33324;&#20219;&#21153;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#36825;&#19977;&#20010;&#38382;&#39064;&#30340;&#21442;&#25968;&#21270;&#22797;&#26434;&#24230;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23618;&#27425;&#21270;&#20219;&#21153;&#32593;&#32476;&#35268;&#21010;&#20013;&#19977;&#20010;&#32463;&#20856;&#38382;&#39064;&#30340;&#21487;&#35745;&#31639;&#36793;&#30028;&#65306;&#25552;&#20379;&#35745;&#21010;&#30340;&#39564;&#35777;&#65292;&#26159;&#21542;&#23384;&#22312;&#21487;&#25191;&#34892;&#35745;&#21010;&#20197;&#21450;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#26576;&#20010;&#35745;&#21010;&#36798;&#21040;&#32473;&#23450;&#29366;&#24577;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24120;&#37327;&#20559;&#24207;&#23485;&#24230;&#30340;&#21407;&#22987;&#20219;&#21153;&#32593;&#32476;&#65288;&#20197;&#21450;&#20854;&#25512;&#24191;&#24418;&#24335;&#65289;&#19978;&#65292;&#36825;&#19977;&#20010;&#38382;&#39064;&#37117;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#65292;&#32780;&#23545;&#20110;&#21518;&#20004;&#20010;&#38382;&#39064;&#65292;&#36825;&#31181;&#24773;&#20917;&#20165;&#22312;&#23545;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#21487;&#35777;&#26126;&#30340;&#24517;&#35201;&#38480;&#21046;&#19979;&#25165;&#25104;&#31435;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#20803;&#23450;&#29702;&#21450;&#30456;&#24212;&#30340;&#19979;&#30028;&#65292;&#20197;&#30830;&#23450;&#20174;&#21407;&#22987;&#20219;&#21153;&#32593;&#32476;&#21040;&#19968;&#33324;&#20219;&#21153;&#32593;&#32476;&#30340;&#19968;&#33324;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#32467;&#26524;&#21487;&#20197;&#34987;&#25552;&#21319;&#30340;&#20005;&#26684;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#36825;&#19977;&#20010;&#38382;&#39064;&#30340;&#21442;&#25968;&#21270;&#22797;&#26434;&#24230;&#26469;&#20016;&#23500;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;(1)&#36890;&#36807;&#23558;&#20559;&#24207;&#23485;&#24230;&#26367;&#25442;&#20026;t&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#19977;&#20010;&#38382;&#39064;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#35745;&#31639;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the complexity-theoretic boundaries of tractability for three classical problems in the context of Hierarchical Task Network Planning: the validation of a provided plan, whether an executable plan exists, and whether a given state can be reached by some plan. We show that all three problems can be solved in polynomial time on primitive task networks of constant partial order width (and a generalization thereof), whereas for the latter two problems this holds only under a provably necessary restriction to the state space. Next, we obtain an algorithmic meta-theorem along with corresponding lower bounds to identify tight conditions under which general polynomial-time solvability results can be lifted from primitive to general task networks. Finally, we enrich our investigation by analyzing the parameterized complexity of the three considered problems, and show that (1) fixed-parameter tractability for all three problems can be achieved by replacing the partial order width with t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#21442;&#25968;&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#39044;&#27979;&#33041;&#32959;&#30244;&#30340;&#32570;&#27687;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26114;&#36149;&#19988;&#26222;&#21450;&#29575;&#26377;&#38480;&#30340;FMISO PET&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;MRI&#25195;&#25551;&#20013;&#39044;&#27979;&#32570;&#27687;&#24773;&#20917;&#65292;&#24182;&#19982;&#23454;&#38469;&#20540;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14171</link><description>&lt;p&gt;
&#20174;&#22810;&#21442;&#25968;MRI&#39044;&#27979;&#33041;&#32959;&#30244;&#32570;&#27687;
&lt;/p&gt;
&lt;p&gt;
Predicting Hypoxia in Brain Tumors from Multiparametric MRI. (arXiv:2401.14171v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#21442;&#25968;&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#39044;&#27979;&#33041;&#32959;&#30244;&#30340;&#32570;&#27687;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26114;&#36149;&#19988;&#26222;&#21450;&#29575;&#26377;&#38480;&#30340;FMISO PET&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;MRI&#25195;&#25551;&#20013;&#39044;&#27979;&#32570;&#27687;&#24773;&#20917;&#65292;&#24182;&#19982;&#23454;&#38469;&#20540;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#21442;&#25968;&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#39044;&#27979;&#33041;&#32959;&#30244;&#32570;&#27687;&#30340;&#26032;&#26041;&#27861;&#12290;&#32570;&#27687;&#26159;&#19968;&#31181;&#20302;&#27687;&#27700;&#24179;&#30340;&#24773;&#20917;&#65292;&#26159;&#19982;&#39044;&#21518;&#19981;&#33391;&#30456;&#20851;&#30340;&#24694;&#24615;&#33041;&#32959;&#30244;&#30340;&#24120;&#35265;&#29305;&#24449;&#12290;&#27679;&#31859;&#32034;&#27679;&#26680;&#32032;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551; (FMISO PET) &#26159;&#19968;&#31181;&#29992;&#20110;&#20307;&#20869;&#26816;&#27979;&#32570;&#27687;&#30340;&#25104;&#29087;&#26041;&#27861;&#65292;&#20294;&#20854;&#26114;&#36149;&#19988;&#26222;&#21450;&#29575;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;MRI&#36825;&#31181;&#26356;&#23481;&#26131;&#33719;&#21462;&#21644;&#32463;&#27982;&#23454;&#24800;&#30340;&#25104;&#20687;&#27169;&#24335;&#26469;&#39044;&#27979;FMISO PET&#20449;&#21495;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;ACRIN 6684&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;(DL)&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#33041;&#32959;&#30244;&#24739;&#32773;&#30340;&#37197;&#23545;MRI&#21644;FMISO PET&#22270;&#20687;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;MRI&#29305;&#24449;&#19982;&#30456;&#24212;&#30340;FMISO PET&#20449;&#21495;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20174;&#32780;&#21487;&#20197;&#20165;&#36890;&#36807;MRI&#25195;&#25551;&#39044;&#27979;&#32570;&#27687;&#12290;&#32467;&#26524;&#26174;&#31034;&#39044;&#27979;&#20540;&#19982;&#23454;&#38469;&#20540;&#20043;&#38388;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper presents a novel approach to the prediction of hypoxia in brain tumors, using multi-parametric Magnetic Resonance Imaging (MRI). Hypoxia, a condition characterized by low oxygen levels, is a common feature of malignant brain tumors associated with poor prognosis. Fluoromisonidazole Positron Emission Tomography (FMISO PET) is a well-established method for detecting hypoxia in vivo, but it is expensive and not widely available. Our study proposes the use of MRI, a more accessible and cost-effective imaging modality, to predict FMISO PET signals. We investigate deep learning models (DL) trained on the ACRIN 6684 dataset, a resource that contains paired MRI and FMISO PET images from patients with brain tumors. Our trained models effectively learn the complex relationships between the MRI features and the corresponding FMISO PET signals, thereby enabling the prediction of hypoxia from MRI scans alone. The results show a strong correlation between the predicted and actual
&lt;/p&gt;</description></item><item><title>BayesPrompt&#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14166</link><description>&lt;p&gt;
BayesPrompt: &#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#19978;&#25351;&#23548;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction. (arXiv:2401.14166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14166
&lt;/p&gt;
&lt;p&gt;
BayesPrompt&#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#24494;&#35843;&#33539;&#24335;&#65292;prompt-tuning&#26088;&#22312;&#32553;&#23567;&#19979;&#28216;&#20219;&#21153;&#19982;&#39044;&#35757;&#32451;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;prompt-tuning&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25345;&#32493;&#36827;&#23637;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#25345;&#20037;&#30340;&#32570;&#38519;&#65306;prompt-tuning&#26041;&#27861;&#26080;&#27861;&#27867;&#21270;&#21040;&#29305;&#23450;&#30340;&#23569;&#26679;&#26412;&#27169;&#24335;&#12290;&#20174;&#20998;&#24067;&#20998;&#26512;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#19968;&#29616;&#35937;&#32972;&#21518;&#30340;&#20869;&#22312;&#38382;&#39064;&#26159;PLMs&#20013;&#21253;&#21547;&#36807;&#22810;&#30340;&#27010;&#24565;&#30693;&#35782;&#21644;&#30446;&#26631;&#19979;&#28216;&#39046;&#22495;&#30340;&#32553;&#20943;&#30693;&#35782;&#65292;&#20004;&#32773;&#20849;&#21516;&#23548;&#33268;PLMs&#22312;&#26222;&#36941;&#30340;&#30693;&#35782;&#23884;&#20837;&#31354;&#38388;&#20013;&#38169;&#35823;&#22320;&#23450;&#20301;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#23545;&#24212;&#30340;&#30693;&#35782;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30452;&#35266;&#22320;&#25506;&#32034;&#20102;&#20197;&#26080;&#20559;&#26041;&#24335;&#36924;&#36817;&#19979;&#28216;&#20219;&#21153;&#30340;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#25277;&#35937;&#36825;&#26679;&#30340;&#39046;&#22495;&#29983;&#25104;&#26377;&#21306;&#21035;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26080;&#27495;&#20041;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns. From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20174;&#29305;&#24449;&#35270;&#35282;&#35299;&#20915;&#20102;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25269;&#25239;&#24322;&#24120;&#33410;&#28857;&#30340;&#39640;&#24322;&#36136;&#24615;&#65292;&#21516;&#26102;&#20174;&#21516;&#36136;&#37051;&#23621;&#20013;&#21463;&#30410;&#20110;&#27491;&#24120;&#33410;&#28857;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.14155</link><description>&lt;p&gt;
&#32531;&#35299;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Alleviating Structural Distribution Shift in Graph Anomaly Detection. (arXiv:2401.14155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14155
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20174;&#29305;&#24449;&#35270;&#35282;&#35299;&#20915;&#20102;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25269;&#25239;&#24322;&#24120;&#33410;&#28857;&#30340;&#39640;&#24322;&#36136;&#24615;&#65292;&#21516;&#26102;&#20174;&#21516;&#36136;&#37051;&#23621;&#20013;&#21463;&#30410;&#20110;&#27491;&#24120;&#33410;&#28857;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#30001;&#20110;&#24322;&#24120;&#33410;&#28857;&#21644;&#27491;&#24120;&#33410;&#28857;&#20043;&#38388;&#30340;&#32467;&#26500;&#20998;&#24067;&#19981;&#21516;&#65292;&#20351;&#24471;&#35813;&#38382;&#39064;&#21464;&#24471;&#22256;&#38590;&#12290;&#24322;&#24120;&#33410;&#28857;&#26159;&#23569;&#25968;&#65292;&#22240;&#27492;&#19982;&#27491;&#24120;&#33410;&#28857;&#30456;&#27604;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24322;&#36136;&#24615;&#21644;&#36739;&#20302;&#30340;&#21516;&#36136;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21508;&#31181;&#26102;&#38388;&#22240;&#32032;&#21644;&#20154;&#31867;&#19987;&#23478;&#30340;&#27880;&#37322;&#20559;&#22909;&#65292;&#24322;&#36136;&#24615;&#21644;&#21516;&#36136;&#24615;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#36825;&#22312;&#26412;&#25991;&#20013;&#34987;&#31216;&#20026;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#65288;SDS&#65289;&#12290;&#20027;&#27969;&#26041;&#27861;&#26159;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26500;&#24314;&#30340;&#65292;&#36890;&#36807;&#32858;&#21512;&#21516;&#36136;&#37051;&#23621;&#26377;&#21033;&#20110;&#27491;&#24120;&#33410;&#28857;&#30340;&#20998;&#31867;&#65292;&#20294;&#24573;&#35270;&#20102;&#24322;&#24120;&#33410;&#28857;&#30340;SDS&#38382;&#39064;&#65292;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#26412;&#30740;&#31350;&#20174;&#29305;&#24449;&#35270;&#35282;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;SDS&#30340;&#31243;&#24230;&#22312;&#24322;&#24120;&#33410;&#28857;&#21644;&#27491;&#24120;&#33410;&#28857;&#20043;&#38388;&#26377;&#25152;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#22312;&#20110;&#23545;&#24322;&#24120;&#33410;&#28857;&#25269;&#25239;&#39640;&#24322;&#36136;&#24615;&#30340;&#21516;&#26102;&#65292;&#20174;&#21516;&#36136;&#37051;&#23621;&#20013;&#21463;&#30410;&#20110;&#27491;&#24120;&#33410;&#28857;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection (GAD) is a challenging binary classification problem due to its different structural distribution between anomalies and normal nodes -- abnormal nodes are a minority, therefore holding high heterophily and low homophily compared to normal nodes. Furthermore, due to various time factors and the annotation preferences of human experts, the heterophily and homophily can change across training and testing data, which is called structural distribution shift (SDS) in this paper. The mainstream methods are built on graph neural networks (GNNs), benefiting the classification of normals from aggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and suffering from poor generalization.  This work solves the problem from a feature view. We observe that the degree of SDS varies between anomalies and normal nodes. Hence to address the issue, the key lies in resisting high heterophily for anomalies meanwhile benefiting the learning of normals from homophi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#25311;&#29615;&#22659;&#26469;&#35780;&#20272;AmI&#22330;&#26223;&#65292;&#36890;&#36807;&#27979;&#37327;&#20195;&#29702;&#28385;&#24847;&#24230;&#21644;&#33410;&#32422;&#26102;&#38388;&#26469;&#35780;&#20272;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2401.14153</link><description>&lt;p&gt;
&#29992;NetLogo&#24320;&#21457;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#25311;&#35780;&#20272;AmI&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Agent-based Simulation with Netlogo to Evaluate AmI Scenarios. (arXiv:2401.14153v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#25311;&#29615;&#22659;&#26469;&#35780;&#20272;AmI&#22330;&#26223;&#65292;&#36890;&#36807;&#27979;&#37327;&#20195;&#29702;&#28385;&#24847;&#24230;&#21644;&#33410;&#32422;&#26102;&#38388;&#26469;&#35780;&#20272;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#25311;&#26469;&#35780;&#20272;&#22522;&#20110;&#20195;&#29702;&#30340;AmI&#22330;&#26223;&#12290;&#35768;&#22810;AmI&#24212;&#29992;&#31243;&#24207;&#37117;&#26159;&#36890;&#36807;&#20195;&#29702;&#26469;&#23454;&#29616;&#30340;&#65292;&#20294;&#27809;&#26377;&#23558;&#23427;&#20204;&#19982;&#20854;&#20182;&#29616;&#26377;&#30340;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#35780;&#20272;&#20351;&#29992;&#23427;&#20204;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#30410;&#12290;&#25552;&#20986;&#30340;&#27169;&#25311;&#29615;&#22659;&#20351;&#29992;&#20004;&#20010;&#35780;&#20272;&#25351;&#26631;&#26469;&#20998;&#26512;&#36825;&#20123;&#25928;&#30410;&#65306;&#39318;&#20808;&#65292;&#36890;&#36807;&#27979;&#37327;&#19981;&#21516;&#31867;&#22411;&#24895;&#26395;&#30340;&#20195;&#29702;&#28385;&#24847;&#24230;&#26469;&#34913;&#37327;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#27491;&#30830;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#25152;&#33719;&#24471;&#30340;&#26102;&#38388;&#33410;&#32422;&#26469;&#34913;&#37327;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20351;&#29992;NetLogo&#27169;&#25311;&#29615;&#22659;&#35780;&#20272;&#20102;&#20043;&#21069;&#25552;&#20986;&#30340;&#22312;&#26426;&#22330;&#25552;&#20379;AmI&#26381;&#21153;&#30340;&#20195;&#29702;&#26550;&#26500;&#12289;&#26412;&#20307;&#35770;&#21644;12&#27493;&#21327;&#35758;&#12290;&#26412;&#25991;&#20351;&#29992;NetLogo&#27169;&#22411;&#32771;&#34385;&#20102;&#35813;&#24212;&#29992;&#39046;&#22495;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;FIPA&#21644;BDI&#25193;&#23637;&#19982;&#25105;&#20204;&#20808;&#21069;&#30340;&#24037;&#20316;&#21644;&#25105;&#20204;&#20808;&#21069;&#30340;JADE&#23454;&#29616;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper an agent-based simulation is developed in order to evaluate an AmI scenario based on agents. Many AmI applications are implemented through agents but they are not compared to any other existing alternative in order to evaluate the relative benefits of using them. The proposal simulation environment developed in Netlogo analyse such benefits using two evaluation criteria: First, measuring agent satisfaction of different types of desires along the execution. Second, measuring time savings obtained through a correct use of context information.  So, here, a previously suggested agent architecture, an ontology and a 12-steps protocol to provide AmI services in airports, is evaluated using a NetLogo simulation environment. The present work uses a NetLogo model considering scalability problems of this application domain but using FIPA and BDI extensions to be coherent with our previous works and our previous JADE implementation of them.  The NetLogo model presented simulates an 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#26469;&#35299;&#20915;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30693;&#35782;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;LLMs&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#21644;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35774;&#35745;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.14151</link><description>&lt;p&gt;
&#30495;&#30693;&#26469;&#28304;&#20110;&#23454;&#36341;&#65306;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20351;LLMs&#19982;&#20855;&#36523;&#29615;&#22659;&#23545;&#40784;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning. (arXiv:2401.14151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#26469;&#35299;&#20915;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30693;&#35782;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;LLMs&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#21644;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35774;&#35745;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20247;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;&#31616;&#21333;&#30340;&#20915;&#31574;&#20219;&#21153;&#19978;&#32463;&#24120;&#22833;&#36133;&#65292;&#21407;&#22240;&#26159;LLMs&#20013;&#30340;&#30693;&#35782;&#19982;&#29615;&#22659;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#31574;&#30053;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22987;&#32456;&#19982;&#29615;&#22659;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#38590;&#20197;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;&#20854;&#20013;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TWOSOME&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;RL&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#24182;&#23454;&#29616;&#23545;&#40784;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20934;&#22791;&#22909;&#30340;&#25968;&#25454;&#38598;&#25110;&#29615;&#22659;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#26597;&#35810;&#27599;&#20010;&#26377;&#25928;&#21160;&#20316;&#30340;&#32852;&#21512;&#27010;&#29575;&#20197;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#22686;&#24378;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#34892;&#20026;&#35780;&#20272;&#21644;&#36873;&#25321;&#31639;&#27861;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the acto
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.14142</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#32479;&#19968;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14142
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411; (CBM)&#65292;&#22312;&#20026;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#27010;&#24565;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26368;&#32456;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#32416;&#27491;&#19968;&#20010;&#39044;&#27979;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33016;&#37096;&#8221;&#65289;&#26080;&#27861;&#24110;&#21161;&#32416;&#27491;&#39640;&#24230;&#30456;&#20851;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33145;&#37096;&#8221;&#65289;&#65292;&#23548;&#33268;&#26368;&#32456;&#20934;&#30830;&#29575;&#19981;&#29702;&#24819;&#65307;&#23427;&#20204;&#26080;&#27861;&#33258;&#28982;&#22320;&#37327;&#21270;&#19981;&#21516;&#27010;&#24565;&#21644;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65288;&#20363;&#22914;&#23545;&#20110;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#26631;&#31614;&#8220;Kentucky Warbler&#8221;&#21644;&#27010;&#24565;&#8220;&#40657;&#33394;&#22068;&#24052;&#8221;&#30340;&#22270;&#20687;&#65292;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#21478;&#19968;&#20010;&#27010;&#24565;&#8220;&#40657;&#33394;&#20896;&#8221;&#30340;&#27010;&#29575;&#26159;&#22810;&#23569;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#40657;&#30418;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#26356;&#28145;&#23618;&#27425;&#30340;&#27934;&#23519;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;Energy-based Concept Bottleneck Models&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
&lt;/p&gt;</description></item><item><title>FP6-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#20845;&#20301;&#37327;&#21270;&#30340;GPU&#31639;&#27861;-&#31995;&#32479;&#21327;&#21516;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#26029;&#25104;&#26412;&#21644;&#27169;&#22411;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.14112</link><description>&lt;p&gt;
FP6-LLM: &#36890;&#36807;FP6&#20013;&#24515;&#31639;&#27861;-&#31995;&#32479;&#21327;&#21516;&#35774;&#35745;&#39640;&#25928;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design. (arXiv:2401.14112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14112
&lt;/p&gt;
&lt;p&gt;
FP6-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#20845;&#20301;&#37327;&#21270;&#30340;GPU&#31639;&#27861;-&#31995;&#32479;&#21327;&#21516;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#26029;&#25104;&#26412;&#21644;&#27169;&#22411;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20845;&#20301;&#37327;&#21270;&#65288;FP6&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23567;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22823;&#23567;&#65292;&#24182;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#19981;&#25552;&#20379;FP6&#37327;&#21270;&#30340;&#24352;&#37327;&#26680;&#24515;&#25903;&#25345;&#65292;&#24182;&#19988;&#22312;LLM&#25512;&#26029;&#36807;&#31243;&#20013;&#24456;&#38590;&#23454;&#29616;&#23454;&#38469;&#24615;&#33021;&#25913;&#36827;&#12290;&#30001;&#20110;&#65288;1&#65289;&#27169;&#22411;&#26435;&#37325;&#20855;&#26377;&#19981;&#35268;&#21017;&#20301;&#23485;&#30340;&#19981;&#21451;&#22909;&#20869;&#23384;&#35775;&#38382;&#21644;&#65288;2&#65289;&#26435;&#37325;&#21435;&#37327;&#21270;&#30340;&#39640;&#36816;&#34892;&#26102;&#24320;&#38144;&#65292;&#25903;&#25345;&#22312;GPU&#19978;&#36827;&#34892;FP6&#37327;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TC-FPx&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#32479;&#19968;&#24352;&#37327;&#26680;&#24515;&#25903;&#25345;&#30340;&#28014;&#28857;&#26435;&#37325;&#30340;&#23436;&#25972;GPU&#20869;&#26680;&#35774;&#35745;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#37327;&#21270;&#20301;&#23485;&#12290;&#25105;&#20204;&#23558;TC-FPx&#20869;&#26680;&#38598;&#25104;&#21040;&#29616;&#26377;&#25512;&#26029;&#31995;&#32479;&#20013;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#31471;&#21040;&#31471;&#25903;&#25345;&#65288;&#31216;&#20026;FP6-LLM&#65289;&#29992;&#20110;&#37327;&#21270;LLM&#25512;&#26029;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25512;&#26029;&#25104;&#26412;&#21644;&#27169;&#22411;&#36136;&#37327;&#20043;&#38388;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FP6-LLM&#20165;&#20351;&#29992;&#19968;&#37096;&#20998;&#23384;&#20648;&#31354;&#38388;&#23601;&#21487;&#20197;&#36827;&#34892;LLaMA-70b&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width. We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved. Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a sin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#21644;&#24494;&#35843;&#39640;&#31471;DNN&#65292;&#23454;&#29616;&#20351;&#29992;&#26356;&#20415;&#23452;&#30340;12&#20301;&#32047;&#21152;&#22120;&#65292;&#32780;&#19981;&#20250;&#23548;&#33268;&#31934;&#24230;&#38477;&#20302;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#32454;&#31890;&#24230;&#26799;&#24230;&#36817;&#20284;&#21487;&#20197;&#25552;&#39640;DNN&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.14110</link><description>&lt;p&gt;
&#20197;&#36739;&#20302;&#27604;&#29305;&#23485;&#24230;&#30340;&#32047;&#21152;&#22120;&#38477;&#20302;&#28145;&#24230;&#32593;&#32476;&#25512;&#29702;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators. (arXiv:2401.14110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#21644;&#24494;&#35843;&#39640;&#31471;DNN&#65292;&#23454;&#29616;&#20351;&#29992;&#26356;&#20415;&#23452;&#30340;12&#20301;&#32047;&#21152;&#22120;&#65292;&#32780;&#19981;&#20250;&#23548;&#33268;&#31934;&#24230;&#38477;&#20302;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#32454;&#31890;&#24230;&#26799;&#24230;&#36817;&#20284;&#21487;&#20197;&#25552;&#39640;DNN&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#37327;&#21270;&#30340;&#30740;&#31350;&#37117;&#30528;&#37325;&#20110;&#38477;&#20302;&#39640;&#32423;&#26694;&#26550;&#21487;&#35265;&#30340;&#24352;&#37327;&#31934;&#24230;&#65288;&#20363;&#22914;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;&#26799;&#24230;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30828;&#20214;&#20173;&#28982;&#20381;&#36182;&#20110;&#39640;&#31934;&#24230;&#30340;&#26680;&#24515;&#25805;&#20316;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#32047;&#21152;&#20056;&#31215;&#30340;&#36816;&#31639;&#12290;&#36825;&#31181;&#39640;&#31934;&#24230;&#32047;&#21152;&#36816;&#31639;&#36880;&#28176;&#25104;&#20026;&#20027;&#35201;&#30340;&#35745;&#31639;&#29942;&#39048;&#12290;&#36825;&#26159;&#22240;&#20026;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20302;&#31934;&#24230;&#32047;&#21152;&#22120;&#30340;&#20351;&#29992;&#23548;&#33268;&#20102;&#24615;&#33021;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#21644;&#24494;&#35843;&#39640;&#31471;DNN&#65292;&#39318;&#27425;&#23454;&#29616;&#20351;&#29992;&#26356;&#20415;&#23452;&#30340;12&#20301;&#32047;&#21152;&#22120;&#65292;&#32780;&#19981;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#31934;&#24230;&#38477;&#20302;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20986;&#38543;&#30528;&#32047;&#21152;&#31934;&#24230;&#36827;&#19968;&#27493;&#38477;&#20302;&#65292;&#20351;&#29992;&#32454;&#31890;&#24230;&#26799;&#24230;&#36817;&#20284;&#21487;&#20197;&#25552;&#39640;DNN&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy.
&lt;/p&gt;</description></item><item><title>CompactifAI&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2401.14109</link><description>&lt;p&gt;
CompactifAI: &#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks. (arXiv:2401.14109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14109
&lt;/p&gt;
&lt;p&gt;
CompactifAI&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;LlaMA&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#24102;&#26469;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#22914;&#24040;&#22823;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#25104;&#26412;&#12289;&#36739;&#22823;&#30340;&#33021;&#28304;&#38656;&#27714;&#20197;&#21450;&#29616;&#22330;&#37096;&#32626;&#30340;&#38480;&#21046;&#12290;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#22914;&#21098;&#26525;&#12289;&#33976;&#39311;&#21644;&#20302;&#31209;&#36924;&#36817;&#20027;&#35201;&#20851;&#27880;&#20943;&#23569;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#26377;&#25928;&#25968;&#37327;&#65292;&#32780;&#37327;&#21270;&#26041;&#27861;&#21017;&#20391;&#37325;&#20110;&#38477;&#20302;&#21333;&#20010;&#26435;&#37325;&#30340;&#25968;&#20540;&#31934;&#24230;&#65292;&#20197;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21516;&#26102;&#20445;&#25345;&#31070;&#32463;&#20803;&#25968;&#30446;&#19981;&#21464;&#12290;&#34429;&#28982;&#36825;&#20123;&#21387;&#32553;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#30456;&#23545;&#25104;&#21151;&#65292;&#20294;&#27809;&#26377;&#20196;&#20154;&#20449;&#26381;&#30340;&#29702;&#30001;&#35748;&#20026;&#25130;&#26029;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#26159;&#19968;&#31181;&#26368;&#20248;&#31574;&#30053;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;LLM&#21387;&#32553;&#26041;&#27861;CompactifAI&#65292;&#23427;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment. Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed. While these compression methods have been relatively successful in practice, there's no compelling reason to believe that truncating the number of neurons is an optimal strategy. In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model's correlation space instead, allowing for a more controlled, refined an
&lt;/p&gt;</description></item><item><title>GQHAN&#26159;&#19968;&#31181;&#21463;&#21040;Grover&#21551;&#21457;&#30340;&#37327;&#23376;&#30828;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#26580;&#24615;&#39044;&#35328;&#26426;&#21644;&#33258;&#36866;&#24212;&#25193;&#25955;&#31639;&#23376;&#26469;&#35299;&#20915;&#20102;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#19981;&#21487;&#24494;&#20998;&#24615;&#65292;&#25552;&#39640;&#20102;&#22788;&#29702;&#37327;&#23376;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.14089</link><description>&lt;p&gt;
GQHAN: &#19968;&#31181;&#21463;&#21040;Grover&#21551;&#21457;&#30340;&#37327;&#23376;&#30828;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GQHAN: A Grover-inspired Quantum Hard Attention Network. (arXiv:2401.14089v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14089
&lt;/p&gt;
&lt;p&gt;
GQHAN&#26159;&#19968;&#31181;&#21463;&#21040;Grover&#21551;&#21457;&#30340;&#37327;&#23376;&#30828;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#26580;&#24615;&#39044;&#35328;&#26426;&#21644;&#33258;&#36866;&#24212;&#25193;&#25955;&#31639;&#23376;&#26469;&#35299;&#20915;&#20102;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#19981;&#21487;&#24494;&#20998;&#24615;&#65292;&#25552;&#39640;&#20102;&#22788;&#29702;&#37327;&#23376;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24403;&#21069;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;(QML)&#27169;&#22411;&#22312;&#35782;&#21035;&#37327;&#23376;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#23548;&#33268;&#22788;&#29702;&#22823;&#35268;&#27169;&#37327;&#23376;&#25968;&#25454;&#38598;&#26102;&#25928;&#26524;&#19979;&#38477;&#12290;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;(HAM)&#34987;&#26399;&#26395;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#19978;&#36848;QML&#29942;&#39048;&#65292;&#20294;&#38754;&#20020;&#26080;&#27861;&#24494;&#20998;&#30340;&#20005;&#37325;&#25361;&#25112;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#24615;&#12290;&#38024;&#23545;HAM&#21644;QML&#30340;&#22256;&#22659;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;Grover&#21551;&#21457;&#30340;&#37327;&#23376;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;(GQHAM)&#65292;&#30001;&#26580;&#24615;&#39044;&#35328;&#26426;(Flexible Oracle, FO)&#21644;&#33258;&#36866;&#24212;&#25193;&#25955;&#31639;&#23376;(Adaptive Diffusion Operator, ADO)&#32452;&#25104;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;FO&#36890;&#36807;&#25191;&#34892;&#20855;&#26377;&#26580;&#24615;&#25511;&#21046;&#30340;&#31163;&#25955;&#21407;&#35821;(Discrete Primitives, DPs)&#30340;&#28608;&#27963;&#25110;&#23631;&#34109;&#26469;&#20811;&#26381;&#19981;&#21487;&#24494;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#32534;&#32455;&#21508;&#31181;&#31163;&#25955;&#30340;&#21629;&#36816;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36825;&#31181;&#31163;&#25955;&#36873;&#25321;&#21487;&#20197;&#36890;&#36807;&#29305;&#27530;&#23450;&#20041;&#30340;&#37327;&#23376;&#30828;&#27880;&#24847;&#21147;&#20998;&#25968;(QHAS)&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;ADO&#26469;&#22686;&#24378;&#36890;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous current Quantum Machine Learning (QML) models exhibit an inadequacy in discerning the significance of quantum data, resulting in diminished efficacy when handling extensive quantum datasets. Hard Attention Mechanism (HAM), anticipated to efficiently tackle the above QML bottlenecks, encounters the substantial challenge of non-differentiability, consequently constraining its extensive applicability. In response to the dilemma of HAM and QML, a Grover-inspired Quantum Hard Attention Mechanism (GQHAM) consisting of a Flexible Oracle (FO) and an Adaptive Diffusion Operator (ADO) is proposed. Notably, the FO is designed to surmount the non-differentiable issue by executing the activation or masking of Discrete Primitives (DPs) with Flexible Control (FC) to weave various discrete destinies. Based on this, such discrete choice can be visualized with a specially defined Quantum Hard Attention Score (QHAS). Furthermore, a trainable ADO is devised to boost the generality and flexibility
&lt;/p&gt;</description></item><item><title>&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.14086</link><description>&lt;p&gt;
&#20351;&#29992;Sum-Product Networks&#29983;&#25104;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generating Likely Counterfactuals Using Sum-Product Networks. (arXiv:2401.14086v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14086
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#65288;GDPR&#12289;AI&#27861;&#26696;&#65289;&#65292;&#38656;&#35201;&#35299;&#37322;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#20915;&#31574;&#24448;&#24448;&#21482;&#33021;&#22312;&#20107;&#21518;&#35299;&#37322;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#25104;&#20026;&#24120;&#35265;&#30340;&#35299;&#37322;&#26041;&#24335;&#12290;&#20160;&#20040;&#26500;&#25104;&#20102;&#26368;&#20339;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#24517;&#39035;&#32771;&#34385;&#22810;&#20010;&#26041;&#38754;&#65292;&#20854;&#20013;&#8220;&#26679;&#26412;&#36317;&#31163;&#8221;&#26159;&#26368;&#24120;&#35265;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#19968;&#35201;&#27714;&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#22826;&#21487;&#33021;&#19988;&#22240;&#27492;&#20215;&#20540;&#26377;&#38480;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#39640;&#21487;&#33021;&#24615;&#35299;&#37322;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#65288;MIO&#65289;&#27169;&#25311;&#23547;&#25214;&#28385;&#36275;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35768;&#22810;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#26377;&#21487;&#33021;&#35299;&#37322;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sum-Product Network&#65288;SPN&#65289;&#30340;MIO&#34920;&#36798;&#65292;&#24182;&#20351;&#29992;SPN&#20272;&#35745;&#21453;&#20107;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#23545;&#29420;&#31435;&#30340;&#20852;&#36259;&#20063;&#26377;&#29992;&#12290;&#19982;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20960;&#31181;&#26041;&#27861;&#36827;&#34892;&#25968;&#20540;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI systems need to be explained. These decisions are often explainable only post hoc, where counterfactual explanations are popular. The question of what constitutes the best counterfactual explanation must consider multiple aspects, where "distance from the sample" is the most common. We argue that this requirement frequently leads to explanations that are unlikely and, therefore, of limited value. Here, we present a system that provides high-likelihood explanations. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using mixed-integer optimization (MIO). In the process, we propose an MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the likelihood of a counterfactual, which can be of independent interest. A numerical comparison against several methods for generating counterfactual explanations is pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21322;&#33258;&#21160;&#36719;&#20214;&#26550;&#26500;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#38656;&#27714;&#29983;&#25104;&#26550;&#26500;&#20505;&#36873;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#23450;&#37327;&#26041;&#27861;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#21644;&#20132;&#21449;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.14079</link><description>&lt;p&gt;
&#20174;&#38656;&#27714;&#21040;&#26550;&#26500;&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21322;&#33258;&#21160;&#29983;&#25104;&#36719;&#20214;&#26550;&#26500;&#30340;&#26053;&#31243;
&lt;/p&gt;
&lt;p&gt;
From Requirements to Architecture: An AI-Based Journey to Semi-Automatically Generate Software Architectures. (arXiv:2401.14079v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21322;&#33258;&#21160;&#36719;&#20214;&#26550;&#26500;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#38656;&#27714;&#29983;&#25104;&#26550;&#26500;&#20505;&#36873;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#23450;&#37327;&#26041;&#27861;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#21644;&#20132;&#21449;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#65292;&#35774;&#35745;&#39046;&#22495;&#27169;&#22411;&#21644;&#36719;&#20214;&#26550;&#26500;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#24471;&#21040;&#30340;&#26550;&#26500;&#23545;&#31995;&#32479;&#30340;&#26381;&#21153;&#36136;&#37327;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30001;&#20110;&#26102;&#38388;&#21387;&#21147;&#65292;&#26550;&#26500;&#24072;&#36890;&#24120;&#21482;&#22522;&#20110;&#33258;&#24049;&#23545;&#39046;&#22495;&#30340;&#26377;&#38480;&#20102;&#35299;&#12289;&#27169;&#24335;&#21644;&#32463;&#39564;&#26469;&#24314;&#27169;&#19968;&#20010;&#26550;&#26500;&#65292;&#32780;&#19981;&#26159;&#23545;&#39046;&#22495;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#21644;&#35780;&#20272;&#22810;&#20010;&#20505;&#36873;&#26041;&#26696;&#65292;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26041;&#26696;&#12290;&#29616;&#26377;&#26041;&#27861;&#23581;&#35797;&#26681;&#25454;&#38656;&#27714;&#26469;&#29983;&#25104;&#39046;&#22495;&#27169;&#22411;&#65292;&#20294;&#20173;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#25163;&#21160;&#21162;&#21147;&#25165;&#33021;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31687;&#23637;&#26395;&#24615;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#21322;&#33258;&#21160;&#22320;&#26681;&#25454;&#38656;&#27714;&#29983;&#25104;&#36719;&#20214;&#26550;&#26500;&#20505;&#36873;&#26041;&#26696;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#24819;&#20351;&#29992;&#26550;&#26500;&#26435;&#34913;&#20998;&#26512;&#26041;&#27861;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#23450;&#37327;&#26041;&#27861;&#33258;&#21160;&#35780;&#20272;&#21644;&#36827;&#34892;&#20132;&#21449;&#20998;&#26512;&#29983;&#25104;&#30340;&#26550;&#26500;&#20505;&#36873;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing domain models and software architectures represents a significant challenge in software development, as the resulting architectures play a vital role in fulfilling the system's quality of service. Due to time pressure, architects often model only one architecture based on their known limited domain understanding, patterns, and experience instead of thoroughly analyzing the domain and evaluating multiple candidates, selecting the best fitting. Existing approaches try to generate domain models based on requirements, but still require time-consuming manual effort to achieve good results. Therefore, in this vision paper, we propose a method to generate software architecture candidates semi-automatically based on requirements using artificial intelligence techniques. We further envision an automatic evaluation and trade-off analysis of the generated architecture candidates using, e.g., the architecture trade-off analysis method combined with large language models and quantitative 
&lt;/p&gt;</description></item><item><title>Ta'keed&#26159;&#39318;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#35770;&#26029;&#30340;&#29983;&#25104;&#24335;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#36890;&#36807;&#22522;&#20110;&#26816;&#32034;&#29255;&#27573;&#30340;&#35770;&#26029;&#30495;&#23454;&#24615;&#35780;&#20272;&#21644;LLM-based&#35770;&#26029;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#30446;&#21069;&#38463;&#25289;&#20271;&#35821;&#39046;&#22495;&#32570;&#20047;&#29983;&#25104;&#35299;&#37322;&#35770;&#26029;&#21487;&#20449;&#24230;&#30340;&#30740;&#31350;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#27979;&#35797;&#40644;&#37329;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#20102;&#31995;&#32479;&#29983;&#25104;&#35299;&#37322;&#19982;&#40644;&#37329;&#26631;&#20934;&#35299;&#37322;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#19981;&#21516;&#29255;&#27573;&#25968;&#37327;&#23545;&#35770;&#26029;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.14067</link><description>&lt;p&gt;
Ta'keed: &#39318;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#35770;&#26029;&#30340;&#29983;&#25104;&#24335;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Ta'keed: The First Generative Fact-Checking System for Arabic Claims. (arXiv:2401.14067v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14067
&lt;/p&gt;
&lt;p&gt;
Ta'keed&#26159;&#39318;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#35770;&#26029;&#30340;&#29983;&#25104;&#24335;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#36890;&#36807;&#22522;&#20110;&#26816;&#32034;&#29255;&#27573;&#30340;&#35770;&#26029;&#30495;&#23454;&#24615;&#35780;&#20272;&#21644;LLM-based&#35770;&#26029;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#30446;&#21069;&#38463;&#25289;&#20271;&#35821;&#39046;&#22495;&#32570;&#20047;&#29983;&#25104;&#35299;&#37322;&#35770;&#26029;&#21487;&#20449;&#24230;&#30340;&#30740;&#31350;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#27979;&#35797;&#40644;&#37329;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#20102;&#31995;&#32479;&#29983;&#25104;&#35299;&#37322;&#19982;&#40644;&#37329;&#26631;&#20934;&#35299;&#37322;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#19981;&#21516;&#29255;&#27573;&#25968;&#37327;&#23545;&#35770;&#26029;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Ta'keed&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#38463;&#25289;&#20271;&#35821;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#23558;&#35770;&#26029;&#20998;&#31867;&#20026;&#8220;&#30495;&#8221;&#25110;&#8220;&#20551;&#8221;&#65292;&#20294;&#23545;&#20110;&#29983;&#25104;&#35770;&#26029;&#21487;&#20449;&#24230;&#30340;&#35299;&#37322;&#23588;&#20854;&#22312;&#38463;&#25289;&#20271;&#35821;&#39046;&#22495;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;Ta'keed&#36890;&#36807;&#22522;&#20110;&#26816;&#32034;&#29255;&#27573;&#30340;&#35770;&#26029;&#30495;&#23454;&#24615;&#35780;&#20272;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#21033;&#29992;&#20449;&#24687;&#26816;&#32034;&#21644;&#22522;&#20110;LLM&#30340;&#35770;&#26029;&#39564;&#35777;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;ArFactEx&#65292;&#19968;&#20010;&#24102;&#26377;&#25163;&#24037;&#35777;&#26126;&#21442;&#32771;&#30340;&#27979;&#35797;&#40644;&#37329;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#35813;&#31995;&#32479;&#12290;&#21021;&#22987;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;0.72&#30340;F1&#24471;&#20998;&#12290;&#19982;&#40644;&#37329;&#26631;&#20934;&#35299;&#37322;&#22312;&#21477;&#27861;&#21644;&#35821;&#20041;&#19978;&#36827;&#34892;&#27604;&#36739;&#65292;&#31995;&#32479;&#29983;&#25104;&#30340;&#35299;&#37322;&#24471;&#21040;&#20102;&#25512;&#33616;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#24179;&#22343;&#20313;&#24358;&#30456;&#20284;&#24230;&#20998;&#25968;&#20026;0.76&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19981;&#21516;&#29255;&#27573;&#25968;&#37327;&#23545;&#35770;&#26029;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;....
&lt;/p&gt;
&lt;p&gt;
This paper introduces Ta'keed, an explainable Arabic automatic fact-checking system. While existing research often focuses on classifying claims as "True" or "False," there is a limited exploration of generating explanations for claim credibility, particularly in Arabic. Ta'keed addresses this gap by assessing claim truthfulness based on retrieved snippets, utilizing two main components: information retrieval and LLM-based claim verification. We compiled the ArFactEx, a testing gold-labelled dataset with manually justified references, to evaluate the system. The initial model achieved a promising F1 score of 0.72 in the classification task. Meanwhile, the system's generated explanations are compared with gold-standard explanations syntactically and semantically. The study recommends evaluating using semantic similarities, resulting in an average cosine similarity score of 0.76. Additionally, we explored the impact of varying snippet quantities on claim classification accuracy, revealin
&lt;/p&gt;</description></item><item><title>CreativeSynth&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25193;&#25955;&#30340;&#21019;&#26032;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#35821;&#20041;&#20869;&#23481;&#23548;&#20837;&#21040;&#33402;&#26415;&#39046;&#22495;&#20013;&#65292;&#33021;&#22815;&#21327;&#35843;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#22810;&#20219;&#21153;&#65292;&#22312;&#33402;&#26415;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14066</link><description>&lt;p&gt;
CreativeSynth&#65306;&#22522;&#20110;&#22810;&#27169;&#24577;&#25193;&#25955;&#30340;&#35270;&#35273;&#33402;&#26415;&#21019;&#24847;&#34701;&#21512;&#19982;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion. (arXiv:2401.14066v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14066
&lt;/p&gt;
&lt;p&gt;
CreativeSynth&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25193;&#25955;&#30340;&#21019;&#26032;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#35821;&#20041;&#20869;&#23481;&#23548;&#20837;&#21040;&#33402;&#26415;&#39046;&#22495;&#20013;&#65292;&#33021;&#22815;&#21327;&#35843;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#22810;&#20219;&#21153;&#65292;&#22312;&#33402;&#26415;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#27493;&#65292;&#23637;&#31034;&#20102;&#20854;&#21512;&#25104;&#21508;&#31181;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#33402;&#26415;&#22270;&#20687;&#32534;&#36753;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#29992;&#25143;&#24448;&#24448;&#38590;&#20197;&#26500;&#24314;&#35814;&#32454;&#25551;&#36848;&#36755;&#20837;&#22270;&#20687;&#35270;&#35273;&#20803;&#32032;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#29305;&#23450;&#21306;&#22495;&#36827;&#34892;&#20462;&#25913;&#26102;&#24120;&#24120;&#20250;&#30772;&#22351;&#25972;&#20307;&#33402;&#26415;&#39118;&#26684;&#65292;&#20351;&#24471;&#23454;&#29616;&#19968;&#33268;&#19988;&#20855;&#26377;&#23457;&#32654;&#32479;&#19968;&#30340;&#20316;&#21697;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;CreativeSynth&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#20855;&#26377;&#21327;&#35843;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#22810;&#20219;&#21153;&#33021;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;CreativeSynth&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#35821;&#20041;&#20869;&#23481;&#23548;&#20837;&#21040;&#33402;&#26415;&#39046;&#22495;&#20013;&#65292;&#23454;&#29616;&#20102;&#21453;&#36716;&#21644;&#23454;&#26102;&#39118;&#26684;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text-to-image generative models have made impressive strides, showcasing their ability to synthesize a vast array of high-quality images. However, adapting these models for artistic image editing presents two significant challenges. Firstly, users struggle to craft textual prompts that meticulously detail visual elements of the input image. Secondly, prevalent models, when effecting modifications in specific zones, frequently disrupt the overall artistic style, complicating the attainment of cohesive and aesthetically unified artworks. To surmount these obstacles, we build the innovative unified framework CreativeSynth, which is based on a diffusion model with the ability to coordinate multimodal inputs and multitask in the field of artistic image generation. By integrating multimodal features with customized attention mechanisms, CreativeSynth facilitates the importation of real-world semantic content into the domain of art through inversion and real-time style transfer. T
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24038;&#21491;&#21322;&#29699;&#19987;&#38376;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19981;&#21516;&#30340;&#36816;&#21160;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21327;&#35843;&#24615;&#12289;&#36816;&#21160;&#25928;&#29575;&#21644;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.14057</link><description>&lt;p&gt;
&#24038;/&#21491;&#33041;&#12289;&#20154;&#31867;&#36816;&#21160;&#25511;&#21046;&#21450;&#23545;&#26426;&#22120;&#20154;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Left/Right Brain, human motor control and the implications for robotics. (arXiv:2401.14057v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24038;&#21491;&#21322;&#29699;&#19987;&#38376;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19981;&#21516;&#30340;&#36816;&#21160;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21327;&#35843;&#24615;&#12289;&#36816;&#21160;&#25928;&#29575;&#21644;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36816;&#21160;&#25511;&#21046;&#22120;&#30456;&#23545;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#20855;&#26377;&#21508;&#31181;&#20248;&#28857;&#65292;&#28982;&#32780;&#30001;&#20110;&#20854;&#26080;&#27861;&#20135;&#29983;&#21487;&#38752;&#30340;&#31934;&#30830;&#36816;&#21160;&#65292;&#22240;&#27492;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#37319;&#29992;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#21452;&#20391;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20316;&#20026;&#36816;&#21160;&#20219;&#21153;&#30340;&#25511;&#21046;&#31995;&#32479;&#12290;&#25105;&#20204;&#26088;&#22312;&#23454;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#30340;&#21322;&#29699;&#19987;&#38376;&#21270;&#65306;&#20248;&#21183;&#31995;&#32479;&#65288;&#36890;&#24120;&#26159;&#21491;&#25163;&#12289;&#24038;&#21322;&#29699;&#65289;&#25797;&#38271;&#21327;&#35843;&#21644;&#36816;&#21160;&#25928;&#29575;&#30340;&#20219;&#21153;&#65292;&#32780;&#38750;&#20248;&#21183;&#31995;&#32479;&#22312;&#38656;&#35201;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#21322;&#29699;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#19987;&#38376;&#21270;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20855;&#26377;&#19987;&#38376;&#21270;&#21322;&#29699;&#21644;&#26080;&#19987;&#38376;&#21270;&#21322;&#29699;&#12289;&#20855;&#26377;&#21322;&#29699;&#38388;&#36830;&#25509;&#65288;&#20195;&#34920;&#29983;&#29289;&#23398;&#33041;&#26725;&#65289;&#21644;&#26080;&#21322;&#29699;&#38388;&#36830;&#25509;&#12289;&#20855;&#26377;&#19987;&#38376;&#21270;&#21644;&#26080;&#19987;&#38376;&#21270;&#30340;&#21333;&#20391;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Network movement controllers promise a variety of advantages over conventional control methods however they are not widely adopted due to their inability to produce reliably precise movements. This research explores a bilateral neural network architecture as a control system for motor tasks. We aimed to achieve hemispheric specialisation similar to what is observed in humans across different tasks; the dominant system (usually the right hand, left hemisphere) excels at tasks involving coordination and efficiency of movement, and the non-dominant system performs better at tasks requiring positional stability. Specialisation was achieved by training the hemispheres with different loss functions tailored toward the expected behaviour of the respective hemispheres. We compared bilateral models with and without specialised hemispheres, with and without inter-hemispheric connectivity (representing the biological Corpus Callosum), and unilateral models with and without specialisation. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#30340;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#20197;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.14043</link><description>&lt;p&gt;
&#26397;&#30528;&#30446;&#26631;&#23548;&#21521;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Goal-oriented Large Language Model Prompting: A Survey. (arXiv:2401.14043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#30340;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#20197;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#32780;&#25552;&#31034;&#24037;&#31243;&#22312;&#20248;&#21270;LLM&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#24378;&#35843;&#35774;&#35745;&#25552;&#31034;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#20445;&#25345;&#20154;&#31867;&#36861;&#27714;LLM&#20687;&#20154;&#31867;&#24605;&#32771;&#30340;&#20154;&#31867;&#23398;&#20551;&#35774;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#30340;&#37325;&#35201;&#24615;&#65292;&#35813;&#20844;&#24335;&#25351;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#23558;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#26041;&#27861;&#20998;&#20026;&#20116;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#38454;&#27573;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#24076;&#26395;&#36827;&#19968;&#27493;&#24378;&#35843;&#21644;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown prominent performance in various downstream tasks in which prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not as an overview of current prompt engineering methods, aims to highlight the limitation of designing prompts while holding an anthropomorphic assumption that expects LLMs to think like humans. From our review of 35 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework by summarizing ten applicable tasks. With four future directions proposed, we hope to further emphasize and promote goal-oriented prompt engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#19977;&#32500;&#37325;&#24314;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#39640;&#26031;&#28857;&#20113;&#28210;&#26579;&#30340;&#22330;&#26223;&#37325;&#26500;&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;U-Scene&#25968;&#25454;&#38598;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#32508;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#22312;&#22330;&#26223;&#37325;&#24314;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14032</link><description>&lt;p&gt;
GauU-Scene: &#20351;&#29992;&#39640;&#26031;&#28857;&#20113;&#28210;&#26579;&#30340;&#22823;&#35268;&#27169;&#19977;&#32500;&#37325;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#22330;&#26223;&#37325;&#24314;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting. (arXiv:2401.14032v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#19977;&#32500;&#37325;&#24314;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#39640;&#26031;&#28857;&#20113;&#28210;&#26579;&#30340;&#22330;&#26223;&#37325;&#26500;&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;U-Scene&#25968;&#25454;&#38598;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#32508;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#22312;&#22330;&#26223;&#37325;&#24314;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35268;&#27169;&#22330;&#26223;&#37325;&#24314;&#22522;&#20934;&#65292;&#20351;&#29992;&#20102;&#26368;&#26032;&#24320;&#21457;&#30340;&#19977;&#32500;&#34920;&#31034;&#26041;&#27861;&#39640;&#26031;&#28857;&#20113;&#28210;&#26579;&#65292;&#22522;&#20110;&#25105;&#20204;&#24222;&#22823;&#30340;U-Scene&#25968;&#25454;&#38598;&#12290;U-Scene&#21253;&#21547;&#20102;&#36229;&#36807;1.5&#24179;&#26041;&#20844;&#37324;&#30340;&#21306;&#22495;&#65292;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;RGB&#25968;&#25454;&#21644;LiDAR&#22320;&#38754;&#30495;&#20540;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;Matrix 300&#26080;&#20154;&#26426;&#21644;&#39640;&#31934;&#24230;Zenmuse L1 LiDAR&#36827;&#34892;&#25968;&#25454;&#37319;&#38598;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#23627;&#39030;&#25968;&#25454;&#25910;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#22478;&#24066;&#21644;&#23398;&#26415;&#29615;&#22659;&#30340;&#29420;&#29305;&#32467;&#21512;&#65292;&#29992;&#20110;&#39640;&#32423;&#31354;&#38388;&#20998;&#26512;&#65292;&#35206;&#30422;&#20102;&#36229;&#36807;1.5&#24179;&#26041;&#20844;&#37324;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#39640;&#26031;&#28857;&#20113;&#28210;&#26579;&#30340;U-Scene&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#21508;&#31181;&#26032;&#39062;&#35270;&#35282;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#20123;&#32467;&#26524;&#19982;&#20934;&#30830;&#30340;&#28857;&#20113;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#31361;&#20986;&#20102;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#24378;&#35843;&#20102;&#32508;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel large-scale scene reconstruction benchmark using the newly developed 3D representation approach, Gaussian Splatting, on our expansive U-Scene dataset. U-Scene encompasses over one and a half square kilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground truth. For data acquisition, we employed the Matrix 300 drone equipped with the high-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This dataset, offers a unique blend of urban and academic environments for advanced spatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with Gaussian Splatting includes a detailed analysis across various novel viewpoints. We also juxtapose these results with those derived from our accurate point cloud dataset, highlighting significant differences that underscore the importance of combine multi-modal information
&lt;/p&gt;</description></item><item><title>Unitxt&#26159;&#19968;&#20010;&#28789;&#27963;&#12289;&#21487;&#20849;&#20139;&#21644;&#21487;&#22797;&#29992;&#30340;&#25968;&#25454;&#20934;&#22791;&#19982;&#35780;&#20272;&#24211;&#65292;&#38024;&#23545;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23450;&#21046;&#65292;&#23454;&#29616;&#20102;&#32467;&#26500;&#21270;&#12289;&#27169;&#22359;&#21270;&#21644;&#21487;&#23450;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#24211;&#38598;&#25104;&#20102;&#24120;&#29992;&#30340;&#24211;&#65292;&#23558;&#22788;&#29702;&#27969;&#31243;&#20998;&#35299;&#20026;&#27169;&#22359;&#21270;&#32452;&#20214;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#21644;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2401.14019</link><description>&lt;p&gt;
Unitxt&#65306;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#28789;&#27963;&#12289;&#21487;&#20849;&#20139;&#21644;&#21487;&#22797;&#29992;&#25968;&#25454;&#20934;&#22791;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI. (arXiv:2401.14019v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14019
&lt;/p&gt;
&lt;p&gt;
Unitxt&#26159;&#19968;&#20010;&#28789;&#27963;&#12289;&#21487;&#20849;&#20139;&#21644;&#21487;&#22797;&#29992;&#30340;&#25968;&#25454;&#20934;&#22791;&#19982;&#35780;&#20272;&#24211;&#65292;&#38024;&#23545;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23450;&#21046;&#65292;&#23454;&#29616;&#20102;&#32467;&#26500;&#21270;&#12289;&#27169;&#22359;&#21270;&#21644;&#21487;&#23450;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#24211;&#38598;&#25104;&#20102;&#24120;&#29992;&#30340;&#24211;&#65292;&#23558;&#22788;&#29702;&#27969;&#31243;&#20998;&#35299;&#20026;&#27169;&#22359;&#21270;&#32452;&#20214;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#21644;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#20256;&#32479;&#30340;&#25991;&#26412;&#22788;&#29702;&#27969;&#31243;&#38480;&#21046;&#20102;&#30740;&#31350;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#38024;&#23545;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#27169;&#22411;&#32452;&#21512;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#38543;&#30528;&#31995;&#32479;&#25552;&#31034;&#12289;&#27169;&#22411;&#29305;&#23450;&#26684;&#24335;&#12289;&#25351;&#20196;&#31561;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#38656;&#35201;&#36716;&#21521;&#19968;&#31181;&#32467;&#26500;&#21270;&#12289;&#27169;&#22359;&#21270;&#21644;&#21487;&#23450;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Unitxt&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#24211;&#65292;&#19987;&#20026;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#30340;&#25991;&#26412;&#25968;&#25454;&#20934;&#22791;&#21644;&#35780;&#20272;&#32780;&#35774;&#35745;&#12290;Unitxt&#19982;HuggingFace&#21644;LM-eval-harness&#31561;&#24120;&#29992;&#24211;&#36827;&#34892;&#20102;&#26412;&#22320;&#38598;&#25104;&#65292;&#24182;&#23558;&#22788;&#29702;&#27969;&#31243;&#20998;&#35299;&#20026;&#27169;&#22359;&#21270;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#26131;&#20110;&#23450;&#21046;&#21644;&#20849;&#20139;&#30340;&#21151;&#33021;&#12290;&#36825;&#20123;&#32452;&#20214;&#21253;&#25324;&#27169;&#22411;&#29305;&#23450;&#26684;&#24335;&#12289;&#20219;&#21153;&#25552;&#31034;&#21644;&#35768;&#22810;&#20854;&#20182;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#22788;&#29702;&#23450;&#20041;&#12290;Unitxt-Catalog&#38598;&#20013;&#20102;&#36825;&#20123;&#32452;&#20214;&#65292;&#20419;&#36827;&#20102;&#29616;&#20195;&#25991;&#26412;&#25968;&#25454;&#27969;&#31243;&#20013;&#30340;&#21327;&#20316;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the dynamic landscape of generative NLP, traditional text processing pipelines limit research flexibility and reproducibility, as they are tailored to specific dataset, task, and model combinations. The escalating complexity, involving system prompts, model-specific formats, instructions, and more, calls for a shift to a structured, modular, and customizable solution. Addressing this need, we present Unitxt, an innovative library for customizable textual data preparation and evaluation tailored to generative language models. Unitxt natively integrates with common libraries like HuggingFace and LM-eval-harness and deconstructs processing flows into modular components, enabling easy customization and sharing between practitioners. These components encompass model-specific formats, task prompts, and many other comprehensive dataset processing definitions. The Unitxt-Catalog centralizes these components, fostering collaboration and exploration in modern textual data workflows. Beyond be
&lt;/p&gt;</description></item><item><title>CMMU&#26159;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#27700;&#24179;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14011</link><description>&lt;p&gt;
CMMU: &#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#19982;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. (arXiv:2401.14011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14011
&lt;/p&gt;
&lt;p&gt;
CMMU&#26159;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#27700;&#24179;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#30693;&#35782;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;MLLM&#30340;&#26234;&#33021;&#27700;&#24179;&#25152;&#38656;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25484;&#25569;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24403;&#21069;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#22810;&#39033;&#36873;&#25321;&#39064;&#19978;&#65292;&#24182;&#19988;&#22312;&#35780;&#20272;&#30340;&#20840;&#38754;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CMMU&#65292;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;CMMU&#21253;&#21547;7&#20010;&#23398;&#31185;&#30340;3603&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20998;&#20026;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31867;&#65292;&#23545;MLLMs&#25552;&#20986;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#35780;&#20272;&#31574;&#30053;&#65292;&#31216;&#20026;ShiftCheck&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models(MLLMs) have achieved remarkable progress and demonstrated powerful knowledge comprehension and reasoning abilities. However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge. Current multi-modal benchmarks for domain-specific knowledge concentrate on multiple-choice questions and are predominantly available in English, which imposes limitations on the comprehensiveness of the evaluation. To this end, we introduce CMMU, a novel benchmark for multi-modal and multi-type question understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The questions can be categorized into 3 types: multiple-choice, multiple-response, and fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we propose a rigorous evaluation strategy called ShiftCheck for assessing multiple-choice questions. The strat
&lt;/p&gt;</description></item><item><title>ConstraintChecker&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25554;&#20214;&#65292;&#29992;&#20110;&#25512;&#29702;&#24120;&#35782;&#30693;&#35782;&#24211;&#12290;&#23427;&#36890;&#36807;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#21644;&#26816;&#26597;&#26174;&#24335;&#32422;&#26463;&#65292;&#24110;&#21161;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#24120;&#35782;&#30693;&#35782;&#24211;&#25512;&#29702;&#26102;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.14003</link><description>&lt;p&gt;
ConstraintChecker&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24120;&#35782;&#30693;&#35782;&#24211;&#30340;&#25554;&#20214;
&lt;/p&gt;
&lt;p&gt;
ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases. (arXiv:2401.14003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14003
&lt;/p&gt;
&lt;p&gt;
ConstraintChecker&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25554;&#20214;&#65292;&#29992;&#20110;&#25512;&#29702;&#24120;&#35782;&#30693;&#35782;&#24211;&#12290;&#23427;&#36890;&#36807;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#21644;&#26816;&#26597;&#26174;&#24335;&#32422;&#26463;&#65292;&#24110;&#21161;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#24120;&#35782;&#30693;&#35782;&#24211;&#25512;&#29702;&#26102;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#24211;&#30340;&#25512;&#29702;&#65288;&#21363;&#24120;&#35782;&#30693;&#35782;&#24211;&#25512;&#29702;&#65289;&#24050;&#34987;&#25506;&#32034;&#20316;&#20026;&#19968;&#31181;&#36890;&#36807;&#21407;&#22987;&#24120;&#35782;&#30693;&#35782;&#24211;&#21644;&#22806;&#37096;&#20808;&#39564;&#30693;&#35782;&#26469;&#33719;&#21462;&#26032;&#30340;&#24120;&#35782;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#24120;&#35782;&#30693;&#35782;&#24211;&#25512;&#29702;&#12290;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#23427;&#20204;&#24456;&#38590;&#21482;&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#20174;&#24120;&#35782;&#30693;&#35782;&#24211;&#20013;&#33719;&#21462;&#26174;&#24335;&#20851;&#31995;&#32422;&#26463;&#65292;&#36825;&#26159;&#22240;&#20026;&#32570;&#20047;&#31526;&#21495;&#25512;&#29702;&#33021;&#21147;&#65288;Bengio&#31561;&#20154;&#65292;2021&#24180;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConstraintChecker&#30340;&#25554;&#20214;&#65292;&#23427;&#22522;&#20110;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#21644;&#26816;&#26597;&#26174;&#24335;&#32422;&#26463;&#12290;&#22312;&#32771;&#34385;&#26032;&#30340;&#30693;&#35782;&#23454;&#20363;&#26102;&#65292;ConstraintChecker&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22359;&#29983;&#25104;&#32422;&#26463;&#21015;&#34920;&#65292;&#28982;&#21518;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#27169;&#22359;&#26816;&#26597;&#35813;&#30693;&#35782;&#23454;&#20363;&#26159;&#21542;&#28385;&#36275;&#25152;&#26377;&#32422;&#26463;&#12290;&#28982;&#21518;&#65292;&#33719;&#21462;&#30340;&#32422;&#26463;&#26816;&#26597;&#32467;&#26524;&#34987;&#32858;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has been explored as a way to acquire new commonsense knowledge based on reference knowledge in the original CSKBs and external prior knowledge. Despite the advancement of Large Language Models (LLM) and prompt engineering techniques in various reasoning tasks, they still struggle to deal with CSKB reasoning. One of the problems is that it is hard for them to acquire explicit relational constraints in CSKBs from only in-context exemplars, due to a lack of symbolic reasoning capabilities (Bengio et al., 2021). To this end, we proposed **ConstraintChecker**, a plugin over prompting techniques to provide and check explicit constraints. When considering a new knowledge instance, ConstraintChecker employs a rule-based module to produce a list of constraints, then it uses a zero-shot learning module to check whether this knowledge instance satisfies all constraints. The acquired constraint-checking result is then aggrega
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35843;&#26597;-&#25972;&#21512;-&#24320;&#21457;&#65288;ICE&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#20219;&#21153;&#38388;&#30340;&#33258;&#36827;&#21270;&#26469;&#25552;&#39640;AI&#20195;&#29702;&#30340;&#36866;&#24212;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;ICE&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;API&#35843;&#29992;&#65292;&#21516;&#26102;&#19982;GPT-3.5&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21508;&#31181;&#20195;&#29702;&#20219;&#21153;&#19978;&#36798;&#21040;&#19982;GPT-4&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13996</link><description>&lt;p&gt;
&#35843;&#26597;-&#25972;&#21512;-&#24320;&#21457;&#65306;&#19968;&#31181;&#29992;&#20110;&#20219;&#21153;&#38388;&#20195;&#29702;&#33258;&#36827;&#21270;&#30340;&#36890;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution. (arXiv:2401.13996v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35843;&#26597;-&#25972;&#21512;-&#24320;&#21457;&#65288;ICE&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#20219;&#21153;&#38388;&#30340;&#33258;&#36827;&#21270;&#26469;&#25552;&#39640;AI&#20195;&#29702;&#30340;&#36866;&#24212;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;ICE&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;API&#35843;&#29992;&#65292;&#21516;&#26102;&#19982;GPT-3.5&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21508;&#31181;&#20195;&#29702;&#20219;&#21153;&#19978;&#36798;&#21040;&#19982;GPT-4&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#35843;&#26597;-&#25972;&#21512;-&#24320;&#21457;&#65288;ICE&#65289;&#30340;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#20219;&#21153;&#38388;&#30340;&#33258;&#36827;&#21270;&#26469;&#25552;&#39640;AI&#20195;&#29702;&#30340;&#36866;&#24212;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#27880;&#37325;&#20219;&#21153;&#20869;&#23398;&#20064;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;ICE&#20419;&#36827;&#20102;&#20219;&#21153;&#38388;&#30693;&#35782;&#30340;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#30495;&#27491;&#30340;&#33258;&#36827;&#21270;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#32463;&#39564;&#23398;&#20064;&#12290;&#35813;&#31574;&#30053;&#21160;&#24577;&#22320;&#35843;&#26597;&#35268;&#21010;&#21644;&#25191;&#34892;&#36712;&#36857;&#65292;&#23558;&#20854;&#25972;&#21512;&#20026;&#31616;&#21270;&#30340;&#24037;&#20316;&#27969;&#21644;&#31649;&#36947;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#25913;&#36827;&#20219;&#21153;&#25191;&#34892;&#12290;&#25105;&#20204;&#22312;XAgent&#26694;&#26550;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ICE&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#23558;API&#35843;&#29992;&#20943;&#23569;&#39640;&#36798;80&#65285;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#33021;&#21147;&#30340;&#38656;&#27714;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#19982;GPT-3.5&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;ICE&#30340;&#24615;&#33021;&#22312;&#21508;&#31181;&#20195;&#29702;&#20219;&#21153;&#19978;&#19982;&#21407;&#22987;GPT-4&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#33258;&#36827;&#21270;&#26041;&#27861;&#20195;&#34920;&#20102;&#20195;&#29702;&#35774;&#35745;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#20026;&#26356;&#24378;&#22823;&#30340;AI&#31038;&#21306;&#21644;&#29983;&#24577;&#31995;&#32479;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#21521;&#20840;&#38754;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#36328;&#36827;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy for enhancing the adaptability and flexibility of AI agents through inter-task self-evolution. Unlike existing methods focused on intra-task learning, ICE promotes the transfer of knowledge between tasks for genuine self-evolution, similar to human experience learning. The strategy dynamically investigates planning and execution trajectories, consolidates them into simplified workflows and pipelines, and exploits them for improved task execution. Our experiments on the XAgent framework demonstrate ICE's effectiveness, reducing API calls by as much as 80% and significantly decreasing the demand for the model's capability. Specifically, when combined with GPT-3.5, ICE's performance matches that of raw GPT-4 across various agent tasks. We argue that this self-evolution approach represents a paradigm shift in agent design, contributing to a more robust AI community and ecosystem, and moving a step closer to full 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ADAPTER&#30340;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#32593;&#32476;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20511;&#21161;&#21452;&#21521;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#65292;ADAPTER&#33021;&#22815;&#22312;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#29305;&#24449;&#65292;&#24182;&#36991;&#20813;&#30417;&#30563;&#23849;&#28291;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;BSCD-FSL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13987</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#32593;&#32476;&#36827;&#34892;&#36328;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Few-Shot Learning via Adaptive Transformer Networks. (arXiv:2401.13987v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ADAPTER&#30340;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#32593;&#32476;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20511;&#21161;&#21452;&#21521;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#65292;ADAPTER&#33021;&#22815;&#22312;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#29305;&#24449;&#65292;&#24182;&#36991;&#20813;&#30417;&#30563;&#23849;&#28291;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;BSCD-FSL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#22522;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#21516;&#39046;&#22495;&#30340;&#20551;&#35774;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#32593;&#32476;&#65288;ADAPTER&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22788;&#29702;&#22522;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22823;&#39046;&#22495;&#36716;&#31227;&#30340;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;ADAPTER&#22522;&#20110;&#21452;&#21521;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#24605;&#24819;&#26500;&#24314;&#65292;&#20197;&#23398;&#20064;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#20351;&#29992;DINO&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#20135;&#29983;&#22810;&#26679;&#30340;&#12289;&#36739;&#23569;&#20559;&#35265;&#30340;&#29305;&#24449;&#65292;&#20197;&#36991;&#20813;&#30417;&#30563;&#23849;&#28291;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#21516;&#26102;&#32771;&#34385;&#25509;&#36817;&#26679;&#26412;&#30340;&#39044;&#27979;&#26631;&#31614;&#65292;&#25552;&#39640;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;ADAPTER&#30340;&#24615;&#33021;&#22312;BSCD-FSL&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#23427;&#20197;&#26174;&#33879;&#30340;&#20248;&#21183;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most few-shot learning works rely on the same domain assumption between the base and the target tasks, hindering their practical applications. This paper proposes an adaptive transformer network (ADAPTER), a simple but effective solution for cross-domain few-shot learning where there exist large domain shifts between the base task and the target task. ADAPTER is built upon the idea of bidirectional cross-attention to learn transferable features between the two domains. The proposed architecture is trained with DINO to produce diverse, and less biased features to avoid the supervision collapse problem. Furthermore, the label smoothing approach is proposed to improve the consistency and reliability of the predictions by also considering the predicted labels of the close samples in the embedding space. The performance of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it outperforms prior arts with significant margins.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#37322;&#19968;&#33268;&#24615;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30456;&#20851;&#31034;&#20363;&#19978;&#29983;&#25104;&#26356;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#25552;&#39640;&#20102;10.0%&#30340;&#35299;&#37322;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#27867;&#21270;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.13986</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#37322;&#19968;&#33268;&#24615;&#24494;&#35843;&#23454;&#29616;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning. (arXiv:2401.13986v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#37322;&#19968;&#33268;&#24615;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30456;&#20851;&#31034;&#20363;&#19978;&#29983;&#25104;&#26356;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#25552;&#39640;&#20102;10.0%&#30340;&#35299;&#37322;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#27867;&#21270;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#33021;&#22815;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#12289;&#27969;&#30021;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#19981;&#21516;&#65292;&#23427;&#20204;&#22312;&#19981;&#21516;&#36755;&#20837;&#19978;&#29983;&#25104;&#30340;&#35299;&#37322;&#24120;&#24120;&#19981;&#19968;&#33268;&#12290;&#20363;&#22914;&#65292;LLM&#22312;&#22238;&#31572;&#38382;&#39064;&#8220;&#40635;&#38592;&#33021;&#39134;&#21527;&#65311;&#8221;&#26102;&#21487;&#33021;&#29983;&#25104;&#35299;&#37322;&#8220;&#25152;&#26377;&#40479;&#37117;&#33021;&#39134;&#8221;&#65292;&#20294;&#21516;&#26102;&#22312;&#22238;&#31572;&#19982;&#20043;&#30456;&#20851;&#30340;&#38382;&#39064;&#8220;&#20225;&#40517;&#33021;&#39134;&#21527;&#65311;&#8221;&#26102;&#22238;&#31572;&#8220;&#19981;&#34892;&#8221;&#12290;&#35299;&#37322;&#24212;&#35813;&#22312;&#30456;&#20851;&#31034;&#20363;&#20013;&#20445;&#25345;&#19968;&#33268;&#65292;&#20197;&#20415;&#35753;&#20154;&#31867;&#33021;&#22815;&#27169;&#25311;LLM&#22312;&#22810;&#20010;&#31034;&#20363;&#19978;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#37322;&#19968;&#33268;&#24615;&#24494;&#35843;&#65288;EC-finetuning&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36866;&#24212;LLM&#22312;&#30456;&#20851;&#31034;&#20363;&#19978;&#29983;&#25104;&#26356;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;EC-finetuning&#21253;&#25324;&#22312;&#32463;&#36807;&#31934;&#24515;&#26500;&#24314;&#30340;&#21253;&#21547;&#19968;&#33268;&#35299;&#37322;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;LLM&#12290;&#22312;&#21508;&#31181;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#65292;EC-finetuning&#22312;&#22235;&#20010;&#24494;&#35843;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#25552;&#39640;&#20102;10.0%&#30340;&#35299;&#37322;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#22312;&#19971;&#20010;&#22806;&#37096;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs. For example, an LLM may generate the explanation "all birds can fly" when answering the question "Can sparrows fly?" but meanwhile answer "no" to the related question "Can penguins fly?". Explanations should be consistent across related examples so that they allow a human to simulate the LLM's decision process on multiple examples. We propose explanation-consistency finetuning (EC-finetuning), a method that adapts LLMs to generate more consistent natural-language explanations on related examples. EC-finetuning involves finetuning LLMs on synthetic data that is carefully constructed to contain consistent explanations. Across a variety of question-answering datasets in various domains, EC-finetuning yields a 10.0% relative explanation consistency improvement on four finetuning datasets, and generalizes to seven out
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Leeroo&#32534;&#25490;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#32534;&#25490;&#22120;&#22312;&#24615;&#33021;&#19978;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#25104;&#26412;&#21482;&#26377;&#20854;&#19977;&#20998;&#20043;&#20108;&#12290;&#24403;&#20801;&#35768;&#26356;&#39640;&#30340;&#25104;&#26412;&#26102;&#65292;Leeroo&#32534;&#25490;&#22120;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;Mixtral&#27169;&#22411;&#65292;&#24182;&#19988;&#24403;&#38598;&#25104;GPT4&#26102;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.13979</link><description>&lt;p&gt;
Leeroo Orchestrator: &#36890;&#36807;&#27169;&#22411;&#38598;&#25104;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration. (arXiv:2401.13979v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Leeroo&#32534;&#25490;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#32534;&#25490;&#22120;&#22312;&#24615;&#33021;&#19978;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#25104;&#26412;&#21482;&#26377;&#20854;&#19977;&#20998;&#20043;&#20108;&#12290;&#24403;&#20801;&#35768;&#26356;&#39640;&#30340;&#25104;&#26412;&#26102;&#65292;Leeroo&#32534;&#25490;&#22120;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;Mixtral&#27169;&#22411;&#65292;&#24182;&#19988;&#24403;&#38598;&#25104;GPT4&#26102;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#30340;&#38598;&#20307;&#30693;&#35782;&#65292;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32534;&#25490;&#22120;&#65292;&#33021;&#22815;&#36873;&#25321;&#26368;&#20339;&#30340;&#24213;&#23618;LLM&#19987;&#23478;&#36827;&#34892;&#20219;&#21153;&#25191;&#34892;&#12290;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#25105;&#23545;&#24328;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26597;&#35810;&#29983;&#25104;&#12289;&#32534;&#25490;&#21644;&#35780;&#20272;&#30340;&#24490;&#29615;&#65292;&#20026;&#32534;&#25490;&#22120;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20027;&#35201;&#38024;&#23545;MMLU&#22522;&#20934;&#65292;&#22312;Hugging Face&#19978;&#20351;&#29992;&#20102;&#20855;&#26377;7B&#12289;13B&#21644;34B&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;Leeroo&#32534;&#25490;&#22120;&#23454;&#29616;&#20102;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#21482;&#20135;&#29983;&#20102;&#20854;&#25104;&#26412;&#30340;&#19977;&#20998;&#20043;&#20108;&#12290;&#27492;&#22806;&#65292;&#22686;&#21152;&#20801;&#35768;&#30340;&#25104;&#26412;&#36229;&#36807;&#20102;Mixtral&#30340;&#20934;&#30830;&#24615;&#65292;&#36798;&#21040;&#20102;75.9%&#30340;&#20934;&#30830;&#24615;&#12290;&#24403;&#23558;GPT4&#38598;&#25104;&#21040;&#24213;&#23618;&#27169;&#22411;&#27744;&#20013;&#26102;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20063;&#24471;&#21040;&#20102;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an architecture to harness the collective knowledge of multiple trained LLMs to create a new state-of-the-art. At the core of this framework is a LLM-based orchestrator that is adept at picking the right underlying LLM experts for optimal task execution. Inspired by self-play in reinforcement learning, we created a loop of query generation, orchestration, and evaluation to generate training data for the orchestrator. Our evaluation focused on the MMLU benchmark, employing models with 7B, 13B, and 34B parameters available on Hugging Face. The results demonstrate new state-of-the-art open-source models: Our Leeroo orchestrator achieves performance on par with the Mixtral model while incurring only two-thirds of its cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by over 5% at the same cost level, reaching an accuracy of 75.9%. Further enhancements were observed when integrating GPT4 into the underlying model pool. The Leeroo orchestrator
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26080;&#35821;&#20041;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#22270;&#20687;&#29983;&#25104;&#30340;&#20219;&#24847;&#39118;&#26684;&#22270;&#20687;&#25805;&#32437;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#33402;&#26415;&#22270;&#20687;&#20013;&#20934;&#30830;&#35821;&#20041;&#33719;&#21462;&#30340;&#22256;&#38590;&#65292;&#24182;&#36991;&#20813;&#20102;&#36328;&#22495;&#20266;&#24433;&#21644;&#31934;&#30830;&#32467;&#26500;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.13976</link><description>&lt;p&gt;
&#23398;&#20064;&#25805;&#32437;&#33402;&#26415;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Learning to Manipulate Artistic Images. (arXiv:2401.13976v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26080;&#35821;&#20041;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#22270;&#20687;&#29983;&#25104;&#30340;&#20219;&#24847;&#39118;&#26684;&#22270;&#20687;&#25805;&#32437;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#33402;&#26415;&#22270;&#20687;&#20013;&#20934;&#30830;&#35821;&#20041;&#33719;&#21462;&#30340;&#22256;&#38590;&#65292;&#24182;&#36991;&#20813;&#20102;&#36328;&#22495;&#20266;&#24433;&#21644;&#31934;&#30830;&#32467;&#26500;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36827;&#23637;&#26174;&#33879;&#38477;&#20302;&#20102;&#33402;&#26415;&#21019;&#20316;&#30340;&#38556;&#30861;&#12290;&#22522;&#20110;&#31034;&#20363;&#30340;&#22270;&#20687;&#36716;&#25442;&#26041;&#27861;&#22240;&#20854;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#35821;&#20041;&#26377;&#25152;&#20551;&#35774;&#25110;&#38656;&#35201;&#35821;&#20041;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780;&#22312;&#33402;&#26415;&#22270;&#20687;&#20013;&#20934;&#30830;&#30340;&#35821;&#20041;&#20449;&#24687;&#24182;&#19981;&#23481;&#26131;&#33719;&#24471;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#20808;&#39564;&#21644;&#22312;&#31354;&#38388;&#22495;&#20013;&#30340;&#29305;&#24449;&#21387;&#32553;&#65292;&#36825;&#20123;&#26041;&#27861;&#20250;&#20135;&#29983;&#36328;&#22495;&#20266;&#24433;&#21644;&#19981;&#31934;&#30830;&#30340;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#24847;&#39118;&#26684;&#22270;&#20687;&#25805;&#32437;&#32593;&#32476;&#65288;SIM-Net&#65289;&#65292;&#21033;&#29992;&#26080;&#35821;&#20041;&#30340;&#20449;&#24687;&#20316;&#20026;&#24341;&#23548;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#21306;&#22495;&#20256;&#36755;&#31574;&#30053;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24179;&#34913;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#38646;&#26679;&#24335;&#22270;&#20687;&#25805;&#20316;&#12290;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancement in computer vision has significantly lowered the barriers to artistic creation. Exemplar-based image translation methods have attracted much attention due to flexibility and controllability. However, these methods hold assumptions regarding semantics or require semantic information as the input, while accurate semantics is not easy to obtain in artistic images. Besides, these methods suffer from cross-domain artifacts due to training data prior and generate imprecise structure due to feature compression in the spatial domain. In this paper, we propose an arbitrary Style Image Manipulation Network (SIM-Net), which leverages semantic-free information as guidance and a region transportation strategy in a self-supervised manner for image generation. Our method balances computational efficiency and high resolution to a certain extent. Moreover, our method facilitates zero-shot style image manipulation. Both qualitative and quantitative experiments demonstrate the superior
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;BootPIG&#26550;&#26500;&#65292;&#22312;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20013;&#24341;&#23548;&#20010;&#24615;&#21270;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#12290;&#35813;&#26550;&#26500;&#20351;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#21442;&#32771;&#22270;&#20687;&#26469;&#25351;&#23548;&#29983;&#25104;&#22270;&#20687;&#20013;&#27010;&#24565;&#30340;&#22806;&#35266;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;BootPIG&#26550;&#26500;&#30340;&#20010;&#24615;&#21270;&#33021;&#21147;&#30340;&#24341;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.13974</link><description>&lt;p&gt;
BootPIG: &#22312;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20013;&#24341;&#23548;&#38646;&#26679;&#26412;&#20010;&#24615;&#21270;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#30340;&#24341;&#23548;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models. (arXiv:2401.13974v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13974
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;BootPIG&#26550;&#26500;&#65292;&#22312;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20013;&#24341;&#23548;&#20010;&#24615;&#21270;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#12290;&#35813;&#26550;&#26500;&#20351;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#21442;&#32771;&#22270;&#20687;&#26469;&#25351;&#23548;&#29983;&#25104;&#22270;&#20687;&#20013;&#27010;&#24565;&#30340;&#22806;&#35266;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;BootPIG&#26550;&#26500;&#30340;&#20010;&#24615;&#21270;&#33021;&#21147;&#30340;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36981;&#24490;&#36755;&#20837;&#25552;&#31034;&#30340;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#35789;&#35821;&#26469;&#25551;&#36848;&#25152;&#38656;&#30340;&#27010;&#24565;&#38480;&#21046;&#20102;&#23545;&#29983;&#25104;&#22270;&#20687;&#22806;&#35266;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#33021;&#21147;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65288;BootPIG&#65289;&#65292;&#20801;&#35768;&#29992;&#25143;&#25552;&#20379;&#30446;&#26631;&#23545;&#35937;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#20197;&#25351;&#23548;&#29983;&#25104;&#22270;&#20687;&#20013;&#27010;&#24565;&#30340;&#22806;&#35266;&#12290;&#25552;&#20986;&#30340;BootPIG&#26550;&#26500;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26368;&#23567;&#30340;&#20462;&#25913;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;UNet&#27169;&#22411;&#26469;&#24341;&#23548;&#29983;&#25104;&#22270;&#20687;&#30340;&#22806;&#35266;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#35757;&#32451;&#36807;&#31243;&#65292;&#33021;&#22815;&#20351;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12289;LLM&#32842;&#22825;&#20195;&#29702;&#21644;i&#65288;&#20505;&#36873;&#22270;&#20687;&#65289;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#24341;&#23548;BootPIG&#26550;&#26500;&#30340;&#20010;&#24615;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-image generation models have demonstrated incredible success in generating images that faithfully follow input prompts. However, the requirement of using words to describe a desired concept provides limited control over the appearance of the generated concepts. In this work, we address this shortcoming by proposing an approach to enable personalization capabilities in existing text-to-image diffusion models. We propose a novel architecture (BootPIG) that allows a user to provide reference images of an object in order to guide the appearance of a concept in the generated images.  The proposed BootPIG architecture makes minimal modifications to a pretrained text-to-image diffusion model and utilizes a separate UNet model to steer the generations toward the desired appearance. We introduce a training procedure that allows us to bootstrap personalization capabilities in the BootPIG architecture using data generated from pretrained text-to-image models, LLM chat agents, and i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Meta-Transformer Networks&#65288;MANTRA&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#21160;&#24577;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24555;&#36895;&#21644;&#24930;&#36895;&#23398;&#20064;&#22120;&#30340;&#32467;&#21512;&#65292;&#20197;&#21450;&#20351;&#29992;&#36890;&#29992;&#34920;&#31034;&#36716;&#25442;&#23618;&#65292;&#23454;&#29616;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.13968</link><description>&lt;p&gt;
&#21160;&#24577;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#22522;&#20110;&#20803;&#36716;&#25442;&#32593;&#32476;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks. (arXiv:2401.13968v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Meta-Transformer Networks&#65288;MANTRA&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#21160;&#24577;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24555;&#36895;&#21644;&#24930;&#36895;&#23398;&#20064;&#22120;&#30340;&#32467;&#21512;&#65292;&#20197;&#21450;&#20351;&#29992;&#36890;&#29992;&#34920;&#31034;&#36716;&#25442;&#23618;&#65292;&#23454;&#29616;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#21487;&#38752;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#20302;&#35745;&#31639;&#21644;&#20869;&#23384;&#21344;&#29992;&#20197;&#21450;&#23545;&#21160;&#24577;&#23398;&#20064;&#29615;&#22659;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Meta-Transformer Networks&#65288;MANTRA&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;MANTRA&#20381;&#36182;&#20110;&#24555;&#36895;&#21644;&#24930;&#36895;&#23398;&#20064;&#22120;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#19968;&#32452;&#24555;&#36895;&#23398;&#20064;&#22120;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21516;&#26102;&#24555;&#36895;&#36866;&#24212;&#21464;&#21270;&#12290;&#24930;&#36895;&#23398;&#20064;&#22120;&#20026;&#24555;&#36895;&#23398;&#20064;&#22120;&#23450;&#21046;&#36866;&#24403;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#34920;&#31034;&#36716;&#25442;&#23618;&#20135;&#29983;&#20219;&#21153;&#36866;&#24212;&#24615;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#65292;&#23454;&#29616;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#24555;&#36895;&#36866;&#24212;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20855;&#26377;&#19981;&#21516;&#39044;&#27979;&#38271;&#24230;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#21464;&#37327;&#21644;&#21333;&#21464;&#37327;&#35774;&#32622;&#19979;&#33267;&#23569;&#27604;&#22522;&#20934;&#31639;&#27861;&#25913;&#36827;&#20102;3&#65285;&#12290;MANTRA&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;&#38142;&#25509;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
A reliable long-term time-series forecaster is highly demanded in practice but comes across many challenges such as low computational and memory footprints as well as robustness against dynamic learning environments. This paper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic long-term time-series forecasting tasks. MANTRA relies on the concept of fast and slow learners where a collection of fast learners learns different aspects of data distributions while adapting quickly to changes. A slow learner tailors suitable representations to fast learners. Fast adaptations to dynamic environments are achieved using the universal representation transformer layers producing task-adapted representations with a small number of parameters. Our experiments using four datasets with different prediction lengths demonstrate the advantage of our approach with at least $3\%$ improvements over the baseline algorithms for both multivariate and univariate settings. Source codes of MANT
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#33258;&#21160;&#31038;&#20250;&#25805;&#20316;&#31995;&#32479;&#65288;ASOS&#65289;&#65292;&#29992;&#20110;&#36890;&#29992;&#31038;&#20250;&#35299;&#20915;&#26041;&#26696;&#30340;&#29983;&#25104;&#65292;&#36890;&#36807;&#36229;&#22270;&#21644;&#31526;&#21495;&#21270;&#28151;&#21512;&#31574;&#30053;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#38382;&#39064;&#20998;&#26512;&#21644;&#35268;&#23450;&#12290;</title><link>http://arxiv.org/abs/2401.13945</link><description>&lt;p&gt;
&#31038;&#20250;&#38382;&#39064;&#30340;&#36890;&#29992;&#33258;&#21160;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
General Automatic Solution Generation of Social Problems. (arXiv:2401.13945v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#33258;&#21160;&#31038;&#20250;&#25805;&#20316;&#31995;&#32479;&#65288;ASOS&#65289;&#65292;&#29992;&#20110;&#36890;&#29992;&#31038;&#20250;&#35299;&#20915;&#26041;&#26696;&#30340;&#29983;&#25104;&#65292;&#36890;&#36807;&#36229;&#22270;&#21644;&#31526;&#21495;&#21270;&#28151;&#21512;&#31574;&#30053;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#38382;&#39064;&#20998;&#26512;&#21644;&#35268;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#24403;&#20195;&#31038;&#20250;&#31995;&#32479;&#26085;&#30410;&#22797;&#26434;&#21644;&#22810;&#23618;&#27425;&#30340;&#29305;&#24615;&#65292;&#25163;&#21160;&#29983;&#25104;&#35299;&#20915;&#30456;&#20851;&#31038;&#20250;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#25104;&#20026;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#25512;&#21160;&#20102;&#35745;&#31639;&#26041;&#27861;&#30340;&#25506;&#32034;&#65292;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#33258;&#21160;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#28041;&#21450;&#29305;&#23450;&#22330;&#26223;&#30340;&#23616;&#37096;&#31038;&#20250;&#35268;&#23450;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#20010;&#19987;&#20026;&#36890;&#29992;&#31038;&#20250;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#32780;&#35774;&#35745;&#30340;&#33258;&#21160;&#31038;&#20250;&#25805;&#20316;&#31995;&#32479;&#65288;ASOS&#65289;&#65292;&#35813;&#31995;&#32479;&#24314;&#31435;&#22312;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#33021;&#22815;&#22312;&#26102;&#31354;&#32500;&#24230;&#19978;&#23545;&#31038;&#20250;&#38382;&#39064;&#36827;&#34892;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#20998;&#26512;&#19982;&#35268;&#23450;&#12290;ASOS&#37319;&#29992;&#21487;&#25193;&#23637;&#30340;&#31038;&#20250;&#35821;&#20041;&#30340;&#36229;&#22270;&#65292;&#20197;&#20840;&#38754;&#21644;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#34920;&#31034;&#31038;&#20250;&#21160;&#24577;&#12290;&#23427;&#36824;&#32467;&#21512;&#20102;&#29992;&#20110;&#26631;&#20934;&#21270;&#36229;&#22270;&#25805;&#20316;&#30340;&#36890;&#29992;&#21327;&#35758;&#21644;&#31526;&#21495;&#21270;&#30340;&#28151;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the escalating intricacy and multifaceted nature of contemporary social systems, manually generating solutions to address pertinent social issues has become a formidable task. In response to this challenge, the rapid development of artificial intelligence has spurred the exploration of computational methodologies aimed at automatically generating solutions. However, current methods for auto-generation of solutions mainly concentrate on local social regulations that pertain to specific scenarios. Here, we report an automatic social operating system (ASOS) designed for general social solution generation, which is built upon agent-based models, enabling both global and local analyses and regulations of social problems across spatial and temporal dimensions. ASOS adopts a hypergraph with extensible social semantics for a comprehensive and structured representation of social dynamics. It also incorporates a generalized protocol for standardized hypergraph operations and a symbolic hyb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33539;&#24335;&#65292;&#22522;&#20110;&#22238;&#28335;&#21453;&#20107;&#23454;&#65292;&#36890;&#36807;&#22238;&#28335;&#24050;&#26377;&#36335;&#24452;&#65292;&#32780;&#19981;&#26159;&#24819;&#35937;&#22312;&#27861;&#24459;&#20445;&#25252;&#29305;&#24449;&#19978;&#36827;&#34892;&#30340;&#21487;&#24178;&#39044;&#30340;&#20551;&#35774;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2401.13935</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#21644;&#25937;&#27982;&#20013;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#26032;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
A New Paradigm for Counterfactual Reasoning in Fairness and Recourse. (arXiv:2401.13935v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33539;&#24335;&#65292;&#22522;&#20110;&#22238;&#28335;&#21453;&#20107;&#23454;&#65292;&#36890;&#36807;&#22238;&#28335;&#24050;&#26377;&#36335;&#24452;&#65292;&#32780;&#19981;&#26159;&#24819;&#35937;&#22312;&#27861;&#24459;&#20445;&#25252;&#29305;&#24449;&#19978;&#36827;&#34892;&#30340;&#21487;&#24178;&#39044;&#30340;&#20551;&#35774;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#26159;&#23457;&#35745;&#21644;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35768;&#22810;&#25216;&#26415;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#20256;&#32479;&#33539;&#24335;&#26159;&#24178;&#39044;&#21453;&#20107;&#23454;&#65292;&#21363;&#24819;&#35937;&#21644;&#27169;&#25311;&#20551;&#35774;&#24615;&#24178;&#39044;&#12290;&#22240;&#27492;&#65292;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#27861;&#24459;&#20445;&#25252;&#21644;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#29702;&#30340;&#20986;&#21457;&#28857;&#26159;&#23545;&#27861;&#24459;&#20445;&#25252;&#29305;&#24449;&#65288;&#22914;&#31181;&#26063;&#12289;&#24615;&#21035;&#12289;&#27531;&#30142;&#12289;&#24180;&#40836;&#31561;&#65289;&#36827;&#34892;&#20551;&#35774;&#24178;&#39044;&#12290;&#25105;&#20204;&#38382;&#30340;&#38382;&#39064;&#26159;&#65292;&#22914;&#26524;&#20320;&#30340;&#31181;&#26063;&#19981;&#21516;&#65292;&#20250;&#21457;&#29983;&#20160;&#20040;&#24773;&#20917;&#65311;&#36825;&#20010;&#33539;&#24335;&#30340;&#19968;&#20010;&#22266;&#26377;&#38480;&#21046;&#26159;&#65292;&#19968;&#20123;&#20154;&#21475;&#32479;&#35745;&#24178;&#39044;&#65288;&#27604;&#22914;&#31181;&#26063;&#24178;&#39044;&#65289;&#21487;&#33021;&#26080;&#27861;&#36716;&#21270;&#20026;&#24178;&#39044;&#21453;&#20107;&#23454;&#30340;&#24418;&#24335;&#20027;&#20041;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#28335;&#21453;&#20107;&#23454;&#30340;&#26032;&#33539;&#24335;&#65292;&#19982;&#24819;&#35937;&#22312;&#27861;&#24459;&#20445;&#25252;&#29305;&#24449;&#19978;&#36827;&#34892;&#30340;&#20551;&#35774;&#24178;&#39044;&#19981;&#21516;&#65292;&#25105;&#20204;&#24819;&#35937;&#21487;&#20197;&#27839;&#30528;&#24050;&#26377;&#30340;&#36335;&#24452;&#36827;&#34892;&#22238;&#28335;&#30340;&#21453;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactuals and counterfactual reasoning underpin numerous techniques for auditing and understanding artificial intelligence (AI) systems. The traditional paradigm for counterfactual reasoning in this literature is the interventional counterfactual, where hypothetical interventions are imagined and simulated. For this reason, the starting point for causal reasoning about legal protections and demographic data in AI is an imagined intervention on a legally-protected characteristic, such as ethnicity, race, gender, disability, age, etc. We ask, for example, what would have happened had your race been different? An inherent limitation of this paradigm is that some demographic interventions -- like interventions on race -- may not translate into the formalisms of interventional counterfactuals. In this work, we explore a new paradigm based instead on the backtracking counterfactual, where rather than imagine hypothetical interventions on legally-protected characteristics, we imagine al
&lt;/p&gt;</description></item><item><title>LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13920</link><description>&lt;p&gt;
LocMoE: &#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#20302;&#24320;&#38144;MoE
&lt;/p&gt;
&lt;p&gt;
LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13920
&lt;/p&gt;
&lt;p&gt;
LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65288;MoE&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#20998;&#24067;&#24335;&#21644;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#26377;&#25928;&#31232;&#30095;&#21644;&#25193;&#23637;&#27169;&#22411;&#65292;&#22240;&#27492;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;MoE&#30340;&#24615;&#33021;&#21463;&#21040;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20840;&#23545;&#20840;&#36890;&#20449;&#30340;&#39640;&#24310;&#36831;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#30001;&#20110;&#22823;&#37327;&#30340;&#19987;&#23478;&#23481;&#37327;&#23548;&#33268;&#30456;&#23545;&#20887;&#20313;&#30340;&#35745;&#31639;&#12290;&#36127;&#36733;&#19981;&#24179;&#34913;&#21487;&#33021;&#26159;&#30001;&#20110;&#29616;&#26377;&#36335;&#30001;&#31574;&#30053;&#22987;&#32456;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#30340;&#19987;&#23478;&#23548;&#33268;&#30340;&#12290;&#20840;&#23545;&#20840;&#36807;&#31243;&#20013;&#39057;&#32321;&#30340;&#33410;&#28857;&#38388;&#36890;&#20449;&#20063;&#26174;&#33879;&#24310;&#38271;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#24615;&#33021;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#19987;&#23478;&#23481;&#37327;&#30340;&#26368;&#23567;&#38408;&#20540;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#30340;&#38376;&#25511;&#26435;&#37325;&#19982;&#20998;&#37197;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#26368;&#22823;&#35282;&#20559;&#24046;&#35745;&#31639;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We por
&lt;/p&gt;</description></item><item><title>WebVoyager&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;Web&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#20132;&#20114;&#26469;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13919</link><description>&lt;p&gt;
WebVoyager&#65306;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#30340;Web Agent
&lt;/p&gt;
&lt;p&gt;
WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13919
&lt;/p&gt;
&lt;p&gt;
WebVoyager&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;Web&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#20132;&#20114;&#26469;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#24341;&#39046;&#20102;&#19968;&#20010;&#30001;&#30495;&#23454;&#19990;&#30028;&#20013;&#33258;&#20027;&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#25152;&#26631;&#24535;&#30340;&#26032;&#26102;&#20195;&#65292;&#25512;&#21160;&#20102;&#22522;&#20110;&#32593;&#32476;&#30340;&#39640;&#32423;&#20195;&#29702;&#30340;&#21019;&#26032;&#12290;&#29616;&#26377;&#30340;&#32593;&#32476;&#20195;&#29702;&#36890;&#24120;&#21482;&#22788;&#29702;&#19968;&#20010;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#19988;&#20165;&#22312;&#31616;&#21270;&#30340;&#32593;&#32476;&#27169;&#25311;&#22120;&#25110;&#38745;&#24577;&#30340;&#32593;&#32476;&#24555;&#29031;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WebVoyager&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;Web&#20195;&#29702;&#65292;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#36827;&#34892;&#20132;&#20114;&#65292;&#33021;&#22815;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#24335;Web&#20195;&#29702;&#20219;&#21153;&#30340;&#33258;&#21160;&#35780;&#20272;&#25361;&#25112;&#65292;&#21033;&#29992;&#20102;GPT-4V&#30340;&#24378;&#22823;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;15&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#32593;&#31449;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;WebVoyager&#23454;&#29616;&#20102;55.7&#65285;&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#26174;&#33879;&#22320;.....
&lt;/p&gt;
&lt;p&gt;
The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#32858;&#31867;&#21644;&#20998;&#24067;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#31163;&#25955;&#20998;&#24067;&#32858;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#26368;&#20248;&#20256;&#36755;&#26500;&#24314;&#30456;&#20284;&#24230;&#30697;&#38453;&#65292;&#25105;&#20204;&#22312;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.13913</link><description>&lt;p&gt;
&#31163;&#25955;&#20998;&#24067;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Spectral Clustering for Discrete Distributions. (arXiv:2401.13913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#32858;&#31867;&#21644;&#20998;&#24067;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#31163;&#25955;&#20998;&#24067;&#32858;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#26368;&#20248;&#20256;&#36755;&#26500;&#24314;&#30456;&#20284;&#24230;&#30697;&#38453;&#65292;&#25105;&#20204;&#22312;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#20998;&#24067;&#32858;&#31867;&#65288;D2C&#65289;&#36890;&#24120;&#36890;&#36807;Wasserstein&#36136;&#24515;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#20551;&#35774;&#19979;&#24037;&#20316;&#65292;&#21363;&#32858;&#31867;&#21487;&#20197;&#36890;&#36807;&#36136;&#24515;&#24456;&#22909;&#22320;&#34920;&#31034;&#65292;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#35889;&#32858;&#31867;&#21644;&#20998;&#24067;&#30456;&#20284;&#24230;&#24230;&#37327;&#65288;&#20363;&#22914;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;Wasserstein&#36317;&#31163;&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;D2C&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32447;&#24615;&#26368;&#20248;&#20256;&#36755;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#39640;&#25928;&#22320;&#26500;&#24314;&#30456;&#20284;&#24230;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20445;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#32858;&#31867;&#20998;&#24067;&#26041;&#38754;&#30340;&#25104;&#21151;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22823;&#22823;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete distribution clustering (D2C) was often solved by Wasserstein barycenter methods. These methods are under a common assumption that clusters can be well represented by barycenters, which may not hold in many real applications. In this work, we propose a simple yet effective framework based on spectral clustering and distribution affinity measures (e.g., maximum mean discrepancy and Wasserstein distance) for D2C. To improve the scalability, we propose to use linear optimal transport to construct affinity matrices efficiently on large datasets. We provide theoretical guarantees for the success of the proposed methods in clustering distributions. Experiments on synthetic and real data show that our methods outperform the baselines largely in terms of both clustering accuracy and computational efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23618;&#27425;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;(UHiSR)&#65292;&#36890;&#36807;&#32467;&#21512;&#23618;&#27425;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#22238;&#24402;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#21462;&#21270;&#23398;&#30452;&#35266;&#30340;&#26497;&#24615;&#25351;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#23558;&#20998;&#23376;&#32467;&#26500;&#19982;&#33394;&#35889;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#30340;&#21487;&#35299;&#37322;&#26041;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.13904</link><description>&lt;p&gt;
&#36171;&#20104;&#26426;&#22120;&#20687;&#21270;&#23398;&#23478;&#19968;&#26679;&#24605;&#32771;&#30340;&#33021;&#21147;&#65306;&#29992;&#23618;&#27425;&#31526;&#21495;&#22238;&#24402;&#25581;&#31034;&#20998;&#23376;&#32467;&#26500;&#26497;&#24615;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Empowering Machines to Think Like Chemists: Unveiling Molecular Structure-Polarity Relationships with Hierarchical Symbolic Regression. (arXiv:2401.13904v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23618;&#27425;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;(UHiSR)&#65292;&#36890;&#36807;&#32467;&#21512;&#23618;&#27425;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#22238;&#24402;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#21462;&#21270;&#23398;&#30452;&#35266;&#30340;&#26497;&#24615;&#25351;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#23558;&#20998;&#23376;&#32467;&#26500;&#19982;&#33394;&#35889;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#30340;&#21487;&#35299;&#37322;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34180;&#23618;&#33394;&#35889;&#27861;(TLC)&#26159;&#20998;&#23376;&#26497;&#24615;&#20998;&#26512;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#23545;&#20110;TLC&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#21033;&#29992;&#39640;&#32500;&#20998;&#23376;&#25351;&#32441;&#65292;&#35201;&#20040;&#21033;&#29992;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#29305;&#24449;&#24037;&#31243;&#65292;&#32463;&#24120;&#38754;&#20020;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#20004;&#38590;&#36873;&#25321;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26080;&#30417;&#30563;&#30340;&#23618;&#27425;&#31526;&#21495;&#22238;&#24402;(UHiSR)&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23618;&#27425;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#22238;&#24402;&#12290;UHiSR&#33258;&#21160;&#25552;&#21462;&#21270;&#23398;&#30452;&#35266;&#30340;&#26497;&#24615;&#25351;&#25968;&#65292;&#24182;&#21457;&#29616;&#23558;&#20998;&#23376;&#32467;&#26500;&#19982;&#33394;&#35889;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#30340;&#21487;&#35299;&#37322;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thin-layer chromatography (TLC) is a crucial technique in molecular polarity analysis. Despite its importance, the interpretability of predictive models for TLC, especially those driven by artificial intelligence, remains a challenge. Current approaches, utilizing either high-dimensional molecular fingerprints or domain-knowledge-driven feature engineering, often face a dilemma between expressiveness and interpretability. To bridge this gap, we introduce Unsupervised Hierarchical Symbolic Regression (UHiSR), combining hierarchical neural networks and symbolic regression. UHiSR automatically distills chemical-intuitive polarity indices, and discovers interpretable equations that link molecular structure to chromatographic behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;&#29366;&#24577;&#36716;&#31227;&#31995;&#32479;&#30340;&#21160;&#24577;&#35268;&#21010;&#25551;&#36848;&#35821;&#35328;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;&#32422;&#26463;&#35268;&#21010;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13883</link><description>&lt;p&gt;
&#39046;&#22495;&#26080;&#20851;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain-Independent Dynamic Programming. (arXiv:2401.13883v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;&#29366;&#24577;&#36716;&#31227;&#31995;&#32479;&#30340;&#21160;&#24577;&#35268;&#21010;&#25551;&#36848;&#35821;&#35328;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;&#32422;&#26463;&#35268;&#21010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#33539;&#20363;&#22914;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010; (MIP) &#21644;&#32422;&#26463;&#35268;&#21010; (CP) &#26088;&#22312;&#35299;&#32806;&#38382;&#39064;&#30340;&#24314;&#27169;&#21644;&#27714;&#35299;&#36807;&#31243;&#65292;&#36825;&#26159;&#22768;&#26126;&#24615;&#38382;&#39064;&#27714;&#35299;&#30340;&#8220;&#22307;&#26479;&#8221;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#21160;&#24577;&#35268;&#21010;&#65288;DIDP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010; (DP) &#30340;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;DP&#24182;&#19981;&#26032;&#40092;&#65292;&#20294;&#36890;&#24120;&#23427;&#34987;&#20316;&#20026;&#19968;&#31181;&#29305;&#23450;&#38382;&#39064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#35268;&#21010;&#25551;&#36848;&#35821;&#35328; (DyPDL)&#65292;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#36716;&#31227;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#35821;&#35328;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;AI&#35268;&#21010;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;&#21487;&#20197;&#29992;&#26469;&#27714;&#35299;DyPDL&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19971;&#31181;DIDP&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#22312;&#24120;&#35265;&#30340;11&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#31867;&#21035;&#30340;&#22522;&#20934;&#23454;&#20363;&#19978;&#65292;&#23558;&#25105;&#20204;&#30340;DIDP&#27714;&#35299;&#22120;&#19982;&#21830;&#19994;MIP&#21644;CP&#27714;&#35299;&#22120;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#65288;&#20998;&#21035;&#27714;&#35299;MIP&#21644;CP&#27169;&#22411;&#65289;&#12290;&#32467;&#26524;&#26174;&#31034;DIDP&#22312;&#20061;&#20010;&#38382;&#39064;&#31867;&#21035;&#20013;&#20248;&#20110;MIP&#65292;&#20063;&#20248;&#20110;CP&#22312;&#20061;&#20010;&#38382;&#39064;&#31867;&#21035;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
For combinatorial optimization problems, model-based paradigms such as mixed-integer programming (MIP) and constraint programming (CP) aim to decouple modeling and solving a problem: the `holy grail' of declarative problem solving. We propose domain-independent dynamic programming (DIDP), a new model-based paradigm based on dynamic programming (DP). While DP is not new, it has typically been implemented as a problem-specific method. We introduce Dynamic Programming Description Language (DyPDL), a formalism to define DP models based on a state transition system, inspired by AI planning. We show that heuristic search algorithms can be used to solve DyPDL models and propose seven DIDP solvers. We experimentally compare our DIDP solvers with commercial MIP and CP solvers (solving MIP and CP models, respectively) on common benchmark instances of eleven combinatorial optimization problem classes. We show that DIDP outperforms MIP in nine problem classes, CP also in nine problem classes, and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21407;&#21017;&#21457;&#29616;&#21644;&#25351;&#23548;&#25552;&#21319;&#23398;&#29983;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;TPD&#26694;&#26550;&#27169;&#25311;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#36890;&#36807;&#29983;&#25104;&#38382;&#39064;&#35299;&#20915;&#25351;&#20196;&#21644;&#32416;&#27491;&#21407;&#21017;&#65292;&#20174;&#32780;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#30340;&#25351;&#23548;&#21644;&#33258;&#36523;&#30340;&#38169;&#35823;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.13849</link><description>&lt;p&gt;
TPD: &#36890;&#36807;&#21407;&#21017;&#21457;&#29616;&#21644;&#25351;&#23548;&#25552;&#21319;&#23398;&#29983;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance. (arXiv:2401.13849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13849
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21407;&#21017;&#21457;&#29616;&#21644;&#25351;&#23548;&#25552;&#21319;&#23398;&#29983;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;TPD&#26694;&#26550;&#27169;&#25311;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#36890;&#36807;&#29983;&#25104;&#38382;&#39064;&#35299;&#20915;&#25351;&#20196;&#21644;&#32416;&#27491;&#21407;&#21017;&#65292;&#20174;&#32780;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#30340;&#25351;&#23548;&#21644;&#33258;&#36523;&#30340;&#38169;&#35823;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26368;&#36817;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#24448;&#24448;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#33021;&#21147;&#20174;&#36739;&#22823;&#30340;&#27169;&#22411;&#36716;&#31227;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#24494;&#35843;&#25968;&#25454;&#25110;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19982;&#20248;&#31168;&#30340;&#25945;&#24072;LLM&#36827;&#34892;&#25345;&#32493;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#24072;&#29983;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;&#36890;&#36807;&#21407;&#21017;&#21457;&#29616;&#25945;&#23398;&#8221;(TPD)&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#21463;&#20154;&#31867;&#23398;&#20064;&#26426;&#21046;&#21551;&#21457;&#65292;TPD&#27169;&#20223;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#37319;&#29992;&#22522;&#20110;&#21407;&#21017;&#30340;&#26041;&#27861;&#12290;&#25945;&#24072;LLM&#29983;&#25104;&#38382;&#39064;&#35299;&#20915;&#25351;&#20196;&#21644;&#32416;&#27491;&#21407;&#21017;&#65292;&#22522;&#20110;&#23398;&#29983;LLM&#30340;&#38169;&#35823;&#12290;&#36825;&#20123;&#21407;&#21017;&#25351;&#23548;&#25351;&#20196;&#30340;&#23436;&#21892;&#21644;&#20174;&#39564;&#35777;&#38598;&#20013;&#36873;&#25321;&#26377;&#25945;&#32946;&#24847;&#20041;&#30340;&#31034;&#20363;&#12290;&#36825;&#20351;&#24471;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#20174;&#25945;&#24072;&#30340;&#25351;&#23548;&#21644;&#33258;&#24049;&#30340;&#38169;&#35823;&#20013;&#23398;&#20064;&#12290;&#19968;&#26086;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently showcased remarkable reasoning abilities. However, larger models often surpass their smaller counterparts in reasoning tasks, posing the challenge of effectively transferring these capabilities from larger models. Existing approaches heavily rely on extensive fine-tuning data or continuous interactions with a superior teacher LLM during inference. We introduce a principle-based teacher-student framework called ``Teaching via Principle Discovery'' (TPD) to address these limitations. Inspired by human learning mechanisms, TPD mimics the interaction between a teacher and a student using a principle-based approach. The teacher LLM generates problem-solving instructions and corrective principles based on the student LLM's errors. These principles guide the refinement of instructions and the selection of instructive examples from a validation set. This enables the student model to learn from both the teacher's guidance and its own mistakes. Once the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;V2X&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#27979;&#37327;&#21644;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;V2V&#36890;&#20449;&#21521;&#20854;&#20182;&#36710;&#36742;&#25552;&#20379;&#23454;&#26102;&#25968;&#25454;&#65292;&#21516;&#26102;&#36890;&#36807;V2N&#38142;&#36335;&#19978;&#30340;FL&#26041;&#26696;&#21019;&#24314;&#20132;&#36890;&#32593;&#32476;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.13848</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;V2X&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#27979;&#37327;&#21644;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A V2X-based Privacy Preserving Federated Measuring and Learning System. (arXiv:2401.13848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;V2X&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#27979;&#37327;&#21644;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;V2V&#36890;&#20449;&#21521;&#20854;&#20182;&#36710;&#36742;&#25552;&#20379;&#23454;&#26102;&#25968;&#25454;&#65292;&#21516;&#26102;&#36890;&#36807;V2N&#38142;&#36335;&#19978;&#30340;FL&#26041;&#26696;&#21019;&#24314;&#20132;&#36890;&#32593;&#32476;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#23558;&#20351;&#29992;&#21508;&#31181;&#20256;&#24863;&#22120;&#29983;&#25104;&#22823;&#37327;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#65292;&#36824;&#21487;&#20197;&#24110;&#21161;&#20854;&#20182;&#36710;&#36742;&#25110;&#22522;&#30784;&#35774;&#26045;&#36827;&#34892;&#23454;&#26102;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#36710;&#36742;&#38656;&#35201;&#36890;&#36807;&#36710;&#36742;&#21040;&#19968;&#20999;(V2X)&#25216;&#26415;&#26469;&#20132;&#25442;&#27979;&#37327;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#36947;&#36335;&#32593;&#32476;&#30340;&#29366;&#24577;&#21487;&#33021;&#20063;&#20250;&#26377;&#30410;&#22788;&#12290;&#36890;&#36807;&#36825;&#31181;&#39044;&#27979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20943;&#36731;&#36947;&#36335;&#25317;&#22581;&#12289;&#24179;&#34913;&#20572;&#36710;&#22330;&#20351;&#29992;&#24773;&#20917;&#25110;&#20248;&#21270;&#20132;&#36890;&#27969;&#21160;&#12290;&#36825;&#23558;&#38477;&#20302;&#36816;&#36755;&#25104;&#26412;&#65292;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#27979;&#37327;&#21644;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#36710;&#36742;&#21040;&#36710;&#36742;(V2V)&#36890;&#20449;&#21521;&#20854;&#20182;&#36710;&#36742;&#25552;&#20379;&#23454;&#26102;&#25968;&#25454;&#65292;&#21516;&#26102;&#36890;&#36807;&#36710;&#36742;&#21040;&#32593;&#32476;(V2N)&#38142;&#36335;&#19978;&#30340;&#32852;&#21512;&#23398;&#20064;(FL)&#26041;&#26696;&#21019;&#24314;&#20132;&#36890;&#32593;&#32476;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#30001;&#20110;&#23578;&#26080;&#30495;&#23454;&#19990;&#30028;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#25968;&#25454;&#65292;
&lt;/p&gt;
&lt;p&gt;
Future autonomous vehicles (AVs) will use a variety of sensors that generate a vast amount of data. Naturally, this data not only serves self-driving algorithms; but can also assist other vehicles or the infrastructure in real-time decision-making. Consequently, vehicles shall exchange their measurement data over Vehicle-to-Everything (V2X) technologies. Moreover, predicting the state of the road network might be beneficial too. With such a prediction, we might mitigate road congestion, balance parking lot usage, or optimize the traffic flow. That would decrease transportation costs as well as reduce its environmental impact.  In this paper, we propose a federated measurement and learning system that provides real-time data to fellow vehicles over Vehicle-to-Vehicle (V2V) communication while also operating a federated learning (FL) scheme over the Vehicle-to-Network (V2N) link to create a predictive model of the transportation network. As we are yet to have real-world AV data, we model
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#36798;&#32622;&#20449;&#24230;&#26041;&#38754;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#40664;&#35748;&#35299;&#37322;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13835</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#27169;&#22411;&#21644;&#20154;&#31867;&#32622;&#20449;&#24230;&#20043;&#38388;&#30340;&#26657;&#20934;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
The Calibration Gap between Model and Human Confidence in Large Language Models. (arXiv:2401.13835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#36798;&#32622;&#20449;&#24230;&#26041;&#38754;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#40664;&#35748;&#35299;&#37322;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#22815;&#33719;&#24471;&#20154;&#31867;&#30340;&#20449;&#20219;&#65292;&#23427;&#20204;&#38656;&#35201;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#23454;&#29616;&#33391;&#22909;&#30340;&#26657;&#20934;&#65292;&#21363;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#21644;&#20256;&#36798;&#23427;&#20204;&#30340;&#39044;&#27979;&#27491;&#30830;&#30340;&#21487;&#33021;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20851;&#27880;&#20102;LLM&#20869;&#37096;&#32622;&#20449;&#24230;&#35780;&#20272;&#30340;&#36136;&#37327;&#65292;&#20294;&#38382;&#39064;&#20173;&#28982;&#26159;LLM&#33021;&#22815;&#22914;&#20309;&#23558;&#36825;&#31181;&#20869;&#37096;&#27169;&#22411;&#32622;&#20449;&#24230;&#20256;&#36798;&#32473;&#20154;&#31867;&#29992;&#25143;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#31867;&#23545;LLM&#21709;&#24212;&#30340;&#22806;&#37096;&#32622;&#20449;&#24230;&#19982;&#27169;&#22411;&#20869;&#37096;&#32622;&#20449;&#24230;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#28041;&#21450;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#26816;&#26597;&#20102;&#20154;&#31867;&#29992;&#25143;&#35782;&#21035;LLM&#36755;&#20986;&#21487;&#20449;&#24230;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#20998;&#20026;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#35780;&#20272;&#29992;&#25143;&#23545;&#30495;&#23454;LLM&#32622;&#20449;&#24230;&#30340;&#24863;&#30693;&#21644;&#65288;2&#65289;&#35843;&#26597;&#20010;&#24615;&#21270;&#35299;&#37322;&#23545;&#35813;&#24863;&#30693;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#30340;&#40664;&#35748;&#35299;&#37322;&#24448;&#24448;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20462;&#25913;&#35299;&#37322;&#30340;&#26041;&#24335;&#21487;&#20197;&#20943;&#23567;&#36825;&#31181;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#35745;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#20107;&#20214;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#27969;&#37327;&#24773;&#20917;&#65292;&#24182;&#20248;&#21270;&#22810;&#20010;&#26080;&#20154;&#26426;&#30340;&#36712;&#36857;&#21644;&#35843;&#24230;&#31574;&#30053;&#65292;&#20197;&#38477;&#20302;&#20449;&#24687;&#26102;&#20195;&#12289;&#33410;&#30465;&#33021;&#37327;&#21644;&#25552;&#39640;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.13827</link><description>&lt;p&gt;
&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#29289;&#32852;&#32593;&#27169;&#22411;&#20013;&#25968;&#25454;&#19978;&#34892;&#30340;&#27969;&#37327;&#23398;&#20064;&#21644;&#20027;&#21160;&#26080;&#20154;&#26426;&#36712;&#36857;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink in Markovian IoT Models. (arXiv:2401.13827v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#35745;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#20107;&#20214;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#27969;&#37327;&#24773;&#20917;&#65292;&#24182;&#20248;&#21270;&#22810;&#20010;&#26080;&#20154;&#26426;&#30340;&#36712;&#36857;&#21644;&#35843;&#24230;&#31574;&#30053;&#65292;&#20197;&#38477;&#20302;&#20449;&#24687;&#26102;&#20195;&#12289;&#33410;&#30465;&#33021;&#37327;&#21644;&#25552;&#39640;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26102;&#20195;(AoI)&#29992;&#20110;&#34913;&#37327;&#25968;&#25454;&#30340;&#26032;&#40092;&#24230;&#12290;&#22312;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#65292;&#20256;&#32479;&#30340;&#36164;&#28304;&#31649;&#29702;&#26041;&#26696;&#20381;&#36182;&#20110;&#35774;&#22791;&#21644;&#22522;&#31449;(BS)&#20043;&#38388;&#30340;&#28040;&#24687;&#20132;&#25442;&#65292;&#23548;&#33268;AoI&#39640;&#12289;&#33021;&#37327;&#28040;&#32791;&#39640;&#19988;&#21487;&#38752;&#24615;&#20302;&#12290;&#20316;&#20026;&#39134;&#34892;&#22522;&#31449;&#30340;&#26080;&#20154;&#26426;(UAV)&#22312;&#20943;&#23567;AoI&#12289;&#33410;&#30465;&#33021;&#37327;&#21644;&#25552;&#39640;&#21534;&#21520;&#37327;&#26041;&#38754;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#26681;&#25454;&#39532;&#23572;&#31185;&#32500;&#20107;&#20214;&#20272;&#35745;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#27969;&#37327;&#21040;&#36798;&#24773;&#20917;&#12290;&#23398;&#20064;&#36807;&#31243;&#29992;&#20110;&#20248;&#21270;&#22810;&#20010;&#26080;&#20154;&#26426;&#30340;&#36712;&#36857;&#21644;&#35843;&#24230;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;BS&#39044;&#27979;&#35774;&#22791;&#26410;&#26469;&#30340;&#27969;&#37327;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#27969;&#37327;&#39044;&#27979;&#22120;&#65306;&#21069;&#21521;&#31639;&#27861;(FA)&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#27599;&#20010;&#26080;&#20154;&#26426;&#26368;&#20248;&#31574;&#30053;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#38024;&#23545;&#25552;&#20986;&#30340;&#20248;&#21270;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#20102;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The age of information (AoI) is used to measure the freshness of the data. In IoT networks, the traditional resource management schemes rely on a message exchange between the devices and the base station (BS) before communication which causes high AoI, high energy consumption, and low reliability. Unmanned aerial vehicles (UAVs) as flying BSs have many advantages in minimizing the AoI, energy-saving, and throughput improvement. In this paper, we present a novel learning-based framework that estimates the traffic arrival of IoT devices based on Markovian events. The learning proceeds to optimize the trajectory of multiple UAVs and their scheduling policy. First, the BS predicts the future traffic of the devices. We compare two traffic predictors: the forward algorithm (FA) and the long short-term memory (LSTM). Afterward, we propose a deep reinforcement learning (DRL) approach to optimize the optimal policy of each UAV. Finally, we manipulate the optimum reward function for the proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;Hugging Face&#19978;&#30340;&#25968;&#25454;&#38598;&#25991;&#26723;&#65292;&#25552;&#20379;&#20102;Hugging Face&#25968;&#25454;&#38598;&#29983;&#24577;&#31995;&#32479;&#30340;&#27010;&#35272;&#24182;&#27934;&#23519;&#21040;&#25968;&#25454;&#38598;&#25991;&#26723;&#23454;&#36341;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25968;&#25454;&#38598;&#21345;&#29255;&#23436;&#25104;&#29575;&#19982;&#25968;&#25454;&#38598;&#21463;&#27426;&#36814;&#31243;&#24230;&#23384;&#22312;&#26174;&#33879;&#24322;&#36136;&#24615;&#65292;&#20174;&#19994;&#32773;&#22312;&#25968;&#25454;&#38598;&#25551;&#36848;&#21644;&#25968;&#25454;&#38598;&#32467;&#26500;&#37096;&#20998;&#26356;&#20026;&#20851;&#27880;&#65292;&#32780;&#23545;&#20110;&#20351;&#29992;&#25968;&#25454;&#30340;&#32771;&#34385;&#37096;&#20998;&#36739;&#20026;&#24573;&#35270;&#12290;</title><link>http://arxiv.org/abs/2401.13822</link><description>&lt;p&gt;
&#22312;AI&#20013;&#27983;&#35272;&#25968;&#25454;&#38598;&#25991;&#26723;&#65306;Hugging Face&#25968;&#25454;&#38598;&#21345;&#29255;&#30340;&#22823;&#35268;&#27169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on Hugging Face. (arXiv:2401.13822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;Hugging Face&#19978;&#30340;&#25968;&#25454;&#38598;&#25991;&#26723;&#65292;&#25552;&#20379;&#20102;Hugging Face&#25968;&#25454;&#38598;&#29983;&#24577;&#31995;&#32479;&#30340;&#27010;&#35272;&#24182;&#27934;&#23519;&#21040;&#25968;&#25454;&#38598;&#25991;&#26723;&#23454;&#36341;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25968;&#25454;&#38598;&#21345;&#29255;&#23436;&#25104;&#29575;&#19982;&#25968;&#25454;&#38598;&#21463;&#27426;&#36814;&#31243;&#24230;&#23384;&#22312;&#26174;&#33879;&#24322;&#36136;&#24615;&#65292;&#20174;&#19994;&#32773;&#22312;&#25968;&#25454;&#38598;&#25551;&#36848;&#21644;&#25968;&#25454;&#38598;&#32467;&#26500;&#37096;&#20998;&#26356;&#20026;&#20851;&#27880;&#65292;&#32780;&#23545;&#20110;&#20351;&#29992;&#25968;&#25454;&#30340;&#32771;&#34385;&#37096;&#20998;&#36739;&#20026;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#19982;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#23494;&#20999;&#30456;&#20851;&#12290;&#34429;&#28982;&#25968;&#25454;&#25991;&#26723;&#34987;&#24191;&#27867;&#35748;&#20026;&#23545;&#20110;ML&#30340;&#21487;&#38752;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#36879;&#26126;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#23545;&#24403;&#21069;&#25968;&#25454;&#38598;&#25991;&#26723;&#23454;&#36341;&#30340;&#31995;&#32479;&#23454;&#35777;&#29702;&#35299;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20197;Hugging Face&#20026;&#20363;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20849;&#20139;&#21644;&#21327;&#20316;ML&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#26368;&#22823;&#24179;&#21488;&#12290;&#36890;&#36807;&#20998;&#26512;Hugging Face&#19978;&#30340;7433&#20010;&#25968;&#25454;&#38598;&#25991;&#26723;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#25552;&#20379;&#20102;Hugging Face&#25968;&#25454;&#38598;&#29983;&#24577;&#31995;&#32479;&#30340;&#27010;&#35272;&#21644;&#23545;&#25968;&#25454;&#38598;&#25991;&#26723;&#23454;&#36341;&#30340;&#27934;&#23519;&#65292;&#24471;&#20986;&#20102;5&#20010;&#20027;&#35201;&#21457;&#29616;&#65306;&#65288;1&#65289;&#25968;&#25454;&#38598;&#21345;&#29255;&#23436;&#25104;&#29575;&#26174;&#31034;&#20986;&#19982;&#25968;&#25454;&#38598;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#30456;&#20851;&#30340;&#26174;&#33879;&#24322;&#36136;&#24615;&#12290;&#65288;2&#65289;&#23545;&#25968;&#25454;&#38598;&#21345;&#29255;&#20013;&#30340;&#27599;&#20010;&#37096;&#20998;&#36827;&#34892;&#32454;&#33268;&#30340;&#32771;&#23519;&#34920;&#26126;&#65292;&#20174;&#19994;&#32773;&#20284;&#20046;&#26356;&#37325;&#35270;&#25968;&#25454;&#38598;&#25551;&#36848;&#21644;&#25968;&#25454;&#38598;&#32467;&#26500;&#37096;&#20998;&#65292;&#32780;&#23545;&#20110;&#20351;&#29992;&#25968;&#25454;&#30340;&#32771;&#34385;&#37096;&#20998;&#26356;&#21152;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in machine learning are closely tied to the creation of datasets. While data documentation is widely recognized as essential to the reliability, reproducibility, and transparency of ML, we lack a systematic empirical understanding of current dataset documentation practices. To shed light on this question, here we take Hugging Face -- one of the largest platforms for sharing and collaborating on ML models and datasets -- as a prominent case study. By analyzing all 7,433 dataset documentation on Hugging Face, our investigation provides an overview of the Hugging Face dataset ecosystem and insights into dataset documentation practices, yielding 5 main findings: (1) The dataset card completion rate shows marked heterogeneity correlated with dataset popularity. (2) A granular examination of each section within the dataset card reveals that the practitioners seem to prioritize Dataset Description and Dataset Structure sections, while the Considerations for Using the Data section rec
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.13802</link><description>&lt;p&gt;
&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13802
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#20195;&#30721;&#29983;&#25104;&#12290;LLMs&#20027;&#35201;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;/&#23569;&#26679;&#26412;&#33539;&#24335;&#20013;&#34987;&#29992;&#20110;&#25351;&#23548;&#27169;&#22411;&#23436;&#25104;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#65288;CCD&#65289;&#36825;&#19968;&#38750;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \textcolor{blac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#20351;&#29992;&#28151;&#21512;&#31574;&#30053;&#36827;&#34892;&#22810;&#30446;&#26631;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#23548;&#33322;&#20013;&#28041;&#21450;&#30340;&#39640;&#32423;&#25512;&#29702;&#12289;&#23616;&#37096;&#35268;&#21010;&#21644;&#31354;&#38388;&#34920;&#31034;&#31561;&#22797;&#26434;&#20219;&#21153;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#19979;&#24050;&#26377;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20294;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23578;&#26410;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.13800</link><description>&lt;p&gt;
&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#20351;&#29992;&#28151;&#21512;&#31574;&#30053;&#36827;&#34892;&#22810;&#30446;&#26631;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Multi-Object Navigation in real environments using hybrid policies. (arXiv:2401.13800v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#20351;&#29992;&#28151;&#21512;&#31574;&#30053;&#36827;&#34892;&#22810;&#30446;&#26631;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#23548;&#33322;&#20013;&#28041;&#21450;&#30340;&#39640;&#32423;&#25512;&#29702;&#12289;&#23616;&#37096;&#35268;&#21010;&#21644;&#31354;&#38388;&#34920;&#31034;&#31561;&#22797;&#26434;&#20219;&#21153;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#19979;&#24050;&#26377;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20294;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23578;&#26410;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23548;&#33322;&#38382;&#39064;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#36890;&#36807;SLAM&#21644;&#35268;&#21010;&#30340;&#32467;&#21512;&#26469;&#35299;&#20915;&#30340;&#12290;&#26368;&#36817;&#65292;&#38500;&#20102;&#33322;&#28857;&#35268;&#21010;&#65292;&#28041;&#21450;&#21040;(&#35270;&#35273;)&#39640;&#32423;&#25512;&#29702;&#30340;&#38382;&#39064;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#25506;&#32034;&#65292;&#24182;&#19988;&#20027;&#35201;&#36890;&#36807;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;RL&#12289;&#31163;&#32447;RL&#25110;&#27169;&#20223;&#23398;&#20064;&#26469;&#35299;&#20915;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#26234;&#33021;&#20307;&#23398;&#20064;&#21508;&#31181;&#25216;&#33021;&#65292;&#27604;&#22914;&#23616;&#37096;&#35268;&#21010;&#12289;&#26144;&#23556;&#23545;&#35937;&#21644;&#26597;&#35810;&#23398;&#20064;&#24471;&#21040;&#30340;&#31354;&#38388;&#34920;&#31034;&#12290;&#19982;&#33322;&#28857;&#35268;&#21010;&#31561;&#31616;&#21333;&#20219;&#21153;&#19981;&#21516;&#65292;&#23545;&#20110;&#36825;&#20123;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#24050;&#32463;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#20805;&#20998;&#30340;&#35780;&#20272;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23578;&#26410;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;&#12290;&#25105;&#20204;&#20197;&#20855;&#26377;&#21407;&#22987;&#34394;&#25311;&#22810;&#30446;&#26631;&#23548;&#33322;&#23545;&#35937;&#23454;&#29289;&#22797;&#21046;&#21697;&#30340;&#29289;&#29702;&#29615;&#22659;&#20026;&#30446;&#26631;&#65292;&#38024;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#30446;&#26631;&#23548;&#33322;&#20219;&#21153;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#23548;&#33322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Navigation has been classically solved in robotics through the combination of SLAM and planning. More recently, beyond waypoint planning, problems involving significant components of (visual) high-level reasoning have been explored in simulated environments, mostly addressed with large-scale machine learning, in particular RL, offline-RL or imitation learning. These methods require the agent to learn various skills like local planning, mapping objects and querying the learned spatial representations. In contrast to simpler tasks like waypoint planning (PointGoal), for these more complex tasks the current state-of-the-art models have been thoroughly evaluated in simulation but, to our best knowledge, not yet in real environments.  In this work we focus on sim2real transfer. We target the challenging Multi-Object Navigation (Multi-ON) task and port it to a physical environment containing real replicas of the originally virtual Multi-ON objects. We introduce a hybrid navigation method, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#65292;&#21363;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#32780;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#23548;&#33268;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#25104;&#31435;&#12290;</title><link>http://arxiv.org/abs/2401.13796</link><description>&lt;p&gt;
&#19981;&#35201;&#25353;&#25353;&#38062;&#65281;&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning. (arXiv:2401.13796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#65292;&#21363;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#32780;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#23548;&#33268;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20026;&#22810;&#20010;&#39046;&#22495;&#25552;&#20379;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;ML&#24037;&#20855;&#30340;&#26085;&#30410;&#21487;&#33719;&#24471;&#24615;&#65292;&#35768;&#22810;&#20174;&#19994;&#32773;&#32570;&#20047;&#28145;&#20837;&#30340;ML&#19987;&#19994;&#30693;&#35782;&#65292;&#37319;&#29992;&#20102;&#8220;&#25353;&#25353;&#38062;&#8221;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#32780;&#24573;&#35270;&#20102;&#24213;&#23618;&#31639;&#27861;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#20294;&#23427;&#24341;&#21457;&#20102;&#23545;&#32467;&#26524;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#24615;&#33021;&#35780;&#20272;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;ML&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#27844;&#38706;&#65292;&#20854;&#20013;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#20102;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#65292;&#29992;&#25143;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#30340;&#20048;&#35266;&#24615;&#33021;&#20272;&#35745;&#12290;&#35780;&#20272;&#24615;&#33021;&#19982;&#23454;&#38469;&#22312;&#26032;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#24046;&#24322;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#26412;&#25991;&#29305;&#21035;&#23558;ML&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#20998;&#20026;&#19981;&#21516;&#31867;&#21035;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a "push the button" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13782</link><description>&lt;p&gt;
&#20174;&#25512;&#29305;&#21040;&#24341;&#29992;&#65306;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21487;&#35265;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#34987;&#25509;&#21463;&#30340;&#35770;&#25991;&#25968;&#37327;&#36798;&#21040;&#25968;&#21315;&#31687;&#65292;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#33719;&#21462;&#21644;&#38405;&#35835;&#30740;&#31350;&#35770;&#25991;&#21464;&#24471;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21487;&#35265;&#24615;&#20013;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#20182;&#20204;&#20998;&#20139;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#25324;8000&#22810;&#31687;&#35770;&#25991;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;2018&#24180;12&#26376;&#33267;2023&#24180;10&#26376;&#30340;&#25512;&#29305;&#65292;&#20197;&#21450;&#22522;&#20110;&#20986;&#29256;&#24180;&#20221;&#12289;&#20250;&#35758;&#22320;&#28857;&#21644;&#25688;&#35201;&#20027;&#39064;&#36827;&#34892;1&#65306;1&#21305;&#37197;&#30340;&#23545;&#29031;&#32452;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#23398;&#26415;&#20132;&#27969;&#20013;&#30340;&#19981;&#26029;&#25193;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#24403;&#20170;&#25968;&#23383;&#21270;&#26102;&#20195;&#19981;&#26029;&#21457;&#23637;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital a
&lt;/p&gt;</description></item><item><title>AlphaMapleSAT&#26159;&#19968;&#31181;&#22522;&#20110;MCTS&#30340;Cube-and-Conquer SAT&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#25512;&#29702;&#39537;&#21160;&#30340;&#20808;&#34892;&#35745;&#31639;&#25216;&#26415;&#26469;&#39640;&#25928;&#35299;&#20915;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13770</link><description>&lt;p&gt;
AlphaMapleSAT&#65306;&#19968;&#31181;&#22522;&#20110;MCTS&#30340;Cube-and-Conquer SAT&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
AlphaMapleSAT: An MCTS-based Cube-and-Conquer SAT Solver for Hard Combinatorial Problems. (arXiv:2401.13770v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13770
&lt;/p&gt;
&lt;p&gt;
AlphaMapleSAT&#26159;&#19968;&#31181;&#22522;&#20110;MCTS&#30340;Cube-and-Conquer SAT&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#25512;&#29702;&#39537;&#21160;&#30340;&#20808;&#34892;&#35745;&#31639;&#25216;&#26415;&#26469;&#39640;&#25928;&#35299;&#20915;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AlphaMapleSAT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Monte Carlo Tree Search (MCTS)&#30340;Cube-and-Conquer (CnC) SAT&#27714;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#23613;&#31649;CnC&#27714;&#35299;&#22120;&#22312;&#35299;&#20915;&#21508;&#31181;&#22256;&#38590;&#30340;&#32452;&#21512;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22810;&#24180;&#26469;&#65292;CnC&#30340;&#20808;&#34892;&#35745;&#31639;&#25216;&#26415;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22823;&#21457;&#23637;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#24456;&#38590;&#25552;&#20986;&#26082;&#20302;&#25104;&#26412;&#21448;&#33021;&#26377;&#25928;&#22320;&#23558;&#36755;&#20837;&#20844;&#24335;&#20998;&#21106;&#20026;&#23376;&#20844;&#24335;&#30340;&#26032;&#22411;&#20998;&#21106;&#25216;&#26415;&#65292;&#20174;&#32780;&#20351;&#25972;&#20307;&#36816;&#34892;&#26102;&#38388;&#26368;&#23567;&#21270;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;CnC&#27714;&#35299;&#22120;&#65288;&#22914;March&#65289;&#20351;&#29992;&#30340;&#20808;&#34892;&#35745;&#31639;&#25216;&#26415;&#36890;&#36807;&#32422;&#26463;&#25628;&#32034;&#26368;&#20248;&#20998;&#21106;&#21464;&#37327;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#19968;&#31181;&#22522;&#20110;&#25512;&#29702;&#39537;&#21160;&#30340;MCTS&#20808;&#34892;&#35745;&#31639;&#25216;&#26415;&#65292;&#36890;&#36807;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#21551;&#21457;&#24335;&#25628;&#32034;&#26469;&#23547;&#25214;&#26377;&#25928;&#30340;&#20998;&#21106;&#65292;&#21516;&#26102;&#20351;&#35745;&#31639;&#25104;&#26412;&#20302;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23545;&#27604;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
This paper introduces AlphaMapleSAT, a novel Monte Carlo Tree Search (MCTS) based Cube-and-Conquer (CnC) SAT solving method aimed at efficiently solving challenging combinatorial problems. Despite the tremendous success of CnC solvers in solving a variety of hard combinatorial problems, the lookahead cubing techniques at the heart of CnC have not evolved much for many years. Part of the reason is the sheer difficulty of coming up with new cubing techniques that are both low-cost and effective in partitioning input formulas into sub-formulas, such that the overall runtime is minimized.  Lookahead cubing techniques used by current state-of-the-art CnC solvers, such as March, keep their cubing costs low by constraining the search for the optimal splitting variables. By contrast, our key innovation is a deductively-driven MCTS-based lookahead cubing technique, that performs a deeper heuristic search to find effective cubes, while keeping the cubing cost low. We perform an extensive compari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#24037;&#20855;&#21464;&#37327;&#65288;IV&#65289;&#27169;&#22411;&#20013;&#65292;&#20855;&#26377;&#20108;&#36827;&#21046;&#21709;&#24212;$Y$&#21644;&#20108;&#36827;&#21046;&#22788;&#29702;$X$&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#25509;&#21463;$K$&#20010;&#29366;&#24577;&#30340;&#20202;&#22120;$Z$&#26102;&#65292;&#20851;&#20110;ACE&#30028;&#38480;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.13758</link><description>&lt;p&gt;
&#24037;&#20855;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#20551;&#35774;&#21644;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Assumptions and Bounds in the Instrumental Variable Model. (arXiv:2401.13758v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#24037;&#20855;&#21464;&#37327;&#65288;IV&#65289;&#27169;&#22411;&#20013;&#65292;&#20855;&#26377;&#20108;&#36827;&#21046;&#21709;&#24212;$Y$&#21644;&#20108;&#36827;&#21046;&#22788;&#29702;$X$&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#25509;&#21463;$K$&#20010;&#29366;&#24577;&#30340;&#20202;&#22120;$Z$&#26102;&#65292;&#20851;&#20110;ACE&#30028;&#38480;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#20851;&#20110;&#20855;&#26377;&#20108;&#36827;&#21046;&#21709;&#24212;$Y$&#21644;&#20108;&#36827;&#21046;&#22788;&#29702;$X$&#30340;&#24037;&#20855;&#21464;&#37327;&#65288;IV&#65289;&#27169;&#22411;&#30340;&#32467;&#26524;&#35777;&#26126;&#65292;&#20294;&#26159;&#20202;&#22120;$Z$&#25509;&#21463;$K$&#20010;&#29366;&#24577;&#65292;&#36825;&#20123;&#29366;&#24577;&#26368;&#21021;&#26159;&#22312;Richardson&#65286;Robins&#65288;2014&#65289;&#30340;&#35770;&#25991;&#8220;ACE Bounds; SEMS with Equilibrium Conditions&#8221;&#20013;&#25552;&#20986;&#30340;&#65288;arXiv:1410.0470&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we give proofs for results relating to the Instrumental Variable (IV) model with binary response $Y$ and binary treatment $X$, but with an instrument $Z$ that takes $K$ states that were originally stated in Richardson &amp; Robins (2014), "ACE Bounds; SEMS with Equilibrium Conditions," arXiv:1410.0470.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#20197;Mothilal&#31561;&#20154;[2021]&#30340;&#24037;&#20316;&#20026;&#36215;&#28857;&#12290;&#35770;&#25991;&#25351;&#20986;Mothilal&#31561;&#20154;&#34429;&#28982;&#22768;&#31216;&#20351;&#29992;Halpern [2016]&#25552;&#20986;&#30340;&#35299;&#37322;&#23450;&#20041;&#65292;&#20294;&#23454;&#38469;&#19978;&#26377;&#19968;&#23450;&#30340;&#20559;&#24046;&#12290;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;Halpern&#30340;&#35299;&#37322;&#23450;&#20041;&#21487;&#20197;&#22788;&#29702;&#20854;&#20182;&#26041;&#27861;&#38590;&#20197;&#35299;&#20915;&#30340;&#32570;&#22833;&#21644;&#32597;&#35265;&#20107;&#20214;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13752</link><description>&lt;p&gt;
&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#22120;&#65288;arXiv&#65306;2401.13752v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Explaining Image Classifiers. (arXiv:2401.13752v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#20197;Mothilal&#31561;&#20154;[2021]&#30340;&#24037;&#20316;&#20026;&#36215;&#28857;&#12290;&#35770;&#25991;&#25351;&#20986;Mothilal&#31561;&#20154;&#34429;&#28982;&#22768;&#31216;&#20351;&#29992;Halpern [2016]&#25552;&#20986;&#30340;&#35299;&#37322;&#23450;&#20041;&#65292;&#20294;&#23454;&#38469;&#19978;&#26377;&#19968;&#23450;&#30340;&#20559;&#24046;&#12290;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;Halpern&#30340;&#35299;&#37322;&#23450;&#20041;&#21487;&#20197;&#22788;&#29702;&#20854;&#20182;&#26041;&#27861;&#38590;&#20197;&#35299;&#20915;&#30340;&#32570;&#22833;&#21644;&#32597;&#35265;&#20107;&#20214;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20197;&#33707;&#33922;&#25289;&#23572;&#31561;&#20154;[2021]&#65288;MMTS&#65289;&#30340;&#24037;&#20316;&#20316;&#20026;&#20986;&#21457;&#28857;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;MMTS&#22768;&#31216;&#20351;&#29992;Halpern [2016]&#25552;&#20986;&#30340;&#35299;&#37322;&#23450;&#20041;&#65292;&#20294;&#24182;&#38750;&#23436;&#20840;&#22914;&#27492;&#12290;&#31895;&#30053;&#22320;&#35828;&#65292;Halpern&#30340;&#23450;&#20041;&#21253;&#21547;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#20004;&#20010;&#26465;&#20214;&#12290;MMTS&#23558;&#24517;&#35201;&#24615;&#26465;&#27454;&#26367;&#25442;&#20026;&#25105;&#20204;&#23637;&#31034;&#30340;&#19968;&#20010;&#35201;&#27714;&#65292;&#24182;&#19988;&#26263;&#21547;&#20102;&#24517;&#35201;&#24615;&#12290;Halpern&#30340;&#23450;&#20041;&#36824;&#20801;&#35768;&#20195;&#29702;&#20154;&#38480;&#21046;&#32771;&#34385;&#30340;&#36873;&#39033;&#38598;&#12290;&#34429;&#28982;&#36825;&#20123;&#24046;&#24322;&#21487;&#33021;&#30475;&#20284;&#24494;&#23567;&#65292;&#20294;&#22914;&#25105;&#20204;&#25152;&#31034;&#65292;&#23427;&#20204;&#23545;&#35299;&#37322;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#65292;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#65292;Halpern&#30340;&#23450;&#20041;&#21487;&#20197;&#22788;&#29702;&#20854;&#20182;&#26041;&#27861;&#38590;&#20197;&#35299;&#20915;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#35299;&#37322;&#32570;&#22833;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#20998;&#31867;&#22120;&#36755;&#20986;&#8220;&#26080;&#32959;&#30244;&#8221;&#65289;&#21644;&#35299;&#37322;&#32597;&#35265;&#20107;&#20214;&#65288;&#27604;&#22914;&#32959;&#30244;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on explaining image classifiers, taking the work of Mothilal et al. [2021] (MMTS) as our point of departure. We observe that, although MMTS claim to be using the definition of explanation proposed by Halpern [2016], they do not quite do so. Roughly speaking, Halpern's definition has a necessity clause and a sufficiency clause. MMTS replace the necessity clause by a requirement that, as we show, implies it. Halpern's definition also allows agents to restrict the set of options considered. While these difference may seem minor, as we show, they can have a nontrivial impact on explanations. We also show that, essentially without change, Halpern's definition can handle two issues that have proved difficult for other approaches: explanations of absence (when, for example, an image classifier for tumors outputs "no tumor") and explanations of rare events (such as tumors).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40065;&#26834;&#24615;&#24314;&#27169;&#12290;&#30740;&#31350;&#21457;&#29616;&#38544;&#34255;&#23618;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#25512;&#24191;&#24615;&#33021;&#26377;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#27979;&#35797;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#31561;&#21442;&#25968;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13751</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#24314;&#27169;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Systematic Approach to Robustness Modelling for Deep Convolutional Neural Networks. (arXiv:2401.13751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40065;&#26834;&#24615;&#24314;&#27169;&#12290;&#30740;&#31350;&#21457;&#29616;&#38544;&#34255;&#23618;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#25512;&#24191;&#24615;&#33021;&#26377;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#27979;&#35797;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#31561;&#21442;&#25968;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26377;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#36235;&#21183;&#26159;&#20351;&#29992;&#20855;&#26377;&#36234;&#26469;&#36234;&#22810;&#21487;&#35843;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27169;&#22411;&#25439;&#22833;&#25110;&#21019;&#24314;&#26356;&#20855;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#30446;&#26631;&#36890;&#24120;&#30456;&#20114;&#30683;&#30462;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#26356;&#22823;&#27169;&#22411;&#33021;&#21542;&#25512;&#24191;&#21040;&#21463;&#25511;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#20043;&#22806;&#30340;&#25968;&#25454;&#30340;&#30097;&#38382;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ResNet&#27169;&#22411;&#20013;&#38544;&#34255;&#23618;&#30340;&#25968;&#37327;&#22312;MNIST&#12289;CIFAR10&#21644;CIFAR100&#25968;&#25454;&#38598;&#19978;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#21442;&#25968;&#65292;&#21253;&#25324;&#27169;&#22411;&#30340;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#65292;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available. The recent trend has been to use models with increasingly larger sets of tunable parameters to increase model accuracy, reduce model loss, or create more adversarially robust models -- goals that are often at odds with one another. In particular, recent theoretical work raises questions about the ability for even larger models to generalize to data outside of the controlled train and test sets. As such, we examine the role of the number of hidden layers in the ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a variety of parameters including the size of the model, the floating point precision, and the noise level of both the training data and the model output. To encapsulate the model's predictive power and computational cost, we provide a method that uses induced failures to model the probability of failure as a fun
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#21644;&#29992;&#25143;&#30340;&#32593;&#32476;&#27983;&#35272;&#25968;&#25454;&#20013;&#30340;&#25233;&#37057;&#25991;&#26412;&#65292;&#24182;&#32467;&#21512;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#29983;&#29702;&#20449;&#21495;&#25552;&#20379;&#38271;&#26399;&#36861;&#36394;&#21644;&#39044;&#21518;&#65292;&#26377;&#26395;&#25552;&#39640;&#26089;&#26399;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#33021;&#21147;&#65292;&#25512;&#21160;&#25972;&#20307;&#24515;&#29702;&#20581;&#24247;&#12290;</title><link>http://arxiv.org/abs/2401.13722</link><description>&lt;p&gt;
&#20027;&#21160;&#24773;&#32490;&#36861;&#36394;&#22120;&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#24773;&#32490;&#21644;&#24515;&#29702;&#29366;&#24577;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Proactive Emotion Tracker: AI-Driven Continuous Mood and Emotion Monitoring. (arXiv:2401.13722v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13722
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#21644;&#29992;&#25143;&#30340;&#32593;&#32476;&#27983;&#35272;&#25968;&#25454;&#20013;&#30340;&#25233;&#37057;&#25991;&#26412;&#65292;&#24182;&#32467;&#21512;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#29983;&#29702;&#20449;&#21495;&#25552;&#20379;&#38271;&#26399;&#36861;&#36394;&#21644;&#39044;&#21518;&#65292;&#26377;&#26395;&#25552;&#39640;&#26089;&#26399;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#33021;&#21147;&#65292;&#25512;&#21160;&#25972;&#20307;&#24515;&#29702;&#20581;&#24247;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39033;&#30446;&#26088;&#22312;&#35299;&#20915;&#24403;&#20170;&#25968;&#23383;&#26102;&#20195;&#19981;&#26029;&#22686;&#38271;&#30340;&#24515;&#29702;&#20581;&#24247;&#25361;&#25112;&#12290;&#23427;&#37319;&#29992;&#25913;&#36827;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#21644;&#29992;&#25143;&#30340;&#32593;&#32476;&#27983;&#35272;&#25968;&#25454;&#20013;&#30340;&#25233;&#37057;&#25991;&#26412;&#65292;&#23454;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;93&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#39033;&#30446;&#26088;&#22312;&#32467;&#21512;&#21487;&#31359;&#25140;&#35774;&#22791;&#65288;&#22914;&#26234;&#33021;&#25163;&#34920;&#21644;&#33041;&#30005;&#20256;&#24863;&#22120;&#65289;&#30340;&#29983;&#29702;&#20449;&#21495;&#65292;&#25552;&#20379;&#24773;&#32490;&#38556;&#30861;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#38271;&#26399;&#36861;&#36394;&#21644;&#39044;&#21518;&#12290;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#25233;&#37057;&#30340;&#26089;&#26399;&#26816;&#27979;&#24182;&#25913;&#21892;&#25972;&#20307;&#24515;&#29702;&#20581;&#24247;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research project aims to tackle the growing mental health challenges in today's digital age. It employs a modified pre-trained BERT model to detect depressive text within social media and users' web browsing data, achieving an impressive 93% test accuracy. Simultaneously, the project aims to incorporate physiological signals from wearable devices, such as smartwatches and EEG sensors, to provide long-term tracking and prognosis of mood disorders and emotional states. This comprehensive approach holds promise for enhancing early detection of depression and advancing overall mental health outcomes.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#30103;&#39046;&#22495;&#20013;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#19968;&#20123;&#38382;&#39064;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.13716</link><description>&lt;p&gt;
&#25105;&#33021;&#30456;&#20449;&#25105;&#30340;&#20551;&#25968;&#25454;&#21527;--&#19968;&#31181;&#32508;&#21512;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#29992;&#20110;&#21307;&#30103;&#39046;&#22495;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can I trust my fake data -- A comprehensive quality assessment framework for synthetic tabular data in healthcare. (arXiv:2401.13716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13716
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#32508;&#21512;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#30103;&#39046;&#22495;&#20013;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#19968;&#20123;&#38382;&#39064;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#23433;&#20840;&#37319;&#29992;&#21462;&#20915;&#20110;&#33719;&#24471;&#36275;&#22815;&#30340;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#39564;&#35777;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#21644;&#30417;&#31649;&#35201;&#27714;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#34987;&#25552;&#20986;&#12290;&#21512;&#25104;&#25968;&#25454;&#26159;&#36890;&#36807;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#29983;&#25104;&#22120;&#26469;&#21019;&#24314;&#20855;&#26377;&#31867;&#20284;&#32479;&#35745;&#23646;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#19981;&#21516;&#20998;&#31867;&#26631;&#20934;&#30340;&#31454;&#20105;&#24615;&#25351;&#26631;&#24050;&#34987;&#25552;&#20986;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#24773;&#20917;&#12290;&#20248;&#21270;&#36136;&#37327;&#38656;&#35201;&#24179;&#34913;&#20351;&#25968;&#25454;&#36866;&#29992;&#20110;&#20351;&#29992;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#20294;&#29616;&#26377;&#26694;&#26550;&#20013;&#30041;&#19979;&#20102;&#19968;&#20123;&#30456;&#20851;&#30340;&#32500;&#24230;&#12290;&#25105;&#20204;&#23545;&#22312;&#34920;&#26684;&#21307;&#30103;&#25968;&#25454;&#33539;&#22260;&#20869;&#35780;&#20272;&#36136;&#37327;&#25351;&#26631;&#21644;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#22522;&#20110;&#27492;&#21644;&#22242;&#38431;&#30340;&#38598;&#20307;&#32463;&#39564;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#30340;&#36136;&#37327;&#20445;&#35777;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#19982;&#33655;&#20848;&#22269;&#23478;&#30284;&#30151;&#30331;&#35760;&#20013;&#24515;&#30340;&#23454;&#38469;&#26696;&#20363;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring safe adoption of AI tools in healthcare hinges on access to sufficient data for training, testing and validation. In response to privacy concerns and regulatory requirements, using synthetic data has been suggested. Synthetic data is created by training a generator on real data to produce a dataset with similar statistical properties. Competing metrics with differing taxonomies for quality evaluation have been suggested, resulting in a complex landscape. Optimising quality entails balancing considerations that make the data fit for use, yet relevant dimensions are left out of existing frameworks. We performed a comprehensive literature review on the use of quality evaluation metrics on SD within the scope of tabular healthcare data and SD made using deep generative methods. Based on this and the collective team experiences, we developed a conceptual framework for quality assurance. The applicability was benchmarked against a practical case from the Dutch National Cancer Regist
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;EMP&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21464;&#21270;&#22810;&#20010;&#23610;&#24230;&#21442;&#25968;&#26469;&#25506;&#32034;&#25968;&#25454;&#65292;&#24182;&#23558;&#25551;&#36848;&#31526;&#20989;&#25968;&#25972;&#21512;&#21040;&#20998;&#26512;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#30340;&#25968;&#25454;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.13713</link><description>&lt;p&gt;
EMP: &#26377;&#25928;&#30340;&#22810;&#32500;&#25345;&#20037;&#21270;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EMP: Effective Multidimensional Persistence for Graph Representation Learning. (arXiv:2401.13713v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;EMP&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21464;&#21270;&#22810;&#20010;&#23610;&#24230;&#21442;&#25968;&#26469;&#25506;&#32034;&#25968;&#25454;&#65292;&#24182;&#23558;&#25551;&#36848;&#31526;&#20989;&#25968;&#25972;&#21512;&#21040;&#20998;&#26512;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#30340;&#25968;&#25454;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;(TDA)&#22312;&#20174;&#27969;&#24418;&#23398;&#20064;&#21040;&#22270;&#20998;&#31867;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;TDA&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#26159;&#25345;&#20037;&#21270;&#21516;&#35843;(PH)&#65292;&#36890;&#36807;&#36319;&#36394;&#28508;&#22312;&#32467;&#26500;&#38543;&#23610;&#24230;&#21442;&#25968;&#21464;&#21270;&#30340;&#28436;&#21270;&#65292;&#20026;&#25968;&#25454;&#25552;&#20379;&#29420;&#29305;&#30340;&#25299;&#25169;&#29305;&#24449;&#12290;&#29616;&#26377;&#30340;PH&#24037;&#20855;&#20165;&#33021;&#36890;&#36807;&#21333;&#20010;&#36807;&#28388;&#21442;&#25968;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24773;&#20917;&#19979;&#38656;&#35201;&#32771;&#34385;&#22810;&#20010;&#30456;&#20851;&#21442;&#25968;&#20197;&#33719;&#21462;&#26356;&#35814;&#32454;&#30340;&#25968;&#25454;&#27934;&#23519;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26377;&#25928;&#30340;&#22810;&#32500;&#25345;&#20037;&#21270;(EMP)&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21516;&#26102;&#25913;&#21464;&#22810;&#20010;&#23610;&#24230;&#21442;&#25968;&#26469;&#25506;&#32034;&#25968;&#25454;&#12290;&#35813;&#26694;&#26550;&#23558;&#25551;&#36848;&#31526;&#20989;&#25968;&#25972;&#21512;&#21040;&#20998;&#26512;&#36807;&#31243;&#20013;&#65292;&#24471;&#21040;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#30340;&#25968;&#25454;&#25688;&#35201;&#12290;&#23427;&#26080;&#32541;&#22320;&#23558;&#24314;&#31435;&#30340;&#21333;&#19968;PH&#25688;&#35201;&#38598;&#25104;&#21040;&#22810;&#32500;&#23545;&#24212;&#29289;&#20013;&#65292;&#22914;EMP&#26223;&#35266;&#12289;&#36718;&#24275;&#32447;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological data analysis (TDA) is gaining prominence across a wide spectrum of machine learning tasks that spans from manifold learning to graph classification. A pivotal technique within TDA is persistent homology (PH), which furnishes an exclusive topological imprint of data by tracing the evolution of latent structures as a scale parameter changes. Present PH tools are confined to analyzing data through a single filter parameter. However, many scenarios necessitate the consideration of multiple relevant parameters to attain finer insights into the data. We address this issue by introducing the Effective Multidimensional Persistence (EMP) framework. This framework empowers the exploration of data by simultaneously varying multiple scale parameters. The framework integrates descriptor functions into the analysis process, yielding a highly expressive data summary. It seamlessly integrates established single PH summaries into multidimensional counterparts like EMP Landscapes, Silhouett
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21152;&#36895;&#21452;&#26354;&#32447;t-SNE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26497;&#22352;&#26631;&#22235;&#21449;&#26641;&#30340;&#21152;&#36895;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#36755;&#20837;&#25968;&#25454;&#26102;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13708</link><description>&lt;p&gt;
&#21152;&#36895;&#21452;&#26354;&#32447;t-SNE&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accelerating hyperbolic t-SNE. (arXiv:2401.13708v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21152;&#36895;&#21452;&#26354;&#32447;t-SNE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26497;&#22352;&#26631;&#22235;&#21449;&#26641;&#30340;&#21152;&#36895;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#36755;&#20837;&#25968;&#25454;&#26102;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#29702;&#35299;&#23618;&#27425;&#21270;&#25110;&#39640;&#32500;&#25968;&#25454;&#30340;&#32467;&#26500;&#26159;&#24517;&#35201;&#30340;&#12290;&#21452;&#26354;&#31354;&#38388;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#23884;&#20837;&#35745;&#31639;&#21644;&#20998;&#26512;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#38750;&#32447;&#24615;&#29305;&#24615;&#24456;&#36866;&#21512;&#22788;&#29702;&#26641;&#29366;&#25110;&#22270;&#24418;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#20063;&#34987;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#21487;&#35270;&#21270;&#65292;&#20854;&#20013;&#23427;&#20204;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#23884;&#20837;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23884;&#20837;&#21040;&#21452;&#26354;&#31354;&#38388;&#30340;&#38477;&#32500;&#26041;&#27861;&#37117;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#30340;&#35268;&#27169;&#25193;&#23637;&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#23884;&#20837;&#26159;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26041;&#26696;&#35745;&#31639;&#30340;&#65292;&#32780;&#27599;&#27425;&#36845;&#20195;&#30340;&#35745;&#31639;&#25104;&#26412;&#19982;&#36755;&#20837;&#35268;&#27169;&#30340;&#24179;&#26041;&#25104;&#27491;&#27604;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21452;&#26354;&#31354;&#38388;&#30340;&#38750;&#32447;&#24615;&#29305;&#24615;&#65292;&#27431;&#20960;&#37324;&#24503;&#21152;&#36895;&#32467;&#26500;&#19981;&#33021;&#30452;&#25509;&#36716;&#21270;&#20026;&#21452;&#26354;&#35774;&#32622;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#21452;&#26354;&#23884;&#20837;&#21152;&#36895;&#30340;&#32467;&#26500;&#65292;&#22522;&#20110;&#26497;&#22352;&#26631;&#22235;&#21449;&#26641;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;t-SNE&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#21152;&#36895;&#21452;&#26354;&#32447;t-SNE&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need to understand the structure of hierarchical or high-dimensional data is present in a variety of fields. Hyperbolic spaces have proven to be an important tool for embedding computations and analysis tasks as their non-linear nature lends itself well to tree or graph data. Subsequently, they have also been used in the visualization of high-dimensional data, where they exhibit increased embedding performance. However, none of the existing dimensionality reduction methods for embedding into hyperbolic spaces scale well with the size of the input data. That is because the embeddings are computed via iterative optimization schemes and the computation cost of every iteration is quadratic in the size of the input. Furthermore, due to the non-linear nature of hyperbolic spaces, Euclidean acceleration structures cannot directly be translated to the hyperbolic setting. This paper introduces the first acceleration structure for hyperbolic embeddings, building upon a polar quadtree. We com
&lt;/p&gt;</description></item><item><title>Java&#20960;&#20309;&#19987;&#23478;&#65288;JGEX&#65289;&#22312;&#23398;&#26657;&#29615;&#22659;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#21450;&#20854;&#22312;&#35299;&#20915;&#25968;&#23398;&#31454;&#36187;&#39064;&#30446;&#20013;&#30340;&#25903;&#25345;&#12290;&#21516;&#26102;&#20063;&#35752;&#35770;&#20102;&#31243;&#24207;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13704</link><description>&lt;p&gt;
&#20351;&#29992;Java&#20960;&#20309;&#19987;&#23478;&#20316;&#20026;&#25968;&#23398;&#31454;&#36187;&#20934;&#22791;&#30340;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Using Java Geometry Expert as Guide in the Preparations for Math Contests. (arXiv:2401.13704v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13704
&lt;/p&gt;
&lt;p&gt;
Java&#20960;&#20309;&#19987;&#23478;&#65288;JGEX&#65289;&#22312;&#23398;&#26657;&#29615;&#22659;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#21450;&#20854;&#22312;&#35299;&#20915;&#25968;&#23398;&#31454;&#36187;&#39064;&#30446;&#20013;&#30340;&#25903;&#25345;&#12290;&#21516;&#26102;&#20063;&#35752;&#35770;&#20102;&#31243;&#24207;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Java&#20960;&#20309;&#19987;&#23478;&#65288;JGEX&#65289;&#22312;&#23398;&#26657;&#29615;&#22659;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#37325;&#28857;&#20851;&#27880;&#22885;&#22320;&#21033;&#30340;&#23398;&#26657;&#31995;&#32479;&#12290;JGEX&#22312;&#26576;&#20123;&#35838;&#22530;&#24773;&#22659;&#19979;&#21487;&#20197;&#25552;&#20379;&#24456;&#22823;&#30340;&#25903;&#25345;&#65292;&#23588;&#20854;&#26159;&#35299;&#20915;&#25968;&#23398;&#31454;&#36187;&#39064;&#30446;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35813;&#31243;&#24207;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give an insight into Java Geometry Expert (JGEX) in use in a school context, focusing on the Austrian school system. JGEX can offer great support in some classroom situations, especially for solving mathematical competition tasks. Also, we discuss some limitations of the program.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;GeoGebra Discovery&#36719;&#20214;&#24037;&#20855;&#65292;&#21033;&#29992;&#35745;&#31639;&#26426;&#35299;&#20915;&#20102;N'aboj 2023&#31454;&#36187;&#30340;&#19968;&#20123;&#20960;&#20309;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#38382;&#39064;&#36755;&#20837;&#30340;&#38590;&#24230;&#21644;&#26410;&#26469;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.13703</link><description>&lt;p&gt;
&#20351;&#29992;GeoGebra Discovery&#33258;&#21160;&#25512;&#29702;&#35299;&#20915;N'aboj 2023&#31454;&#36187;&#30340;&#19968;&#20123;&#20960;&#20309;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Some Geometry Problems of the N\'aboj 2023 Contest with Automated Deduction in GeoGebra Discovery. (arXiv:2401.13703v1 [math.HO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;GeoGebra Discovery&#36719;&#20214;&#24037;&#20855;&#65292;&#21033;&#29992;&#35745;&#31639;&#26426;&#35299;&#20915;&#20102;N'aboj 2023&#31454;&#36187;&#30340;&#19968;&#20123;&#20960;&#20309;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#38382;&#39064;&#36755;&#20837;&#30340;&#38590;&#24230;&#21644;&#26410;&#26469;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35745;&#31639;&#26426;&#35299;&#20915;&#20102;N'aboj 2023&#31454;&#36187;&#20013;&#30340;&#19968;&#20123;&#20960;&#20309;&#38382;&#39064;&#65292;&#20511;&#21161;&#35745;&#31639;&#36719;&#20214;&#24037;&#20855;GeoGebra Discovery&#36827;&#34892;&#35745;&#31639;&#12290;&#22312;&#27599;&#20010;&#26696;&#20363;&#20013;&#65292;&#35745;&#31639;&#37117;&#38656;&#35201;&#31526;&#21495;&#35745;&#31639;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#23558;&#38382;&#39064;&#36755;&#20837;&#35745;&#31639;&#26426;&#30340;&#38590;&#24230;&#65292;&#24182;&#21046;&#23450;&#20102;&#36827;&#19968;&#27493;&#30340;&#30446;&#26631;&#65292;&#20197;&#20351;&#36825;&#31867;&#31454;&#36187;&#38382;&#39064;&#22312;&#23558;&#26469;&#26356;&#26131;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we solve some of the geometry problems of the N\'aboj 2023 competition with the help of a computer, using examples that the software tool GeoGebra Discovery can calculate. In each case, the calculation requires symbolic computations. We analyze the difficulty of feeding the problem into the machine and set further goals to make the problems of this type of contests even more tractable in the future.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#29983;&#25104;&#30452;&#23610;&#21644;&#22278;&#35268;&#26500;&#36896;&#38382;&#39064;&#30340;&#21487;&#35835;&#19982;&#24418;&#24335;&#21270;&#27491;&#30830;&#24615;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21512;&#20316;&#23454;&#29616;&#12290;&#30446;&#26631;&#26159;&#20174;&#22522;&#26412;&#20844;&#29702;&#24418;&#24335;&#21270;&#35777;&#26126;&#25152;&#26377;&#39640;&#32423;&#24341;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.13700</link><description>&lt;p&gt;
&#23454;&#29616;&#33258;&#21160;&#21487;&#35835;&#30340;&#30452;&#23610;&#21644;&#22278;&#35268;&#26500;&#36896;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Towards Automated Readable Proofs of Ruler and Compass Constructions. (arXiv:2401.13700v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#29983;&#25104;&#30452;&#23610;&#21644;&#22278;&#35268;&#26500;&#36896;&#38382;&#39064;&#30340;&#21487;&#35835;&#19982;&#24418;&#24335;&#21270;&#27491;&#30830;&#24615;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21512;&#20316;&#23454;&#29616;&#12290;&#30446;&#26631;&#26159;&#20174;&#22522;&#26412;&#20844;&#29702;&#24418;&#24335;&#21270;&#35777;&#26126;&#25152;&#26377;&#39640;&#32423;&#24341;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24050;&#32463;&#26377;&#20960;&#20010;&#31995;&#32479;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#30452;&#23610;&#21644;&#22278;&#35268;&#26500;&#36896;&#38382;&#39064;&#30340;&#26500;&#36896;&#27493;&#39588;&#65292;&#20294;&#23427;&#20204;&#37117;&#27809;&#26377;&#25552;&#20379;&#21487;&#35835;&#30340;&#21512;&#25104;&#27491;&#30830;&#24615;&#35777;&#26126;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#19977;&#35282;&#24418;&#26500;&#36896;&#27714;&#35299;&#22120;ArgoTriCS&#22914;&#20309;&#33021;&#22815;&#19982;&#19968;&#38454;&#36923;&#36753;&#21644;&#36830;&#32493;&#36923;&#36753;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21512;&#20316;&#65292;&#29983;&#25104;&#26082;&#21487;&#34987;&#20154;&#31867;&#29702;&#35299;&#21448;&#21487;&#20197;&#30001;&#20132;&#20114;&#24335;&#23450;&#29702;&#35777;&#26126;&#22120;&#65288;&#22914;Coq&#25110;Isabelle/HOL&#65289;&#36827;&#34892;&#26816;&#26597;&#30340;&#26500;&#36896;&#27491;&#30830;&#24615;&#35777;&#26126;&#12290;&#36825;&#20123;&#35777;&#26126;&#30446;&#21069;&#20381;&#36182;&#20110;&#35768;&#22810;&#39640;&#32423;&#24341;&#29702;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#20960;&#20309;&#30340;&#22522;&#26412;&#20844;&#29702;&#20013;&#24418;&#24335;&#21270;&#35777;&#26126;&#25152;&#26377;&#36825;&#20123;&#24341;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although there are several systems that successfully generate construction steps for ruler and compass construction problems, none of them provides readable synthetic correctness proofs for generated constructions. In the present work, we demonstrate how our triangle construction solver ArgoTriCS can cooperate with automated theorem provers for first order logic and coherent logic so that it generates construction correctness proofs, that are both human-readable and formal (can be checked by interactive theorem provers such as Coq or Isabelle/HOL). These proofs currently rely on many high-level lemmas and our goal is to have them all formally shown from the basic axioms of geometry.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#20013;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#30340;&#24212;&#29992;&#12290;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#20316;&#20026;&#22810;&#21151;&#33021;&#12289;&#29983;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#27169;&#25311;&#32467;&#26524;&#24182;&#25351;&#23548;&#23454;&#38469;&#27835;&#30103;&#65292;&#20174;&#32780;&#25552;&#39640;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13699</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Generative AI-Driven Human Digital Twin in IoT-Healthcare: A Comprehensive Survey. (arXiv:2401.13699v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#20013;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#30340;&#24212;&#29992;&#12290;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#20316;&#20026;&#22810;&#21151;&#33021;&#12289;&#29983;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#27169;&#25311;&#32467;&#26524;&#24182;&#25351;&#23548;&#23454;&#38469;&#27835;&#30103;&#65292;&#20174;&#32780;&#25552;&#39640;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20154;&#31867;&#29983;&#27963;&#30340;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#20581;&#24247;&#25252;&#29702;&#26041;&#38754;&#21560;&#24341;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#21516;&#26102;&#65292;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#20840;&#38754;&#22320;&#25551;&#36848;&#20010;&#20307;&#20154;&#20307;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#22797;&#21046;&#65292;&#24182;&#23454;&#26102;&#21453;&#26144;&#20854;&#29289;&#29702;&#29366;&#20917;&#12290;&#33258;&#28982;&#22320;&#65292;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#34987;&#35774;&#24819;&#20026;&#36890;&#36807;&#20805;&#24403;&#22810;&#21151;&#33021;&#12289;&#29983;&#21160;&#30340;&#20154;&#31867;&#25968;&#23383;&#27979;&#35797;&#24179;&#21488;&#26469;&#22686;&#24378;&#29289;&#32852;&#32593;&#20581;&#24247;&#25252;&#29702;&#30340;&#33021;&#21147;&#65292;&#27169;&#25311;&#32467;&#26524;&#24182;&#25351;&#23548;&#23454;&#38469;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#25104;&#21151;&#24314;&#31435;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#38656;&#35201;&#39640;&#20445;&#30495;&#24230;&#30340;&#34394;&#25311;&#24314;&#27169;&#21644;&#24378;&#22823;&#30340;&#20449;&#24687;&#20132;&#20114;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#31232;&#32570;&#12289;&#20559;&#20506;&#21644;&#22122;&#22768;&#25968;&#25454;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#26368;&#36817;&#27969;&#34892;&#30340;&#19968;&#31181;&#21517;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#30340;&#25216;&#26415;&#21487;&#33021;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21033;&#29992;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#33258;&#21160;&#29983;&#25104;&#12289;&#25805;&#20316;&#21644;&#20462;&#25913;&#28176;&#21464;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet of things (IoT) can significantly enhance the quality of human life, specifically in healthcare, attracting extensive attentions to IoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as an innovative paradigm that can comprehensively characterize the replication of the individual human body in the digital world and reflect its physical status in real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the application of healthcare monitoring by acting as a versatile and vivid human digital testbed, simulating the outcomes and guiding the practical treatments. However, successfully establishing HDT requires high-fidelity virtual modeling and strong information interactions but possibly with scarce, biased and noisy data. Fortunately, a recent popular technology called generative artificial intelligence (GAI) may be a promising solution because it can leverage advanced AI algorithms to automatically create, manipulate, and modify val
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TRML&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#38543;&#26426;&#32570;&#22833;&#27169;&#24577;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#20581;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;TRML&#21033;&#29992;&#29983;&#25104;&#30340;&#34394;&#25311;&#27169;&#24577;&#26367;&#25442;&#32570;&#22833;&#30340;&#27169;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#40784;&#35821;&#20041;&#31354;&#38388;&#26469;&#35299;&#20915;&#27169;&#24577;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13697</link><description>&lt;p&gt;
&#36808;&#21521;&#20351;&#29992;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#31283;&#20581;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Toward Robust Multimodal Learning using Multimodal Foundational Models. (arXiv:2401.13697v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13697
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TRML&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#38543;&#26426;&#32570;&#22833;&#27169;&#24577;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#20581;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;TRML&#21033;&#29992;&#29983;&#25104;&#30340;&#34394;&#25311;&#27169;&#24577;&#26367;&#25442;&#32570;&#22833;&#30340;&#27169;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#40784;&#35821;&#20041;&#31354;&#38388;&#26469;&#35299;&#20915;&#27169;&#24577;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#39640;&#24230;&#20381;&#36182;&#20110;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#26159;&#23436;&#25972;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20551;&#35774;&#65292;&#28982;&#32780;&#36825;&#20010;&#20551;&#35774;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24448;&#24448;&#24456;&#38590;&#25104;&#31435;&#65306;&#22810;&#27169;&#24577;&#25968;&#25454;&#24448;&#24448;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26159;&#19981;&#23436;&#25972;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#23384;&#22312;&#38543;&#26426;&#32570;&#22833;&#27169;&#24577;&#30340;&#22330;&#26223;&#20013;&#65292;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#23558;&#20250;&#26356;&#21463;&#27426;&#36814;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;CLIP&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#21644;&#25991;&#26412;&#23545;&#30340;&#36328;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#65292;&#22312;&#20247;&#22810;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#20063;&#26080;&#27861;&#30452;&#25509;&#35299;&#20915;&#28041;&#21450;&#27169;&#24577;&#32570;&#22833;&#30340;&#22330;&#26223;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21363;TRML&#65288;Toward Robust Multimodal Learning using Multimodal Foundational Models&#65289;&#12290;TRML&#21033;&#29992;&#29983;&#25104;&#30340;&#34394;&#25311;&#27169;&#24577;&#26367;&#25442;&#32570;&#22833;&#30340;&#27169;&#24577;&#65292;&#24182;&#19988;&#23545;&#29983;&#25104;&#30340;&#27169;&#24577;&#21644;&#32570;&#22833;&#30340;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32570;&#22833;&#27169;&#24577;&#29983;&#25104;&#27169;&#22359;&#65292;&#21487;&#20197;&#29983;&#25104;&#32570;&#22833;&#30340;&#27169;&#24577;&#65292;&#28982;&#21518;&#36890;&#36807;&#29305;&#24449;&#23545;&#40784;&#27169;&#22359;&#26469;&#23398;&#20064;&#27169;&#24577;&#20043;&#38388;&#30340;&#23545;&#40784;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing multimodal sentiment analysis tasks are highly rely on the assumption that the training and test sets are complete multimodal data, while this assumption can be difficult to hold: the multimodal data are often incomplete in real-world scenarios. Therefore, a robust multimodal model in scenarios with randomly missing modalities is highly preferred. Recently, CLIP-based multimodal foundational models have demonstrated impressive performance on numerous multimodal tasks by learning the aligned cross-modal semantics of image and text pairs, but the multimodal foundational models are also unable to directly address scenarios involving modality absence. To alleviate this issue, we propose a simple and effective framework, namely TRML, Toward Robust Multimodal Learning using Multimodal Foundational Models. TRML employs generated virtual modalities to replace missing modalities, and aligns the semantic spaces between the generated and missing modalities. Concretely, we design a missin
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;&#35774;&#35745;&#26159;&#19968;&#39033;&#31867;&#20284;&#20110;&#25512;&#20986;&#20135;&#21697;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#21046;&#23450;&#26377;&#25928;&#30340;&#28216;&#25103;&#35268;&#21017;&#65292;&#24182;&#32771;&#34385;&#35299;&#20915;&#29616;&#23454;&#38382;&#39064;&#12289;&#25512;&#36827;&#31185;&#23398;&#25216;&#26415;&#12289;&#31185;&#23398;&#21457;&#29616;&#21644;&#25945;&#32946;&#20844;&#20247;&#31561;&#22810;&#20010;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.13693</link><description>&lt;p&gt;
&#25361;&#25112;&#35774;&#35745;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Challenge design roadmap. (arXiv:2401.13693v1 [cs.OH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13693
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#35774;&#35745;&#26159;&#19968;&#39033;&#31867;&#20284;&#20110;&#25512;&#20986;&#20135;&#21697;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#21046;&#23450;&#26377;&#25928;&#30340;&#28216;&#25103;&#35268;&#21017;&#65292;&#24182;&#32771;&#34385;&#35299;&#20915;&#29616;&#23454;&#38382;&#39064;&#12289;&#25512;&#36827;&#31185;&#23398;&#25216;&#26415;&#12289;&#31185;&#23398;&#21457;&#29616;&#21644;&#25945;&#32946;&#20844;&#20247;&#31561;&#22810;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#31181;&#28608;&#21169;&#21442;&#19982;&#32773;&#35299;&#20915;&#20005;&#32899;&#20219;&#21153;&#30340;&#28216;&#25103;&#12290;&#22240;&#27492;&#65292;&#31454;&#36187;&#32452;&#32455;&#32773;&#24517;&#39035;&#21046;&#23450;&#26377;&#25928;&#30340;&#28216;&#25103;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35268;&#21017;&#38500;&#20102;&#20351;&#28216;&#25103;&#23545;&#21442;&#19982;&#32773;&#26469;&#35828;&#36259;&#21619;&#21313;&#36275;&#20043;&#22806;&#36824;&#26377;&#22810;&#20010;&#30446;&#26631;&#12290;&#36825;&#20123;&#30446;&#26631;&#21487;&#33021;&#21253;&#25324;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#12289;&#25512;&#36827;&#31185;&#23398;&#25110;&#25216;&#26415;&#39046;&#22495;&#12289;&#36827;&#34892;&#31185;&#23398;&#21457;&#29616;&#21644;&#25945;&#32946;&#20844;&#20247;&#12290;&#22312;&#35768;&#22810;&#26041;&#38754;&#65292;&#21019;&#24314;&#19968;&#39033;&#25361;&#25112;&#31867;&#20284;&#20110;&#25512;&#20986;&#19968;&#27454;&#20135;&#21697;&#12290;&#23427;&#38656;&#35201;&#30456;&#21516;&#30340;&#20852;&#22859;&#21644;&#20005;&#26684;&#30340;&#27979;&#35797;&#65292;&#20854;&#30446;&#26631;&#26159;&#20197;&#21442;&#19982;&#32773;&#30340;&#24418;&#24335;&#21560;&#24341;&#8220;&#23458;&#25143;&#8221;&#12290;&#36825;&#20010;&#36807;&#31243;&#20174;&#19968;&#20010;&#22362;&#23454;&#30340;&#35745;&#21010;&#24320;&#22987;&#65292;&#27604;&#22914;&#19968;&#20010;&#31454;&#36187;&#25552;&#26696;&#65292;&#26368;&#32456;&#23558;&#25552;&#20132;&#32473;&#22269;&#38469;&#20250;&#35758;&#24182;&#25509;&#21463;&#21516;&#34892;&#35780;&#23457;&#12290;&#34429;&#28982;&#21516;&#34892;&#35780;&#23457;&#19981;&#33021;&#20445;&#35777;&#36136;&#37327;&#65292;&#20294;&#23427;&#30830;&#23454;&#36843;&#20351;&#32452;&#32455;&#32773;&#32771;&#34385;&#20182;&#20204;&#30340;&#25361;&#25112;&#30340;&#24433;&#21709;&#65292;&#35782;&#21035;&#28508;&#22312;&#30340;&#30095;&#28431;&#65292;&#24182;&#36890;&#24120;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;&#26412;&#31456;&#25552;&#20379;&#20102;&#25351;&#23548;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Challenges can be seen as a type of game that motivates participants to solve serious tasks. As a result, competition organizers must develop effective game rules. However, these rules have multiple objectives beyond making the game enjoyable for participants. These objectives may include solving real-world problems, advancing scientific or technical areas, making scientific discoveries, and educating the public. In many ways, creating a challenge is similar to launching a product. It requires the same level of excitement and rigorous testing, and the goal is to attract ''customers'' in the form of participants. The process begins with a solid plan, such as a competition proposal that will eventually be submitted to an international conference and subjected to peer review. Although peer review does not guarantee quality, it does force organizers to consider the impact of their challenge, identify potential oversights, and generally improve its quality. This chapter provides guidelines 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#23558;&#36807;&#31243;&#25366;&#25496;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.13677</link><description>&lt;p&gt;
&#29992;&#20110;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#36807;&#31243;&#25366;&#25496;&#65306;&#25361;&#25112;&#19982;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Process Mining for Unstructured Data: Challenges and Research Directions. (arXiv:2401.13677v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13677
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#23558;&#36807;&#31243;&#25366;&#25496;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#36807;&#31243;&#25366;&#25496;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21487;&#33021;&#26174;&#33879;&#25552;&#21319;&#23545;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#24120;&#35265;&#25968;&#25454;&#26684;&#24335;&#39046;&#22495;&#30340;&#26032;&#35265;&#35299;&#12290;&#35201;&#26377;&#25928;&#20998;&#26512;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#23545;&#20998;&#26512;&#32467;&#26524;&#20256;&#36798;&#20449;&#24515;&#65292;&#38656;&#35201;&#20811;&#26381;&#22810;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#35752;&#35770;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#24182;&#25551;&#36848;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#25991;&#31456;&#20026;&#26410;&#26469;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#30340;&#21512;&#20316;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of process mining for unstructured data might significantly elevate novel insights into disciplines where unstructured data is a common data format. To efficiently analyze unstructured data by process mining and to convey confidence into the analysis result, requires bridging multiple challenges. The purpose of this paper is to discuss these challenges, present initial solutions and describe future research directions. We hope that this article lays the foundations for future collaboration on this topic.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26234;&#33021;&#25968;&#25454;&#31649;&#29702;&#21644;&#27934;&#23519;&#21147;&#21487;&#20197;&#35299;&#20915;&#29616;&#20195;&#20892;&#19994;&#38754;&#20020;&#30340;&#22686;&#38271;&#38656;&#27714;&#21644;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#65292;&#36890;&#36807;&#25968;&#25454;&#21019;&#26032;&#21487;&#20197;&#25552;&#39640;&#20892;&#19994;&#29983;&#20135;&#21147;&#12289;&#21487;&#25345;&#32493;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13672</link><description>&lt;p&gt;
&#29992;&#26234;&#33021;&#25968;&#25454;&#31649;&#29702;&#21644;&#27934;&#23519;&#21147;&#25913;&#21464;&#20892;&#19994;
&lt;/p&gt;
&lt;p&gt;
Transforming Agriculture with Intelligent Data Management and Insights. (arXiv:2401.13672v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13672
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#25968;&#25454;&#31649;&#29702;&#21644;&#27934;&#23519;&#21147;&#21487;&#20197;&#35299;&#20915;&#29616;&#20195;&#20892;&#19994;&#38754;&#20020;&#30340;&#22686;&#38271;&#38656;&#27714;&#21644;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#65292;&#36890;&#36807;&#25968;&#25454;&#21019;&#26032;&#21487;&#20197;&#25552;&#39640;&#20892;&#19994;&#29983;&#20135;&#21147;&#12289;&#21487;&#25345;&#32493;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20892;&#19994;&#38754;&#20020;&#30528;&#22312;&#27668;&#20505;&#21464;&#21270;&#21644;&#33258;&#28982;&#36164;&#28304;&#20943;&#23569;&#30340;&#32422;&#26463;&#19979;&#65292;&#22312;&#20154;&#21475;&#22686;&#38271;&#30340;&#24773;&#20917;&#19979;&#28385;&#36275;&#31918;&#39135;&#12289;&#29123;&#26009;&#12289;&#39282;&#26009;&#21644;&#32420;&#32500;&#30340;&#22686;&#21152;&#38656;&#27714;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#24613;&#38656;&#25968;&#25454;&#21019;&#26032;&#26469;&#30830;&#20445;&#21644;&#25552;&#39640;&#20892;&#19994;&#29983;&#24577;&#31995;&#32479;&#30340;&#29983;&#20135;&#21147;&#12289;&#21487;&#25345;&#32493;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#38543;&#30528;&#21508;&#31181;&#20256;&#24863;&#22120;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#29992;&#12289;&#21487;&#36127;&#25285;&#24471;&#36215;&#12289;&#21487;&#38752;&#12289;&#31283;&#23450;&#65292;&#21487;&#20197;&#36827;&#34892;&#22810;&#26102;&#31354;&#23610;&#24230;&#12289;&#23454;&#26102;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#25910;&#38598;&#12289;&#25972;&#21512;&#21644;&#20998;&#26512;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#23545;&#25968;&#25454;&#23384;&#20648;&#21644;&#20998;&#26512;&#26500;&#25104;&#20102;&#24040;&#22823;&#25361;&#25112;&#65292;&#31185;&#23398;&#23478;&#20204;&#24120;&#35268;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#20998;&#26512;&#23454;&#36341;&#36234;&#26469;&#36234;&#20302;&#25928;&#12290;&#27492;&#22806;&#65292;&#26469;&#33258;&#19981;&#21516;&#23398;&#31185;&#30340;&#25968;&#25454;&#65292;&#22914;&#22522;&#22240;&#32452;&#23398;&#12289;&#34920;&#22411;&#23398;&#12289;&#29615;&#22659;&#23398;&#12289;&#20892;&#23398;&#21644;&#31038;&#20250;&#32463;&#27982;&#23398;&#65292;&#21487;&#33021;&#39640;&#24230;&#24322;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern agriculture faces grand challenges to meet increased demands for food, fuel, feed, and fiber with population growth under the constraints of climate change and dwindling natural resources. Data innovation is urgently required to secure and improve the productivity, sustainability, and resilience of our agroecosystems. As various sensors and Internet of Things (IoT) instrumentation become more available, affordable, reliable, and stable, it has become possible to conduct data collection, integration, and analysis at multiple temporal and spatial scales, in real-time, and with high resolutions. At the same time, the sheer amount of data poses a great challenge to data storage and analysis, and the \textit{de facto} data management and analysis practices adopted by scientists have become increasingly inefficient. Additionally, the data generated from different disciplines, such as genomics, phenomics, environment, agronomy, and socioeconomic, can be highly heterogeneous. That is, d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#36716;&#31227;&#24773;&#20917;&#19979;&#36807;&#20110;&#33258;&#20449;&#12290;&#36825;&#24378;&#35843;&#20102;&#23545;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#21487;&#38752;&#20272;&#35745;&#21450;&#20854;&#21521;&#26368;&#32456;&#29992;&#25143;&#20256;&#36798;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13657</link><description>&lt;p&gt;
&#24120;&#35265;&#30340;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#21487;&#38752;&#24615;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;
Inadequacy of common stochastic neural networks for reliable clinical decision support. (arXiv:2401.13657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#36716;&#31227;&#24773;&#20917;&#19979;&#36807;&#20110;&#33258;&#20449;&#12290;&#36825;&#24378;&#35843;&#20102;&#23545;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#21487;&#38752;&#20272;&#35745;&#21450;&#20854;&#21521;&#26368;&#32456;&#29992;&#25143;&#20256;&#36798;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20262;&#29702;&#21644;&#23433;&#20840;&#30456;&#20851;&#30340;&#20851;&#20999;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#23398;&#20915;&#31574;&#20173;&#28982;&#21463;&#38459;&#12290;&#23545;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26469;&#35828;&#65292;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#36716;&#31227;&#19979;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#12290;&#36825;&#31181;&#22312;&#22522;&#20110;&#35777;&#25454;&#30340;&#22330;&#26223;&#20043;&#22806;&#19981;&#24688;&#24403;&#30340;&#25512;&#29702;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#12290;&#36825;&#20984;&#26174;&#20102;&#23545;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#21487;&#38752;&#20272;&#35745;&#21450;&#20854;&#21521;&#26368;&#32456;&#29992;&#25143;&#20256;&#36798;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#34987;&#35465;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20854;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#20197;&#20174;MIMIC3&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;EHR&#30340;ICU&#20303;&#38498;&#30149;&#27515;&#29575;&#39044;&#27979;&#20026;&#20363;&#26469;&#36827;&#34892;&#20998;&#26512;&#12290;&#23545;&#20110;EHR&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20165;&#32534;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#32435;&#20837;&#24120;&#35265;&#26041;&#27861;&#26469;&#23454;&#29616;&#27169;&#22411;&#20989;&#25968;&#30340;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Widespread adoption of AI for medical decision making is still hindered due to ethical and safety-related concerns. For AI-based decision support systems in healthcare settings it is paramount to be reliable and trustworthy. Common deep learning approaches, however, have the tendency towards overconfidence under data shift. Such inappropriate extrapolation beyond evidence-based scenarios may have dire consequences. This highlights the importance of reliable estimation of local uncertainty and its communication to the end user. While stochastic neural networks have been heralded as a potential solution to these issues, this study investigates their actual reliability in clinical applications. We centered our analysis on the exemplary use case of mortality prediction for ICU hospitalizations using EHR from MIMIC3 study. For predictions on the EHR time series, Encoder-Only Transformer models were employed. Stochasticity of model functions was achieved by incorporating common methods such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#19981;&#36830;&#32493;&#30028;&#38754;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#22312;&#32500;&#24230;n = 2&#21644;n = 4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13652</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#32593;&#26684;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#30340;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors. (arXiv:2401.13652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#19981;&#36830;&#32493;&#30028;&#38754;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#33021;&#21147;&#65292;&#22312;&#32500;&#24230;n = 2&#21644;n = 4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#19981;&#36830;&#32493;&#20989;&#25968;&#30340;&#19981;&#36830;&#32493;&#30028;&#38754;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;GINNs&#65289;&#21644;&#31232;&#30095;&#32593;&#26684;&#26469;&#35299;&#20915;&#32500;&#24230;&#22823;&#20110;3&#30340;&#24773;&#20917;&#19979;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#12290;&#35757;&#32451;&#36807;&#30340;GINNs&#22312;&#31232;&#30095;&#32593;&#26684;&#19978;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#28857;&#65292;&#24182;&#21033;&#29992;&#26500;&#24314;&#22312;&#32593;&#26684;&#19978;&#30340;&#22270;&#32467;&#26500;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#19981;&#36830;&#32493;&#24615;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#36882;&#24402;&#31639;&#27861;&#29992;&#20110;&#19968;&#33324;&#30340;&#22522;&#20110;&#31232;&#30095;&#32593;&#26684;&#30340;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#26131;&#20110;&#24212;&#29992;&#24615;&#12290;&#22312;&#32500;&#24230;n=2&#21644;n=4&#30340;&#20989;&#25968;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;GINNs&#22312;&#26816;&#27979;&#19981;&#36830;&#32493;&#30028;&#38754;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#21644;&#40065;&#26834;&#27867;&#21270;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;GINNs&#20855;&#26377;&#21487;&#31227;&#26893;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#21508;&#31181;&#31639;&#27861;&#20013;&#24182;&#20849;&#20139;&#32473;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for detecting the discontinuity interfaces of a discontinuous function. This approach leverages Graph-Informed Neural Networks (GINNs) and sparse grids to address discontinuity detection also in domains of dimension larger than 3. GINNs, trained to identify troubled points on sparse grids, exploit graph structures built on the grids to achieve efficient and accurate discontinuity detection performances. We also introduce a recursive algorithm for general sparse grid-based detectors, characterized by convergence properties and easy applicability. Numerical experiments on functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust generalization of GINNs in detecting discontinuity interfaces. Notably, the trained GINNs offer portability and versatility, allowing integration into various algorithms and sharing among users.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21463;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#21457;&#29616;&#35299;&#37322;&#24448;&#24448;&#19981;&#33021;&#28385;&#36275;&#20182;&#20204;&#30340;&#20851;&#27880;&#28857;&#65292;&#23548;&#33268;&#23545;&#30417;&#31649;&#26694;&#26550;&#30340;&#29702;&#35299;&#21644;&#36981;&#23432;&#20135;&#29983;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;XAI&#21021;&#23398;&#32773;&#38382;&#39064;&#24211;&#65292;&#28085;&#30422;&#20102;&#23601;&#19994;&#39044;&#27979;&#21644;&#20581;&#24247;&#30417;&#27979;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.13324</link><description>&lt;p&gt;
&#26377;&#20851;&#31639;&#27861;&#20915;&#31574;&#30340;&#20449;&#24687;&#65306;&#25506;&#32034;&#21463;&#21040;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions. (arXiv:2401.13324v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21463;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#21457;&#29616;&#35299;&#37322;&#24448;&#24448;&#19981;&#33021;&#28385;&#36275;&#20182;&#20204;&#30340;&#20851;&#27880;&#28857;&#65292;&#23548;&#33268;&#23545;&#30417;&#31649;&#26694;&#26550;&#30340;&#29702;&#35299;&#21644;&#36981;&#23432;&#20135;&#29983;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;XAI&#21021;&#23398;&#32773;&#38382;&#39064;&#24211;&#65292;&#28085;&#30422;&#20102;&#23601;&#19994;&#39044;&#27979;&#21644;&#20581;&#24247;&#30417;&#27979;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#30340;&#35299;&#37322;&#24456;&#23569;&#28041;&#21450;&#21040;&#21463;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#36825;&#31181;&#20256;&#36798;&#20449;&#24687;&#19982;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#25152;&#20851;&#24515;&#30340;&#20449;&#24687;&#20043;&#38388;&#30340;&#24046;&#36317;&#21487;&#33021;&#38459;&#30861;&#23545;&#30417;&#31649;&#26694;&#26550;&#65288;&#22914;AI&#27861;&#26696;&#65289;&#30340;&#29702;&#35299;&#21644;&#36981;&#23432;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;XAI&#21021;&#23398;&#32773;&#38382;&#39064;&#24211;&#8221;&#65306;&#36825;&#26159;&#19968;&#20010;&#28085;&#30422;&#20004;&#20010;&#31639;&#27861;&#20915;&#31574;&#24212;&#29992;&#39046;&#22495;&#65288;&#23601;&#19994;&#39044;&#27979;&#21644;&#20581;&#24247;&#30417;&#27979;&#65289;&#20013;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30446;&#24405;&#65292;&#21253;&#25324;&#25968;&#25454;&#12289;&#31995;&#32479;&#32972;&#26223;&#12289;&#31995;&#32479;&#20351;&#29992;&#21644;&#31995;&#32479;&#35268;&#33539;&#31561;&#31867;&#21035;&#12290;&#20449;&#24687;&#38656;&#27714;&#26159;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#25910;&#38598;&#30340;&#65292;&#21442;&#19982;&#32773;&#26681;&#25454;&#33258;&#24049;&#30340;&#38382;&#39064;&#33719;&#24471;&#35299;&#37322;&#12290;&#21442;&#19982;&#32773;&#36824;&#25253;&#21578;&#20102;&#20182;&#20204;&#30340;&#29702;&#35299;&#21644;&#20915;&#31574;&#20449;&#24515;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#22312;&#25509;&#21463;&#35299;&#37322;&#21518;&#20449;&#24515;&#20542;&#21521;&#20110;&#22686;&#21152;&#65292;&#20294;&#21442;&#19982;&#32773;&#20063;&#38754;&#20020;&#30528;&#29702;&#35299;&#19978;&#30340;&#25361;&#25112;&#65292;&#22914;&#26080;&#27861;&#35299;&#37322;&#20026;&#20160;&#20040;&#33258;&#24049;&#30340;&#29702;&#35299;&#24863;&#35273;&#19981;&#23436;&#25972;&#12290;&#35299;&#37322;&#36824;&#23545;&#29702;&#35299;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanations of AI systems rarely address the information needs of people affected by algorithmic decision-making (ADM). This gap between conveyed information and information that matters to affected stakeholders can impede understanding and adherence to regulatory frameworks such as the AI Act. To address this gap, we present the "XAI Novice Question Bank": A catalog of affected stakeholders' information needs in two ADM use cases (employment prediction and health monitoring), covering the categories data, system context, system usage, and system specifications. Information needs were gathered in an interview study where participants received explanations in response to their inquiries. Participants further reported their understanding and decision confidence, showing that while confidence tended to increase after receiving explanations, participants also met understanding challenges, such as being unable to tell why their understanding felt incomplete. Explanations further influenced
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#12290;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#20943;&#36731;AI&#20195;&#29702;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2401.13138</link><description>&lt;p&gt;
&#23545;AI&#20195;&#29702;&#30340;&#21487;&#35265;&#24615;
&lt;/p&gt;
&lt;p&gt;
Visibility into AI Agents. (arXiv:2401.13138v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#12290;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#20943;&#36731;AI&#20195;&#29702;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21830;&#19994;&#12289;&#31185;&#23398;&#12289;&#25919;&#24220;&#21644;&#20010;&#20154;&#27963;&#21160;&#22996;&#25176;&#32473;&#20855;&#26377;&#26377;&#38480;&#30417;&#30563;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#31995;&#32479;&#65292;&#21487;&#33021;&#20250;&#21152;&#21095;&#29616;&#26377;&#30340;&#31038;&#20250;&#39118;&#38505;&#24182;&#24341;&#20837;&#26032;&#30340;&#39118;&#38505;&#12290;&#29702;&#35299;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#28041;&#21450;&#23545;&#29616;&#26377;&#27835;&#29702;&#32467;&#26500;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#20462;&#35746;&#21644;&#35843;&#25972;&#65292;&#24182;&#30830;&#20445;&#20851;&#38190;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#38382;&#36131;&#21046;&#12290;&#25105;&#20204;&#23558;AI&#20195;&#29702;&#30340;&#20351;&#29992;&#22320;&#28857;&#12289;&#21407;&#22240;&#12289;&#26041;&#24335;&#20197;&#21450;&#20351;&#29992;&#32773;&#31561;&#20449;&#24687;&#31216;&#20026;&#8220;&#21487;&#35265;&#24615;&#8221;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#19978;&#36848;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#12290;&#23545;&#20110;&#27599;&#19968;&#31181;&#25514;&#26045;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#65292;&#36825;&#20123;&#26041;&#24335;&#22312;&#20405;&#20837;&#24615;&#21644;&#20449;&#24687;&#24615;&#26041;&#38754;&#26377;&#25152;&#24046;&#24322;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increased delegation of commercial, scientific, governmental, and personal activities to AI agents -- systems capable of pursuing complex goals with limited supervision -- may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as \textbf{visibility}, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: \textbf{agent identifiers}, \textbf{real-time monitoring}, and \textbf{activity logging}. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for vario
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#26469;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;&#26102;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.12806</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Binary structured physics-informed neural networks for solving equations with rapidly changing solutions. (arXiv:2401.12806v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#26469;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;&#26102;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24050;&#25104;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;PDEs&#25551;&#36848;&#30340;&#29289;&#29702;&#20449;&#24687;&#23884;&#20837;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;PINNs&#34987;&#35757;&#32451;&#20026;&#26367;&#20195;&#27169;&#22411;&#65292;&#20197;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#32780;&#26080;&#38656;&#26631;&#31614;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;PINNs&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#35299;&#30340;&#26041;&#31243;&#26102;&#21487;&#33021;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#36825;&#20123;&#22256;&#38590;&#21253;&#25324;&#25910;&#25947;&#36895;&#24230;&#24930;&#12289;&#26131;&#38519;&#20837;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#35299;&#20915;&#31934;&#24230;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(BsPINN)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;(BsNN)&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;&#36890;&#36807;&#21033;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#65292;BsPINNs&#22312;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs), rooted in deep learning, have emerged as a promising approach for solving partial differential equations (PDEs). By embedding the physical information described by PDEs into feedforward neural networks, PINNs are trained as surrogate models to approximate solutions without the need for label data. Nevertheless, even though PINNs have shown remarkable performance, they can face difficulties, especially when dealing with equations featuring rapidly changing solutions. These difficulties encompass slow convergence, susceptibility to becoming trapped in local minima, and reduced solution accuracy. To address these issues, we propose a binary structured physics-informed neural network (BsPINN) framework, which employs binary structured neural network (BsNN) as the neural network component. By leveraging a binary structure that reduces inter-neuron connections compared to fully connected neural networks, BsPINNs excel in capturing the local features 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#26679;&#26412;&#27169;&#22359;&#32452;&#21512;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#36873;&#25321;&#12289;&#21152;&#26435;&#21644;&#32452;&#21512;&#21442;&#25968;&#27169;&#22359;&#30340;&#21508;&#31181;&#21464;&#21270;&#12290;&#20197;&#39046;&#22495;&#30693;&#35782;&#21644;&#36866;&#37197;&#22120;&#23618;&#20026;&#22330;&#26223;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#32479;&#19968;&#27010;&#24565;&#65292;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.12756</link><description>&lt;p&gt;
What the Weight?! &#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition. (arXiv:2401.12756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#26679;&#26412;&#27169;&#22359;&#32452;&#21512;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#36873;&#25321;&#12289;&#21152;&#26435;&#21644;&#32452;&#21512;&#21442;&#25968;&#27169;&#22359;&#30340;&#21508;&#31181;&#21464;&#21270;&#12290;&#20197;&#39046;&#22495;&#30693;&#35782;&#21644;&#36866;&#37197;&#22120;&#23618;&#20026;&#22330;&#26223;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#32479;&#19968;&#27010;&#24565;&#65292;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20013;&#25152;&#23553;&#35013;&#30340;&#30693;&#35782;&#26159;&#30830;&#23450;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#26368;&#32456;&#24615;&#33021;&#30340;&#26680;&#24515;&#22240;&#32032;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#35768;&#22810;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#23384;&#20648;&#21644;&#35843;&#25972;&#19981;&#21516;&#31867;&#22411;&#30693;&#35782;&#30340;&#26377;&#25928;&#26041;&#27861;&#19978;&#65292;&#20363;&#22914;&#22312;&#19987;&#29992;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#23398;&#20064;&#39069;&#22806;&#30340;&#21442;&#25968;&#26469;&#26377;&#25928;&#22320;&#32452;&#21512;&#36825;&#20123;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#23384;&#22312;&#35768;&#22810;&#21487;&#33021;&#30340;&#36873;&#39033;&#65292;&#23545;&#20110;&#36825;&#20123;&#32452;&#21512;&#20013;&#28041;&#21450;&#30340;&#26426;&#21046;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#22240;&#27492;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#24212;&#35813;&#20351;&#29992;&#21738;&#20123;&#31574;&#30053;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#26679;&#26412;&#27169;&#22359;&#32452;&#21512;&#26694;&#26550;&#65292;&#23427;&#28085;&#30422;&#20102;&#29616;&#26377;&#30340;&#19968;&#20123;&#36873;&#25321;&#12289;&#21152;&#26435;&#21644;&#32452;&#21512;&#21442;&#25968;&#27169;&#22359;&#30340;&#21464;&#21270;&#65292;&#32479;&#19968;&#20102;&#36825;&#20123;&#27010;&#24565;&#12290;&#22312;&#32858;&#28966;&#39046;&#22495;&#30693;&#35782;&#21644;&#36866;&#37197;&#22120;&#23618;&#30340;&#24773;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#32479;&#19968;&#27010;&#24565;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#39318;&#27425;&#20840;&#38754;&#30340;&#21508;&#31181;&#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The knowledge encapsulated in a model is the core factor determining its final performance on downstream tasks. Much research in NLP has focused on efficient methods for storing and adapting different types of knowledge, e.g., in dedicated modularized structures, and on how to effectively combine these, e.g., by learning additional parameters. However, given the many possible options, a thorough understanding of the mechanisms involved in these compositions is missing, and hence it remains unclear which strategies to utilize. To address this research gap, we propose a novel framework for zero-shot module composition, which encompasses existing and some novel variations for selecting, weighting, and combining parameter modules under a single unified notion. Focusing on the scenario of domain knowledge and adapter layers, our framework provides a systematic unification of concepts, allowing us to conduct the first comprehensive benchmarking study of various zero-shot knowledge compositio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12689</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#12289;&#20551;&#35774;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#24182;&#19981;&#24120;&#35265;&#12290;&#33258;&#21160;&#27169;&#22411;&#35780;&#20272;&#65288;AutoEval&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#24037;&#20316;&#27969;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#25104;&#19968;&#20010;&#25509;&#36817;&#39044;&#27979;&#24615;&#33021;&#30340;&#27979;&#35797;&#31649;&#32447;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#31614;&#30340;&#23384;&#22312;&#12290;&#23613;&#31649;AutoEval&#26694;&#26550;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26041;&#24335;&#8212;&#8212;&#20803;&#20998;&#24067;&#33021;&#37327;&#65288;MDE&#65289;&#65292;&#23427;&#21487;&#20197;&#20351;AutoEval&#26694;&#26550;&#26356;&#21152;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;MDE&#30340;&#26680;&#24515;&#26159;&#24314;&#31435;&#19968;&#20010;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#65288;&#33021;&#37327;&#65289;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#33021;&#37327;&#30340;&#23398;&#20064;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;MDE&#19982;&#20998;&#31867;&#25439;&#22833;&#30456;&#36830;&#25509;&#65292;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#25454;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive
&lt;/p&gt;</description></item><item><title>BiTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#35843;&#25972;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#23427;&#37319;&#29992;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#21516;&#26102;&#36827;&#34892;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2401.12522</link><description>&lt;p&gt;
BiTA: &#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#25439;&#21152;&#36895;&#30340;&#21452;&#21521;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. (arXiv:2401.12522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12522
&lt;/p&gt;
&lt;p&gt;
BiTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#35843;&#25972;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#23427;&#37319;&#29992;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#21516;&#26102;&#36827;&#34892;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#23548;&#33268;&#39640;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#21644;&#24310;&#36831;&#24310;&#38271;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#8212;&#8212;&#21452;&#21521;&#35843;&#25972;&#20197;&#23454;&#29616;&#26080;&#25439;&#21152;&#36895;&#65288;BiTA&#65289;&#65292;&#36890;&#36807;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#26469;&#21152;&#36895;LLMs&#12290;&#21463;&#21551;&#21457;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#35774;&#35745;&#65292;&#31216;&#20026;&#21452;&#21521;&#35843;&#25972;&#65292;&#26469;&#22686;&#24378;LLMs&#22312;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#37319;&#29992;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#65292;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#33609;&#31295;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#30830;&#20445;&#36755;&#20986;&#32467;&#26524;&#19982;&#23427;&#20204;&#30340;&#33258;&#22238;&#24402;&#23545;&#24212;&#29289;&#22312;&#36138;&#23146;&#25277;&#26679;&#19979;&#23436;&#20840;&#30456;&#21516;&#12290;BiTA&#20316;&#20026;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#21487;&#20197;&#26080;&#32541;&#22686;&#24378;&#29616;&#26377;LLMs&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#25215;&#25285;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#25552;&#20986;&#30340;BiTA&#65292;LLaMA-2-70B-Chat&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) commonly employ autoregressive generation during inference, leading to high memory bandwidth demand and consequently extended latency. To mitigate this inefficiency, we present Bi-directional Tuning for lossless Acceleration (BiTA), an innovative method expediting LLMs via streamlined semi-autoregressive generation and draft verification. Inspired by the concept of prompt tuning, we enhance LLMs with a parameter-efficient design called bi-directional tuning for the capability in semi-autoregressive generation. Employing efficient tree-based decoding, the models perform draft candidate generation and verification in parallel, ensuring outputs identical to their autoregressive counterparts under greedy sampling. BiTA serves as a lightweight plug-in module, seamlessly boosting the inference efficiency of existing LLMs without requiring additional assistance models or incurring significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat achieve
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.12255</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#25351;&#32441;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Instructional Fingerprinting of Large Language Models. (arXiv:2401.12255v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24040;&#22823;&#25104;&#26412;&#20351;&#24471;&#23545;&#27169;&#22411;&#36827;&#34892;&#25351;&#32441;&#35782;&#21035;&#20197;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#25104;&#20026;&#24517;&#35201;&#65292;&#36890;&#36807;&#25152;&#26377;&#26435;&#35748;&#35777;&#24182;&#30830;&#20445;&#19979;&#28216;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#65288;&#22914;&#38480;&#21046;&#21830;&#19994;&#20351;&#29992;&#65289;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#25351;&#32441;&#35782;&#21035;&#30340;&#35797;&#28857;&#30740;&#31350;&#65292;&#20316;&#20026;&#19968;&#31181;&#38750;&#24120;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#24418;&#24335;&#12290;&#27169;&#22411;&#21457;&#24067;&#32773;&#25351;&#23450;&#19968;&#20010;&#26426;&#23494;&#30340;&#31169;&#38053;&#65292;&#24182;&#23558;&#20854;&#26893;&#20837;&#20026;&#19968;&#20010;&#25351;&#20196;&#21518;&#38376;&#65292;&#24403;&#23494;&#38053;&#23384;&#22312;&#26102;&#65292;&#23548;&#33268;LLM&#29983;&#25104;&#29305;&#23450;&#30340;&#25991;&#26412;&#12290;&#23545;11&#20010;&#24120;&#29992;LLMs&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#36731;&#37327;&#32423;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#12290;&#23427;&#36824;&#21487;&#20197;&#38450;&#27490;&#21457;&#24067;&#32773;&#36807;&#24230;&#23459;&#31216;&#65292;&#23545;&#25351;&#32441;&#29468;&#27979;&#21644;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#31867;&#20284;&#20110;MIT&#35768;&#21487;&#35777;&#30340;&#22810;&#38454;&#27573;&#25351;&#32441;&#35782;&#21035;&#12290;&#20195;&#30721;&#21487;&#22312;https://cnut1648.github.io/Model-Fingerprint/&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (e.g. restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License. Code is available in https://cnut1648.github.io/Model-Fingerprint/.
&lt;/p&gt;</description></item><item><title>Mementos&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#22312;&#20934;&#30830;&#25551;&#36848;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23481;&#26131;&#23548;&#33268;&#29289;&#20307;&#21450;&#20854;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;</title><link>http://arxiv.org/abs/2401.10529</link><description>&lt;p&gt;
Mementos: &#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10529
&lt;/p&gt;
&lt;p&gt;
Mementos&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#22312;&#20934;&#30830;&#25551;&#36848;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23481;&#26131;&#23548;&#33268;&#29289;&#20307;&#21450;&#20854;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#22788;&#29702;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#39640;&#36229;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;MLLM&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#21333;&#20010;&#22270;&#20687;&#30340;&#38745;&#24577;&#20449;&#24687;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#29616;&#20195;MLLM&#22312;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#36827;&#34892;&#25512;&#26029;&#30340;&#33021;&#21147;&#65292;&#22312;&#29702;&#35299;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#21364;&#34987;&#36739;&#23569;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;Mementos&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#30340;&#24207;&#21015;&#22270;&#20687;&#25512;&#29702;&#33021;&#21147;&#12290;Mementos&#21253;&#25324;4761&#20010;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#22810;&#26679;&#30340;&#22270;&#20687;&#24207;&#21015;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;GPT-4&#36741;&#21161;&#26041;&#27861;&#26469;&#35780;&#20272;MLLM&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;Mementos&#20013;&#21253;&#25324;GPT-4V&#21644;Gemini&#22312;&#20869;&#30340;&#20061;&#20010;&#26368;&#26032;MLLM&#36827;&#34892;&#20180;&#32454;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#20934;&#30830;&#25551;&#36848;&#25152;&#32473;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24448;&#24448;&#23548;&#33268;&#23545;&#35937;&#21450;&#20854;&#23545;&#24212;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitati
&lt;/p&gt;</description></item><item><title>&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#30340;&#20219;&#21153;&#20013;&#12290;&#27492;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2401.10286</link><description>&lt;p&gt;
&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#20348;&#20348;&#32773;&#65306;&#33521;&#25991;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Top in Chinese Data Processing: English Code Models. (arXiv:2401.10286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10286
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#30340;&#20219;&#21153;&#20013;&#12290;&#27492;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#20013;&#65292;&#20219;&#21153;&#19982;&#35757;&#32451;&#35821;&#26009;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#20849;&#35782;&#65292;&#20294;&#25105;&#20204;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#21644;&#25105;&#20204;&#35774;&#35745;&#30340;&#35780;&#20272;&#25351;&#26631;&#34920;&#26126;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#19982;&#20219;&#21153;&#32039;&#23494;&#21305;&#37197;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#31243;&#24230;&#36739;&#39640;&#30340;&#20219;&#21153;&#20013;&#65292;&#23637;&#31034;&#36739;&#23569;&#20013;&#25991;&#35821;&#35328;&#29305;&#24449;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#29992;&#20195;&#30721;&#27169;&#22411;&#26367;&#25442;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#22914;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#20934;&#22791;&#25968;&#25454;&#65292;&#24456;&#23481;&#26131;&#24471;&#21040;&#22797;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the alignment between tasks and training corpora is a fundamental consensus in the application of language models, our series of experiments and the metrics we designed reveal that code-based Large Language Models (LLMs) significantly outperform models trained on data that is closely matched to the tasks in non-coding Chinese tasks. Moreover, in tasks high sensitivity to Chinese hallucinations, models exhibiting fewer linguistic features of the Chinese language achieve better performance. Our experimental results can be easily replicated in Chinese data processing tasks, such as preparing data for Retrieval-Augmented Generation (RAG), by simply replacing the base model with a code-based model. Additionally, our research offers a distinct perspective for discussion on the philosophical "Chinese Room" thought experiment.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;SAiD&#65289;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;Transformer-based U-Net&#27169;&#22411;&#21644;&#38899;&#39057;&#19982;&#35270;&#35273;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#20559;&#24046;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#21767;&#37096;&#21516;&#27493;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.08655</link><description>&lt;p&gt;
SAiD: &#20351;&#29992;&#25193;&#25955;&#26041;&#27861;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#34920;&#24773;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
SAiD: Speech-driven Blendshape Facial Animation with Diffusion. (arXiv:2401.08655v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08655
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;SAiD&#65289;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;Transformer-based U-Net&#27169;&#22411;&#21644;&#38899;&#39057;&#19982;&#35270;&#35273;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#20559;&#24046;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#21767;&#37096;&#21516;&#27493;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#35270;&#21548;&#25968;&#25454;&#38598;&#65292;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22823;&#22810;&#25968;&#36807;&#21435;&#30340;&#24037;&#20316;&#36890;&#24120;&#37319;&#29992;&#26368;&#23567;&#20108;&#20056;&#27861;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#22238;&#24402;&#27169;&#22411;&#65292;&#20294;&#22312;&#20174;&#35821;&#38899;&#29983;&#25104;&#21508;&#31181;&#21767;&#37096;&#21160;&#20316;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;SAiD&#65289;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;Transformer&#30340;U-Net&#27169;&#22411;&#65292;&#20855;&#26377;&#38899;&#39057;&#21644;&#35270;&#35273;&#20043;&#38388;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#20559;&#24046;&#65292;&#20197;&#22686;&#24378;&#21767;&#37096;&#21516;&#27493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BlendVOCA&#65292;&#36825;&#26159;&#19968;&#31181;&#35821;&#38899;&#38899;&#39057;&#21644;&#28151;&#21512;&#24418;&#29366;&#38754;&#37096;&#27169;&#22411;&#21442;&#25968;&#23545;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#20844;&#20849;&#36164;&#28304;&#30340;&#32570;&#20047;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21767;&#37096;&#21516;&#27493;&#26041;&#38754;&#36798;&#21040;&#20102;&#19982;&#22522;&#32447;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#30830;&#20445;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#21767;&#37096;&#36816;&#21160;&#65292;&#24182;&#31616;&#21270;&#20102;&#21160;&#30011;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-driven 3D facial animation is challenging due to the scarcity of large-scale visual-audio datasets despite extensive research. Most prior works, typically focused on learning regression models on a small dataset using the method of least squares, encounter difficulties generating diverse lip movements from speech and require substantial effort in refining the generated outputs. To address these issues, we propose a speech-driven 3D facial animation with a diffusion model (SAiD), a lightweight Transformer-based U-Net with a cross-modality alignment bias between audio and visual to enhance lip synchronization. Moreover, we introduce BlendVOCA, a benchmark dataset of pairs of speech audio and parameters of a blendshape facial model, to address the scarcity of public resources. Our experimental results demonstrate that the proposed approach achieves comparable or superior performance in lip synchronization to baselines, ensures more diverse lip movements, and streamlines the animati
&lt;/p&gt;</description></item><item><title>DiConStruct&#26159;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#25552;&#20379;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.08534</link><description>&lt;p&gt;
DiConStruct: &#22522;&#20110;&#40657;&#30418;&#31934;&#21326;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DiConStruct: Causal Concept-based Explanations through Black-Box Distillation. (arXiv:2401.08534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08534
&lt;/p&gt;
&lt;p&gt;
DiConStruct&#26159;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#25552;&#20379;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#31995;&#32479;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#35299;&#37322;&#24212;&#35813;&#20351;&#29992;&#20154;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#27010;&#24565;&#26469;&#34920;&#36798;&#12290;&#27492;&#22806;&#65292;&#35299;&#37322;&#22120;&#24212;&#35813;&#25429;&#25417;&#36825;&#20123;&#27010;&#24565;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#20415;&#23545;&#35299;&#37322;&#36827;&#34892;&#25512;&#29702;&#12290;&#26368;&#21518;&#65292;&#35299;&#37322;&#26041;&#27861;&#24212;&#35813;&#39640;&#25928;&#65292;&#24182;&#19981;&#25439;&#23475;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;AI&#35299;&#37322;&#24615;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#33267;&#20170;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#28385;&#36275;&#36825;&#19977;&#20010;&#26465;&#20214;&#12290;&#20107;&#23454;&#19978;&#65292;&#20027;&#27969;&#30340;&#23616;&#37096;&#27010;&#24565;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#19981;&#20135;&#29983;&#22240;&#26524;&#35299;&#37322;&#65292;&#24182;&#22312;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DiConStruct&#65292;&#19968;&#31181;&#26082;&#22522;&#20110;&#27010;&#24565;&#21448;&#20855;&#26377;&#22240;&#26524;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#21019;&#24314;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#22120;&#20316;&#20026;&#19968;&#20010;&#31934;&#21326;&#27169;&#22411;&#36866;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the performance of the predictive task. Despite the rapid advances in AI explainability in recent years, as far as we know to date, no method fulfills these three properties. Indeed, mainstream methods for local concept explainability do not produce causal explanations and incur a trade-off between explainability and prediction performance. We present DiConStruct, an explanation method that is both concept-based and causal, with the goal of creating more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine l
&lt;/p&gt;</description></item><item><title>CoSSegGaussians&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#31354;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#23454;&#29616;&#32039;&#20945;&#21644;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2401.05925</link><description>&lt;p&gt;
CoSSegGaussians&#65306;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians. (arXiv:2401.05925v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05925
&lt;/p&gt;
&lt;p&gt;
CoSSegGaussians&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#31354;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#23454;&#29616;&#32039;&#20945;&#21644;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65288;CoSSegGaussians&#65289;&#65292;&#35813;&#26041;&#27861;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#36755;&#20837;&#65292;&#20197;&#24555;&#36895;&#30340;&#28210;&#26579;&#36895;&#24230;&#23454;&#29616;&#32039;&#20945;&#30340;3D&#19968;&#33268;&#24615;&#22330;&#26223;&#20998;&#21106;&#12290;&#20808;&#21069;&#22522;&#20110;NeRF&#30340;3D&#20998;&#21106;&#26041;&#27861;&#20381;&#36182;&#20110;&#38544;&#24335;&#25110;&#20307;&#32032;&#31070;&#32463;&#22330;&#34920;&#31034;&#21644;&#20809;&#32447;&#34892;&#36827;&#20307;&#31215;&#28210;&#26579;&#65292;&#36825;&#20123;&#26041;&#27861;&#32791;&#26102;&#36739;&#38271;&#12290;&#26368;&#36817;&#30340;3D&#39640;&#26031;&#22330;&#25237;&#24433;&#26174;&#33879;&#25552;&#39640;&#20102;&#28210;&#26579;&#36895;&#24230;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#20998;&#21106;&#26041;&#27861;&#65288;&#20363;&#22914;&#39640;&#26031;&#20998;&#32452;&#65289;&#22312;&#38646;&#26679;&#26412;&#20998;&#21106;&#20013;&#27809;&#26377;&#25552;&#20379;&#32039;&#20945;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#22312;&#36935;&#21040;&#19981;&#19968;&#33268;&#30340;2D&#26426;&#22120;&#29983;&#25104;&#26631;&#31614;&#26102;&#65292;&#26080;&#27861;&#30452;&#25509;&#20026;&#27599;&#20010;&#39640;&#26031;&#20998;&#37197;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#32039;&#20945;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#27973;&#23618;&#35299;&#30721;&#32593;&#32476;&#23558;&#27599;&#20010;&#39640;&#26031;&#28857;&#30340;&#34701;&#21512;&#31354;&#38388;&#21644;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#26144;&#23556;&#65292;&#36805;&#36895;&#23454;&#29616;&#32039;&#20945;&#19988;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based 3D segmentation methods have relied on implicit or voxel neural scene representation and ray-marching volume rendering which are time consuming. Recent 3D Gaussian Splatting significantly improves the rendering speed, however, existing Gaussians-based segmentation methods(eg: Gaussian Grouping) fail to provide compact segmentation masks especially in zero-shot segmentation, which is mainly caused by the lack of robustness and compactness for straightforwardly assigning learnable parameters to each Gaussian when encountering inconsistent 2D machine-generated labels. Our method aims to achieve compact and reliable zero-shot scene segmentation swiftly by mapping fused spatial and semantically meaningful features for each Gaussian point with a shallow decoding network. Specifically, our method fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;Split Learning&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36807;&#31243;&#25552;&#39640;&#20102;&#20551;&#32930;&#25511;&#21046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03233</link><description>&lt;p&gt;
&#22522;&#20110;Split Learning&#30340;&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices. (arXiv:2401.03233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;Split Learning&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36807;&#31243;&#25552;&#39640;&#20102;&#20551;&#32930;&#25511;&#21046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Split Learning (SL)&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#24212;&#29992;&#20110;&#22522;&#20110;&#32908;&#30005;&#30340;&#20551;&#32930;&#25511;&#21046;&#12290;&#19982;&#28145;&#24230;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#31561;&#20854;&#20182;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;SL&#33021;&#22815;&#25552;&#20379;&#26356;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#20551;&#32930;&#35774;&#22791;&#22312;&#22788;&#29702;&#33021;&#21147;&#21644;&#30005;&#27744;&#23551;&#21629;&#26041;&#38754;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;SL&#30340;&#21487;&#34892;&#24615;&#28304;&#20110;&#20854;&#22266;&#26377;&#30340;&#27169;&#22411;&#20998;&#21106;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#25191;&#34892;&#36739;&#23567;&#30340;&#27169;&#22411;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#19981;&#24688;&#24403;&#30340;&#20999;&#23618;&#20250;&#38459;&#30861;SL&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#12290;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#25913;&#21892;&#20551;&#32930;&#25511;&#21046;&#30340;&#32908;&#30005;&#27169;&#24335;&#35782;&#21035;&#20219;&#21153;&#20013;&#26174;&#33879;&#21152;&#36895;&#20102;&#25910;&#25947;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split Learning (SL) is a promising Distributed Learning approach in electromyography (EMG) based prosthetic control, due to its applicability within resource-constrained environments. Other learning approaches, such as Deep Learning and Federated Learning (FL), provide suboptimal solutions, since prosthetic devices are extremely limited in terms of processing power and battery life. The viability of implementing SL in such scenarios is caused by its inherent model partitioning, with clients executing the smaller model segment. However, selecting an inadequate cut layer hinders the training process in SL systems. This paper presents an algorithm for optimal cut layer selection in terms of maximizing the convergence rate of the model. The performance evaluation demonstrates that the proposed algorithm substantially accelerates the convergence in an EMG pattern recognition task for improving prosthetic device control.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2401.01623</link><description>&lt;p&gt;
AI&#26159;&#21542;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#20855;&#22791;&#21019;&#36896;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI Be as Creative as Humans?. (arXiv:2401.01623v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#21147;&#26159;&#31038;&#20250;&#36827;&#27493;&#21644;&#21019;&#26032;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#20027;&#35266;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33021;&#22815;&#23436;&#25104;&#26366;&#32463;&#21482;&#23646;&#20110;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#20219;&#21153;&#65292;&#25506;&#32034;AI&#30340;&#21019;&#36896;&#28508;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#20854;&#36127;&#36131;&#20219;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#8220;&#30456;&#23545;&#21019;&#36896;&#21147;&#8221;&#30340;&#26032;&#27010;&#24565;&#26469;&#35299;&#20915;&#23450;&#20041;&#21644;&#35780;&#20272;&#21019;&#36896;&#21147;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#19981;&#20877;&#35797;&#22270;&#23545;&#21019;&#36896;&#21147;&#36827;&#34892;&#26222;&#36941;&#23450;&#20041;&#65292;&#32780;&#26159;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#19968;&#20301;&#20551;&#35774;&#30340;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;&#36825;&#31181;&#35266;&#28857;&#20511;&#37492;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#24605;&#24819;&#65292;&#24182;&#25193;&#23637;&#20854;&#33539;&#22260;&#20197;&#35299;&#20915;&#35780;&#20272;&#21019;&#36896;&#21147;&#20013;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#21644;&#20027;&#35266;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#36716;&#21464;&#20351;&#24471;&#23545;AI&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#32479;&#35745;&#21019;&#36896;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;AI&#19982;&#29305;&#23450;&#20154;&#31867;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creativity serves as a cornerstone for societal progress and innovation, but its assessment remains a complex and often subjective endeavor. With the rise of advanced generative AI models capable of tasks once reserved for human creativity, the study of AI's creative potential becomes imperative for its responsible development and application. This paper addresses the complexities in defining and evaluating creativity by introducing a new concept called Relative Creativity. Instead of trying to define creativity universally, we shift the focus to whether AI can match the creative abilities of a hypothetical human. This perspective draws inspiration from the Turing Test, expanding upon it to address the challenges and subjectivities inherent in evaluating creativity. This methodological shift facilitates a statistically quantifiable evaluation of AI's creativity, which we term Statistical Creativity. This approach allows for direct comparisons of AI's creative abilities with those of sp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#20316;&#20026;&#23454;&#29616;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#30340;&#21021;&#22987;&#27493;&#39588;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#26356;&#25509;&#36817;&#21442;&#32771;&#20551;&#35774;&#30340;&#36923;&#36753;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#35266;&#23519;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;RLF-KG&#65289;&#65292;&#26368;&#23567;&#21270;&#35266;&#23519;&#32467;&#26524;&#19982;&#32467;&#35770;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2312.15643</link><description>&lt;p&gt;
&#36890;&#36807;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#25512;&#36827;&#35825;&#23548;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation. (arXiv:2312.15643v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15643
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#20316;&#20026;&#23454;&#29616;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#30340;&#21021;&#22987;&#27493;&#39588;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#26356;&#25509;&#36817;&#21442;&#32771;&#20551;&#35774;&#30340;&#36923;&#36753;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#35266;&#23519;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;RLF-KG&#65289;&#65292;&#26368;&#23567;&#21270;&#35266;&#23519;&#32467;&#26524;&#19982;&#32467;&#35770;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35825;&#23548;&#25512;&#29702;&#26159;&#36890;&#36807;&#20570;&#20986;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#26469;&#35299;&#37322;&#35266;&#23519;&#32467;&#26524;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#20351;&#29992;&#30693;&#35782;&#36827;&#34892;&#35299;&#37322;&#65292;&#20294;&#23558;&#35825;&#23548;&#25512;&#29702;&#19982;&#32467;&#26500;&#21270;&#30693;&#35782;&#65288;&#22914;&#30693;&#35782;&#22270;&#35889;&#65289;&#32467;&#21512;&#20351;&#29992;&#30340;&#26041;&#27861;&#20173;&#28982;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#20316;&#20026;&#23454;&#29616;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#30340;&#21021;&#22987;&#27493;&#39588;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#22797;&#26434;&#30340;&#36923;&#36753;&#20551;&#35774;&#65292;&#20197;&#35299;&#37322;&#19968;&#32452;&#35266;&#23519;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32463;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#26356;&#25509;&#36817;&#21442;&#32771;&#20551;&#35774;&#30340;&#36923;&#36753;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#24403;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#35266;&#23519;&#32467;&#26524;&#26102;&#65292;&#36825;&#31181;&#35757;&#32451;&#30446;&#26631;&#24182;&#19981;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#20551;&#35774;&#29983;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;RLF-KG&#65289;&#65292;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#35266;&#23519;&#32467;&#26524;&#19982;&#32467;&#35770;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abductive reasoning is the process of making educated guesses to provide explanations for observations. Although many applications require the use of knowledge for explanations, the utilization of abductive reasoning in conjunction with structured knowledge, such as a knowledge graph, remains largely unexplored. To fill this gap, this paper introduces the task of complex logical hypothesis generation, as an initial step towards abductive logical reasoning with KG. In this task, we aim to generate a complex logical hypothesis so that it can explain a set of observations. We find that the supervised trained generative model can generate logical hypotheses that are structurally closer to the reference hypothesis. However, when generalized to unseen observations, this training objective does not guarantee better hypothesis generation. To address this, we introduce the Reinforcement Learning from Knowledge Graph (RLF-KG) method, which minimizes differences between observations and conclusio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#21160;&#24577;&#28145;&#24230;&#36335;&#30001;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;(D2R)&#65292;&#35813;&#26694;&#26550;&#28789;&#27963;&#36873;&#25321;&#19981;&#21516;&#20219;&#21153;&#25152;&#38656;&#30340;&#27169;&#22359;&#25968;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;ResRouting&#26041;&#27861;&#21644;&#33258;&#21160;&#36335;&#30001;&#24179;&#34913;&#26426;&#21046;&#26469;&#35299;&#20915;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.14472</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#20219;&#21153;&#37117;&#19968;&#26679;&#22256;&#38590;&#65306;&#20855;&#26377;&#21160;&#24577;&#28145;&#24230;&#36335;&#30001;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Not All Tasks Are Equally Difficult: Multi-Task Deep Reinforcement Learning with Dynamic Depth Routing. (arXiv:2312.14472v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#21160;&#24577;&#28145;&#24230;&#36335;&#30001;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;(D2R)&#65292;&#35813;&#26694;&#26550;&#28789;&#27963;&#36873;&#25321;&#19981;&#21516;&#20219;&#21153;&#25152;&#38656;&#30340;&#27169;&#22359;&#25968;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;ResRouting&#26041;&#27861;&#21644;&#33258;&#21160;&#36335;&#30001;&#24179;&#34913;&#26426;&#21046;&#26469;&#35299;&#20915;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#21333;&#20010;&#31574;&#30053;&#23436;&#25104;&#19968;&#32452;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#26469;&#22686;&#24378;&#25968;&#25454;&#25928;&#29575;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#23558;&#32593;&#32476;&#20998;&#21106;&#25104;&#19981;&#21516;&#30340;&#27169;&#22359;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#36335;&#30001;&#32593;&#32476;&#23558;&#36825;&#20123;&#27169;&#22359;&#37325;&#26032;&#32452;&#21512;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36335;&#30001;&#26041;&#27861;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#20351;&#29992;&#22266;&#23450;&#25968;&#37327;&#30340;&#27169;&#22359;&#65292;&#24573;&#30053;&#19981;&#21516;&#38590;&#24230;&#30340;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#19981;&#21516;&#25968;&#37327;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#28145;&#24230;&#36335;&#30001; (D2R) &#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#36339;&#36807;&#26576;&#20123;&#20013;&#38388;&#27169;&#22359;&#65292;&#20174;&#32780;&#28789;&#27963;&#22320;&#20026;&#27599;&#20010;&#20219;&#21153;&#36873;&#25321;&#19981;&#21516;&#25968;&#37327;&#30340;&#27169;&#22359;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181; ResRouting &#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#35757;&#32451;&#36807;&#31243;&#20013;&#34892;&#20026;&#31574;&#30053;&#21644;&#30446;&#26631;&#31574;&#30053;&#20043;&#38388;&#25130;&#28982;&#19981;&#21516;&#30340;&#36335;&#30001;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#21160;&#30340;&#36335;&#30001;&#24179;&#34913;&#26426;&#21046;&#65292;&#40723;&#21169;&#23545;&#26410;&#25484;&#25569;&#20219;&#21153;&#36827;&#34892;&#25345;&#32493;&#30340;&#36335;&#30001;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task reinforcement learning endeavors to accomplish a set of different tasks with a single policy. To enhance data efficiency by sharing parameters across multiple tasks, a common practice segments the network into distinct modules and trains a routing network to recombine these modules into task-specific policies. However, existing routing approaches employ a fixed number of modules for all tasks, neglecting that tasks with varying difficulties commonly require varying amounts of knowledge. This work presents a Dynamic Depth Routing (D2R) framework, which learns strategic skipping of certain intermediate modules, thereby flexibly choosing different numbers of modules for each task. Under this framework, we further introduce a ResRouting method to address the issue of disparate routing paths between behavior and target policies during off-policy training. In addition, we design an automatic route-balancing mechanism to encourage continued routing exploration for unmastered tasks 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#22411;&#37096;&#32626;&#21644;&#24182;&#34892;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;RLHF&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#20004;&#31181;&#28789;&#27963;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#65292;&#20854;&#20013;&#20132;&#26367;&#31574;&#30053;&#26377;&#21161;&#20110;&#20943;&#23569;&#20869;&#23384;&#20887;&#20313;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2312.11819</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;RLHF&#35757;&#32451;&#30340;&#33258;&#36866;&#24212;&#37096;&#32626;&#21644;&#24182;&#34892;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training. (arXiv:2312.11819v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11819
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#22411;&#37096;&#32626;&#21644;&#24182;&#34892;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;RLHF&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#20004;&#31181;&#28789;&#27963;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#65292;&#20854;&#20013;&#20132;&#26367;&#31574;&#30053;&#26377;&#21161;&#20110;&#20943;&#23569;&#20869;&#23384;&#20887;&#20313;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#25110;InstructGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#22797;&#29616;&#22797;&#26434;&#30340;InstructGPT&#30340;&#35757;&#32451;&#27969;&#31243;&#65292;&#21363;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12290;&#28982;&#32780;&#65292;&#20027;&#27969;&#30340;&#20998;&#24067;&#24335;RLHF&#35757;&#32451;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#22266;&#23450;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#65292;&#31216;&#20026;Flattening&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#23558;RLHF&#20013;&#28041;&#21450;&#30340;&#22235;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#27169;&#22411;&#35270;&#20026;&#21333;&#20010;&#23454;&#20307;&#65292;&#23558;&#23427;&#20204;&#20998;&#37197;&#21040;&#25152;&#26377;&#35774;&#22791;&#19978;&#65292;&#24182;&#24212;&#29992;&#20110;&#21333;&#20010;&#27169;&#22411;&#35774;&#35745;&#30340;&#24182;&#34892;&#25216;&#26415;&#65292;&#32780;&#19981;&#32771;&#34385;&#27599;&#20010;&#27169;&#22411;&#22266;&#26377;&#30340;&#19981;&#21516;&#24037;&#20316;&#36127;&#36733;&#12290;&#32467;&#26524;&#65292;&#35813;&#31574;&#30053;&#21152;&#21095;&#20102;RLHF&#35757;&#32451;&#20013;&#30340;&#29983;&#25104;&#29942;&#39048;&#65292;&#24182;&#38477;&#20302;&#20102;&#25972;&#20307;&#35757;&#32451;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#22411;&#37096;&#32626;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#28789;&#27963;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#12290;&#20132;&#26367;&#31574;&#30053;&#26377;&#21161;&#20110;&#20943;&#23569;&#20869;&#23384;&#20887;&#20313;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ChatGPT or InstructGPT like large language models (LLM) has made a significant impact in the AI world. Many works have attempted to reproduce the complex InstructGPT's training pipeline, namely Reinforcement Learning with Human Feedback (RLHF). However, the mainstream distributed RLHF training methods typically adopt a fixed model placement strategy, referred to as the Flattening strategy. This strategy treats all four interdependent models involved in RLHF as a single entity, distributing them across all devices and applying parallelism techniques designed for a single model, regardless of the different workloads inherent to each model. As a result, this strategy exacerbates the generation bottlenecks in the RLHF training and degrades the overall training efficiency. To address these issues, we propose an adaptive model placement framework that offers two flexible model placement strategies. The Interleaving strategy helps reduce memory redundancy and communication costs of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#30740;&#31350;&#65292;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#12289;&#26041;&#27861;&#21644;&#22522;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#25512;&#29702;&#33021;&#21147;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2312.11562</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Reasoning with Foundation Models. (arXiv:2312.11562v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#30740;&#31350;&#65292;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#12289;&#26041;&#27861;&#21644;&#22522;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#25512;&#29702;&#33021;&#21147;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#26159;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#20851;&#38190;&#33021;&#21147;&#65292;&#22312;&#35848;&#21028;&#12289;&#21307;&#23398;&#35786;&#26029;&#21644;&#21009;&#20107;&#35843;&#26597;&#31561;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23427;&#22312;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#39046;&#22495;&#20013;&#20316;&#20026;&#19968;&#31181;&#22522;&#26412;&#26041;&#27861;&#23398;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#32773;&#23545;&#23427;&#20204;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#25512;&#29702;&#30340;&#24320;&#21019;&#24615;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#31361;&#20986;&#20102;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#12289;&#26041;&#27861;&#21644;&#22522;&#20934;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#25512;&#29702;&#33021;&#21147;&#30340;&#28508;&#22312;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#35752;&#35770;&#20102;&#22810;&#27169;&#24335;&#23398;&#20064;&#12289;&#33258;&#20027;&#20195;&#29702;&#21644;&#36229;&#32423;&#23545;&#40784;&#22312;&#25512;&#29702;&#32972;&#26223;&#19979;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#35752;&#35770;&#36825;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#25105;&#20204;&#24076;&#26395;&#28608;&#21457;&#30740;&#31350;&#32773;&#20204;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#25506;&#32034;&#65292;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, e.g., Large Language Models (LLMs), there is a growing interest in exploring their abilities in reasoning tasks. In this paper, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advance
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36825;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;LLMs&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;LLMs&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#36739;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2312.05934</link><description>&lt;p&gt;
Fine-Tuning&#36824;&#26159;&#26816;&#32034;&#65311;&#27604;&#36739;&#22312;LLMs&#20013;&#30340;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. (arXiv:2312.05934v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36825;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;LLMs&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;LLMs&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#36739;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20854;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#20013;&#23553;&#35013;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#27491;&#22914;&#23427;&#20204;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30693;&#35782;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22806;&#37096;&#25968;&#25454;&#38598;&#26469;&#25972;&#21512;&#26032;&#30340;&#20449;&#24687;&#25110;&#25913;&#36827;LLMs&#22312;&#24050;&#35265;&#20449;&#24687;&#19978;&#30340;&#33021;&#21147;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65306;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20027;&#39064;&#30340;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#33021;&#22815;&#25552;&#20379;&#19968;&#23450;&#30340;&#25913;&#36827;&#65292;&#20294;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#23436;&#20840;&#26032;&#30693;&#35782;&#19978;&#22987;&#32456;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#24456;&#38590;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#24182;&#19988;&#26292;&#38706;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#25511;&#21046;&#26041;&#27861;&#22312;&#20303;&#23429;&#24314;&#31569;&#20379;&#26262;&#30340;&#38656;&#27714;&#21709;&#24212;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28789;&#27963;&#30340;&#20248;&#21270;&#25972;&#21512;&#20102;&#22806;&#37096;&#32422;&#26463;&#26465;&#20214;&#65292;&#20026;&#23454;&#29616;&#33021;&#28304;&#28040;&#32791;&#30340;&#20248;&#21270;&#21644;&#29992;&#25143;&#28909;&#33298;&#36866;&#24230;&#30340;&#20445;&#35777;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.03365</link><description>&lt;p&gt;
&#20303;&#23429;&#24314;&#31569;&#20379;&#26262;&#30340;&#38656;&#27714;&#21709;&#24212;&#65306;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Demand response for residential building heating: Effective Monte Carlo Tree Search control based on physics-informed neural networks. (arXiv:2312.03365v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#25511;&#21046;&#26041;&#27861;&#22312;&#20303;&#23429;&#24314;&#31569;&#20379;&#26262;&#30340;&#38656;&#27714;&#21709;&#24212;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28789;&#27963;&#30340;&#20248;&#21270;&#25972;&#21512;&#20102;&#22806;&#37096;&#32422;&#26463;&#26465;&#20214;&#65292;&#20026;&#23454;&#29616;&#33021;&#28304;&#28040;&#32791;&#30340;&#20248;&#21270;&#21644;&#29992;&#25143;&#28909;&#33298;&#36866;&#24230;&#30340;&#20445;&#35777;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#24314;&#31569;&#29289;&#30340;&#33021;&#28304;&#28040;&#32791;&#36890;&#36807;&#38656;&#27714;&#21709;&#24212;&#24050;&#25104;&#20026;&#20943;&#23569;&#20840;&#29699;&#30899;&#25490;&#25918;&#21644;&#38480;&#21046;&#27668;&#20505;&#21464;&#21270;&#30340;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#25163;&#27573;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#20303;&#23429;&#24314;&#31569;&#20379;&#26262;&#31995;&#32479;&#30340;&#25511;&#21046;&#65292;&#20197;&#20248;&#21270;&#33021;&#28304;&#28040;&#32791;&#21516;&#26102;&#20445;&#35777;&#29992;&#25143;&#30340;&#28909;&#33298;&#36866;&#24230;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#65292;&#20363;&#22914;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#65292;&#25110;&#32773;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#38656;&#27714;&#21709;&#24212;&#31639;&#27861;&#12290;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#22312;&#26827;&#30424;&#28216;&#25103;&#65288;&#22260;&#26827;&#12289;&#22269;&#38469;&#35937;&#26827;&#65289;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#25104;&#21151;&#30340;RL&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#24314;&#31569;&#25511;&#21046;&#26041;&#38754;&#65292;MCTS&#20173;&#28982;&#34987;&#36739;&#23569;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#38376;&#30740;&#31350;&#20102;MCTS&#22312;&#24314;&#31569;&#38656;&#27714;&#21709;&#24212;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#20854;&#33258;&#28982;&#30340;&#32467;&#26500;&#20801;&#35768;&#28789;&#27963;&#30340;&#20248;&#21270;&#65292;&#38544;&#24335;&#22320;&#38598;&#25104;&#22806;&#37096;&#32422;&#26463;&#26465;&#20214;&#65288;&#19982;&#20256;&#32479;&#30340;RL&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65289;&#65292;&#20351;MCTS&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling energy consumption in buildings through demand response (DR) has become increasingly important to reduce global carbon emissions and limit climate change. In this paper, we specifically focus on controlling the heating system of a residential building to optimize its energy consumption while respecting user's thermal comfort. Recent works in this area have mainly focused on either model-based control, e.g., model predictive control (MPC), or model-free reinforcement learning (RL) to implement practical DR algorithms. A specific RL method that recently has achieved impressive success in domains such as board games (go, chess) is Monte Carlo Tree Search (MCTS). Yet, for building control it has remained largely unexplored. Thus, we study MCTS specifically for building demand response. Its natural structure allows a flexible optimization that implicitly integrate exogenous constraints (as opposed, for example, to conventional RL solutions), making MCTS a promising candidate for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24863;&#30693;&#32452;&#20998;&#35789;&#22120;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#36845;&#20195;&#20998;&#32452;&#25805;&#20316;&#26469;&#25552;&#21462;&#35270;&#35273;&#29305;&#24449;&#24182;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#19982;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#26550;&#26500;&#30456;&#31454;&#20105;&#65292;&#24182;&#20855;&#26377;&#33258;&#36866;&#24212;&#35745;&#31639;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2311.18296</link><description>&lt;p&gt;
&#24863;&#30693;&#32452;&#20998;&#35789;&#22120;&#65306;&#36890;&#36807;&#36845;&#20195;&#20998;&#32452;&#26500;&#24314;&#35270;&#35273;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Perceptual Group Tokenizer: Building Perception with Iterative Grouping. (arXiv:2311.18296v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24863;&#30693;&#32452;&#20998;&#35789;&#22120;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#36845;&#20195;&#20998;&#32452;&#25805;&#20316;&#26469;&#25552;&#21462;&#35270;&#35273;&#29305;&#24449;&#24182;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#19982;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#26550;&#26500;&#30456;&#31454;&#20105;&#65292;&#24182;&#20855;&#26377;&#33258;&#36866;&#24212;&#35745;&#31639;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35270;&#35273;&#35782;&#21035;&#31995;&#32479;&#23637;&#31034;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#23558;&#35270;&#35273;&#20449;&#24687;&#21387;&#32553;&#25104;&#19968;&#32452;&#21547;&#26377;&#20016;&#23500;&#34920;&#31034;&#30340;&#26631;&#35760;&#65292;&#32780;&#26080;&#38656;&#26631;&#31614;&#30417;&#30563;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#39537;&#21160;&#21407;&#21017;&#26159;&#24863;&#30693;&#20998;&#32452;&#12290;&#23613;&#31649;&#22312;2010&#24180;&#20195;&#26089;&#26399;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#20294;&#24863;&#30693;&#20998;&#32452;&#33021;&#21542;&#34987;&#29992;&#26469;&#25512;&#23548;&#20986;&#29983;&#25104;&#24378;&#22823;&#34920;&#31034;&#30340;&#31070;&#32463;&#35270;&#35273;&#35782;&#21035;&#26694;&#26550;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Perceptual Group Tokenizer&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#20381;&#36182;&#20998;&#32452;&#25805;&#20316;&#26469;&#25552;&#21462;&#35270;&#35273;&#29305;&#24449;&#21644;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#31995;&#21015;&#30340;&#20998;&#32452;&#25805;&#20316;&#34987;&#29992;&#26469;&#36845;&#20195;&#24615;&#22320;&#25512;&#26029;&#20687;&#32032;&#25110;&#36229;&#20687;&#32032;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#25913;&#36827;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#26550;&#26500;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#33258;&#36866;&#24212;&#35745;&#31639;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#21487;&#21462;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human visual recognition system shows astonishing capability of compressing visual information into a set of tokens containing rich representations without label supervision. One critical driving principle behind it is perceptual grouping. Despite being widely used in computer vision in the early 2010s, it remains a mystery whether perceptual grouping can be leveraged to derive a neural visual recognition backbone that generates as powerful representations. In this paper, we propose the Perceptual Group Tokenizer, a model that entirely relies on grouping operations to extract visual features and perform self-supervised representation learning, where a series of grouping operations are used to iteratively hypothesize the context for pixels or superpixels to refine feature representations. We show that the proposed model can achieve competitive performance compared to state-of-the-art vision architectures, and inherits desirable properties including adaptive computation without re-traini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340; LSTM &#27169;&#22411;&#65292;&#29992;&#20110;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22522;&#20110;&#30456;&#20851;&#29109;&#30340;&#26426;&#21046;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#19982;&#20154;&#20043;&#38388;&#20132;&#20114;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#27599;&#20010;&#34892;&#20154;&#24314;&#31435;&#20010;&#20154;&#31354;&#38388;&#12290;&#36825;&#20010;&#25968;&#25454;&#39537;&#21160;&#26426;&#21046;&#21487;&#20197;&#26377;&#25928;&#25552;&#21462;&#21160;&#24577;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2311.15193</link><description>&lt;p&gt;
IA-LSTM: &#20132;&#20114;&#24863;&#30693; LSTM &#29992;&#20110;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
IA-LSTM: Interaction-Aware LSTM for Pedestrian Trajectory Prediction. (arXiv:2311.15193v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340; LSTM &#27169;&#22411;&#65292;&#29992;&#20110;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22522;&#20110;&#30456;&#20851;&#29109;&#30340;&#26426;&#21046;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#19982;&#20154;&#20043;&#38388;&#20132;&#20114;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#27599;&#20010;&#34892;&#20154;&#24314;&#31435;&#20010;&#20154;&#31354;&#38388;&#12290;&#36825;&#20010;&#25968;&#25454;&#39537;&#21160;&#26426;&#21046;&#21487;&#20197;&#26377;&#25928;&#25552;&#21462;&#21160;&#24577;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#25110;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#39044;&#27979;&#25317;&#25380;&#22330;&#26223;&#20013;&#34892;&#20154;&#30340;&#36712;&#36857;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#65292;&#22240;&#20026;&#20272;&#35745;&#21608;&#22260;&#34892;&#20154;&#30340;&#26410;&#26469;&#20301;&#32622;&#26377;&#21161;&#20110;&#20915;&#31574;&#36991;&#20813;&#30896;&#25758;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#20154;&#31867;&#20855;&#26377;&#19981;&#21516;&#30340;&#34892;&#36208;&#26041;&#24335;&#65292;&#24403;&#21069;&#29615;&#22659;&#20013;&#30340;&#20154;&#19982;&#29289;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#29305;&#21035;&#26159;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#20132;&#20114;&#26159;&#22797;&#26434;&#30340;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#32773;&#20851;&#27880;&#22914;&#20309;&#24314;&#27169;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20294;&#24573;&#35270;&#20102;&#20132;&#20114;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#29109;&#30340;&#26032;&#26426;&#21046;&#12290;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#19981;&#20165;&#21487;&#20197;&#34913;&#37327;&#20154;&#19982;&#20154;&#20043;&#38388;&#20132;&#20114;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#36824;&#21487;&#20197;&#20026;&#27599;&#20010;&#34892;&#20154;&#24314;&#31435;&#20010;&#20154;&#31354;&#38388;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#21253;&#25324;&#36825;&#20010;&#25968;&#25454;&#39537;&#21160;&#26426;&#21046;&#30340;&#20132;&#20114;&#27169;&#22359;&#12290;&#22312;&#25552;&#20986;&#30340;&#27169;&#22359;&#20013;&#65292;&#25968;&#25454;&#39537;&#21160;&#26426;&#21046;&#21487;&#20197;&#26377;&#25928;&#25552;&#21462;&#21160;&#24577;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the trajectory of pedestrians in crowd scenarios is indispensable in self-driving or autonomous mobile robot field because estimating the future locations of pedestrians around is beneficial for policy decision to avoid collision. It is a challenging issue because humans have different walking motions, and the interactions between humans and objects in the current environment, especially between humans themselves, are complex. Previous researchers focused on how to model human-human interactions but neglected the relative importance of interactions. To address this issue, a novel mechanism based on correntropy is introduced. The proposed mechanism not only can measure the relative importance of human-human interactions but also can build personal space for each pedestrian. An interaction module including this data-driven mechanism is further proposed. In the proposed module, the data-driven mechanism can effectively extract the feature representations of dynamic human-human 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36890;&#29992;&#30701;&#35821;&#21435;&#20559;&#22120;&#8221;&#30340;&#33258;&#21160;&#22810;&#26631;&#35760;&#21435;&#20559;&#31649;&#36947;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#36731;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30701;&#35821;&#32423;&#21035;&#20559;&#35265;&#65292;&#24182;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.13892</link><description>&lt;p&gt;
&#36890;&#29992;&#30701;&#35821;&#21435;&#20559;&#22120;&#65306;&#22312;&#22810;&#26631;&#35760;&#32423;&#21035;&#19978;&#28040;&#38500;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
General Phrase Debiaser: Debiasing Masked Language Models at a Multi-Token Level. (arXiv:2311.13892v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36890;&#29992;&#30701;&#35821;&#21435;&#20559;&#22120;&#8221;&#30340;&#33258;&#21160;&#22810;&#26631;&#35760;&#21435;&#20559;&#31649;&#36947;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#36731;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30701;&#35821;&#32423;&#21035;&#20559;&#35265;&#65292;&#24182;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25152;&#25581;&#31034;&#30340;&#31038;&#20250;&#20559;&#35265;&#21644;&#19981;&#21463;&#27426;&#36814;&#30340;&#21051;&#26495;&#21360;&#35937;&#27491;&#22312;&#25104;&#20026;&#20854;&#24212;&#29992;&#30340;&#38556;&#30861;&#12290;&#19982;&#38024;&#23545;&#35789;&#32423;&#21035;&#30340;&#20247;&#22810;&#21435;&#20559;&#26041;&#27861;&#30456;&#27604;&#65292;&#23545;&#20110;&#30701;&#35821;&#32423;&#21035;&#30340;&#20559;&#35265;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#65292;&#38480;&#21046;&#20102;&#23398;&#31185;&#39046;&#22495;&#21435;&#20559;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36890;&#29992;&#30701;&#35821;&#21435;&#20559;&#22120;&#8221;&#30340;&#33258;&#21160;&#22810;&#26631;&#35760;&#21435;&#20559;&#31649;&#36947;&#65292;&#33021;&#22815;&#20943;&#36731;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30701;&#35821;&#32423;&#21035;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#8220;&#30701;&#35821;&#36807;&#28388;&#38454;&#27573;&#8221;&#65292;&#20174;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#20013;&#29983;&#25104;&#21051;&#26495;&#21360;&#35937;&#30340;&#30701;&#35821;&#65292;&#20197;&#21450;&#19968;&#20010;&#8220;&#27169;&#22411;&#21435;&#20559;&#38454;&#27573;&#8221;&#65292;&#21487;&#20197;&#22312;&#22810;&#26631;&#35760;&#32423;&#21035;&#19978;&#21435;&#20559;&#27169;&#22411;&#20197;&#24212;&#23545;&#30701;&#35821;&#19978;&#30340;&#20559;&#35265;&#25361;&#25112;&#12290;&#21518;&#32773;&#23547;&#25214;&#35302;&#21457;&#27169;&#22411;&#20559;&#35265;&#30340;&#25552;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20110;&#21435;&#20559;&#12290;&#26631;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#26368;&#26032;&#25104;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#32844;&#19994;&#21644;&#21152;&#24378;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
The social biases and unwelcome stereotypes revealed by pretrained language models are becoming obstacles to their application. Compared to numerous debiasing methods targeting word level, there has been relatively less attention on biases present at phrase level, limiting the performance of debiasing in discipline domains. In this paper, we propose an automatic multi-token debiasing pipeline called \textbf{General Phrase Debiaser}, which is capable of mitigating phrase-level biases in masked language models. Specifically, our method consists of a \textit{phrase filter stage} that generates stereotypical phrases from Wikipedia pages as well as a \textit{model debias stage} that can debias models at the multi-token level to tackle bias challenges on phrases. The latter searches for prompts that trigger model's bias, and then uses them for debiasing. State-of-the-art results on standard datasets and metrics show that our approach can significantly reduce gender biases on both career and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.11482</link><description>&lt;p&gt;
AGI&#31995;&#32479;&#30340;&#20803;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Meta Prompting for AGI Systems. (arXiv:2311.11482v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#25552;&#31034;(meta prompting)&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#25216;&#26415;&#65292;&#37325;&#26032;&#22609;&#36896;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12289;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#21033;&#29992;&#12290;&#22522;&#20110;&#31867;&#22411;&#29702;&#35770;&#21644;&#33539;&#30068;&#35770;&#65292;&#20803;&#25552;&#31034;&#27880;&#37325;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#20197;&#20869;&#23481;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20803;&#25552;&#31034;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#23558;&#20854;&#19982;&#23569;&#26679;&#26412;&#25552;&#31034;(few-shot prompting)&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#37325;&#28857;&#20851;&#27880;&#23558;&#20803;&#25552;&#31034;&#25193;&#23637;&#21040;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;&#23637;&#31034;&#22914;&#20309;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#20998;&#25104;&#36739;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20196;&#29260;&#25928;&#29575;&#65292;&#24182;&#20351;&#38382;&#39064;&#27714;&#35299;&#30340;&#27604;&#36739;&#26356;&#21152;&#20844;&#24179;&#65292;&#23588;&#20854;&#26159;&#19982;&#23569;&#26679;&#26412;&#31034;&#20363;&#26041;&#27861;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#25552;&#31034;&#20219;&#21153;&#65292;&#20801;&#35768;LLMs&#20197;&#36845;&#20195;&#30340;&#20803;&#32534;&#31243;&#24418;&#24335;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive study of Meta Prompting, an innovative technique reshaping the utilization of large language models (LLMs), multi-modal foundation models, and AI systems in problem-solving and data interpretation. Grounded in type theory and category theory, Meta Prompting emphasizes the structure and syntax of information over traditional content-centric methods. The paper explores the formal definitions of Meta Prompting (MP), sets it apart from Few-Shot Prompting, and underlines its effectiveness in various AI applications. A key focus is on extending Meta Prompting to complex reasoning tasks, showing how it effectively deconstructs intricate problems into simpler sub-problems, enhancing token efficiency and enabling more equitable problem-solving comparisons, especially against few-shot example methods. Additionally, the paper introduces Meta Prompting for Prompting Tasks, allowing LLMs to self-generate new prompts in an iterative, metaprogramming-like manner. T
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#32500;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#36816;&#21160;&#24615;&#22330;&#26223;&#19979;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;OTFS&#31526;&#21495;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#23558;OTFS&#31995;&#32479;&#30340;&#39046;&#22495;&#30693;&#35782;&#19982;&#22312;&#32447;&#23376;&#24103;&#31526;&#21495;&#26816;&#27979;&#30340;&#35774;&#35745;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20108;&#32500;&#24490;&#29615;&#22635;&#20805;&#21644;&#28388;&#27874;&#32467;&#26500;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2311.08543</link><description>&lt;p&gt;
2D-RC: &#20108;&#32500;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#29992;&#20110;OTFS&#31526;&#21495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
2D-RC: Two-Dimensional Neural Network Approach for OTFS Symbol Detection. (arXiv:2311.08543v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08543
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#32500;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#36816;&#21160;&#24615;&#22330;&#26223;&#19979;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;OTFS&#31526;&#21495;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#23558;OTFS&#31995;&#32479;&#30340;&#39046;&#22495;&#30693;&#35782;&#19982;&#22312;&#32447;&#23376;&#24103;&#31526;&#21495;&#26816;&#27979;&#30340;&#35774;&#35745;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20108;&#32500;&#24490;&#29615;&#22635;&#20805;&#21644;&#28388;&#27874;&#32467;&#26500;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#20132;&#26102;&#39057;&#31354;&#38388;&#65288;OTFS&#65289;&#26159;&#39640;&#36816;&#21160;&#24615;&#22330;&#26223;&#19979;&#26080;&#32447;&#36890;&#20449;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35843;&#21046;&#26041;&#26696;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#22522;&#20110;&#20648;&#23618;&#35745;&#31639;&#65288;RC&#65289;&#30340;&#26041;&#27861;&#24050;&#32463;&#24341;&#20837;&#21040;OTFS&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#23376;&#24103;&#31526;&#21495;&#26816;&#27979;&#20013;&#65292;&#35813;&#26041;&#27861;&#21482;&#21033;&#29992;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#31354;&#20013;&#65288;OTA&#65289;&#23548;&#39057;&#31526;&#21495;&#29992;&#20110;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;OTFS&#31995;&#32479;&#29305;&#23450;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#26080;&#27861;&#20805;&#20998;&#21457;&#25381;RC&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#32500;RC&#65288;2D-RC&#65289;&#26041;&#27861;&#65292;&#23558;OTFS&#31995;&#32479;&#30340;&#39046;&#22495;&#30693;&#35782;&#32467;&#21512;&#21040;&#22312;&#32447;&#23376;&#24103;&#31526;&#21495;&#26816;&#27979;&#30340;&#35774;&#35745;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#24310;&#36831;-&#22810;&#26222;&#21202;&#65288;DD&#65289;&#22495;&#20013;&#30340;&#20449;&#36947;&#30456;&#20114;&#20316;&#29992;&#26159;&#19968;&#20010;&#20108;&#32500;&#65288;2D&#65289;&#30340;&#24490;&#29615;&#25805;&#20316;&#65292;2D-RC&#34987;&#35774;&#35745;&#20026;&#20855;&#26377;2D&#24490;&#29615;&#22635;&#20805;&#36807;&#31243;&#21644;2D&#28388;&#27874;&#32467;&#26500;&#20197;&#23884;&#20837;&#27492;&#30693;&#35782;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#31181;&#26550;&#26500;&#65292;2D-RC&#21487;&#20197;&#22312;DD&#22495;&#20013;&#20165;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#30340;&#23548;&#39057;&#31526;&#21495;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orthogonal time frequency space (OTFS) is a promising modulation scheme for wireless communication in high-mobility scenarios. Recently, a reservoir computing (RC) based approach has been introduced for online subframe-based symbol detection in the OTFS system, where only a limited number of over-the-air (OTA) pilot symbols are utilized for training. However, this approach does not leverage the domain knowledge specific to the OTFS system to fully unlock the potential of RC. This paper introduces a novel two-dimensional RC (2D-RC) method that incorporates the domain knowledge of the OTFS system into the design for symbol detection in an online subframe-based manner. Specifically, as the channel interaction in the delay-Doppler (DD) domain is a two-dimensional (2D) circular operation, the 2D-RC is designed to have the 2D circular padding procedure and the 2D filtering structure to embed this knowledge. With the introduced architecture, 2D-RC can operate in the DD domain with only a sing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20419;&#36827;&#38598;&#20307;&#20915;&#31574;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#31649;&#29702;&#23545;&#35805;&#21644;&#24179;&#34913;&#20010;&#20154;&#20559;&#22909;&#26469;&#25552;&#20379;&#28385;&#36275;&#25104;&#21592;&#38656;&#27714;&#30340;&#36873;&#39033;&#65292;&#23454;&#29616;&#39640;&#25928;&#21327;&#35843;&#24182;&#19981;&#26029;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.04928</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#20307;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Collective Decision-Making. (arXiv:2311.04928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20419;&#36827;&#38598;&#20307;&#20915;&#31574;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#31649;&#29702;&#23545;&#35805;&#21644;&#24179;&#34913;&#20010;&#20154;&#20559;&#22909;&#26469;&#25552;&#20379;&#28385;&#36275;&#25104;&#21592;&#38656;&#27714;&#30340;&#36873;&#39033;&#65292;&#23454;&#29616;&#39640;&#25928;&#21327;&#35843;&#24182;&#19981;&#26029;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24037;&#20316;&#29615;&#22659;&#20013;&#65292;&#22914;&#20250;&#35758;&#23433;&#25490;&#12289;&#21512;&#20316;&#21644;&#39033;&#30446;&#35268;&#21010;&#20013;&#65292;&#38598;&#20307;&#20915;&#31574;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20294;&#30001;&#20110;&#20010;&#20307;&#20559;&#22909;&#22810;&#26679;&#24615;&#12289;&#24037;&#20316;&#28966;&#28857;&#19981;&#21516;&#21644;&#25104;&#21592;&#20043;&#38388;&#30340;&#26435;&#21147;&#21160;&#24577;&#31561;&#22240;&#32032;&#65292;&#24120;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#20419;&#36827;&#32676;&#20307;&#20915;&#31574;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#31649;&#29702;&#23545;&#35805;&#21644;&#24179;&#34913;&#20010;&#20154;&#20559;&#22909;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26088;&#22312;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#20010;&#20307;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#28385;&#36275;&#25104;&#21592;&#20559;&#22909;&#30340;&#36873;&#39033;&#12290;&#25105;&#20204;&#29305;&#21035;&#23558;&#27492;&#31995;&#32479;&#24212;&#29992;&#20110;&#20225;&#19994;&#20250;&#35758;&#23433;&#25490;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#21019;&#24314;&#20102;&#21512;&#25104;&#21592;&#24037;&#37197;&#32622;&#25991;&#20214;&#65292;&#24182;&#27169;&#25311;&#20102;&#22823;&#35268;&#27169;&#30340;&#23545;&#35805;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#35780;&#20272;&#31995;&#32479;&#34920;&#29616;&#26469;&#20316;&#20026;&#24320;&#23637;&#29992;&#25143;&#30740;&#31350;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31995;&#32479;&#33021;&#23454;&#29616;&#25104;&#21592;&#19982;LLM&#31995;&#32479;&#20043;&#38388;&#30340;&#39640;&#25928;&#21327;&#35843;&#65292;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#23545;&#20854;&#25552;&#20986;&#30340;&#36873;&#39033;&#36827;&#34892;&#25913;&#36827;&#21644;&#23436;&#21892;&#65292;&#30830;&#20445;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In various work contexts, such as meeting scheduling, collaborating, and project planning, collective decision-making is essential but often challenging due to diverse individual preferences, varying work focuses, and power dynamics among members. To address this, we propose a system leveraging Large Language Models (LLMs) to facilitate group decision-making by managing conversations and balancing preferences among individuals. Our system aims to extract individual preferences from conversations and suggest options that satisfy the preferences of the members. We specifically apply this system to corporate meeting scheduling. We create synthetic employee profiles and simulate conversations at scale, leveraging LLMs to evaluate the system performance as a novel approach to conducting a user study. Our results indicate efficient coordination with reduced interactions between the members and the LLM-based system. The system refines and improves its proposed options over time, ensuring that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#38754;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#23545;&#19978;&#19979;&#25991;&#32622;&#20449;&#24230;&#30340;&#25361;&#25112;&#26102;&#65292;&#37319;&#21462;&#30340;&#20004;&#31867;&#31574;&#30053;&#65306;&#36943;&#21046;&#31574;&#30053;&#21644;&#21160;&#21592;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2311.01193</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#32622;&#20449;&#24230;&#21644;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Contextual Confidence and Generative AI. (arXiv:2311.01193v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#38754;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#23545;&#19978;&#19979;&#25991;&#32622;&#20449;&#24230;&#30340;&#25361;&#25112;&#26102;&#65292;&#37319;&#21462;&#30340;&#20004;&#31867;&#31574;&#30053;&#65306;&#36943;&#21046;&#31574;&#30053;&#21644;&#21160;&#21592;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25200;&#20081;&#20102;&#26377;&#25928;&#20154;&#38469;&#27807;&#36890;&#30340;&#22522;&#30784;&#65292;&#32473;&#19978;&#19979;&#25991;&#32622;&#20449;&#24230;&#24102;&#26469;&#26032;&#30340;&#25361;&#25112;&#65292;&#20351;&#21442;&#19982;&#32773;&#38590;&#20197;&#30830;&#23450;&#27807;&#36890;&#30340;&#30495;&#23454;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#20445;&#25252;&#27807;&#36890;&#19981;&#34987;&#22312;&#24847;&#22270;&#20043;&#22806;&#30340;&#29615;&#22659;&#20013;&#37325;&#22797;&#20351;&#29992;&#21644;&#37325;&#32452;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#26088;&#22312;&#22312;&#38754;&#23545;&#36825;&#20123;&#25361;&#25112;&#26102;&#31283;&#23450;&#27807;&#36890;&#30340;&#31574;&#30053;-&#24037;&#20855;&#12289;&#25216;&#26415;&#21644;&#25919;&#31574;&#12290;&#25105;&#20204;&#35752;&#35770;&#30340;&#31574;&#30053;&#21487;&#20998;&#20026;&#20004;&#20010;&#22823;&#31867;&#12290;&#36943;&#21046;&#31574;&#30053;&#26088;&#22312;&#22312;&#24403;&#21069;&#34987;&#23041;&#32961;&#19978;&#19979;&#25991;&#33258;&#30001;&#30340;&#26399;&#26395;&#21644;&#35268;&#33539;&#30340;&#29615;&#22659;&#20013;&#37325;&#26032;&#30830;&#23450;&#19978;&#19979;&#25991;-&#36825;&#26159;&#23545;&#20114;&#32852;&#32593;&#25152;&#24314;&#31435;&#30340;&#26080;&#19978;&#19979;&#25991;&#26399;&#26395;&#21644;&#35268;&#33539;&#30340;&#19968;&#31181;&#21453;&#24212;&#12290;&#30456;&#21453;&#65292;&#21160;&#21592;&#31574;&#30053;&#23558;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#23835;&#36215;&#35270;&#20026;&#22312;&#23186;&#20307;&#27807;&#36890;&#20013;&#20027;&#21160;&#24314;&#31435;&#38544;&#31169;&#21644;&#30495;&#23454;&#24615;&#26032;&#26399;&#26395;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models perturb the foundations of effective human communication. They present new challenges to contextual confidence, disrupting participants' ability to identify the authentic context of communication and their ability to protect communication from reuse and recombination outside its intended context. In this paper, we describe strategies--tools, technologies and policies--that aim to stabilize communication in the face of these challenges. The strategies we discuss fall into two broad categories. Containment strategies aim to reassert context in environments where it is currently threatened--a reaction to the context-free expectations and norms established by the internet. Mobilization strategies, by contrast, view the rise of generative AI as an opportunity to proactively set new and higher expectations around privacy and authenticity in mediated communication.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;EMR&#25968;&#25454;&#38598;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#36807;&#28193;&#27169;&#22411;&#35299;&#20915;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.07799</link><description>&lt;p&gt;
&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;EMR&#25968;&#25454;&#38598;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets. (arXiv:2310.07799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;EMR&#25968;&#25454;&#38598;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#36807;&#28193;&#27169;&#22411;&#35299;&#20915;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#26032;&#20852;&#30142;&#30149;&#30340;&#20449;&#24687;&#26377;&#38480;&#65292;&#30151;&#29366;&#24456;&#38590;&#34987;&#23519;&#35273;&#21644;&#35748;&#35782;&#21040;&#65292;&#22240;&#27492;&#21487;&#33021;&#24573;&#35270;&#20020;&#24202;&#24178;&#39044;&#30340;&#31383;&#21475;&#12290;&#26399;&#26395;&#33021;&#22815;&#24314;&#31435;&#19968;&#20010;&#26377;&#25928;&#30340;&#39044;&#21518;&#27169;&#22411;&#65292;&#36741;&#21161;&#21307;&#29983;&#36827;&#34892;&#27491;&#30830;&#35786;&#26029;&#21644;&#21046;&#23450;&#20010;&#24615;&#21270;&#27835;&#30103;&#26041;&#26696;&#65292;&#20174;&#32780;&#21450;&#26102;&#39044;&#38450;&#19981;&#21033;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#30142;&#30149;&#26089;&#26399;&#38454;&#27573;&#65292;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#21644;&#20020;&#24202;&#32463;&#39564;&#26377;&#38480;&#65292;&#20877;&#21152;&#19978;&#23545;&#38544;&#31169;&#21644;&#20262;&#29702;&#30340;&#32771;&#34385;&#65292;&#23548;&#33268;&#21487;&#20379;&#21442;&#32771;&#30340;&#25968;&#25454;&#21463;&#38480;&#65292;&#29978;&#33267;&#38590;&#20197;&#27491;&#30830;&#26631;&#35760;&#25968;&#25454;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30142;&#30149;&#25110;&#21516;&#19968;&#30142;&#30149;&#19981;&#21516;&#26469;&#28304;&#30340;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65288;EMR&#65289;&#25968;&#25454;&#21487;&#33021;&#23384;&#22312;&#20005;&#37325;&#30340;&#36328;&#25968;&#25454;&#38598;&#29305;&#24449;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20005;&#37325;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24314;&#31435;&#19968;&#20010;&#20174;&#28304;&#25968;&#25454;&#38598;&#21040;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#36807;&#28193;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#36827;&#34892;&#32422;&#26463;&#65292;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the limited information about emerging diseases, symptoms are hard to be noticed and recognized, so that the window for clinical intervention could be ignored. An effective prognostic model is expected to assist doctors in making right diagnosis and designing personalized treatment plan, so to promptly prevent unfavorable outcomes. However, in the early stage of a disease, limited data collection and clinical experiences, plus the concern out of privacy and ethics, may result in restricted data availability for reference, to the extent that even data labels are difficult to mark correctly. In addition, Electronic Medical Record (EMR) data of different diseases or of different sources of the same disease can prove to be having serious cross-dataset feature misalignment problems, greatly mutilating the efficiency of deep learning models. This article introduces a transfer learning method to build a transition model from source dataset to target dataset. By way of constraining the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.05207</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12289;&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#26469;&#25552;&#39640;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#30340;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914;&#20309;&#23558;&#22823;&#37327;&#30340;&#22312;&#37326;&#38750;&#26631;&#35760;&#38754;&#37096;&#22270;&#20687;&#24341;&#20837;&#30417;&#30563;&#24335;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#20013;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AU&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21516;&#26500;&#38754;&#37096;&#25552;&#21462;&#27169;&#22359;&#30340;&#21442;&#25968;&#65292;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#21516;&#26102;&#23398;&#20064;AU&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#20197;&#21450;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#29305;&#24449;&#23545;&#40784;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#22120;&#21644;&#25913;&#36827;&#30340;&#23545;&#27604;&#25439;&#22833;&#28155;&#21152;&#20102;&#22235;&#20010;&#39069;&#22806;&#30340;&#20013;&#38388;&#30417;&#30563;&#22120;&#26469;&#20419;&#36827;&#29305;&#24449;&#37325;&#24314;&#30340;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#25512;&#33616;&#22242;&#38431;&#65292;&#20351;&#24471;&#27599;&#20010;&#22242;&#38431;&#33021;&#22815;&#28385;&#36275;&#39033;&#30446;&#35201;&#27714;&#30340;&#25216;&#33021;&#35206;&#30422;&#65292;&#24182;&#19988;&#24179;&#34913;&#20505;&#36873;&#25104;&#21592;&#20043;&#38388;&#30340;&#24037;&#20316;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2309.09404</link><description>&lt;p&gt;
&#29992;&#24320;&#25918;&#25968;&#25454;&#39537;&#21160;&#30340;&#22242;&#38431;&#25512;&#33616;&#20419;&#36827;&#30740;&#31350;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals. (arXiv:2309.09404v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09404
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#25512;&#33616;&#22242;&#38431;&#65292;&#20351;&#24471;&#27599;&#20010;&#22242;&#38431;&#33021;&#22815;&#28385;&#36275;&#39033;&#30446;&#35201;&#27714;&#30340;&#25216;&#33021;&#35206;&#30422;&#65292;&#24182;&#19988;&#24179;&#34913;&#20505;&#36873;&#25104;&#21592;&#20043;&#38388;&#30340;&#24037;&#20316;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22242;&#38431;&#24314;&#35774;&#21644;&#20419;&#36827;&#21512;&#20316;&#26159;&#38750;&#24120;&#24120;&#35265;&#30340;&#21830;&#19994;&#27963;&#21160;&#12290;&#19968;&#20010;&#20363;&#23376;&#23601;&#26159;TeamingForFunding&#38382;&#39064;&#65292;&#30740;&#31350;&#26426;&#26500;&#21644;&#30740;&#31350;&#20154;&#21592;&#22312;&#21521;&#36164;&#21161;&#26426;&#26500;&#30003;&#35831;&#26102;&#65292;&#24076;&#26395;&#33021;&#22815;&#25214;&#21040;&#21512;&#20316;&#26426;&#20250;&#20197;&#22238;&#24212;&#21518;&#32773;&#30340;&#39033;&#30446;&#30003;&#35831;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#25512;&#33616;&#22242;&#38431;&#65292;&#20351;&#24471;&#27599;&#20010;&#22242;&#38431;&#37117;&#33021;&#22815;&#36798;&#21040;&#26426;&#20250;&#35201;&#27714;&#30340;&#26368;&#39640;&#25216;&#33021;&#35206;&#30422;&#65292;&#24182;&#19988;&#20505;&#36873;&#25104;&#21592;&#20043;&#38388;&#30340;&#24037;&#20316;&#20998;&#37197;&#26159;&#24179;&#34913;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#21462;&#24320;&#25918;&#25968;&#25454;&#20013;&#30340;&#39033;&#30446;&#30003;&#35831;&#65288;&#38656;&#27714;&#65289;&#21644;&#30740;&#31350;&#20154;&#21592;&#31616;&#20171;&#65288;&#20379;&#32473;&#65289;&#20013;&#30340;&#25216;&#33021;&#28508;&#21147;&#65292;&#20351;&#29992;&#20998;&#31867;&#27861;&#23545;&#20854;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#21305;&#37197;&#38656;&#27714;&#21644;&#20379;&#32473;&#12290;&#25105;&#20204;&#21019;&#24314;&#22242;&#38431;&#20197;&#26368;&#22823;&#21270;&#19968;&#20010;&#26032;&#30340;&#34913;&#37327;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#30340;&#24230;&#37327;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23450;&#37327;&#39564;&#35777;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#25104;&#21151;&#65292;&#36890;&#36807;&#8230;
&lt;/p&gt;
&lt;p&gt;
Building teams and promoting collaboration are two very common business activities. An example of these are seen in the TeamingForFunding problem, where research institutions and researchers are interested to identify collaborative opportunities when applying to funding agencies in response to latter's calls for proposals. We describe a novel system to recommend teams using a variety of AI methods, such that (1) each team achieves the highest possible skill coverage that is demanded by the opportunity, and (2) the workload of distributing the opportunities is balanced amongst the candidate members. We address these questions by extracting skills latent in open data of proposal calls (demand) and researcher profiles (supply), normalizing them using taxonomies, and creating efficient algorithms that match demand to supply. We create teams to maximize goodness along a novel metric balancing short- and long-term objectives. We validate the success of our algorithms (1) quantitatively, by e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04160</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#26469;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#32463;&#24120;&#21576;&#29616;&#31232;&#30095;&#29305;&#24449;&#65292;&#32473;&#39044;&#27979;&#24314;&#27169;&#24102;&#26469;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#30452;&#25509;&#25554;&#34917;&#26041;&#27861;&#65288;&#22914;&#30697;&#38453;&#25554;&#34917;&#26041;&#27861;&#65289;&#20381;&#36182;&#20110;&#21442;&#32771;&#31867;&#20284;&#34892;&#25110;&#21015;&#26469;&#23436;&#25104;&#21407;&#22987;&#32570;&#22833;&#25968;&#25454;&#65292;&#19981;&#21306;&#20998;&#25554;&#34917;&#21644;&#23454;&#38469;&#20540;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23558;&#19982;&#39044;&#27979;&#30446;&#26631;&#26080;&#20851;&#30340;&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20174;&#32780;&#25439;&#23475;&#19979;&#28216;&#24615;&#33021;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#22312;&#30452;&#25509;&#25554;&#34917;&#21518;&#37325;&#26032;&#26657;&#20934;&#25110;&#22686;&#24378;EHR&#23884;&#20837;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38169;&#35823;&#22320;&#20248;&#20808;&#32771;&#34385;&#25554;&#34917;&#29305;&#24449;&#12290;&#36825;&#31181;&#20248;&#20808;&#38169;&#35823;&#21487;&#33021;&#20250;&#32473;&#27169;&#22411;&#24341;&#20837;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37319;&#29992;&#38388;&#25509;&#25554;&#34917;&#65292;&#21033;&#29992;&#31867;&#20284;&#24739;&#32773;&#30340;&#21407;&#22411;&#34920;&#31034;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#12290;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#26102;&#36890;&#24120;&#23558;&#32570;&#22833;&#29305;&#24449;&#19982;&#23384;&#22312;&#29305;&#24449;&#30456;&#21516;&#30340;&#38480;&#21046;&#26102;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#65292;&#37319;&#29992;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.03251</link><description>&lt;p&gt;
&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning. (arXiv:2309.03251v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#65292;&#37319;&#29992;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#26159;&#20256;&#32479;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#30340;&#25193;&#23637;&#65292;&#34701;&#20837;&#20102;&#26102;&#38388;&#32500;&#24230;&#12290;&#22312;TKGs&#19978;&#36827;&#34892;&#25512;&#29702;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20107;&#20214;&#39044;&#27979;&#26410;&#26469;&#20107;&#23454;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#25581;&#31034;&#21382;&#21490;&#23376;&#22270;&#21644;&#26102;&#38388;&#27169;&#24335;&#20013;&#30340;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#38752;&#23454;&#20307;&#24314;&#27169;&#26469;&#27169;&#25311;TKGs&#65292;&#22240;&#20026;&#22270;&#20013;&#30340;&#33410;&#28857;&#22312;&#30693;&#35782;&#34920;&#31034;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#22330;&#26223;&#36890;&#24120;&#28041;&#21450;&#22823;&#37327;&#23454;&#20307;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20250;&#20986;&#29616;&#26032;&#23454;&#20307;&#12290;&#36825;&#20351;&#24471;&#20381;&#36182;&#20110;&#23454;&#20307;&#30340;&#26041;&#27861;&#24456;&#38590;&#24212;&#23545;&#22823;&#37327;&#23454;&#20307;&#65292;&#24182;&#19988;&#26377;&#25928;&#22788;&#29702;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#20063;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#65292;&#23427;&#20197;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#23545;&#21382;&#21490;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TiPNN&#37319;&#29992;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#65292;&#21517;&#20026;&#21382;&#21490;&#26102;&#38388;&#22270;&#65292;&#26469;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph (KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial task that aims to predict future facts based on historical occurrences. The key challenge lies in uncovering structural dependencies within historical subgraphs and temporal patterns. Most existing approaches model TKGs relying on entity modeling, as nodes in the graph play a crucial role in knowledge representation. However, the real-world scenario often involves an extensive number of entities, with new entities emerging over time. This makes it challenging for entity-dependent methods to cope with extensive volumes of entities, and effectively handling newly emerging entities also becomes a significant challenge. Therefore, we propose Temporal Inductive Path Neural Network (TiPNN), which models historical information in an entity-independent perspective. Specifically, TiPNN adopts a unified graph, namely history temporal graph, to
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12243</link><description>&lt;p&gt;
&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#65292;&#20250;&#33258;&#28982;&#22320;&#20986;&#29616;&#19981;&#21516;&#30340;&#20914;&#31361;&#20248;&#21270;&#20934;&#21017;&#12290;&#36825;&#20123;&#20934;&#21017;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#30340;&#20027;&#20219;&#21153;&#65288;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65289;&#65292;&#20063;&#21487;&#20197;&#35299;&#20915;&#20027;&#35201;&#20219;&#21153;&#21644;&#27425;&#35201;&#20219;&#21153;&#65292;&#20363;&#22914;&#25439;&#22833;&#26368;&#23567;&#21270;&#19982;&#31232;&#30095;&#24615;&#12290;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#31616;&#21333;&#22320;&#21152;&#26435;&#20934;&#21017;&#65292;&#20294;&#22312;&#20984;&#35774;&#32622;&#20013;&#25165;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#23545;&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#25913;&#36827;&#30340;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26631;&#37327;&#21270;&#25216;&#26415;&#65292;&#31639;&#27861;&#21487;&#20197;&#35782;&#21035;&#21407;&#22987;&#38382;&#39064;&#30340;&#25152;&#26377;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#23558;&#20854;&#22797;&#26434;&#24615;&#38477;&#20302;&#20026;&#19968;&#31995;&#21015;&#21333;&#30446;&#26631;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#22686;&#24191;Lagrangian&#26041;&#27861;&#26469;&#35299;&#20915;&#31616;&#21270;&#21518;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#22914;Adam&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#22788;&#29702;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#35299;&#20915;&#32463;&#27982;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20869;&#37096;&#30340;&#20540;&#31354;&#38388;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01154</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#36816;&#31639;&#65306;&#20174;&#35760;&#24518;&#21040;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Arithmetic with Language Models: from Memorization to Computation. (arXiv:2308.01154v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20869;&#37096;&#30340;&#20540;&#31354;&#38388;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26356;&#22909;&#22320;&#29702;&#35299;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#24615;&#35745;&#31639;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#23545;&#20110;&#36827;&#19968;&#27493;&#25913;&#36827;&#23427;&#20204;&#24182;&#25299;&#23485;&#20854;&#36866;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22312;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#12290;&#20108;&#36827;&#21046;&#21152;&#27861;&#21644;&#20056;&#27861;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#27979;&#35797;&#22522;&#30784;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#38750;&#24120;&#23567;&#30340;&#35789;&#27719;&#34920;&#65292;&#24182;&#19988;&#22312;&#36755;&#20837;/&#36755;&#20986;&#19978;&#23637;&#31034;&#20102;&#30456;&#20851;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#20351;&#24471;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#24179;&#28369;&#30340;&#36755;&#20837;&#25554;&#20540;&#26080;&#25928;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#23398;&#20064;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20854;&#22806;&#25512;&#33021;&#21147;&#21644;&#20869;&#37096;&#20449;&#24687;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#36825;&#26679;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#32534;&#30721;-&#22238;&#24402;-&#35299;&#30721;&#26426;&#22120;&#65292;&#19968;&#26086;&#23558;&#36755;&#20837;&#26631;&#35760;&#34920;&#31034;&#26144;&#23556;&#21040;&#21512;&#36866;&#30340;&#20869;&#37096;&#20540;&#31354;&#38388;&#65292;&#35745;&#31639;&#23601;&#22312;&#20540;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypotheses that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate intern
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;KoBBQ&#65292;&#19968;&#20010;&#38024;&#23545;&#38889;&#22269;&#25991;&#21270;&#30340;&#20559;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#30340;&#25991;&#21270;&#36866;&#24212;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#35843;&#26597;&#25910;&#38598;&#21644;&#39564;&#35777;&#20102;&#21453;&#26144;&#38889;&#22269;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#30340;&#31038;&#20250;&#20559;&#35265;&#21644;&#20559;&#35265;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.16778</link><description>&lt;p&gt;
KoBBQ: &#38024;&#23545;&#38382;&#31572;&#30340;&#38889;&#22269;&#20559;&#35265;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KoBBQ: Korean Bias Benchmark for Question Answering. (arXiv:2307.16778v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;KoBBQ&#65292;&#19968;&#20010;&#38024;&#23545;&#38889;&#22269;&#25991;&#21270;&#30340;&#20559;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#30340;&#25991;&#21270;&#36866;&#24212;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#35843;&#26597;&#25910;&#38598;&#21644;&#39564;&#35777;&#20102;&#21453;&#26144;&#38889;&#22269;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#30340;&#31038;&#20250;&#20559;&#35265;&#21644;&#20559;&#35265;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#35265;&#38382;&#31572;&#30340;&#22522;&#20934;&#65288;BBQ&#65289;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20294;&#26159;&#23558;&#27492;&#22522;&#20934;&#36866;&#24212;&#20110;&#32654;&#22269;&#20197;&#22806;&#30340;&#25991;&#21270;&#32972;&#26223;&#24182;&#19981;&#31616;&#21333;&#65292;&#22240;&#20026;&#31038;&#20250;&#20559;&#35265;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25991;&#21270;&#32972;&#26223;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;KoBBQ&#65292;&#19968;&#20010;&#38889;&#22269;&#20559;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#30340;&#25991;&#21270;&#36866;&#24212;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;BBQ&#25968;&#25454;&#38598;&#20998;&#20026;&#19977;&#31867;&#8212;&#8212;&#31616;&#21333;&#36716;&#25442;&#65288;&#21487;&#20197;&#22312;&#25991;&#21270;&#32763;&#35793;&#21518;&#30452;&#25509;&#20351;&#29992;&#65289;&#12289;&#30446;&#26631;&#20462;&#25913;&#65288;&#38656;&#35201;&#22312;&#30446;&#26631;&#32676;&#20307;&#20013;&#36827;&#34892;&#26412;&#22320;&#21270;&#65289;&#21644;&#26679;&#26412;&#21024;&#38500;&#65288;&#19981;&#36866;&#21512;&#38889;&#22269;&#25991;&#21270;&#65289;&#65292;&#24182;&#28155;&#21152;&#20102;&#22235;&#20010;&#38024;&#23545;&#38889;&#22269;&#25991;&#21270;&#29305;&#23450;&#30340;&#20559;&#35265;&#31867;&#21035;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#35843;&#26597;&#65292;&#25910;&#38598;&#21644;&#39564;&#35777;&#20102;&#21453;&#26144;&#38889;&#22269;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#30340;&#31038;&#20250;&#20559;&#35265;&#21644;&#20559;&#35265;&#30446;&#26631;&#12290;&#26368;&#32456;&#30340;KoBBQ&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;268&#20010;&#27169;&#26495;&#21644;76,048&#20010;&#26679;&#26412;&#65292;&#28085;&#30422;&#20102;12&#20010;&#31038;&#20250;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bias Benchmark for Question Answering (BBQ) is designed to evaluate social biases of language models (LMs), but it is not simple to adapt this benchmark to cultural contexts other than the US because social biases depend heavily on the cultural context. In this paper, we present KoBBQ, a Korean bias benchmark dataset, and we propose a general framework that addresses considerations for cultural adaptation of a dataset. Our framework includes partitioning the BBQ dataset into three classes--Simply-Transferred (can be used directly after cultural translation), Target-Modified (requires localization in target groups), and Sample-Removed (does not fit Korean culture)-- and adding four new categories of bias specific to Korean culture. We conduct a large-scale survey to collect and validate the social biases and the targets of the biases that reflect the stereotypes in Korean culture. The resulting KoBBQ dataset comprises 268 templates and 76,048 samples across 12 categories of social b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#24615;&#27133;&#23383;&#20856;&#65288;PSD&#65289;&#23454;&#29616;&#20102;&#19987;&#38376;&#30340;&#27133;&#20301;&#32465;&#23450;&#21644;&#29289;&#20307;&#23618;&#27425;&#35843;&#33410;&#20998;&#24067;&#65292;&#22312;&#22810;&#20010;&#21518;&#32493;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.09437</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#29992;&#20110;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Conditional Slot Attention for Object Centric Learning. (arXiv:2307.09437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#24615;&#27133;&#23383;&#20856;&#65288;PSD&#65289;&#23454;&#29616;&#20102;&#19987;&#38376;&#30340;&#27133;&#20301;&#32465;&#23450;&#21644;&#29289;&#20307;&#23618;&#27425;&#35843;&#33410;&#20998;&#24067;&#65292;&#22312;&#22810;&#20010;&#21518;&#32493;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21462;&#29289;&#20307;&#23618;&#27425;&#30340;&#34920;&#31034;&#20197;&#36827;&#34892;&#21518;&#32493;&#30340;&#25512;&#29702;&#20219;&#21153;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#28044;&#29616;&#30340;&#19968;&#20010;&#39046;&#22495;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#35774;&#32622;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23558;&#20219;&#24847;&#25968;&#37327;&#30340;&#29289;&#20307;&#23454;&#20363;&#32465;&#23450;&#21040;&#19987;&#38376;&#30340;&#29289;&#20307;&#27133;&#20301;&#12290;&#26368;&#36817;&#30340;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#26041;&#27861;&#22914;&#27133;&#20301;&#27880;&#24847;&#21147;&#21033;&#29992;&#36845;&#20195;&#24335;&#27880;&#24847;&#21147;&#23398;&#20064;&#20855;&#26377;&#21160;&#24577;&#25512;&#29702;&#23618;&#32423;&#32465;&#23450;&#30340;&#21487;&#32452;&#21512;&#34920;&#31034;&#65292;&#20294;&#26410;&#33021;&#36798;&#21040;&#19987;&#38376;&#30340;&#27133;&#20301;&#32465;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26032;&#39062;&#30340;&#27010;&#29575;&#24615;&#27133;&#23383;&#20856;&#65288;PSD&#65289;&#30340;&#26080;&#30417;&#30563;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#23558;PSD&#23450;&#20041;&#20026;&#65288;i&#65289;&#25277;&#35937;&#30340;&#29289;&#20307;&#23618;&#27425;&#23646;&#24615;&#21521;&#37327;&#20316;&#20026;&#38190;&#65292;&#65288;ii&#65289;&#21442;&#25968;&#21270;&#39640;&#26031;&#20998;&#24067;&#20316;&#20026;&#30456;&#24212;&#30340;&#20540;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#21518;&#32493;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#20855;&#20307;&#29289;&#20307;&#23618;&#27425;&#35843;&#33410;&#20998;&#24067;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#29289;&#20307;&#21457;&#29616;&#12289;&#32452;&#21512;&#24335;&#22330;&#26223;&#29983;&#25104;&#21644;&#32452;&#21512;&#24335;&#35270;&#35273;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting object-level representations for downstream reasoning tasks is an emerging area in AI. Learning object-centric representations in an unsupervised setting presents multiple challenges, a key one being binding an arbitrary number of object instances to a specialized object slot. Recent object-centric representation methods like Slot Attention utilize iterative attention to learn composable representations with dynamic inference level binding but fail to achieve specialized slot level binding. To address this, in this paper we propose Unsupervised Conditional Slot Attention using a novel Probabilistic Slot Dictionary (PSD). We define PSD with (i) abstract object-level property vectors as key and (ii) parametric Gaussian distribution as its corresponding value. We demonstrate the benefits of the learnt specific object-level conditioning distributions in multiple downstream tasks, namely object discovery, compositional scene generation, and compositional visual reasoning. We show
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DyGATAD&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#38598;&#20307;&#24322;&#24120;&#34892;&#20026;&#65292;&#20854;&#20013;&#24322;&#24120;&#34892;&#20026;&#21487;&#33021;&#30001;&#20110;&#31995;&#32479;&#20869;&#37096;&#30456;&#20114;&#20851;&#31995;&#30340;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#30417;&#25511;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.03761</link><description>&lt;p&gt;
&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks. (arXiv:2307.03761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03761
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DyGATAD&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#38598;&#20307;&#24322;&#24120;&#34892;&#20026;&#65292;&#20854;&#20013;&#24322;&#24120;&#34892;&#20026;&#21487;&#33021;&#30001;&#20110;&#31995;&#32479;&#20869;&#37096;&#30456;&#20114;&#20851;&#31995;&#30340;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#30417;&#25511;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#26102;&#20195;&#65292;&#30001;&#24037;&#19994;&#29289;&#32852;&#32593; (IIoT) &#30417;&#25511;&#30340;&#31995;&#32479;&#36890;&#36807;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#29983;&#25104;&#22823;&#37327;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015; (MTS) &#25968;&#25454;&#12290;&#34429;&#28982;&#36825;&#20123;&#25968;&#25454;&#26377;&#21161;&#20110;&#26465;&#20214;&#30417;&#25511;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#20294;&#26159;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#26085;&#30410;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20851;&#31995;&#20063;&#32473;&#24322;&#24120;&#26816;&#27979;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#24322;&#24120;&#21644;&#32972;&#26223;&#24322;&#24120;&#65292;&#23545;&#38598;&#20307;&#24322;&#24120;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#38598;&#20307;&#24322;&#24120;&#30340;&#19968;&#31181;&#24120;&#35265;&#21464;&#31181;&#26159;&#24322;&#24120;&#38598;&#20307;&#34892;&#20026;&#30001;&#31995;&#32479;&#20869;&#37096;&#30340;&#30456;&#20114;&#20851;&#31995;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#24322;&#24120;&#29615;&#22659;&#26465;&#20214;&#65288;&#22914;&#36807;&#28909;&#65289;&#12289;&#30001;&#20110;&#32593;&#32476;&#25915;&#20987;&#36896;&#25104;&#30340;&#19981;&#27491;&#30830;&#25805;&#20316;&#35774;&#32622;&#25110;&#31995;&#32479;&#32423;&#25925;&#38556;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; DyGATAD&#65288;&#19968;&#31181;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65289;&#65292;&#37319;&#29992;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of digital transformation, systems monitored by the Industrial Internet of Things (IIoTs) generate large amounts of Multivariate Time Series (MTS) data through heterogeneous sensor networks. While this data facilitates condition monitoring and anomaly detection, the increasing complexity and interdependencies within the sensor network pose significant challenges for anomaly detection. Despite progress in this field, much of the focus has been on point anomalies and contextual anomalies, with lesser attention paid to collective anomalies. A less addressed but common variant of collective anomalies is when the abnormal collective behavior is caused by shifts in interrelationships within the system. This can be due to abnormal environmental conditions like overheating, improper operational settings resulting from cyber-physical attacks, or system-level faults. To address these challenges, this paper proposes DyGATAD (Dynamic Graph Attention for Anomaly Detection), a graph-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#35757;&#32451;&#27169;&#22411;&#21644;&#25512;&#36827;&#39046;&#22495;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.16424</link><description>&lt;p&gt;
&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#29992;&#20110;&#21453;&#27927;&#38065;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Realistic Synthetic Financial Transactions for Anti-Money Laundering Models. (arXiv:2306.16424v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#35757;&#32451;&#27169;&#22411;&#21644;&#25512;&#36827;&#39046;&#22495;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37329;&#34701;&#30340;&#24191;&#27867;&#25968;&#23383;&#21270;&#21644;&#21152;&#23494;&#36135;&#24065;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#35774;&#35745;&#30340;&#27450;&#35784;&#26041;&#26696;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#27927;&#38065;&#8212;&#8212;&#23558;&#38750;&#27861;&#36164;&#37329;&#31227;&#21160;&#20197;&#25513;&#30422;&#20854;&#26469;&#28304;&#8212;&#8212;&#21487;&#20197;&#36328;&#36234;&#38134;&#34892;&#21644;&#22269;&#30028;&#65292;&#20135;&#29983;&#22797;&#26434;&#30340;&#20132;&#26131;&#27169;&#24335;&#12290;&#32852;&#21512;&#22269;&#20272;&#35745;&#27599;&#24180;&#20840;&#29699;&#27927;&#38065;&#37329;&#39069;&#21344;&#20840;&#29699;GDP&#30340;2-5%&#65292;&#32422;&#20026;0.8-2.0&#19975;&#20159;&#32654;&#20803;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36890;&#24120;&#26080;&#27861;&#33719;&#24471;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#27927;&#38065;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#19988;&#20043;&#21069;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#23384;&#22312;&#26174;&#33879;&#32570;&#38519;&#12290;&#20026;&#20102;&#27604;&#36739;&#27169;&#22411;&#24182;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#38656;&#35201;&#19968;&#20010;&#36924;&#30495;&#12289;&#26631;&#20934;&#21270;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#26681;&#25454;&#23454;&#38469;&#20132;&#26131;&#23613;&#21487;&#33021;&#22320;&#26657;&#20934;&#20102;&#36825;&#20010;&#22522;&#20110;&#20195;&#29702;&#30340;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread digitization of finance and the increasing popularity of cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering -- the movement of illicit funds to conceal their origins -- can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5\% of global GDP or \$0.8 - \$2.0 trillion dollars are laundered globally each year. Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area.  To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets. We have calibrated this agent-based generator to match real transactions as closely as possible and
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#36127;&#26679;&#26412;&#24230;&#37327;&#23398;&#20064;&#26694;&#26550;&#65288;P3M&#65289;&#29992;&#20110;&#20855;&#26377;&#19981;&#23436;&#25972;&#26631;&#27880;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65292;&#36890;&#36807;&#25289;&#36817;&#23454;&#20307;&#23884;&#20837;&#21644;&#20854;&#23545;&#24212;&#20851;&#31995;&#23884;&#20837;&#30340;&#36317;&#31163;&#65292;&#21516;&#26102;&#20351;&#20854;&#19982;&#38750;&#31867;&#21035;&#20851;&#31995;&#23884;&#20837;&#30340;&#36317;&#31163;&#25512;&#36828;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.14806</link><description>&lt;p&gt;
&#29992;&#20110;&#20855;&#26377;&#19981;&#23436;&#25972;&#26631;&#27880;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#27491;&#36127;&#26679;&#26412;&#24230;&#37327;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Positive-Unlabeled Metric Learning Framework for Document-Level Relation Extraction with Incomplete Labeling. (arXiv:2306.14806v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14806
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#36127;&#26679;&#26412;&#24230;&#37327;&#23398;&#20064;&#26694;&#26550;&#65288;P3M&#65289;&#29992;&#20110;&#20855;&#26377;&#19981;&#23436;&#25972;&#26631;&#27880;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65292;&#36890;&#36807;&#25289;&#36817;&#23454;&#20307;&#23884;&#20837;&#21644;&#20854;&#23545;&#24212;&#20851;&#31995;&#23884;&#20837;&#30340;&#36317;&#31163;&#65292;&#21516;&#26102;&#20351;&#20854;&#19982;&#38750;&#31867;&#21035;&#20851;&#31995;&#23884;&#20837;&#30340;&#36317;&#31163;&#25512;&#36828;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#36328;&#22810;&#20010;&#21477;&#23376;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26368;&#36817;&#65292;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#19981;&#23436;&#25972;&#26631;&#27880;&#38382;&#39064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#19968;&#20123;&#30740;&#31350;&#37319;&#29992;&#27491;&#36127;&#26679;&#26412;&#23398;&#20064;&#31561;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#26679;&#26412;&#22686;&#24378;&#21644;&#27491;&#26679;&#26412;&#28151;&#21512;&#30340;&#27491;&#36127;&#26679;&#26412;&#24230;&#37327;&#23398;&#20064;&#26694;&#26550;&#65288;P3M&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#24418;&#24335;&#21270;&#20026;&#24230;&#37327;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#26088;&#22312;&#25289;&#36817;&#23454;&#20307;&#23545;&#23884;&#20837;&#21644;&#23427;&#20204;&#23545;&#24212;&#30340;&#20851;&#31995;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#19982;&#38750;&#31867;&#21035;&#20851;&#31995;&#23884;&#20837;&#30340;&#36317;&#31163;&#25512;&#36828;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27491;&#36127;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#35813;&#25439;&#22833;&#30446;&#26631;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;dropout&#26469;&#22686;&#21152;&#27491;&#26679;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#27491;-&#38750;&#31867;&#21035;&#28151;&#21512;&#26041;&#27861;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;P3M&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of document-level relation extraction (RE) is to identify relations between entities that span multiple sentences. Recently, incomplete labeling in document-level RE has received increasing attention, and some studies have used methods such as positive-unlabeled learning to tackle this issue, but there is still a lot of room for improvement. Motivated by this, we propose a positive-augmentation and positive-mixup positive-unlabeled metric learning framework (P3M). Specifically, we formulate document-level RE as a metric learning problem. We aim to pull the distance closer between entity pair embedding and their corresponding relation embedding, while pushing it farther away from the none-class relation embedding. Additionally, we adapt the positive-unlabeled learning to this loss objective. In order to improve the generalizability of the model, we use dropout to augment positive samples and propose a positive-none-class mixup method. Extensive experiments show that P3M improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20219;&#21153;&#24230;&#37327;&#35774;&#35745;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13588</link><description>&lt;p&gt;
&#31995;&#32479;&#32423;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
System-Level Natural Language Feedback. (arXiv:2306.13588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20219;&#21153;&#24230;&#37327;&#35774;&#35745;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#21253;&#21547;&#20102;&#20016;&#23500;&#30340;&#29992;&#25143;&#20307;&#39564;&#20449;&#24687;&#12290;&#29616;&#26377;&#30740;&#31350;&#32858;&#28966;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#21453;&#39304;&#29992;&#20110;&#32454;&#21270;&#29305;&#23450;&#20363;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#31995;&#32479;&#33539;&#22260;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#26159;&#36890;&#36807;&#20197;&#19979;&#20004;&#26041;&#38754;&#23454;&#29616;&#30340;&#65306;(i) &#20219;&#21153;&#24230;&#37327;&#35774;&#35745;; (ii) &#29992;&#20110;&#25913;&#36827;&#27169;&#22411;&#21709;&#24212;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#26469;&#25913;&#36827;&#25628;&#32034;&#26597;&#35810;&#29983;&#25104;&#21644;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#32452;&#21512;&#24102;&#26469;&#20102;&#36827;&#19968;&#27493;&#30340;&#25910;&#30410;&#65292;&#24182;&#19988;&#30001;&#20154;&#31867;&#25776;&#20889;&#30340;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#23548;&#33268;&#27604;GPT-3.5&#25776;&#20889;&#30340;&#21453;&#39304;&#26356;&#21152;&#25166;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language (NL) feedback contains rich information about the user experience. Existing studies focus on an instance-level approach, where feedback is used to refine specific examples, disregarding its system-wide application. This paper proposes a general framework for unlocking the system-level use of NL feedback. We show how to use feedback to formalize system-level design decisions in a human-in-the-loop-process -- in order to produce better models. In particular this is done through: (i) metric design for tasks; and (ii) language model prompt design for refining model responses. We conduct two case studies of this approach for improving search query generation and dialog response generation, demonstrating the effectiveness of the use of system-level feedback. We show the combination of system-level feedback and instance-level feedback brings further gains, and that human written instance-level feedback results in more grounded refinements than GPT-3.5 written ones, underlying
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#36335;&#32447;&#22270;&#65292;&#36890;&#36807;&#19977;&#20010;&#26694;&#26550;&#65306;&#22686;&#24378;KGs&#30340;LLMs&#65292;&#30693;&#35782;&#22686;&#24378;KGs&#21644;LLMs&#19982;KGs&#30340;&#32852;&#21512;&#25512;&#29702;&#65292;&#32508;&#21512;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.08302</link><description>&lt;p&gt;
&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;: &#19968;&#26465;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Unifying Large Language Models and Knowledge Graphs: A Roadmap. (arXiv:2306.08302v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#36335;&#32447;&#22270;&#65292;&#36890;&#36807;&#19977;&#20010;&#26694;&#26550;&#65306;&#22686;&#24378;KGs&#30340;LLMs&#65292;&#30693;&#35782;&#22686;&#24378;KGs&#21644;LLMs&#19982;KGs&#30340;&#32852;&#21512;&#25512;&#29702;&#65292;&#32508;&#21512;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;GPT4&#27491;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#25472;&#36215;&#26032;&#30340;&#28909;&#28526;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#31361;&#29616;&#33021;&#21147;&#21644;&#19968;&#33324;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLM&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#24448;&#24448;&#19981;&#33021;&#25429;&#25417;&#21644;&#33719;&#21462;&#23454;&#38469;&#30693;&#35782;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#21326;&#26222;&#21017;&#26159;&#26126;&#30830;&#23384;&#20648;&#20016;&#23500;&#23454;&#38469;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#27169;&#22411;&#12290;KGs&#21487;&#20197;&#36890;&#36807;&#20026;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#22806;&#37096;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#12290;&#21516;&#26102;&#65292;KGs&#30340;&#26500;&#24314;&#22256;&#38590;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#28436;&#21270;&#65292;&#36825;&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;KGs&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#20107;&#23454;&#24182;&#34920;&#31034;&#26410;&#35265;&#36807;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#32479;&#19968;LLMs&#21644;KGs&#24182;&#21516;&#26102;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#28857;&#26159;&#26377;&#30410;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#32479;&#19968;LLMs&#21644;KGs&#30340;&#36335;&#32447;&#22270;&#12290;&#25105;&#20204;&#30340;&#36335;&#32447;&#22270;&#21253;&#25324;&#19977;&#20010;&#19968;&#33324;&#26694;&#26550;&#65292;&#21363;1&#65289;&#22686;&#24378;KGs&#30340;LLMs&#65292;&#23427;&#20204;&#23558;&#30693;&#35782;&#34920;&#31034;&#20026;LM&#30340;&#19968;&#37096;&#20998;&#65292;&#20174;&#32780;&#33021;&#22815;&#25429;&#25417;&#20016;&#23500;&#30340;&#23454;&#20307;&#20851;&#31995;&#65292;2&#65289;&#30693;&#35782;&#22686;&#24378;KGs&#65292;&#23427;&#20204;&#23558;LLMs&#29992;&#20316;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#20248;&#31168;&#24037;&#20855;&#65292;3&#65289;LLMs&#19982;KGs&#30340;&#32852;&#21512;&#25512;&#29702;&#65292;&#20854;&#20013;LLMs&#21644;KGs&#30456;&#20114;&#22686;&#24378;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorpo
&lt;/p&gt;</description></item><item><title>GateON&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#27963;&#21160;&#38376;&#25511;&#21644;&#21442;&#25968;&#30456;&#20851;&#24615;&#30340;&#22312;&#32447;&#20272;&#35745;&#26469;&#38450;&#27490;&#37325;&#35201;&#30693;&#35782;&#34987;&#35206;&#30422;&#65292;&#21516;&#26102;&#36890;&#36807;&#23450;&#28857;&#31070;&#32463;&#20803;&#30340;&#37325;&#26032;&#28608;&#27963;&#26426;&#21046;&#35299;&#20915;&#20102;&#32593;&#32476;&#39281;&#21644;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01690</link><description>&lt;p&gt;
GateON: &#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GateON: an unsupervised method for large scale continual learning. (arXiv:2306.01690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01690
&lt;/p&gt;
&lt;p&gt;
GateON&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#27963;&#21160;&#38376;&#25511;&#21644;&#21442;&#25968;&#30456;&#20851;&#24615;&#30340;&#22312;&#32447;&#20272;&#35745;&#26469;&#38450;&#27490;&#37325;&#35201;&#30693;&#35782;&#34987;&#35206;&#30422;&#65292;&#21516;&#26102;&#36890;&#36807;&#23450;&#28857;&#31070;&#32463;&#20803;&#30340;&#37325;&#26032;&#28608;&#27963;&#26426;&#21046;&#35299;&#20915;&#20102;&#32593;&#32476;&#39281;&#21644;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#30446;&#26631;&#26159;&#22312;&#19981;&#23545;&#26089;&#26399;&#20219;&#21153;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25353;&#39034;&#24207;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#32463;&#36807;CL&#35757;&#32451;&#21518;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;'Gate and Obstruct Network'&#65288;GateON&#65289;&#12290;GateON&#23558;&#21487;&#23398;&#20064;&#30340;&#27963;&#21160;&#38376;&#25511;&#19982;&#21442;&#25968;&#30456;&#20851;&#24615;&#30340;&#22312;&#32447;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#20197;&#38450;&#27490;&#37325;&#35201;&#30693;&#35782;&#34987;&#35206;&#30422;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20219;&#21153;&#20043;&#38388;&#29983;&#25104;&#37096;&#20998;&#37325;&#21472;&#30340;&#36335;&#24452;&#65292;&#20801;&#35768;&#22312;&#39034;&#24207;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#27491;&#21521;&#21644;&#21453;&#21521;&#36716;&#31227;&#12290;GateON&#36890;&#36807;&#23450;&#28857;&#31070;&#32463;&#20803;&#30340;&#37325;&#26032;&#28608;&#27963;&#26426;&#21046;&#26469;&#35299;&#20915;&#21442;&#25968;&#22266;&#23450;&#21518;&#32593;&#32476;&#39281;&#21644;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#12290;GateON&#36866;&#29992;&#20110;&#21508;&#31181;&#32593;&#32476;&#65288;&#20840;&#36830;&#25509;&#12289;CNN&#12289;Transformers&#65289;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#20302;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#39640;&#36798;100&#20010;MNIST&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;BERT&#20013;&#21462;&#24471;&#20102;&#39030;&#23574;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of continual learning (CL) is to learn tasks sequentially without retraining on earlier tasks. However, when subjected to CL, traditional neural networks exhibit catastrophic forgetting and limited generalization. To overcome these problems, we introduce a novel method called 'Gate and Obstruct Network' (GateON). GateON combines learnable gating of activity and online estimation of parameter relevance to safeguard crucial knowledge from being overwritten. Our method generates partially overlapping pathways between tasks which permits forward and backward transfer during sequential learning. GateON addresses the issue of network saturation after parameter fixation by a re-activation mechanism of fixed neurons, enabling large-scale continual learning. GateON is implemented on a wide range of networks (fully-connected, CNN, Transformers), has low computational complexity, effectively learns up to 100 MNIST learning tasks, and achieves top-tier results for pre-trained BERT in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#26694;&#26550;&#20013;&#22686;&#21152;&#19968;&#27493;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#24182;&#20943;&#23569;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#38388;&#38553;(TRFGap)&#12290;</title><link>http://arxiv.org/abs/2306.00789</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#38899;&#32763;&#35793;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Transfer Learning for Low-Resource Speech Translation. (arXiv:2306.00789v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#26694;&#26550;&#20013;&#22686;&#21152;&#19968;&#27493;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#24182;&#20943;&#23569;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#38388;&#38553;(TRFGap)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#23558;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#27493;&#39588;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20004;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;XLS-R&#20013;&#12290;&#36825;&#19968;&#39069;&#22806;&#30340;&#27493;&#39588;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#35821;&#38899;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#23545;&#22810;&#35821;&#35328;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#32534;&#30721;&#35821;&#20041;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#35299;&#20915;&#20102;XLS-R&#26694;&#26550;&#20013;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#30340;&#22823;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;CoVoST-2&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#21644;&#27604;&#36739;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#32763;&#35793;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#36801;&#31227;&#38388;&#38553;(TRFGap)&#26377;&#26126;&#26174;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents a novel three-step transfer learning framework for enhancing cross-lingual transfer from high- to low-resource languages in the downstream application of Automatic Speech Translation. The approach integrates a semantic knowledge-distillation step into the existing two-step cross-lingual transfer learning framework XLS-R. This extra step aims to encode semantic knowledge in the multilingual speech encoder pre-trained via Self-Supervised Learning using unlabeled speech. Our proposed three-step cross-lingual transfer learning framework addresses the large cross-lingual transfer gap (TRFGap) observed in the XLS-R framework between high-resource and low-resource languages. We validate our proposal through extensive experiments and comparisons on the CoVoST-2 benchmark, showing significant improvements in translation performance, especially for low-resource languages, and a notable reduction in the TRFGap.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04928</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#30417;&#30563;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20381;&#36182;&#20110;&#20855;&#26377;&#32473;&#23450;&#21629;&#21517;&#23454;&#20307;&#30340;&#22823;&#37327;&#27880;&#37322;&#25991;&#26412;&#65292;&#20854;&#21019;&#24314;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#25552;&#21462;&#26032;&#23454;&#20307;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#27880;&#37322;&#20219;&#21153;&#21644;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65288;&#26631;&#35760;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#25110;&#19981;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#65289;&#65292;&#24182;&#22312;&#26356;&#22810;&#30340;&#25968;&#25454;&#38598;&#21644;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#23398;&#20064;&#21040;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;9&#31181;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#65292;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;NER&#12289;&#19968;&#27425;&#26679;&#26412;NER&#12289;10&#27425;&#26679;&#26412;NER&#21644;100&#27425;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;&#20998;&#21035;&#20026;35.44&#65285;&#12289;50.10&#65285;&#12289;69.94&#65285;&#21644;79.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#39044;&#21069;&#32622;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#35268;&#27169;&#65292;&#24182;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13496</link><description>&lt;p&gt;
MAE&#39044;&#21069;&#32622;&#35757;&#32451;&#23545;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of MAE pre-pretraining for billion-scale pretraining. (arXiv:2303.13496v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#39044;&#21069;&#32622;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#35268;&#27169;&#65292;&#24182;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#30340;&#26631;&#20934;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#20351;&#29992;&#25968;&#21313;&#20159;&#24352;&#22270;&#20687;&#30340;&#22823;&#35268;&#27169;&#65288;&#24369;&#65289;&#30417;&#30563;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#39044;&#21069;&#32622;&#35757;&#32451;&#38454;&#27573;&#65292;&#23427;&#20351;&#29992;&#20102;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#26469;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;&#34429;&#28982;MAE&#25216;&#26415;&#20165;&#34987;&#35777;&#26126;&#33021;&#22815;&#19982;&#27169;&#22411;&#22823;&#23567;&#30456;&#32553;&#25918;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20063;&#21487;&#20197;&#38543;&#25968;&#25454;&#38598;&#22823;&#23567;&#32553;&#25918;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;MAE&#30340;&#39044;&#21069;&#32622;&#35757;&#32451;&#21487;&#21516;&#26102;&#36866;&#29992;&#20110;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#12290;&#39044;&#21069;&#32622;&#35757;&#32451;&#22312;&#19968;&#31995;&#21015;&#27169;&#22411;&#35268;&#27169;&#65288;&#21442;&#25968;&#25968;&#30334;&#19975;&#21040;&#25968;&#21313;&#20159;&#65289;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65288;&#22270;&#20687;&#25968;&#30334;&#19975;&#21040;&#25968;&#21313;&#20159;&#65289;&#19978;&#19968;&#33268;&#25552;&#39640;&#20102;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#65292;&#19988;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#20854;&#22312;10&#20010;&#19981;&#21516;&#30340;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#35270;&#39057;&#35782;&#21035;&#21644;&#30446;&#26631;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video reco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#23454;&#29616;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#29992;&#20110;&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.03106</link><description>&lt;p&gt;
&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#29992;&#20110;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Rotation Invariant Quantization for Model Compression. (arXiv:2303.03106v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#23454;&#29616;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#29992;&#20110;&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#21387;&#32553;&#26159;&#19968;&#31181;&#23558;&#22823;&#22411;&#12289;&#28040;&#32791;&#20869;&#23384;&#30340;&#27169;&#22411;&#37096;&#32626;&#21040;&#20869;&#23384;&#36164;&#28304;&#26377;&#38480;&#35774;&#22791;&#19978;&#30340;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;NN&#27169;&#22411;&#21387;&#32553;&#30340;&#36895;&#29575;-&#22833;&#30495;&#26435;&#34913;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#37327;&#21270;&#25972;&#20010;NN&#27169;&#22411;&#65292;&#22312;&#27599;&#20010;&#23618;&#27425;&#19978;&#24471;&#21040;&#19981;&#21516;&#30340;&#36895;&#29575;&#65292;&#21363;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26059;&#36716;&#19981;&#21464;&#37327;&#26041;&#27861;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23545;RIQ&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;RIQ&#22312;&#39044;&#35757;&#32451;&#30340;VGG&#31264;&#23494;&#21644;&#20462;&#21098;&#27169;&#22411;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;19.4&#20493;&#21644;52.9&#20493;&#30340;&#21387;&#32553;&#27604;&#65292;&#31934;&#24230;&#38477;&#20302;&#23567;&#20110;0.4%&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;\url{https://github.com/ehaleva/RIQ}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training Neural Network (NN) model compression is an attractive approach for deploying large, memory-consuming models on devices with limited memory resources. In this study, we investigate the rate-distortion tradeoff for NN model compression. First, we suggest a Rotation-Invariant Quantization (RIQ) technique that utilizes a single parameter to quantize the entire NN model, yielding a different rate at each layer, i.e., mixed-precision quantization. Then, we prove that our rotation-invariant approach is optimal in terms of compression. We rigorously evaluate RIQ and demonstrate its capabilities on various models and tasks. For example, RIQ facilitates $\times 19.4$ and $\times 52.9$ compression ratios on pre-trained VGG dense and pruned models, respectively, with $&lt;0.4\%$ accuracy degradation. Code is available in \url{https://github.com/ehaleva/RIQ}.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21307;&#23398;&#24433;&#20687;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2302.01622</link><description>&lt;p&gt;
&#31169;&#23494;&#12289;&#20844;&#24179;&#19988;&#31934;&#30830;&#65306;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#35757;&#32451;&#22823;&#35268;&#27169;&#38544;&#31169;&#20445;&#25252;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging. (arXiv:2302.01622v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21307;&#23398;&#24433;&#20687;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;&#39640;&#24230;&#25935;&#24863;&#24615;&#65292;&#38656;&#35201;&#37319;&#21462;&#29305;&#27530;&#25514;&#26045;&#30830;&#20445;&#20854;&#20445;&#25252;&#12290;&#20445;&#25252;&#38544;&#31169;&#30340;&#40644;&#37329;&#26631;&#20934;&#26159;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;DP&#23545;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#22312;&#21307;&#23398;&#20013;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#65292;&#24182;&#19988;&#26159;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23545;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;N=193,311&#65289;&#30340;&#39640;&#36136;&#37327;&#20020;&#24202;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#65292;&#21644;&#65288;2&#65289;&#19968;&#20010;&#25968;&#25454;&#38598;&#65288;N=1,625&#65289;&#30340;3D&#33145;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#65292;&#29992;&#20110;&#20998;&#31867;&#33008;&#33146;&#23548;&#31649;&#33146;&#30284;&#65288;PDAC&#65289;&#30340;&#23384;&#22312;&#12290;&#20004;&#20010;&#25968;&#25454;&#38598;&#22343;&#20026;&#22238;&#39038;&#24615;&#37319;&#38598;&#65292;&#24182;&#30001;&#32463;&#39564;&#20016;&#23500;&#30340;&#21307;&#23398;&#24433;&#20687;&#19987;&#23478;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) models are increasingly used in the medical domain. However, as medical data is highly sensitive, special precautions to ensure its protection are required. The gold standard for privacy preservation is the introduction of differential privacy (DP) to model training. Prior work indicates that DP has negative implications on model accuracy and fairness, which are unacceptable in medicine and represent a main barrier to the widespread use of privacy-preserving techniques. In this work, we evaluated the effect of privacy-preserving training of AI models regarding accuracy and fairness compared to non-private training. For this, we used two datasets: (1) A large dataset (N=193,311) of high quality clinical chest radiographs, and (2) a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC). Both were retrospectively collected and manually labeled by experienced radio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#20056;&#23458;&#30340;&#36215;&#28857;-&#32456;&#28857;&#65288;OD&#65289;&#27969;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21508;&#31181;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25429;&#25417;&#37325;&#22797;&#27169;&#24335;&#21644;&#19978;&#19979;&#25991;&#25968;&#25454;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20223;&#30495;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.02515</link><description>&lt;p&gt;
&#22522;&#20110;GNN&#30340;&#20056;&#23458;&#35831;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GNN-based Passenger Request Prediction. (arXiv:2301.02515v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#20056;&#23458;&#30340;&#36215;&#28857;-&#32456;&#28857;&#65288;OD&#65289;&#27969;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21508;&#31181;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25429;&#25417;&#37325;&#22797;&#27169;&#24335;&#21644;&#19978;&#19979;&#25991;&#25968;&#25454;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20223;&#30495;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20056;&#23458;&#35831;&#27714;&#39044;&#27979;&#23545;&#20110;&#20056;&#36710;&#20849;&#20139;&#24179;&#21488;&#30340;&#36816;&#33829;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#38656;&#27714;&#39044;&#27979;&#38382;&#39064;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20056;&#23458;&#30340;&#36215;&#28857;-&#32456;&#28857;&#65288;OD&#65289;&#27969;&#39044;&#27979;&#21364;&#22312;&#30740;&#31350;&#30028;&#20013;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#20056;&#23458;&#30340;OD&#27969;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#19981;&#21516;&#20301;&#32622;&#36215;&#22987;&#30340;&#35831;&#27714;&#20043;&#38388;&#20135;&#29983;&#30340;&#21508;&#31181;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25429;&#25417;&#20102;&#35813;&#22320;&#28857;&#30340;&#37325;&#22797;&#27169;&#24335;&#21644;&#19978;&#19979;&#25991;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#30830;&#23450;&#20102;&#35206;&#30422;&#36947;&#36335;&#32593;&#32476;&#24182;&#20445;&#25345;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#32593;&#26684;&#21333;&#20803;&#30340;&#26368;&#20339;&#22823;&#23567;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20223;&#30495;&#26469;&#26816;&#26597;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21450;&#20854;&#21508;&#20010;&#32452;&#20214;&#30340;&#29305;&#24449;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#27604;&#29616;&#26377;&#22522;&#32447;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passenger request prediction is essential for operations planning, control, and management in ride-sharing platforms. While the demand prediction problem has been studied extensively, the Origin-Destination (OD) flow prediction of passengers has received less attention from the research community. This paper develops a Graph Neural Network framework along with the Attention Mechanism to predict the OD flow of passengers. The proposed framework exploits various linear and non-linear dependencies that arise among requests originating from different locations and captures the repetition pattern and the contextual data of that place. Moreover, the optimal size of the grid cell that covers the road network and preserves the complexity and accuracy of the model is determined. Extensive simulations are conducted to examine the characteristics of our proposed approach and its various components. The results show the superior performance of our proposed model compared to the existing baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HyperSound&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#37325;&#26500;&#22768;&#27874;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.01839</link><description>&lt;p&gt;
HyperSound&#65306;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
HyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks. (arXiv:2211.01839v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01839
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HyperSound&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#37325;&#26500;&#22768;&#27874;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#20379;&#20102;&#22810;&#23186;&#20307;&#20449;&#21495;&#30340;&#26367;&#20195;&#34920;&#31034;&#26041;&#24335;&#12290;&#26368;&#36817;&#65292;INRs&#30340;&#24212;&#29992;&#21253;&#25324;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#12289;&#39640;&#32500;&#20449;&#21495;&#21387;&#32553;&#25110;3D&#28210;&#26579;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#38598;&#20013;&#22312;&#35270;&#35273;&#25968;&#25454;&#19978;&#65292;&#23558;&#23427;&#20204;&#36866;&#24212;&#21040;&#38899;&#39057;&#39046;&#22495;&#24182;&#19981;&#23481;&#26131;&#12290;&#27492;&#22806;&#65292;&#36825;&#38656;&#35201;&#20026;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperSound&#65292;&#19968;&#31181;&#21033;&#29992;&#36229;&#32593;&#32476;&#23454;&#29616;&#22312;&#35757;&#32451;&#26102;&#26410;&#35265;&#36807;&#30340;&#38899;&#39057;&#20449;&#21495;&#30340;INRs&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#37325;&#26500;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#22768;&#27874;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit neural representations (INRs) are a rapidly growing research field, which provides alternative ways to represent multimedia signals. Recent applications of INRs include image super-resolution, compression of high-dimensional signals, or 3D rendering. However, these solutions usually focus on visual data, and adapting them to the audio domain is not trivial. Moreover, it requires a separately trained model for every data sample. To address this limitation, we propose HyperSound, a meta-learning method leveraging hypernetworks to produce INRs for audio signals unseen at training time. We show that our approach can reconstruct sound waves with quality comparable to other state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32593;&#32476;&#35266;&#27979;&#25968;&#25454;&#20013;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#30340;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21152;&#26435;&#22238;&#24402;&#65288;DWR&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#27880;&#24847;&#26435;&#37325;&#26469;&#35299;&#20915;&#32593;&#32476;&#24178;&#25200;&#21644;&#24322;&#36136;&#24178;&#39044;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2210.14080</link><description>&lt;p&gt;
&#22312;&#32593;&#32476;&#20013;&#23398;&#20064;&#24322;&#36136;&#24178;&#39044;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Learning Individual Treatment Effects under Heterogeneous Interference in Networks. (arXiv:2210.14080v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32593;&#32476;&#35266;&#27979;&#25968;&#25454;&#20013;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#30340;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21152;&#26435;&#22238;&#24402;&#65288;DWR&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#27880;&#24847;&#26435;&#37325;&#26469;&#35299;&#20915;&#32593;&#32476;&#24178;&#25200;&#21644;&#24322;&#36136;&#24178;&#39044;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20174;&#32593;&#32476;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#24341;&#36215;&#20102;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#32593;&#32476;&#22330;&#26223;&#20013;&#65292;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#31283;&#23450;&#21333;&#20803;&#27835;&#30103;&#20215;&#20540;&#20551;&#35774;&#65288;SUTVA&#65289;&#30340;&#36829;&#21453;&#65292;&#35813;&#20551;&#35774;&#35748;&#20026;&#19968;&#20010;&#21333;&#20301;&#30340;&#27835;&#30103;&#20998;&#37197;&#19981;&#20250;&#24433;&#21709;&#20854;&#20182;&#20154;&#30340;&#32467;&#26524;&#12290;&#22312;&#32593;&#32476;&#25968;&#25454;&#20013;&#65292;&#30001;&#20110;&#24178;&#25200;&#65292;&#19968;&#20010;&#21333;&#20301;&#30340;&#32467;&#26524;&#19981;&#20165;&#21463;&#21040;&#20854;&#27835;&#30103;&#30340;&#24433;&#21709;&#65288;&#21363;&#30452;&#25509;&#25928;&#24212;&#65289;&#65292;&#36824;&#21463;&#21040;&#20854;&#20182;&#20154;&#30340;&#27835;&#30103;&#24433;&#21709;&#65288;&#21363;&#28322;&#20986;&#25928;&#24212;&#65289;&#12290;&#27492;&#22806;&#65292;&#20854;&#20182;&#21333;&#20301;&#30340;&#24433;&#21709;&#22987;&#32456;&#26159;&#24322;&#36136;&#30340;&#65288;&#20363;&#22914;&#65292;&#19982;&#20852;&#36259;&#30456;&#20284;&#30340;&#26379;&#21451;&#23545;&#19968;&#20010;&#20154;&#30340;&#24433;&#21709;&#19982;&#20852;&#36259;&#19981;&#21516;&#30340;&#26379;&#21451;&#19981;&#21516;&#65289;&#12290;&#26412;&#25991;&#38024;&#23545;&#22312;&#24322;&#36136;&#24178;&#25200;&#19979;&#20272;&#35745;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#65288;&#21253;&#25324;&#30452;&#25509;&#25928;&#24212;&#21644;&#28322;&#20986;&#25928;&#24212;&#65289;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21152;&#26435;&#22238;&#24402;&#65288;DWR&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#25429;&#25417;&#24322;&#36136;&#24178;&#39044;&#30340;&#27880;&#24847;&#26435;&#37325;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimates of individual treatment effects from networked observational data are attracting increasing attention these days. One major challenge in network scenarios is the violation of the stable unit treatment value assumption (SUTVA), which assumes that the treatment assignment of a unit does not influence others' outcomes. In network data, due to interference, the outcome of a unit is influenced not only by its treatment (i.e., direct effects) but also by others' treatments (i.e., spillover effects). Furthermore, the influences from other units are always heterogeneous (e.g., friends with similar interests affect a person differently than friends with different interests). In this paper, we focus on the problem of estimating individual treatment effects (both direct and spillover effects) under heterogeneous interference. To address this issue, we propose a novel Dual Weighting Regression (DWR) algorithm by simultaneously learning attention weights that capture the heterogeneous int
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#36951;&#25022;&#20445;&#35777;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#19978;&#30028;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2210.14051</link><description>&lt;p&gt;
&#35777;&#26126;&#20102;&#20998;&#24067;&#24335;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#19982;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#21487;&#35777;&#26126;&#36951;&#25022;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
Bridging Distributional and Risk-sensitive Reinforcement Learning with Provable Regret Bounds. (arXiv:2210.14051v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#36951;&#25022;&#20445;&#35777;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#19978;&#30028;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23545;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#65288;RSRL&#65289;&#30340;&#36951;&#25022;&#20445;&#35777;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#30446;&#26631;&#20026;&#22238;&#25253;&#30340;&#29109;&#39118;&#38505;&#27979;&#24230;&#65288;EntRM&#65289;&#30340;&#26377;&#38480;&#24773;&#33410;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;EntRM&#30340;&#19968;&#20010;&#20851;&#38190;&#23646;&#24615;&#65292;&#29420;&#31435;&#24615;&#23646;&#24615;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#39118;&#38505;&#25935;&#24863;&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#26694;&#26550;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#26696;&#23454;&#29616;&#20102;&#20048;&#35266;&#24615;&#65292;&#21253;&#25324;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#36798;&#21040;&#20102;$\tilde{\mathcal{O}}(\frac{\exp(|\beta| H)-1}{|\beta|}H\sqrt{S^2AK})$&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#20854;&#20013;$S$&#65292;$A$&#65292;$K$&#21644;$H$&#20998;&#21035;&#34920;&#31034;&#29366;&#24577;&#30340;&#25968;&#37327;&#65292;&#21160;&#20316;&#30340;&#25968;&#37327;&#65292;&#24773;&#33410;&#30340;&#25968;&#37327;&#21644;&#26102;&#38388;&#30340;&#38271;&#24230;&#12290;&#36825;&#19982;\cite{fei2021exponential}&#20013;&#25552;&#20986;&#30340;RSVI2&#30456;&#19968;&#33268;&#65292;&#24182;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#20998;&#26512;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20197;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#21521;&#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#21644;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#32852;&#31995;&#36215;&#26469;&#30340;&#36951;&#25022;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the regret guarantee for risk-sensitive reinforcement learning (RSRL) via distributional reinforcement learning (DRL) methods. In particular, we consider finite episodic Markov decision processes whose objective is the entropic risk measure (EntRM) of return. By leveraging a key property of the EntRM, the independence property, we establish the risk-sensitive distributional dynamic programming framework. We then propose two novel DRL algorithms that implement optimism through two different schemes, including a model-free one and a model-based one.  We prove that they both attain $\tilde{\mathcal{O}}(\frac{\exp(|\beta| H)-1}{|\beta|}H\sqrt{S^2AK})$ regret upper bound, where $S$, $A$, $K$, and $H$ represent the number of states, actions, episodes, and the time horizon, respectively. It matches RSVI2 proposed in \cite{fei2021exponential}, with novel distributional analysis. To the best of our knowledge, this is the first regret analysis that bridges DRL and RSRL in terms of sampl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35299;&#37322;&#21487;&#20197;&#24433;&#21709;&#20844;&#27491;&#24615;&#24863;&#30693;&#21644;&#20154;&#20204;&#23545;AI&#24314;&#35758;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24182;&#19981;&#33021;&#24110;&#21161;&#20154;&#20204;&#21306;&#20998;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;AI&#24314;&#35758;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#20419;&#36827;&#26377;&#25928;&#30340;&#20915;&#31574;&#21644;&#20844;&#27491;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2209.11812</link><description>&lt;p&gt;
&#35299;&#37322;&#12289;&#20844;&#27491;&#24615;&#21644;&#20154;&#31867;-AI&#20915;&#31574;&#20013;&#30340;&#36866;&#24403;&#20381;&#36182;&#65288;arXiv:2209.11812v3 [cs.HC] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making. (arXiv:2209.11812v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35299;&#37322;&#21487;&#20197;&#24433;&#21709;&#20844;&#27491;&#24615;&#24863;&#30693;&#21644;&#20154;&#20204;&#23545;AI&#24314;&#35758;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24182;&#19981;&#33021;&#24110;&#21161;&#20154;&#20204;&#21306;&#20998;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;AI&#24314;&#35758;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#20419;&#36827;&#26377;&#25928;&#30340;&#20915;&#31574;&#21644;&#20844;&#27491;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;&#20174;&#31616;&#30701;&#30340;&#25991;&#26412;&#31616;&#20171;&#20013;&#39044;&#27979;&#32844;&#19994;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#36825;&#20123;&#24433;&#21709;&#22914;&#20309;&#36890;&#36807;&#20154;&#20204;&#30340;&#20844;&#27491;&#24615;&#24863;&#30693;&#21644;&#23545;AI&#24314;&#35758;&#30340;&#20381;&#36182;&#26469;&#35843;&#33410;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#37322;&#24433;&#21709;&#20102;&#20844;&#27491;&#24615;&#24863;&#30693;&#65292;&#32780;&#20844;&#27491;&#24615;&#24863;&#30693;&#21448;&#19982;&#20154;&#20204;&#36981;&#24490;AI&#24314;&#35758;&#30340;&#20542;&#21521;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#26679;&#30340;&#35299;&#37322;&#24182;&#19981;&#33021;&#35753;&#20154;&#20204;&#21306;&#20998;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;AI&#24314;&#35758;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#35299;&#37322;&#21487;&#33021;&#20250;&#24433;&#21709;&#20381;&#36182;&#65292;&#32780;&#19981;&#35770;AI&#24314;&#35758;&#30340;&#27491;&#30830;&#24615;&#22914;&#20309;&#12290;&#21462;&#20915;&#20110;&#35299;&#37322;&#31361;&#20986;&#30340;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#20419;&#36827;&#25110;&#38459;&#30861;&#20998;&#37197;&#20844;&#27491;&#24615;&#65306;&#24403;&#35299;&#37322;&#31361;&#20986;&#19982;&#25935;&#24863;&#23646;&#24615;&#26126;&#26174;&#30456;&#20851;&#30340;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#29305;&#24449;&#26102;&#65292;&#36825;&#20250;&#20419;&#20351;&#20154;&#20204;&#35206;&#30422;AI&#19982;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#19968;&#33268;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the effects of feature-based explanations on distributive fairness of AI-assisted decisions, specifically focusing on the task of predicting occupations from short textual bios. We also investigate how any effects are mediated by humans' fairness perceptions and their reliance on AI recommendations. Our findings show that explanations influence fairness perceptions, which, in turn, relate to humans' tendency to adhere to AI recommendations. However, we see that such explanations do not enable humans to discern correct and incorrect AI recommendations. Instead, we show that they may affect reliance irrespective of the correctness of AI recommendations. Depending on which features an explanation highlights, this can foster or hinder distributive fairness: when explanations highlight features that are task-irrelevant and evidently associated with the sensitive attribute, this prompts overrides that counter AI recommendations that align with gender stereotypes. Meanw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#20102;&#38750;&#20108;&#36827;&#21046;&#23450;&#24615;&#27010;&#29575;&#32593;&#32476;&#20013;&#30340;&#38169;&#35823;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#31034;&#20363;&#21644;&#35299;&#20915;&#26041;&#26696;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2208.09344</link><description>&lt;p&gt;
&#20851;&#20110;&#38750;&#20108;&#36827;&#21046;&#23450;&#24615;&#27010;&#29575;&#32593;&#32476;&#20013;&#38169;&#35823;&#25512;&#29702;&#30340;&#19968;&#28857;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
A note on incorrect inferences in non-binary qualitative probabilistic networks. (arXiv:2208.09344v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#20102;&#38750;&#20108;&#36827;&#21046;&#23450;&#24615;&#27010;&#29575;&#32593;&#32476;&#20013;&#30340;&#38169;&#35823;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#31034;&#20363;&#21644;&#35299;&#20915;&#26041;&#26696;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#24615;&#27010;&#29575;&#32593;&#32476;&#65288;QPNs&#65289;&#23558;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#20551;&#35774;&#19982;&#27491;&#36127;&#20381;&#36182;&#24615;&#30340;&#23450;&#24615;&#29305;&#24615;&#30456;&#32467;&#21512;&#12290;&#23427;&#20204;&#24418;&#24335;&#21270;&#20102;&#27491;&#20381;&#36182;&#24615;&#30340;&#21508;&#31181;&#30452;&#35266;&#29305;&#24615;&#65292;&#20197;&#20415;&#22312;&#22823;&#22411;&#21464;&#37327;&#32593;&#32476;&#20013;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35777;&#26126;&#30001;&#20110;&#19968;&#20010;&#19981;&#27491;&#30830;&#30340;&#23545;&#31216;&#24615;&#36136;&#65292;&#22312;&#38750;&#20108;&#36827;&#21046;QPNs&#20013;&#24471;&#21040;&#30340;&#35768;&#22810;&#25512;&#29702;&#22312;&#25968;&#23398;&#19978;&#19981;&#25104;&#31435;&#12290;&#25105;&#20204;&#23558;&#25552;&#20379;&#19968;&#20123;&#27492;&#31867;&#38169;&#35823;&#25512;&#29702;&#30340;&#20363;&#23376;&#65292;&#24182;&#31616;&#35201;&#35752;&#35770;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Qualitative probabilistic networks (QPNs) combine the conditional independence assumptions of Bayesian networks with the qualitative properties of positive and negative dependence. They formalise various intuitive properties of positive dependence to allow inferences over a large network of variables. However, we will demonstrate in this paper that, due to an incorrect symmetry property, many inferences obtained in non-binary QPNs are not mathematically true. We will provide examples of such incorrect inferences and briefly discuss possible resolutions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#24615;&#20445;&#25345;&#30340;&#22270;&#32467;&#26500;&#32454;&#21270;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24050;&#23398;&#20064;&#30340;&#33410;&#28857;&#34920;&#31034;&#26469;&#36880;&#27493;&#25913;&#21892;&#22270;&#24418;&#32467;&#26500;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#32858;&#31867;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2103.07295</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26679;&#24615;&#20445;&#25345;&#30340;&#22270;&#32467;&#26500;&#32454;&#21270;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Representation Learning via Diversity-preserving Graph Refinement. (arXiv:2103.07295v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.07295
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#24615;&#20445;&#25345;&#30340;&#22270;&#32467;&#26500;&#32454;&#21270;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24050;&#23398;&#20064;&#30340;&#33410;&#28857;&#34920;&#31034;&#26469;&#36880;&#27493;&#25913;&#21892;&#22270;&#24418;&#32467;&#26500;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#32858;&#31867;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30495;&#23454;&#30340;&#22270;&#25968;&#25454;&#65292;&#33410;&#28857;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#30828;&#24615;&#20108;&#36827;&#21046;&#38142;&#25509;&#12290;&#26174;&#28982;&#65292;&#36825;&#26159;&#19968;&#31181;&#31163;&#25955;&#21644;&#31616;&#21270;&#30340;&#36830;&#32493;&#20851;&#31995;&#24418;&#24335;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#23398;&#20064;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#30340;&#21487;&#34920;&#36798;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23884;&#20837;&#31354;&#38388;&#20013;&#33719;&#24471;&#30340;&#33410;&#28857;&#34920;&#31034;&#21487;&#20197;&#21453;&#36807;&#26469;&#25581;&#31034;&#33410;&#28857;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29305;&#24449;&#21270;&#33410;&#28857;&#20851;&#31995;&#24182;&#36827;&#19968;&#27493;&#20419;&#36827;&#33410;&#28857;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#19968;&#31181;&#30452;&#35266;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#23884;&#20837;&#30340;&#33410;&#28857;&#34920;&#31034;&#26469;&#32454;&#21270;&#26368;&#21021;&#32473;&#23450;&#30340;&#22270;&#32467;&#26500;&#12290;&#20294;&#26159;&#65292;&#20840;&#23616;&#32454;&#21270;&#25152;&#26377;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#26080;&#27861;&#21306;&#20998;&#23558;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#19968;&#20123;&#22122;&#22768;&#36793;&#32536;&#65292;&#36825;&#21487;&#33021;&#36827;&#19968;&#27493;&#28151;&#28102;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#22823;&#22411;&#22270;&#24418;&#19978;&#20063;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#32467;&#26500;&#24863;&#30693;&#30340;&#22270;&#24418;&#32454;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#24050;&#32463;&#23398;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#36880;&#27493;&#25913;&#21892;&#22270;&#24418;&#32467;&#26500;&#36136;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23545;&#22270;&#20013;&#30340;&#38543;&#26426;&#28216;&#36208;&#27169;&#25311;&#29983;&#25104;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#37051;&#22495;&#32467;&#26500;&#38598;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#27599;&#20010;&#27169;&#25311;&#37051;&#22495;&#65292;&#25105;&#20204;&#22312;&#25972;&#20010;&#32454;&#21270;&#36807;&#31243;&#20013;&#20445;&#25345;&#37051;&#22495;&#32467;&#26500;&#30340;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#32454;&#21270;&#37051;&#22495;&#20869;&#30340;&#33410;&#28857;&#20851;&#31995;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#32858;&#31867;&#65292;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For real-world graph data, the complex relationship between nodes is often represented as a hard binary link. Obviously, it is a discrete and simplified form of continuous relationship between nodes, which seriously limits the expressibility of the learned node representation. On the other hand, the node representation obtained in the embedding space can in turn be used to reveal the intrinsic relationship between nodes. To better characterize the node relationships and further facilitate the learning of node representation, an intuitive way is to refine the originally given graph structure with the embedded node representations. However, such global refinement of the relationships among all nodes without distinction will inevitably lead to some noisy edges, which may further confuse the training of the node representation learning model. In addition, it also has scalability problems on large graphs. To address these issues, we propose a local structure aware graph refinement to progre
&lt;/p&gt;</description></item></channel></rss>