<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#30340;&#32858;&#21512;&#38450;&#24481;&#31574;&#30053;&#30340;&#23454;&#36341;&#26041;&#38754;&#65292;&#24182;&#38024;&#23545;Deep Partition Aggregation&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#25928;&#29575;&#12289;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#32553;&#25918;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#32858;&#21512;&#38450;&#24481;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.16415</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#30340;&#32858;&#21512;&#38450;&#24481;&#30340;&#23454;&#36341;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
On Practical Aspects of Aggregation Defenses against Data Poisoning Attacks. (arXiv:2306.16415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#30340;&#32858;&#21512;&#38450;&#24481;&#31574;&#30053;&#30340;&#23454;&#36341;&#26041;&#38754;&#65292;&#24182;&#38024;&#23545;Deep Partition Aggregation&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#25928;&#29575;&#12289;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#32553;&#25918;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#32858;&#21512;&#38450;&#24481;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#26469;&#35828;&#65292;&#25968;&#25454;&#30340;&#22686;&#21152;&#19981;&#20165;&#24102;&#26469;&#26426;&#20250;&#65292;&#20063;&#24102;&#26469;&#39118;&#38505;&#65292;&#22240;&#20026;&#24694;&#24847;&#35757;&#32451;&#26679;&#26412;&#21487;&#20197;&#25805;&#32437;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#25915;&#20987;&#34987;&#31216;&#20026;&#25968;&#25454;&#20013;&#27602;&#12290;&#36817;&#26399;&#23545;&#25239;&#25968;&#25454;&#20013;&#27602;&#30340;&#38450;&#24481;&#31574;&#30053;&#30340;&#36827;&#23637;&#31361;&#20986;&#20102;&#32858;&#21512;&#26041;&#26696;&#22312;&#23454;&#29616;&#35748;&#35777;&#20013;&#27602;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#36341;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;Deep Partition Aggregation&#65292;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#32858;&#21512;&#38450;&#24481;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#23454;&#38469;&#26041;&#38754;&#65292;&#21253;&#25324;&#25928;&#29575;&#12289;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#34987;&#35843;&#25972;&#21040;64&#215;64&#20998;&#36776;&#29575;&#30340;ImageNet&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#22312;&#27604;&#20197;&#21069;&#26356;&#22823;&#30340;&#35268;&#27169;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#23454;&#29992;&#30340;&#22522;&#20110;&#32553;&#25918;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#25913;&#21892;&#20102;&#32858;&#21512;&#38450;&#24481;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25903;&#25345;&#25968;&#25454;&#21078;&#20998;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing access to data poses both opportunities and risks in deep learning, as one can manipulate the behaviors of deep learning models with malicious training samples. Such attacks are known as data poisoning. Recent advances in defense strategies against data poisoning have highlighted the effectiveness of aggregation schemes in achieving state-of-the-art results in certified poisoning robustness. However, the practical implications of these approaches remain unclear. Here we focus on Deep Partition Aggregation, a representative aggregation defense, and assess its practical aspects, including efficiency, performance, and robustness. For evaluations, we use ImageNet resized to a resolution of 64 by 64 to enable evaluations at a larger scale than previous ones. Firstly, we demonstrate a simple yet practical approach to scaling base models, which improves the efficiency of training and inference for aggregation defenses. Secondly, we provide empirical evidence supporting the data
&lt;/p&gt;</description></item><item><title>MultiZoo&#21644;MultiBench&#26159;&#29992;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#20934;&#21270;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#22810;&#27169;&#24577;&#31639;&#27861;&#30340;&#23454;&#29616;&#21644;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20419;&#36827;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#21147;&#21644;&#38480;&#21046;&#30340;&#29702;&#35299;&#65292;&#24182;&#30830;&#20445;&#26131;&#29992;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16413</link><description>&lt;p&gt;
MultiZoo &amp; MultiBench: &#29992;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#20934;&#21270;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
MultiZoo &amp; MultiBench: A Standardized Toolkit for Multimodal Deep Learning. (arXiv:2306.16413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16413
&lt;/p&gt;
&lt;p&gt;
MultiZoo&#21644;MultiBench&#26159;&#29992;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#20934;&#21270;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#22810;&#27169;&#24577;&#31639;&#27861;&#30340;&#23454;&#29616;&#21644;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#20419;&#36827;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#21147;&#21644;&#38480;&#21046;&#30340;&#29702;&#35299;&#65292;&#24182;&#30830;&#20445;&#26131;&#29992;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#28041;&#21450;&#25972;&#21512;&#26469;&#33258;&#22810;&#31181;&#24322;&#26500;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#21152;&#24555;&#23545;&#23569;&#30740;&#31350;&#30340;&#27169;&#24577;&#21644;&#20219;&#21153;&#30340;&#36827;&#23637;&#65292;&#21516;&#26102;&#30830;&#20445;&#29616;&#23454;&#19990;&#30028;&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;MultiZoo&#65292;&#19968;&#20010;&#20844;&#20849;&#24037;&#20855;&#21253;&#65292;&#20854;&#20013;&#21253;&#21547;&gt; 20&#20010;&#26680;&#24515;&#22810;&#27169;&#24577;&#31639;&#27861;&#30340;&#26631;&#20934;&#21270;&#23454;&#29616;&#65292;&#20197;&#21450;MultiBench&#65292;&#19968;&#20010;&#28085;&#30422;15&#20010;&#25968;&#25454;&#38598;&#65292;10&#20010;&#27169;&#24577;&#65292;20&#20010;&#39044;&#27979;&#20219;&#21153;&#21644;6&#20010;&#30740;&#31350;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#20123;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#31471;&#21040;&#31471;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#31616;&#21270;&#21644;&#26631;&#20934;&#21270;&#25968;&#25454;&#21152;&#36733;&#12289;&#23454;&#39564;&#35774;&#32622;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;&#20026;&#20102;&#23454;&#29616;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#65288;1&#65289;&#27867;&#21270;&#33021;&#21147;&#65292;&#65288;2&#65289;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#21644;&#65288;3&#65289;&#27169;&#24577;&#40065;&#26834;&#24615;&#12290;MultiBench&#20026;&#26356;&#22909;&#22320;&#20102;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#21151;&#33021;&#21644;&#38480;&#21046;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#21516;&#26102;&#30830;&#20445;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#21253;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of &gt; 20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20840;&#29699;&#35266;&#28857;&#30340;&#20195;&#34920;&#24615;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#36328;&#22269;&#35843;&#26597;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23450;&#20041;&#19968;&#20010;&#30456;&#20284;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#40664;&#35748;&#24773;&#20917;&#19979;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#24212;&#26356;&#20542;&#21521;&#20110;&#26576;&#20123;&#20154;&#32676;&#30340;&#35266;&#28857;&#65292;&#20294;&#24403;&#27169;&#22411;&#32771;&#34385;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#26102;&#65292;&#22238;&#24212;&#20250;&#26356;&#21152;&#36148;&#36817;&#35813;&#22269;&#23478;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.16388</link><description>&lt;p&gt;
&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#20013;&#20027;&#35266;&#20840;&#29699;&#35266;&#28857;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Measuring the Representation of Subjective Global Opinions in Language Models. (arXiv:2306.16388v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20840;&#29699;&#35266;&#28857;&#30340;&#20195;&#34920;&#24615;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#36328;&#22269;&#35843;&#26597;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23450;&#20041;&#19968;&#20010;&#30456;&#20284;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#40664;&#35748;&#24773;&#20917;&#19979;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#24212;&#26356;&#20542;&#21521;&#20110;&#26576;&#20123;&#20154;&#32676;&#30340;&#35266;&#28857;&#65292;&#20294;&#24403;&#27169;&#22411;&#32771;&#34385;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#26102;&#65292;&#22238;&#24212;&#20250;&#26356;&#21152;&#36148;&#36817;&#35813;&#22269;&#23478;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#26080;&#27861;&#20844;&#24179;&#22320;&#20195;&#34920;&#31038;&#20250;&#38382;&#39064;&#20013;&#22810;&#26679;&#21270;&#30340;&#20840;&#29699;&#35266;&#28857;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#37327;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#31572;&#19982;&#21738;&#20123;&#20154;&#30340;&#35266;&#28857;&#26356;&#20026;&#30456;&#20284;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;GlobalOpinionQA&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;&#36328;&#22269;&#35843;&#26597;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#26088;&#22312;&#25429;&#25417;&#19981;&#21516;&#22269;&#23478;&#20851;&#20110;&#20840;&#29699;&#38382;&#39064;&#30340;&#22810;&#26679;&#35266;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#22269;&#23478;&#20026;&#26465;&#20214;&#65292;&#37327;&#21270;&#20102;LLM&#29983;&#25104;&#30340;&#35843;&#26597;&#22238;&#31572;&#19982;&#20154;&#31867;&#22238;&#31572;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#32463;&#36807;&#23466;&#27861;AI&#22521;&#35757;&#30340;LLM&#36827;&#34892;&#20102;&#19977;&#20010;&#23454;&#39564;&#65292;&#20998;&#21035;&#32771;&#34385;&#20854;&#24110;&#21161;&#24615;&#12289;&#35802;&#23454;&#24615;&#21644;&#26080;&#23475;&#24615;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#22238;&#24212;&#26356;&#20542;&#21521;&#20110;&#19982;&#26576;&#20123;&#20154;&#32676;&#30340;&#35266;&#28857;&#26356;&#31867;&#20284;&#65292;&#20363;&#22914;&#26469;&#33258;&#32654;&#22269;&#12289;&#27431;&#27954;&#21644;&#21335;&#32654;&#27954;&#30340;&#20154;&#32676;&#65292;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#20559;&#35265;&#12290;&#24403;&#25105;&#20204;&#25552;&#31034;&#27169;&#22411;&#32771;&#34385;&#26576;&#20010;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#26102;&#65292;&#22238;&#24212;&#20250;&#26356;&#21152;&#31867;&#20284;&#20110;&#35813;&#22269;&#23478;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;GPU&#21457;&#36215;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;&#26469;&#21152;&#36895;GNN&#26694;&#26550;&#20013;&#30340;&#37319;&#26679;&#21644;&#32858;&#21512;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#22270;&#19978;&#26102;CPU&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;GPU&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16384</link><description>&lt;p&gt;
&#21152;&#36895;GNN&#26694;&#26550;&#20013;&#30340;&#37319;&#26679;&#21644;&#32858;&#21512;&#25805;&#20316;&#65306;&#21033;&#29992;GPU&#21457;&#36215;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;
&lt;/p&gt;
&lt;p&gt;
Accelerating Sampling and Aggregation Operations in GNN Frameworks with GPU Initiated Direct Storage Accesses. (arXiv:2306.16384v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;GPU&#21457;&#36215;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;&#26469;&#21152;&#36895;GNN&#26694;&#26550;&#20013;&#30340;&#37319;&#26679;&#21644;&#32858;&#21512;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#22270;&#19978;&#26102;CPU&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;GPU&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#27491;&#22312;&#25104;&#20026;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#21644;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#19968;&#20010;&#24378;&#22823;&#24037;&#20855;&#65292;&#36866;&#29992;&#20110;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;GNNs&#22312;&#20013;&#31561;&#35268;&#27169;&#30340;&#22270;&#19978;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#35757;&#32451;&#20173;&#28982;&#38754;&#20020;&#30528;&#25968;&#25454;&#35775;&#38382;&#21644;&#25968;&#25454;&#31227;&#21160;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;&#29616;&#26377;&#30340;GNN&#35757;&#32451;&#26694;&#26550;&#20351;&#29992;CPU&#36827;&#34892;&#22270;&#37319;&#26679;&#21644;&#29305;&#24449;&#32858;&#21512;&#65292;&#32780;&#27169;&#22411;&#26435;&#37325;&#30340;&#35757;&#32451;&#21644;&#26356;&#26032;&#21017;&#30001;GPU&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#21457;&#29616;CPU&#26080;&#27861;&#23454;&#29616;&#25152;&#38656;&#30340;&#21534;&#21520;&#37327;&#20197;&#20805;&#20998;&#21033;&#29992;&#26114;&#36149;&#30340;GPU&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#24403;&#22270;&#21644;&#20854;&#23884;&#20837;&#19981;&#33021;&#36866;&#24212;CPU&#20869;&#23384;&#26102;&#65292;&#25805;&#20316;&#31995;&#32479;&#24341;&#20837;&#30340;&#24320;&#38144;&#65292;&#22914;&#22788;&#29702;&#39029;&#38754;&#38169;&#35823;&#65292;&#20250;&#25104;&#20026;&#20851;&#38190;&#36335;&#24452;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPU&#21457;&#36215;&#30340;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are emerging as a powerful tool for learning from graph-structured data and performing sophisticated inference tasks in various application domains. Although GNNs have been shown to be effective on modest-sized graphs, training them on large-scale graphs remains a significant challenge due to lack of efficient data access and data movement methods. Existing frameworks for training GNNs use CPUs for graph sampling and feature aggregation, while the training and updating of model weights are executed on GPUs. However, our in-depth profiling shows the CPUs cannot achieve the throughput required to saturate GNN model training throughput, causing gross under-utilization of expensive GPU resources. Furthermore, when the graph and its embeddings do not fit in the CPU memory, the overhead introduced by the operating system, say for handling page-faults, comes in the critical path of execution.  To address these issues, we propose the GPU Initiated Direct Storage Ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#30340;A*&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#21551;&#21457;&#24335;&#37096;&#20998;&#24341;&#20837;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26080;&#20154;&#26426;&#36335;&#24452;&#35268;&#21010;&#20013;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20854;&#20182;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#31639;&#27861;&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.16368</link><description>&lt;p&gt;
&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#30340;A*&#31639;&#27861;&#29992;&#20110;&#33258;&#21160;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Lagrangian based A* algorithm for automated reasoning. (arXiv:2306.16368v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#30340;A*&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#21551;&#21457;&#24335;&#37096;&#20998;&#24341;&#20837;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26080;&#20154;&#26426;&#36335;&#24452;&#35268;&#21010;&#20013;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#20854;&#20182;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#31639;&#27861;&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23545;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#36827;&#34892;A*&#31639;&#27861;&#30340;&#20462;&#25913;&#12290;&#22312;A*&#31639;&#27861;&#30340;&#21551;&#21457;&#24335;&#37096;&#20998;&#24341;&#20837;&#20102;&#26435;&#37325;&#20197;&#25552;&#39640;&#20854;&#25928;&#29575;&#12290;&#23558;&#35813;&#31639;&#27861;&#24212;&#29992;&#20110;&#26080;&#20154;&#26426;&#36335;&#24452;&#35268;&#21010;&#20013;&#65292;&#20854;&#20013;&#36895;&#24230;&#34987;&#35270;&#20026;&#21551;&#21457;&#24335;&#30340;&#26435;&#37325;&#12290;&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;&#21464;&#20998;&#24494;&#31215;&#20998;&#30340;&#25289;&#26684;&#26391;&#26085;&#26041;&#31243;&#26469;&#30830;&#23450;&#36895;&#24230;&#20316;&#20026;&#21160;&#24577;&#31995;&#32479;&#30340;&#20915;&#23450;&#24615;&#22240;&#32032;&#12290;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#29992;&#20110;&#20854;&#20182;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#36825;&#20123;&#39046;&#22495;&#20013;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a modification of A* algorithm is considered for the shortest path problem. A weightage is introduced in the heuristic part of the A* algorithm to improve its efficiency. An application of the algorithm is considered for UAV path planning wherein velocity is taken as the weigtage to the heuristic. At the outset, calculus of variations based Lagrange's equation was used to identify velocity as the decisive factor for the dynamical system. This approach would be useful for other problems as well to improve the efficiency of algorithms in those areas.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#32544;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.16334</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#26631;&#24535;&#26816;&#27979;&#26469;&#35782;&#21035;&#31163;&#25955;&#21270;&#28508;&#22312;&#22352;&#26631;&#31995;&#32479;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability of Discretized Latent Coordinate Systems via Density Landmarks Detection. (arXiv:2306.16334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#32544;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#26088;&#22312;&#20165;&#20174;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#20013;&#24674;&#22797;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#30495;&#23454;&#22240;&#32032;&#12290; &#21487;&#35782;&#21035;&#24615;&#20026;&#35299;&#32544;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290; &#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#33258;&#36866;&#24212;&#29420;&#31435;&#28508;&#21464;&#37327;&#22240;&#23376;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#20809;&#28369;&#22240;&#23376;&#21040;&#35266;&#27979;&#30340;&#26144;&#23556;&#19979;&#65292;&#26080;&#30417;&#30563;&#30340;&#21487;&#35782;&#21035;&#24615;&#22312;i.i.d.&#35774;&#32622;&#19979;&#26159;&#29702;&#35770;&#19978;&#19981;&#21487;&#33021;&#30340;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#24120;&#24778;&#20154;&#30340;&#26159;&#65292;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#20809;&#28369;&#26144;&#23556;&#65288;&#19968;&#20010;&#24494;&#20998;&#21516;&#32986;&#65289;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#26144;&#23556;&#36827;&#34892;&#20219;&#20309;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290; &#36825;&#26159;&#22312;&#20551;&#35774;&#28508;&#22312;&#23494;&#24230;&#20855;&#26377;&#36724;&#23545;&#40784;&#30340;&#19981;&#36830;&#32493;&#26631;&#24535;&#30340;&#24773;&#20917;&#19979;&#65292;&#20294;&#19981;&#20570;&#22240;&#32032;&#30340;&#32479;&#35745;&#29420;&#31435;&#30340;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#12290; &#25105;&#20204;&#24341;&#20837;&#20102;&#36825;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#23545;&#24674;&#22797;&#31163;&#25955;&#22352;&#26631;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentanglement aims to recover meaningful latent ground-truth factors from only the observed distribution. Identifiability provides the theoretical grounding for disentanglement to be well-founded. Unfortunately, unsupervised identifiability of independent latent factors is a theoretically proven impossibility in the i.i.d. setting under a general nonlinear smooth map from factors to observations. In this work, we show that, remarkably, it is possible to recover discretized latent coordinates under a highly generic nonlinear smooth mapping (a diffeomorphism) without any additional inductive bias on the mapping. This is, assuming that latent density has axis-aligned discontinuity landmarks, but without making the unrealistic assumption of statistical independence of the factors. We introduce this novel form of identifiability, termed quantized coordinate identifiability, and provide a comprehensive proof of the recovery of discretized coordinates.
&lt;/p&gt;</description></item><item><title>VBN&#26159;&#19968;&#31181;&#21033;&#29992;&#23618;&#27425;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23454;&#20307;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25968;&#25454;&#31232;&#32570;&#30340;&#8220;&#38271;&#23614;&#8221;&#23454;&#20307;&#24314;&#27169;&#12290;&#36890;&#36807;&#20351;&#29992;&#23618;&#27425;&#20808;&#39564;&#21644;&#26126;&#30830;&#20851;&#31995;&#32422;&#26463;&#65292;VBN&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24314;&#27169;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23494;&#24230;&#34920;&#31034;&#23454;&#20307;&#65292;&#23545;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#26469;&#23454;&#29616;&#24555;&#36895;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2306.16326</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#36125;&#21494;&#26031;&#32593;&#32476;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning via Variational Bayesian Networks. (arXiv:2306.16326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16326
&lt;/p&gt;
&lt;p&gt;
VBN&#26159;&#19968;&#31181;&#21033;&#29992;&#23618;&#27425;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23454;&#20307;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25968;&#25454;&#31232;&#32570;&#30340;&#8220;&#38271;&#23614;&#8221;&#23454;&#20307;&#24314;&#27169;&#12290;&#36890;&#36807;&#20351;&#29992;&#23618;&#27425;&#20808;&#39564;&#21644;&#26126;&#30830;&#20851;&#31995;&#32422;&#26463;&#65292;VBN&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24314;&#27169;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23494;&#24230;&#34920;&#31034;&#23454;&#20307;&#65292;&#23545;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#26469;&#23454;&#29616;&#24555;&#36895;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23454;&#20307;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;-&#21464;&#20998;&#36125;&#21494;&#26031;&#32593;&#32476; (VBN)&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#23618;&#27425;&#21644;&#20851;&#31995;&#20449;&#24687;&#65292;&#24182;&#23545;&#8220;&#38271;&#23614;&#8221;&#20013;&#30340;&#23454;&#20307;&#24314;&#27169;&#29305;&#21035;&#26377;&#29992;&#65292;&#22240;&#20026;&#36825;&#31181;&#24773;&#20917;&#19979;&#25968;&#25454;&#31232;&#32570;&#12290;VBN&#36890;&#36807;&#20004;&#31181;&#20114;&#34917;&#26426;&#21046;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38271;&#23614;&#23454;&#20307;&#24314;&#27169;&#65306;&#39318;&#20808;&#65292;VBN&#37319;&#29992;&#20102;&#20449;&#24687;&#20016;&#23500;&#30340;&#23618;&#27425;&#20808;&#39564;&#65292;&#20351;&#20849;&#20139;&#20849;&#21516;&#31062;&#20808;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#25773;&#25104;&#20026;&#21487;&#33021;&#12290;&#27492;&#22806;&#65292;VBN&#24314;&#27169;&#20102;&#23454;&#20307;&#20043;&#38388;&#30340;&#26126;&#30830;&#20851;&#31995;&#65292;&#24378;&#21046;&#23454;&#20307;&#20043;&#38388;&#30340;&#20114;&#34917;&#32467;&#26500;&#21644;&#19968;&#33268;&#24615;&#65292;&#24341;&#23548;&#23398;&#20064;&#30340;&#34920;&#31034;&#21521;&#26356;&#26377;&#24847;&#20041;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;&#20854;&#27425;&#65292;VBN&#36890;&#36807;&#23494;&#24230;&#34920;&#31034;&#23454;&#20307;&#65288;&#32780;&#19981;&#26159;&#21521;&#37327;&#65289;&#65292;&#20174;&#32780;&#23545;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#36215;&#21040;&#34917;&#20805;&#20316;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;VBN&#22312;&#35821;&#35328;&#23398;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Variational Bayesian Network (VBN) - a novel Bayesian entity representation learning model that utilizes hierarchical and relational side information and is particularly useful for modeling entities in the ``long-tail'', where the data is scarce. VBN provides better modeling for long-tail entities via two complementary mechanisms: First, VBN employs informative hierarchical priors that enable information propagation between entities sharing common ancestors. Additionally, VBN models explicit relations between entities that enforce complementary structure and consistency, guiding the learned representations towards a more meaningful arrangement in space. Second, VBN represents entities by densities (rather than vectors), hence modeling uncertainty that plays a complementary role in coping with data scarcity. Finally, we propose a scalable Variational Bayes optimization algorithm that enables fast approximate Bayesian inference. We evaluate the effectiveness of VBN on linguist
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#30340;&#23545;&#25239;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#21644;&#35821;&#20041;&#26816;&#27979;&#65292;&#22312;&#20013;&#25991;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#22686;&#24378;&#20102;&#23383;&#31526;&#22810;&#20041;&#30340;&#24314;&#27169;&#21644;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16313</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#30340;&#23545;&#25239;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#21644;&#35821;&#20041;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Multi-Task Learning Method for Chinese Text Correction with Semantic Detection. (arXiv:2306.16313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#30340;&#23545;&#25239;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#21644;&#35821;&#20041;&#26816;&#27979;&#65292;&#22312;&#20013;&#25991;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#22686;&#24378;&#20102;&#23383;&#31526;&#22810;&#20041;&#30340;&#24314;&#27169;&#21644;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32416;&#38169;&#65292;&#23588;&#20854;&#26159;&#26356;&#24191;&#27867;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#35821;&#20041;&#32416;&#38169;&#65292;&#23545;&#20110;&#25552;&#39640;&#25991;&#26412;&#30340;&#27969;&#30021;&#24615;&#21644;&#20889;&#20316;&#25928;&#29575;&#26377;&#30528;&#26497;&#39640;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#20013;&#25991;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#23383;&#31526;&#22810;&#20041;&#30340;&#24314;&#27169;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;&#20854;&#20013;&#65292;&#24341;&#20837;&#20102;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20998;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#19968;&#23545;&#19981;&#20165;&#32806;&#21512;&#32780;&#19988;&#23545;&#25239;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31574;&#30053;&#21644;&#19968;&#20010;&#31574;&#30053;&#32593;&#32476;&#65292;&#20197;&#23454;&#29616;&#24102;&#26377;&#35821;&#20041;&#26816;&#27979;&#30340;&#39640;&#25928;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#20219;&#21153;&#12290;&#23454;&#39564;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#20116;&#31181;&#21487;&#27604;&#36739;&#30340;&#26041;&#27861;&#19978;&#36827;&#34892;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20013;&#25991;&#25991;&#26412;&#32416;&#38169;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#23454;&#29616;&#35821;&#20041;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text correction, especially the semantic correction of more widely used scenes, is strongly required to improve, for the fluency and writing efficiency of the text. An adversarial multi-task learning method is proposed to enhance the modeling and detection ability of character polysemy in Chinese sentence context. Wherein, two models, the masked language model and scoring language model, are introduced as a pair of not only coupled but also adversarial learning tasks. Moreover, the Monte Carlo tree search strategy and a policy network are introduced to accomplish the efficient Chinese text correction task with semantic detection. The experiments are executed on three datasets and five comparable methods, and the experimental results show that our method can obtain good performance in Chinese text correction task for better semantic rationality.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;SocialVec&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#31038;&#20132;&#32593;&#32476;&#20013;&#25552;&#21462;&#20302;&#32500;&#23454;&#20307;&#23884;&#20837;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#19987;&#38376;&#25429;&#25417;&#31038;&#20132;&#19990;&#30028;&#30693;&#35782;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2306.16299</link><description>&lt;p&gt;
&#31038;&#20132;&#19990;&#30028;&#30693;&#35782;&#65306;&#24314;&#27169;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Social World Knowledge: Modeling and Applications. (arXiv:2306.16299v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;SocialVec&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#31038;&#20132;&#32593;&#32476;&#20013;&#25552;&#21462;&#20302;&#32500;&#23454;&#20307;&#23884;&#20837;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#19987;&#38376;&#25429;&#25417;&#31038;&#20132;&#19990;&#30028;&#30693;&#35782;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#19990;&#30028;&#30693;&#35782;&#26159;&#20154;&#31867;&#21644;&#26426;&#22120;&#26377;&#25928;&#27807;&#36890;&#21644;&#20449;&#24687;&#22788;&#29702;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#34920;&#31034;&#20107;&#23454;&#19990;&#30028;&#30693;&#35782;&#30340;&#30693;&#35782;&#24211;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#36164;&#28304;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;&#31038;&#20132;&#26041;&#38754;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#26412;&#30740;&#31350;&#21521;&#26500;&#24314;&#27492;&#31867;&#36164;&#28304;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SocialVec&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#31038;&#20132;&#32593;&#32476;&#20013;&#21457;&#29983;&#30340;&#31038;&#20132;&#29615;&#22659;&#20013;&#25552;&#21462;&#20302;&#32500;&#23454;&#20307;&#23884;&#20837;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#23454;&#20307;&#23545;&#24212;&#20110;&#24341;&#36215;&#26222;&#36941;&#20852;&#36259;&#30340;&#39640;&#24230;&#21463;&#27426;&#36814;&#30340;&#24080;&#25143;&#12290;&#25105;&#20204;&#20551;&#35774;&#20010;&#20307;&#29992;&#25143;&#20542;&#21521;&#20110;&#20849;&#21516;&#20851;&#27880;&#30340;&#23454;&#20307;&#26159;&#31038;&#20132;&#30456;&#20851;&#30340;&#65292;&#24182;&#20351;&#29992;&#36825;&#20010;&#31038;&#20132;&#29615;&#22659;&#30340;&#23450;&#20041;&#26469;&#23398;&#20064;&#23454;&#20307;&#23884;&#20837;&#12290;&#31867;&#20284;&#20110;&#26377;&#21161;&#20110;&#28041;&#21450;&#25991;&#26412;&#35821;&#20041;&#30340;&#20219;&#21153;&#30340;&#35789;&#23884;&#20837;&#65292;&#25105;&#20204;&#26399;&#26395;&#23398;&#21040;&#30340;&#31038;&#20132;&#23454;&#20307;&#23884;&#20837;&#23558;&#26377;&#30410;&#20110;&#22810;&#20010;&#20855;&#26377;&#31038;&#20132;&#29305;&#33394;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social world knowledge is a key ingredient in effective communication and information processing by humans and machines alike. As of today, there exist many knowledge bases that represent factual world knowledge. Yet, there is no resource that is designed to capture social aspects of world knowledge. We believe that this work makes an important step towards the formulation and construction of such a resource. We introduce SocialVec, a general framework for eliciting low-dimensional entity embeddings from the social contexts in which they occur in social networks. In this framework, entities correspond to highly popular accounts which invoke general interest. We assume that entities that individual users tend to co-follow are socially related, and use this definition of social context to learn the entity embeddings. Similar to word embeddings which facilitate tasks that involve text semantics, we expect the learned social entity embeddings to benefit multiple tasks of social flavor. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#21644;&#20462;&#21098;&#30456;&#20851;&#23454;&#20307;&#65292;&#20174;&#32780;&#24341;&#23548;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#39046;&#22495;&#21644;&#24322;&#36136;&#31181;&#23376;&#23454;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#24212;&#29992;&#31867;&#27604;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.16296</link><description>&lt;p&gt;
&#30456;&#20851;&#23454;&#20307;&#36873;&#25321;&#65306;&#36890;&#36807;&#38646;&#26679;&#26412;&#31867;&#27604;&#20462;&#21098;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Relevant Entity Selection: Knowledge Graph Bootstrapping via Zero-Shot Analogical Pruning. (arXiv:2306.16296v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#21644;&#20462;&#21098;&#30456;&#20851;&#23454;&#20307;&#65292;&#20174;&#32780;&#24341;&#23548;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#39046;&#22495;&#21644;&#24322;&#36136;&#31181;&#23376;&#23454;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#24212;&#29992;&#31867;&#27604;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#20174;&#39640;&#36136;&#37327;&#30340;&#26680;&#24515;&#24320;&#22987;&#65292;&#36890;&#36807;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#19981;&#26029;&#25913;&#36827;&#12290;&#36825;&#26679;&#30340;&#26680;&#24515;&#21487;&#20197;&#20174;&#20687;Wikidata&#36825;&#26679;&#30340;&#24320;&#25918;&#24335;&#30693;&#35782;&#22270;&#35889;&#20013;&#33719;&#24471;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#36890;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#35268;&#27169;&#65292;&#23558;&#20854;&#20316;&#20026;&#25972;&#20307;&#38598;&#25104;&#21487;&#33021;&#20250;&#21253;&#21547;&#26080;&#20851;&#20869;&#23481;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#65292;&#20174;&#36890;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#24863;&#20852;&#36259;&#31181;&#23376;&#23454;&#20307;&#24320;&#22987;&#65292;&#24182;&#20445;&#30041;&#25110;&#20462;&#21098;&#20854;&#30456;&#37051;&#23454;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598; &#22312;Wikidata&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#39046;&#22495;&#21516;&#36136;&#25110;&#24322;&#36136;&#30340;&#31181;&#23376;&#23454;&#20307;&#12290;&#25105;&#20204;&#20174;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#20248;&#20110;LSTM&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#19988;&#21442;&#25968;&#25968;&#37327;&#22823;&#22823;&#20943;&#23569;&#12290;&#25105;&#20204;&#36824;&#22312;&#36801;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#36827;&#19968;&#27493;&#23558;&#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#38598;&#25104;&#21040;&#30456;&#20851;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Construction (KGC) can be seen as an iterative process starting from a high quality nucleus that is refined by knowledge extraction approaches in a virtuous loop. Such a nucleus can be obtained from knowledge existing in an open KG like Wikidata. However, due to the size of such generic KGs, integrating them as a whole may entail irrelevant content and scalability issues. We propose an analogy-based approach that starts from seed entities of interest in a generic KG, and keeps or prunes their neighboring entities. We evaluate our approach on Wikidata through two manually labeled datasets that contain either domain-homogeneous or -heterogeneous seed entities. We empirically show that our analogy-based approach outperforms LSTM, Random Forest, SVM, and MLP, with a drastically lower number of parameters. We also evaluate its generalization potential in a transfer learning setting. These results advocate for the further integration of analogy-based inference in tasks relate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#19982;ChatGPT&#25110;GPT-4&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#39135;&#29289;&#24433;&#21709;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20135;&#21697;&#29305;&#23450;&#25351;&#23548;(PSG)&#24320;&#21457;&#20013;&#20855;&#26377;&#28508;&#21147;&#24212;&#29992;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.16275</link><description>&lt;p&gt;
&#21033;&#29992;GPT-4&#36827;&#34892;&#39135;&#29289;&#24433;&#21709;&#25688;&#35201;&#20197;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#22686;&#24378;&#20135;&#21697;&#29305;&#23450;&#25351;&#23548;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting. (arXiv:2306.16275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;&#19982;ChatGPT&#25110;GPT-4&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#39135;&#29289;&#24433;&#21709;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20135;&#21697;&#29305;&#23450;&#25351;&#23548;(PSG)&#24320;&#21457;&#20013;&#20855;&#26377;&#28508;&#21147;&#24212;&#29992;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#24433;&#21709;&#20174;&#26032;&#33647;&#30003;&#35831;(NDA)&#20013;&#30340;&#25688;&#35201;&#26159;&#20135;&#21697;&#29305;&#23450;&#25351;&#23548;(PSG)&#24320;&#21457;&#21644;&#35780;&#20272;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20174;&#22823;&#37327;&#33647;&#29289;&#30003;&#35831;&#23457;&#26597;&#25991;&#20214;&#20013;&#25163;&#21160;&#25688;&#35201;&#39135;&#29289;&#24433;&#21709;&#26159;&#32791;&#26102;&#30340;&#65292;&#36825;&#24341;&#21457;&#20102;&#24320;&#21457;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;ChatGPT&#21644;GPT-4&#30340;&#36827;&#23637;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#25913;&#21892;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#25928;&#26524;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20854;&#22312;PSG&#35780;&#20272;&#20013;&#20934;&#30830;&#27010;&#25324;&#39135;&#29289;&#24433;&#21709;&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#36845;&#20195;&#25552;&#31034;&#65292;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#22320;&#19982;ChatGPT&#25110;GPT-4&#36827;&#34892;&#20114;&#21160;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#36718;&#36845;&#20195;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#39135;&#29289;&#24433;&#21709;&#25688;&#35201;&#65292;&#20854;&#20013;&#22312;&#36830;&#32493;&#30340;&#36718;&#27425;&#20013;&#20998;&#21035;&#25552;&#20379;&#20102;&#20851;&#38190;&#23383;&#32858;&#28966;&#21644;&#38271;&#24230;&#25511;&#21046;&#30340;&#25552;&#31034;&#20197;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Food effect summarization from New Drug Application (NDA) is an essential component of product-specific guidance (PSG) development and assessment. However, manual summarization of food effect from extensive drug application review documents is time-consuming, which arouses a need to develop automated methods. Recent advances in large language models (LLMs) such as ChatGPT and GPT-4, have demonstrated great potential in improving the effectiveness of automated text summarization, but its ability regarding the accuracy in summarizing food effect for PSG assessment remains unclear. In this study, we introduce a simple yet effective approach, iterative prompting, which allows one to interact with ChatGPT or GPT-4 more effectively and efficiently through multi-turn interaction. Specifically, we propose a three-turn iterative prompting approach to food effect summarization in which the keyword-focused and length-controlled prompts are respectively provided in consecutive turns to refine the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#38463;&#23500;&#27735;&#26222;&#20160;&#22270;&#35821;&#21464;&#20307;&#30340;&#24773;&#32490;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;7600&#26465;&#25512;&#25991;&#65292;&#20197;&#30740;&#31350;&#22612;&#21033;&#29677;&#31105;&#27490;&#22919;&#22899;&#25509;&#21463;&#25945;&#32946;&#30340;&#24773;&#32490;&#21453;&#24212;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#31070;&#32463;&#26550;&#26500;&#23545;&#36798;&#37324;&#35821;&#24773;&#24863;&#20998;&#31867;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2306.16268</link><description>&lt;p&gt;
&#22312;&#38463;&#23500;&#27735;&#31105;&#27490;&#25945;&#32946;&#30340;&#25512;&#25991;&#24773;&#32490;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Emotion Analysis of Tweets Banning Education in Afghanistan. (arXiv:2306.16268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16268
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#38463;&#23500;&#27735;&#26222;&#20160;&#22270;&#35821;&#21464;&#20307;&#30340;&#24773;&#32490;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;7600&#26465;&#25512;&#25991;&#65292;&#20197;&#30740;&#31350;&#22612;&#21033;&#29677;&#31105;&#27490;&#22919;&#22899;&#25509;&#21463;&#25945;&#32946;&#30340;&#24773;&#32490;&#21453;&#24212;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#31070;&#32463;&#26550;&#26500;&#23545;&#36798;&#37324;&#35821;&#24773;&#24863;&#20998;&#31867;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#38463;&#23500;&#27735;&#26222;&#20160;&#22270;&#35821;&#21464;&#20307;&#30340;&#24773;&#32490;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;LetHerLearn&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;7600&#26465;&#25512;&#25991;&#65292;&#36825;&#20123;&#25512;&#25991;&#26159;&#23545;&#22612;&#21033;&#29677;&#20110;2022&#24180;&#31105;&#27490;&#22919;&#22899;&#25509;&#21463;&#25945;&#32946;&#30340;&#21453;&#24212;&#65292;&#24182;&#19988;&#24050;&#26681;&#25454;&#22467;&#20811;&#26364;&#24773;&#32490;&#31867;&#21035;&#36827;&#34892;&#20102;&#25163;&#21160;&#26631;&#27880;&#12290;&#25105;&#20204;&#22312;&#27492;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#36807;&#31243;&#65292;&#21576;&#29616;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#32479;&#35745;&#20449;&#24687;&#20197;&#21450;&#23545;&#25152;&#24471;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#21021;&#27493;&#23454;&#39564;&#65292;&#23545;&#36798;&#37324;&#35821;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#22810;&#31181;&#19981;&#21516;&#31070;&#32463;&#26550;&#26500;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the first emotion annotated dataset for the Dari variant of Persian spoken in Afghanistan. The LetHerLearn dataset contains 7,600 tweets posted in reaction to the Taliban ban of women rights to education in 2022 and has been manually annotated according to Ekman emotion categories. We here detail the data collection and annotation process, present relevant dataset statistics as well as initial experiments on the resulting dataset, benchmarking a number of different neural architectures for the task of Dari emotion classification.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#21512;&#20316;&#26500;&#24314;&#30340;CBBQ&#20013;&#25991;&#20559;&#24046;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20840;&#38754;&#34913;&#37327;&#20102;&#19982;&#20013;&#22269;&#25991;&#21270;&#21644;&#20215;&#20540;&#35266;&#30456;&#20851;&#30340;14&#20010;&#31038;&#20250;&#32500;&#24230;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#31038;&#20250;&#20559;&#35265;&#65292;&#23545;&#20110;&#26816;&#27979;&#27169;&#22411;&#20559;&#35265;&#20855;&#26377;&#24191;&#27867;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#39640;&#24230;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16244</link><description>&lt;p&gt;
CBBQ: &#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#21512;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20013;&#25991;&#20559;&#24046;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models. (arXiv:2306.16244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16244
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#21512;&#20316;&#26500;&#24314;&#30340;CBBQ&#20013;&#25991;&#20559;&#24046;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20840;&#38754;&#34913;&#37327;&#20102;&#19982;&#20013;&#22269;&#25991;&#21270;&#21644;&#20215;&#20540;&#35266;&#30456;&#20851;&#30340;14&#20010;&#31038;&#20250;&#32500;&#24230;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#31038;&#20250;&#20559;&#35265;&#65292;&#23545;&#20110;&#26816;&#27979;&#27169;&#22411;&#20559;&#35265;&#20855;&#26377;&#24191;&#27867;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#39640;&#24230;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#38754;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#20559;&#35265;&#23545;&#20110;&#26816;&#27979;&#21644;&#38477;&#20302;&#39640;&#33021;&#21147;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36947;&#24503;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;&#20154;&#31867;&#19987;&#23478;&#21644;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20849;&#21516;&#26500;&#24314;&#30340;&#20013;&#25991;&#20559;&#24046;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19982;&#20013;&#22269;&#25991;&#21270;&#21644;&#20215;&#20540;&#35266;&#30456;&#20851;&#30340;14&#20010;&#31038;&#20250;&#32500;&#24230;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#31038;&#20250;&#20559;&#35265;&#12290;&#22312;&#25968;&#25454;&#38598;&#30340;&#25972;&#29702;&#36807;&#31243;&#20013;&#65292;&#21253;&#25324;4&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#36890;&#36807;&#24191;&#27867;&#30340;&#25991;&#29486;&#35780;&#35770;&#35782;&#21035;&#20559;&#35265;&#65292;&#29983;&#25104;&#27169;&#31946;&#30340;&#19978;&#19979;&#25991;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#28040;&#38500;&#27169;&#31946;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#21450;&#25163;&#21160;&#23457;&#26597;&#21644;&#37325;&#32452;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27979;&#35797;&#23454;&#20363;&#26159;&#20174;3000&#22810;&#20010;&#32463;&#36807;&#20005;&#26684;&#36136;&#37327;&#25511;&#21046;&#30340;&#39640;&#36136;&#37327;&#27169;&#26495;&#25163;&#21160;&#25552;&#21462;&#30340;&#12290;&#25968;&#25454;&#38598;&#20855;&#26377;&#24191;&#27867;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#39640;&#24230;&#30340;&#22810;&#26679;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#25968;&#25454;&#38598;&#22312;&#26816;&#27979;&#27169;&#22411;&#20559;&#35265;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;10&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22343;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Holistically measuring societal biases of large language models is crucial for detecting and reducing ethical risks in highly capable AI models. In this work, we present a Chinese Bias Benchmark dataset that consists of over 100K questions jointly constructed by human experts and generative language models, covering stereotypes and societal biases in 14 social dimensions related to Chinese culture and values. The curation process contains 4 essential steps: bias identification via extensive literature review, ambiguous context generation, AI-assisted disambiguous context generation, snd manual review \&amp; recomposition. The testing instances in the dataset are automatically derived from 3K+ high-quality templates manually authored with stringent quality control. The dataset exhibits wide coverage and high diversity. Extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available Chinese large language models exhibiting strong bia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#34892;&#21160;&#21644;&#25351;&#20196;&#25512;&#26029;&#21512;&#20316;&#22242;&#38431;&#30340;&#30446;&#26631;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16207</link><description>&lt;p&gt;
&#20174;&#34892;&#21160;&#21644;&#25351;&#20196;&#20013;&#25512;&#26029;&#27807;&#36890;&#20195;&#29702;&#30340;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Inferring the Goals of Communicating Agents from Actions and Instructions. (arXiv:2306.16207v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#34892;&#21160;&#21644;&#25351;&#20196;&#25512;&#26029;&#21512;&#20316;&#22242;&#38431;&#30340;&#30446;&#26631;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#21512;&#20316;&#26102;&#65292;&#20182;&#20204;&#32463;&#24120;&#36890;&#36807;&#21475;&#22836;&#27807;&#36890;&#21644;&#38750;&#21475;&#22836;&#34892;&#21160;&#26469;&#21327;&#35843;&#27963;&#21160;&#65292;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#25512;&#26029;&#20849;&#21516;&#30340;&#30446;&#26631;&#21644;&#35745;&#21010;&#12290;&#25105;&#20204;&#22914;&#20309;&#24314;&#27169;&#36825;&#31181;&#25512;&#29702;&#33021;&#21147;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21512;&#20316;&#22242;&#38431;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#65288;&#20027;&#35201;&#30340;&#65289;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21521;&#21478;&#19968;&#20010;&#20195;&#29702;&#65288;&#21161;&#29702;&#65289;&#20256;&#36798;&#20851;&#20110;&#20849;&#21516;&#35745;&#21010;&#30340;&#20449;&#24687;&#65292;&#20351;&#29992;GPT-3&#20316;&#20026;&#25351;&#20196;&#35821;&#21477;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31532;&#19977;&#26041;&#35266;&#23519;&#32773;&#22914;&#20309;&#36890;&#36807;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#36870;&#21521;&#35268;&#21010;&#20174;&#34892;&#21160;&#21644;&#25351;&#20196;&#20013;&#25512;&#26029;&#22242;&#38431;&#30340;&#30446;&#26631;&#65292;&#35745;&#31639;&#22312;&#20195;&#29702;&#20154;&#20250;&#37319;&#21462;&#21644;&#20132;&#27969;&#20197;&#23454;&#29616;&#30446;&#26631;&#30340;&#20551;&#35774;&#19979;&#30340;&#30446;&#26631;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#32593;&#26684;&#19990;&#30028;&#20013;&#30340;&#20154;&#31867;&#30446;&#26631;&#25512;&#26029;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#65292;&#21457;&#29616;&#25105;&#20204;&#27169;&#22411;&#30340;&#25512;&#26029;&#19982;&#20154;&#31867;&#21028;&#26029;&#23494;&#20999;&#30456;&#20851;&#65288;R = 0.96&#65289;&#12290;&#19982;&#20165;&#20174;&#34892;&#21160;&#25512;&#26029;&#30456;&#27604;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
When humans cooperate, they frequently coordinate their activity through both verbal communication and non-verbal actions, using this information to infer a shared goal and plan. How can we model this inferential ability? In this paper, we introduce a model of a cooperative team where one agent, the principal, may communicate natural language instructions about their shared plan to another agent, the assistant, using GPT-3 as a likelihood function for instruction utterances. We then show how a third person observer can infer the team's goal via multi-modal Bayesian inverse planning from actions and instructions, computing the posterior distribution over goals under the assumption that agents will act and communicate rationally to achieve them. We evaluate this approach by comparing it with human goal inferences in a multi-agent gridworld, finding that our model's inferences closely correlate with human judgments (R = 0.96). When compared to inference from actions alone, we also find th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#25552;&#20379;&#26377;&#25928;&#23398;&#20064;&#30340;&#22242;&#38431;&#32467;&#26500;&#65292;&#21457;&#29616;&#26576;&#20123;&#22242;&#38431;&#32467;&#26500;&#26377;&#21161;&#20110;&#20195;&#29702;&#23398;&#20064;&#19987;&#19994;&#21270;&#21040;&#20855;&#20307;&#35282;&#33394;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#25972;&#20307;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22242;&#38431;&#23384;&#22312;&#36131;&#20219;&#24402;&#22240;&#38590;&#39064;&#65292;&#23548;&#33268;&#21327;&#35843;&#24615;&#38477;&#20302;&#65292;&#22823;&#22242;&#38431;&#34920;&#29616;&#36739;&#23567;&#22242;&#38431;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.16205</link><description>&lt;p&gt;
&#36816;&#29992;&#22810;&#26234;&#33021;&#20307;&#22242;&#38431;&#23454;&#29616;&#26356;&#22909;&#30340;&#23398;&#20064;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards a Better Understanding of Learning with Multiagent Teams. (arXiv:2306.16205v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#25552;&#20379;&#26377;&#25928;&#23398;&#20064;&#30340;&#22242;&#38431;&#32467;&#26500;&#65292;&#21457;&#29616;&#26576;&#20123;&#22242;&#38431;&#32467;&#26500;&#26377;&#21161;&#20110;&#20195;&#29702;&#23398;&#20064;&#19987;&#19994;&#21270;&#21040;&#20855;&#20307;&#35282;&#33394;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#25972;&#20307;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22242;&#38431;&#23384;&#22312;&#36131;&#20219;&#24402;&#22240;&#38590;&#39064;&#65292;&#23548;&#33268;&#21327;&#35843;&#24615;&#38477;&#20302;&#65292;&#22823;&#22242;&#38431;&#34920;&#29616;&#36739;&#23567;&#22242;&#38431;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38271;&#26399;&#20197;&#26469;&#20154;&#20204;&#24050;&#32463;&#35748;&#35782;&#21040;&#19968;&#20010;&#30001;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#32452;&#25104;&#30340;&#22242;&#38431;&#21487;&#20197;&#36229;&#36234;&#20854;&#20010;&#20307;&#30340;&#33021;&#21147;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35268;&#27169;&#36739;&#22823;&#30340;&#22242;&#38431;&#24182;&#19981;&#19968;&#23450;&#27604;&#35268;&#27169;&#36739;&#23567;&#30340;&#22242;&#38431;&#26356;&#26377;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20026;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#25552;&#20379;&#26377;&#25928;&#23398;&#20064;&#30340;&#26576;&#20123;&#22242;&#38431;&#32467;&#26500;&#20026;&#20309;&#20197;&#21450;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#33021;&#22815;&#25552;&#20379;&#26377;&#25928;&#23398;&#20064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#65292;&#26576;&#20123;&#22242;&#38431;&#32467;&#26500;&#26377;&#21161;&#20110;&#20195;&#29702;&#23398;&#20064;&#19987;&#19994;&#21270;&#21040;&#20855;&#20307;&#35282;&#33394;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#25972;&#20307;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22242;&#38431;&#23384;&#22312;&#36131;&#20219;&#24402;&#22240;&#38590;&#39064;&#65292;&#23548;&#33268;&#21327;&#35843;&#24615;&#38477;&#20302;&#65292;&#22823;&#22242;&#38431;&#34920;&#29616;&#36739;&#23567;&#22242;&#38431;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
While it has long been recognized that a team of individual learning agents can be greater than the sum of its parts, recent work has shown that larger teams are not necessarily more effective than smaller ones. In this paper, we study why and under which conditions certain team structures promote effective learning for a population of individual learning agents. We show that, depending on the environment, some team structures help agents learn to specialize into specific roles, resulting in more favorable global results. However, large teams create credit assignment challenges that reduce coordination, leading to large teams performing poorly compared to smaller ones. We support our conclusions with both theoretical analysis and empirical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#22270;&#30693;&#35782;&#32858;&#21512;&#26469;&#25552;&#21319;&#23545;&#35805;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#26469;&#33258;&#24086;&#23376;&#21644;&#22806;&#37096;&#22270;&#30693;&#35782;&#30340;&#24322;&#36136;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#22270;&#30693;&#35782;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16195</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#22270;&#30693;&#35782;&#32858;&#21512;&#25552;&#21319;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation. (arXiv:2306.16195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#22270;&#30693;&#35782;&#32858;&#21512;&#26469;&#25552;&#21319;&#23545;&#35805;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#26469;&#33258;&#24086;&#23376;&#21644;&#22806;&#37096;&#22270;&#30693;&#35782;&#30340;&#24322;&#36136;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#22270;&#30693;&#35782;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22806;&#37096;&#22270;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#23545;&#35805;&#26426;&#22120;&#20154;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#25552;&#21319;&#23545;&#35805;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#65292;&#22270;&#19978;&#30340;&#20449;&#24687;&#20256;&#36882;&#19982;&#25991;&#26412;&#26080;&#20851;&#65292;&#23548;&#33268;&#22270;&#34920;&#24449;&#21644;&#25991;&#26412;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#24322;&#12290;&#29616;&#26377;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#24335;&#23548;&#33268;&#20102;&#22270;&#30693;&#35782;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#22270;&#22686;&#24378;&#23545;&#35805;&#29983;&#25104;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21160;&#24577;&#26500;&#24314;&#19968;&#20010;&#24102;&#26377;&#20266;&#33410;&#28857;&#30340;&#22810;&#36339;&#30693;&#35782;&#22270;&#65292;&#22312;&#22270;&#20013;&#30340;&#27599;&#19968;&#27493;&#20013;&#37117;&#23558;&#35821;&#35328;&#27169;&#22411;&#32435;&#20837;&#29305;&#24449;&#32858;&#21512;&#12290;&#20026;&#20102;&#36991;&#20813;&#23398;&#20064;&#22312;&#26222;&#36890;&#23376;&#22270;&#19978;&#24341;&#36215;&#30340;&#35821;&#20041;&#20559;&#24046;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20102;&#20998;&#23618;&#22270;&#27880;&#24847;&#21147;&#65292;&#20197;&#32858;&#21512;&#20266;&#33410;&#28857;&#19978;&#30340;&#22270;&#29305;&#24449;&#65292;&#26368;&#32456;&#33719;&#24471;&#20840;&#23616;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#26469;&#33258;&#24086;&#23376;&#21644;&#22806;&#37096;&#22270;&#30693;&#35782;&#30340;&#24322;&#36136;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating external graph knowledge into neural chatbot models has been proven effective for enhancing dialogue generation. However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing from that of the text. This training regime of existing models therefore leads to a semantic gap between graph knowledge and text. In this study, we propose a novel framework for knowledge graph enhanced dialogue generation. We dynamically construct a multi-hop knowledge graph with pseudo nodes to involve the language model in feature aggregation within the graph at all steps. To avoid the semantic biases caused by learning on vanilla subgraphs, the proposed framework applies hierarchical graph attention to aggregate graph features on pseudo nodes and then attains a global feature. Therefore, the framework can better utilise the heterogeneous features from both the post and external graph knowle
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;Segment Anything Model (SAM)&#20013;&#24341;&#20837;&#21644;&#35757;&#32451;&#19968;&#32452;&#38754;&#26009;&#32570;&#38519;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#26080;&#32541;&#22320;&#23558;&#19987;&#19994;&#30693;&#35782;&#27880;&#20837;&#21040;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38754;&#26009;&#32570;&#38519;&#20998;&#21106;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.16186</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#23450;&#30340;&#30693;&#35782;&#27880;&#20837;&#65292;&#26377;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#36716;&#31227;&#21040;&#38754;&#26009;&#32570;&#38519;&#20998;&#21106;&#20013;
&lt;/p&gt;
&lt;p&gt;
Effective Transfer of Pretrained Large Visual Model for Fabric Defect Segmentation via Specifc Knowledge Injection. (arXiv:2306.16186v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;Segment Anything Model (SAM)&#20013;&#24341;&#20837;&#21644;&#35757;&#32451;&#19968;&#32452;&#38754;&#26009;&#32570;&#38519;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#26080;&#32541;&#22320;&#23558;&#19987;&#19994;&#30693;&#35782;&#27880;&#20837;&#21040;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38754;&#26009;&#32570;&#38519;&#20998;&#21106;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#26009;&#32570;&#38519;&#20998;&#21106;&#26159;&#32442;&#32455;&#21697;&#36136;&#37327;&#25511;&#21046;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#39640;&#36136;&#37327;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#38754;&#26009;&#32570;&#38519;&#30340;&#22810;&#26679;&#24615;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#24212;&#29992;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#22240;&#32032;&#38480;&#21046;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#27867;&#21270;&#21644;&#20998;&#21106;&#24615;&#33021;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22788;&#29702;&#21508;&#31181;&#19981;&#21516;&#38754;&#26009;&#31867;&#22411;&#21644;&#32570;&#38519;&#30340;&#22797;&#26434;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#38754;&#26009;&#32570;&#38519;&#30340;&#19987;&#19994;&#30693;&#35782;&#27880;&#20837;&#21040;Segment Anything Model (SAM) &#36825;&#20010;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#24341;&#20837;&#24182;&#35757;&#32451;&#19968;&#32452;&#29420;&#29305;&#30340;&#19982;&#38754;&#26009;&#32570;&#38519;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#23545;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#22823;&#37327;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#32541;&#22320;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;SAM&#20013;&#12290;&#25913;&#36827;&#21518;&#30340;SAM&#27169;&#22411;&#21033;&#29992;&#20174;&#22823;&#35268;&#27169;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#23398;&#21040;&#30340;&#24191;&#20041;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#38754;&#26009;&#32570;&#38519;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fabric defect segmentation is integral to textile quality control. Despite this, the scarcity of high-quality annotated data and the diversity of fabric defects present significant challenges to the application of deep learning in this field. These factors limit the generalization and segmentation performance of existing models, impeding their ability to handle the complexity of diverse fabric types and defects. To overcome these obstacles, this study introduces an innovative method to infuse specialized knowledge of fabric defects into the Segment Anything Model (SAM), a large-scale visual model. By introducing and training a unique set of fabric defect-related parameters, this approach seamlessly integrates domain-specific knowledge into SAM without the need for extensive modifications to the pre-existing model parameters. The revamped SAM model leverages generalized image understanding learned from large-scale natural image datasets while incorporating fabric defect-specific knowled
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#28508;&#21147;&#21644;&#24212;&#29992;&#24191;&#27867;&#24615;&#65292;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#23450;&#20041;&#30340;&#20887;&#20313;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16177</link><description>&lt;p&gt;
&#23450;&#20041;&#25968;&#25454;&#31185;&#23398;&#65306;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defining data science: a new field of inquiry. (arXiv:2306.16177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16177
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#33539;&#24335;&#65292;&#20855;&#26377;&#28508;&#21147;&#21644;&#24212;&#29992;&#24191;&#27867;&#24615;&#65292;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#23450;&#20041;&#30340;&#20887;&#20313;&#21644;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#19981;&#26159;&#19968;&#38376;&#31185;&#23398;&#65292;&#32780;&#26159;&#19968;&#31181;&#30740;&#31350;&#33539;&#24335;&#12290;&#23427;&#30340;&#21147;&#37327;&#12289;&#33539;&#22260;&#21644;&#35268;&#27169;&#23558;&#36229;&#36234;&#31185;&#23398;&#65292;&#25104;&#20026;&#20419;&#20351;&#30693;&#35782;&#21457;&#29616;&#24182;&#25913;&#21464;&#19990;&#30028;&#30340;&#37325;&#35201;&#25163;&#27573;&#12290;&#25105;&#20204;&#23578;&#26410;&#29702;&#35299;&#21644;&#23450;&#20041;&#23427;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#20854;&#28508;&#21147;&#21644;&#31649;&#29702;&#20854;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#33258;1962&#24180;&#20197;&#26469;&#32531;&#24930;&#21457;&#23637;&#65292;&#24182;&#19988;&#33258;2000&#24180;&#20197;&#26469;&#21457;&#23637;&#36805;&#36895;&#65292;&#23427;&#26159;&#19968;&#31181;&#26681;&#26412;&#24615;&#30340;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26159;21&#19990;&#32426;&#26368;&#27963;&#36291;&#12289;&#26368;&#24378;&#22823;&#21644;&#21457;&#23637;&#26368;&#24555;&#30340;&#21019;&#26032;&#20043;&#19968;&#12290;&#30001;&#20110;&#20854;&#20215;&#20540;&#12289;&#21147;&#37327;&#21644;&#36866;&#29992;&#24615;&#65292;&#23427;&#27491;&#22312;40&#22810;&#20010;&#23398;&#31185;&#12289;&#25968;&#30334;&#20010;&#30740;&#31350;&#39046;&#22495;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#25968;&#20197;&#30334;&#19975;&#35745;&#30340;&#25968;&#25454;&#31185;&#23398;&#20986;&#29256;&#29289;&#20013;&#21253;&#21547;&#20102;&#26080;&#25968;&#20851;&#20110;&#25968;&#25454;&#31185;&#23398;&#21644;&#25968;&#25454;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#23450;&#20041;&#12290;&#30001;&#20110;&#20854;&#36215;&#27493;&#38454;&#27573;&#65292;&#35768;&#22810;&#23450;&#20041;&#26159;&#29420;&#31435;&#30340;&#12289;&#24212;&#29992;&#29305;&#23450;&#30340;&#12289;&#30456;&#20114;&#19981;&#23436;&#25972;&#30340;&#12289;&#20887;&#20313;&#30340;&#25110;&#19981;&#19968;&#33268;&#30340;&#65292;&#22240;&#27492;&#25968;&#25454;&#31185;&#23398;&#20063;&#26159;&#22914;&#27492;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#22810;&#37325;&#23450;&#20041;&#25361;&#25112;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data science is not a science. It is a research paradigm. Its power, scope, and scale will surpass science, our most powerful research paradigm, to enable knowledge discovery and change our world. We have yet to understand and define it, vital to realizing its potential and managing its risks. Modern data science is in its infancy. Emerging slowly since 1962 and rapidly since 2000, it is a fundamentally new field of inquiry, one of the most active, powerful, and rapidly evolving 21st century innovations. Due to its value, power, and applicability, it is emerging in 40+ disciplines, hundreds of research areas, and thousands of applications. Millions of data science publications contain myriad definitions of data science and data science problem solving. Due to its infancy, many definitions are independent, application-specific, mutually incomplete, redundant, or inconsistent, hence so is data science. This research addresses this data science multiple definitions challenge by proposing 
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#32508;&#36848;&#30740;&#31350;&#20102;&#28304;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#37327;&#21644;&#20811;&#38534;&#26816;&#27979;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#65292;&#21457;&#29616;&#20102;80&#31181;&#36719;&#20214;&#24037;&#20855;&#65292;&#28041;&#21450;&#20102;&#22810;&#20010;&#32534;&#31243;&#35821;&#35328;&#65292;&#20026;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2306.16171</link><description>&lt;p&gt;
&#19968;&#20221;&#20851;&#20110;&#28304;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#37327;&#21644;&#20811;&#38534;&#26816;&#27979;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65306;&#25216;&#26415;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A systematic literature review on source code similarity measurement and clone detection: techniques, applications, and challenges. (arXiv:2306.16171v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16171
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#32508;&#36848;&#30740;&#31350;&#20102;&#28304;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#37327;&#21644;&#20811;&#38534;&#26816;&#27979;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#65292;&#21457;&#29616;&#20102;80&#31181;&#36719;&#20214;&#24037;&#20855;&#65292;&#28041;&#21450;&#20102;&#22810;&#20010;&#32534;&#31243;&#35821;&#35328;&#65292;&#20026;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#21644;&#35780;&#20272;&#28304;&#20195;&#30721;&#30456;&#20284;&#24615;&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;&#36719;&#20214;&#24037;&#31243;&#27963;&#21160;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#20195;&#30721;&#25512;&#33616;&#12289;&#37325;&#22797;&#20195;&#30721;&#12289;&#25220;&#34989;&#12289;&#24694;&#24847;&#36719;&#20214;&#21644;&#20195;&#30721;&#36136;&#37327;&#26816;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20221;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#20803;&#20998;&#26512;&#65292;&#20851;&#20110;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#37327;&#21644;&#35780;&#20272;&#25216;&#26415;&#65292;&#20197;&#25581;&#31034;&#29616;&#26377;&#26041;&#27861;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#26597;&#35810;&#22235;&#20010;&#25968;&#23383;&#22270;&#20070;&#39302;&#26368;&#21021;&#25214;&#21040;&#20102;&#36229;&#36807;10000&#31687;&#25991;&#31456;&#65292;&#24182;&#26368;&#32456;&#36873;&#25321;&#20102;136&#20010;&#20027;&#35201;&#30740;&#31350;&#12290;&#36825;&#20123;&#30740;&#31350;&#26681;&#25454;&#20854;&#26041;&#27861;&#23398;&#12289;&#32534;&#31243;&#35821;&#35328;&#12289;&#25968;&#25454;&#38598;&#12289;&#24037;&#20855;&#21644;&#24212;&#29992;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#28145;&#20837;&#30740;&#31350;&#21457;&#29616;&#20102;80&#31181;&#36719;&#20214;&#24037;&#20855;&#65292;&#20351;&#29992;&#20102;&#20843;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#28041;&#21450;&#20116;&#20010;&#24212;&#29992;&#39046;&#22495;&#12290;&#36817;49%&#30340;&#24037;&#20855;&#36866;&#29992;&#20110;Java&#31243;&#24207;&#65292;37%&#25903;&#25345;C&#21644;C++&#65292;&#32780;&#35768;&#22810;&#32534;&#31243;&#35821;&#35328;&#27809;&#26377;&#30456;&#24212;&#30340;&#25903;&#25345;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#19968;&#28857;&#26159;&#23384;&#22312;&#30528;&#12290;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring and evaluating source code similarity is a fundamental software engineering activity that embraces a broad range of applications, including but not limited to code recommendation, duplicate code, plagiarism, malware, and smell detection. This paper proposes a systematic literature review and meta-analysis on code similarity measurement and evaluation techniques to shed light on the existing approaches and their characteristics in different applications. We initially found over 10000 articles by querying four digital libraries and ended up with 136 primary studies in the field. The studies were classified according to their methodology, programming languages, datasets, tools, and applications. A deep investigation reveals 80 software tools, working with eight different techniques on five application domains. Nearly 49% of the tools work on Java programs and 37% support C and C++, while there is no support for many programming languages. A noteworthy point was the existence of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#26367;&#20195;&#27169;&#22411;&#30340;&#22312;&#32447;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#29983;&#25104;&#25968;&#20540;&#27169;&#25311;&#21644;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25233;&#21046;&#20102;&#30913;&#30424;&#21152;&#36733;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#29942;&#39048;&#65292;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.16133</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#22312;&#32447;&#23398;&#20064;&#35757;&#32451;&#28145;&#24230;&#26367;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Deep Surrogate Models with Large Scale Online Learning. (arXiv:2306.16133v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#26367;&#20195;&#27169;&#22411;&#30340;&#22312;&#32447;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#29983;&#25104;&#25968;&#20540;&#27169;&#25311;&#21644;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25233;&#21046;&#20102;&#30913;&#30424;&#21152;&#36733;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#29942;&#39048;&#65292;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26102;&#31354;&#20998;&#36776;&#29575;&#22312;&#25551;&#36848;&#19990;&#30028;&#30340;&#29289;&#29702;&#29616;&#35937;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#31185;&#23398;&#23478;&#21644;&#24037;&#31243;&#24072;&#36890;&#36807;&#20351;&#29992;&#35745;&#31639;&#22797;&#26434;&#30340;&#27714;&#35299;&#22120;&#26469;&#25968;&#20540;&#27714;&#35299;PDE&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24050;&#25104;&#20026;&#33719;&#24471;PDE&#24555;&#36895;&#35299;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#27714;&#35299;&#22120;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23384;&#20648;&#22312;&#30913;&#30424;&#19978;&#24182;&#35835;&#21462;&#22238;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#25351;&#20986;&#65292;&#20381;&#36182;&#20256;&#32479;&#38745;&#24577;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#24182;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#27714;&#35299;&#22120;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#30340;&#20840;&#37096;&#20248;&#21183;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#28304;&#30340;&#29992;&#20110;&#28145;&#24230;&#26367;&#20195;&#27169;&#22411;&#30340;&#22312;&#32447;&#35757;&#32451;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#20010;&#23618;&#27425;&#30340;&#24182;&#34892;&#24615;&#65292;&#21516;&#26102;&#29983;&#25104;&#25968;&#20540;&#27169;&#25311;&#21644;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25233;&#21046;&#19982;&#30913;&#30424;&#21152;&#36733;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;I/O&#21644;&#23384;&#20648;&#29942;&#39048;&#65292;&#24182;&#20026;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#24320;&#36767;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spatiotemporal resolution of Partial Differential Equations (PDEs) plays important roles in the mathematical description of the world's physical phenomena. In general, scientists and engineers solve PDEs numerically by the use of computationally demanding solvers. Recently, deep learning algorithms have emerged as a viable alternative for obtaining fast solutions for PDEs. Models are usually trained on synthetic data generated by solvers, stored on disk and read back for training. This paper advocates that relying on a traditional static dataset to train these models does not allow the full benefit of the solver to be used as a data generator. It proposes an open source online training framework for deep surrogate models. The framework implements several levels of parallelism focused on simultaneously generating numerical simulations and training deep neural networks. This approach suppresses the I/O and storage bottleneck associated with disk-loaded datasets, and opens the way to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#22235;&#20010;&#39044;&#27979;&#23376;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#36755;&#20837;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16125</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#26694;&#26550;&#65306;MentalRiskES@IberLEF 2023
&lt;/p&gt;
&lt;p&gt;
A Framework for Identifying Depression on Social Media: MentalRiskES@IberLEF 2023. (arXiv:2306.16125v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16125
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#22235;&#20010;&#39044;&#27979;&#23376;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#36755;&#20837;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#19982;IberLEF 2023&#30340;MentalRiskES&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#26681;&#25454;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#27963;&#21160;&#26469;&#39044;&#27979;&#20182;&#20204;&#21487;&#33021;&#24739;&#25233;&#37057;&#30151;&#30340;&#21487;&#33021;&#24615;&#12290;&#25968;&#25454;&#38598;&#30001;175&#20010;Telegram&#29992;&#25143;&#30340;&#23545;&#35805;&#32452;&#25104;&#65292;&#27599;&#20010;&#29992;&#25143;&#26681;&#25454;&#20182;&#20204;&#24739;&#30149;&#35777;&#25454;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#22235;&#20010;&#39044;&#27979;&#23376;&#20219;&#21153;&#65306;&#20108;&#20998;&#31867;&#12289;&#31616;&#21333;&#22238;&#24402;&#12289;&#22810;&#31867;&#21035;&#20998;&#31867;&#21644;&#22810;&#31867;&#21035;&#22238;&#24402;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#31867;&#21035;&#22238;&#24402;&#38382;&#39064;&#65292;&#28982;&#21518;&#23558;&#39044;&#27979;&#32467;&#26524;&#36716;&#25442;&#20026;&#36866;&#29992;&#20110;&#20854;&#20182;&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#24314;&#27169;&#26041;&#27861;&#30340;&#24615;&#33021;&#65306;&#23545;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21644;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#36755;&#20837;&#65292;&#21518;&#32773;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#22797;&#29616;&#25105;&#20204;&#32467;&#26524;&#30340;&#20195;&#30721;&#65306;https://github.com/simonsanvil/EarlyDep
&lt;/p&gt;
&lt;p&gt;
This paper describes our participation in the MentalRiskES task at IberLEF 2023. The task involved predicting the likelihood of an individual experiencing depression based on their social media activity. The dataset consisted of conversations from 175 Telegram users, each labeled according to their evidence of suffering from the disorder. We used a combination of traditional machine learning and deep learning techniques to solve four predictive subtasks: binary classification, simple regression, multiclass classification, and multiclass regression. We approached this by training a model to solve the multiclass regression case and then transforming the predictions to work for the other three subtasks. We compare the performance of two different modeling approaches: fine-tuning a BERT-based model and using sentence embeddings as inputs to a linear regressor, with the latter yielding better results. The code to reproduce our results can be found at: https://github.com/simonsanvil/EarlyDep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32463;&#39564;&#20998;&#26512;&#20102;&#21452;&#26354;&#27491;&#20999;&#12289;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#21644;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#28608;&#27963;&#20989;&#25968;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26354;&#38754;&#65292;&#21457;&#29616;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#21576;&#29616;&#26368;&#20984;&#22411;&#26354;&#38754;&#65292;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#21576;&#29616;&#26368;&#24179;&#22374;&#26354;&#38754;&#24182;&#20855;&#26377;&#26356;&#20248;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25152;&#26377;&#28608;&#27963;&#20989;&#25968;&#30340;&#25439;&#22833;&#26354;&#38754;&#20013;&#23384;&#22312;&#23485;&#38420;&#21644;&#29421;&#31364;&#30340;&#23665;&#35895;&#65292;&#32780;&#29421;&#31364;&#30340;&#23665;&#35895;&#19982;&#39281;&#21644;&#31070;&#32463;&#20803;&#21644;&#38544;&#21547;&#30340;&#27491;&#21017;&#21270;&#32593;&#32476;&#32467;&#26500;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2306.16090</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#30340;&#32463;&#39564;&#25439;&#22833;&#26354;&#38754;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical Loss Landscape Analysis of Neural Network Activation Functions. (arXiv:2306.16090v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32463;&#39564;&#20998;&#26512;&#20102;&#21452;&#26354;&#27491;&#20999;&#12289;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#21644;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#28608;&#27963;&#20989;&#25968;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26354;&#38754;&#65292;&#21457;&#29616;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#21576;&#29616;&#26368;&#20984;&#22411;&#26354;&#38754;&#65292;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#21576;&#29616;&#26368;&#24179;&#22374;&#26354;&#38754;&#24182;&#20855;&#26377;&#26356;&#20248;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25152;&#26377;&#28608;&#27963;&#20989;&#25968;&#30340;&#25439;&#22833;&#26354;&#38754;&#20013;&#23384;&#22312;&#23485;&#38420;&#21644;&#29421;&#31364;&#30340;&#23665;&#35895;&#65292;&#32780;&#29421;&#31364;&#30340;&#23665;&#35895;&#19982;&#39281;&#21644;&#31070;&#32463;&#20803;&#21644;&#38544;&#21547;&#30340;&#27491;&#21017;&#21270;&#32593;&#32476;&#32467;&#26500;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#22312;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#20250;&#24433;&#21709;&#25439;&#22833;&#26354;&#38754;&#30340;&#24615;&#36136;&#12290;&#20102;&#35299;&#28608;&#27963;&#20989;&#25968;&#19982;&#25439;&#22833;&#26354;&#38754;&#24615;&#36136;&#30340;&#20851;&#31995;&#23545;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#35774;&#35745;&#26159;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#20174;&#32463;&#39564;&#19978;&#20998;&#26512;&#20102;&#19982;&#21452;&#26354;&#27491;&#20999;&#12289;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#21644;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#28608;&#27963;&#20989;&#25968;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#26354;&#38754;&#12290;&#23454;&#39564;&#35777;&#26126;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#20135;&#29983;&#26368;&#20984;&#22411;&#30340;&#25439;&#22833;&#26354;&#38754;&#65292;&#25351;&#25968;&#32447;&#24615;&#21333;&#20803;&#20135;&#29983;&#26368;&#24179;&#22374;&#30340;&#25439;&#22833;&#26354;&#38754;&#65292;&#24182;&#19988;&#23637;&#29616;&#20986;&#26356;&#20248;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23545;&#20110;&#25152;&#26377;&#28608;&#27963;&#20989;&#25968;&#65292;&#25439;&#22833;&#26354;&#38754;&#20013;&#23384;&#22312;&#23485;&#38420;&#21644;&#29421;&#31364;&#30340;&#23665;&#35895;&#65292;&#24182;&#19988;&#29421;&#31364;&#30340;&#23665;&#35895;&#19982;&#39281;&#21644;&#31070;&#32463;&#20803;&#21644;&#38544;&#21547;&#30340;&#27491;&#21017;&#21270;&#32593;&#32476;&#32467;&#26500;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activation functions play a significant role in neural network design by enabling non-linearity. The choice of activation function was previously shown to influence the properties of the resulting loss landscape. Understanding the relationship between activation functions and loss landscape properties is important for neural architecture and training algorithm design. This study empirically investigates neural network loss landscapes associated with hyperbolic tangent, rectified linear unit, and exponential linear unit activation functions. Rectified linear unit is shown to yield the most convex loss landscape, and exponential linear unit is shown to yield the least flat loss landscape, and to exhibit superior generalisation performance. The presence of wide and narrow valleys in the loss landscape is established for all activation functions, and the narrow valleys are shown to correlate with saturated neurons and implicitly regularised network configurations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#24403;&#21069;&#36187;&#36710;&#27169;&#25311;&#23384;&#22312;&#30340;&#31890;&#24230;&#12289;&#24314;&#27169;&#21644;&#25163;&#21160;&#36755;&#20837;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#20223;&#30495;&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#21270;&#20102;&#31574;&#30053;&#20915;&#31574;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#20223;&#30495;&#19982;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#20248;&#21270;&#36187;&#36710;&#27604;&#36187;&#31574;&#30053;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2306.16088</link><description>&lt;p&gt;
&#25484;&#25569;&#21271;&#29615;&#8212;&#20840;&#38754;&#30340;AI&#31574;&#30053;&#20915;&#31574;&#36187;&#36710;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Mastering Nordschleife -- A comprehensive race simulation for AI strategy decision-making in motorsports. (arXiv:2306.16088v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24403;&#21069;&#36187;&#36710;&#27169;&#25311;&#23384;&#22312;&#30340;&#31890;&#24230;&#12289;&#24314;&#27169;&#21644;&#25163;&#21160;&#36755;&#20837;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#20223;&#30495;&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#21270;&#20102;&#31574;&#30053;&#20915;&#31574;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#20223;&#30495;&#19982;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#20248;&#21270;&#36187;&#36710;&#27604;&#36187;&#31574;&#30053;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36187;&#36710;&#36816;&#21160;&#39046;&#22495;&#65292;&#27604;&#36187;&#31574;&#30053;&#22312;&#20915;&#23450;&#27604;&#36187;&#32467;&#26524;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#19968;&#31574;&#30053;&#38598;&#20013;&#22312;&#20572;&#36710;&#26102;&#38388;&#30340;&#36873;&#25321;&#19978;&#65292;&#36825;&#26159;&#30001;&#20110;&#29123;&#27833;&#28040;&#32791;&#21644;&#36718;&#32974;&#24615;&#33021;&#36864;&#21270;&#32780;&#24517;&#35201;&#30340;&#12290;&#27604;&#36187;&#31574;&#30053;&#30340;&#30446;&#26631;&#26159;&#22312;&#36718;&#32974;&#26356;&#25442;&#21644;&#21152;&#27833;&#31561;&#20572;&#36710;&#20248;&#21183;&#19982;&#22312;&#20572;&#36710;&#21306;&#22495;&#25152;&#20135;&#29983;&#30340;&#26102;&#38388;&#25439;&#22833;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#30446;&#21069;&#30340;&#27604;&#36187;&#27169;&#25311;&#22312;&#31890;&#24230;&#12289;&#27010;&#29575;&#20107;&#20214;&#24314;&#27169;&#21644;&#38656;&#35201;&#25163;&#21160;&#36755;&#20837;&#36827;&#31449;&#26102;&#38388;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#19987;&#20026;GT&#36187;&#36710;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#20223;&#30495;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#26469;&#33258;&#21160;&#21270;&#25112;&#30053;&#20915;&#31574;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#36890;&#36807;&#23558;&#20223;&#30495;&#19982;OpenAI&#30340;Gym&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#26234;&#33021;&#20307;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#37197;&#32622;&#12289;&#35266;&#27979;&#31354;&#38388;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#21033;&#29992;&#21382;&#21490;&#26102;&#38388;&#25968;&#25454;&#36827;&#34892;&#20102;&#32472;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of circuit motorsports, race strategy plays a pivotal role in determining race outcomes. This strategy focuses on the timing of pit stops, which are necessary due to fuel consumption and tire performance degradation. The objective of race strategy is to balance the advantages of pit stops, such as tire replacement and refueling, with the time loss incurred in the pit lane. Current race simulations, used to estimate the best possible race strategy, vary in granularity, modeling of probabilistic events, and require manual input for in-laps. This paper addresses these limitations by developing a novel simulation model tailored to GT racing and leveraging artificial intelligence to automate strategic decisions. By integrating the simulation with OpenAI's Gym framework, a reinforcement learning environment is created and an agent is trained. The study evaluates various hyperparameter configurations, observation spaces, and reward functions, drawing upon historical timing data f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#20445;&#25252;&#38544;&#31169;&#24182;&#22312;&#19978;&#28216;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ZOO-based VFL&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16077</link><description>&lt;p&gt;
&#23433;&#20840;&#39640;&#25928;&#30340;&#24322;&#27493;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;:&#22522;&#20110;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Secure and Fast Asynchronous Vertical Federated Learning via Cascaded Hybrid Optimization. (arXiv:2306.16077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#20445;&#25252;&#38544;&#31169;&#24182;&#22312;&#19978;&#28216;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ZOO-based VFL&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;(VFL)&#22240;&#33021;&#22815;&#22312;&#22402;&#30452;&#20998;&#21106;&#30340;&#25968;&#25454;&#19978;&#32852;&#21512;&#35757;&#32451;&#38544;&#31169;&#20445;&#25252;&#27169;&#22411;&#32780;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24212;&#29992;&#38646;&#38454;&#20248;&#21270;(ZOO)&#22312;&#26500;&#24314;&#23454;&#29992;&#30340;VFL&#31639;&#27861;&#26041;&#38754;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;ZOO&#30340;VFL&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#20854;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#22788;&#29702;&#29616;&#20195;&#22823;&#22411;&#27169;&#22411;&#26102;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;VFL&#20013;&#20351;&#29992;&#32423;&#32852;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20013;&#65292;&#19979;&#28216;&#27169;&#22411;&#65288;&#23458;&#25143;&#31471;&#65289;&#20351;&#29992;ZOO&#36827;&#34892;&#35757;&#32451;&#20197;&#20445;&#25252;&#38544;&#31169;&#24182;&#30830;&#20445;&#19981;&#20849;&#20139;&#20869;&#37096;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#19978;&#28216;&#27169;&#22411;&#65288;&#26381;&#21153;&#22120;&#65289;&#22312;&#26412;&#22320;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;(FOO)&#36827;&#34892;&#26356;&#26032;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#20351;&#24471;&#33021;&#22815;&#22312;&#19981;&#25439;&#23475;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#21069;&#25552;&#19979;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;VFL&#26694;&#26550;&#27604;&#22522;&#20110;ZOO&#30340;VFL&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) attracts increasing attention because it empowers multiple parties to jointly train a privacy-preserving model over vertically partitioned data. Recent research has shown that applying zeroth-order optimization (ZOO) has many advantages in building a practical VFL algorithm. However, a vital problem with the ZOO-based VFL is its slow convergence rate, which limits its application in handling modern large models. To address this problem, we propose a cascaded hybrid optimization method in VFL. In this method, the downstream models (clients) are trained with ZOO to protect privacy and ensure that no internal information is shared. Meanwhile, the upstream model (server) is updated with first-order optimization (FOO) locally, which significantly improves the convergence rate, making it feasible to train the large models without compromising privacy and security. We theoretically prove that our VFL framework converges faster than the ZOO-based VFL, as the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#30340;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20256;&#36755;&#25552;&#31034;&#19982;&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#36828;&#31243;&#21512;&#25104;&#26377;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#36890;&#20449;&#25928;&#29575;&#12289;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#12289;&#25552;&#21319;&#24615;&#33021;&#12289;&#21152;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.16064</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#30340;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Generative Learning with Foundation Models. (arXiv:2306.16064v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#30340;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20256;&#36755;&#25552;&#31034;&#19982;&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#36828;&#31243;&#21512;&#25104;&#26377;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#36890;&#20449;&#25928;&#29575;&#12289;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#12289;&#25552;&#21319;&#24615;&#33021;&#12289;&#21152;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#38598;&#20013;&#22312;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#20256;&#36755;&#29305;&#24449;&#12289;&#21442;&#25968;&#25110;&#26799;&#24230;&#65292;&#36825;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#20302;&#25928;&#21644;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;&#20511;&#21161;&#26032;&#20852;&#30340;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#32852;&#37030;&#29983;&#25104;&#23398;&#20064;&#65292;&#23427;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#20256;&#36755;&#19982;&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#25552;&#31034;&#12290;&#36890;&#36807;&#25509;&#25910;&#21040;&#30340;&#21253;&#21547;&#36739;&#23569;&#38544;&#31169;&#20449;&#24687;&#30340;&#25552;&#31034;&#20197;&#21450;&#22522;&#30784;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36828;&#31243;&#21512;&#25104;&#26377;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20010;&#26032;&#26694;&#26550;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#65292;&#21253;&#25324;&#25913;&#21892;&#20102;&#36890;&#20449;&#25928;&#29575;&#12289;&#26356;&#22909;&#30340;&#36866;&#24212;&#20998;&#24067;&#36716;&#31227;&#12289;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12289;&#21152;&#24378;&#20102;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;ImageNet&#21644;DomainNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing federated learning solutions focus on transmitting features, parameters or gadients between clients and server, which suffer from serious low-efficiency and privacy-leakage problems. Thanks to the emerging foundation generative models, we propose a novel federated learning framework, namely Federated Generative Learning, that transmits prompts associated with distributed training data between clients and server. The informative training data can be synthesized remotely based on received prompts containing little privacy and the foundation generative models. The new framework possesses multiple advantages, including improved communication efficiency, better resilience to distribution shift, substantial performance gains, and enhanced privacy protection, which are verified in extensive experiments on ImageNet and DomainNet datasets.
&lt;/p&gt;</description></item><item><title>RoMo-HER&#26159;&#19968;&#20010;&#40065;&#26834;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20107;&#21518;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#21069;&#30651;&#37325;&#26032;&#26631;&#35760;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.16061</link><description>&lt;p&gt;
RoMo-HER: &#40065;&#26834;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20107;&#21518;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RoMo-HER: Robust Model-based Hindsight Experience Replay. (arXiv:2306.16061v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16061
&lt;/p&gt;
&lt;p&gt;
RoMo-HER&#26159;&#19968;&#20010;&#40065;&#26834;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20107;&#21518;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#21069;&#30651;&#37325;&#26032;&#26631;&#35760;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#31232;&#30095;&#22870;&#21169;&#26159;&#23548;&#33268;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#20302;&#30340;&#22240;&#32032;&#20043;&#19968;&#12290;&#22522;&#20110;&#20107;&#21518;&#32463;&#39564;&#22238;&#25918;&#65288;HER&#65289;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#26631;&#35760;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#33719;&#21462;&#34394;&#25311;&#36712;&#36857;&#26469;&#37325;&#26032;&#26631;&#35760;&#30446;&#26631;&#65292;&#22312;&#20934;&#30830;&#21487;&#24314;&#27169;&#30340;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#26159;&#26080;&#25928;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;RoMo-HER&#30340;&#40065;&#26834;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#25552;&#39640;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#12290;RoMo-HER&#22522;&#20110;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#19968;&#31181;&#31216;&#20026;&#21069;&#30651;&#37325;&#26032;&#26631;&#35760;&#65288;FR&#65289;&#30340;&#26032;&#22411;&#30446;&#26631;&#37325;&#26032;&#26631;&#35760;&#25216;&#26415;&#26500;&#24314;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#29305;&#23450;&#31574;&#30053;&#36873;&#25321;&#39044;&#27979;&#36215;&#22987;&#29366;&#24577;&#65292;&#39044;&#27979;&#36215;&#22987;&#29366;&#24577;&#30340;&#26410;&#26469;&#36712;&#36857;&#65292;&#28982;&#21518;&#20351;&#29992;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#26368;&#26032;&#30340;&#20449;&#24687;&#37325;&#26032;&#26631;&#35760;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse rewards are one of the factors leading to low sample efficiency in multi-goal reinforcement learning (RL). Based on Hindsight Experience Replay (HER), model-based relabeling methods have been proposed to relabel goals using virtual trajectories obtained by interacting with the trained model, which can effectively enhance the sample efficiency in accurately modelable sparse-reward environments. However, they are ineffective in robot manipulation environment. In our paper, we design a robust framework called Robust Model-based Hindsight Experience Replay (RoMo-HER) which can effectively utilize the dynamical model in robot manipulation environments to enhance the sample efficiency. RoMo-HER is built upon a dynamics model and a novel goal relabeling technique called Foresight relabeling (FR), which selects the prediction starting state with a specific strategy, predicts the future trajectory of the starting state, and then relabels the goal using the dynamics model and the latest p
&lt;/p&gt;</description></item><item><title>DUET&#26159;&#19968;&#31181;2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36755;&#20837;&#21464;&#25442;&#20449;&#24687;&#30340;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16058</link><description>&lt;p&gt;
DUET: 2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DUET: 2D Structured and Approximately Equivariant Representations. (arXiv:2306.16058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16058
&lt;/p&gt;
&lt;p&gt;
DUET&#26159;&#19968;&#31181;2D&#32467;&#26500;&#21270;&#19988;&#36817;&#20284;&#31561;&#21464;&#34920;&#31034;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#36755;&#20837;&#21464;&#25442;&#20449;&#24687;&#30340;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;(MSSL)&#22522;&#20110;&#23398;&#20064;&#30456;&#23545;&#20110;&#19968;&#32452;&#36755;&#20837;&#21464;&#25442;&#30340;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#21464;&#24615;&#20174;&#34920;&#31034;&#20013;&#37096;&#20998;&#25110;&#23436;&#20840;&#31227;&#38500;&#19982;&#21464;&#25442;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23545;&#38656;&#35201;&#36825;&#20123;&#20449;&#24687;&#30340;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#36896;&#25104;&#25439;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;2D&#32467;&#26500;&#21270;&#21644;&#31561;&#21464;&#34920;&#31034;&#65292;&#31216;&#20026;DUET&#65292;&#23427;&#20204;&#26159;&#20197;&#30697;&#38453;&#32467;&#26500;&#32452;&#32455;&#30340;2D&#34920;&#31034;&#65292;&#24182;&#19988;&#23545;&#20316;&#29992;&#20110;&#36755;&#20837;&#25968;&#25454;&#30340;&#21464;&#25442;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;DUET&#34920;&#31034;&#20445;&#30041;&#26377;&#20851;&#36755;&#20837;&#21464;&#25442;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#20041;&#34920;&#36798;&#33021;&#21147;&#12290;&#19982;SimCLR&#65288;Chen&#31561;&#65292;2020&#65289;&#65288;&#26080;&#32467;&#26500;&#21644;&#19981;&#21464;&#24615;&#65289;&#21644;ESSL&#65288;Dangovski&#31561;&#65292;2022&#65289;&#65288;&#26080;&#32467;&#26500;&#21644;&#31561;&#21464;&#24615;&#65289;&#30456;&#27604;&#65292;DUET&#34920;&#31034;&#30340;&#32467;&#26500;&#21270;&#21644;&#31561;&#21464;&#24615;&#20351;&#24471;&#29983;&#25104;&#20855;&#26377;&#26356;&#20302;&#30340;&#37325;&#24314;&#35823;&#24046;&#30340;&#21487;&#25511;&#24615;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;SimCLR&#25110;ESSL&#21017;&#26080;&#27861;&#23454;&#29616;&#21487;&#25511;&#24615;&#12290;DUET&#36824;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiview Self-Supervised Learning (MSSL) is based on learning invariances with respect to a set of input transformations. However, invariance partially or totally removes transformation-related information from the representations, which might harm performance for specific downstream tasks that require such information. We propose 2D strUctured and EquivarianT representations (coined DUET), which are 2d representations organized in a matrix structure, and equivariant with respect to transformations acting on the input data. DUET representations maintain information about an input transformation, while remaining semantically expressive. Compared to SimCLR (Chen et al., 2020) (unstructured and invariant) and ESSL (Dangovski et al., 2022) (unstructured and equivariant), the structured and equivariant nature of DUET representations enables controlled generation with lower reconstruction error, while controllability is not possible with SimCLR or ESSL. DUET also achieves higher accuracy fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;VLMs&#22312;&#35782;&#21035;&#32454;&#31890;&#24230;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#25351;&#20986;&#20102;VLMs&#20013;&#30456;&#20284;&#24230;&#20998;&#25968;&#19981;&#33021;&#20005;&#26684;&#21453;&#26144;&#27491;&#30830;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.16048</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65306;&#31890;&#24230;&#21644;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Challenges of Zero-Shot Recognition with Vision-Language Models: Granularity and Correctness. (arXiv:2306.16048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;VLMs&#22312;&#35782;&#21035;&#32454;&#31890;&#24230;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#25351;&#20986;&#20102;VLMs&#20013;&#30456;&#20284;&#24230;&#20998;&#25968;&#19981;&#33021;&#20005;&#26684;&#21453;&#26144;&#27491;&#30830;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24212;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#26816;&#26597;&#20102;VLMs&#22312;&#19981;&#21516;&#31890;&#24230;&#27010;&#24565;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#27491;&#35780;&#20272;&#20004;&#31181;&#23454;&#39564;&#35774;&#32622;&#19979;&#24615;&#33021;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;VLMs&#22312;&#35782;&#21035;&#32454;&#31890;&#24230;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;VLMs&#20135;&#29983;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#24182;&#19981;&#33021;&#20005;&#26684;&#21453;&#26144;&#25991;&#26412;&#36755;&#20837;&#22312;&#35270;&#35273;&#36755;&#20837;&#19979;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21327;&#35758;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#20998;&#25968;&#21487;&#33021;&#20250;&#20559;&#21521;&#26356;&#20855;&#20449;&#24687;&#30340;&#25551;&#36848;&#65292;&#24182;&#19988;&#30001;&#20110;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#24615;&#36136;&#65292;&#23545;&#20110;VLMs&#26469;&#35828;&#35782;&#21035;&#30456;&#20284;&#20294;&#38169;&#35823;&#30340;&#25551;&#36848;&#20043;&#38388;&#30340;&#27491;&#30830;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#20351;&#29992;VLMs&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the challenges of applying vision-language models (VLMs) to zero-shot visual recognition tasks in an open-world setting, with a focus on contrastive vision-language models such as CLIP. We first examine the performance of VLMs on concepts of different granularity levels. We propose a way to fairly evaluate the performance discrepancy under two experimental setups and find that VLMs are better at recognizing fine-grained concepts. Furthermore, we find that the similarity scores from VLMs do not strictly reflect the correctness of the textual inputs given visual input. We propose an evaluation protocol to test our hypothesis that the scores can be biased towards more informative descriptions, and the nature of the similarity score between embedding makes it challenging for VLMs to recognize the correctness between similar but wrong descriptions. Our study highlights the challenges of using VLMs in open-world settings and suggests directions for future research to 
&lt;/p&gt;</description></item><item><title>OpenNDD&#26159;&#19968;&#20010;&#29992;&#20110;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#25239;&#24490;&#29615;&#28857;&#24320;&#25918;&#24615;&#35782;&#21035;&#25216;&#26415;&#65292;&#33021;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#31867;&#21035;&#24182;&#35782;&#21035;&#26410;&#36935;&#21040;&#30340;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2306.16045</link><description>&lt;p&gt;
OpenNDD:&#29992;&#20110;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
OpenNDD: Open Set Recognition for Neurodevelopmental Disorders Detection. (arXiv:2306.16045v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16045
&lt;/p&gt;
&lt;p&gt;
OpenNDD&#26159;&#19968;&#20010;&#29992;&#20110;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#25239;&#24490;&#29615;&#28857;&#24320;&#25918;&#24615;&#35782;&#21035;&#25216;&#26415;&#65292;&#33021;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#31867;&#21035;&#24182;&#35782;&#21035;&#26410;&#36935;&#21040;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;(NDDs)&#26159;&#19968;&#32452;&#39640;&#24739;&#30149;&#29575;&#30340;&#38556;&#30861;&#65292;&#34920;&#29616;&#20986;&#20020;&#24202;&#34892;&#20026;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#24471;&#31934;&#30830;&#35782;&#21035;&#19981;&#21516;&#30340;NDDs&#65288;&#22914;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#21644;&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861;&#65288;ADHD&#65289;&#65289;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;NDDs&#35786;&#26029;&#24182;&#27809;&#26377;&#21487;&#38752;&#30340;&#29983;&#29702;&#26631;&#24535;&#29289;&#65292;&#32780;&#20165;&#20381;&#36182;&#20110;&#24515;&#29702;&#35780;&#20272;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26234;&#33021;&#36741;&#21161;&#35786;&#26029;&#26469;&#38450;&#27490;&#35823;&#35786;&#21644;&#28431;&#35786;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#19982;&#38543;&#21518;&#30340;&#30456;&#24212;&#27835;&#30103;&#23494;&#20999;&#30456;&#20851;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;NDDs&#31579;&#26597;&#21644;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;&#26694;&#26550;&#65292;&#36825;&#26159;&#22312;&#35813;&#39046;&#22495;&#20013;&#39318;&#27425;&#24212;&#29992;&#24320;&#25918;&#24615;&#35782;&#21035;&#12290;&#23427;&#32467;&#21512;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#25239;&#24490;&#29615;&#28857;&#24320;&#25918;&#24615;&#35782;&#21035;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#31867;&#21035;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#36807;&#21435;&#26410;&#36935;&#21040;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neurodevelopmental disorders (NDDs) are a highly prevalent group of disorders and represent strong clinical behavioral similarities, and that make it very challenging for accurate identification of different NDDs such as autism spectrum disorder (ASD) and attention-deficit hyperactivity disorder (ADHD). Moreover, there is no reliable physiological markers for NDDs diagnosis and it solely relies on psychological evaluation criteria. However, it is crucial to prevent misdiagnosis and underdiagnosis by intelligent assisted diagnosis, which is closely related to the follow-up corresponding treatment. In order to relieve these issues, we propose a novel open set recognition framework for NDDs screening and detection, which is the first application of open set recognition in this field. It combines auto encoder and adversarial reciprocal points open set recognition to accurately identify known classes as well as recognize classes never encountered. And considering the strong similarities bet
&lt;/p&gt;</description></item><item><title>Stone Needle&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#27169;&#22411;&#26694;&#26550;&#65292;&#19987;&#38376;&#20026;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#32780;&#35774;&#35745;&#12290;&#23427;&#38598;&#25104;&#20102;&#21508;&#31181;&#27169;&#24577;&#65292;&#21487;&#20197;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#24182;&#23454;&#29616;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2306.16034</link><description>&lt;p&gt;
Stone Needle: &#19968;&#20010;&#36890;&#29992;&#30340;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#30340;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Stone Needle: A General Multimodal Large-scale Model Framework towards Healthcare. (arXiv:2306.16034v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16034
&lt;/p&gt;
&lt;p&gt;
Stone Needle&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#27169;&#22411;&#26694;&#26550;&#65292;&#19987;&#38376;&#20026;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#32780;&#35774;&#35745;&#12290;&#23427;&#38598;&#25104;&#20102;&#21508;&#31181;&#27169;&#24577;&#65292;&#21487;&#20197;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#24182;&#23454;&#29616;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#22810;&#27169;&#24577;&#25968;&#25454;&#24191;&#27867;&#23384;&#22312;&#65292;&#24182;&#19988;&#38656;&#35201;&#22312;&#35786;&#26029;&#20915;&#31574;&#20043;&#21069;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#12289;&#20020;&#24202;&#25253;&#21578;&#31561;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#21333;&#27169;&#24577;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#24573;&#35270;&#20102;&#22810;&#27169;&#24577;&#30340;&#25972;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Stone Needle&#65292;&#19968;&#20010;&#19987;&#38376;&#20026;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#32780;&#35774;&#35745;&#30340;&#36890;&#29992;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#27169;&#22411;&#26694;&#26550;&#12290;Stone Needle&#20316;&#20026;&#19968;&#20010;&#20840;&#38754;&#30340;&#21307;&#30103;&#22810;&#27169;&#24577;&#27169;&#22411;&#22522;&#30784;&#65292;&#38598;&#25104;&#20102;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#31561;&#21508;&#31181;&#27169;&#24577;&#65292;&#20811;&#26381;&#20102;&#21333;&#27169;&#24577;&#31995;&#32479;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#24847;&#22270;&#20998;&#26512;&#12289;&#21307;&#30103;&#22522;&#30784;&#27169;&#22411;&#12289;&#25552;&#31034;&#31649;&#29702;&#22120;&#21644;&#21307;&#23398;&#35821;&#35328;&#27169;&#22359;&#31561;&#26694;&#26550;&#32452;&#20214;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#33021;&#22815;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#36827;&#34892;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#27169;&#22411;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#22810;&#26679;&#21270;&#30340;&#27169;&#24577;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#36827;&#34892;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In healthcare, multimodal data is prevalent and requires to be comprehensively analyzed before diagnostic decisions, including medical images, clinical reports, etc. However, current large-scale artificial intelligence models predominantly focus on single-modal cognitive abilities and neglect the integration of multiple modalities. Therefore, we propose Stone Needle, a general multimodal large-scale model framework tailored explicitly for healthcare applications. Stone Needle serves as a comprehensive medical multimodal model foundation, integrating various modalities such as text, images, videos, and audio to surpass the limitations of single-modal systems. Through the framework components of intent analysis, medical foundation models, prompt manager, and medical language module, our architecture can perform multi-modal interaction in multiple rounds of dialogue. Our method is a general multimodal large-scale model framework, integrating diverse modalities and allowing us to tailor fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#32852;&#30431;&#21306;&#22359;&#38142;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#21464;&#38382;&#39064;&#20013;&#24322;&#26500;&#27169;&#22411;&#21644;&#27169;&#22411;&#38388;&#21327;&#20316;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16023</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#27169;&#22411;&#38598;&#25104;&#24322;&#26500;&#27169;&#22411;&#21644;&#32852;&#30431;&#21306;&#22359;&#38142;&#35299;&#20915;&#26102;&#21464;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Distributed Computation Model Based on Federated Learning Integrates Heterogeneous models and Consortium Blockchain for Solving Time-Varying Problems. (arXiv:2306.16023v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#32852;&#30431;&#21306;&#22359;&#38142;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#21464;&#38382;&#39064;&#20013;&#24322;&#26500;&#27169;&#22411;&#21644;&#27169;&#22411;&#38388;&#21327;&#20316;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#26102;&#21464;&#38382;&#39064;&#65292;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#21457;&#23637;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38598;&#20013;&#24335;&#22788;&#29702;&#30340;&#26041;&#24335;&#38480;&#21046;&#65292;&#27169;&#22411;&#24615;&#33021;&#21463;&#21040;&#29616;&#23454;&#20013;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#23396;&#31435;&#38382;&#39064;&#31561;&#22240;&#32032;&#30340;&#26497;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#20998;&#24067;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;&#20363;&#22914;&#32852;&#37030;&#23398;&#20064;&#65289;&#30340;&#20986;&#29616;&#20351;&#24471;&#27169;&#22411;&#38388;&#30340;&#21160;&#24577;&#32858;&#21512;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#38598;&#25104;&#36807;&#31243;&#20173;&#28982;&#20381;&#36182;&#20110;&#26381;&#21153;&#22120;&#65292;&#21487;&#33021;&#32473;&#25972;&#20307;&#27169;&#22411;&#24102;&#26469;&#24456;&#22823;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#21482;&#20801;&#35768;&#21516;&#36136;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#24182;&#19988;&#27809;&#26377;&#24456;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#24322;&#26500;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#30431;&#21306;&#22359;&#38142;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#27169;&#22411;&#65288;DCM&#65289;&#65292;&#20197;&#25552;&#39640;&#25972;&#20307;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#24322;&#26500;&#27169;&#22411;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23618;&#32423;&#38598;&#25104;&#65288;DHI&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recurrent neural network has been greatly developed for effectively solving time-varying problems corresponding to complex environments. However, limited by the way of centralized processing, the model performance is greatly affected by factors like the silos problems of the models and data in reality. Therefore, the emergence of distributed artificial intelligence such as federated learning (FL) makes it possible for the dynamic aggregation among models. However, the integration process of FL is still server-dependent, which may cause a great risk to the overall model. Also, it only allows collaboration between homogeneous models, and does not have a good solution for the interaction between heterogeneous models. Therefore, we propose a Distributed Computation Model (DCM) based on the consortium blockchain network to improve the credibility of the overall model and effective coordination among heterogeneous models. In addition, a Distributed Hierarchical Integration (DHI) algorith
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#32467;&#26500;&#30340;&#35282;&#33394;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#21508;&#20010;&#23376;&#39046;&#22495;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#26041;&#38754;&#25152;&#20570;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.16021</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32467;&#26500;&#65306;&#35843;&#26597;&#19982;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Structure in Reinforcement Learning: A Survey and Open Problems. (arXiv:2306.16021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16021
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#32467;&#26500;&#30340;&#35282;&#33394;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#21508;&#20010;&#23376;&#39046;&#22495;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#26041;&#38754;&#25152;&#20570;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20511;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#20989;&#25968;&#36924;&#36817;&#26041;&#38754;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#24212;&#23545;&#22810;&#26679;&#19988;&#19981;&#21487;&#39044;&#27979;&#30340;&#21160;&#24577;&#12289;&#22024;&#26434;&#20449;&#21495;&#20197;&#21450;&#24222;&#22823;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#31561;&#21508;&#31181;&#30495;&#23454;&#22330;&#26223;&#26102;&#65292;&#20854;&#23454;&#29992;&#24615;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;&#35832;&#22914;&#25968;&#25454;&#25928;&#29575;&#20302;&#12289;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12289;&#32570;&#23569;&#23433;&#20840;&#20445;&#35777;&#21644;&#19981;&#21487;&#35299;&#37322;&#24615;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#24182;&#22312;&#36825;&#20123;&#20851;&#38190;&#25351;&#26631;&#19978;&#25552;&#39640;&#24615;&#33021;&#65292;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#26159;&#23558;&#38382;&#39064;&#30340;&#38468;&#21152;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#24378;&#21270;&#23398;&#20064;&#30340;&#21508;&#20010;&#23376;&#39046;&#22495;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#32435;&#20837;&#36825;&#26679;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#26041;&#27861;&#32479;&#19968;&#21040;&#19968;&#20010;&#26694;&#26550;&#19979;&#65292;&#25581;&#31034;&#32467;&#26500;&#22312;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep Neural Networks (DNNs) for function approximation, has demonstrated considerable success in numerous applications. However, its practicality in addressing a wide range of real-world scenarios, characterized by diverse and unpredictable dynamics, noisy signals, and large state and action spaces, remains limited. This limitation stems from issues such as poor data efficiency, limited generalization capabilities, a lack of safety guarantees, and the absence of interpretability, among other factors. To overcome these challenges and improve performance across these crucial metrics, one promising avenue is to incorporate additional structural information about the problem into the RL learning process. Various sub-fields of RL have proposed methods for incorporating such inductive biases. We amalgamate these diverse methodologies under a unified framework, shedding light on the role of structure in the learning prob
&lt;/p&gt;</description></item><item><title>BayesFlow&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21151;&#33021;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#36825;&#31181;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2306.16015</link><description>&lt;p&gt;
BayesFlow: &#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25674;&#36824;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
BayesFlow: Amortized Bayesian Workflows With Neural Networks. (arXiv:2306.16015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16015
&lt;/p&gt;
&lt;p&gt;
BayesFlow&#26159;&#19968;&#20010;Python&#24211;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#21151;&#33021;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#36825;&#31181;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36125;&#21494;&#26031;&#25512;&#26029;&#28041;&#21450;&#19968;&#31995;&#21015;&#35745;&#31639;&#25216;&#26415;&#65292;&#29992;&#20110;&#20272;&#35745;&#12289;&#39564;&#35777;&#21644;&#20174;&#27010;&#29575;&#27169;&#22411;&#20013;&#24471;&#20986;&#32467;&#35770;&#65292;&#20316;&#20026;&#25968;&#25454;&#20998;&#26512;&#20013;&#26377;&#21407;&#21017;&#30340;&#24037;&#20316;&#27969;&#30340;&#19968;&#37096;&#20998;&#12290;&#36125;&#21494;&#26031;&#24037;&#20316;&#27969;&#20013;&#30340;&#20856;&#22411;&#38382;&#39064;&#21253;&#25324;&#36817;&#20284;&#19981;&#21487;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#27169;&#22411;&#31867;&#22411;&#65292;&#20197;&#21450;&#36890;&#36807;&#22797;&#26434;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#27604;&#36739;&#21516;&#19968;&#36807;&#31243;&#30340;&#31454;&#20105;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Python&#24211;BayesFlow&#65292;&#29992;&#20110;&#22522;&#20110;&#20223;&#30495;&#35757;&#32451;&#24050;&#24314;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#25674;&#36824;&#25968;&#25454;&#21387;&#32553;&#21644;&#25512;&#26029;&#12290;&#22312;BayesFlow&#20013;&#23454;&#29616;&#30340;&#25674;&#36824;&#36125;&#21494;&#26031;&#25512;&#26029;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#27169;&#22411;&#20223;&#30495;&#19978;&#35757;&#32451;&#23450;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#36825;&#20123;&#32593;&#32476;&#37325;&#29992;&#20110;&#27169;&#22411;&#30340;&#20219;&#20309;&#21518;&#32493;&#24212;&#29992;&#12290;&#30001;&#20110;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#21487;&#20197;&#20960;&#20046;&#21363;&#26102;&#22320;&#25191;&#34892;&#25512;&#26029;&#65292;&#22240;&#27492;&#21069;&#26399;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#24456;&#24555;&#23601;&#33021;&#22815;&#25674;&#36824;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern Bayesian inference involves a mixture of computational techniques for estimating, validating, and drawing conclusions from probabilistic models as part of principled workflows for data analysis. Typical problems in Bayesian workflows are the approximation of intractable posterior distributions for diverse model types and the comparison of competing models of the same process in terms of their complexity and predictive performance. This manuscript introduces the Python library BayesFlow for simulation-based training of established neural network architectures for amortized data compression and inference. Amortized Bayesian inference, as implemented in BayesFlow, enables users to train custom neural networks on model simulations and re-use these networks for any subsequent application of the models. Since the trained networks can perform inference almost instantaneously, the upfront neural network training is quickly amortized.
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#37325;&#20889;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#23436;&#20840;&#25351;&#23450;&#26426;&#22120;&#24847;&#22270;&#30340;&#33258;&#28982;&#35821;&#35328;&#26469;&#25913;&#36827;&#24847;&#22270;&#29702;&#35299;&#21644;&#26500;&#24314;&#39640;&#24615;&#33021;&#26816;&#32034;&#31995;&#32479;&#12290;&#36825;&#31181;&#26694;&#26550;&#30340;&#33021;&#22815;&#20197;&#33258;&#28982;&#35821;&#35328;&#21576;&#29616;&#12289;&#20132;&#20114;&#21644;&#25512;&#29702;&#26426;&#22120;&#24847;&#22270;&#20855;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.16004</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#26597;&#35810;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Query Understanding in the Age of Large Language Models. (arXiv:2306.16004v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16004
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#37325;&#20889;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#23436;&#20840;&#25351;&#23450;&#26426;&#22120;&#24847;&#22270;&#30340;&#33258;&#28982;&#35821;&#35328;&#26469;&#25913;&#36827;&#24847;&#22270;&#29702;&#35299;&#21644;&#26500;&#24314;&#39640;&#24615;&#33021;&#26816;&#32034;&#31995;&#32479;&#12290;&#36825;&#31181;&#26694;&#26550;&#30340;&#33021;&#22815;&#20197;&#33258;&#28982;&#35821;&#35328;&#21576;&#29616;&#12289;&#20132;&#20114;&#21644;&#25512;&#29702;&#26426;&#22120;&#24847;&#22270;&#20855;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20852;&#36215;&#21644;&#24212;&#29992;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#26597;&#35810;&#12289;&#23545;&#35805;&#21644;&#25511;&#21046;&#25628;&#32034;&#21644;&#20449;&#24687;&#26816;&#32034;&#30028;&#38754;&#27491;&#22312;&#36805;&#36895;&#26222;&#21450;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;LLM&#36827;&#34892;&#20132;&#20114;&#24335;&#26597;&#35810;&#37325;&#20889;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26088;&#22312;&#20026;&#25913;&#36827;&#21644;&#36879;&#26126;&#21270;&#24847;&#22270;&#29702;&#35299;&#20197;&#21450;&#20351;&#29992;LLM&#26500;&#24314;&#39640;&#24615;&#33021;&#26816;&#32034;&#31995;&#32479;&#24320;&#36767;&#26032;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#37325;&#20889;&#22120;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23436;&#20840;&#25351;&#23450;&#26426;&#22120;&#24847;&#22270;&#65292;&#36825;&#20010;&#26426;&#22120;&#24847;&#22270;&#21487;&#20197;&#22312;&#26368;&#32456;&#26816;&#32034;&#38454;&#27573;&#20043;&#21069;&#36827;&#19968;&#27493;&#32454;&#21270;&#12289;&#25511;&#21046;&#21644;&#32534;&#36753;&#12290;&#20197;&#33258;&#28982;&#35821;&#35328;&#21576;&#29616;&#12289;&#20132;&#20114;&#21644;&#25512;&#29702;&#24213;&#23618;&#30340;&#26426;&#22120;&#24847;&#22270;&#23545;&#36879;&#26126;&#24230;&#12289;&#25490;&#21517;&#24615;&#33021;&#20197;&#21450;&#31163;&#24320;&#20256;&#32479;&#24847;&#22270;&#29702;&#35299;&#20013;&#25910;&#38598;&#30417;&#30563;&#20449;&#21495;&#30340;&#26041;&#24335;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#36825;&#19968;&#27010;&#24565;&#65292;&#24182;&#25903;&#25345;&#21021;&#27493;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Querying, conversing, and controlling search and information-seeking interfaces using natural language are fast becoming ubiquitous with the rise and adoption of large-language models (LLM). In this position paper, we describe a generic framework for interactive query-rewriting using LLMs. Our proposal aims to unfold new opportunities for improved and transparent intent understanding while building high-performance retrieval systems using LLMs. A key aspect of our framework is the ability of the rewriter to fully specify the machine intent by the search engine in natural language that can be further refined, controlled, and edited before the final retrieval phase. The ability to present, interact, and reason over the underlying machine intent in natural language has profound implications on transparency, ranking performance, and a departure from the traditional way in which supervised signals were collected for understanding intents. We detail the concept, backed by initial experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21307;&#23398;&#23454;&#20307;&#12289;&#26631;&#20934;&#21270;&#23454;&#20307;&#21644;&#20998;&#37197;UMLS&#27010;&#24565;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;COVID-19&#30456;&#20851;&#25512;&#25991;&#30340;&#30151;&#29366;&#35789;&#20856;&#12290;</title><link>http://arxiv.org/abs/2306.16001</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#20197;&#25903;&#25345;&#20844;&#20849;&#21355;&#29983;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Streamlining Social Media Information Retrieval for Public Health Research with Deep Learning. (arXiv:2306.16001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31616;&#21270;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#21307;&#23398;&#23454;&#20307;&#12289;&#26631;&#20934;&#21270;&#23454;&#20307;&#21644;&#20998;&#37197;UMLS&#27010;&#24565;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;COVID-19&#30456;&#20851;&#25512;&#25991;&#30340;&#30151;&#29366;&#35789;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#22312;&#27969;&#34892;&#30149;&#30417;&#27979;&#20013;&#30340;&#21033;&#29992;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#35777;&#23454;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#35789;&#27719;&#34920;&#26469;&#26816;&#32034;&#30456;&#20851;&#35821;&#26009;&#24211;&#26102;&#65292;&#24120;&#24120;&#20250;&#24341;&#20837;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#26500;&#24314;&#21307;&#23398;&#20439;&#35821;&#21644;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#27010;&#24565;&#30340;&#24191;&#27867;&#23383;&#20856;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#22522;&#20110;BERT&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#20013;&#35782;&#21035;&#20986;&#21307;&#23398;&#23454;&#20307;&#65307;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#26631;&#20934;&#21270;&#27169;&#22359;&#65292;&#29992;&#20110;&#23545;&#25552;&#21462;&#20986;&#30340;&#23454;&#20307;&#36827;&#34892;&#35268;&#33539;&#21270;&#22788;&#29702;&#65307;&#21322;&#30417;&#30563;&#32858;&#31867;&#27169;&#22359;&#65292;&#23558;&#26368;&#21487;&#33021;&#30340;UMLS&#27010;&#24565;&#20998;&#37197;&#32473;&#27599;&#20010;&#35268;&#33539;&#21270;&#23454;&#20307;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#20174;2020&#24180;2&#26376;1&#26085;&#21040;2022&#24180;4&#26376;30&#26085;&#26399;&#38388;&#19982;COVID-19&#30456;&#20851;&#30340;&#25512;&#25991;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#30151;&#29366;&#35789;&#20856;&#65288;&#21487;&#22312;https://github.com/ningkko/UMLS_colloquialism/&#19978;&#33719;&#21462;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;9,249&#20010;&#26631;&#20934;&#21270;&#23454;&#20307;&#65292;&#26144;&#23556;&#21040;876&#20010;UMLS&#27010;&#24565;&#21644;38,175&#20010;&#20442;&#35821;&#34920;&#36798;&#12290;&#35813;&#26694;&#26550;&#30340;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
The utilization of social media in epidemic surveillance has been well established. Nonetheless, bias is often introduced when pre-defined lexicons are used to retrieve relevant corpus. This study introduces a framework aimed at curating extensive dictionaries of medical colloquialisms and Unified Medical Language System (UMLS) concepts. The framework comprises three modules: a BERT-based Named Entity Recognition (NER) model that identifies medical entities from social media content, a deep-learning powered normalization module that standardizes the extracted entities, and a semi-supervised clustering module that assigns the most probable UMLS concept to each standardized entity. We applied this framework to COVID-19-related tweets from February 1, 2020, to April 30, 2022, generating a symptom dictionary (available at https://github.com/ningkko/UMLS_colloquialism/) composed of 9,249 standardized entities mapped to 876 UMLS concepts and 38,175 colloquial expressions. This framework demo
&lt;/p&gt;</description></item><item><title>Tensorformer&#26159;&#19968;&#31181;&#24402;&#19968;&#21270;&#30697;&#38453;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#30340;&#28857;&#20113;&#37325;&#24314;&#12290;&#23427;&#36890;&#36807;&#30697;&#38453;&#27880;&#24847;&#21147;&#23454;&#29616;&#20102;&#36880;&#28857;&#21644;&#36880;&#36890;&#36947;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#23616;&#37096;&#20960;&#20309;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15989</link><description>&lt;p&gt;
Tensorformer: &#39640;&#36136;&#37327;&#28857;&#20113;&#37325;&#24314;&#30340;&#24402;&#19968;&#21270;&#30697;&#38453;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Tensorformer: Normalized Matrix Attention Transformer for High-quality Point Cloud Reconstruction. (arXiv:2306.15989v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15989
&lt;/p&gt;
&lt;p&gt;
Tensorformer&#26159;&#19968;&#31181;&#24402;&#19968;&#21270;&#30697;&#38453;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#30340;&#28857;&#20113;&#37325;&#24314;&#12290;&#23427;&#36890;&#36807;&#30697;&#38453;&#27880;&#24847;&#21147;&#23454;&#29616;&#20102;&#36880;&#28857;&#21644;&#36880;&#36890;&#36947;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#23616;&#37096;&#20960;&#20309;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#30028;&#65292;&#20174;&#21407;&#22987;&#28857;&#20113;&#36827;&#34892;&#34920;&#38754;&#37325;&#24314;&#30340;&#30740;&#31350;&#24050;&#32463;&#36827;&#34892;&#20102;&#20960;&#21313;&#24180;&#65292;&#36825;&#22312;&#29616;&#20170;&#30340;&#24314;&#27169;&#21644;&#28210;&#26579;&#24212;&#29992;&#20013;&#38656;&#27714;&#38750;&#24120;&#39640;&#12290;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;Poisson&#34920;&#38754;&#37325;&#24314;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#28857;&#27861;&#32447;&#36755;&#20837;&#20197;&#20135;&#29983;&#21512;&#29702;&#30340;&#32467;&#26524;&#12290;&#29616;&#20195;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#27861;&#32447;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#20294;&#30001;&#20110;&#31163;&#25955;&#28857;&#30340;&#23616;&#37096;&#34701;&#21512;&#32534;&#30721;&#24615;&#33021;&#26377;&#38480;&#65292;&#32467;&#26524;&#36739;&#20026;&#31895;&#31961;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24402;&#19968;&#21270;&#30697;&#38453;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65288;Tensorformer&#65289;&#26469;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#12290;&#25152;&#25552;&#20986;&#30340;&#30697;&#38453;&#27880;&#24847;&#21147;&#20801;&#35768;&#21516;&#26102;&#36827;&#34892;&#36880;&#28857;&#21644;&#36880;&#36890;&#36947;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#32780;&#20043;&#21069;&#30340;&#21521;&#37327;&#27880;&#24847;&#21147;&#22312;&#19981;&#21516;&#36890;&#36947;&#20043;&#38388;&#20002;&#22833;&#20102;&#30456;&#37051;&#28857;&#30340;&#20449;&#24687;&#12290;&#23427;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#24102;&#26469;&#26356;&#22810;&#33258;&#30001;&#24230;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;ShapeNetCore&#21644;ABC&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#65292;&#24182;&#19988;
&lt;/p&gt;
&lt;p&gt;
Surface reconstruction from raw point clouds has been studied for decades in the computer graphics community, which is highly demanded by modeling and rendering applications nowadays. Classic solutions, such as Poisson surface reconstruction, require point normals as extra input to perform reasonable results. Modern transformer-based methods can work without normals, while the results are less fine-grained due to limited encoding performance in local fusion from discrete points. We introduce a novel normalized matrix attention transformer (Tensorformer) to perform high-quality reconstruction. The proposed matrix attention allows for simultaneous point-wise and channel-wise message passing, while the previous vector attention loses neighbor point information across different channels. It brings more degree of freedom in feature learning and thus facilitates better modeling of local geometries. Our method achieves state-of-the-art on two commonly used datasets, ShapeNetCore and ABC, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#21644;&#35266;&#23519;&#20174;&#31616;&#21333;&#21644;&#22256;&#38590;&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#21457;&#29616;&#20102;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#21644;&#32500;&#24230;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32500;&#24230;&#32467;&#26500;&#30340;&#36328;&#27169;&#24577;&#23398;&#20064;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#36328;&#27169;&#24577;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15977</link><description>&lt;p&gt;
&#22522;&#20110;&#32500;&#24230;&#32467;&#26500;&#30340;&#36328;&#27169;&#24577;&#23398;&#20064;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Dimensional Structure based Knowledge Distillation Method for Cross-Modal Learning. (arXiv:2306.15977v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#21644;&#35266;&#23519;&#20174;&#31616;&#21333;&#21644;&#22256;&#38590;&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#21457;&#29616;&#20102;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#21644;&#32500;&#24230;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32500;&#24230;&#32467;&#26500;&#30340;&#36328;&#27169;&#24577;&#23398;&#20064;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#36328;&#27169;&#24577;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#38480;&#21046;&#65292;&#19968;&#20123;&#37325;&#35201;&#30340;&#35270;&#35273;&#20219;&#21153;&#24456;&#38590;&#29420;&#31435;&#23436;&#25104;&#12290;&#24341;&#20837;&#20808;&#21069;&#19981;&#21487;&#29992;&#30340;&#20449;&#24687;&#20197;&#36716;&#31227;&#26377;&#20449;&#24687;&#30340;&#40657;&#26263;&#30693;&#35782;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#20219;&#21153;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20026;&#20160;&#20040;&#36716;&#31227;&#30693;&#35782;&#26377;&#25928;&#30340;&#30740;&#31350;&#36824;&#27809;&#26377;&#24191;&#27867;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#35266;&#23519;&#20174;&#31616;&#21333;&#20219;&#21153;&#21644;&#22256;&#38590;&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#21457;&#29616;&#20102;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#21644;&#32500;&#24230;&#32467;&#26500;&#65288;DS&#65289;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#20013;&#38388;&#31354;&#38388;&#20998;&#24067;&#26469;&#34920;&#31034;DS&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#65288;CMKD&#65289;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#30417;&#30563;&#24335;&#36328;&#27169;&#24577;&#23398;&#20064;&#65288;CML&#65289;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24378;&#21046;&#36755;&#20986;&#29305;&#24449;&#26159;&#36890;&#36947;&#29420;&#31435;&#30340;&#65292;&#19988;&#20013;&#38388;&#29305;&#24449;&#26159;&#22343;&#21248;&#20998;&#24067;&#30340;&#65292;&#20174;&#32780;&#20174;&#22256;&#38590;&#20219;&#21153;&#20013;&#23398;&#20064;&#35821;&#20041;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to limitations in data quality, some essential visual tasks are difficult to perform independently. Introducing previously unavailable information to transfer informative dark knowledge has been a common way to solve such hard tasks. However, research on why transferred knowledge works has not been extensively explored. To address this issue, in this paper, we discover the correlation between feature discriminability and dimensional structure (DS) by analyzing and observing features extracted from simple and hard tasks. On this basis, we express DS using deep channel-wise correlation and intermediate spatial distribution, and propose a novel cross-modal knowledge distillation (CMKD) method for better supervised cross-modal learning (CML) performance. The proposed method enforces output features to be channel-wise independent and intermediate ones to be uniformly distributed, thereby learning semantically irrelevant features from the hard task to boost its accuracy. This is especial
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#31181;&#21452;&#27169;&#24577;&#21464;&#25442;&#22120;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#21382;&#21490;&#34880;&#27969;&#21644;&#31070;&#32463;&#27963;&#21160;&#26469;&#25512;&#26029;&#24403;&#21069;&#34880;&#27969;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#34880;&#28082;&#21160;&#21147;&#23398;&#21709;&#24212;&#31070;&#32463;&#27963;&#21160;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2306.15971</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#27169;&#24577;&#21464;&#25442;&#22120;&#37325;&#24314;&#34880;&#28082;&#21160;&#21147;&#23398;&#21709;&#24212;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer. (arXiv:2306.15971v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#31181;&#21452;&#27169;&#24577;&#21464;&#25442;&#22120;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#21382;&#21490;&#34880;&#27969;&#21644;&#31070;&#32463;&#27963;&#21160;&#26469;&#25512;&#26029;&#24403;&#21069;&#34880;&#27969;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#34880;&#28082;&#21160;&#21147;&#23398;&#21709;&#24212;&#31070;&#32463;&#27963;&#21160;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34880;&#27969;&#19982;&#31070;&#32463;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#34987;&#24191;&#27867;&#35748;&#21487;&#65292;&#22312;fMRI&#30740;&#31350;&#20013;&#65292;&#34880;&#27969;&#32463;&#24120;&#34987;&#29992;&#20316;&#31070;&#32463;&#27963;&#21160;&#30340;&#26367;&#20195;&#25351;&#26631;&#12290;&#22312;&#24494;&#35266;&#27700;&#24179;&#19978;&#65292;&#24050;&#32463;&#26174;&#31034;&#31070;&#32463;&#27963;&#21160;&#20250;&#24433;&#21709;&#38468;&#36817;&#34880;&#31649;&#30340;&#34880;&#27969;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#30452;&#25509;&#22312;&#26126;&#30830;&#30340;&#31070;&#32463;&#20803;&#32676;&#20307;&#27700;&#24179;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20351;&#29992;&#28165;&#37266;&#23567;&#40736;&#30340;&#20307;&#20869;&#35760;&#24405;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#21452;&#27169;&#24577;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#26681;&#25454;&#21382;&#21490;&#34880;&#27969;&#21644;&#25345;&#32493;&#33258;&#21457;&#31070;&#32463;&#27963;&#21160;&#26469;&#25512;&#26029;&#24403;&#21069;&#30340;&#34880;&#27969;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#32467;&#21512;&#31070;&#32463;&#27963;&#21160;&#26126;&#26174;&#25913;&#21892;&#20102;&#27169;&#22411;&#39044;&#27979;&#34880;&#27969;&#20540;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#34880;&#28082;&#21160;&#21147;&#23398;&#21709;&#24212;&#31070;&#32463;&#27963;&#21160;&#30340;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#24615;&#36136;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relationship between blood flow and neuronal activity is widely recognized, with blood flow frequently serving as a surrogate for neuronal activity in fMRI studies. At the microscopic level, neuronal activity has been shown to influence blood flow in nearby blood vessels. This study introduces the first predictive model that addresses this issue directly at the explicit neuronal population level. Using in vivo recordings in awake mice, we employ a novel spatiotemporal bimodal transformer architecture to infer current blood flow based on both historical blood flow and ongoing spontaneous neuronal activity. Our findings indicate that incorporating neuronal activity significantly enhances the model's ability to predict blood flow values. Through analysis of the model's behavior, we propose hypotheses regarding the largely unexplored nature of the hemodynamic response to neuronal activity.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15969</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Separable Physics-Informed Neural Networks. (arXiv:2306.15969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15969
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20998;&#31163;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;SPINN&#65289;&#65292;&#36890;&#36807;&#36880;&#20010;&#22788;&#29702;&#36724;&#26469;&#26174;&#33879;&#20943;&#23569;&#20102;&#22810;&#32500; PDE &#20013;&#30340;&#32593;&#32476;&#20256;&#25773;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#26222;&#36890; GPU &#19978;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#26377;&#24076;&#26395;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;PDE&#27714;&#35299;&#22120;&#65292;&#22312;&#21508;&#31181;PDE&#19978;&#26174;&#31034;&#20986;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;PINNs&#26469;&#35299;&#20915;&#22810;&#32500;PDE&#21644;&#36924;&#36817;&#39640;&#24230;&#22797;&#26434;&#35299;&#20989;&#25968;&#23384;&#22312;&#26681;&#26412;&#38480;&#21046;&#12290;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;PDE&#19978;&#25152;&#38656;&#30340;&#35757;&#32451;&#28857;&#25968;&#37327;(&#37197;&#28857;)&#22823;&#22823;&#22686;&#21152;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#24222;&#22823;&#30340;&#20869;&#23384;&#24320;&#38144;&#65292;&#20854;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PINNs&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20998;&#31163;&#30340;PINN (SPINN)&#65292;&#22312;&#22810;&#32500;PDE&#20013;&#25353;&#36724;&#36880;&#20010;&#22788;&#29702;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#20256;&#25773;&#30340;&#25968;&#37327;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;PINNs&#20013;&#30340;&#36880;&#28857;&#22788;&#29702;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#26469;&#38477;&#20302;&#35745;&#31639;PDE&#27531;&#24046;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20174;&#32780;&#22312;&#21333;&#20010;&#26222;&#36890;GPU&#19978;&#21487;&#20197;&#20351;&#29992;&#22823;&#37327;&#30340;&#37197;&#28857;(&gt;10^7)&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (&gt;10^7) on a single commodity GPU. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.15951</link><description>&lt;p&gt;
&#36890;&#36807;&#36339;&#36807;&#38646;&#20803;&#32032;&#38477;&#20302;&#21367;&#31215;&#23618;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#24182;&#34892;&#22788;&#29702;&#22120;&#36827;&#34892;&#21152;&#36895;&#12290;&#20026;&#20102;&#20026;&#20854;&#35774;&#35745;&#36816;&#31639;&#31526;&#65292;&#38656;&#35201;&#19981;&#20165;&#26377;&#20248;&#21270;&#31639;&#27861;&#20197;&#38477;&#20302;&#22797;&#26434;&#24230;&#65292;&#36824;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#30828;&#20214;&#36164;&#28304;&#12290;&#21367;&#31215;&#23618;&#20027;&#35201;&#21253;&#21547;&#19977;&#31181;&#36816;&#31639;&#31526;&#65306;&#21069;&#21521;&#20256;&#25773;&#30340;&#21367;&#31215;&#65292;&#21453;&#21521;&#20256;&#25773;&#30340;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#12290;&#24403;&#25191;&#34892;&#36825;&#20123;&#36816;&#31639;&#26102;&#65292;&#22987;&#32456;&#20250;&#21521;&#24352;&#37327;&#20013;&#28155;&#21152;0&#20803;&#32032;&#65292;&#23548;&#33268;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65288;ConvV2, KS-deconv, Sk-dilated&#65289;&#65292;&#20197;&#20004;&#31181;&#26041;&#24335;&#36339;&#36807;&#36825;&#20123;0&#20803;&#32032;&#65306;&#20462;&#21098;&#28388;&#27874;&#22120;&#20197;&#25490;&#38500;&#22635;&#20805;&#30340;0&#20803;&#32032;&#65307;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#20026;&#31264;&#23494;&#24352;&#37327;&#65292;&#36991;&#20813;&#22312;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#20013;&#25554;&#20837;0&#20803;&#32032;&#12290;&#19982;&#26222;&#36890;&#21367;&#31215;&#30456;&#27604;&#65292;&#21453;&#21367;&#31215;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#32780;&#38590;&#20197;&#21152;&#36895;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;C-K-S&#30340;&#39640;&#24615;&#33021;GPU&#23454;&#29616;&#65292;&#24182;&#36890;&#36807;&#19982;PyTorch&#30340;&#27604;&#36739;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks rely on parallel processors for acceleration. To design operators for them, it requires not only good algorithm to reduce complexity, but also sufficient utilization of hardwares. Convolutional layers mainly contain 3 kinds of operators: convolution in forward propagation, deconvolution and dilated-convolution in backward propagation. When executing these operators, 0s are always added to tensors, causing redundant calculations. This paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these 0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors to dense tensors, to avoid inserted 0s in deconvolution and dilated-convolution. In contrast to regular convolution, deconvolution is hard to accelerate due to its complicacy. This paper provides high-performance GPU implementations of C-K-S, and verifies their effectiveness with comparison to PyTorch. According to the experiments, C-K-S has advantages over PyTorch in certain cas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Opti-Mile"&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26368;&#21518;&#19968;&#20844;&#37324;&#26381;&#21153;&#19982;&#20844;&#20849;&#20132;&#36890;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#25143;&#26080;&#38656;&#25442;&#20056;&#65292;&#35299;&#20915;&#20102;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#26377;&#38480;&#21487;&#36798;&#24615;&#21644;&#25442;&#20056;&#24341;&#36215;&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15943</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#25442;&#20056;&#65306;&#20351;&#29992;Opti-Mile&#23558;&#26368;&#21518;&#19968;&#20844;&#37324;&#19982;&#20844;&#20849;&#20132;&#36890;&#25972;&#21512;&#22312;&#19968;&#36215;
&lt;/p&gt;
&lt;p&gt;
No Transfers Required: Integrating Last Mile with Public Transit Using Opti-Mile. (arXiv:2306.15943v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15943
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Opti-Mile"&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26368;&#21518;&#19968;&#20844;&#37324;&#26381;&#21153;&#19982;&#20844;&#20849;&#20132;&#36890;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#25143;&#26080;&#38656;&#25442;&#20056;&#65292;&#35299;&#20915;&#20102;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#26377;&#38480;&#21487;&#36798;&#24615;&#21644;&#25442;&#20056;&#24341;&#36215;&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20844;&#20849;&#20132;&#36890;&#20855;&#26377;&#32463;&#27982;&#23454;&#24800;&#30340;&#20248;&#28857;&#65292;&#20294;&#30001;&#20110;&#22823;&#37096;&#20998;&#22320;&#21306;&#38656;&#35201;&#25442;&#20056;&#65292;&#23548;&#33268;&#19981;&#20415;&#21033;&#12290;&#20026;&#20102;&#35299;&#20915;&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#20013;&#26377;&#38480;&#30340;&#21487;&#36798;&#24615;&#21644;&#25442;&#20056;&#24341;&#36215;&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26053;&#34892;&#35745;&#21010;&#26041;&#27861;&#65292;&#21363;"Opti-Mile"&#65292;&#23427;&#23558;&#26368;&#21518;&#19968;&#20844;&#37324;&#26381;&#21153;&#19982;&#20844;&#20849;&#20132;&#36890;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#25143;&#26080;&#38656;&#25442;&#20056;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public transit is a popular mode of transit due to its affordability, despite the inconveniences due to the necessity of transfers required to reach most areas. For example, in the bus and metro network of New Delhi, only 30\% of stops can be directly accessed from any starting point, thus requiring transfers for most commutes. Additionally, last-mile services like rickshaws, tuk-tuks or shuttles are commonly used as feeders to the nearest public transit access points, which further adds to the complexity and inefficiency of a journey. Ultimately, users often face a tradeoff between coverage and transfers to reach their destination, regardless of the mode of transit or the use of last-mile services. To address the problem of limited accessibility and inefficiency due to transfers in public transit systems, we propose ``opti-mile," a novel trip planning approach that combines last-mile services with public transit such that no transfers are required. Opti-mile allows users to customise 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#22686;&#24378;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#22120;&#24615;&#33021;&#30340;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#37319;&#29992;&#20102;UNet-TCN&#32467;&#26500;&#24314;&#27169;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#22120;&#23545;&#31354;&#38388;&#20449;&#24687;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.15942</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#38388;&#20449;&#24687;&#30340;&#22686;&#24378;&#22411;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#22120;&#29992;&#20110;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Enhanced Neural Beamformer with Spatial Information for Target Speech Extraction. (arXiv:2306.15942v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#22686;&#24378;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#22120;&#24615;&#33021;&#30340;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#37319;&#29992;&#20102;UNet-TCN&#32467;&#26500;&#24314;&#27169;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#22120;&#23545;&#31354;&#38388;&#20449;&#24687;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27874;&#26463;&#24418;&#25104;&#31639;&#27861;&#22312;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#31995;&#32479;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#26469;&#22686;&#24378;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#22120;&#24615;&#33021;&#30340;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;UNet-TCN&#32467;&#26500;&#26469;&#24314;&#27169;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#36991;&#20813;&#20854;&#20182;&#27169;&#22411;&#20013;&#30340;&#30452;&#25509;&#38477;&#32500;&#36896;&#25104;&#30340;&#20449;&#24687;&#25439;&#22833;&#26469;&#25552;&#39640;&#35821;&#38899;&#39044;&#20998;&#31163;&#27169;&#22359;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#38453;&#21015;&#25509;&#25910;&#21040;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#22120;&#23545;&#31354;&#38388;&#20449;&#24687;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#26356;&#21512;&#29702;&#30340;&#30446;&#26631;&#25513;&#30721;&#20272;&#35745;&#32593;&#32476;&#21644;&#22522;&#20110;&#31354;&#38388;&#20449;&#24687;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#24341;&#20837;&#31070;&#32463;&#27874;&#26463;&#24418;&#25104;&#22120;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning-based beamforming algorithms have shown promising performance in target speech extraction tasks. However, most systems do not fully utilize spatial information. In this paper, we propose a target speech extraction network that utilizes spatial information to enhance the performance of neural beamformer. To achieve this, we first use the UNet-TCN structure to model input features and improve the estimation accuracy of the speech pre-separation module by avoiding information loss caused by direct dimensionality reduction in other models. Furthermore, we introduce a multi-head cross-attention mechanism that enhances the neural beamformer's perception of spatial information by making full use of the spatial information received by the array. Experimental results demonstrate that our approach, which incorporates a more reasonable target mask estimation network and a spatial information-based cross-attention mechanism into the neural beamformer, effectively improves s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#27010;&#24565;&#22312;&#25163;&#26426;&#32593;&#32476;&#20013;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26500;&#25439;&#22833;&#21644;Z&#20998;&#25968;&#26469;&#26816;&#27979;&#24322;&#24120;&#65292;&#24182;&#36890;&#36807;K-means&#31639;&#27861;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#24322;&#24120;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#20026;&#25163;&#26426;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#25552;&#20379;&#20102;&#26356;&#24555;&#19988;&#33258;&#20027;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#22823;&#25968;&#25454;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.15938</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#27010;&#24565;&#22312;&#25163;&#26426;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpretable Anomaly Detection in Cellular Networks by Learning Concepts in Variational Autoencoders. (arXiv:2306.15938v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#27010;&#24565;&#22312;&#25163;&#26426;&#32593;&#32476;&#20013;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26500;&#25439;&#22833;&#21644;Z&#20998;&#25968;&#26469;&#26816;&#27979;&#24322;&#24120;&#65292;&#24182;&#36890;&#36807;K-means&#31639;&#27861;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#24322;&#24120;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#20026;&#25163;&#26426;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#25552;&#20379;&#20102;&#26356;&#24555;&#19988;&#33258;&#20027;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#22823;&#25968;&#25454;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#25163;&#26426;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAEs)&#23398;&#20064;&#27599;&#20010;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;(KPI)&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#22522;&#20110;&#37325;&#26500;&#25439;&#22833;&#21644;Z&#20998;&#25968;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;K-means&#31639;&#27861;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#38468;&#21152;&#20449;&#24687;&#20013;&#24515;&#28857;(c)&#30830;&#20445;&#24322;&#24120;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#29305;&#23450;KPI&#30340;&#28508;&#22312;&#32500;&#24230;&#20013;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#21644;&#24322;&#24120;&#12290;&#35813;&#25552;&#35758;&#30340;&#26694;&#26550;&#20026;&#22312;&#25163;&#26426;&#32593;&#32476;&#20013;&#26816;&#27979;&#24322;&#24120;&#25552;&#20379;&#20102;&#26356;&#24555;&#19988;&#33258;&#20027;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#22788;&#29702;&#22823;&#25968;&#25454;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenges of detecting anomalies in cellular networks in an interpretable way and proposes a new approach using variational autoencoders (VAEs) that learn interpretable representations of the latent space for each Key Performance Indicator (KPI) in the dataset. This enables the detection of anomalies based on reconstruction loss and Z-scores. We ensure the interpretability of the anomalies via additional information centroids (c) using the K-means algorithm to enhance representation learning. We evaluate the performance of the model by analyzing patterns in the latent dimension for specific KPIs and thereby demonstrate the interpretability and anomalies. The proposed framework offers a faster and autonomous solution for detecting anomalies in cellular networks and showcases the potential of deep learning-based algorithms in handling big data.
&lt;/p&gt;</description></item><item><title>&#22909;&#22855;&#22238;&#25918;&#26159;&#19968;&#31181;&#38024;&#23545;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#20195;&#29702;&#30340;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22909;&#22855;&#24230;&#22522;&#30784;&#30340;&#20248;&#20808;&#20449;&#21495;&#65292;&#23427;&#25552;&#39640;&#20102;&#25506;&#32034;&#24615;&#33021;&#65292;&#24182;&#22312;Crafter&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2306.15934</link><description>&lt;p&gt;
&#23545;&#20110;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#36866;&#24212;&#24615;&#30340;&#22909;&#22855;&#22238;&#25918;
&lt;/p&gt;
&lt;p&gt;
Curious Replay for Model-based Adaptation. (arXiv:2306.15934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15934
&lt;/p&gt;
&lt;p&gt;
&#22909;&#22855;&#22238;&#25918;&#26159;&#19968;&#31181;&#38024;&#23545;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#20195;&#29702;&#30340;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22909;&#22855;&#24230;&#22522;&#30784;&#30340;&#20248;&#20808;&#20449;&#21495;&#65292;&#23427;&#25552;&#39640;&#20102;&#25506;&#32034;&#24615;&#33021;&#65292;&#24182;&#22312;Crafter&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#24517;&#39035;&#33021;&#22815;&#22312;&#29615;&#22659;&#25913;&#21464;&#26102;&#24555;&#36895;&#36866;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#36825;&#26041;&#38754;&#20570;&#24471;&#19981;&#22909;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#23427;&#20204;&#22914;&#20309;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#26469;&#35757;&#32451;&#20854;&#19990;&#30028;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22909;&#22855;&#22238;&#25918;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#38024;&#23545;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#19968;&#31181;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22909;&#22855;&#24230;&#22522;&#30784;&#30340;&#20248;&#20808;&#20449;&#21495;&#12290;&#20351;&#29992;&#22909;&#22855;&#22238;&#25918;&#30340;&#20195;&#29702;&#22312;&#21463;&#21040;&#21160;&#29289;&#34892;&#20026;&#21551;&#21457;&#30340;&#25506;&#32034;&#33539;&#24335;&#21644;Crafter&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#24102;&#26377;&#22909;&#22855;&#22238;&#25918;&#30340;DreamerV3&#22312;Crafter&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;19.4&#30340;&#24179;&#22343;&#20998;&#25968;&#65292;&#22823;&#22823;&#25913;&#21892;&#20102;&#20043;&#21069;DreamerV3&#20351;&#29992;&#22343;&#21248;&#22238;&#25918;&#26102;&#30340;&#26368;&#39640;&#20998;&#25968;14.5&#65292;&#24182;&#19988;&#22312;Deepmind Control Suite&#19978;&#30340;&#24615;&#33021;&#20063;&#30456;&#20284;&#12290;&#22909;&#22855;&#22238;&#25918;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/AutonomousAgentsLab/curiousreplay&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents must be able to adapt quickly as an environment changes. We find that existing model-based reinforcement learning agents are unable to do this well, in part because of how they use past experiences to train their world model. Here, we present Curious Replay -- a form of prioritized experience replay tailored to model-based agents through use of a curiosity-based priority signal. Agents using Curious Replay exhibit improved performance in an exploration paradigm inspired by animal behavior and on the Crafter benchmark. DreamerV3 with Curious Replay surpasses state-of-the-art performance on Crafter, achieving a mean score of 19.4 that substantially improves on the previous high score of 14.5 by DreamerV3 with uniform replay, while also maintaining similar performance on the Deepmind Control Suite. Code for Curious Replay is available at https://github.com/AutonomousAgentsLab/curiousreplay
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#39588;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#32416;&#27491;&#30340;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#26041;&#27861;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#26469;&#25913;&#21892;&#36755;&#20986;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15933</link><description>&lt;p&gt;
&#36890;&#36807;&#39564;&#35777;&#21644;&#32416;&#27491;&#25552;&#31034;&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
You Can Generate It Again: Data-to-text Generation with Verification and Correction Prompting. (arXiv:2306.15933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#39588;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#32416;&#27491;&#30340;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#26041;&#27861;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#26469;&#25913;&#21892;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#26377;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#36755;&#20837;&#29983;&#25104;&#25991;&#26412;&#25551;&#36848;&#65288;&#31216;&#20026;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#25324;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#32416;&#27491;&#38454;&#27573;&#30340;&#22810;&#27493;&#39588;&#36807;&#31243;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#19968;&#27425;&#24615;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;VCP&#65288;&#39564;&#35777;&#21644;&#32416;&#27491;&#25552;&#31034;&#65289;&#65292;&#20174;&#27169;&#22411;&#29983;&#25104;&#21021;&#22987;&#36755;&#20986;&#24320;&#22987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32487;&#32493;&#39564;&#35777;&#25152;&#29983;&#25104;&#25991;&#26412;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#27491;&#30830;&#24615;&#12290;&#39564;&#35777;&#27493;&#39588;&#30340;&#35266;&#23519;&#32467;&#26524;&#34987;&#36716;&#21270;&#20026;&#19987;&#38376;&#30340;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#65292;&#35813;&#25552;&#31034;&#25351;&#31034;&#27169;&#22411;&#22312;&#37325;&#26032;&#29983;&#25104;&#36755;&#20986;&#26102;&#32771;&#34385;&#24050;&#35782;&#21035;&#30340;&#38169;&#35823;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#32416;&#27491;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22521;&#35757;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#20351;&#27169;&#22411;&#33021;&#22815;&#34701;&#20837;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#25913;&#21892;&#36755;&#20986;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant advancements in existing models, generating text descriptions from structured data input, known as data-to-text generation, remains a challenging task. In this paper, we propose a novel approach that goes beyond traditional one-shot generation methods by introducing a multi-step process consisting of generation, verification, and correction stages. Our approach, VCP(Verification and Correction Prompting), begins with the model generating an initial output. We then proceed to verify the correctness of different aspects of the generated text. The observations from the verification step are converted into a specialized error-indication prompt, which instructs the model to regenerate the output while considering the identified errors. To enhance the model's correction ability, we have developed a carefully designed training procedure. This procedure enables the model to incorporate feedback from the error-indication prompt, resulting in improved output generation. Throu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#36807;&#28388;&#20989;&#25968;&#26469;&#29983;&#25104;&#26377;&#38480;&#32422;&#26463;&#25991;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#38656;&#27714;&#29983;&#25104;&#24102;&#26377;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#30340;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.15926</link><description>&lt;p&gt;
&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#25104;&#20026;&#35799;&#20154;&#65306;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#21644;&#26377;&#38480;&#25991;&#26412;&#29983;&#25104;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Most Language Models can be Poets too: An AI Writing Assistant and Constrained Text Generation Studio. (arXiv:2306.15926v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15926
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#36807;&#28388;&#20989;&#25968;&#26469;&#29983;&#25104;&#26377;&#38480;&#32422;&#26463;&#25991;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#38656;&#27714;&#29983;&#25104;&#24102;&#26377;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#20851;&#26377;&#38480;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#23545;&#24050;&#34987;&#35789;&#27719;&#12289;&#35821;&#20041;&#25110;&#38899;&#38901;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#30740;&#31350;&#26102;&#38388;&#24456;&#23569;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#21363;&#20351;&#22312;&#26174;&#33879;&#32422;&#26463;&#19979;&#20063;&#33021;&#29983;&#25104;&#24341;&#20154;&#20837;&#32988;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26222;&#36866;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#25991;&#26412;&#21333;&#20803;&#20043;&#21069;&#32452;&#21512;&#24212;&#29992;&#36807;&#28388;&#20989;&#25968;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#65292;&#26469;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#21363;&#25554;&#21363;&#29992;&#30340;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#20462;&#25913;&#12290;&#20026;&#23637;&#31034;&#36825;&#31181;&#25216;&#26415;&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#21517;&#20026;&#26377;&#38480;&#25991;&#26412;&#29983;&#25104;&#24037;&#20855;&#65288;CTGS&#65289;&#12290;CTGS&#20801;&#35768;&#29992;&#25143;&#29983;&#25104;&#25110;&#36873;&#25321;&#20855;&#26377;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#30340;&#25991;&#26412;&#65292;&#20363;&#22914;&#31105;&#27490;&#26576;&#20010;&#23383;&#27597;&#65292;&#24378;&#21046;&#29983;&#25104;&#30340;&#21333;&#35789;&#20855;&#26377;&#19968;&#23450;&#30340;&#38899;&#33410;&#25968;&#65292;&#25110;&#24378;&#21046;&#29983;&#25104;&#19982;&#32473;&#23450;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#21333;&#35789;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite rapid advancement in the field of Constrained Natural Language Generation, little time has been spent on exploring the potential of language models which have had their vocabularies lexically, semantically, and/or phonetically constrained. We find that most language models generate compelling text even under significant constraints. We present a simple and universally applicable technique for modifying the output of a language model by compositionally applying filter functions to the language models vocabulary before a unit of text is generated. This approach is plug-and-play and requires no modification to the model. To showcase the value of this technique, we present an easy to use AI writing assistant called Constrained Text Generation Studio (CTGS). CTGS allows users to generate or choose from text with any combination of a wide variety of constraints, such as banning a particular letter, forcing the generated words to have a certain number of syllables, and/or forcing the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#19977;&#32500;&#29289;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31995;&#32479;&#26469;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#24182;&#35780;&#20272;&#20854;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15919</link><description>&lt;p&gt;
&#31934;&#32454;&#21270;&#30340;&#19977;&#32500;&#29289;&#20307;&#35782;&#21035;&#65306;&#19968;&#31181;&#26041;&#27861;&#21644;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Fine-grained 3D object recognition: an approach and experiments. (arXiv:2306.15919v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#19977;&#32500;&#29289;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31995;&#32479;&#26469;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#24182;&#35780;&#20272;&#20854;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#29289;&#20307;&#35782;&#21035;&#25216;&#26415;&#27491;&#22312;&#25104;&#20026;&#33258;&#21160;&#39550;&#39542;&#31561;&#20808;&#36827;&#25216;&#26415;&#20013;&#30340;&#26680;&#24515;&#25216;&#26415;&#12290;&#30446;&#21069;&#26377;&#20004;&#31181;&#19977;&#32500;&#29289;&#20307;&#35782;&#21035;&#26041;&#27861;&#65306;&#65288;i&#65289;&#22522;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#22914;&#20840;&#23616;&#27491;&#20132;&#29289;&#20307;&#25551;&#36848;&#31526;&#65288;GOOD&#65289;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22914;MobileNet&#21644;VGG&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20010;&#24320;&#25918;&#24615;&#30340;&#39046;&#22495;&#20013;&#65292;&#38656;&#35201;&#30693;&#36947;&#21738;&#31181;&#26041;&#27861;&#22312;&#26102;&#38388;&#25512;&#31227;&#21644;&#24050;&#30693;&#31867;&#21035;&#25968;&#37327;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19988;&#31995;&#32479;&#38656;&#35201;&#29992;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#23398;&#20064;&#26032;&#30340;&#29289;&#20307;&#31867;&#21035;&#12290;&#26412;&#25991;&#39318;&#20808;&#23454;&#29616;&#20102;&#19968;&#20010;&#31163;&#32447;&#30340;&#19977;&#32500;&#29289;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#23558;&#29289;&#20307;&#35270;&#22270;&#20316;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#20998;&#31867;&#26631;&#31614;&#20316;&#20026;&#36755;&#20986;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;&#20351;&#29992;&#22522;&#20110;&#23454;&#20363;&#30340;&#23398;&#20064;&#65288;IBL&#65289;&#26469;&#24418;&#25104;&#19968;&#20010;&#26032;&#30340;&#31867;&#21035;&#65292;&#24182;&#20351;&#29992;K&#25240;&#20132;&#21449;&#39564;&#35777;&#26469;&#35780;&#20272;&#33719;&#24471;&#30340;&#29289;&#20307;&#35782;&#21035;&#24615;&#33021;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#22312;&#32447;&#27169;&#24335;&#19979;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Three-dimensional (3D) object recognition technology is being used as a core technology in advanced technologies such as autonomous driving of automobiles. There are two sets of approaches for 3D object recognition: (i) hand-crafted approaches like Global Orthographic Object Descriptor (GOOD), and (ii) deep learning-based approaches such as MobileNet and VGG. However, it is needed to know which of these approaches works better in an open-ended domain where the number of known categories increases over time, and the system should learn about new object categories using few training examples. In this paper, we first implemented an offline 3D object recognition system that takes an object view as input and generates category labels as output. In the offline stage, instance-based learning (IBL) is used to form a new category and we use K-fold cross-validation to evaluate the obtained object recognition performance. We then test the proposed approach in an online fashion by integrating the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#36890;&#36947;&#21160;&#20316;&#23884;&#20837;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#23398;&#20064;&#31283;&#20581;&#31574;&#30053;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#22312;2D&#36855;&#23467;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#30005;&#23376;&#21830;&#21153;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.15913</link><description>&lt;p&gt;
DCT: &#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#24378;&#21270;&#23398;&#20064;&#30340;&#21452;&#36890;&#36947;&#21160;&#20316;&#23884;&#20837;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DCT: Dual Channel Training of Action Embeddings for Reinforcement Learning with Large Discrete Action Spaces. (arXiv:2306.15913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#36890;&#36947;&#21160;&#20316;&#23884;&#20837;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#23398;&#20064;&#31283;&#20581;&#31574;&#30053;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#22312;2D&#36855;&#23467;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#30005;&#23376;&#21830;&#21153;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#20020;&#32500;&#24230;&#28798;&#38590;&#30340;&#22024;&#26434;&#29615;&#22659;&#20013;&#65292;&#23398;&#20064;&#31283;&#20581;&#31574;&#30053;&#24182;&#24191;&#20041;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#26159;&#26234;&#33021;&#31995;&#32479;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#65292;&#21516;&#26102;&#23454;&#29616;&#23545;&#21407;&#22987;&#21160;&#20316;&#30340;&#37325;&#26500;&#20197;&#21450;&#23545;&#26410;&#26469;&#29366;&#24577;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#36827;&#34892;&#21160;&#20316;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#21452;&#36890;&#36947;&#25439;&#22833;&#26469;&#24179;&#34913;&#21160;&#20316;&#37325;&#26500;&#21644;&#29366;&#24577;&#39044;&#27979;&#31934;&#24230;&#12290;&#25105;&#20204;&#23558;&#35757;&#32451;&#22909;&#30340;&#35299;&#30721;&#22120;&#19982;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#21160;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#19968;&#20010;&#20855;&#26377;4000&#22810;&#20010;&#31163;&#25955;&#22122;&#22768;&#21160;&#20316;&#30340;2D&#36855;&#23467;&#29615;&#22659;&#21644;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30005;&#23376;&#21830;&#21153;&#20132;&#26131;&#25968;&#25454;&#30340;&#20135;&#21697;&#25512;&#33616;&#20219;&#21153;&#20013;&#33021;&#22815;&#32988;&#36807;&#20004;&#20010;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn robust policies while generalizing over large discrete action spaces is an open challenge for intelligent systems, especially in noisy environments that face the curse of dimensionality. In this paper, we present a novel framework to efficiently learn action embeddings that simultaneously allow us to reconstruct the original action as well as to predict the expected future state. We describe an encoder-decoder architecture for action embeddings with a dual channel loss that balances between action reconstruction and state prediction accuracy. We use the trained decoder in conjunction with a standard reinforcement learning algorithm that produces actions in the embedding space. Our architecture is able to outperform two competitive baselines in two diverse environments: a 2D maze environment with more than 4000 discrete noisy actions, and a product recommendation task that uses real-world e-commerce transaction data. Empirical results show that the model results in 
&lt;/p&gt;</description></item><item><title>RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15909</link><description>&lt;p&gt;
RL$^3$:&#36890;&#36807;RL&#20869;&#37096;&#30340;RL$^2$&#25552;&#21319;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15909
&lt;/p&gt;
&lt;p&gt;
RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;meta-RL&#65289;&#26041;&#27861;&#65292;&#22914;RL$^2$&#65292;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#38024;&#23545;&#32473;&#23450;&#20219;&#21153;&#20998;&#24067;&#30340;&#25968;&#25454;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#38271;&#26399;&#20219;&#21153;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#32463;&#39564;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#23558;&#23427;&#20204;&#24635;&#32467;&#20026;&#19968;&#33324;&#30340;&#24378;&#21270;&#23398;&#20064;&#32452;&#20214;&#65292;&#20363;&#22914;&#20215;&#20540;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;transformers&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#21464;&#24471;&#31105;&#27490;&#20043;&#21069;&#20063;&#23545;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#25512;&#29702;&#30340;&#21382;&#21490;&#38271;&#24230;&#26377;&#23454;&#38469;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#19981;&#36275;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#38543;&#30528;&#26356;&#22810;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#23427;&#20204;&#20250;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL$^3$&#65292;&#19968;&#31181;&#32452;&#21512;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#36807;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21040;&#30340;&#29305;&#23450;&#20219;&#21153;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
&lt;/p&gt;</description></item><item><title>&#22810;&#26679;&#24615;&#21363;&#23454;&#21147;&#65288;DIS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DRL&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#22810;&#31181;&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#24182;&#21033;&#29992;&#20114;&#32852;&#30340;&#21382;&#21490;&#27169;&#22411;&#27744;&#32467;&#26500;&#22686;&#24378;&#20854;&#31574;&#30053;&#22810;&#26679;&#24615;&#21644;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#20351;&#29992;&#20154;&#31867;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#12289;&#21487;&#25512;&#24191;&#21644;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#31574;&#30053;&#65292;&#24182;&#22312;AI&#31454;&#36187;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2306.15903</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#24378;&#21270;&#23398;&#20064;&#22312;&#36275;&#29699;&#20840;&#22330;&#27604;&#36187;&#20013;&#30340;&#24212;&#29992;&#65306;&#22810;&#31181;AI&#30340;&#20114;&#21160;&#35757;&#32451;&#20197;&#25552;&#39640;&#23454;&#21147;(arXiv:2306.15903v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Diversity is Strength: Mastering Football Full Game with Interactive Reinforcement Learning of Multiple AIs. (arXiv:2306.15903v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15903
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#24615;&#21363;&#23454;&#21147;&#65288;DIS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DRL&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#22810;&#31181;&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#24182;&#21033;&#29992;&#20114;&#32852;&#30340;&#21382;&#21490;&#27169;&#22411;&#27744;&#32467;&#26500;&#22686;&#24378;&#20854;&#31574;&#30053;&#22810;&#26679;&#24615;&#21644;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#20351;&#29992;&#20154;&#31867;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#12289;&#21487;&#25512;&#24191;&#21644;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#31574;&#30053;&#65292;&#24182;&#22312;AI&#31454;&#36187;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#35757;&#32451;&#20855;&#26377;&#24378;&#22823;&#21644;&#20016;&#23500;&#31574;&#30053;&#30340;&#20154;&#24037;&#26234;&#33021;&#20173;&#28982;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#21147;&#19982;&#20854;&#31574;&#30053;&#30340;&#22810;&#26679;&#24615;&#23494;&#20999;&#30456;&#20851;&#65292;&#32780;&#36825;&#31181;&#20851;&#31995;&#21487;&#20197;&#25351;&#23548;&#25105;&#20204;&#35757;&#32451;&#20855;&#26377;&#24378;&#22823;&#21644;&#20016;&#23500;&#31574;&#30053;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26679;&#24615;&#21363;&#23454;&#21147;&#65288;DIS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DRL&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35757;&#32451;&#22810;&#31181;&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#19968;&#20010;&#20114;&#32852;&#30340;&#21382;&#21490;&#27169;&#22411;&#27744;&#32467;&#26500;&#30456;&#20114;&#36830;&#25509;&#65292;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#31574;&#30053;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#27169;&#22411;&#35780;&#20272;&#21644;&#31579;&#36873;&#26041;&#26696;&#65292;&#20197;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#26469;&#20016;&#23500;&#27169;&#22411;&#27744;&#24182;&#24471;&#21040;&#26368;&#32456;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#12289;&#21487;&#25512;&#24191;&#21644;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#20154;&#31867;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#35895;&#27468;&#30740;&#31350;&#36275;&#29699;&#65288;GRF&#65289;&#30340;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#36194;&#24471;&#20102;5v5&#21644;11v11&#36187;&#36947;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;GRF&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#22312;5
&lt;/p&gt;
&lt;p&gt;
Training AI with strong and rich strategies in multi-agent environments remains an important research topic in Deep Reinforcement Learning (DRL). The AI's strength is closely related to its diversity of strategies, and this relationship can guide us to train AI with both strong and rich strategies. To prove this point, we propose Diversity is Strength (DIS), a novel DRL training framework that can simultaneously train multiple kinds of AIs. These AIs are linked through an interconnected history model pool structure, which enhances their capabilities and strategy diversities. We also design a model evaluation and screening scheme to select the best models to enrich the model pool and obtain the final AI. The proposed training method provides diverse, generalizable, and strong AI strategies without using human data. We tested our method in an AI competition based on Google Research Football (GRF) and won the 5v5 and 11v11 tracks. The method enables a GRF AI to have a high level on both 5
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#20010;&#21035;&#21644;&#32467;&#26500;&#21270;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;IS-GIB&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22495;&#22806;&#22270;&#20687;&#36890;&#29992;&#21270;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20002;&#24323;&#34394;&#20551;&#29305;&#24449;&#21644;&#21033;&#29992;&#32467;&#26500;&#20851;&#32852;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15902</link><description>&lt;p&gt;
&#20010;&#21035;&#21644;&#32467;&#26500;&#21270;&#22270;&#20449;&#24687;&#29942;&#39048;&#23545;&#20110;&#22495;&#22806;&#36890;&#29992;&#21270;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Individual and Structural Graph Information Bottlenecks for Out-of-Distribution Generalization. (arXiv:2306.15902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15902
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#20010;&#21035;&#21644;&#32467;&#26500;&#21270;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;IS-GIB&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22495;&#22806;&#22270;&#20687;&#36890;&#29992;&#21270;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20002;&#24323;&#34394;&#20551;&#29305;&#24449;&#21644;&#21033;&#29992;&#32467;&#26500;&#20851;&#32852;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#22806;&#22270;&#20687;&#36890;&#29992;&#21270;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#20002;&#24323;&#19982;&#26631;&#31614;&#26080;&#20851;&#30340;&#36755;&#20837;&#20013;&#30340;&#34394;&#20551;&#25110;&#22122;&#22768;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20027;&#35201;&#36827;&#34892;&#23454;&#20363;&#32423;&#21035;&#30340;&#31867;&#19981;&#21464;&#22270;&#23398;&#20064;&#65292;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#22270;&#23454;&#20363;&#20043;&#38388;&#30340;&#32467;&#26500;&#21270;&#31867;&#21035;&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#31216;&#20026;&#20010;&#21035;&#21644;&#32467;&#26500;&#21270;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;IS-GIB&#65289;&#12290;&#20026;&#20102;&#28040;&#38500;&#30001;&#20998;&#24067;&#20559;&#31227;&#24341;&#36215;&#30340;&#31867;&#21035;&#34394;&#20551;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20010;&#21035;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;I-GIB&#65289;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#22270;&#19982;&#20854;&#23884;&#20837;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#20002;&#24323;&#19981;&#30456;&#20851;&#20449;&#24687;&#12290;&#20026;&#20102;&#21033;&#29992;&#32467;&#26500;&#20869;&#37096;&#21644;&#36328;&#22495;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;S-GIB&#65289;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#19968;&#25209;&#20855;&#26377;&#22810;&#20010;&#22495;&#30340;&#22270;&#65292;S-GIB&#39318;&#20808;&#35745;&#31639;&#25104;&#23545;&#30340;&#36755;&#20837;-&#36755;&#20837;&#12289;&#23884;&#20837;-&#23884;&#20837;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) graph generalization are critical for many real-world applications. Existing methods neglect to discard spurious or noisy features of inputs, which are irrelevant to the label. Besides, they mainly conduct instance-level class-invariant graph learning and fail to utilize the structural class relationships between graph instances. In this work, we endeavor to address these issues in a unified framework, dubbed Individual and Structural Graph Information Bottlenecks (IS-GIB). To remove class spurious feature caused by distribution shifts, we propose Individual Graph Information Bottleneck (I-GIB) which discards irrelevant information by minimizing the mutual information between the input graph and its embeddings. To leverage the structural intra- and inter-domain correlations, we propose Structural Graph Information Bottleneck (S-GIB). Specifically for a batch of graphs with multiple domains, S-GIB first computes the pair-wise input-input, embedding-embedding, a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#24615;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#24402;&#22240;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#23646;&#24615;&#21270;&#25552;&#31034;&#23545;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#20851;&#20110;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#31995;&#32479;&#24615;&#20559;&#24046;&#23384;&#22312;&#20110;&#29983;&#25104;&#25968;&#25454;&#20013;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15895</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#65306;&#22810;&#26679;&#24615;&#21644;&#20559;&#24046;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. (arXiv:2306.15895v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#24615;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#24402;&#22240;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#23646;&#24615;&#21270;&#25552;&#31034;&#23545;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#20851;&#20110;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#31995;&#32479;&#24615;&#20559;&#24046;&#23384;&#22312;&#20110;&#29983;&#25104;&#25968;&#25454;&#20013;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#31867;&#21035;&#26465;&#20214;&#25552;&#31034;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#29983;&#25104;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#32487;&#25215;&#20102;LLM&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#23646;&#24615;&#30340;&#25552;&#31034;(&#20363;&#22914;&#25351;&#23450;&#38271;&#24230;&#21644;&#39118;&#26684;&#31561;&#23646;&#24615;)&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#65292;&#36825;&#26377;&#28508;&#21147;&#20135;&#29983;&#22810;&#26679;&#21644;&#24402;&#22240;&#30340;&#29983;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#20855;&#26377;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23646;&#24615;&#21270;&#25552;&#31034;&#22312;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#31616;&#21333;&#30340;&#31867;&#21035;&#26465;&#20214;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#39033;&#21253;&#25324;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#31561;&#20851;&#38190;&#26041;&#38754;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#39318;&#20808;&#65292;&#31995;&#32479;&#24615;&#20559;&#24046;&#22312;&#29983;&#25104;&#25968;&#25454;&#20013;&#23384;&#22312;&#65307;&#20854;&#27425;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65307;&#26368;&#21518;&#65292;&#36827;&#34892;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, sy
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;GPT3.5&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#21327;&#35758;&#20998;&#37197;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#21487;&#20449;&#24230;&#65292;&#21457;&#29616;&#20854;&#22312;&#24615;&#33021;&#19978;&#19981;&#22914;BERT&#21644;&#25918;&#23556;&#31185;&#21307;&#24072;&#65292;&#20294;&#22312;&#35299;&#37322;&#20915;&#31574;&#33021;&#21147;&#12289;&#26816;&#27979;&#30456;&#20851;&#35789;&#26631;&#35782;&#21644;&#27169;&#22411;&#26657;&#20934;&#26041;&#38754;&#20248;&#20110;BERT&#12290;</title><link>http://arxiv.org/abs/2306.15887</link><description>&lt;p&gt;
&#36229;&#36234;&#28818;&#20316;&#65306;&#35780;&#20272;GPT3.5&#22312;&#24615;&#33021;&#12289;&#21487;&#20449;&#24230;&#21644;&#20020;&#24202;&#36866;&#29992;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of GPT3.5. (arXiv:2306.15887v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15887
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;GPT3.5&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#21327;&#35758;&#20998;&#37197;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#21487;&#20449;&#24230;&#65292;&#21457;&#29616;&#20854;&#22312;&#24615;&#33021;&#19978;&#19981;&#22914;BERT&#21644;&#25918;&#23556;&#31185;&#21307;&#24072;&#65292;&#20294;&#22312;&#35299;&#37322;&#20915;&#31574;&#33021;&#21147;&#12289;&#26816;&#27979;&#30456;&#20851;&#35789;&#26631;&#35782;&#21644;&#27169;&#22411;&#26657;&#20934;&#26041;&#38754;&#20248;&#20110;BERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#39640;&#39118;&#38505;&#30340;&#29615;&#22659;&#20013;&#65292;&#22914;&#21307;&#30103;&#29615;&#22659;&#19979;&#65292;&#23545;LLM&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#26159;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;GPT3.5&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#21327;&#35758;&#20998;&#37197;&#26041;&#38754;&#24615;&#33021;&#21644;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#27169;&#22411;&#21644;&#25918;&#23556;&#31185;&#21307;&#24072;&#36827;&#34892;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35831;&#19968;&#20301;&#25918;&#23556;&#31185;&#21307;&#24072;&#23457;&#26597;GPT3.5&#30340;&#36755;&#20986;&#65292;&#35780;&#20272;&#20854;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#21253;&#25324;&#27178;&#36328;&#25972;&#20010;&#22836;&#37096;&#30340;11&#20010;&#25104;&#20687;&#21327;&#35758;&#31867;&#21035;&#20013;&#30340;4,700&#20010;&#21307;&#24072;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;GPT3.5&#30340;&#24615;&#33021;&#22312;BERT&#21644;&#25918;&#23556;&#31185;&#21307;&#24072;&#20043;&#21518;&#12290;&#28982;&#32780;&#65292;GPT3.5&#22312;&#35299;&#37322;&#20854;&#20915;&#31574;&#33021;&#21147;&#12289;&#26816;&#27979;&#30456;&#20851;&#35789;&#26631;&#35782;&#21644;&#27169;&#22411;&#26657;&#20934;&#26041;&#38754;&#20248;&#20110;BERT&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;GPT3.5&#38169;&#35823;&#20998;&#31867;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;&#20854;&#22312;&#24615;&#33021;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of large language models (LLMs) in healthcare is gaining popularity, but their practicality and safety in clinical settings have not been thoroughly assessed. In high-stakes environments like medical settings, trust and safety are critical issues for LLMs. To address these concerns, we present an approach to evaluate the performance and trustworthiness of a GPT3.5 model for medical image protocol assignment. We compare it with a fine-tuned BERT model and a radiologist. In addition, we have a radiologist review the GPT3.5 output to evaluate its decision-making process. Our evaluation dataset consists of 4,700 physician entries across 11 imaging protocol classes spanning the entire head. Our findings suggest that the GPT3.5 performance falls behind BERT and a radiologist. However, GPT3.5 outperforms BERT in its ability to explain its decision, detect relevant word indicators, and model calibration. Furthermore, by analyzing the explanations of GPT3.5 for misclassifications, we re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#39046;&#22495;&#30340;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#65292;&#22312;&#19982;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#31561;&#30456;&#20851;&#27010;&#24565;&#30340;&#27604;&#36739;&#20013;&#65292;&#24635;&#32467;&#21644;&#20998;&#26512;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.15880</link><description>&lt;p&gt;
&#38754;&#21521;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#30340;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Towards Open Vocabulary Learning: A Survey. (arXiv:2306.15880v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#39046;&#22495;&#30340;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#65292;&#22312;&#19982;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#31561;&#30456;&#20851;&#27010;&#24565;&#30340;&#27604;&#36739;&#20013;&#65292;&#24635;&#32467;&#21644;&#20998;&#26512;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#39046;&#22495;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#26816;&#27979;&#31561;&#21508;&#31181;&#26680;&#24515;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#22522;&#20110;&#23553;&#38381;&#38598;&#30340;&#20551;&#35774;&#65292;&#21363;&#27169;&#22411;&#21482;&#33021;&#35782;&#21035;&#35757;&#32451;&#38598;&#20013;&#24050;&#23450;&#20041;&#30340;&#31867;&#21035;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#12290;&#36825;&#20123;&#26032;&#26041;&#27861;&#26088;&#22312;&#23450;&#20301;&#21644;&#35782;&#21035;&#36229;&#20986;&#27880;&#37322;&#26631;&#31614;&#31354;&#38388;&#30340;&#31867;&#21035;&#12290;&#19982;&#24369;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#30456;&#27604;&#65292;&#24320;&#25918;&#35789;&#27719;&#26041;&#27861;&#26356;&#21152;&#36890;&#29992;&#12289;&#23454;&#29992;&#21644;&#26377;&#25928;&#12290;&#26412;&#25991;&#23545;&#24320;&#25918;&#35789;&#27719;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#24635;&#32467;&#21644;&#20998;&#26512;&#20102;&#36817;&#26399;&#22312;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#19982;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#24320;&#25918;&#38598;&#35782;&#21035;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#31561;&#30456;&#20851;&#27010;&#24565;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#28982;&#21518;&#65292;&#22312;&#20998;&#21106;&#20219;&#21153;&#30340;&#20960;&#20010;&#32039;&#23494;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of visual scene understanding, deep neural networks have made impressive advancements in various core tasks like segmentation, tracking, and detection. However, most approaches operate on the close-set assumption, meaning that the model can only identify pre-defined categories that are present in the training set. Recently, open vocabulary settings were proposed due to the rapid progress of vision language pre-training. These new approaches seek to locate and recognize categories beyond the annotated label space. The open vocabulary approach is more general, practical, and effective compared to weakly supervised and zero-shot settings. This paper provides a thorough review of open vocabulary learning, summarizing and analyzing recent developments in the field. In particular, we begin by comparing it to related concepts such as zero-shot learning, open-set recognition, and out-of-distribution detection. Then, we review several closely related tasks in the case of segmentati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#23454;&#29616;&#20960;&#20309;&#20449;&#24687;&#26377;&#26681;&#25454;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.15858</link><description>&lt;p&gt;
&#29992;&#20110;&#25163;&#20013;&#29289;&#20307;&#33258;&#24863;&#30693;6D&#23039;&#24577;&#20272;&#35745;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Graph Neural Networks for Proprioceptive 6D Pose Estimation of In-hand Objects. (arXiv:2306.15858v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#32467;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#23454;&#29616;&#20960;&#20309;&#20449;&#24687;&#26377;&#26681;&#25454;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#29305;&#21035;&#26159;&#25163;&#20013;&#29289;&#20307;&#30340;&#25805;&#20316;&#65292;&#36890;&#24120;&#38656;&#35201;&#20934;&#30830;&#20272;&#35745;&#29289;&#20307;&#30340;6D&#23039;&#24577;&#12290;&#20026;&#20102;&#25552;&#39640;&#20272;&#35745;&#23039;&#24577;&#30340;&#20934;&#30830;&#24615;&#65292;&#30446;&#21069;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20351;&#29992;&#26469;&#33258;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#20363;&#22914;RGB&#22270;&#20687;&#12289;&#28145;&#24230;&#21644;&#35302;&#35273;&#35835;&#25968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23545;&#36825;&#20123;&#27169;&#24577;&#25429;&#33719;&#30340;&#29289;&#20307;&#30340;&#22522;&#26412;&#20960;&#20309;&#32467;&#26500;&#30340;&#21033;&#29992;&#26377;&#38480;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#23545;&#35270;&#35273;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;&#36825;&#23548;&#33268;&#24403;&#38754;&#23545;&#32570;&#20047;&#36825;&#31181;&#35270;&#35273;&#29305;&#24449;&#30340;&#29289;&#20307;&#25110;&#32773;&#35270;&#35273;&#29305;&#24449;&#34987;&#36974;&#25377;&#26102;&#65292;&#24615;&#33021;&#36739;&#24046;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#20063;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#25163;&#25351;&#20301;&#32622;&#20013;&#23884;&#20837;&#30340;&#24863;&#35273;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32467;&#21512;&#22810;&#27169;&#24577;&#65288;&#35270;&#35273;&#21644;&#35302;&#35273;&#65289;&#25968;&#25454;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20960;&#20309;&#20449;&#24687;&#26377;&#26681;&#25454;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic manipulation, in particular in-hand object manipulation, often requires an accurate estimate of the object's 6D pose. To improve the accuracy of the estimated pose, state-of-the-art approaches in 6D object pose estimation use observational data from one or more modalities, e.g., RGB images, depth, and tactile readings. However, existing approaches make limited use of the underlying geometric structure of the object captured by these modalities, thereby, increasing their reliance on visual features. This results in poor performance when presented with objects that lack such visual features or when visual features are simply occluded. Furthermore, current approaches do not take advantage of the proprioceptive information embedded in the position of the fingers. To address these limitations, in this paper: (1) we introduce a hierarchical graph neural network architecture for combining multimodal (vision and touch) data that allows for a geometrically informed 6D object pose estima
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#20132;&#20114;&#23398;&#20064;&#21644;&#21629;&#21517;&#28216;&#25103;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#36890;&#36807;&#35789;&#24207;&#21015;&#20132;&#27969;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20855;&#26377;&#32452;&#21512;&#24615;&#30340;&#35821;&#20041;&#30693;&#35782;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.15837</link><description>&lt;p&gt;
&#20174;&#20154;&#38469;&#20132;&#20114;&#38754;&#20020;&#22330;&#26223;&#23398;&#20064;&#30475;&#31526;&#21495;&#30340;&#20986;&#29616;&#65306;&#35821;&#20041;&#30693;&#35782;&#30340;&#32452;&#21512;&#24615;&#20986;&#29616;&#26426;&#21046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Symbol emergence as interpersonal cross-situational learning: the emergence of lexical knowledge with combinatoriality. (arXiv:2306.15837v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#20132;&#20114;&#23398;&#20064;&#21644;&#21629;&#21517;&#28216;&#25103;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#36890;&#36807;&#35789;&#24207;&#21015;&#20132;&#27969;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20855;&#26377;&#32452;&#21512;&#24615;&#30340;&#35821;&#20041;&#30693;&#35782;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;Metropolis-Hastings&#21629;&#21517;&#28216;&#25103;&#21644;&#20132;&#20114;&#23398;&#20064;&#65292;&#20351;&#20195;&#29702;&#22312;&#31526;&#21495;&#20986;&#29616;&#31995;&#32479;&#20013;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#24615;&#33719;&#24471;&#35789;&#27719;&#30693;&#35782;&#12290;&#35768;&#22810;&#35745;&#31639;&#27169;&#22411;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#30740;&#31350;&#32039;&#24613;&#20132;&#27969;&#20013;&#30340;&#32452;&#21512;&#24615;&#21644;&#35748;&#30693;&#19982;&#21457;&#23637;&#26426;&#22120;&#20154;&#20013;&#30340;&#31526;&#21495;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#22522;&#20110;&#24863;&#30693;&#36816;&#21160;&#20449;&#24687;&#30340;&#31867;&#21035;&#24418;&#25104;&#21644;&#36890;&#36807;&#21333;&#19968;&#32508;&#21512;&#27169;&#22411;&#20013;&#30340;&#35789;&#24207;&#21015;&#20132;&#27969;&#30340;&#31526;&#21495;&#20132;&#27969;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#24863;&#30693;&#36816;&#21160;&#20449;&#24687;&#36827;&#34892;&#31867;&#21035;&#24418;&#25104;&#65292;&#24182;&#36890;&#36807;&#20195;&#29702;&#38388;&#30340;&#35789;&#24207;&#21015;&#20132;&#27969;&#23454;&#29616;&#31526;&#21495;&#30340;&#20986;&#29616;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20855;&#26377;&#32452;&#21512;&#24615;&#30340;&#35821;&#20041;&#30693;&#35782;&#30340;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#32467;&#21512;&#19982;&#35789;&#30456;&#20851;&#30340;&#20449;&#24687;&#39044;&#27979;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#30340;&#24863;&#30693;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a computational model for a symbol emergence system that enables the emergence of lexical knowledge with combinatoriality among agents through a Metropolis-Hastings naming game and cross-situational learning. Many computational models have been proposed to investigate combinatoriality in emergent communication and symbol emergence in cognitive and developmental robotics. However, existing models do not sufficiently address category formation based on sensory-motor information and semiotic communication through the exchange of word sequences within a single integrated model. Our proposed model facilitates the emergence of lexical knowledge with combinatoriality by performing category formation using multimodal sensory-motor information and enabling semiotic communication through the exchange of word sequences among agents in a unified model. Furthermore, the model enables an agent to predict sensory-motor information for unobserved situations by combining words associated wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#32531;&#35299;&#39068;&#33394;&#20559;&#31227;&#30340;&#35745;&#31639;&#24265;&#20215;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#32469;&#36807;&#36830;&#25509;&#26469;&#25913;&#21892;&#29983;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#22343;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.15832</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#32531;&#35299;&#39068;&#33394;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Easing Color Shifts in Score-Based Diffusion Models. (arXiv:2306.15832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#32531;&#35299;&#39068;&#33394;&#20559;&#31227;&#30340;&#35745;&#31639;&#24265;&#20215;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#32469;&#36807;&#36830;&#25509;&#26469;&#25913;&#21892;&#29983;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24471;&#20998;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#21487;&#33021;&#20250;&#22240;&#31354;&#38388;&#22343;&#20540;&#30340;&#38169;&#35823;&#32780;&#20986;&#29616;&#39068;&#33394;&#20559;&#31227;&#65292;&#36825;&#31181;&#25928;&#24212;&#22312;&#36739;&#22823;&#30340;&#22270;&#20687;&#20013;&#20250;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#24265;&#20215;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20943;&#36731;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#39068;&#33394;&#20559;&#31227;&#12290;&#25105;&#20204;&#22312;&#24471;&#20998;&#32593;&#32476;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#32469;&#36807;&#36830;&#25509;&#65292;&#29992;&#20110;&#22788;&#29702;&#36755;&#20837;&#30340;&#31354;&#38388;&#22343;&#20540;&#65292;&#24182;&#39044;&#27979;&#24471;&#20998;&#20989;&#25968;&#30340;&#22343;&#20540;&#12290;&#36825;&#31181;&#32593;&#32476;&#26550;&#26500;&#26174;&#33879;&#25913;&#21892;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#22343;&#20540;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25913;&#36827;&#19982;&#29983;&#25104;&#22270;&#20687;&#22823;&#23567;&#30340;&#20851;&#31995;&#36817;&#20284;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20026;&#36328;&#22270;&#20687;&#23610;&#23544;&#30340;&#39068;&#33394;&#20559;&#31227;&#38382;&#39064;&#25552;&#20379;&#20102;&#30456;&#23545;&#24265;&#20215;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#29702;&#24819;&#21270;&#24773;&#20917;&#19979;&#39068;&#33394;&#20559;&#31227;&#30340;&#36215;&#28304;&#65292;&#20197;&#25512;&#21160;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25552;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generated images of score-based models can suffer from errors in their spatial means, an effect, referred to as a color shift, which grows for larger images. This paper introduces a computationally inexpensive solution to mitigate color shifts in score-based diffusion models. We propose a simple nonlinear bypass connection in the score network, designed to process the spatial mean of the input and to predict the mean of the score function. This network architecture substantially improves the resulting spatial means of the generated images, and we show that the improvement is approximately independent of the size of the generated images. As a result, our solution offers a comparatively inexpensive solution for the color shift problem across image sizes. Lastly, we discuss the origin of color shifts in an idealized setting in order to motivate our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#31574;&#30053;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65288;MAT&#65289;&#65292;&#36890;&#36807;&#22312;&#32454;&#35843;&#38454;&#27573;&#21152;&#20837;&#23545;&#25239;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#37319;&#26679;&#26041;&#27861;&#24314;&#31435;&#20102;MAT&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MAT&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15826</link><description>&lt;p&gt;
MAT: &#23545;&#24494;&#35843;&#20013;&#23545;&#25239;&#35757;&#32451;&#30340;&#28151;&#21512;&#31574;&#30053;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
MAT: Mixed-Strategy Game of Adversarial Training in Fine-tuning. (arXiv:2306.15826v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#31574;&#30053;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65288;MAT&#65289;&#65292;&#36890;&#36807;&#22312;&#32454;&#35843;&#38454;&#27573;&#21152;&#20837;&#23545;&#25239;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#37319;&#26679;&#26041;&#27861;&#24314;&#31435;&#20102;MAT&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MAT&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26377;&#25928;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#32454;&#35843;&#38454;&#27573;&#21152;&#20837;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#21338;&#24328;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#23545;&#25239;&#35757;&#32451;&#30340;&#24212;&#29992;&#23545;&#24212;&#20110;&#32431;&#31574;&#30053;&#28216;&#25103;&#65292;&#20854;&#22312;&#31574;&#30053;&#33539;&#22260;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;&#20026;&#20102;&#25512;&#21160;&#24615;&#33021;&#36793;&#30028;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#31574;&#30053;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65288;MAT&#65289;&#12290;&#22312;&#26041;&#27861;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#29109;&#38236;&#20687;&#19979;&#38477;&#25512;&#23548;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#28151;&#21512;&#31574;&#30053;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#36890;&#36807;&#37319;&#26679;&#26041;&#27861;&#24314;&#31435;&#20102;MAT&#12290;&#20026;&#20102;&#39564;&#35777;MAT&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;BERT&#21644;RoBERTa&#31561;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#22522;&#20934;&#23454;&#39564;&#12290;MAT&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large-scale pre-trained language models has been demonstrated effective for various natural language processing (NLP) tasks. Previous studies have established that incorporating adversarial training during the fine-tuning stage can significantly enhance model generalization and robustness. However, from the perspective of game theory, such utilizations of adversarial training correspond to pure-strategy games, which are inherently limited in terms of the scope of their strategies, thereby still having room for improvement. In order to push the performance boundaries, we propose a novel Mixed-strategy Adversarial Training algorithm (MAT). Methodologically, we derive the Nash equilibrium of a mixed-strategy game for adversarial training using Entropy Mirror Descent to establish MAT by sampling method. To verify the effectiveness of MAT, we conducted extensive benchmark experiments on large-scale pre-trained models, such as BERT and RoBERTa. MAT significantly outperforms the s
&lt;/p&gt;</description></item><item><title>G\"odel-Dummett&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#20351;&#29992;&#23454;&#20540;&#21644;&#21452;&#20851;&#31995;&#35821;&#20041;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#25311;&#27169;&#22411;&#31639;&#27861;&#35299;&#20915;&#20102;&#35777;&#20266;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15805</link><description>&lt;p&gt;
G\"odel-Dummett&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
G\"odel-Dummett linear temporal logic. (arXiv:2306.15805v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15805
&lt;/p&gt;
&lt;p&gt;
G\"odel-Dummett&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#20351;&#29992;&#23454;&#20540;&#21644;&#21452;&#20851;&#31995;&#35821;&#20041;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#25311;&#27169;&#22411;&#31639;&#27861;&#35299;&#20915;&#20102;&#35777;&#20266;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#29256;&#26412;&#30340;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65292;&#20854;&#20013;&#21629;&#39064;&#29255;&#27573;&#26159;G\"odel-Dummett&#36923;&#36753;&#65288;&#23427;&#26082;&#26159;&#19968;&#20010;&#36229;&#30452;&#35273;&#36923;&#36753;&#21448;&#26159;&#19968;&#20010;t-&#33539;&#25968;&#27169;&#31946;&#36923;&#36753;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#33258;&#28982;&#35821;&#20041;&#23450;&#20041;&#20102;&#35813;&#36923;&#36753;&#65306;&#31532;&#19968;&#31181;&#26159;&#23454;&#20540;&#35821;&#20041;&#65292;&#20854;&#20013;&#35821;&#21477;&#22312;&#23454;&#25968;&#21333;&#20301;&#21306;&#38388;&#20013;&#20855;&#26377;&#30495;&#23454;&#24230;&#65307;&#31532;&#20108;&#31181;&#26159;"&#21452;&#20851;&#31995;"&#35821;&#20041;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#35821;&#20041;&#30830;&#23454;&#23450;&#20041;&#20102;&#21516;&#19968;&#31181;&#36923;&#36753;&#65306;&#22312;&#23454;&#20540;&#35821;&#20041;&#20013;&#25104;&#31435;&#30340;&#35821;&#21477;&#19982;&#22312;"&#21452;&#20851;&#31995;"&#35821;&#20041;&#20013;&#25104;&#31435;&#30340;&#35821;&#21477;&#30456;&#21516;&#12290;&#36825;&#20010;G\"odel&#26102;&#38388;&#36923;&#36753;&#22312;&#36825;&#20004;&#31181;&#35821;&#20041;&#20013;&#37117;&#27809;&#26377;&#20219;&#20309;&#24418;&#24335;&#30340;&#26377;&#38480;&#27169;&#22411;&#29305;&#24615;&#65306;&#26377;&#19968;&#20123;&#38750;&#25104;&#31435;&#30340;&#35821;&#21477;&#21482;&#33021;&#22312;&#26080;&#38480;&#27169;&#22411;&#19978;&#34987;&#35777;&#20266;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20351;&#29992;&#25311;&#27169;&#22411;&#30340;&#25216;&#26415;&#27010;&#24565;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#20010;&#21487;&#20197;&#35777;&#20266;&#30340;&#35821;&#21477;&#22312;&#26377;&#38480;&#25311;&#27169;&#22411;&#19978;&#37117;&#21487;&#20197;&#35777;&#20266;&#65292;&#20174;&#32780;&#32473;&#20986;&#20102;&#21028;&#26029;&#35821;&#21477;&#26159;&#21542;&#25104;&#31435;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate a version of linear temporal logic whose propositional fragment is G\"odel-Dummett logic (which is well known both as a superintuitionistic logic and a t-norm fuzzy logic). We define the logic using two natural semantics: first a real-valued semantics, where statements have a degree of truth in the real unit interval and second a `bi-relational' semantics. We then show that these two semantics indeed define one and the same logic: the statements that are valid for the real-valued semantics are the same as those that are valid for the bi-relational semantics. This G\"odel temporal logic does not have any form of the finite model property for these two semantics: there are non-valid statements that can only be falsified on an infinite model. However, by using the technical notion of a quasimodel, we show that every falsifiable statement is falsifiable on a finite quasimodel, yielding an algorithm for deciding if a statement is valid or not. Later, we strengthen this decida
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#35299;&#37322;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#36923;&#36753;&#30340;&#35299;&#37322;&#35745;&#31639;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#35745;&#31639;&#35299;&#37322;&#30340;&#31639;&#27861;&#20063;&#36866;&#29992;&#20110;&#32473;&#23450;&#37096;&#20998;&#25351;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#12290;&#35813;&#30740;&#31350;&#20026;&#22788;&#29702;&#37096;&#20998;&#25351;&#23450;&#36755;&#20837;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24212;&#29992;&#20110;&#20998;&#31867;&#22120;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.15803</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#36923;&#36753;&#30340;&#35299;&#37322;&#24615;&#21644;&#37096;&#20998;&#25351;&#23450;&#36755;&#20837;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Logic-Based Explainability with Partially Specified Inputs. (arXiv:2306.15803v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#35299;&#37322;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#36923;&#36753;&#30340;&#35299;&#37322;&#35745;&#31639;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#35745;&#31639;&#35299;&#37322;&#30340;&#31639;&#27861;&#20063;&#36866;&#29992;&#20110;&#32473;&#23450;&#37096;&#20998;&#25351;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#12290;&#35813;&#30740;&#31350;&#20026;&#22788;&#29702;&#37096;&#20998;&#25351;&#23450;&#36755;&#20837;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24212;&#29992;&#20110;&#20998;&#31867;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#38469;&#37096;&#32626;&#20013;&#65292;&#32570;&#22833;&#25968;&#25454;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#12290;&#32570;&#22833;&#25968;&#25454;&#36890;&#24120;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#36827;&#34892;&#22788;&#29702;&#12290;&#20294;&#26159;&#65292;&#22312;&#20915;&#31574;&#39044;&#27979;&#21644;&#35299;&#37322;&#36825;&#20123;&#39044;&#27979;&#26102;&#65292;&#20063;&#38656;&#35201;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#32570;&#22833;&#25968;&#25454;&#20026;&#37096;&#20998;&#25351;&#23450;&#24453;&#35299;&#37322;&#30340;&#36755;&#20837;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#25351;&#23450;&#36755;&#20837;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#35299;&#37322;&#35745;&#31639;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#30340;&#22823;&#22810;&#25968;&#29992;&#20110;&#35745;&#31639;&#22522;&#20110;&#36923;&#36753;&#30340;&#35299;&#37322;&#30340;&#31639;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#35745;&#31639;&#32473;&#23450;&#37096;&#20998;&#25351;&#23450;&#36755;&#20837;&#30340;&#35299;&#37322;&#12290;&#19968;&#20010;&#30456;&#20851;&#30340;&#32467;&#26524;&#26159;&#35745;&#31639;&#22522;&#20110;&#36923;&#36753;&#30340;&#35299;&#37322;&#30340;&#22797;&#26434;&#24615;&#20445;&#25345;&#19981;&#21464;&#12290;&#22312;&#21463;&#36755;&#20837;&#32422;&#26463;&#30340;&#36923;&#36753;&#35299;&#37322;&#24615;&#24773;&#20917;&#19979;&#65292;&#35777;&#26126;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23558;&#35745;&#31639;&#32473;&#23450;&#37096;&#20998;&#25351;&#23450;&#36755;&#20837;&#30340;&#35299;&#37322;&#30340;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#24212;&#29992;&#20110;&#20998;&#31867;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the practical deployment of machine learning (ML) models, missing data represents a recurring challenge. Missing data is often addressed when training ML models. But missing data also needs to be addressed when deciding predictions and when explaining those predictions. Missing data represents an opportunity to partially specify the inputs of the prediction to be explained. This paper studies the computation of logic-based explanations in the presence of partially specified inputs. The paper shows that most of the algorithms proposed in recent years for computing logic-based explanations can be generalized for computing explanations given the partially specified inputs. One related result is that the complexity of computing logic-based explanations remains unchanged. A similar result is proved in the case of logic-based explainability subject to input constraints. Furthermore, the proposed solution for computing explanations given partially specified inputs is applied to classifiers
&lt;/p&gt;</description></item><item><title>ConKI&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#24615;&#30693;&#35782;&#27880;&#20837;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#22312;&#36890;&#29992;&#30693;&#35782;&#34920;&#31034;&#30340;&#22522;&#30784;&#19978;&#23398;&#20064;&#27599;&#31181;&#27169;&#24577;&#30340;&#29305;&#23450;&#30693;&#35782;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15796</link><description>&lt;p&gt;
ConKI: &#23545;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#23545;&#27604;&#24615;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
ConKI: Contrastive Knowledge Injection for Multimodal Sentiment Analysis. (arXiv:2306.15796v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15796
&lt;/p&gt;
&lt;p&gt;
ConKI&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#24615;&#30693;&#35782;&#27880;&#20837;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#22312;&#36890;&#29992;&#30693;&#35782;&#34920;&#31034;&#30340;&#22522;&#30784;&#19978;&#23398;&#20064;&#27599;&#31181;&#27169;&#24577;&#30340;&#29305;&#23450;&#30693;&#35782;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#21495;&#26469;&#26816;&#27979;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#38598;&#20013;&#20110;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#33719;&#24471;&#30340;&#36890;&#29992;&#30693;&#35782;&#19978;&#36827;&#34892;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#34920;&#31034;&#23398;&#20064;&#65292;&#24573;&#30053;&#20102;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#24615;&#30693;&#35782;&#27880;&#20837;&#65288;ConKI&#65289;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#36866;&#37197;&#22120;&#32467;&#26500;&#22312;&#36890;&#29992;&#30693;&#35782;&#34920;&#31034;&#30340;&#22522;&#30784;&#19978;&#23398;&#20064;&#27599;&#31181;&#27169;&#24577;&#30340;&#29305;&#23450;&#30693;&#35782;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;ConKI&#36824;&#20351;&#29992;&#23618;&#27425;&#21270;&#23545;&#27604;&#23398;&#20064;&#36807;&#31243;&#65292;&#22312;&#27599;&#20010;&#27169;&#24577;&#20869;&#37096;&#30340;&#30693;&#35782;&#31867;&#22411;&#20043;&#38388;&#12289;&#27599;&#20010;&#26679;&#26412;&#20869;&#37096;&#30340;&#27169;&#24577;&#20043;&#38388;&#12289;&#20197;&#21450;&#26679;&#26412;&#20043;&#38388;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#20419;&#36827;&#25152;&#25552;&#20986;&#30340;&#34920;&#31034;&#30340;&#26377;&#25928;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;&#23545;&#19977;&#20010;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ConKI&#30340;&#24615;&#33021;&#20248;&#20110;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis leverages multimodal signals to detect the sentiment of a speaker. Previous approaches concentrate on performing multimodal fusion and representation learning based on general knowledge obtained from pretrained models, which neglects the effect of domain-specific knowledge. In this paper, we propose Contrastive Knowledge Injection (ConKI) for multimodal sentiment analysis, where specific-knowledge representations for each modality can be learned together with general knowledge representations via knowledge injection based on an adapter architecture. In addition, ConKI uses a hierarchical contrastive learning procedure performed between knowledge types within every single modality, across modalities within each sample, and across samples to facilitate the effective learning of the proposed representations, hence improving multimodal sentiment predictions. The experiments on three popular multimodal sentiment analysis benchmarks show that ConKI outperforms a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#21160;&#21147;&#23398;&#22312;&#31283;&#20581;&#27493;&#34892;&#26426;&#22120;&#20154;&#20013;&#30340;&#20154;&#32676;&#23618;&#27963;&#21160;&#65292;&#25581;&#31034;&#20102;&#25511;&#21046;&#22120;&#30340;&#25299;&#25169;&#32467;&#26500;&#23545;&#24179;&#34913;&#33021;&#21147;&#30340;&#24433;&#21709;&#65307;&#36890;&#36807;&#24212;&#29992;&#31070;&#32463;&#24178;&#25200;&#25506;&#31350;&#31995;&#32479;&#30340;&#24378;&#36843;&#21709;&#24212;&#65292;&#21457;&#29616;&#24490;&#29615;&#29366;&#24577;&#21160;&#21147;&#23398;&#20855;&#26377;&#32467;&#26500;&#21270;&#21644;&#20302;&#32500;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#30340;&#23384;&#22312;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.15793</link><description>&lt;p&gt;
&#19968;&#20010;&#23545;&#31283;&#20581;&#24577;&#27493;&#34892;&#26426;&#22120;&#20154;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#20154;&#32676;&#23618;&#38754;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Population-Level Analysis of Neural Dynamics in Robust Legged Robots. (arXiv:2306.15793v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#21160;&#21147;&#23398;&#22312;&#31283;&#20581;&#27493;&#34892;&#26426;&#22120;&#20154;&#20013;&#30340;&#20154;&#32676;&#23618;&#27963;&#21160;&#65292;&#25581;&#31034;&#20102;&#25511;&#21046;&#22120;&#30340;&#25299;&#25169;&#32467;&#26500;&#23545;&#24179;&#34913;&#33021;&#21147;&#30340;&#24433;&#21709;&#65307;&#36890;&#36807;&#24212;&#29992;&#31070;&#32463;&#24178;&#25200;&#25506;&#31350;&#31995;&#32479;&#30340;&#24378;&#36843;&#21709;&#24212;&#65292;&#21457;&#29616;&#24490;&#29615;&#29366;&#24577;&#21160;&#21147;&#23398;&#20855;&#26377;&#32467;&#26500;&#21270;&#21644;&#20302;&#32500;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#30340;&#23384;&#22312;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#22686;&#24378;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#23436;&#25104;&#22797;&#26434;&#30340;&#36816;&#21160;&#25511;&#21046;&#20219;&#21153;&#65292;&#22914;&#27493;&#24577;&#21644;&#25805;&#20316;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#38590;&#20197;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#26041;&#27861;&#26469;&#29702;&#35299;&#31283;&#20581;&#26426;&#22120;&#20154;&#27493;&#34892;&#25511;&#21046;&#22120;&#30340;&#20154;&#32676;&#23618;&#27963;&#21160;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#20998;&#26512;&#25299;&#25169;&#32467;&#26500;&#24320;&#22987;&#65292;&#21457;&#29616;&#33030;&#24369;&#30340;&#25511;&#21046;&#22120;&#20855;&#26377;&#26356;&#22810;&#30340;&#22266;&#23450;&#28857;&#21644;&#19981;&#31283;&#23450;&#26041;&#21521;&#65292;&#23548;&#33268;&#22312;&#25351;&#23548;&#19979;&#20445;&#25345;&#24179;&#34913;&#26102;&#26356;&#24046;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#20027;&#23548;&#20154;&#32676;&#23618;&#27963;&#21160;&#26041;&#21521;&#19978;&#24212;&#29992;&#26377;&#38024;&#23545;&#24615;&#30340;&#31070;&#32463;&#24178;&#25200;&#26469;&#20998;&#26512;&#31995;&#32479;&#30340;&#24378;&#36843;&#21709;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#24490;&#29615;&#29366;&#24577;&#21160;&#21147;&#23398;&#22312;&#34892;&#36208;&#36807;&#31243;&#20013;&#20855;&#26377;&#32467;&#26500;&#21270;&#21644;&#20302;&#32500;&#29305;&#24449;&#65292;&#19982;&#28789;&#38271;&#31867;&#21160;&#29289;&#30340;&#30740;&#31350;&#32467;&#26524;&#30456;&#31526;&#12290;&#27492;&#22806;&#65292;&#24403;&#24490;&#29615;&#29366;&#24577;&#25200;&#21160;&#20026;&#38646;&#26102;&#65292;&#33030;&#24369;&#30340;&#25511;&#21046;&#22120;&#20173;&#33021;&#22815;&#34892;&#36208;&#65292;&#36825;&#34920;&#26126;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#26426;&#21046;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural network-based reinforcement learning systems are capable of complex motor control tasks such as locomotion and manipulation, however, much of their underlying mechanisms still remain difficult to interpret. Our aim is to leverage computational neuroscience methodologies to understanding the population-level activity of robust robot locomotion controllers. Our investigation begins by analyzing topological structure, discovering that fragile controllers have a higher number of fixed points with unstable directions, resulting in poorer balance when instructed to stand in place. Next, we analyze the forced response of the system by applying targeted neural perturbations along directions of dominant population-level activity. We find evidence that recurrent state dynamics are structured and low-dimensional during walking, which aligns with primate studies. Additionally, when recurrent states are perturbed to zero, fragile agents continue to walk, which is indicative of a st
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;GPT-4&#30340;&#21484;&#22238;&#29575;&#36739;&#39640;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#36807;&#24230;&#20462;&#27491;&#12290;</title><link>http://arxiv.org/abs/2306.15788</link><description>&lt;p&gt;
&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#35780;&#20272;GPT-3.5&#21644;GPT-4
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese. (arXiv:2306.15788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;GPT-4&#30340;&#21484;&#22238;&#29575;&#36739;&#39640;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#36807;&#24230;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;GPT-3.5&#21644;GPT-4&#36825;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#24037;&#20855;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;Microsoft Word&#21644;Google Docs&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;GEC&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22235;&#20010;&#31867;&#21035;&#65306;&#35821;&#27861;&#12289;&#25340;&#20889;&#12289;&#20114;&#32852;&#32593;&#21644;&#24555;&#36895;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;GPT-4&#30340;&#21484;&#22238;&#29575;&#27604;&#20854;&#20182;&#26041;&#27861;&#39640;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#20855;&#26377;&#36739;&#20302;&#30340;&#31934;&#30830;&#24230;&#65292;&#23548;&#33268;&#36807;&#24230;&#20462;&#27491;&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#23454;&#38469;GEC&#24037;&#20855;&#30340;&#28508;&#21147;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#21644;&#20854;&#20182;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the effectiveness of GPT-3.5 and GPT-4, two large language models, as Grammatical Error Correction (GEC) tools for Brazilian Portuguese and compare their performance against Microsoft Word and Google Docs. We introduce a GEC dataset for Brazilian Portuguese with four categories: Grammar, Spelling, Internet, and Fast typing. Our results show that while GPT-4 has higher recall than other methods, LLMs tend to have lower precision, leading to overcorrection. This study demonstrates the potential of LLMs as practical GEC tools for Brazilian Portuguese and encourages further exploration of LLMs for non-English languages and other educational settings.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#32599;&#29983;&#38376;&#25928;&#24212;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#24433;&#21709;&#65292;&#36825;&#20026;&#20043;&#21069;&#30340;&#36726;&#20107;&#35777;&#25454;&#25552;&#20379;&#20102;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23637;&#31034;&#20102;&#31185;&#23398;&#23478;&#21644;&#23454;&#36341;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.15786</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#32599;&#29983;&#38376;&#25928;&#24212;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of the Rashomon Effect in Explainable Machine Learning. (arXiv:2306.15786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15786
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#32599;&#29983;&#38376;&#25928;&#24212;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#24433;&#21709;&#65292;&#36825;&#20026;&#20043;&#21069;&#30340;&#36726;&#20107;&#35777;&#25454;&#25552;&#20379;&#20102;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23637;&#31034;&#20102;&#31185;&#23398;&#23478;&#21644;&#23454;&#36341;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32599;&#29983;&#38376;&#25928;&#24212;&#25551;&#36848;&#20102;&#20197;&#19979;&#29616;&#35937;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#20855;&#26377;&#30456;&#21516;&#33391;&#22909;&#24615;&#33021;&#20294;&#37319;&#29992;&#19981;&#21516;&#35299;&#20915;&#31574;&#30053;&#30340;&#27169;&#22411;&#12290;&#32599;&#29983;&#38376;&#25928;&#24212;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#23545;&#35299;&#37322;&#30340;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#23545;&#19977;&#31181;&#19981;&#21516;&#27604;&#36739;&#22330;&#26223;&#25552;&#20379;&#20102;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#12289;&#24402;&#22240;&#26041;&#27861;&#21644;&#25351;&#26631;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#36229;&#21442;&#25968;&#35843;&#25972;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#65292;&#25351;&#26631;&#36873;&#25321;&#20063;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20808;&#21069;&#30340;&#36726;&#20107;&#35777;&#25454;&#25552;&#20379;&#20102;&#23454;&#35777;&#25903;&#25345;&#65292;&#24182;&#23637;&#31034;&#20102;&#31185;&#23398;&#23478;&#21644;&#23454;&#36341;&#32773;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Rashomon Effect describes the following phenomenon: for a given dataset there may exist many models with equally good performance but with different solution strategies. The Rashomon Effect has implications for Explainable Machine Learning, especially for the comparability of explanations. We provide a unified view on three different comparison scenarios and conduct a quantitative evaluation across different datasets, models, attribution methods, and metrics. We find that hyperparameter-tuning plays a role and that metric selection matters. Our results provide empirical support for previously anecdotal evidence and exhibit challenges for both scientists and practitioners.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#21360;&#21047;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;OCR&#30340;&#31471;&#21040;&#31471;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2306.15782</link><description>&lt;p&gt;
UTRNet: &#21360;&#21047;&#25991;&#26723;&#20013;&#39640;&#20998;&#36776;&#29575;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
UTRNet: High-Resolution Urdu Text Recognition In Printed Documents. (arXiv:2306.15782v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#21360;&#21047;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;OCR&#30340;&#31471;&#21040;&#31471;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#12289;&#22810;&#23610;&#24230;&#30340;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;UTRNet&#26550;&#26500;&#65292;&#19968;&#20010;&#28151;&#21512;CNN-RNN&#27169;&#22411;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#24037;&#20316;&#24456;&#38590;&#25512;&#24191;&#21040;&#20044;&#23572;&#37117;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#36275;&#22815;&#30340;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UTRSet-Real&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;11,000&#34892;&#30340;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;UTRSet-Synth&#65292;&#19968;&#20010;&#19982;&#23454;&#38469;&#19990;&#30028;&#38750;&#24120;&#30456;&#20284;&#30340;&#21547;&#26377;20,000&#34892;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29616;&#26377;&#30340;IIITH&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#30495;&#23454;&#24615;&#36827;&#34892;&#20102;&#20462;&#27491;&#65292;&#20351;&#20854;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#26356;&#21487;&#38752;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;UrduDoc&#65292;&#19968;&#31181;&#29992;&#20110;&#25195;&#25551;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#24037;&#20855;&#65292;&#36890;&#36807;&#23558;UTRNet&#19982;&#25991;&#26412;&#30340;&#31471;&#21040;&#31471;&#20044;&#23572;&#37117;OCR&#38598;&#25104;&#22312;&#21360;&#21047;&#25991;&#26723;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach to address the challenges of printed Urdu text recognition using high-resolution, multi-scale semantic feature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model, demonstrates state-of-the-art performance on benchmark datasets. To address the limitations of previous works, which struggle to generalize to the intricacies of the Urdu script and the lack of sufficient annotated real-world data, we have introduced the UTRSet-Real, a large-scale annotated real-world dataset comprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000 lines closely resembling real-world and made corrections to the ground truth of the existing IIITH dataset, making it a more reliable resource for future research. We also provide UrduDoc, a benchmark dataset for Urdu text line detection in scanned documents. Additionally, we have developed an online tool for end-to-end Urdu OCR from printed documents by integrating UTRNet with a tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#19968;&#33268;&#30340;&#29983;&#25104;&#36741;&#21161;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#36755;&#20837;&#19978;&#30340;&#26174;&#33879;&#24615;&#22320;&#22270;&#65292;&#21152;&#36895;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#30456;&#27604;&#22522;&#32447;CycleGAN&#26550;&#26500;&#26377;&#26356;&#39640;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.15760</link><description>&lt;p&gt;
xAI-CycleGAN&#65292;&#19968;&#31181;&#24490;&#29615;&#19968;&#33268;&#30340;&#29983;&#25104;&#36741;&#21161;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
xAI-CycleGAN, a Cycle-Consistent Generative Assistive Network. (arXiv:2306.15760v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#19968;&#33268;&#30340;&#29983;&#25104;&#36741;&#21161;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#36755;&#20837;&#19978;&#30340;&#26174;&#33879;&#24615;&#22320;&#22270;&#65292;&#21152;&#36895;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#30456;&#27604;&#22522;&#32447;CycleGAN&#26550;&#26500;&#26377;&#26356;&#39640;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#29983;&#25104;&#36716;&#25442;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#22270;&#20687;&#36716;&#25442;&#30340;&#39046;&#22495;&#20013;&#65292;CycleGAN&#24050;&#25104;&#20026;&#39318;&#36873;&#26550;&#26500;&#12290;&#36825;&#31181;&#26550;&#26500;&#30340;&#20027;&#35201;&#32570;&#28857;&#20043;&#19968;&#26159;&#20854;&#30456;&#23545;&#36739;&#24930;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#36776;&#21035;&#22120;&#39537;&#21160;&#30340;&#21487;&#35299;&#37322;&#24615;&#26469;&#21152;&#36895;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#36776;&#21035;&#22120;&#30340;&#26174;&#33879;&#24615;&#22320;&#22270;&#26469;&#36974;&#34109;&#29983;&#25104;&#22120;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#65292;&#22522;&#20110;Nagisetty&#31561;&#20154;&#30340;&#24037;&#20316;&#65292;&#24182;&#24341;&#20837;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#25513;&#34109;&#30340;&#36755;&#20837;&#19978;&#30340;&#26174;&#33879;&#24615;&#22320;&#22270;&#65292;&#20351;&#29992;&#22522;&#20110;Wang M.&#30340;Mask CycleGAN&#30340;&#21487;&#35299;&#37322;&#30340;&#28508;&#21464;&#37327;&#12290;&#36825;&#20801;&#35768;&#22312;&#20004;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#34701;&#21512;&#65292;&#24182;&#21033;&#29992;&#28155;&#21152;&#22122;&#22768;&#30340;&#36755;&#20837;&#19978;&#30340;&#26174;&#33879;&#24615;&#22320;&#22270;&#20316;&#20026;&#22522;&#20110;&#35777;&#25454;&#30340;&#23545;&#29031;&#36807;&#28388;&#12290;&#36825;&#31181;&#26032;&#30340;&#26550;&#26500;&#27604;&#22522;&#32447;CycleGAN&#26550;&#26500;&#20855;&#26377;&#26356;&#39640;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of unsupervised image-to-image transformation using generative transformative models, CycleGAN has become the architecture of choice. One of the primary downsides of this architecture is its relatively slow rate of convergence. In this work, we use discriminator-driven explainability to speed up the convergence rate of the generative model by using saliency maps from the discriminator that mask the gradients of the generator during backpropagation, based on the work of Nagisetty et al., and also introducing the saliency map on input, added onto a Gaussian noise mask, by using an interpretable latent variable based on Wang M.'s Mask CycleGAN. This allows for an explainability fusion in both directions, and utilizing the noise-added saliency map on input as evidence-based counterfactual filtering. This new architecture has much higher rate of convergence than a baseline CycleGAN architecture while preserving the image quality.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.15749</link><description>&lt;p&gt;
&#20309;&#21435;&#20309;&#20174;&#65306;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#30340;&#25968;&#23383;&#30828;&#20214;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15749
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#22312;&#28085;&#30422;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#31454;&#20105;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#25928;&#29575;&#20026;&#20195;&#20215;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#33021;&#21147;&#12290;&#29983;&#29289;&#33041;&#30340;&#21151;&#32791;&#25928;&#29575;&#36229;&#36807;&#20219;&#20309;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65307;&#22240;&#27492;&#65292;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#35797;&#22270;&#27169;&#20223;&#33041;&#37096;&#25805;&#20316;&#65292;&#20363;&#22914;&#22522;&#20110;&#33033;&#20914;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;DL&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#33041;&#37096;&#26377;&#35832;&#22914;&#39640;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#12289;&#23494;&#38598;&#30340;&#31070;&#32463;&#20803;&#36830;&#25509;&#21644;&#35745;&#31639;&#19982;&#23384;&#20648;&#30340;&#20849;&#21516;&#20301;&#32622;&#31561;&#20248;&#21183;&#65292;&#20294;&#21487;&#29992;&#30340;&#29983;&#29289;&#22522;&#24213;&#20005;&#37325;&#38480;&#21046;&#20102;&#29983;&#29289;&#22823;&#33041;&#30340;&#36827;&#21270;&#12290;&#30005;&#23376;&#30828;&#20214;&#27809;&#26377;&#30456;&#21516;&#30340;&#32422;&#26463;&#65307;&#22240;&#27492;&#65292;&#34429;&#28982;&#24314;&#27169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21487;&#33021;&#25581;&#31034;&#20102;&#19968;&#20010;&#35868;&#39064;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;AI&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#21147;&#27874;&#25506;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28151;&#21512;&#33192;&#32960;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#24341;&#21147;&#27874;&#20449;&#21495;&#30340;&#26102;&#31354;&#20851;&#32852;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#35757;&#32451;&#22810;&#20010;AI&#27169;&#22411;&#65292;&#24182;&#22312;&#30701;&#26102;&#38388;&#20869;&#33719;&#24471;&#26368;&#20339;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15728</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;AI&#38598;&#21512;&#29992;&#20110;&#24341;&#21147;&#27874;&#25506;&#27979;
&lt;/p&gt;
&lt;p&gt;
Physics-inspired spatiotemporal-graph AI ensemble for gravitational wave detection. (arXiv:2306.15728v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;AI&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#21147;&#27874;&#25506;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#28151;&#21512;&#33192;&#32960;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#24341;&#21147;&#27874;&#20449;&#21495;&#30340;&#26102;&#31354;&#20851;&#32852;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#35757;&#32451;&#22810;&#20010;AI&#27169;&#22411;&#65292;&#24182;&#22312;&#30701;&#26102;&#38388;&#20869;&#33719;&#24471;&#26368;&#20339;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25506;&#27979;&#24341;&#21147;&#27874;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#65306;1&#65289;&#28151;&#21512;&#33192;&#32960;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#24341;&#21147;&#27874;&#20449;&#21495;&#30340;&#30701;&#26102;&#21644;&#38271;&#26102;&#24207;&#21015;&#20449;&#24687;&#65307;&#20197;&#21450;2&#65289;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25429;&#25417;&#24341;&#21147;&#27874;&#22825;&#25991;&#35266;&#27979;&#31449;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#32852;&#65292;&#20197;&#19968;&#33268;&#22320;&#25551;&#36848;&#21644;&#35782;&#21035;&#25506;&#27979;&#22120;&#32593;&#32476;&#20013;&#30340;&#20449;&#21495;&#23384;&#22312;&#12290;&#36825;&#20123;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;AI&#27169;&#22411;&#32463;&#36807;&#27979;&#35797;&#65292;&#29992;&#20110;&#25506;&#27979;&#36817;&#22278;&#38750;&#33258;&#26059;&#21644;&#36817;&#22278;&#33258;&#26059;&#38750;&#36827;&#21160;&#30340;&#20108;&#36827;&#21046;&#40657;&#27934;&#21512;&#24182;&#20135;&#29983;&#30340;&#24341;&#21147;&#27874;&#20449;&#21495;&#12290;&#23545;&#20110;&#21518;&#19968;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#21253;&#21547;120&#19975;&#20010;&#27169;&#25311;&#27874;&#24418;&#30340;&#25968;&#25454;&#38598;&#26469;&#23494;&#38598;&#37319;&#26679;&#36825;&#20010;&#20449;&#21495;&#27969;&#24418;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#38463;&#36129;&#22269;&#23478;&#23454;&#39564;&#23460;&#40857;&#22836;&#36229;&#32423;&#35745;&#31639;&#26426;Polaris&#19978;&#20351;&#29992;256&#20010;NVIDIA A100 GPU&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#22312;1.7&#23567;&#26102;&#20869;&#23454;&#29616;&#20102;&#35757;&#32451;&#26102;&#38388;&#21040;&#35299;&#20915;&#26041;&#26696;&#30340;&#20943;&#23567;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel method for gravitational wave detection that combines: 1) hybrid dilated convolution neural networks to accurately model both shortand long-range temporal sequential information of gravitational wave signals; and 2) graph neural networks to capture spatial correlations among gravitational wave observatories to consistently describe and identify the presence of a signal in a detector network. These spatiotemporal-graph AI models are tested for signal detection of gravitational waves emitted by quasi-circular, non-spinning and quasi-circular, spinning, non-precessing binary black hole mergers. For the latter case, we needed a dataset of 1.2 million modeled waveforms to densely sample this signal manifold. Thus, we reduced time-to-solution by training several AI models in the Polaris supercomputer at the Argonne Leadership Supercomputing Facility within 1.7 hours by distributing the training over 256 NVIDIA A100 GPUs, achieving optimal classification performance. Th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;REFLECT&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22833;&#36133;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.15724</link><description>&lt;p&gt;
REFLECT:&#23545;&#26426;&#22120;&#20154;&#32463;&#21382;&#36827;&#34892;&#24635;&#32467;&#65292;&#20197;&#29992;&#20110;&#22833;&#36133;&#35299;&#37322;&#21644;&#32416;&#27491;
&lt;/p&gt;
&lt;p&gt;
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;REFLECT&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22833;&#36133;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#22833;&#36133;&#25191;&#34892;&#26159;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#31283;&#20581;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#36755;&#20837;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#21033;&#29992;LLM&#30340;&#21147;&#37327;&#36827;&#34892;&#26426;&#22120;&#20154;&#22833;&#36133;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;REFLECT&#65292;&#23558;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#36807;&#21435;&#32463;&#39564;&#30340;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#36880;&#27493;&#22833;&#36133;&#35299;&#37322;&#31639;&#27861;&#26597;&#35810;LLM&#12290;&#22522;&#20110;&#35299;&#37322;&#65292;&#22833;&#36133;&#32416;&#27491;&#35268;&#21010;&#22120;&#29983;&#25104;&#19968;&#20010;&#21487;&#25191;&#34892;&#35745;&#21010;&#65292;&#20197;&#32416;&#27491;&#22833;&#36133;&#24182;&#23436;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;RoboFail&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#20174;&#32780;&#24110;&#21161;&#25104;&#21151;&#30340;&#32416;&#27491;&#35268;&#21010;&#12290;&#39033;&#30446;&#32593;&#31449;&#65306;https://roboreflect.github.io/
&lt;/p&gt;
&lt;p&gt;
The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong common sense reasoning skills on textual inputs. To leverage the power of LLM for robot failure explanation, we propose a framework REFLECT, which converts multi-sensory data into a hierarchical summary of robot past experiences and queries LLM with a progressive failure explanation algorithm. Conditioned on the explanation, a failure correction planner generates an executable plan for the robot to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset and show that our LLM-based framework is able to generate informative failure explanations that assist successful correction planning. Project website: https://roboreflect.github.io/
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20849;&#20139;&#30340;&#20004;&#20010;&#25110;&#22810;&#20010;&#36755;&#20837;&#27169;&#24577;&#30340;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#21322;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;&#36825;&#31181;&#26550;&#26500;&#21487;&#20197;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#65292;&#24182;&#27169;&#20223;&#20154;&#31867;&#20174;&#26377;&#38480;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.15711</link><description>&lt;p&gt;
&#36890;&#36807;&#20840;&#23616;&#24037;&#20316;&#21306;&#30340;&#21322;&#30417;&#30563;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Multimodal Representation Learning through a Global Workspace. (arXiv:2306.15711v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20849;&#20139;&#30340;&#20004;&#20010;&#25110;&#22810;&#20010;&#36755;&#20837;&#27169;&#24577;&#30340;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#21322;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;&#36825;&#31181;&#26550;&#26500;&#21487;&#20197;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#65292;&#24182;&#27169;&#20223;&#20154;&#31867;&#20174;&#26377;&#38480;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#32452;&#21512;&#19981;&#21516;&#30340;&#36755;&#20837;&#27169;&#24577;&#65288;&#20363;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#24182;&#23398;&#20064;&#23545;&#20854;&#28508;&#22312;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#65292;&#25110;&#32773;&#23558;&#19968;&#20010;&#39046;&#22495;&#30340;&#20449;&#21495;&#36716;&#21270;&#20026;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#20449;&#21495;&#65288;&#20363;&#22914;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#25110;&#32773;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#26292;&#21147;&#30417;&#30563;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#65288;&#21644;&#20854;&#20182;&#21160;&#29289;&#65289;&#21487;&#20197;&#36890;&#36807;&#21305;&#37197;&#30340;&#36328;&#27169;&#24577;&#25968;&#25454;&#30340;&#31232;&#30095;&#32463;&#39564;&#26469;&#23398;&#20064;&#26377;&#29992;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#21463;&#35748;&#30693;&#27010;&#24565;&#8220;&#20840;&#23616;&#24037;&#20316;&#21306;&#8221;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#33021;&#21147;&#65306;&#19968;&#20010;&#20849;&#20139;&#30340;&#20004;&#20010;&#65288;&#25110;&#22810;&#20010;&#65289;&#36755;&#20837;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;&#27599;&#20010;&#27169;&#24577;&#37117;&#32463;&#36807;&#19968;&#20010;&#19987;&#38376;&#30340;&#31995;&#32479;&#22788;&#29702;&#65288;&#22312;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#65292;&#24182;&#38543;&#21518;&#20923;&#32467;&#65289;&#12290;&#30456;&#24212;&#30340;&#28508;&#22312;&#34920;&#31034;&#28982;&#21518;&#34987;&#32534;&#30721;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#24037;&#20316;&#21306;&#24182;&#20174;&#20013;&#35299;&#30721;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#26550;&#26500;&#36866;&#29992;&#20110;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent deep learning models can efficiently combine inputs from different modalities (e.g., images and text) and learn to align their latent representations, or to translate signals from one domain to another (as in image captioning, or text-to-image generation). However, current approaches mainly rely on brute-force supervised training over large multimodal datasets. In contrast, humans (and other animals) can learn useful multimodal representations from only sparse experience with matched cross-modal data. Here we evaluate the capabilities of a neural network architecture inspired by the cognitive notion of a "Global Workspace": a shared representation for two (or more) input modalities. Each modality is processed by a specialized system (pretrained on unimodal data, and subsequently frozen). The corresponding latent representations are then encoded to and decoded from a single shared workspace. Importantly, this architecture is amenable to self-supervised training via cycle-consiste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#39564;&#26041;&#27861;&#65292;&#20351;&#29992;&#21442;&#25968;&#21270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20026;&#30410;&#26234;&#28216;&#25103;Lily's Garden&#29983;&#25104;&#20851;&#21345;&#12290;&#34429;&#28982;GAN&#22312;&#36924;&#36817;&#22320;&#22270;&#24418;&#29366;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#36924;&#36817;&#26041;&#22359;&#20998;&#24067;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21487;&#33021;&#36890;&#36807;&#23581;&#35797;&#26367;&#20195;GAN&#30340;&#26550;&#26500;&#26469;&#25913;&#36827;&#36825;&#19968;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.15696</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#30410;&#26234;&#28216;&#25103;&#30340;&#36807;&#31243;&#21270;&#20869;&#23481;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Procedural content generation of puzzle games using conditional generative adversarial networks. (arXiv:2306.15696v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#39564;&#26041;&#27861;&#65292;&#20351;&#29992;&#21442;&#25968;&#21270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20026;&#30410;&#26234;&#28216;&#25103;Lily's Garden&#29983;&#25104;&#20851;&#21345;&#12290;&#34429;&#28982;GAN&#22312;&#36924;&#36817;&#22320;&#22270;&#24418;&#29366;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#36924;&#36817;&#26041;&#22359;&#20998;&#24067;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21487;&#33021;&#36890;&#36807;&#23581;&#35797;&#26367;&#20195;GAN&#30340;&#26550;&#26500;&#26469;&#25913;&#36827;&#36825;&#19968;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#21270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20026;&#30410;&#26234;&#28216;&#25103;Lily's Garden&#29983;&#25104;&#20851;&#21345;&#30340;&#23454;&#39564;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#30495;&#23454;&#20851;&#21345;&#20013;&#25552;&#21462;&#20004;&#20010;&#26465;&#20214;&#21521;&#37327;&#65292;&#20197;&#25511;&#21046;GAN&#36755;&#20986;&#30340;&#32454;&#33410;&#12290;&#34429;&#28982;GAN&#22312;&#36924;&#36817;&#31532;&#19968;&#20010;&#26465;&#20214;&#65288;&#22320;&#22270;&#24418;&#29366;&#65289;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#36924;&#36817;&#31532;&#20108;&#20010;&#26465;&#20214;&#65288;&#26041;&#22359;&#20998;&#24067;&#65289;&#26041;&#38754;&#21364;&#26377;&#22256;&#38590;&#12290;&#25105;&#20204;&#20551;&#35774;&#36890;&#36807;&#23581;&#35797;&#26367;&#20195;GAN&#30340;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#26550;&#26500;&#21487;&#33021;&#20250;&#25913;&#36827;&#36825;&#19968;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we present an experimental approach to using parameterized Generative Adversarial Networks (GANs) to produce levels for the puzzle game Lily's Garden. We extract two condition vectors from the real levels in an effort to control the details of the GAN's outputs. While the GANs perform well in approximating the first condition (map shape), they struggle to approximate the second condition (piece distribution). We hypothesize that this might be improved by trying out alternative architectures for both the Generator and Discriminator of the GANs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KAPLA&#65292;&#19968;&#20010;&#29992;&#20110;&#21487;&#25193;&#23637;NN&#21152;&#36895;&#22120;&#25968;&#25454;&#27969;&#20248;&#21270;&#30340;&#24555;&#36895;&#27714;&#35299;&#22120;&#12290;&#36890;&#36807;&#23454;&#29992;&#30340;&#25351;&#20196;&#21644;&#20840;&#38754;&#30340;&#25968;&#25454;&#27969;&#34920;&#31034;&#65292;KAPLA&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#35774;&#35745;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#24182;&#24555;&#36895;&#30830;&#23450;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.15676</link><description>&lt;p&gt;
KAPLA&#65306;&#21487;&#25193;&#23637;NN&#21152;&#36895;&#22120;&#25968;&#25454;&#27969;&#30340;&#23454;&#29992;&#21270;&#34920;&#31034;&#21644;&#24555;&#36895;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
KAPLA: Pragmatic Representation and Fast Solving of Scalable NN Accelerator Dataflow. (arXiv:2306.15676v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KAPLA&#65292;&#19968;&#20010;&#29992;&#20110;&#21487;&#25193;&#23637;NN&#21152;&#36895;&#22120;&#25968;&#25454;&#27969;&#20248;&#21270;&#30340;&#24555;&#36895;&#27714;&#35299;&#22120;&#12290;&#36890;&#36807;&#23454;&#29992;&#30340;&#25351;&#20196;&#21644;&#20840;&#38754;&#30340;&#25968;&#25454;&#27969;&#34920;&#31034;&#65292;KAPLA&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#35774;&#35745;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#24182;&#24555;&#36895;&#30830;&#23450;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27969;&#35843;&#24230;&#20915;&#31574;&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21152;&#36895;&#22120;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#21487;&#25193;&#23637;&#30340;NN&#21152;&#36895;&#22120;&#25903;&#25345;&#19968;&#32452;&#20016;&#23500;&#30340;&#20808;&#36827;&#25968;&#25454;&#27969;&#25216;&#26415;&#12290;&#22240;&#27492;&#65292;&#20840;&#38754;&#34920;&#31034;&#21644;&#24555;&#36895;&#25214;&#21040;&#20248;&#21270;&#30340;&#25968;&#25454;&#27969;&#26041;&#26696;&#30340;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#22810;&#33410;&#28857;NN&#26550;&#26500;&#19978;&#26102;&#38388;&#21644;&#31354;&#38388;&#35843;&#24230;&#30340;&#20840;&#38754;&#23454;&#29992;&#21270;&#25968;&#25454;&#27969;&#34920;&#31034;&#12290;&#19968;&#20221;&#38750;&#27491;&#24335;&#30340;&#20998;&#23618;&#20998;&#31867;&#34920;&#26126;&#65292;&#25968;&#25454;&#27969;&#31354;&#38388;&#19981;&#21516;&#23618;&#27425;&#20043;&#38388;&#30340;&#32039;&#23494;&#32806;&#21512;&#26159;&#24555;&#36895;&#35774;&#35745;&#25506;&#32034;&#30340;&#20027;&#35201;&#38590;&#28857;&#12290;&#19968;&#32452;&#24418;&#24335;&#21270;&#30340;&#24352;&#37327;&#20013;&#24515;&#25351;&#20196;&#20934;&#30830;&#22320;&#34920;&#31034;&#21508;&#31181;&#23618;&#38388;&#21644;&#23618;&#20869;&#26041;&#26696;&#65292;&#24182;&#20801;&#35768;&#24555;&#36895;&#30830;&#23450;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#20248;&#21270;&#30340;&#21644;&#24555;&#36895;&#30340;&#25968;&#25454;&#27969;&#27714;&#35299;&#22120;KAPLA&#65292;&#21033;&#29992;&#23454;&#29992;&#30340;&#25351;&#20196;&#26469;&#36827;&#34892;&#35774;&#35745;&#31354;&#38388;&#30340;&#26377;&#25928;&#26377;&#25928;&#24615;&#26816;&#26597;&#21644;&#20248;&#21270;&#26041;&#26696;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataflow scheduling decisions are of vital importance to neural network (NN) accelerators. Recent scalable NN accelerators support a rich set of advanced dataflow techniques. The problems of comprehensively representing and quickly finding optimized dataflow schemes thus become significantly more complicated and challenging. In this work, we first propose comprehensive and pragmatic dataflow representations for temporal and spatial scheduling on scalable multi-node NN architectures. An informal hierarchical taxonomy highlights the tight coupling across different levels of the dataflow space as the major difficulty for fast design exploration. A set of formal tensor-centric directives accurately express various inter-layer and intra-layer schemes, and allow for quickly determining their validity and efficiency. We then build a generic, optimized, and fast dataflow solver, KAPLA, which makes use of the pragmatic directives to explore the design space with effective validity check and eff
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#20998;&#31163;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24322;&#27493;&#35745;&#31639;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#22522;&#30784;&#65292;&#36827;&#34892;&#20102;&#24322;&#27493;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2306.15632</link><description>&lt;p&gt;
&#24322;&#27493;&#31639;&#27861;&#19982;Cocycles&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Algorithmic Alignment with Cocycles. (arXiv:2306.15632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#20998;&#31163;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24322;&#27493;&#35745;&#31639;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#22522;&#30784;&#65292;&#36827;&#34892;&#20102;&#24322;&#27493;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#12290;&#20294;&#26159;&#65292;&#20856;&#22411;&#30340;GNN&#22312;&#23450;&#20041;&#21644;&#35843;&#29992;&#28040;&#24687;&#20989;&#25968;&#20043;&#38388;&#27169;&#31946;&#20102;&#21306;&#21035;&#65292;&#36843;&#20351;&#33410;&#28857;&#22312;&#27599;&#19968;&#23618;&#37117;&#21521;&#20854;&#37051;&#23621;&#21457;&#36865;&#28040;&#24687;&#65292;&#21516;&#27493;&#22320;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;GNN&#24212;&#29992;&#20110;&#23398;&#20064;&#25191;&#34892;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26102;&#65292;&#22823;&#22810;&#25968;&#27493;&#39588;&#21482;&#26377;&#23569;&#25968;&#20960;&#20010;&#33410;&#28857;&#20250;&#26377;&#26377;&#24847;&#20041;&#30340;&#26356;&#26032;&#35201;&#21457;&#36865;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#22312;&#22270;&#20013;&#21457;&#36865;&#22826;&#22810;&#26080;&#20851;&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#23548;&#33268;&#20302;&#25928;&#29575;&#65292;&#32780;&#35768;&#22810;&#20013;&#38388;&#30340;GNN&#27493;&#39588;&#24517;&#39035;&#23398;&#20064;&#36523;&#20221;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#20998;&#31163;&#20102;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#36825;&#31181;&#20998;&#31163;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#25968;&#23398;&#34920;&#36798;&#65292;&#21487;&#20197;&#35753;&#25105;&#20204;&#24605;&#32771;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24322;&#27493;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art neural algorithmic reasoners make use of message passing in graph neural networks (GNNs). But typical GNNs blur the distinction between the definition and invocation of the message function, forcing a node to send messages to its neighbours at every layer, synchronously. When applying GNNs to learn to execute dynamic programming algorithms, however, on most steps only a handful of the nodes would have meaningful updates to send. One, hence, runs the risk of inefficiencies by sending too much irrelevant data across the graph -- with many intermediate GNN steps having to learn identity functions. In this work, we explicitly separate the concepts of node state update and message function invocation. With this separation, we obtain a mathematical formulation that allows us to reason about asynchronous computation in both algorithms and neural networks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#25105;&#20204;&#20445;&#25345;&#20102;&#25193;&#23637;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#20219;&#21153;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15595</link><description>&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;
&lt;/p&gt;
&lt;p&gt;
Extending Context Window of Large Language Models via Positional Interpolation. (arXiv:2306.15595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15595
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#25105;&#20204;&#20445;&#25345;&#20102;&#25193;&#23637;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#20219;&#21153;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20301;&#32622;&#25554;&#20540;&#65288;PI&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;LLaMA&#27169;&#22411;&#65289;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#19988;&#22312;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#30340;&#21508;&#31181;&#20219;&#21153;&#65288;&#21253;&#25324;&#23494;&#38053;&#26816;&#32034;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#31687;&#25991;&#26723;&#25688;&#35201;&#31561;&#65289;&#19978;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#25193;&#23637;&#30340;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#30340;&#20219;&#21153;&#20013;&#30456;&#23545;&#20445;&#25345;&#33391;&#22909;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20301;&#32622;&#25554;&#20540;&#32447;&#24615;&#22320;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#20197;&#21305;&#37197;&#21407;&#22987;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#65292;&#32780;&#19981;&#26159;&#36229;&#36807;&#35757;&#32451;&#26102;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#39640;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#23436;&#20840;&#30772;&#22351;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#25554;&#20540;&#30340;&#19978;&#30028;&#33267;&#23569;&#26159;&#25512;&#26029;&#30340;&#19978;&#30028;&#30340;$\sim 600 \times$&#35201;&#23567;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least $\sim 600 \times$ smaller than that of extrapolation, further demonstrating its stability. Models extend
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;&#21069;&#20307;-&#24322;&#24120;&#26816;&#27979;&#65288;PoA&#26816;&#27979;&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#19981;&#21516;&#65292;PoA&#26816;&#27979;&#26088;&#22312;&#22312;&#24322;&#24120;&#21457;&#29983;&#20043;&#21069;&#26816;&#27979;&#21040;&#26410;&#26469;&#30340;&#24322;&#24120;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;17&#20010;&#22522;&#20934;&#32447;&#21644;3&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15489</link><description>&lt;p&gt;
&#38024;&#23545;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#21069;&#20307;
&lt;/p&gt;
&lt;p&gt;
Precursor-of-Anomaly Detection for Irregular Time Series. (arXiv:2306.15489v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;&#21069;&#20307;-&#24322;&#24120;&#26816;&#27979;&#65288;PoA&#26816;&#27979;&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#19981;&#21516;&#65292;PoA&#26816;&#27979;&#26088;&#22312;&#22312;&#24322;&#24120;&#21457;&#29983;&#20043;&#21069;&#26816;&#27979;&#21040;&#26410;&#26469;&#30340;&#24322;&#24120;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;17&#20010;&#22522;&#20934;&#32447;&#21644;3&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#26088;&#22312;&#35782;&#21035;&#24847;&#22806;&#30340;&#27169;&#24335;&#25110;&#25968;&#25454;&#28857;&#65292;&#24182;&#19982;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#23494;&#20999;&#30456;&#20851;&#65292;&#23588;&#20854;&#26159;&#22312;&#37329;&#34701;&#12289;&#21046;&#36896;&#12289;&#32593;&#32476;&#23433;&#20840;&#31561;&#24212;&#29992;&#20013;&#12290;&#34429;&#28982;&#24322;&#24120;&#26816;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#24322;&#24120;&#21457;&#29983;&#20043;&#21069;&#26816;&#27979;&#21040;&#26410;&#26469;&#30340;&#24322;&#24120;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#21069;&#20307;-&#24322;&#24120;&#8221;&#65288;PoA&#65289;&#26816;&#27979;&#12290;&#19982;&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#19981;&#21516;&#65292;&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#20391;&#37325;&#20110;&#30830;&#23450;&#32473;&#23450;&#26102;&#38388;&#24207;&#21015;&#35266;&#27979;&#20540;&#26159;&#21542;&#20026;&#24322;&#24120;&#65292;&#32780;PoA&#26816;&#27979;&#26088;&#22312;&#22312;&#24322;&#24120;&#21457;&#29983;&#20043;&#21069;&#26816;&#27979;&#21040;&#26410;&#26469;&#30340;&#24322;&#24120;&#12290;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#22810;&#20219;&#21153;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;17&#20010;&#22522;&#20934;&#32447;&#21644;3&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#35268;&#21017;&#21644;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is an important field that aims to identify unexpected patterns or data points, and it is closely related to many real-world problems, particularly to applications in finance, manufacturing, cyber security, and so on. While anomaly detection has been studied extensively in various fields, detecting future anomalies before they occur remains an unexplored territory. In this paper, we present a novel type of anomaly detection, called \emph{\textbf{P}recursor-of-\textbf{A}nomaly} (PoA) detection. Unlike conventional anomaly detection, which focuses on determining whether a given time series observation is an anomaly or not, PoA detection aims to detect future anomalies before they happen. To solve both problems at the same time, we present a neural controlled differential equation-based neural network and its multi-task learning algorithm. We conduct experiments using 17 baselines and 3 datasets, including regular and irregular time series, and demonstrate that our prese
&lt;/p&gt;</description></item><item><title>MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15128</link><description>&lt;p&gt;
MIMIC: &#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MIMIC: Masked Image Modeling with Image Correspondences. (arXiv:2306.15128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15128
&lt;/p&gt;
&lt;p&gt;
MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20687;&#32032;&#32423;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#8212;&#8212;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#8212;&#8212;&#22914;&#20170;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#31579;&#36873;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20165;&#36890;&#36807;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#24102;&#26377;&#27880;&#37322;&#30340;3D&#32593;&#26684;&#12289;&#28857;&#20113;&#21644;&#30456;&#26426;&#21442;&#25968;&#31579;&#36873;&#32780;&#26469;&#65292;&#24182;&#19981;&#20855;&#22791;&#22810;&#35270;&#35282;&#22330;&#26223;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#31579;&#36873;&#26426;&#21046;&#12290;&#25105;&#20204;&#20174;&#24320;&#28304;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#30340;3D&#29615;&#22659;&#20013;&#25366;&#25496;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;MIMIC-1M(&#21253;&#21547;1.3M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#21644;MIMIC-3M(&#21253;&#21547;3.1M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#30446;&#26631;&#65292;&#23637;&#31034;&#20102;&#20197;&#19979;&#21457;&#29616;&#65306;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;MIMIC-3M&#35757;&#32451;&#30340;&#34920;&#31034;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#34920;&#38754;&#27861;&#32447;&#21644;&#23039;&#24577;&#20272;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many pixelwise dense prediction tasks-depth estimation and semantic segmentation in computer vision today rely on pretrained image representations. Therefore, curating effective pretraining datasets is vital. Unfortunately, the effective pretraining datasets are those with multi-view scenes and have only been curated using annotated 3D meshes, point clouds, and camera parameters from simulated environments. We propose a dataset-curation mechanism that does not require any annotations. We mine two datasets: MIMIC-1M with 1.3M and MIMIC-3M with 3.1M multi-view image pairs from open-sourced video datasets and from synthetic 3D environments. We train multiple self-supervised models with different masked image modeling objectives to showcase the following findings: Representations trained on MIMIC-3M outperform those mined using annotations on multiple downstream tasks, including depth estimation, semantic segmentation, surface normals, and pose estimation. They also outperform representati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#20197;Transformer&#20026;&#22522;&#30784;&#27169;&#22411;&#24182;&#32467;&#21512;&#35760;&#24518;&#65292;&#25193;&#23637;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#21644;&#31354;&#38388;&#27880;&#24847;&#21147;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#19982;ResNet50&#30456;&#32467;&#21512;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#30340;&#35748;&#30693;&#26550;&#26500;GAMR&#65292;&#23427;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#32452;&#21512;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26550;&#26500;&#65292;&#24182;&#20855;&#26377;&#23545;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.14650</link><description>&lt;p&gt;
&#21338;&#22763;&#35770;&#25991;&#65306;&#25506;&#32034;&#35748;&#30693;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26550;&#26500;&#20013;&#30340;(&#33258;&#25105;)&#27880;&#24847;&#21147;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
PhD Thesis: Exploring the role of (self-)attention in cognitive and computer vision architecture. (arXiv:2306.14650v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14650
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#20197;Transformer&#20026;&#22522;&#30784;&#27169;&#22411;&#24182;&#32467;&#21512;&#35760;&#24518;&#65292;&#25193;&#23637;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#21644;&#31354;&#38388;&#27880;&#24847;&#21147;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#19982;ResNet50&#30456;&#32467;&#21512;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#30340;&#35748;&#30693;&#26550;&#26500;GAMR&#65292;&#23427;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#32452;&#21512;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26550;&#26500;&#65292;&#24182;&#20855;&#26377;&#23545;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#22522;&#20110;Transformer&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#27169;&#22411;&#24182;&#23558;&#20854;&#19982;&#35760;&#24518;&#30456;&#32467;&#21512;&#26469;&#25193;&#23637;&#23427;&#12290;&#36890;&#36807;&#30740;&#31350;&#21512;&#25104;&#35270;&#35273;&#25512;&#29702;&#27979;&#35797;&#65292;&#25105;&#20204;&#23436;&#21892;&#20102;&#25512;&#29702;&#20219;&#21153;&#30340;&#20998;&#31867;&#27861;&#12290;&#36890;&#36807;&#23558;&#33258;&#25105;&#27880;&#24847;&#21147;&#19982;ResNet50&#32467;&#21512;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#21644;&#31354;&#38388;&#27880;&#24847;&#21147;&#22686;&#24378;&#29305;&#24449;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#30340;&#39640;&#25928;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#29702;&#35299;SVRT&#20219;&#21153;&#23545;&#27880;&#24847;&#21147;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GAMR&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#28789;&#24863;&#26469;&#33258;&#20027;&#21160;&#35270;&#35273;&#29702;&#35770;&#12290;GAMR&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#32452;&#21512;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26550;&#26500;&#65292;&#24182;&#22312;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the role of attention and memory in complex reasoning tasks. We analyze Transformer-based self-attention as a model and extend it with memory. By studying a synthetic visual reasoning test, we refine the taxonomy of reasoning tasks. Incorporating self-attention with ResNet50, we enhance feature maps using feature-based and spatial attention, achieving efficient solving of challenging visual reasoning tasks. Our findings contribute to understanding the attentional needs of SVRT tasks. Additionally, we propose GAMR, a cognitive architecture combining attention and memory, inspired by active vision theory. GAMR outperforms other architectures in sample efficiency, robustness, and compositionality, and shows zero-shot generalization on new reasoning tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#35821;&#35328;&#36755;&#20837;&#20013;&#36827;&#34892;&#30446;&#26631;&#25512;&#26029;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.14325</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21453;&#21521;&#35268;&#21010;&#24341;&#25806;&#65288;NIPE&#65289;&#65306;&#22522;&#20110;&#35821;&#35328;&#36755;&#20837;&#30340;&#27010;&#29575;&#31038;&#20132;&#25512;&#29702;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
The Neuro-Symbolic Inverse Planning Engine (NIPE): Modeling Probabilistic Social Inferences from Linguistic Inputs. (arXiv:2306.14325v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#35821;&#35328;&#36755;&#20837;&#20013;&#36827;&#34892;&#30446;&#26631;&#25512;&#26029;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26159;&#31038;&#20132;&#24615;&#30340;&#29983;&#29289;&#12290;&#25105;&#20204;&#32463;&#24120;&#25512;&#29702;&#20854;&#20182;&#26234;&#33021;&#20307;&#65292;&#32780;&#36825;&#31181;&#31038;&#20132;&#25512;&#29702;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#22312;&#20102;&#35299;&#20182;&#20204;&#30340;&#34892;&#20026;&#26102;&#25512;&#26029;&#20182;&#20204;&#30340;&#30446;&#26631;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#35821;&#35328;&#25551;&#36848;&#30340;&#26234;&#33021;&#20307;&#12289;&#21160;&#20316;&#21644;&#32972;&#26223;&#29615;&#22659;&#20013;&#36827;&#34892;&#30452;&#35266;&#20294;&#21487;&#38752;&#30340;&#30446;&#26631;&#25512;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#27010;&#29575;&#30446;&#26631;&#25512;&#26029;&#39046;&#22495;&#20013;&#65292;&#35821;&#35328;&#39537;&#21160;&#21644;&#24433;&#21709;&#31038;&#20132;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20174;&#26234;&#33021;&#20307;&#22330;&#26223;&#30340;&#35821;&#35328;&#36755;&#20837;&#20013;&#36827;&#34892;&#30446;&#26631;&#25512;&#26029;&#12290;&#20854;&#20013;&#30340;&#8220;&#31070;&#32463;&#8221;&#37096;&#20998;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23558;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#20195;&#30721;&#34920;&#31034;&#65292;&#32780;&#8220;&#31526;&#21495;&#8221;&#37096;&#20998;&#21017;&#26159;&#19968;&#20010;&#36125;&#21494;&#26031;&#21453;&#21521;&#35268;&#21010;&#24341;&#25806;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#36827;&#34892;&#20102;&#19968;&#20010;&#20851;&#20110;&#35821;&#35328;&#30446;&#26631;&#25512;&#26029;&#20219;&#21153;&#30340;&#20154;&#31867;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#21453;&#24212;&#27169;&#24335;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#19988;&#27604;&#21333;&#29420;&#20351;&#29992;LLM&#26356;&#22909;&#22320;&#39044;&#27979;&#20102;&#20154;&#31867;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human beings are social creatures. We routinely reason about other agents, and a crucial component of this social reasoning is inferring people's goals as we learn about their actions. In many settings, we can perform intuitive but reliable goal inference from language descriptions of agents, actions, and the background environments. In this paper, we study this process of language driving and influencing social reasoning in a probabilistic goal inference domain. We propose a neuro-symbolic model that carries out goal inference from linguistic inputs of agent scenarios. The "neuro" part is a large language model (LLM) that translates language descriptions to code representations, and the "symbolic" part is a Bayesian inverse planning engine. To test our model, we design and run a human experiment on a linguistic goal inference task. Our model closely matches human response patterns and better predicts human judgements than using an LLM alone.
&lt;/p&gt;</description></item><item><title>G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.11667</link><description>&lt;p&gt;
G-NM&#65306;&#19968;&#32452;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-NM: A Group of Numerical Time Series Prediction Models. (arXiv:2306.11667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11667
&lt;/p&gt;
&lt;p&gt;
G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24320;&#21457;&#21644;&#23454;&#26045;&#19968;&#20010;&#32508;&#21512;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38598;&#21512;&#65292;&#32479;&#31216;&#20026;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#32452;&#65288;G-NM&#65289;&#12290;&#35813;&#38598;&#21512;&#21253;&#25324;&#20256;&#32479;&#27169;&#22411;&#22914;&#33258;&#22238;&#24402;&#32508;&#21512;&#31227;&#21160;&#24179;&#22343;&#65288;ARIMA&#65289;&#12289;Holt-Winters&#26041;&#27861;&#21644;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#65292;&#20197;&#21450;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12290;G-NM&#26126;&#30830;&#26500;&#24314;&#20197;&#22686;&#24378;&#25105;&#20204;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#22266;&#26377;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#19982;&#36825;&#20123;&#20107;&#20214;&#30456;&#20851;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;G-NM&#20415;&#20110;&#23545;&#27492;&#31867;&#29616;&#35937;&#22312;&#24310;&#38271;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25512;&#36827;&#25105;&#20204;&#23545;&#27492;&#31867;&#20107;&#20214;&#30340;&#29702;&#35299;&#65292;&#24182;&#22823;&#24133;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;G-NM&#21253;&#25324;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21450;&#23395;&#33410;&#24615;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we focus on the development and implementation of a comprehensive ensemble of numerical time series forecasting models, collectively referred to as the Group of Numerical Time Series Prediction Model (G-NM). This inclusive set comprises traditional models such as Autoregressive Integrated Moving Average (ARIMA), Holt-Winters' method, and Support Vector Regression (SVR), in addition to modern neural network models including Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM is explicitly constructed to augment our predictive capabilities related to patterns and trends inherent in complex natural phenomena. By utilizing time series data relevant to these events, G-NM facilitates the prediction of such phenomena over extended periods. The primary objective of this research is to both advance our understanding of such occurrences and to significantly enhance the accuracy of our forecasts. G-NM encapsulates both linear and non-linear dependencies, seasonal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10946</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network. (arXiv:2306.10946v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#33616;&#31639;&#27861;&#22312;&#30456;&#23545;&#25104;&#29087;&#38454;&#27573;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25512;&#33616;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#20363;&#22914;&#22312;&#26053;&#28216;&#39046;&#22495;&#65292;&#36873;&#25321;&#36866;&#21512;&#30340;&#26053;&#28216;&#26223;&#28857;&#23646;&#24615;&#27969;&#31243;&#20316;&#20026;&#25512;&#33616;&#22522;&#30784;&#36739;&#20026;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#25913;&#36827;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;(Att-KGCN)&#65292;&#33258;&#21160;&#35821;&#20041;&#22320;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#23558;&#30456;&#23545;&#30456;&#20284;&#30340;&#20301;&#32622;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#36890;&#36807;&#25512;&#29702;&#26053;&#23458;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#12290;&#23454;&#39564;&#20013;&#65292;&#37319;&#29992;&#32034;&#31185;&#29305;&#25289;&#23707;-&#20063;&#38376;&#30340;&#26053;&#28216;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#26053;&#28216;&#39046;&#22495;&#30340;&#26223;&#28857;&#25512;&#33616;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation algorithm based on knowledge graphs is at a relatively mature stage. However, there are still some problems in the recommendation of specific areas. For example, in the tourism field, selecting suitable tourist attraction attributes process is complicated as the recommendation basis for tourist attractions. In this paper, we propose the improved Attention Knowledge Graph Convolution Network model, named (Att-KGCN), which automatically discovers the neighboring entities of the target scenic spot semantically. The attention layer aggregates relatively similar locations and represents them with an adjacent vector. Then, according to the tourist's preferred choices, the model predicts the probability of similar spots as a recommendation system. A knowledge graph dataset of tourist attractions used based on tourism data on Socotra Island-Yemen. Through experiments, it is verified that the Attention Knowledge Graph Convolution Network has a good effect on the recommendatio
&lt;/p&gt;</description></item><item><title>&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2306.09633</link><description>&lt;p&gt;
&#34394;&#20551;&#40654;&#26126;&#65306;&#37325;&#26032;&#35780;&#20272;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09633
&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#26377;&#20851;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#33455;&#29255;&#30340;&#35770;&#25991;&#65292;&#22240;&#20026;&#25152;&#22768;&#31216;&#30340;&#32467;&#26524;&#32570;&#20047;&#20805;&#20998;&#30340;&#25991;&#20214;&#35760;&#24405;&#21644;&#20851;&#38190;&#27493;&#39588;&#30340;&#35828;&#26126;&#65292;&#24341;&#21457;&#20105;&#35758;&#24182;&#21463;&#21040;&#23186;&#20307;&#30340;&#25209;&#35780;&#25253;&#36947;&#12290; &#32780;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#22635;&#34917;&#20102;&#31354;&#30333;&#65292;&#35777;&#26126;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#33853;&#21518;&#20110;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#33853;&#21518;&#20110;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#36824;&#33853;&#21518;&#20110;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#12290;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#34892;&#20026;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#12298;&#33258;&#28982;&#12299;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09364</link><description>&lt;p&gt;
TSMixer: &#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09364
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22240;&#20854;&#33021;&#22815;&#25429;&#25417;&#38271;&#24207;&#21015;&#20132;&#20114;&#32780;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#20854;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#38382;&#39064;&#23545;&#38271;&#26399;&#39044;&#27979;&#26500;&#25104;&#20102;&#20005;&#37325;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TSMixer&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#26550;&#26500;&#65292;&#19987;&#20026;&#22810;&#20803;&#39044;&#27979;&#21644;&#34917;&#19969;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#32780;&#35774;&#35745;&#65292;&#26159;Transformers&#30340;&#26377;&#25928;&#26367;&#20195;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20511;&#37492;&#20102;MLP-Mixer&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25104;&#21151;&#32463;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#35270;&#35273;MLP-Mixer&#36866;&#24212;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25361;&#25112;&#65292;&#24182;&#24341;&#20837;&#20102;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#32452;&#20214;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#36825;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#21363;&#23558;&#22312;&#32447;&#21327;&#35843;&#22836;&#38468;&#21152;&#21040;MLP-Mixer&#39592;&#24178;&#19978;&#65292;&#20197;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#23646;&#24615;&#65292;&#22914;&#23618;&#27425;&#32467;&#26500;&#21644;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36890;&#36947;&#24314;&#27169;&#26041;&#27861;&#65292;&#24179;&#34913;&#20102;&#32534;&#30721;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#36890;&#36947;&#21644;&#20445;&#30041;&#21333;&#20010;&#36890;&#36947;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TSMixer&#22312;&#19968;&#20803;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#27604;&#22522;&#20110;Transformers&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#32599;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#21453;&#39539;&#29468;&#24819;&#24182;&#35777;&#26126;&#22270;&#35770;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22312;&#21453;&#39539;&#22810;&#20010;&#29468;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20248;&#20110;&#24050;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07956</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#32599;&#25628;&#32034;&#29992;&#20110;&#22270;&#35770;&#29468;&#24819;&#35777;&#20266;
&lt;/p&gt;
&lt;p&gt;
Adaptive Monte Carlo Search for Conjecture Refutation in Graph Theory. (arXiv:2306.07956v1 [math.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#32599;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#21453;&#39539;&#29468;&#24819;&#24182;&#35777;&#26126;&#22270;&#35770;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22312;&#21453;&#39539;&#22810;&#20010;&#29468;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20248;&#20110;&#24050;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#35770;&#26159;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20855;&#26377;&#22312;&#25968;&#23398;&#24314;&#27169;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22270;&#35770;&#30740;&#31350;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#23450;&#29702;&#30340;&#21019;&#36896;&#65292;&#36824;&#28041;&#21450;&#21040;&#29468;&#24819;&#30340;&#25552;&#20986;&#12290;&#35777;&#20266;&#31639;&#27861;&#36890;&#36807;&#22312;&#22270;&#19978;&#26368;&#22823;&#21270;&#26576;&#20123;&#24471;&#20998;&#20989;&#25968;&#26469;&#23547;&#25214;&#21453;&#20363;&#65292;&#20174;&#32780;&#35797;&#22270;&#35777;&#20266;&#29468;&#24819;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29468;&#24819;&#35777;&#20266;&#31639;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#32599;&#25628;&#32034;&#65288;AMCS&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#24471;&#21040;&#12290;&#36890;&#36807;&#23545;&#20854;&#22312;&#21457;&#29616;&#25968;&#20010;&#22270;&#35770;&#29468;&#24819;&#30340;&#21453;&#20363;&#26041;&#38754;&#30340;&#25104;&#21151;&#35780;&#20272;&#65292;AMCS&#20248;&#20110;&#29616;&#26377;&#30340;&#29468;&#24819;&#35777;&#20266;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36824;&#34987;&#29992;&#20110;&#35777;&#20266;&#20102;&#20845;&#20010;&#24320;&#25918;&#29468;&#24819;&#65292;&#20854;&#20013;&#20004;&#20010;&#26159;&#30001;Liu&#31561;&#20154;&#20110;2021&#24180;&#25552;&#20986;&#30340;&#21270;&#23398;&#22270;&#35770;&#29468;&#24819;&#65292;&#21478;&#22806;&#22235;&#20010;&#26159;AutoGraphiX&#35745;&#31639;&#26426;&#31995;&#32479;&#20110;2006&#24180;&#25552;&#20986;&#30340;&#29468;&#24819;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;AMCS&#35777;&#26126;&#20102;&#20854;&#20013;&#22235;&#20010;&#24320;&#25918;&#29468;&#24819;.
&lt;/p&gt;
&lt;p&gt;
Graph theory is an interdisciplinary field of study that has various applications in mathematical modeling and computer science. Research in graph theory depends on the creation of not only theorems but also conjectures. Conjecture-refuting algorithms attempt to refute conjectures by searching for counterexamples to those conjectures, often by maximizing certain score functions on graphs. This study proposes a novel conjecture-refuting algorithm, referred to as the adaptive Monte Carlo search (AMCS) algorithm, obtained by modifying the Monte Carlo tree search algorithm. Evaluated based on its success in finding counterexamples to several graph theory conjectures, AMCS outperforms existing conjecture-refuting algorithms. The algorithm is further utilized to refute six open conjectures, two of which were chemical graph theory conjectures formulated by Liu et al. in 2021 and four of which were formulated by the AutoGraphiX computer system in 2006. Finally, four of the open conjectures are
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#22797;&#26029;&#24320;&#32954;&#27668;&#36947;&#21644;&#34880;&#31649;&#25299;&#25169;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#38190;&#28857;&#26816;&#27979;&#20219;&#21153;&#39044;&#27979;&#21487;&#20197;&#36830;&#25509;&#26029;&#24320;&#32452;&#20214;&#30340;&#20851;&#38190;&#28857;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.07089</link><description>&lt;p&gt;
&#20462;&#22797;&#26029;&#24320;&#30340;&#32954;&#27668;&#36947;&#21644;&#34880;&#31649;&#30340;&#25299;&#25169;&#32467;&#26500;&#65306;&#22522;&#32447;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset. (arXiv:2306.07089v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#22797;&#26029;&#24320;&#32954;&#27668;&#36947;&#21644;&#34880;&#31649;&#25299;&#25169;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#38190;&#28857;&#26816;&#27979;&#20219;&#21153;&#39044;&#27979;&#21487;&#20197;&#36830;&#25509;&#26029;&#24320;&#32452;&#20214;&#30340;&#20851;&#38190;&#28857;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#32954;&#27668;&#36947;&#21644;&#34880;&#31649;&#20998;&#21106;&#23545;&#20110;&#32954;&#37096;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#36830;&#25509;&#24615;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#20854;&#20020;&#24202;&#24212;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#20462;&#22797;&#26029;&#24320;&#30340;&#32954;&#37096;&#31649;&#29366;&#32467;&#26500;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20851;&#38190;&#28857;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#21487;&#20197;&#36830;&#25509;&#26029;&#24320;&#32452;&#20214;&#30340;&#20851;&#38190;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#31649;&#29366;&#32467;&#26500;&#30340;&#26029;&#24320;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#26032;&#30340;&#32954;&#37096;&#26641;&#20462;&#22797;&#65288;PTR&#65289;&#25968;&#25454;&#38598;&#20844;&#24320;&#21487;&#29992;&#65292;&#21253;&#25324;800&#20010;&#23436;&#25972;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#21253;&#25324;&#32954;&#27668;&#36947;&#12289;&#21160;&#33033;&#21644;&#38745;&#33033;&#65292;&#20197;&#21450;&#21512;&#25104;&#30340;&#26029;&#24320;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/M3DV/pulmonary-tree-repairing&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate segmentation of pulmonary airways and vessels is crucial for the diagnosis and treatment of pulmonary diseases. However, current deep learning approaches suffer from disconnectivity issues that hinder their clinical usefulness. To address this challenge, we propose a post-processing approach that leverages a data-driven method to repair the topology of disconnected pulmonary tubular structures. Our approach formulates the problem as a keypoint detection task, where a neural network is trained to predict keypoints that can bridge disconnected components. We use a training data synthesis pipeline that generates disconnected data from complete pulmonary structures. Moreover, the new Pulmonary Tree Repairing (PTR) dataset is publicly available, which comprises 800 complete 3D models of pulmonary airways, arteries, and veins, as well as the synthetic disconnected data. Our code and data are available at https://github.com/M3DV/pulmonary-tree-repairing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20013;&#22914;&#20309;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#30340;&#35821;&#20041;&#30456;&#21516;&#65292;&#22312;&#38271;&#23614;&#29616;&#35937;&#20013;&#25506;&#35752;&#20102;&#21387;&#32553;&#25552;&#39640;&#25512;&#24191;&#24615;&#33021;&#30340;&#35760;&#24518;&#35201;&#32032;&#12290;</title><link>http://arxiv.org/abs/2306.06238</link><description>&lt;p&gt;
&#29702;&#35299;&#38271;&#23614;&#25928;&#24212;&#23545;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effect of the Long Tail on Neural Network Compression. (arXiv:2306.06238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20013;&#22914;&#20309;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#30340;&#35821;&#20041;&#30456;&#21516;&#65292;&#22312;&#38271;&#23614;&#29616;&#35937;&#20013;&#25506;&#35752;&#20102;&#21387;&#32553;&#25552;&#39640;&#25512;&#24191;&#24615;&#33021;&#30340;&#35760;&#24518;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#21387;&#32553;&#29616;&#22312;&#26159;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#30340;&#19968;&#20010;&#25104;&#29087;&#30340;&#23376;&#39046;&#22495;&#65292;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20197;&#20943;&#23567;&#27169;&#22411;&#23610;&#23544;&#21644;&#21152;&#36895;&#25512;&#26029;&#20026;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#35266;&#23519;&#21040;&#65292;&#20165;&#20851;&#27880;&#24635;&#20307;&#20934;&#30830;&#24615;&#21487;&#33021;&#26159;&#35823;&#23548;&#30340;&#12290;&#20363;&#22914;&#65292;&#24050;&#32463;&#35777;&#26126;&#20840;&#27169;&#22411;&#21644;&#21387;&#32553;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250;&#20559;&#21521;&#20110;&#22312;&#25968;&#25454;&#38598;&#20013;&#20302;&#39057;&#30340;&#31867;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#8220;&#25105;&#20204;&#33021;&#21542;&#22312;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#35821;&#20041;&#31561;&#21516;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#32593;&#32476;&#21387;&#32553;&#65311;&#8221;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;Feldman&#31561;&#20154;&#35266;&#23519;&#21040;&#30340;&#8220;&#38271;&#23614;&#8221;&#29616;&#35937;&#12290;&#20182;&#20204;&#35748;&#20026;&#65292;&#26576;&#20123;&#36755;&#20837;&#65288;&#36866;&#24403;&#23450;&#20041;&#65289;&#30340;&#35760;&#24518;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#26159;&#24517;&#35201;&#30340;&#12290;&#30001;&#20110;&#21387;&#32553;&#38480;&#21046;&#20102;&#32593;&#32476;&#30340;&#23481;&#37327;&#65288;&#22240;&#27492;&#20063;&#38480;&#21046;&#20102;&#20854;&#35760;&#24518;&#33021;&#21147;&#65289;&#65292;&#25152;&#20197;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;
&lt;/p&gt;
&lt;p&gt;
Network compression is now a mature sub-field of neural network research: over the last decade, significant progress has been made towards reducing the size of models and speeding up inference, while maintaining the classification accuracy. However, many works have observed that focusing on just the overall accuracy can be misguided. E.g., it has been shown that mismatches between the full and compressed models can be biased towards under-represented classes. This raises the important research question, \emph{can we achieve network compression while maintaining ``semantic equivalence'' with the original network?} In this work, we study this question in the context of the ``long tail'' phenomenon in computer vision datasets observed by Feldman, et al. They argue that \emph{memorization} of certain inputs (appropriately defined) is essential to achieving good generalization. As compression limits the capacity of a network (and hence also its ability to memorize), we study the question: a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#20174;&#20004;&#20010;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.05817</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22914;&#20309;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21463;&#30410;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Can Recommender Systems Benefit from Large Language Models: A Survey. (arXiv:2306.05817v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#20174;&#20004;&#20010;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#21305;&#37197;&#20114;&#32852;&#32593;&#24212;&#29992;&#31243;&#24207;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#26032;&#20852;&#33021;&#21147;&#65288;&#20363;&#22914;&#25351;&#20196;&#36319;&#36394;&#12289;&#25512;&#29702;&#65289;&#65292;&#20174;&#32780;&#20026;&#23558;LLM&#35843;&#25972;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#30340;&#30740;&#31350;&#26041;&#21521;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#24212;&#29992;&#23548;&#21521;&#30340;&#35282;&#24230;&#23545;&#27492;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#20004;&#20010;&#27491;&#20132;&#30340;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#23545;&#20110;&#8220;&#22312;&#21738;&#37324;&#8221;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;LLM&#22312;&#25512;&#33616;&#27969;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#20013;&#21487;&#33021;&#21457;&#25381;&#30340;&#20316;&#29992;&#65292;&#21363;&#29305;&#24449;&#24037;&#31243;&#12289;&#29305;&#24449;&#32534;&#30721;&#22120;&#12289;&#35780;&#20998;/&#25490;&#21517;&#20989;&#25968;&#21644;&#27969;&#31243;&#25511;&#21046;&#22120;&#12290;&#23545;&#20110;&#8220;&#22914;&#20309;&#8221;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#20174;&#32780;&#24471;&#20986;&#20004;&#20010;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#26631;&#20934;&#65292;&#21363;&#26159;&#21542;&#35843;&#25972;LLM&#21644;&#26159;&#21542;&#23558;LLM&#20316;&#20026;&#29420;&#31435;&#27169;&#22411;&#25110;&#28151;&#21512;&#27169;&#22411;&#32452;&#20214;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#23558;LLM&#35843;&#25972;&#21040;RS&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#28508;&#22312;&#26041;&#21521;&#65292;&#21253;&#25324;&#19982;&#29616;&#26377;&#31995;&#32479;&#30340;&#38598;&#25104;&#12289;&#29992;&#25143;&#21453;&#39304;&#12289;&#35780;&#20272;&#24230;&#37327;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems (RS) play important roles to match users' information needs for Internet applications. In natural language processing (NLP) domains, large language model (LLM) has shown astonishing emergent abilities (e.g., instruction following, reasoning), thus giving rise to the promising research direction of adapting LLM to RS for performance enhancements and user experience improvements. In this paper, we conduct a comprehensive survey on this research direction from an application-oriented view. We first summarize existing research works from two orthogonal perspectives: where and how to adapt LLM to RS. For the "WHERE" question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, and pipeline controller. For the "HOW" question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLMs or not, a
&lt;/p&gt;</description></item><item><title>GeoDiffusion&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04607</link><description>&lt;p&gt;
&#23558;&#20960;&#20309;&#25511;&#21046;&#38598;&#25104;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt. (arXiv:2306.04607v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04607
&lt;/p&gt;
&lt;p&gt;
GeoDiffusion&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#22312;&#21019;&#24314;&#20869;&#23481;&#21644;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#26174;&#30528;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#19981;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;&#19981;&#20165;&#22270;&#20687;&#27700;&#24179;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#32780;&#19988;&#36793;&#30028;&#26694;&#21644;&#30456;&#26426;&#35270;&#22270;&#31561;&#20960;&#20309;&#26465;&#20214;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#21069;&#26399;&#30740;&#31350;&#20351;&#29992;&#27169;&#22359;&#32534;&#30721;&#35821;&#20041;&#24067;&#23616;&#26469;&#23454;&#29616;&#22797;&#21046;&#31896;&#36148;&#21512;&#25104;&#25110;&#24067;&#23616;&#21040;&#22270;&#20687;(L2I)&#29983;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GeoDiffusion&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;(T2I)&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#12290;&#19982;&#20197;&#24448;&#30340;L2I&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;GeoDiffusion&#19981;&#20165;&#33021;&#22815;&#32534;&#30721;&#36793;&#30028;&#26694;&#65292;&#36824;&#33021;&#22815;&#32534;&#30721;&#33258;&#39550;&#22330;&#26223;&#20013;&#30340;&#39069;&#22806;&#20960;&#20309;&#26465;&#20214;&#65292;&#22914;&#25668;&#20687;&#22836;&#35270;&#22270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GeoDiffusion&#22312;&#29289;&#20307;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#29983;&#25104;&#20855;&#26377;&#26356;&#39640;&#24863;&#30693;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have attracted significant attention due to their remarkable ability to create content and generate data for tasks such as image classification. However, the usage of diffusion models to generate high-quality object detection data remains an underexplored area, where not only the image-level perceptual quality but also geometric conditions such as bounding boxes and camera views are essential. Previous studies have utilized either copy-paste synthesis or layout-to-image (L2I) generation with specifically designed modules to encode semantic layouts. In this paper, we propose GeoDiffusion, a simple framework that can flexibly translate various geometric conditions into text prompts and empower the pre-trained text-to-image (T2I) diffusion models for high-quality detection data generation. Unlike previous L2I methods, our GeoDiffusion is able to encode not only bounding boxes but also extra geometric conditions such as camera views in self-driving scenes. Extensive experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04502</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22522;&#20110;&#26799;&#24230;&#30340;&#24322;&#24120;&#20540;&#21435;&#38500;&#30340;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21487;&#38752;&#21644;&#39640;&#24615;&#33021;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#21363;&#20415;&#26159;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20063;&#20250;&#21253;&#21547;&#38169;&#35823;&#65292;&#26356;&#19981;&#29992;&#35828;&#33258;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20102;&#12290;&#29616;&#26377;&#30340;&#19968;&#20123;&#25968;&#25454;&#21435;&#22122;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#26816;&#27979;&#24322;&#24120;&#20540;&#24182;&#36827;&#34892;&#27704;&#20037;&#24615;&#21435;&#38500;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24456;&#23481;&#26131;&#36807;&#24230;&#25110;&#32773;&#27424;&#24230;&#36807;&#28388;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65288;AGRA&#65289;&#65292;&#19981;&#21516;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#28165;&#27927;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27604;&#36739;&#19968;&#32452;&#26679;&#26412;&#30340;&#32047;&#31215;&#26799;&#24230;&#21644;&#21333;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20915;&#23450;&#26159;&#21542;&#22312;&#24403;&#21069;&#26356;&#26032;&#26102;&#20445;&#30041;&#23545;&#24212;&#30340;&#26679;&#26412;&#65292;&#20197;&#27492;&#26469;&#30830;&#23450;&#23427;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;AGRA&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#20840;&#38754;&#30340;&#32467;&#26524;&#20998;&#26512;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis sup
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#21516;&#36136;&#21270;&#30340;&#27169;&#22411;&#65292;&#27169;&#22411;&#35780;&#20272;&#38656;&#35201;&#25552;&#20379;&#26377;&#25928;&#30340;&#35780;&#20272;&#65292;&#20197;&#21028;&#26029;&#29305;&#23450;&#27169;&#22411;&#26159;&#21542;&#22312;&#19979;&#28216;&#20351;&#29992;&#22330;&#26223;&#20013;&#21487;&#20197;&#28385;&#36275;&#22810;&#23569;&#20154;&#31867;&#38656;&#27714;&#65292;&#24182;&#19988;&#24212;&#35813;&#26681;&#25454;&#30495;&#23454;&#30340;&#31038;&#20250;&#38656;&#27714;&#26469;&#24320;&#21457;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#25317;&#25265;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03100</link><description>&lt;p&gt;
&#23558;&#27169;&#22411;&#35780;&#20272;&#37325;&#26032;&#32771;&#34385;&#20026;&#32553;&#23567;&#31038;&#20250;&#25216;&#26415;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Rethinking Model Evaluation as Narrowing the Socio-Technical Gap. (arXiv:2306.03100v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03100
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21516;&#36136;&#21270;&#30340;&#27169;&#22411;&#65292;&#27169;&#22411;&#35780;&#20272;&#38656;&#35201;&#25552;&#20379;&#26377;&#25928;&#30340;&#35780;&#20272;&#65292;&#20197;&#21028;&#26029;&#29305;&#23450;&#27169;&#22411;&#26159;&#21542;&#22312;&#19979;&#28216;&#20351;&#29992;&#22330;&#26223;&#20013;&#21487;&#20197;&#28385;&#36275;&#22810;&#23569;&#20154;&#31867;&#38656;&#27714;&#65292;&#24182;&#19988;&#24212;&#35813;&#26681;&#25454;&#30495;&#23454;&#30340;&#31038;&#20250;&#38656;&#27714;&#26469;&#24320;&#21457;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#25317;&#25265;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#21457;&#23637;&#32473;&#27169;&#22411;&#35780;&#20272;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#30028;&#21644;&#24037;&#19994;&#30028;&#27491;&#22312;&#21162;&#21147;&#24212;&#23545;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#25165;&#22810;&#33402;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#22859;&#65292;&#20294;&#23427;&#20204;&#20063;&#19981;&#21487;&#36991;&#20813;&#22320;&#21521;&#21516;&#36136;&#21270;&#36808;&#36827;&#65306;&#29992;&#21333;&#20010;&#24120;&#31216;&#20043;&#20026;&#8220;&#36890;&#29992;&#8221;&#30340;&#27169;&#22411;&#20026;&#19968;&#31995;&#21015;&#24212;&#29992;&#25552;&#20379;&#21160;&#21147;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#27169;&#22411;&#35780;&#20272;&#23454;&#36341;&#24517;&#39035;&#25215;&#25285;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20197;&#24212;&#23545;&#36825;&#31181;&#21516;&#36136;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#21644;&#36131;&#20219;&#65306;&#20026;&#29305;&#23450;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#30340;&#35780;&#20272;&#65292;&#21028;&#26029;&#26159;&#21542;&#20197;&#21450;&#22312;&#19979;&#28216;&#20351;&#29992;&#22330;&#26223;&#20013;&#21487;&#20197;&#36890;&#36807;&#32473;&#23450;&#27169;&#22411;&#28385;&#36275;&#22810;&#23569;&#20154;&#31867;&#38656;&#27714;&#65288;&#8220;&#31038;&#20250;&#25216;&#26415;&#24046;&#36317;&#8221;&#65289;&#12290;&#25105;&#20204;&#27762;&#21462;&#31038;&#20250;&#31185;&#23398;&#12289;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#21644;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#36328;&#23398;&#31185;&#39046;&#22495;&#30340;&#32463;&#39564;&#65292;&#25958;&#20419;&#31038;&#21306;&#24320;&#21457;&#22522;&#20110;&#30495;&#23454;&#31038;&#20250;&#38656;&#27714;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#25317;&#25265;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development of generative and large language models (LLMs) poses new challenges for model evaluation that the research community and industry are grappling with. While the versatile capabilities of these models ignite excitement, they also inevitably make a leap toward homogenization: powering a wide range of applications with a single, often referred to as ``general-purpose'', model. In this position paper, we argue that model evaluation practices must take on a critical task to cope with the challenges and responsibilities brought by this homogenization: providing valid assessments for whether and how much human needs in downstream use cases can be satisfied by the given model (\textit{socio-technical gap}). By drawing on lessons from the social sciences, human-computer interaction (HCI), and the interdisciplinary field of explainable AI (XAI), we urge the community to develop evaluation methods based on real-world socio-requirements and embrace diverse evaluation methods 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20010;&#24103;&#30340;&#26032;&#22411;&#37325;&#25773;&#26426;&#21046;SMILE&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#39057;&#36830;&#32493;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20869;&#23384;&#21463;&#21040;&#26497;&#31471;&#38480;&#21046;&#26102;&#65292;&#35270;&#39057;&#30340;&#22810;&#26679;&#24615;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.18418</link><description>&lt;p&gt;
&#19968;&#30629;&#65306;&#37325;&#26032;&#24605;&#32771;&#35270;&#39057;&#19981;&#26029;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Just a Glimpse: Rethinking Temporal Information for Video Continual Learning. (arXiv:2305.18418v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20010;&#24103;&#30340;&#26032;&#22411;&#37325;&#25773;&#26426;&#21046;SMILE&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#39057;&#36830;&#32493;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20869;&#23384;&#21463;&#21040;&#26497;&#31471;&#38480;&#21046;&#26102;&#65292;&#35270;&#39057;&#30340;&#22810;&#26679;&#24615;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#26159;&#36830;&#32493;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#37325;&#35201;&#30340;&#35774;&#32622;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#22330;&#26223;&#23494;&#20999;&#30456;&#20851;&#12290;&#38543;&#30528;&#31867;&#21035;/&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30001;&#20110;&#21463;&#21040;&#20869;&#23384;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#20250;&#20986;&#29616;&#12290;&#22312;&#35270;&#39057;&#39046;&#22495;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#38754;&#20020;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#35270;&#39057;&#25968;&#25454;&#21253;&#21547;&#22823;&#37327;&#24103;&#65292;&#36825;&#20250;&#20351;&#22238;&#25918;&#35760;&#24518;&#36127;&#25285;&#26356;&#37325;&#12290;&#30446;&#21069;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159;&#20174;&#35270;&#39057;&#27969;&#20013;&#23545;&#24103;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#22238;&#25918;&#35760;&#24518;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20010;&#24103;&#30340;&#26032;&#22411;&#37325;&#25773;&#26426;&#21046;SMILE&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#39057;&#36830;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#26497;&#31471;&#20869;&#23384;&#38480;&#21046;&#19979;&#65292;&#35270;&#39057;&#30340;&#22810;&#26679;&#24615;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#20174;&#20195;&#34920;&#22823;&#37327;&#29420;&#29305;&#35270;&#39057;&#30340;&#23569;&#37327;&#24103;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#35270;&#39057;&#25968;&#25454;&#38598;Kin&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning is one of the most important settings for the study of Continual Learning, as it closely resembles real-world application scenarios. With constrained memory sizes, catastrophic forgetting arises as the number of classes/tasks increases. Studying continual learning in the video domain poses even more challenges, as video data contains a large number of frames, which places a higher burden on the replay memory. The current common practice is to sub-sample frames from the video stream and store them in the replay memory. In this paper, we propose SMILE a novel replay mechanism for effective video continual learning based on individual/single frames. Through extensive experimentation, we show that under extreme memory constraints, video diversity plays a more significant role than temporal information. Therefore, our method focuses on learning from a small number of frames that represent a large number of unique videos. On three representative video datasets, Kin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22987;&#32456;&#33021;&#22815;&#25104;&#21151;&#32763;&#36716;&#27979;&#35797;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#12289;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#31561;&#22810;&#37325;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.12809</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#26469;&#32763;&#36716;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Relabel Minimal Training Subset to Flip a Prediction. (arXiv:2305.12809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22987;&#32456;&#33021;&#22815;&#25104;&#21151;&#32763;&#36716;&#27979;&#35797;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#12289;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#31561;&#22810;&#37325;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Yang&#31561;&#20154;&#21457;&#29616;&#65292;&#20165;&#21024;&#38500;1%&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#32467;&#26524;&#32763;&#36716;&#12290;&#37492;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#22122;&#22768;&#25968;&#25454;&#30340;&#26222;&#36941;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#36890;&#36807;&#37325;&#26032;&#26631;&#35760;&#19968;&#20010;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#21487;&#21542;&#23548;&#33268;&#27979;&#35797;&#32467;&#26524;&#32763;&#36716;&#65311;&#26412;&#25991;&#21033;&#29992;&#25193;&#23637;&#24433;&#21709;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35782;&#21035;&#21644;&#37325;&#26032;&#26631;&#35760;&#36825;&#31181;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22987;&#32456;&#33021;&#22815;&#20135;&#29983;&#25104;&#21151;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#26426;&#21046;&#26377;&#22810;&#37325;&#20316;&#29992;&#65306;&#65288;1&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24674;&#22797;&#21487;&#33021;&#38169;&#35823;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#25361;&#25112;&#27169;&#22411;&#39044;&#27979;&#65307;&#65288;2&#65289;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#22240;&#20026;&#26412;&#25991;&#21457;&#29616;&#23376;&#38598;&#30340;&#22823;&#23567;&#19982;&#35757;&#32451;&#38598;&#20013;&#22122;&#22768;&#25968;&#25454;&#30340;&#27604;&#20363;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#20851;&#31995;&#65307;&#65288;3&#65289;&#25552;&#20379;&#20102;&#27934;&#23519;&#35757;&#32451;&#38598;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#20195;&#34920;&#20102;&#23545;&#35782;&#21035;&#26368;&#23567;&#35757;&#32451;&#23376;&#38598;&#38382;&#39064;&#30340;&#31532;&#19968;&#27425;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Yang et al. (2023) discovered that removing a mere 1% of training points can often lead to the flipping of a prediction. Given the prevalence of noisy data in machine learning models, we pose the question: can we also result in the flipping of a test prediction by relabeling a small subset of the training data before the model is trained? In this paper, utilizing the extended influence function, we propose an efficient procedure for identifying and relabeling such a subset, demonstrating consistent success. This mechanism serves multiple purposes: (1) providing a complementary approach to challenge model predictions by recovering potentially mislabeled training points; (2) evaluating model resilience, as our research uncovers a significant relationship between the subset's size and the ratio of noisy data in the training set; and (3) offering insights into bias within the training set. To the best of our knowledge, this work represents the first investigation into the problem of identi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#25216;&#26415;&#30340;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#24494;&#27874;&#25955;&#23556;&#24341;&#36215;&#30340;&#30005;&#22330;&#27169;&#24335;&#26469;&#20272;&#35745;&#23494;&#24230;&#21078;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.14807</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#25216;&#26415;&#30340;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning assisted microwave-plasma interaction based technique for plasma density estimation. (arXiv:2304.14807v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#25216;&#26415;&#30340;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#24494;&#27874;&#25955;&#23556;&#24341;&#36215;&#30340;&#30005;&#22330;&#27169;&#24335;&#26469;&#20272;&#35745;&#23494;&#24230;&#21078;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#23494;&#24230;&#26159;&#34920;&#24449;&#20219;&#20309;&#31561;&#31163;&#23376;&#20307;&#30340;&#20851;&#38190;&#21442;&#25968;&#12290;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#24212;&#29992;&#21644;&#30740;&#31350;&#37117;&#22522;&#20110;&#31561;&#31163;&#23376;&#20307;&#23494;&#24230;&#21644;&#31561;&#31163;&#23376;&#20307;&#28201;&#24230;&#12290;&#20256;&#32479;&#30340;&#30005;&#23376;&#23494;&#24230;&#27979;&#37327;&#26041;&#27861;&#38024;&#23545;&#32473;&#23450;&#32447;&#24615;&#20302;&#28201;&#31561;&#31163;&#23376;&#20307;&#35774;&#22791;&#25552;&#20379;&#36724;&#21521;&#21644;&#24452;&#21521;&#21078;&#38754;&#12290;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#25805;&#20316;&#33539;&#22260;&#36739;&#23567;&#12289;&#20202;&#22120;&#27785;&#37325;&#20197;&#21450;&#25968;&#25454;&#20998;&#26512;&#36807;&#31243;&#22797;&#26434;&#31561;&#20027;&#35201;&#32570;&#28857;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#23454;&#38469;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#24494;&#27874;&#31561;&#31163;&#23376;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#22815;&#30830;&#23450;&#31561;&#31163;&#23376;&#20307;&#20869;&#30005;&#23376;&#23494;&#24230;&#21078;&#38754;&#12290;&#36890;&#36807;&#27979;&#37327;&#24494;&#27874;&#25955;&#23556;&#24341;&#36215;&#30340;&#30005;&#22330;&#27169;&#24335;&#26469;&#20272;&#35745;&#23494;&#24230;&#21078;&#38754;&#12290;&#35813;&#31574;&#30053;&#38024;&#23545;&#19968;&#20010;&#27169;&#25311;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#65292;&#20854;&#20013;&#21253;&#25324;&#20302;&#28201;&#12289;&#38750;&#30913;&#21270;&#21644;&#30896;&#25758;&#24615;&#31561;&#31163;&#23376;&#20307;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#39640;&#26031;&#24418;&#29366;&#23494;&#24230;&#21078;&#38754;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electron density is a key parameter to characterize any plasma. Most of the plasma applications and research in the area of low-temperature plasmas (LTPs) is based on plasma density and plasma temperature. The conventional methods for electron density measurements offer axial and radial profiles for any given linear LTP device. These methods have major disadvantages of operational range (not very wide), cumbersome instrumentation, and complicated data analysis procedures. To address such practical concerns, the article proposes a novel machine learning (ML) assisted microwave-plasma interaction based strategy which is capable enough to determine the electron density profile within the plasma. The electric field pattern due to microwave scattering is measured to estimate the density profile. The proof of concept is tested for a simulated training data set comprising a low-temperature, unmagnetized, collisional plasma. Different types of Gaussian-shaped density profiles, in the range
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#29031;&#26126;&#31995;&#32479;DeePLT&#65292;&#20854;&#36890;&#36807;&#36712;&#36857;&#39044;&#27979;&#23454;&#29616;&#26234;&#33021;&#23478;&#23621;&#20013;&#30340;&#20010;&#24615;&#21270;&#29031;&#26126;&#35843;&#25972;&#65292;&#32473;&#27599;&#20010;&#20154;&#23450;&#21046;&#29420;&#29305;&#30340;&#20010;&#20154;&#36164;&#26009;&#24182;&#26681;&#25454;&#20854;&#36712;&#36857;&#33258;&#21160;&#35843;&#25972;&#28783;&#20809;&#12290;</title><link>http://arxiv.org/abs/2304.08027</link><description>&lt;p&gt;
DeePLT&#65306;&#22522;&#20110;&#36712;&#36857;&#39044;&#27979;&#23454;&#29616;&#26234;&#33021;&#23478;&#23621;&#20013;&#30340;&#20010;&#24615;&#21270;&#29031;&#26126;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DeePLT: Personalized Lighting Facilitates by Trajectory Prediction of Recognized Residents in the Smart Home. (arXiv:2304.08027v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#29031;&#26126;&#31995;&#32479;DeePLT&#65292;&#20854;&#36890;&#36807;&#36712;&#36857;&#39044;&#27979;&#23454;&#29616;&#26234;&#33021;&#23478;&#23621;&#20013;&#30340;&#20010;&#24615;&#21270;&#29031;&#26126;&#35843;&#25972;&#65292;&#32473;&#27599;&#20010;&#20154;&#23450;&#21046;&#29420;&#29305;&#30340;&#20010;&#20154;&#36164;&#26009;&#24182;&#26681;&#25454;&#20854;&#36712;&#36857;&#33258;&#21160;&#35843;&#25972;&#28783;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23478;&#23621;&#21508;&#37096;&#20998;&#30340;&#26234;&#33021;&#21270;&#24050;&#25104;&#20026;&#29616;&#20195;&#23478;&#23621;&#30340;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#12290;&#20854;&#20013;&#20043;&#19968;&#20415;&#26159;&#26234;&#33021;&#29031;&#26126;&#31995;&#32479;&#65292;&#21487;&#20026;&#27599;&#20010;&#20154;&#23450;&#21046;&#20010;&#24615;&#21270;&#20809;&#29031;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#31995;&#32479;&#65292;&#36890;&#36807;&#36712;&#36857;&#39044;&#27979;&#20010;&#24615;&#21270;&#29031;&#26126;&#31995;&#32479;&#21363;&#26102;&#35843;&#25972;&#23478;&#20013;&#28783;&#20809;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21253;&#25324;&#20197;&#19979;&#27169;&#22359;&#65306;&#65288;I&#65289;&#20154;&#20307;&#26816;&#27979;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#23450;&#20301;&#27599;&#20010;&#32473;&#23450;&#35270;&#39057;&#24103;&#20013;&#30340;&#20154;&#29289;&#65292;&#65288;II&#65289;&#20154;&#33080;&#35782;&#21035;&#65292;&#29992;&#20110;&#35782;&#21035;&#26816;&#27979;&#21040;&#30340;&#20154;&#29289;&#65292;&#65288;III&#65289;&#20154;&#20307;&#36319;&#36394;&#65292;&#29992;&#20110;&#36319;&#36394;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;&#20154;&#29289;&#65292;&#20197;&#21450;&#65288;IV&#65289;&#36712;&#36857;&#39044;&#27979;&#65292;&#20351;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#29992;&#25143;&#22312;&#26410;&#26469;&#30340;&#20301;&#32622;&#12290;&#35813;&#26041;&#27861;&#20026;&#27599;&#20010;&#20154;&#25552;&#20379;&#19968;&#20010;&#29420;&#29305;&#30340;&#20010;&#20154;&#36164;&#26009;&#65292;&#21253;&#25324;&#35268;&#26684;&#12289;&#20154;&#33080;&#22270;&#20687;&#21644;&#33258;&#23450;&#20041;&#29031;&#26126;&#35774;&#32622;&#65292;&#32780;&#35813;&#20010;&#20154;&#36164;&#26009;&#29992;&#20110;&#29031;&#26126;&#35843;&#25972;&#36807;&#31243;&#12290;&#19982;&#20854;&#20182;&#29031;&#26126;&#31995;&#32479;&#19981;&#21516;&#65292;&#26412;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#26681;&#25454;&#27599;&#20010;&#20154;&#30340;&#36712;&#36857;&#33258;&#21160;&#35843;&#25972;&#28783;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the intelligence of various parts of the home has become one of the essential features of any modern home. One of these parts is the intelligence lighting system that personalizes the light for each person. This paper proposes an intelligent system based on machine learning that personalizes lighting in the instant future location of a recognized user, inferred by trajectory prediction. Our proposed system consists of the following modules: (I) human detection to detect and localize the person in each given video frame, (II) face recognition to identify the detected person, (III) human tracking to track the person in the sequence of video frames and (IV) trajectory prediction to forecast the future location of the user in the environment using Inverse Reinforcement Learning. The proposed method provides a unique profile for each person, including specifications, face images, and custom lighting settings. This profile is used in the lighting adjustment process. Unlike o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.01046</link><description>&lt;p&gt;
&#8220;Polytuplet Loss: &#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#8221;
&lt;/p&gt;
&lt;p&gt;
Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#23398;&#26657;&#25945;&#32946;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#20204;&#23558;&#21463;&#21040;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32771;&#39564;&#12290;&#23398;&#29983;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#23436;&#25104;&#27492;&#31867;&#32771;&#35797;&#65292;&#20854;&#20013;&#26377;&#20123;&#34987;&#35748;&#20026;&#26159;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#30340;&#12290;&#36825;&#26679;&#19968;&#31181;&#31574;&#30053;&#28041;&#21450;&#24378;&#35843;&#30456;&#23545;&#20934;&#30830;&#24615;&#32780;&#38750;&#32477;&#23545;&#20934;&#30830;&#24615;&#65292;&#29702;&#35770;&#19978;&#21487;&#20197;&#22312;&#19981;&#23436;&#20840;&#25484;&#25569;&#35299;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#35757;&#32451;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20197;&#35299;&#20915;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;ReClor&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36923;&#36753;&#25512;&#29702;&#25216;&#33021;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Polytuplet Loss&#20989;&#25968;&#65292;&#26159;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#25193;&#23637;&#65292;&#20197;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#32780;&#38750;&#23398;&#20064;&#32477;&#23545;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
&lt;/p&gt;</description></item><item><title>Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;</title><link>http://arxiv.org/abs/2303.17503</link><description>&lt;p&gt;
Pgx:&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#30340;&#24182;&#34892;&#28216;&#25103;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pgx: Hardware-accelerated parallel game simulation for reinforcement learning. (arXiv:2303.17503v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17503
&lt;/p&gt;
&lt;p&gt;
Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Pgx&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#26827;&#30424;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#12290;&#30001;&#20110;JAX&#30340;&#33258;&#21160;&#21521;&#37327;&#21270;&#21644;&#21363;&#26102;&#32534;&#35793;&#21151;&#33021;&#65292;Pgx&#26131;&#20110;&#22312;GPU/TPU&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#24182;&#34892;&#25191;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21333;&#20010;A100 GPU&#19978;&#30340;Pgx&#27169;&#25311;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290;Pgx&#23454;&#29616;&#20102;&#34987;&#35748;&#20026;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#28216;&#25103;&#65292;&#22914;Backgammon&#65292;Shogi&#21644;Go&#12290; Pgx&#21487;&#22312;https://github.com/sotetsuk/pgx&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Pgx, a collection of board game simulators written in JAX. Thanks to auto-vectorization and Just-In-Time compilation of JAX, Pgx scales easily to thousands of parallel execution on GPU/TPU accelerators. We found that the simulation of Pgx on a single A100 GPU is 10x faster than that of existing reinforcement learning libraries. Pgx implements games considered vital benchmarks in artificial intelligence research, such as Backgammon, Shogi, and Go. Pgx is available at https://github.com/sotetsuk/pgx.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BFP&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#26032;&#29305;&#24449;&#21464;&#25442;&#20026;&#26087;&#29305;&#24449;&#30340;&#32447;&#24615;&#21464;&#25442;&#26469;&#32500;&#25252;&#32447;&#24615;&#21487;&#20998;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#26032;&#29305;&#24449;&#26041;&#21521;&#30340;&#20986;&#29616;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#26087;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.14595</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#29305;&#24449;&#25237;&#24433;&#22312;&#19981;&#26029;&#23398;&#20064;&#20013;&#32500;&#25252;&#32447;&#24615;&#21487;&#20998;&#24615;
&lt;/p&gt;
&lt;p&gt;
Preserving Linear Separability in Continual Learning by Backward Feature Projection. (arXiv:2303.14595v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14595
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BFP&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#26032;&#29305;&#24449;&#21464;&#25442;&#20026;&#26087;&#29305;&#24449;&#30340;&#32447;&#24615;&#21464;&#25442;&#26469;&#32500;&#25252;&#32447;&#24615;&#21487;&#20998;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#26032;&#29305;&#24449;&#26041;&#21521;&#30340;&#20986;&#29616;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#26087;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#23398;&#20064;&#20013;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#27169;&#22411;&#38656;&#35201;&#22312;&#26377;&#38480;&#25110;&#27809;&#26377;&#20197;&#21069;&#26597;&#30475;&#20219;&#21153;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#24182;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#30452;&#25509;&#32422;&#26463;&#26032;&#29305;&#24449;&#20197;&#21305;&#37197;&#26087;&#29305;&#24449;&#65292;&#24573;&#35270;&#20102;&#21487;&#22609;&#24615;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Backward Feature Projection&#65288;BFP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#26032;&#29305;&#24449;&#22312;&#26087;&#29305;&#24449;&#30340;&#21487;&#23398;&#20064;&#32447;&#24615;&#21464;&#25442;&#20013;&#21457;&#29983;&#21464;&#21270;&#12290;BFP&#20445;&#30041;&#26087;&#31867;&#21035;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#65292;&#21516;&#26102;&#20801;&#35768;&#26032;&#30340;&#29305;&#24449;&#26041;&#21521;&#20986;&#29616;&#20197;&#36866;&#24212;&#26032;&#30340;&#31867;&#21035;&#12290;BFP&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#32463;&#39564;&#37325;&#25773;&#26041;&#27861;&#38598;&#25104;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;BFP&#26377;&#21161;&#20110;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting has been a major challenge in continual learning, where the model needs to learn new tasks with limited or no access to data from previously seen tasks. To tackle this challenge, methods based on knowledge distillation in feature space have been proposed and shown to reduce forgetting. However, most feature distillation methods directly constrain the new features to match the old ones, overlooking the need for plasticity. To achieve a better stability-plasticity trade-off, we propose Backward Feature Projection (BFP), a method for continual learning that allows the new features to change up to a learnable linear transformation of the old features. BFP preserves the linear separability of the old classes while allowing the emergence of new feature directions to accommodate new classes. BFP can be integrated with existing experience replay methods and boost performance by a significant margin. We also demonstrate that BFP helps learn a better representation space,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;</title><link>http://arxiv.org/abs/2303.11702</link><description>&lt;p&gt;
&#36830;&#25509;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Linking generative semi-supervised learning and generative open-set recognition. (arXiv:2303.11702v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25506;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;OSR&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#20197;&#21069;&#27809;&#26377;&#27491;&#24335;&#23558;SSL&#21644;OSR&#32852;&#31995;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#21508;&#33258;&#30340;&#26041;&#27861;&#26377;&#24778;&#20154;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SSL-GAN&#21644;OSR-GAN&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;&#29983;&#25104;&#26679;&#26412;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;SSL&#21644;OSR&#20998;&#31867;&#22120;&#37117;&#21487;&#20197;&#23436;&#20840;&#35782;&#21035;&#24320;&#25918;&#31354;&#38388;&#12290;&#20026;&#20102;&#35777;&#26126;SSL&#21644;OSR&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;SSL-GAN&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;OSR-GAN&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25991;&#29486;&#22522;&#30784;&#26356;&#21152;&#29282;&#22266;&#30340;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#22312;&#26576;&#20123;&#19968;&#33324;&#30340;OSR&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;OSR&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#20114;&#24800;&#28857;&#65288;ARP&#65289;-GAN&#22312;&#19968;&#20123;OSR&#20219;&#21153;&#20013;&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) in the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require generator to produce samples in the complementary space. Subsequently, by regularising networks with generated samples, both SSL and OSR classifiers generalize the open space. To demonstrate the connection between SSL and OSR, we theoretically and experimentally compare state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate that the SSL optimised margin-GANs, which have a stronger foundation in literature, set the new standard for the combined SSL-OSR task and achieves new state-of-other art results in certain general OSR experiments. However, the OSR optimised adversarial reciprocal point (ARP)-GANs still slightly out-performe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;QR-CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#36827;&#34892;&#20301;&#32622;&#21644;&#26102;&#38388;&#25512;&#29702;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#32422;10%&#21644;130%&#30340;&#30456;&#23545;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2302.00952</link><description>&lt;p&gt;
QR-CLIP: &#24341;&#20837;&#26174;&#24335;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#36827;&#34892;&#20301;&#32622;&#21644;&#26102;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
QR-CLIP: Introducing Explicit Open-World Knowledge for Location and Time Reasoning. (arXiv:2302.00952v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;QR-CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#36827;&#34892;&#20301;&#32622;&#21644;&#26102;&#38388;&#25512;&#29702;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#32422;10%&#21644;130%&#30340;&#30456;&#23545;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#30340;&#22270;&#20687;&#21487;&#33021;&#20256;&#36798;&#38656;&#35201;&#25105;&#20204;&#20174;&#20013;&#35760;&#24518;&#21644;&#25512;&#26029;&#20986;&#28145;&#21051;&#20449;&#24687;&#30340;&#25277;&#35937;&#21547;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25945;&#20250;&#26426;&#22120;&#39044;&#27979;&#22270;&#29255;&#25293;&#25668;&#30340;&#22320;&#28857;&#21644;&#26102;&#38388;&#65292;&#32780;&#19981;&#26159;&#25191;&#34892;&#20256;&#32479;&#30340;&#20998;&#21106;&#25110;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#40723;&#21169;&#20154;&#31867;&#31867;&#20284;&#30340;&#25512;&#29702;&#12290;&#21463;&#21040;Horn&#30340;QR&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#20004;&#20010;&#37096;&#20998;&#32452;&#25104;&#30340;&#26032;&#22411;QR-CLIP&#27169;&#22411;: 1) &#25968;&#37327;&#27169;&#22359;&#39318;&#20808;&#22238;&#39038;&#26356;&#22810;&#30340;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#20316;&#20026;&#20505;&#36873;&#30340;&#35821;&#35328;&#36755;&#20837;; 2) &#30456;&#20851;&#24615;&#27169;&#22359;&#20180;&#32454;&#20272;&#35745;&#35270;&#35273;&#21644;&#35821;&#35328;&#32447;&#32034;&#65292;&#24182;&#25512;&#26029;&#20986;&#20301;&#32622;&#21644;&#26102;&#38388;&#12290;&#23454;&#39564;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;QR-CLIP&#21313;&#20998;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312;&#27599;&#20010;&#20219;&#21153;&#19978;&#37117;&#27604;&#20043;&#21069;&#30340;&#26368;&#39640;&#27700;&#24179;&#34920;&#29616;&#24179;&#22343;&#25552;&#21319;&#20102;&#32422;10%&#21644;130%&#30340;&#30456;&#23545;&#25552;&#21319;&#12290;&#26412;&#30740;&#31350;&#20026;&#20301;&#32622;&#21644;&#26102;&#38388;&#25512;&#29702;&#22880;&#23450;&#20102;&#25216;&#26415;&#22522;&#30784;&#65292;&#24182;&#34920;&#26126;&#26377;&#25928;&#24341;&#20837;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#26159;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Daily images may convey abstract meanings that require us to memorize and infer profound information from them. To encourage such human-like reasoning, in this work, we teach machines to predict where and when it was taken rather than performing basic tasks like traditional segmentation or classification. Inspired by Horn's QR theory, we designed a novel QR-CLIP model consisting of two components: 1) the Quantity module first retrospects more open-world knowledge as the candidate language inputs; 2) the Relevance module carefully estimates vision and language cues and infers the location and time. Experiments show our QR-CLIP's effectiveness, and it outperforms the previous SOTA on each task by an average of about 10% and 130% relative lift in terms of location and time reasoning. This study lays a technical foundation for location and time reasoning and suggests that effectively introducing open-world knowledge is one of the panaceas for the tasks.
&lt;/p&gt;</description></item><item><title>Reef-Insight&#26159;&#19968;&#31181;&#21033;&#29992;&#32858;&#31867;&#26041;&#27861;&#21644;&#36965;&#24863;&#25216;&#26415;&#36827;&#34892;&#29642;&#29786;&#30977;&#26646;&#24687;&#22320;&#26144;&#23556;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#36965;&#24863;&#25968;&#25454;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#29642;&#29786;&#30977;&#26646;&#24687;&#22320;&#30340;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2301.10876</link><description>&lt;p&gt;
Reef-insight:&#19968;&#31181;&#36890;&#36807;&#36965;&#24863;&#36827;&#34892;&#27700;&#22495;&#26646;&#24687;&#22320;&#26144;&#23556;&#30340;&#32858;&#31867;&#26041;&#27861;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Reef-insight: A framework for reef habitat mapping with clustering methods via remote sensing. (arXiv:2301.10876v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10876
&lt;/p&gt;
&lt;p&gt;
Reef-Insight&#26159;&#19968;&#31181;&#21033;&#29992;&#32858;&#31867;&#26041;&#27861;&#21644;&#36965;&#24863;&#25216;&#26415;&#36827;&#34892;&#29642;&#29786;&#30977;&#26646;&#24687;&#22320;&#26144;&#23556;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#36965;&#24863;&#25968;&#25454;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#29642;&#29786;&#30977;&#26646;&#24687;&#22320;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#25439;&#23475;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#22823;&#20851;&#27880;&#28857;&#65292;&#29305;&#21035;&#26159;&#22312;&#28023;&#23736;&#21306;&#22495;&#21644;&#28023;&#27915;&#20013;&#65292;&#32771;&#34385;&#21040;&#27668;&#20505;&#21464;&#21270;&#21644;&#27745;&#26579;&#21450;&#26497;&#31471;&#27668;&#20505;&#20107;&#20214;&#30340;&#20005;&#37325;&#24433;&#21709;&#12290;&#25105;&#20204;&#30446;&#21069;&#30340;&#20998;&#26512;&#33021;&#21147;&#20197;&#21450;&#36965;&#24863;&#31561;&#20449;&#24687;&#33719;&#21462;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#21487;&#20197;&#29992;&#20110;&#31649;&#29702;&#21644;&#30740;&#31350;&#29642;&#29786;&#30977;&#29983;&#24577;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reef-Insight&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20855;&#26377;&#20808;&#36827;&#30340;&#32858;&#31867;&#26041;&#27861;&#21644;&#36965;&#24863;&#25216;&#26415;&#65292;&#29992;&#20110;&#29642;&#29786;&#30977;&#26646;&#24687;&#22320;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#36965;&#24863;&#25968;&#25454;&#27604;&#36739;&#19981;&#21516;&#30340;&#32858;&#31867;&#26041;&#27861;&#36827;&#34892;&#29642;&#29786;&#30977;&#26646;&#24687;&#22320;&#26144;&#23556;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#23450;&#24615;&#21644;&#35270;&#35273;&#35780;&#20272;&#30340;&#22235;&#31181;&#20027;&#35201;&#32858;&#31867;&#26041;&#27861;&#65292;&#21253;&#25324;k-means&#12289;&#23618;&#27425;&#32858;&#31867;&#12289;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#23494;&#24230;&#32858;&#31867;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#28595;&#22823;&#21033;&#20122;&#21335;&#22823;&#22561;&#30977;&#30340;One Tree Island&#29642;&#29786;&#30977;&#30340;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#35797;&#39564;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#36827;&#34892;&#29642;&#29786;&#30977;&#26646;&#24687;&#22320;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Environmental damage has been of much concern, particularly in coastal areas and the oceans, given climate change and the drastic effects of pollution and extreme climate events. Our present-day analytical capabilities, along with advancements in information acquisition techniques such as remote sensing, can be utilised for the management and study of coral reef ecosystems. In this paper, we present Reef-Insight, an unsupervised machine learning framework that features advanced clustering methods and remote sensing for reef habitat mapping. Our framework compares different clustering methods for reef habitat mapping using remote sensing data. We evaluate four major clustering approaches based on qualitative and visual assessments which include k-means, hierarchical clustering, Gaussian mixture model, and density-based clustering. We utilise remote sensing data featuring the One Tree Island reef in Australia's Southern Great Barrier Reef. Our results indicate that clustering methods usi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#30340;&#25991;&#26412;&#36716;SQL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#19968;&#31995;&#21015;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;SQL&#26597;&#35810;&#12289;&#29702;&#35299;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#21450;&#21306;&#20998;&#26377;&#26080;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.07695</link><description>&lt;p&gt;
EHRSQL&#65306;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#30340;&#23454;&#29992;&#25991;&#26412;&#36716;SQL&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#30340;&#25991;&#26412;&#36716;SQL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#19968;&#31995;&#21015;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;SQL&#26597;&#35810;&#12289;&#29702;&#35299;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#21450;&#21306;&#20998;&#26377;&#26080;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#12290;&#23545;&#35805;&#26159;&#30001;222&#20010;&#21307;&#38498;&#24037;&#20316;&#20154;&#21592;&#21253;&#25324;&#21307;&#29983;&#12289;&#25252;&#22763;&#12289;&#20445;&#38505;&#23457;&#26597;&#21644;&#20581;&#24247;&#26723;&#26696;&#22242;&#38431;&#31561;&#25163;&#26426;&#32780;&#26469;&#12290;&#20026;&#20102;&#26500;&#24314;&#20851;&#20110;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#30340;QA&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;&#19968;&#25152;&#22823;&#23398;&#21307;&#38498;&#36827;&#34892;&#20102;&#19968;&#27425;&#27665;&#35843;&#24182;&#21046;&#20316;&#20102;&#27169;&#26495;&#35805;&#26415;&#20197;&#21019;&#24314;&#31181;&#23376;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25163;&#21160;&#23558;&#23427;&#20204;&#38142;&#25509;&#21040;&#20004;&#20010;&#24320;&#28304;&#30340;EHR&#25968;&#25454;&#24211;&#65288;MIMIC-III&#21644;eICU&#65289;&#20013;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20102;&#26469;&#33258;&#27665;&#24847;&#35843;&#26597;&#30340;&#21508;&#31181;&#26102;&#38388;&#34920;&#36798;&#24335;&#21644;&#26410;&#33021;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#27169;&#22411;&#38656;&#35201; 1&#65289;&#29983;&#25104;&#21453;&#26144;&#21307;&#38498;&#20013;&#21508;&#31181;&#38656;&#27714;&#30340;SQL&#26597;&#35810;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#26816;&#32034;&#21644;&#22797;&#26434;&#30340;&#25805;&#20316;&#65292;&#22914;&#35745;&#31639;&#29983;&#23384;&#29575;&#65292;2&#65289;&#29702;&#35299;&#21508;&#31181;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#22238;&#31572;&#19982;&#26102;&#38388;&#25935;&#24863;&#30340;&#21307;&#30103;&#38382;&#39064;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;3&#65289;&#26681;&#25454;&#39044;&#27979;&#21306;&#20998;&#32473;&#23450;&#38382;&#39064;&#26159;&#21487;&#22238;&#31572;&#36824;&#26159;&#19981;&#21487;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new text-to-SQL dataset for electronic health records (EHRs). The utterances were collected from 222 hospital staff, including physicians, nurses, insurance review and health records teams, and more. To construct the QA dataset on structured EHR data, we conducted a poll at a university hospital and templatized the responses to create seed questions. Then, we manually linked them to two open-source EHR databases, MIMIC-III and eICU, and included them with various time expressions and held-out unanswerable questions in the dataset, which were all collected from the poll. Our dataset poses a unique set of challenges: the model needs to 1) generate SQL queries that reflect a wide range of needs in the hospital, including simple retrieval and complex operations such as calculating survival rate, 2) understand various time expressions to answer time-sensitive questions in healthcare, and 3) distinguish whether a given question is answerable or unanswerable based on the predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20869;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20004;&#31181;&#36884;&#24452;&#26469;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#20135;&#29983;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#12290;&#19968;&#31181;&#26159;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23558;&#33521;&#25991;&#36164;&#28304;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#65292;&#27880;&#37325;&#25968;&#37327;&#65307;&#21478;&#19968;&#31181;&#26159;&#30452;&#25509;&#22522;&#20110;&#39640;&#36136;&#37327;&#12289;&#29421;&#35889;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#26412;&#22320;&#21270;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#36164;&#28304;&#36739;&#23569;&#35821;&#35328;&#22914;&#24847;&#22823;&#21033;&#35821;&#30340;&#39046;&#22495;&#20869;&#36866;&#24212;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10422</link><description>&lt;p&gt;
&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#20869;&#33258;&#36866;&#24212;&#30340;&#26412;&#22320;&#21270;
&lt;/p&gt;
&lt;p&gt;
Localising In-Domain Adaptation of Transformer-Based Biomedical Language Models. (arXiv:2212.10422v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20869;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20004;&#31181;&#36884;&#24452;&#26469;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#20135;&#29983;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#12290;&#19968;&#31181;&#26159;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23558;&#33521;&#25991;&#36164;&#28304;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#65292;&#27880;&#37325;&#25968;&#37327;&#65307;&#21478;&#19968;&#31181;&#26159;&#30452;&#25509;&#22522;&#20110;&#39640;&#36136;&#37327;&#12289;&#29421;&#35889;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#26412;&#22320;&#21270;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#36164;&#28304;&#36739;&#23569;&#35821;&#35328;&#22914;&#24847;&#22823;&#21033;&#35821;&#30340;&#39046;&#22495;&#20869;&#36866;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21307;&#30103;&#26102;&#20195;&#65292;&#21307;&#38498;&#27599;&#22825;&#20135;&#29983;&#30340;&#22823;&#37327;&#25991;&#26412;&#20449;&#24687;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#36164;&#20135;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#20219;&#21153;&#12289;&#31934;&#32454;&#35843;&#25972;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#26469;&#21033;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#21644;&#31649;&#29702;&#12290;&#23545;&#20110;&#36825;&#20123;&#19987;&#38376;&#39046;&#22495;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26469;&#33258;&#24191;&#35206;&#30422;&#28857;&#26816;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#39046;&#22495;&#20869;&#36164;&#28304;&#30340;&#39069;&#22806;&#35757;&#32451;&#36718;&#27425;&#19978;&#21487;&#20197;&#33719;&#30410;&#24456;&#22823;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36164;&#28304;&#36890;&#24120;&#23545;&#20110;&#20687;&#24847;&#22823;&#21033;&#36825;&#26679;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#26159;&#19981;&#21487;&#21450;&#30340;&#65292;&#20351;&#24471;&#24403;&#22320;&#21307;&#30103;&#26426;&#26500;&#26080;&#27861;&#36827;&#34892;&#39046;&#22495;&#20869;&#36866;&#24212;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#20004;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#29983;&#25104;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#24847;&#22823;&#21033;&#35821;&#20026;&#20855;&#20307;&#26696;&#20363;&#65306;&#19968;&#31181;&#22522;&#20110;&#33521;&#25991;&#36164;&#28304;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65292;&#36861;&#27714;&#25968;&#37327;&#32780;&#19981;&#26159;&#36136;&#37327;&#65307;&#21478;&#19968;&#31181;&#22522;&#20110;&#39640;&#36136;&#37327;&#12289;&#29421;&#35889;&#30340;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#36827;&#34892;&#26412;&#22320;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of digital healthcare, the huge volumes of textual information generated every day in hospitals constitute an essential but underused asset that could be exploited with task-specific, fine-tuned biomedical language representation models, improving patient care and management. For such specialized domains, previous research has shown that fine-tuning models stemming from broad-coverage checkpoints can largely benefit additional training rounds over large-scale in-domain resources. However, these resources are often unreachable for less-resourced languages like Italian, preventing local medical institutions to employ in-domain adaptation. In order to reduce this gap, our work investigates two accessible approaches to derive biomedical language models in languages other than English, taking Italian as a concrete use-case: one based on neural machine translation of English resources, favoring quantity over quality; the other based on a high-grade, narrow-scoped corpus natively w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38754;&#23545;&#23545;&#25239;&#24615;&#29366;&#24577;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29366;&#24577;&#23545;&#25239;&#24615;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#25552;&#20986;&#20102;&#40065;&#26834;&#26234;&#33021;&#20307;&#31574;&#30053;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#26377;&#38480;&#29366;&#24577;&#21644;&#26377;&#38480;&#21160;&#20316;&#24773;&#20917;&#19979;&#30340;&#23384;&#22312;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#40065;&#26834;&#24615;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.02705</link><description>&lt;p&gt;
&#24590;&#26679;&#35299;&#20915;&#38754;&#23545;&#23545;&#25239;&#29366;&#24577;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
What is the Solution for State-Adversarial Multi-Agent Reinforcement Learning?. (arXiv:2212.02705v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38754;&#23545;&#23545;&#25239;&#24615;&#29366;&#24577;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29366;&#24577;&#23545;&#25239;&#24615;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#25552;&#20986;&#20102;&#40065;&#26834;&#26234;&#33021;&#20307;&#31574;&#30053;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#26377;&#38480;&#29366;&#24577;&#21644;&#26377;&#38480;&#21160;&#20316;&#24773;&#20917;&#19979;&#30340;&#23384;&#22312;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#40065;&#26834;&#24615;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#20013;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20551;&#35774;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#22522;&#20110;&#20934;&#30830;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#23398;&#20064;&#30340;&#31574;&#30053;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#29366;&#24577;&#25200;&#21160;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29366;&#24577;&#23545;&#25239;&#24615;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;(SAMG)&#65292;&#24182;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#19979;MARL&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;SAMG&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#30340;&#26368;&#20248;&#26234;&#33021;&#20307;&#31574;&#30053;&#21644;&#40065;&#26834;&#32435;&#20160;&#22343;&#34913;&#35299;&#20915;&#27010;&#24565;&#24182;&#19981;&#24635;&#26159;&#23384;&#22312;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#22256;&#38590;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#31216;&#20026;&#40065;&#26834;&#26234;&#33021;&#20307;&#31574;&#30053;&#30340;&#26032;&#35299;&#20915;&#27010;&#24565;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#39044;&#26399;&#29366;&#24577;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26377;&#38480;&#29366;&#24577;&#21644;&#26377;&#38480;&#21160;&#20316;SAMG&#20013;&#23384;&#22312;&#40065;&#26834;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;(RMA3C)&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;MARL&#26234;&#33021;&#20307;&#30340;&#40065;&#26834;&#24615;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various methods for Multi-Agent Reinforcement Learning (MARL) have been developed with the assumption that agents' policies are based on accurate state information. However, policies learned through Deep Reinforcement Learning (DRL) are susceptible to adversarial state perturbation attacks. In this work, we propose a State-Adversarial Markov Game (SAMG) and make the first attempt to investigate the fundamental properties of MARL under state uncertainties. Our analysis shows that the commonly used solution concepts of optimal agent policy and robust Nash equilibrium do not always exist in SAMGs. To circumvent this difficulty, we consider a new solution concept called robust agent policy, where agents aim to maximize the worst-case expected state value. We prove the existence of robust agent policy for finite state and finite action SAMGs. Additionally, we propose a Robust Multi-Agent Adversarial Actor-Critic (RMA3C) algorithm to learn robust policies for MARL agents under state uncertai
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#24494;&#20998;&#30340;&#29992;&#25143;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#24191;&#27867;&#24212;&#29992;&#30340;&#21487;&#24494;&#20998;&#26367;&#20195;&#21697;&#35299;&#20915;&#20102;&#29616;&#20195;&#20808;&#36827;&#29992;&#25143;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#19981;&#20860;&#23481;&#24615;&#21644;&#35745;&#31639;&#20195;&#20215;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#32447;&#24212;&#29992;&#20013;&#21487;&#20197;&#23454;&#29616;&#19982;&#29616;&#26377;&#26080;&#20284;&#28982;&#25512;&#29702;&#26041;&#27861;&#30456;&#24403;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33756;&#21333;&#25628;&#32034;&#20219;&#21153;&#20013;&#22914;&#20309;&#21033;&#29992;&#35748;&#30693;&#27169;&#22411;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2211.16277</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#30340;&#29992;&#25143;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentiable User Models. (arXiv:2211.16277v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16277
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#24494;&#20998;&#30340;&#29992;&#25143;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#24191;&#27867;&#24212;&#29992;&#30340;&#21487;&#24494;&#20998;&#26367;&#20195;&#21697;&#35299;&#20915;&#20102;&#29616;&#20195;&#20808;&#36827;&#29992;&#25143;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#19981;&#20860;&#23481;&#24615;&#21644;&#35745;&#31639;&#20195;&#20215;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#32447;&#24212;&#29992;&#20013;&#21487;&#20197;&#23454;&#29616;&#19982;&#29616;&#26377;&#26080;&#20284;&#28982;&#25512;&#29702;&#26041;&#27861;&#30456;&#24403;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33756;&#21333;&#25628;&#32034;&#20219;&#21153;&#20013;&#22914;&#20309;&#21033;&#29992;&#35748;&#30693;&#27169;&#22411;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#29992;&#25143;&#24314;&#27169;&#23545;&#20110;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#20808;&#36827;&#30340;&#29992;&#25143;&#27169;&#22411;&#36890;&#24120;&#34987;&#35774;&#35745;&#20026;&#35748;&#30693;&#34892;&#20026;&#27169;&#25311;&#22120;&#65292;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#19981;&#20860;&#23481;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#35745;&#31639;&#20195;&#20215;&#36807;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#24191;&#27867;&#24212;&#29992;&#30340;&#21487;&#24494;&#20998;&#26367;&#20195;&#21697;&#65292;&#32469;&#36807;&#35745;&#31639;&#29942;&#39048;&#65292;&#20351;&#29616;&#20195;&#35748;&#30693;&#27169;&#22411;&#30340;&#25512;&#29702;&#26356;&#39640;&#25928;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20197;&#36866;&#29992;&#20110;&#22312;&#32447;&#24212;&#29992;&#30340;&#35745;&#31639;&#25104;&#26412;&#23454;&#29616;&#19982;&#29616;&#26377;&#30340;&#26080;&#20284;&#28982;&#25512;&#29702;&#26041;&#27861;&#30456;&#24403;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22914;&#20309;&#22312;&#33756;&#21333;&#25628;&#32034;&#20219;&#21153;&#20013;&#20351;&#29992;&#35748;&#30693;&#27169;&#22411;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#65292;&#32780;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#36890;&#24120;&#38656;&#35201;&#25968;&#23567;&#26102;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic user modeling is essential for building machine learning systems in the ubiquitous cases with humans in the loop. However, modern advanced user models, often designed as cognitive behavior simulators, are incompatible with modern machine learning pipelines and computationally prohibitive for most practical applications. We address this problem by introducing widely-applicable differentiable surrogates for bypassing this computational bottleneck; the surrogates enable computationally efficient inference with modern cognitive models. We show experimentally that modeling capabilities comparable to the only available solution, existing likelihood-free inference methods, are achievable with a computational cost suitable for online applications. Finally, we demonstrate how AI-assistants can now use cognitive models for online interaction in a menu-search task, which has so far required hours of computation during interaction.
&lt;/p&gt;</description></item><item><title>QueryForm&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#34920;&#21333;&#23454;&#20307;&#26597;&#35810;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#25552;&#31034;&#26426;&#21046;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#26597;&#35810;-&#23454;&#20307;&#23545;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#33021;&#22815;&#20174;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20540;&#65292;&#26080;&#38656;&#30446;&#26631;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.07730</link><description>&lt;p&gt;
QueryForm: &#19968;&#31181;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#34920;&#21333;&#23454;&#20307;&#26597;&#35810;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
QueryForm: A Simple Zero-shot Form Entity Query Framework. (arXiv:2211.07730v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07730
&lt;/p&gt;
&lt;p&gt;
QueryForm&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#34920;&#21333;&#23454;&#20307;&#26597;&#35810;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#25552;&#31034;&#26426;&#21046;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#26597;&#35810;-&#23454;&#20307;&#23545;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#33021;&#22815;&#20174;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20540;&#65292;&#26080;&#38656;&#30446;&#26631;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#23545;&#20110;&#25991;&#26723;&#29702;&#35299;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#22330;&#26223;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#26631;&#27880;&#25991;&#26723;&#23454;&#20307;&#25152;&#38656;&#30340;&#39640;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#26694;&#26550;QueryForm&#65292;&#35813;&#26694;&#26550;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#20174;&#31867;&#20284;&#34920;&#21333;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20540;&#12290;QueryForm&#21253;&#21547;&#19968;&#20010;&#21452;&#37325;&#25552;&#31034;&#26426;&#21046;&#65292;&#23558;&#25991;&#26723;&#27169;&#24335;&#21644;&#29305;&#23450;&#23454;&#20307;&#31867;&#22411;&#32452;&#21512;&#25104;&#19968;&#20010;&#26597;&#35810;&#65292;&#29992;&#20110;&#25552;&#31034;Transformer&#27169;&#22411;&#25191;&#34892;&#21333;&#20010;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#20174;&#31867;&#20284;&#34920;&#21333;&#30340;&#32593;&#39029;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#26597;&#35810;-&#23454;&#20307;&#23545;&#36827;&#34892;QueryForm&#30340;&#39044;&#35757;&#32451;&#65292;&#36825;&#20123;&#32593;&#39029;&#24102;&#26377;&#24369;HTML&#27880;&#37322;&#12290;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#32479;&#19968;&#21040;&#30456;&#21516;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#26694;&#26550;&#20013;&#65292;QueryForm&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#21253;&#21547;&#21508;&#31181;&#23454;&#20307;&#21644;&#24067;&#23616;&#30340;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#30446;&#26631;&#25991;&#26723;&#31867;&#22411;&#65292;&#26080;&#38656;&#30446;&#26631;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;QueryForm&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, QueryForm, that extracts entity values from form-like documents in a zero-shot fashion. QueryForm contains a dual prompting mechanism that composes both the document schema and a specific entity type into a query, which is used to prompt a Transformer model to perform a single entity extraction task. Furthermore, we propose to leverage large-scale query-entity pairs generated from form-like webpages with weak HTML annotations to pre-train QueryForm. By unifying pre-training and fine-tuning into the same query-based framework, QueryForm enables models to learn from structured documents containing various entities and layouts, leading to better generalization to target document types without the need for target-specific training data. QueryForm sets new state-of-the-art average 
&lt;/p&gt;</description></item><item><title>&#23618;&#27425;&#28151;&#21512;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#19981;&#24179;&#34913;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20013;&#30340;&#29420;&#29305;&#38382;&#39064;&#65292;&#21253;&#25324;&#23618;&#27425;&#32467;&#26500;&#30340;&#26631;&#31614;&#12289;&#24322;&#26500;&#30340;&#35821;&#20041;&#21644;&#19981;&#24179;&#34913;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.13912</link><description>&lt;p&gt;
&#23618;&#27425;&#28151;&#21512;&#22810;&#26631;&#31614;&#20998;&#31867;&#22312;&#19981;&#24179;&#34913;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical MixUp Multi-label Classification with Imbalanced Interdisciplinary Research Proposals. (arXiv:2209.13912v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13912
&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#28151;&#21512;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#19981;&#24179;&#34913;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20013;&#30340;&#29420;&#29305;&#38382;&#39064;&#65292;&#21253;&#25324;&#23618;&#27425;&#32467;&#26500;&#30340;&#26631;&#31614;&#12289;&#24322;&#26500;&#30340;&#35821;&#20041;&#21644;&#19981;&#24179;&#34913;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#21161;&#26426;&#26500;&#20381;&#36182;&#20110;&#39046;&#22495;&#19987;&#23478;&#21644;&#30740;&#31350;&#25552;&#26696;&#20043;&#38388;&#30340;&#20027;&#39064;&#21305;&#37197;&#26469;&#25351;&#23450;&#25552;&#26696;&#23457;&#38405;&#20154;&#12290;&#38543;&#30528;&#25552;&#26696;&#36234;&#26469;&#36234;&#36328;&#23398;&#31185;&#65292;&#22914;&#20309;&#20934;&#30830;&#22320;&#23545;&#25552;&#26696;&#30340;&#36328;&#23398;&#31185;&#24615;&#36136;&#36827;&#34892;&#24314;&#27169;&#21644;&#20998;&#31867;&#65292;&#24182;&#25214;&#21040;&#20855;&#26377;&#21512;&#36866;&#19987;&#19994;&#30693;&#35782;&#30340;&#19987;&#23478;&#23457;&#38405;&#20154;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#20851;&#38190;&#27493;&#39588;&#26159;&#20934;&#30830;&#22320;&#23545;&#25552;&#26696;&#30340;&#36328;&#23398;&#31185;&#26631;&#31614;&#36827;&#34892;&#24314;&#27169;&#21644;&#20998;&#31867;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#30456;&#20851;&#25991;&#29486;&#65292;&#22914;&#25991;&#26412;&#20998;&#31867;&#21644;&#25552;&#26696;&#20998;&#31867;&#65292;&#22312;&#21516;&#26102;&#35299;&#20915;&#30001;&#36328;&#23398;&#31185;&#25552;&#26696;&#25968;&#25454;&#24341;&#20837;&#30340;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#36824;&#19981;&#36275;&#65306;1&#65289;&#25552;&#26696;&#30340;&#23398;&#31185;&#26631;&#31614;&#20855;&#26377;&#20174;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20363;&#22914;&#20174;&#20449;&#24687;&#31185;&#23398;&#21040;AI&#21040;AI&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;2&#65289;&#21508;&#20010;&#20027;&#35201;&#25991;&#26412;&#37096;&#20998;&#20855;&#26377;&#24322;&#26500;&#30340;&#35821;&#20041;&#65292;&#36215;&#19981;&#21516;&#30340;&#20316;&#29992;&#12290;3&#65289;&#25552;&#26696;&#30340;&#25968;&#37327;&#22312;&#21508;&#20010;&#26631;&#31614;&#20043;&#38388;&#26159;&#19981;&#24179;&#34913;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Funding agencies are largely relied on a topic matching between domain experts and research proposals to assign proposal reviewers. As proposals are increasingly interdisciplinary, it is challenging to profile the interdisciplinary nature of a proposal, and, thereafter, find expert reviewers with an appropriate set of expertise. An essential step in solving this challenge is to accurately model and classify the interdisciplinary labels of a proposal. Existing methodological and application-related literature, such as textual classification and proposal classification, are insufficient in jointly addressing the three key unique issues introduced by interdisciplinary proposal data: 1) the hierarchical structure of discipline labels of a proposal from coarse-grain to fine-grain, e.g., from information science to AI to fundamentals of AI. 2) the heterogeneous semantics of various main textual parts that play different roles in a proposal; 3) the number of proposals is imbalanced between no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;&#23431;&#23449;xURLLC&#26381;&#21153;&#36164;&#28304;&#20998;&#37197;&#21644;QoE&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#20248;&#21512;&#21516;&#35774;&#35745;&#26694;&#26550;&#12290;&#22312;&#25968;&#23398;&#19978;&#27169;&#25311;QoE&#30340;&#26032;&#22411;&#24230;&#37327;&#26631;&#20934;Meta-Immersion&#26377;&#21161;&#20110;&#22312;&#28385;&#36275;&#23458;&#25143;&#31471;&#29289;&#29702;&#38656;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2208.05438</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20803;&#23431;&#23449;xURLLC&#26381;&#21153;&#36164;&#28304;&#20998;&#37197;&#21644;QoE&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Attention-aware Resource Allocation and QoE Analysis for Metaverse xURLLC Services. (arXiv:2208.05438v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;&#23431;&#23449;xURLLC&#26381;&#21153;&#36164;&#28304;&#20998;&#37197;&#21644;QoE&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#20248;&#21512;&#21516;&#35774;&#35745;&#26694;&#26550;&#12290;&#22312;&#25968;&#23398;&#19978;&#27169;&#25311;QoE&#30340;&#26032;&#22411;&#24230;&#37327;&#26631;&#20934;Meta-Immersion&#26377;&#21161;&#20110;&#22312;&#28385;&#36275;&#23458;&#25143;&#31471;&#29289;&#29702;&#38656;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23431;&#23449;&#20195;&#34920;&#20102;&#25105;&#20204;&#23545;&#19979;&#19968;&#20195;&#20114;&#32852;&#32593;&#30340;&#26399;&#26395;&#65292;&#24182;&#24102;&#26469;&#20102;&#26032;&#30340;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#65288;KPI&#65289; &#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#36229;&#21487;&#38752;&#24615;&#21644;&#20302;&#26102;&#24310;&#36890;&#20449;&#65288;URLLC&#65289;&#21487;&#20197;&#28385;&#36275;&#23458;&#35266;&#30340;KPI&#65292;&#20294;&#24456;&#38590;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#27785;&#28024;&#24335;&#20307;&#39564;&#65292;&#36825;&#26159;&#20803;&#23431;&#23449;&#30340;&#29420;&#29305;&#29305;&#28857;&#12290;&#30001;&#20110;&#20307;&#39564;&#36136;&#37327;&#65288;QoE&#65289;&#21487;&#20197;&#34987;&#35270;&#20026;&#32508;&#21512;&#30340;KPI&#65292;&#22240;&#27492;URLLC&#34987;&#28436;&#21464;&#20026;&#20855;&#26377;&#20010;&#24615;&#21270;&#36164;&#28304;&#20998;&#37197;&#26041;&#26696;&#30340;&#19979;&#19968;&#20195;URLLC&#65288;xURLLC&#65289;&#26469;&#23454;&#29616;&#26356;&#39640;&#30340;QoE&#12290;&#20026;&#20102;&#37096;&#32626;&#20803;&#23431;&#23449;xURLLC&#26381;&#21153;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20803;&#23431;&#23449;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;MSP&#65289;&#21644;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#25552;&#20379;&#21830;&#65288;InP&#65289;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#20248;&#21512;&#21516;&#35774;&#35745;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26088;&#22312;&#26368;&#22823;&#21270;MSP&#30340;&#25928;&#29992;&#65292;&#35813;&#25928;&#29992;&#34987;&#23450;&#20041;&#20026;&#20803;&#23431;&#23449;&#29992;&#25143;QoE&#30340;&#20989;&#25968;&#65292;&#21516;&#26102;&#30830;&#20445;InP&#30340;&#28608;&#21169;&#12290;&#20026;&#20102;&#22312;&#25968;&#23398;&#19978;&#27169;&#25311;QoE&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Meta-Immersion&#30340;&#26032;&#22411;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaverse encapsulates our expectations of the next-generation Internet, while bringing new key performance indicators (KPIs). Although conventional ultra-reliable and low-latency communications (URLLC) can satisfy objective KPIs, it is difficult to provide a personalized immersive experience that is a distinctive feature of the Metaverse. Since the quality of experience (QoE) can be regarded as a comprehensive KPI, the URLLC is evolved towards the next generation URLLC (xURLLC) with a personalized resource allocation scheme to achieve higher QoE. To deploy Metaverse xURLLC services, we study the interaction between the Metaverse service provider (MSP) and the network infrastructure provider (InP), and provide an optimal contract design framework. Specifically, the utility of the MSP, defined as a function of Metaverse users' QoE, is to be maximized, while ensuring the incentives of the InP. To model the QoE mathematically, we propose a novel metric named Meta-Immersion that incorporat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#36827;&#21270;&#20256;&#36755;&#20248;&#21270;&#65288;ETO&#65289;&#39046;&#22495;&#30340;&#19968;&#39033;&#37325;&#35201;&#24037;&#20316;&#65292;&#36890;&#36807;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#19982;&#21333;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26680;&#24515;&#20256;&#36755;&#26426;&#21046;&#21644;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#31163;&#25955;&#24773;&#20917;&#19979;&#23454;&#29616;&#26234;&#33021;&#35843;&#24230;&#21644;&#32511;&#33394;&#35843;&#24230;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2206.12906</link><description>&lt;p&gt;
&#36808;&#21521;KAB2S: &#20174;&#21333;&#30446;&#26631;&#38382;&#39064;&#21040;&#22810;&#30446;&#26631;&#38382;&#39064;&#23398;&#20064;&#20851;&#38190;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Towards KAB2S: Learning Key Knowledge from Single-Objective Problems to Multi-Objective Problem. (arXiv:2206.12906v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#36827;&#21270;&#20256;&#36755;&#20248;&#21270;&#65288;ETO&#65289;&#39046;&#22495;&#30340;&#19968;&#39033;&#37325;&#35201;&#24037;&#20316;&#65292;&#36890;&#36807;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#19982;&#21333;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26680;&#24515;&#20256;&#36755;&#26426;&#21046;&#21644;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#31163;&#25955;&#24773;&#20917;&#19979;&#23454;&#29616;&#26234;&#33021;&#35843;&#24230;&#21644;&#32511;&#33394;&#35843;&#24230;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#8220;&#36827;&#21270;&#35745;&#31639;&#30740;&#31350;&#30340;&#26032;&#30340;&#21069;&#27839;&#8221;&#65292;&#36827;&#21270;&#20256;&#36755;&#20248;&#21270;&#65288;ETO&#65289;&#23558;&#20811;&#26381;&#20256;&#32479;&#33539;&#24335;&#20013;&#22312;&#36827;&#21270;&#35745;&#31639;&#30740;&#31350;&#20013;&#23545;&#24050;&#35299;&#20915;&#36807;&#30340;&#30456;&#20851;&#32463;&#39564;&#21644;&#30693;&#35782;&#30340;&#38646;&#37325;&#29992;&#12290;&#22312;&#36890;&#36807;ETO&#36827;&#34892;&#35843;&#24230;&#24212;&#29992;&#20013;&#65292;&#21487;&#20197;&#24418;&#25104;&#19968;&#20010;&#38750;&#24120;&#21560;&#24341;&#20154;&#19988;&#31454;&#20105;&#28608;&#28872;&#30340;&#26694;&#26550;&#65292;&#23588;&#20854;&#23545;&#20110;&#20013;&#22269;&#25552;&#20986;&#30340;&#8220;&#30899;&#20013;&#21644;&#8221;&#22269;&#38469;&#25215;&#35834;&#26469;&#35828;&#65292;&#36825;&#23558;&#20026;&#26234;&#33021;&#35843;&#24230;&#21644;&#32511;&#33394;&#35843;&#24230;&#25552;&#20379;&#19968;&#27425;&#8220;&#20250;&#38754;&#8221;&#30340;&#26426;&#20250;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#20851;&#20110;&#35843;&#24230;&#30340;&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#20010;ETO&#26694;&#26550;&#31867;&#30340;&#20316;&#21697;&#20013;&#30340;&#31532;&#19968;&#39033;&#24037;&#20316;&#65292;&#35813;&#26694;&#26550;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#19982;&#31163;&#25955;&#24773;&#20917;&#19979;&#30340;&#21333;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#8220;&#30896;&#25758;&#8221;&#65288;&#32780;&#19981;&#26159;&#22810;&#20219;&#21153;&#20248;&#21270;&#65289;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#36890;&#36807;&#26032;&#30340;&#26680;&#24515;&#20256;&#36755;&#26426;&#21046;&#21644;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#20256;&#36882;&#32473;&#24037;&#19994;&#24212;&#29992;&#30340;&#20851;&#38190;&#30693;&#35782;&#65292;&#20363;&#22914;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#35774;&#32622;&#30340;&#20301;&#32622;&#26500;&#24314;&#22359;&#65292;&#21487;&#20197;&#29992;&#20110;&#32622;&#25442;&#27969;&#27700;&#36710;&#38388;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As "a new frontier in evolutionary computation research", evolutionary transfer optimization(ETO) will overcome the traditional paradigm of zero reuse of related experience and knowledge from solved past problems in researches of evolutionary computation. In scheduling applications via ETO, a quite appealing and highly competitive framework "meeting" between them could be formed for both intelligent scheduling and green scheduling, especially for international pledge of "carbon neutrality" from China. To the best of our knowledge, our paper on scheduling here, serves as the 1st work of a class of ETO frameworks when multiobjective optimization problem "meets" single-objective optimization problems in discrete case (not multitasking optimization). More specifically, key knowledge conveyed for industrial applications, like positional building blocks with genetic algorithm based settings, could be used via the new core transfer mechanism and learning techniques for permutation flow shop s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#23558;ETO&#24212;&#29992;&#20110;&#22810;&#30446;&#26631;&#38382;&#39064;&#21644;&#32452;&#21512;&#38382;&#39064;&#20013;&#30340;&#35843;&#24230;&#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#36890;&#36807;&#20174;&#35299;&#20915;&#21333;&#30446;&#26631;&#38382;&#39064;&#20013;&#23398;&#20064;&#21644;&#36801;&#31227;&#20851;&#38190;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;ETO-PFSP&#26694;&#26550;&#22312;&#25490;&#21015;&#27969;&#27700;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#30456;&#23545;&#26377;&#25928;&#21644;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.12902</link><description>&lt;p&gt;
ETO&#19982;&#35843;&#24230;&#65306;&#20174;&#21333;&#30446;&#26631;&#38382;&#39064;&#21040;&#22810;&#30446;&#26631;&#38382;&#39064;&#23398;&#20064;&#20851;&#38190;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
ETO Meets Scheduling: Learning Key Knowledge from Single-Objective Problems to Multi-Objective Problem. (arXiv:2206.12902v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#23558;ETO&#24212;&#29992;&#20110;&#22810;&#30446;&#26631;&#38382;&#39064;&#21644;&#32452;&#21512;&#38382;&#39064;&#20013;&#30340;&#35843;&#24230;&#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#36890;&#36807;&#20174;&#35299;&#20915;&#21333;&#30446;&#26631;&#38382;&#39064;&#20013;&#23398;&#20064;&#21644;&#36801;&#31227;&#20851;&#38190;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;ETO-PFSP&#26694;&#26550;&#22312;&#25490;&#21015;&#27969;&#27700;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#30456;&#23545;&#26377;&#25928;&#21644;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#20256;&#36882;&#20248;&#21270;&#65288;ETO&#65289;&#34987;&#35270;&#20026;&#8220;&#36827;&#21270;&#35745;&#31639;&#30740;&#31350;&#30340;&#26032;&#39046;&#22495;&#8221;&#65292;&#23427;&#36991;&#20813;&#20102;&#20256;&#32479;&#36827;&#21270;&#35745;&#31639;&#20013;&#20174;&#24050;&#35299;&#20915;&#38382;&#39064;&#20013;&#38646;&#37325;&#29992;&#32463;&#39564;&#21644;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#22312;ETO&#30340;&#35843;&#24230;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#20043;&#38388;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#39640;&#24230;&#31454;&#20105;&#30340;&#8220;&#20132;&#27969;&#8221;&#26694;&#26550;&#65292;&#23454;&#29616;&#26234;&#33021;&#35843;&#24230;&#21644;&#32511;&#33394;&#35843;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#20013;&#22269;&#30340;&#30899;&#20013;&#21644;&#32972;&#26223;&#19979;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#23545;&#35843;&#24230;&#38382;&#39064;&#30340;&#30740;&#31350;&#26159;ETO&#22312;&#22810;&#30446;&#26631;&#38382;&#39064;&#8220;&#36935;&#35265;&#8221;&#32452;&#21512;&#38382;&#39064;&#65288;&#32780;&#19981;&#26159;&#22810;&#20219;&#21153;&#20248;&#21270;&#65289;&#30340;&#31532;&#19968;&#20010;&#24037;&#20316;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20687;&#20301;&#32622;&#26500;&#24314;&#22359;&#32858;&#31867;&#36825;&#26679;&#30340;&#20851;&#38190;&#30693;&#35782;&#21487;&#20197;&#29992;&#20110;&#25490;&#21015;&#27969;&#27700;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;PFSP&#65289;&#12290;&#23545;&#20110;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23454;&#35777;&#30740;&#31350;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;ETO-PFSP&#26694;&#26550;&#30340;&#30456;&#23545;&#26377;&#25928;&#24615;&#21644;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary transfer optimization(ETO) serves as "a new frontier in evolutionary computation research", which will avoid zero reuse of experience and knowledge from solved problems in traditional evolutionary computation. In scheduling applications via ETO, a highly competitive "meeting" framework between them could be constituted towards both intelligent scheduling and green scheduling, especially for carbon neutrality within the context of China. To the best of our knowledge, our study on scheduling here, is the 1st work of ETO for complex optimization when multiobjective problem "meets" single-objective problems in combinatorial case (not multitasking optimization). More specifically, key knowledge like positional building blocks clustered, could be learned and transferred for permutation flow shop scheduling problem (PFSP). Empirical studies on well-studied benchmarks validate relatively firm effectiveness and great potential of our proposed ETO-PFSP framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#33410;&#30340;&#26412;&#22320;&#23383;&#33410;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;NLP&#27169;&#22411;&#20013;&#23376;&#35789;&#26631;&#35760;&#21270;&#26041;&#26696;&#30340;&#21018;&#24615;&#21644;&#23545;&#20854;&#20182;&#35821;&#26009;&#24211;&#36866;&#24212;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#22312;&#22810;&#35821;&#31181;&#35821;&#26009;&#24211;&#20013;&#36807;&#24230;&#20999;&#20998;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2205.11490</link><description>&lt;p&gt;
&#26412;&#22320;&#23383;&#33410;&#34701;&#21512;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Local Byte Fusion for Neural Machine Translation. (arXiv:2205.11490v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#33410;&#30340;&#26412;&#22320;&#23383;&#33410;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;NLP&#27169;&#22411;&#20013;&#23376;&#35789;&#26631;&#35760;&#21270;&#26041;&#26696;&#30340;&#21018;&#24615;&#21644;&#23545;&#20854;&#20182;&#35821;&#26009;&#24211;&#36866;&#24212;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#22312;&#22810;&#35821;&#31181;&#35821;&#26009;&#24211;&#20013;&#36807;&#24230;&#20999;&#20998;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;NLP&#27169;&#22411;&#20013;&#65292;&#23376;&#35789;&#26631;&#35760;&#21270;&#26041;&#26696;&#26159;&#20027;&#35201;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#26696;&#21487;&#33021;&#36807;&#20110;&#27515;&#26495;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#35821;&#26009;&#24211;&#19978;&#26500;&#24314;&#30340;&#26631;&#35760;&#22120;&#23545;&#20854;&#20182;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#36866;&#24212;&#24615;&#19981;&#20339;&#12290;&#35266;&#23519;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#31181;&#35821;&#26009;&#24211;&#20013;&#65292;&#23376;&#35789;&#26631;&#35760;&#21270;&#26041;&#26696;&#20250;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#36807;&#24230;&#20999;&#20998;&#65292;&#20174;&#32780;&#23548;&#33268;&#32763;&#35793;&#24615;&#33021;&#19979;&#38477;&#12290;&#23376;&#35789;&#26631;&#35760;&#21270;&#30340;&#19968;&#20010;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#26159;&#22522;&#20110;&#23383;&#33410;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#32534;&#30721;&#26041;&#26696;&#65288;&#22914;UTF-8&#65289;&#23558;&#36755;&#20837;&#36827;&#34892;&#23383;&#33410;&#24207;&#21015;&#26631;&#35760;&#21270;&#12290;&#23383;&#33410;&#26631;&#35760;&#36890;&#24120;&#22312;&#23376;&#23383;&#31526;&#31890;&#24230;&#19978;&#34920;&#31034;&#36755;&#20837;&#65292;&#21363;&#19968;&#20010;&#23383;&#31526;&#21487;&#20197;&#30001;&#22810;&#20010;&#23383;&#33410;&#26631;&#35760;&#24207;&#21015;&#34920;&#31034;&#12290;&#36825;&#23548;&#33268;&#23383;&#33410;&#24207;&#21015;&#27604;&#23383;&#31526;&#24207;&#21015;&#38271;&#24471;&#22810;&#12290;&#22312;&#36739;&#20302;&#23618;&#20013;&#24378;&#21046;&#25191;&#34892;&#23616;&#37096;&#20449;&#24687;&#30340;&#32858;&#21512;&#21487;&#20197;&#25351;&#23548;&#27169;&#22411;&#26500;&#24314;&#26356;&#39640;&#23618;&#27425;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23383;&#33410;n-gram&#21644;&#21333;&#35789;&#36793;&#30028;&#30340;&#26412;&#22320;&#23383;&#33410;&#34701;&#21512;&#65288;LOBEF&#65289;&#26041;&#27861;&#29992;&#20110;&#22522;&#20110;&#23383;&#33410;&#30340;&#26426;&#22120;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subword tokenization schemes are the dominant technique used in current NLP models. However, such schemes can be rigid and tokenizers built on one corpus do not adapt well to other parallel corpora. It has also been observed that in multilingual corpora, subword tokenization schemes over-segment low-resource languages leading to a drop in translation performance. A simple alternative to subword tokenizers is byte-based methods i.e. tokenization into byte sequences using encoding schemes such as UTF-8. Byte tokens often represent inputs at a sub-character granularity i.e. one character can be represented by a sequence of multiple byte tokens. This results in byte sequences that are significantly longer than character sequences. Enforcing aggregation of local information in the lower layers can guide the model to build higher-level semantic information. We propose a Local Byte Fusion (LOBEF) method for byte-based machine translation -- utilizing byte $n$-gram and word boundaries -- to ag
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#22797;&#21512;&#26680;&#26469;&#23558;&#20808;&#39564;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#38544;&#24335;&#23450;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#26680;&#20989;&#25968;&#21644;&#36873;&#25321;&#30340;&#31532;&#20108;&#20010;&#26680;&#20989;&#25968;&#65292;&#21487;&#20197;&#27169;&#25311;&#24050;&#30693;&#29305;&#24615;&#65292;&#24182;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.07384</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#22797;&#21512;&#26680;&#23558;&#20808;&#39564;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel. (arXiv:2205.07384v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#22797;&#21512;&#26680;&#26469;&#23558;&#20808;&#39564;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#38544;&#24335;&#23450;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#26680;&#20989;&#25968;&#21644;&#36873;&#25321;&#30340;&#31532;&#20108;&#20010;&#26680;&#20989;&#25968;&#65292;&#21487;&#20197;&#27169;&#25311;&#24050;&#30693;&#29305;&#24615;&#65292;&#24182;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#23398;&#20064;&#20197;&#20808;&#39564;&#30693;&#35782;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35768;&#22810;&#24050;&#30693;&#29305;&#24615;&#65292;&#22914;&#31354;&#38388;&#24179;&#28369;&#24615;&#25110;&#23395;&#33410;&#24615;&#65292;&#22312;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20013;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#26680;&#20989;&#25968;&#26469;&#24314;&#27169;&#26159;&#30452;&#25509;&#30340;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#36825;&#20123;&#24050;&#30693;&#29305;&#24615;&#26469;&#25913;&#36827;&#12290;&#20363;&#22914;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#24191;&#27867;&#29992;&#20110;&#36965;&#24863;&#65292;&#36825;&#21463;&#21040;&#24378;&#28872;&#30340;&#23395;&#33410;&#25928;&#24212;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20351;&#29992;&#30001;&#31070;&#32463;&#32593;&#32476;&#38544;&#24335;&#23450;&#20041;&#30340;&#26680;&#20989;&#25968;&#19982;&#36873;&#25321;&#29992;&#20110;&#24314;&#27169;&#24050;&#30693;&#29305;&#24615;&#30340;&#31532;&#20108;&#20010;&#26680;&#20989;&#25968;&#65288;&#20363;&#22914;&#23395;&#33410;&#24615;&#65289;&#30456;&#32467;&#21512;&#30340;&#22797;&#21512;&#26680;&#26469;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;GP&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#28145;&#24230;&#32593;&#32476;&#21644;&#22522;&#20110;Nystrom&#36817;&#20284;&#30340;&#39640;&#25928;&#26144;&#23556;&#30456;&#32467;&#21512;&#26469;&#23454;&#29616;&#36825;&#19968;&#24819;&#27861;&#65292;&#23558;&#20854;&#31216;&#20026;&#38544;&#24335;&#22797;&#21512;&#26680;&#65288;ICK&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#26679;&#26412;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#23436;&#25972;&#30340;GP&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ICK&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#36965;&#24863;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is challenging to guide neural network (NN) learning with prior knowledge. In contrast, many known properties, such as spatial smoothness or seasonality, are straightforward to model by choosing an appropriate kernel in a Gaussian process (GP). Many deep learning applications could be enhanced by modeling such known properties. For example, convolutional neural networks (CNNs) are frequently used in remote sensing, which is subject to strong seasonal effects. We propose to blend the strengths of deep learning and the clear modeling capabilities of GPs by using a composite kernel that combines a kernel implicitly defined by a neural network with a second kernel function chosen to model known properties (e.g., seasonality). We implement this idea by combining a deep network and an efficient mapping based on the Nystrom approximation, which we call Implicit Composite Kernel (ICK). We then adopt a sample-then-optimize approach to approximate the full GP posterior distribution. We demons
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25554;&#27133;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#26512;&#20998;&#24067;&#22806;&#30340;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#25439;&#22833;&#21644;&#24314;&#27169;&#20559;&#24046;&#65292;&#35813;&#27169;&#22411;&#22312;&#22330;&#26223;&#20998;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.11194</link><description>&lt;p&gt;
&#20197;&#25554;&#27133;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#30340;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Test-time Adaptation with Slot-Centric Models. (arXiv:2203.11194v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25554;&#27133;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#26512;&#20998;&#24067;&#22806;&#30340;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#25439;&#22833;&#21644;&#24314;&#27169;&#20559;&#24046;&#65292;&#35813;&#27169;&#22411;&#22312;&#22330;&#26223;&#20998;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35270;&#35273;&#26816;&#27979;&#22120;&#22312;&#35757;&#32451;&#20998;&#24067;&#20869;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#23558;&#20998;&#24067;&#22806;&#30340;&#22330;&#26223;&#35299;&#26512;&#20026;&#20854;&#32452;&#25104;&#23454;&#20307;&#12290;&#26368;&#36817;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#20351;&#29992;&#36741;&#21161;&#30340;&#33258;&#30417;&#30563;&#25439;&#22833;&#26469;&#29420;&#31435;&#22320;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#24050;&#32463;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#22312;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#27867;&#21270;&#32467;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25439;&#22833;&#23545;&#20110;&#22330;&#26223;&#20998;&#35299;&#20219;&#21153;&#26469;&#35828;&#26159;&#19981;&#36275;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#24314;&#27169;&#20559;&#24046;&#12290;&#26368;&#36817;&#30340;&#20197;&#25554;&#27133;&#20026;&#20013;&#24515;&#30340;&#29983;&#25104;&#27169;&#22411;&#23581;&#35797;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#22330;&#26223;&#20998;&#35299;&#20026;&#23454;&#20307;&#65292;&#36890;&#36807;&#37325;&#24314;&#20687;&#32032;&#26469;&#23454;&#29616;&#12290;&#32467;&#21512;&#36825;&#20004;&#20010;&#24037;&#20316;&#32447;&#36335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#25554;&#27133;&#20026;&#20013;&#24515;&#30340;&#22330;&#26223;&#20998;&#35299;&#27169;&#22411;&#65292;&#21363;Slot-TTA&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#37325;&#24314;&#25110;&#20132;&#21449;&#35270;&#22270;&#32508;&#21512;&#30446;&#26631;&#22312;&#27599;&#20010;&#22330;&#26223;&#19978;&#36827;&#34892;&#26799;&#24230;&#19979;&#38477;&#36866;&#24212;&#12290;&#25105;&#20204;&#23545;Slot-TTA&#22312;&#22810;&#31181;&#36755;&#20837;&#27169;&#24335;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current visual detectors, though impressive within their training distribution, often fail to parse out-of-distribution scenes into their constituent entities. Recent test-time adaptation methods use auxiliary self-supervised losses to adapt the network parameters to each test example independently and have shown promising results towards generalization outside the training distribution for the task of image classification. In our work, we find evidence that these losses are insufficient for the task of scene decomposition, without also considering architectural inductive biases. Recent slot-centric generative models attempt to decompose scenes into entities in a self-supervised manner by reconstructing pixels. Drawing upon these two lines of work, we propose Slot-TTA, a semi-supervised slot-centric scene decomposition model that at test time is adapted per scene through gradient descent on reconstruction or cross-view synthesis objectives. We evaluate Slot-TTA across multiple input mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#27169;&#24577;&#22330;&#26223;&#19979;&#30340;&#39640;&#25928;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#27169;&#24577;&#21644;&#20132;&#20114;&#30340;&#24322;&#36136;&#24615;&#65292;&#20197;&#21152;&#36895;&#23545;&#22810;&#26679;&#21270;&#21644;&#23569;&#34987;&#30740;&#31350;&#30340;&#27169;&#24577;&#30340;&#25512;&#24191;&#12290; (arXiv:2203.01311v4 [cs.LG] UPDATED)</title><link>http://arxiv.org/abs/2203.01311</link><description>&lt;p&gt;
&#39640;&#27169;&#24577;&#22810;&#27169;&#24577;Transformer&#65306;&#37327;&#21270;&#27169;&#24577;&#19982;&#20132;&#20114;&#24322;&#36136;&#24615;&#20197;&#36827;&#34892;&#39640;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
High-Modality Multimodal Transformer: Quantifying Modality &amp; Interaction Heterogeneity for High-Modality Representation Learning. (arXiv:2203.01311v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#27169;&#24577;&#22330;&#26223;&#19979;&#30340;&#39640;&#25928;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#27169;&#24577;&#21644;&#20132;&#20114;&#30340;&#24322;&#36136;&#24615;&#65292;&#20197;&#21152;&#36895;&#23545;&#22810;&#26679;&#21270;&#21644;&#23569;&#34987;&#30740;&#31350;&#30340;&#27169;&#24577;&#30340;&#25512;&#24191;&#12290; (arXiv:2203.01311v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#20363;&#22914;&#20154;&#31867;&#29992;&#20110;&#20132;&#27969;&#30340;&#21475;&#35821;&#12289;&#25163;&#21183;&#21644;&#35821;&#29992;&#23398;&#65292;&#20197;&#21450;&#26426;&#22120;&#20154;&#19978;&#30340;&#21147;&#12289;&#26412;&#20307;&#24863;&#21644;&#35270;&#35273;&#20256;&#24863;&#22120;&#12290;&#34429;&#28982;&#22810;&#27169;&#24577;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#19968;&#23567;&#32452;&#27169;&#24577;&#65292;&#20027;&#35201;&#26159;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#12290;&#20026;&#20102;&#21152;&#36895;&#21521;&#22810;&#26679;&#21270;&#21644;&#23569;&#34987;&#30740;&#31350;&#30340;&#27169;&#24577;&#25512;&#24191;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#27169;&#24577;&#22330;&#26223;&#19979;&#30340;&#39640;&#25928;&#34920;&#31034;&#23398;&#20064;&#65292;&#28041;&#21450;&#19968;&#20010;&#22823;&#37327;&#30340;&#19981;&#21516;&#27169;&#24577;&#12290;&#30001;&#20110;&#20026;&#27599;&#20010;&#26032;&#27169;&#24577;&#28155;&#21152;&#26032;&#27169;&#22411;&#21464;&#24471;&#20195;&#20215;&#36807;&#39640;&#65292;&#20851;&#38190;&#30340;&#25216;&#26415;&#25361;&#25112;&#26159;&#24322;&#36136;&#24615;&#37327;&#21270;&#65306;&#25105;&#20204;&#22914;&#20309;&#34913;&#37327;&#21738;&#20123;&#27169;&#24577;&#32534;&#30721;&#20102;&#31867;&#20284;&#30340;&#20449;&#24687;&#21644;&#20132;&#20114;&#65292;&#20197;&#20415;&#20801;&#35768;&#19982;&#20808;&#21069;&#30340;&#27169;&#24577;&#20849;&#20139;&#21442;&#25968;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#24322;&#36136;&#24615;&#65306;(1)&#27169;&#24577;&#24322;&#36136;&#24615;&#30740;&#31350;&#20102;&#20004;&#20010;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world problems are inherently multimodal, from spoken language, gestures, and paralinguistics humans use to communicate, to force, proprioception, and visual sensors on robots. While there has been an explosion of interest in multimodal learning, these methods are focused on a small set of modalities primarily in language, vision, and audio. In order to accelerate generalization towards diverse and understudied modalities, this paper studies efficient representation learning for high-modality scenarios involving a large set of diverse modalities. Since adding new models for every new modality becomes prohibitively expensive, a critical technical challenge is heterogeneity quantification: how can we measure which modalities encode similar information and interactions in order to permit parameter sharing with previous modalities? This paper proposes two new information theoretic metrics for heterogeneity quantification: (1) modality heterogeneity studies how similar 2 modalitie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30196;&#21574;&#30417;&#27979;&#21644;&#35786;&#26029;&#12290;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#12289;&#35328;&#35821;&#21644;&#35821;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#21306;&#20998;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#24739;&#32773;&#21644;&#23545;&#29031;&#32452;&#65292;&#20174;&#32780;&#20026;&#30196;&#21574;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2109.01537</link><description>&lt;p&gt;
&#29992;&#20110;&#30196;&#21574;&#30417;&#27979;&#21644;&#35786;&#26029;&#30340;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis. (arXiv:2109.01537v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30196;&#21574;&#30417;&#27979;&#21644;&#35786;&#26029;&#12290;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#12289;&#35328;&#35821;&#21644;&#35821;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#21306;&#20998;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#24739;&#32773;&#21644;&#23545;&#29031;&#32452;&#65292;&#20174;&#32780;&#20026;&#30196;&#21574;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30196;&#21574;&#26159;&#19968;&#31995;&#21015;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#24433;&#21709;&#36234;&#26469;&#36234;&#22810;&#30340;&#20840;&#29699;&#32769;&#40836;&#20154;&#21475;&#30340;&#35760;&#24518;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;&#33258;&#21160;&#21270;&#20998;&#26512;&#35821;&#35328;&#12289;&#35328;&#35821;&#21644;&#35821;&#29992;&#25351;&#26631;&#20316;&#20026;&#35748;&#30693;&#34928;&#36864;&#30340;&#28508;&#22312;&#25351;&#26631;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#33258;&#28982;&#29615;&#22659;&#19979;&#25910;&#38598;&#20102;&#36731;&#24230;&#30196;&#21574;&#24739;&#32773;&#21644;&#37197;&#23545;&#30340;&#24180;&#40836;&#21305;&#37197;&#23545;&#29031;&#32452;&#30340;&#25968;&#25454;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;&#20960;&#20010;&#26376;&#12290;&#22810;&#27169;&#24577;&#25968;&#25454;&#21253;&#25324;&#21475;&#22836;&#20250;&#35805;&#65292;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#34987;&#36716;&#24405;&#65292;&#20197;&#21450;&#36755;&#20837;&#21644;&#20070;&#20889;&#30340;&#24605;&#32771;&#20869;&#23481;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#38750;&#35821;&#35328;&#20449;&#24687;&#65292;&#22914;&#31508;&#30011;&#21644;&#25353;&#38190;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#35813;&#25968;&#25454;&#38598;&#65292;&#24182;&#30528;&#37325;&#35752;&#35770;&#20102;&#20351;&#29992;&#35821;&#38899;&#27169;&#24577;&#30340;&#20219;&#21153;&#12290;&#21518;&#32773;&#28041;&#21450;&#21033;&#29992;&#25968;&#25454;&#30340;&#32437;&#21521;&#29305;&#24615;&#26469;&#21306;&#20998;&#23545;&#29031;&#32452;&#21644;&#30196;&#21574;&#24739;&#32773;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#20250;&#35805;&#38388;&#35821;&#38899;&#30340;&#21464;&#21270;&#22312;&#19981;&#21516;&#30340;&#20250;&#35805;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dementia is a family of neurogenerative conditions affecting memory and cognition in an increasing number of individuals in our globally aging population. Automated analysis of language, speech and paralinguistic indicators have been gaining popularity as potential indicators of cognitive decline. Here we propose a novel longitudinal multi-modal dataset collected from people with mild dementia and age matched controls over a period of several months in a natural setting. The multi-modal data consists of spoken conversations, a subset of which are transcribed, as well as typed and written thoughts and associated extra-linguistic information such as pen strokes and keystrokes. We describe the dataset in detail and proceed to focus on a task using the speech modality. The latter involves distinguishing controls from people with dementia by exploiting the longitudinal nature of the data. Our experiments showed significant differences in how the speech varied from session to session in the 
&lt;/p&gt;</description></item></channel></rss>