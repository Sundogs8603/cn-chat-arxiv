<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35843;&#26597;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#21644;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#21015;&#20030;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#30740;&#31350;&#21457;&#29616;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01327</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#20013;&#30340;&#30417;&#30563;&#31639;&#27861;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Supervised Algorithmic Fairness in Distribution Shifts: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01327
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#30740;&#31350;&#20102;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35843;&#26597;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#21644;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#21015;&#20030;&#20102;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#30740;&#31350;&#21457;&#29616;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30417;&#30563;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;&#38754;&#23545;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#65292;&#22914;&#20309;&#20445;&#25345;&#20844;&#24179;&#21644;&#26080;&#20559;&#39044;&#27979;&#30340;&#25361;&#25112;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22312;&#37096;&#32626;&#26102;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#22240;&#21508;&#31181;&#22240;&#32032;&#32780;&#38543;&#26102;&#38388;&#21457;&#29983;&#21464;&#21270;&#12290;&#36825;&#31181;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#65292;&#23545;&#29305;&#23450;&#36890;&#36807;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#26469;&#34920;&#24449;&#30340;&#32676;&#20307;&#20135;&#29983;&#19981;&#22343;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#20102;&#24635;&#32467;&#65292;&#24182;&#20840;&#38754;&#35843;&#26597;&#20102;&#22522;&#20110;&#36825;&#20123;&#21464;&#21270;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#25991;&#29486;&#20013;&#31361;&#20986;&#20102;&#20845;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20221;&#35843;&#26597;&#21015;&#20986;&#20102;&#29992;&#20110;&#23454;&#35777;&#30740;&#31350;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#19982;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#30340;&#20132;&#20114;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20013;&#30340;&#37325;&#35201;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised fairness-aware machine learning under distribution shifts is an emerging field that addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains. In real-world applications, machine learning models are often trained on a specific dataset but deployed in environments where the data distribution may shift over time due to various factors. This shift can lead to unfair predictions, disproportionately affecting certain groups characterized by sensitive attributes, such as race and gender. In this survey, we provide a summary of various types of distribution shifts and comprehensively investigate existing methods based on these shifts, highlighting six commonly used approaches in the literature. Additionally, this survey lists publicly available datasets and evaluation metrics for empirical studies. We further explore the interconnection with related research fields, discuss the significant c
&lt;/p&gt;</description></item><item><title>&#35813;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#30340;&#37325;&#35201;&#27010;&#24565;&#21450;&#20851;&#38190;&#30340;&#25216;&#26415;&#21644;&#22240;&#32032;&#65292;&#20197;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00897</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#40065;&#26834;&#24615;&#65306;&#20837;&#38376;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Robustness: A Primer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00897
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#30340;&#37325;&#35201;&#27010;&#24565;&#21450;&#20851;&#38190;&#30340;&#25216;&#26415;&#21644;&#22240;&#32032;&#65292;&#20197;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#31283;&#20581;&#24615;&#30340;&#22522;&#26412;&#27010;&#24565;&#21450;&#20854;&#22312;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#35752;&#35770;&#22987;&#20110;&#31283;&#20581;&#24615;&#30340;&#35814;&#32454;&#23450;&#20041;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;ML&#27169;&#22411;&#22312;&#21508;&#31181;&#19981;&#21516;&#21644;&#24847;&#22806;&#30340;&#29615;&#22659;&#26465;&#20214;&#19979;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;ML&#40065;&#26834;&#24615;&#36890;&#36807;&#22810;&#20010;&#35270;&#35282;&#36827;&#34892;&#20102;&#21078;&#26512;&#65306;&#20854;&#19982;&#27867;&#21270;&#33021;&#21147;&#30340;&#20114;&#34917;&#24615;&#65307;&#20854;&#20316;&#20026;&#21487;&#20449;AI&#30340;&#35201;&#27714;&#65307;&#20854;&#23545;&#25239;&#24615;&#19982;&#38750;&#23545;&#25239;&#24615;&#26041;&#38754;&#65307;&#20854;&#25968;&#37327;&#21270;&#25351;&#26631;&#65307;&#20197;&#21450;&#20854;&#21487;&#22797;&#29616;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#25351;&#26631;&#12290;&#31456;&#33410;&#28145;&#20837;&#25506;&#35752;&#20102;&#24433;&#21709;&#40065;&#26834;&#24615;&#30340;&#22240;&#32032;&#65292;&#22914;&#25968;&#25454;&#20559;&#24046;&#12289;&#27169;&#22411;&#22797;&#26434;&#24615;&#20197;&#21450;ML&#27969;&#31243;&#19981;&#26126;&#30830;&#30340;&#39118;&#38505;&#12290;&#23427;&#20174;&#24191;&#27867;&#30340;&#35270;&#35282;&#35843;&#26597;&#20102;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21253;&#25324;&#25968;&#23383;&#21644;&#29289;&#29702;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00897v1 Announce Type: cross  Abstract: This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its adversarial vs non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including adversarial attacks, encompassing both digital and physical realms.
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#21307;&#23398;&#20915;&#31574;&#20013;&#25552;&#20379;&#37325;&#35201;&#21453;&#39304;&#65292;&#21487;&#20197;&#25361;&#25112;&#19981;&#27491;&#30830;&#30340;&#35786;&#26029;&#65292;&#20419;&#36827;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.20288</link><description>&lt;p&gt;
LLM&#33021;&#22815;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#32416;&#27491;&#21307;&#29983;&#21527;&#65311;&#30740;&#31350;&#26377;&#25928;&#30340;&#20132;&#20114;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20288
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#21307;&#23398;&#20915;&#31574;&#20013;&#25552;&#20379;&#37325;&#35201;&#21453;&#39304;&#65292;&#21487;&#20197;&#25361;&#25112;&#19981;&#27491;&#30830;&#30340;&#35786;&#26029;&#65292;&#20419;&#36827;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21327;&#21161;&#24182;&#21487;&#33021;&#32416;&#27491;&#21307;&#29983;&#36827;&#34892;&#21307;&#30103;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;LLMs&#65292;&#21253;&#25324;Meditron&#12289;Llama2&#21644;Mistral&#65292;&#20998;&#26512;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#19982;&#21307;&#29983;&#26377;&#25928;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;PubMedQA&#30340;&#38382;&#39064;&#21644;&#20960;&#39033;&#20219;&#21153;&#65292;&#20174;&#20108;&#20803;&#65288;&#26159;/&#21542;&#65289;&#22238;&#31572;&#21040;&#38271;&#31572;&#26696;&#29983;&#25104;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#31572;&#26696;&#26159;&#22312;&#19982;&#21307;&#29983;&#20132;&#20114;&#21518;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#31034;&#35774;&#35745;&#26174;&#33879;&#24433;&#21709;&#20102;LLMs&#30340;&#19979;&#28216;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;LLMs&#21487;&#20197;&#20026;&#21307;&#29983;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21453;&#39304;&#65292;&#25361;&#25112;&#19981;&#27491;&#30830;&#30340;&#35786;&#26029;&#65292;&#20419;&#36827;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#20363;&#22914;&#65292;&#24403;&#21307;&#29983;&#20934;&#30830;&#29575;&#20026;38%&#26102;&#65292;Mistral&#21487;&#20197;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#26681;&#25454;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#65292;&#23558;&#20934;&#30830;&#24615;&#25552;&#39640;&#21040;74%&#65292;&#32780;Llama2&#21644;Meditron&#27169;&#22411;&#20063;&#33021;&#25552;&#20379;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20288v1 Announce Type: cross  Abstract: We explore the potential of Large Language Models (LLMs) to assist and potentially correct physicians in medical decision-making tasks. We evaluate several LLMs, including Meditron, Llama2, and Mistral, to analyze the ability of these models to interact effectively with physicians across different scenarios. We consider questions from PubMedQA and several tasks, ranging from binary (yes/no) responses to long answer generation, where the answer of the model is produced after an interaction with a physician. Our findings suggest that prompt design significantly influences the downstream accuracy of LLMs and that LLMs can provide valuable feedback to physicians, challenging incorrect diagnoses and contributing to more accurate decision-making. For example, when the physician is accurate 38% of the time, Mistral can produce the correct answer, improving accuracy up to 74% depending on the prompt being used, while Llama2 and Meditron models
&lt;/p&gt;</description></item><item><title>MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;</title><link>https://arxiv.org/abs/2403.17141</link><description>&lt;p&gt;
MetaAligner&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#36890;&#29992;&#22810;&#30446;&#26631;&#23545;&#40784;&#30340;&#26465;&#20214;&#20174;&#24369;&#21040;&#24378;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17141
&lt;/p&gt;
&lt;p&gt;
MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#26088;&#22312;&#36890;&#36807;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26469;&#35299;&#20915;&#24322;&#36136;&#20154;&#31867;&#26399;&#26395;&#21644;&#20215;&#20540;&#35266;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#21463;&#21040;&#31574;&#30053;&#27169;&#22411;&#30340;&#21442;&#25968;&#38480;&#21046;&#65292;&#23548;&#33268;&#20004;&#20010;&#20851;&#38190;&#23616;&#38480;&#24615;&#65306;&#65288;1&#65289;&#23427;&#20204;&#30340;&#23545;&#40784;&#31639;&#27861;&#23545;&#20110;&#27599;&#20010;&#26032;&#30446;&#26631;&#27169;&#22411;&#30340;&#37325;&#22797;&#25104;&#26412;&#24456;&#39640;&#65307;&#65288;2&#65289;&#30001;&#20110;&#20854;&#38745;&#24577;&#23545;&#40784;&#30446;&#26631;&#65292;&#23427;&#20204;&#26080;&#27861;&#25193;&#23637;&#21040;&#26410;&#35265;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-Objective Aligner&#65288;MetaAligner&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25191;&#34892;&#26465;&#20214;&#20174;&#24369;&#21040;&#24378;&#26657;&#27491;&#20197;&#36924;&#36817;&#24378;&#21709;&#24212;&#30340;&#27169;&#22411;&#12290;MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MetaAligner&#21462;&#24471;&#20102;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17141v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) aim to tackle heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are parameter-adherent to the policy model, leading to two key limitations: (1) the high-cost repetition of their alignment algorithms for each new target model; (2) they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), a model that performs conditional weak-to-strong correction for weak responses to approach strong responses. MetaAligner is the first policy-agnostic and generalizable method for multi-objective preference alignment, which enables plug-and-play alignment by decoupling parameter updates from the policy models and facilitates zero-shot preference alignment for unseen objectives via in-context learning. Experimental results show that MetaAligner achieves sign
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27169;&#22411;&#33976;&#39311;&#30340;&#19968;&#33324;&#29702;&#35770;&#65292;&#36890;&#36807;PAC-&#33976;&#39311;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#25277;&#21462;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26435;&#37325;&#30693;&#35782;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#33976;&#39311;&#27604;&#20174;&#22836;&#23398;&#20064;&#26356;&#20415;&#23452;&#19988;&#26377;&#21161;&#20110;&#29702;&#35299;&#20854;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09053</link><description>&lt;p&gt;
&#36208;&#21521;&#27169;&#22411;&#33976;&#39311;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Towards a theory of model distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09053
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22411;&#33976;&#39311;&#30340;&#19968;&#33324;&#29702;&#35770;&#65292;&#36890;&#36807;PAC-&#33976;&#39311;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#25277;&#21462;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26435;&#37325;&#30693;&#35782;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#33976;&#39311;&#27604;&#20174;&#22836;&#23398;&#20064;&#26356;&#20415;&#23452;&#19988;&#26377;&#21161;&#20110;&#29702;&#35299;&#20854;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33976;&#39311;&#26159;&#23558;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26367;&#25442;&#20026;&#31616;&#21270;&#27169;&#22411;&#26469;&#36817;&#20284;&#21407;&#27169;&#22411;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#20851;&#20110;&#27169;&#22411;&#33976;&#39311;&#30340;&#31243;&#24230;&#12289;&#25152;&#38656;&#36816;&#34892;&#26102;&#38388;&#21644;&#25968;&#25454;&#37327;&#30340;&#22522;&#26412;&#38382;&#39064;&#20173;&#28982;&#22823;&#22810;&#26410;&#35299;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#22987;&#20102;&#33976;&#39311;&#30340;&#19968;&#33324;&#29702;&#35770;&#65292;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;PAC-&#33976;&#39311; [Val84]&#65292;&#25552;&#20986;&#20102;&#25552;&#21462;&#35757;&#32451;&#26435;&#37325;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#30340;&#26032;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#8220;&#32447;&#24615;&#34920;&#31034;&#20551;&#35774;&#8221;&#23558;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#22320;&#33976;&#39311;&#25104;&#31616;&#26126;&#26126;&#20102;&#30340;&#20915;&#31574;&#26641;&#34920;&#31034;&#65292;&#36824;&#35777;&#26126;&#20102;&#33976;&#39311;&#21487;&#20197;&#27604;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#20415;&#23452;&#24471;&#22810;&#65292;&#24182;&#22312;&#34920;&#24449;&#20854;&#22797;&#26434;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09053v1 Announce Type: cross  Abstract: Distillation is the task of replacing a complicated machine learning model with a simpler model that approximates the original [BCNM06,HVD15]. Despite many practical applications, basic questions about the extent to which models can be distilled, and the runtime and amount of data needed to distill, remain largely open.   To study these questions, we initiate a general theory of distillation, defining PAC-distillation in an analogous way to PAC-learning [Val84]. As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently distill neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that distillation can be much cheaper than learning from scratch, and make progress on characterizing its complexity.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#26469;&#21033;&#29992;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;</title><link>https://arxiv.org/abs/2403.06659</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#27979;&#35797;&#26102;&#20020;&#24202;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06659
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#26469;&#21033;&#29992;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#20020;&#24202;&#23454;&#36341;&#20013;&#29992;&#20110;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#30142;&#30149;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#12290;&#22312;&#26410;&#32463;&#27880;&#37322;&#30340;ECG&#25968;&#25454;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;eSSL&#65289;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#34920;&#24449;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#21487;&#20197;&#22312;&#25253;&#21578;&#20013;&#25214;&#21040;&#30340;&#20020;&#24202;&#30693;&#35782;&#12290;&#26412;&#25991;&#36890;&#36807;&#22810;&#27169;&#24577;&#23398;&#20064;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#65292;&#25552;&#20986;&#20102;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#38646;&#26679;&#26412;ECG&#20998;&#31867;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21033;&#29992;&#22806;&#37096;&#19987;&#23478;&#39564;&#35777;&#30340;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#65292;&#29983;&#25104;&#26356;&#22810;&#20851;&#20110;&#24739;&#32773;&#30149;&#21490;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06659v1 Announce Type: cross  Abstract: Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports. This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility. In this work, we address these issues with the Multimodal ECG Representation Learning (MERL}) framework. Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks. At test time, we propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04306</link><description>&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Effectiveness Assessment of Recent Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#20986;&#29616;&#20195;&#34920;&#30528;&#36808;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#31243;&#24230;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#27969;&#34892;&#30340;LVLMs&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#22312;&#19987;&#19994;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#37327;&#36523;&#23450;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#33258;&#28982;&#12289;&#21307;&#30103;&#21644;&#24037;&#19994;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#30340;&#20840;&#38754;&#27979;&#35797;&#24179;&#21488;&#65292;&#28085;&#30422;&#20845;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#26174;&#33879;&#12289;&#20266;&#35013;&#21644;&#36879;&#26126;&#29289;&#20307;&#26816;&#27979;&#65292;&#20197;&#21450;&#24687;&#32905;&#21644;&#30382;&#32932;&#30149;&#21464;&#26816;&#27979;&#65292;&#20197;&#21450;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#26368;&#36817;&#19977;&#31181;&#24320;&#28304;LVLMs--MiniGPT-v2&#12289;LLaVA-1.5&#21644;Shikra--&#22312;&#35270;&#35273;&#35782;&#21035;&#21644;&#23450;&#20301;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#21306;&#27573;&#30340;&#22270;&#20687;&#34920;&#31034;&#26469;&#24314;&#27169;&#22797;&#26434;&#24615;&#65292;&#19982;&#20043;&#21069;&#22797;&#26434;&#30340;&#22270;&#20687;&#22797;&#26434;&#24615;&#27169;&#22411;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#26082;&#33021;&#27867;&#21270;&#65292;&#21448;&#33021;&#20026;&#29702;&#35770;&#29702;&#35299;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.03134</link><description>&lt;p&gt;
&#22797;&#26434;&#20013;&#30340;&#31616;&#21333;
&lt;/p&gt;
&lt;p&gt;
Simplicity in Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#21306;&#27573;&#30340;&#22270;&#20687;&#34920;&#31034;&#26469;&#24314;&#27169;&#22797;&#26434;&#24615;&#65292;&#19982;&#20043;&#21069;&#22797;&#26434;&#30340;&#22270;&#20687;&#22797;&#26434;&#24615;&#27169;&#22411;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#26082;&#33021;&#27867;&#21270;&#65292;&#21448;&#33021;&#20026;&#29702;&#35770;&#29702;&#35299;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21050;&#28608;&#30340;&#22797;&#26434;&#24615;&#22312;&#35768;&#22810;&#35748;&#30693;&#29616;&#35937;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#27880;&#24847;&#21147;&#12289;&#21442;&#19982;&#24230;&#12289;&#26131;&#35760;&#24615;&#12289;&#26102;&#38388;&#24863;&#30693;&#21644;&#32654;&#23398;&#35780;&#20215;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#22797;&#26434;&#24615;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#65292;&#35773;&#21050;&#30340;&#26159;&#65292;&#20808;&#21069;&#30340;&#22270;&#20687;&#22797;&#26434;&#24615;&#27169;&#22411;&#30456;&#24403;&#22797;&#26434;&#12290;&#26089;&#20808;&#30340;&#27169;&#22411;&#35797;&#22270;&#23547;&#25214;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#26469;&#35299;&#37322;&#22797;&#26434;&#24615;&#65292;&#20294;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#26159;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#65292;&#22240;&#27492;&#26080;&#27861;&#27867;&#21270;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#22797;&#26434;&#24615;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#35299;&#37322;&#65292;&#24182;&#19988;&#19981;&#25351;&#23548;&#23545;&#38382;&#39064;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#21306;&#27573;&#30340;&#22270;&#20687;&#34920;&#31034;&#26469;&#24314;&#27169;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#27169;&#22411;SAM&#21644;FC-CLIP&#65292;&#26469;&#37327;&#21270;&#22270;&#20687;&#20013;&#30340;&#22810;&#20010;&#31890;&#24230;&#30340;&#21306;&#27573;&#25968;&#37327;&#65292;&#20197;&#21450;&#22270;&#20687;&#20013;&#30340;&#31867;&#21035;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03134v1 Announce Type: cross  Abstract: The complexity of visual stimuli plays an important role in many cognitive phenomena, including attention, engagement, memorability, time perception and aesthetic evaluation. Despite its importance, complexity is poorly understood and ironically, previous models of image complexity have been quite \textit{complex}. There have been many attempts to find handcrafted features that explain complexity, but these features are usually dataset specific, and hence fail to generalise. On the other hand, more recent work has employed deep neural networks to predict complexity, but these models remain difficult to interpret, and do not guide a theoretical understanding of the problem. Here we propose to model complexity using segment-based representations of images. We use state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the number of segments at multiple granularities, and the number of classes in an image respectively. We find 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#32946;&#19987;&#23478;&#26469;&#35780;&#20272;&#25945;&#32946;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;LMs&#20316;&#20026;&#21487;&#38752;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#25351;&#23548;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02795</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21644;&#20248;&#21270;&#25945;&#32946;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Optimizing Educational Content with Large Language Model Judgments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02795
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#32946;&#19987;&#23478;&#26469;&#35780;&#20272;&#25945;&#32946;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;LMs&#20316;&#20026;&#21487;&#38752;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#25351;&#23548;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#26377;&#25928;&#30340;&#25945;&#32946;&#26448;&#26009;&#36890;&#24120;&#38656;&#35201;&#23545;&#23398;&#29983;&#23398;&#20064;&#25104;&#26524;&#36827;&#34892;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38556;&#30861;&#65292;&#19968;&#20010;&#24819;&#27861;&#26159;&#26500;&#24314;&#23398;&#29983;&#23398;&#20064;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#20248;&#21270;&#25945;&#23398;&#26448;&#26009;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#23398;&#20064;&#21160;&#24577;&#30340;&#35748;&#30693;&#36807;&#31243;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20316;&#20026;&#25945;&#32946;&#19987;&#23478;&#26469;&#35780;&#20272;&#21508;&#31181;&#25351;&#23548;&#23545;&#23398;&#20064;&#32467;&#26524;&#24433;&#21709;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#26469;&#35780;&#20272;&#25351;&#23548;&#26448;&#26009;&#23545;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#30340;&#25972;&#20307;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#22797;&#21046;&#35832;&#22914;&#19987;&#19994;&#36870;&#36716;&#25928;&#24212;&#21644;&#21464;&#24322;&#25928;&#24212;&#31561;&#24050;&#32463;&#24314;&#31435;&#30340;&#25945;&#32946;&#21457;&#29616;&#12290;&#36825;&#23637;&#31034;&#20102;LMs&#20316;&#20026;&#25945;&#32946;&#20869;&#23481;&#21487;&#38752;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#25351;&#23548;&#20248;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;LM&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02795v1 Announce Type: new  Abstract: Creating effective educational materials generally requires expensive and time-consuming studies of student learning outcomes. To overcome this barrier, one idea is to build computational models of student learning and use them to optimize instructional materials. However, it is difficult to model the cognitive processes of learning dynamics. We propose an alternative approach that uses Language Models (LMs) as educational experts to assess the impact of various instructions on learning outcomes. Specifically, we use GPT-3.5 to evaluate the overall effect of instructional materials on different student groups and find that it can replicate well-established educational findings such as the Expertise Reversal Effect and the Variability Effect. This demonstrates the potential of LMs as reliable evaluators of educational content. Building on this insight, we introduce an instruction optimization approach in which one LM generates instruction
&lt;/p&gt;</description></item><item><title>&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#20316;&#29992;&#65292;&#26159;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#35780;&#20272;&#20854;&#24433;&#21709;&#26469;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#30340;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02622</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#19968;&#39033;&#21021;&#27493;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
World Models for Autonomous Driving: An Initial Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02622
&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#20316;&#29992;&#65292;&#26159;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#35780;&#20272;&#20854;&#24433;&#21709;&#26469;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#30340;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#39550;&#39542;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24182;&#35780;&#20272;&#20854;&#24433;&#21709;&#23545;&#20110;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65292;&#20851;&#38190;&#22320;&#24110;&#21161;&#20915;&#31574;&#36807;&#31243;&#12290;&#19990;&#30028;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#20351;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#33021;&#22815;&#32508;&#21512;&#21644;&#35299;&#37322;&#22823;&#37327;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#20174;&#32780;&#39044;&#27979;&#28508;&#22312;&#30340;&#26410;&#26469;&#24773;&#26223;&#24182;&#24357;&#34917;&#20449;&#24687;&#32570;&#21475;&#12290;&#26412;&#25991;&#23545;&#33258;&#20027;&#39550;&#39542;&#20013;&#19990;&#30028;&#27169;&#22411;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#21457;&#23637;&#36827;&#34892;&#20102;&#21021;&#27493;&#23457;&#26597;&#65292;&#28085;&#30422;&#20102;&#20854;&#29702;&#35770;&#22522;&#30784;&#12289;&#23454;&#38469;&#24212;&#29992;&#20197;&#21450;&#26088;&#22312;&#20811;&#26381;&#29616;&#26377;&#38480;&#21046;&#30340;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#24378;&#35843;&#20102;&#19990;&#30028;&#27169;&#22411;&#22312;&#25512;&#21160;&#33258;&#20027;&#39550;&#39542;&#25216;&#26415;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#25104;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#22522;&#30784;&#21442;&#32771;&#65292;&#20415;&#20110;&#24555;&#36895;&#33719;&#24471;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02622v1 Announce Type: cross  Abstract: In the rapidly evolving landscape of autonomous driving, the capability to accurately predict future events and assess their implications is paramount for both safety and efficiency, critically aiding the decision-making process. World models have emerged as a transformative approach, enabling autonomous driving systems to synthesize and interpret vast amounts of sensor data, thereby predicting potential future scenarios and compensating for information gaps. This paper provides an initial review of the current state and prospective advancements of world models in autonomous driving, spanning their theoretical underpinnings, practical applications, and the ongoing research efforts aimed at overcoming existing limitations. Highlighting the significant role of world models in advancing autonomous driving technologies, this survey aspires to serve as a foundational reference for the research community, facilitating swift access to and com
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#24212;&#29992;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#20197;&#21152;&#36895;&#37327;&#21270;LLM&#25512;&#29702;&#36807;&#31243;&#30340;&#19968;&#39033;&#21021;&#27493;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.01384</link><description>&lt;p&gt;
&#20851;&#20110;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#21387;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Compressibility of Quantized Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01384
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#24212;&#29992;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#20197;&#21152;&#36895;&#37327;&#21270;LLM&#25512;&#29702;&#36807;&#31243;&#30340;&#19968;&#39033;&#21021;&#27493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21040;&#36793;&#32536;&#25110;&#31227;&#21160;&#35774;&#22791;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#22914;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#21644;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#24212;&#29992;&#20110;&#20943;&#23569;&#25968;&#25454;&#31227;&#21160;&#65292;&#20174;&#32780;&#21152;&#36895;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#37327;&#21270;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01384v1 Announce Type: cross  Abstract: Deploying Large Language Models (LLMs) on edge or mobile devices offers significant benefits, such as enhanced data privacy and real-time processing capabilities. However, it also faces critical challenges due to the substantial memory requirement of LLMs. Quantization is an effective way of reducing the model size while maintaining good performance. However, even after quantization, LLMs may still be too big to fit entirely into the limited memory of edge or mobile devices and have to be partially loaded from the storage to complete the inference. In this case, the I/O latency of model loading becomes the bottleneck of the LLM inference latency. In this work, we take a preliminary step of studying applying data compression techniques to reduce data movement and thus speed up the inference of quantized LLM on memory-constrained devices. In particular, we discussed the compressibility of quantized LLMs, the trade-off between the compres
&lt;/p&gt;</description></item><item><title>CoTeX&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#26469;&#20419;&#36827;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#28860;LLMs&#30340;&#33021;&#21147;&#20026;&#22788;&#29702;&#38750;&#24179;&#34892;&#25968;&#25454;&#21644;&#24179;&#34892;&#25968;&#25454;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#30417;&#30563;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36879;&#26126;&#30340;&#35299;&#37322;&#22312;&#39118;&#26684;&#36716;&#31227;&#36807;&#31243;&#20013;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01106</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#25105;&#35299;&#37322;&#25552;&#28860;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Distilling Text Style Transfer With Self-Explanation From LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01106
&lt;/p&gt;
&lt;p&gt;
CoTeX&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#26469;&#20419;&#36827;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#28860;LLMs&#30340;&#33021;&#21147;&#20026;&#22788;&#29702;&#38750;&#24179;&#34892;&#25968;&#25454;&#21644;&#24179;&#34892;&#25968;&#25454;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#30417;&#30563;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36879;&#26126;&#30340;&#35299;&#37322;&#22312;&#39118;&#26684;&#36716;&#31227;&#36807;&#31243;&#20013;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#65288;TST&#65289;&#26088;&#22312;&#25913;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#21516;&#26102;&#20445;&#30041;&#20854;&#26680;&#24515;&#20869;&#23481;&#12290;&#37492;&#20110;TST&#30340;&#26377;&#38480;&#24179;&#34892;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoTeX&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#26469;&#20419;&#36827;TST&#30340;&#26694;&#26550;&#12290;CoTeX&#23558;LLMs&#30340;&#22797;&#26434;&#37325;&#20889;&#21644;&#25512;&#29702;&#33021;&#21147;&#25552;&#28860;&#25104;&#26356;&#31616;&#21270;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#38750;&#24179;&#34892;&#25968;&#25454;&#21644;&#24179;&#34892;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#22235;&#20010;TST&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;CoTeX&#26174;&#31034;&#20986;&#36229;&#36234;&#20256;&#32479;&#30417;&#30563;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23558;CoTeX&#19982;&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#12289;&#30417;&#30563;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25216;&#26415;&#20197;&#21450;&#25351;&#23548;&#35843;&#25972;&#30340;LLMs&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;CoTeX&#36890;&#36807;&#25552;&#20379;&#36879;&#26126;&#30340;&#35299;&#37322;&#20854;&#39118;&#26684;&#36716;&#31227;&#36807;&#31243;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01106v1 Announce Type: cross  Abstract: Text Style Transfer (TST) seeks to alter the style of text while retaining its core content. Given the constraints of limited parallel datasets for TST, we propose CoTeX, a framework that leverages large language models (LLMs) alongside chain-of-thought (CoT) prompting to facilitate TST. CoTeX distills the complex rewriting and reasoning capabilities of LLMs into more streamlined models capable of working with both non-parallel and parallel data. Through experimentation across four TST datasets, CoTeX is shown to surpass traditional supervised fine-tuning and knowledge distillation methods, particularly in low-resource settings. We conduct a comprehensive evaluation, comparing CoTeX against current unsupervised, supervised, in-context learning (ICL) techniques, and instruction-tuned LLMs. Furthermore, CoTeX distinguishes itself by offering transparent explanations for its style transfer process.
&lt;/p&gt;</description></item><item><title>PEM&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;&#39640;&#25928;MaskFormer&#65292;&#36890;&#36807;&#24341;&#20837;&#21407;&#22411;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#39640;&#25928;&#36816;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.19422</link><description>&lt;p&gt;
PEM&#65306;&#22522;&#20110;&#21407;&#22411;&#30340;&#39640;&#25928;MaskFormer&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
PEM: Prototype-based Efficient MaskFormer for Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19422
&lt;/p&gt;
&lt;p&gt;
PEM&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;&#39640;&#25928;MaskFormer&#65292;&#36890;&#36807;&#24341;&#20837;&#21407;&#22411;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#39640;&#25928;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#22312;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#65292;&#23427;&#20204;&#22312;&#22810;&#20010;&#20998;&#21106;&#20219;&#21153;&#20013;&#33719;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#22914;&#35821;&#20041;&#20998;&#21106;&#21644;&#20840;&#26223;&#20998;&#21106;&#65292;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;&#39640;&#25928;MaskFormer&#65288;PEM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#22810;&#20010;&#20998;&#21106;&#20219;&#21153;&#20013;&#36816;&#34892;&#30340;&#39640;&#25928;transformer&#26550;&#26500;&#12290;PEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#21033;&#29992;&#35270;&#35273;&#29305;&#24449;&#30340;&#20887;&#20313;&#24615;&#26469;&#38480;&#21046;&#35745;&#31639;&#24182;&#25552;&#39640;&#25928;&#29575;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;PEM&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#33021;&#22815;&#25552;&#21462;&#20855;&#26377;&#39640;&#35821;&#20041;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19422v1 Announce Type: cross  Abstract: Recent transformer-based architectures have shown impressive results in the field of image segmentation. Thanks to their flexibility, they obtain outstanding performance in multiple segmentation tasks, such as semantic and panoptic, under a single unified framework. To achieve such impressive performance, these architectures employ intensive operations and require substantial computational resources, which are often not available, especially on edge devices. To fill this gap, we propose Prototype-based Efficient MaskFormer (PEM), an efficient transformer-based architecture that can operate in multiple segmentation tasks. PEM proposes a novel prototype-based cross-attention which leverages the redundancy of visual features to restrict the computation and improve the efficiency without harming the performance. In addition, PEM introduces an efficient multi-scale feature pyramid network, capable of extracting features that have high seman
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.19379</link><description>&lt;p&gt;
&#30789;&#35895;&#20154;&#32676;&#30340;&#26234;&#24935;&#65306;LLM&#38598;&#25104;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#20154;&#32676;&#20934;&#30830;&#29575;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#25928;&#24212;&#65292;&#21363;&#36890;&#36807;&#32858;&#21512;&#19968;&#32676;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#36807;&#21435;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#21069;&#27839;LLMs&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#20154;&#31867;&#32676;&#20307;&#39044;&#27979;&#27604;&#36187;&#30340;&#40644;&#37329;&#26631;&#20934;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30001;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;31&#20010;&#20108;&#20803;&#38382;&#39064;&#30340;&#32858;&#21512;LLM&#39044;&#27979;&#19982;&#19968;&#20010;&#26469;&#33258;&#19977;&#20010;&#26376;&#39044;&#27979;&#27604;&#36187;&#30340;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#32676;&#20307;&#30340;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#31181;&#39034;&#20174;&#25928;&#24212;&#65292;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26126;&#26174;&#39640;&#20110;50%&#65292;&#23613;&#31649;&#20960;&#20046;&#26159;&#24179;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.16310</link><description>&lt;p&gt;
REPLAY: &#23545;&#31232;&#30095;&#36712;&#36857;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#32622;&#39044;&#27979;&#26159;&#26681;&#25454;&#21382;&#21490;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#26469;&#39044;&#27979;&#29992;&#25143;&#20301;&#32622;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#30340;&#22266;&#26377;&#31232;&#30095;&#38382;&#39064;&#65292;&#26102;&#31354;&#19978;&#19979;&#25991;&#34987;&#35777;&#26126;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#26159;&#23558;&#20301;&#32622;&#20043;&#38388;&#30340;&#26102;&#31354;&#36317;&#31163;&#32435;&#20837;&#21040;&#31227;&#21160;&#36712;&#36857;&#20013;&#65292;&#35201;&#20040;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#38468;&#21152;&#36755;&#20837;&#25552;&#20379;&#32473;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#26469;&#23547;&#25214;&#26377;&#20449;&#24687;&#30340;&#36807;&#21435;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#26410;&#33021;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#20363;&#22914;&#65292;&#20154;&#31867;&#31227;&#21160;&#22312;&#26089;&#26216;&#36890;&#24120;&#27604;&#20854;&#20182;&#26102;&#38388;&#26356;&#26377;&#35268;&#24459;&#65307;&#36825;&#26263;&#31034;&#20102;&#23454;&#38469;&#26102;&#38388;&#25139;&#30340;&#26377;&#29992;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#32972;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REPLAY&#65292;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;RNN&#26550;&#26500;&#65292;&#26088;&#22312;&#25429;&#25417;&#26102;&#38388;&#21464;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#35268;&#24459;&#20197;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16310v1 Announce Type: cross  Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;BERT&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#24066;&#22330;&#19979;&#25387;&#24182;&#31283;&#23450;&#22238;&#25253;&#12290;</title><link>https://arxiv.org/abs/2402.10481</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#30340;&#21152;&#23494;&#36164;&#20135;&#24066;&#22330;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Emoji Driven Crypto Assets Market Reactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;BERT&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#24066;&#22330;&#19979;&#25387;&#24182;&#31283;&#23450;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21152;&#23494;&#36135;&#24065;&#39046;&#22495;&#65292;&#35832;&#22914;Twitter&#20043;&#31867;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24050;&#32463;&#25104;&#20026;&#24433;&#21709;&#24066;&#22330;&#36235;&#21183;&#21644;&#25237;&#36164;&#32773;&#24773;&#32490;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#23545;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#34920;&#24773;&#31526;&#21495;&#36716;&#21270;&#20026;&#21487;&#37327;&#21270;&#30340;&#24773;&#24863;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#35265;&#35299;&#19982;BTC&#20215;&#26684;&#21644;VCRIX&#25351;&#25968;&#31561;&#20851;&#38190;&#24066;&#22330;&#25351;&#26631;&#36827;&#34892;&#20102;&#30456;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26088;&#22312;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#20803;&#32032;&#35782;&#21035;&#21644;&#39044;&#27979;&#24066;&#22330;&#36235;&#21183;&#30340;&#20132;&#26131;&#31574;&#30053;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#26377;&#21161;&#20110;&#36991;&#20813;&#37325;&#22823;&#24066;&#22330;&#19979;&#25387;&#65292;&#24182;&#26377;&#21161;&#20110;&#22238;&#25253;&#30340;&#31283;&#23450;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#23558;&#20808;&#36827;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20998;&#26512;&#25972;&#21512;&#21040;&#37329;&#34701;&#31574;&#30053;&#20013;&#30340;&#23454;&#38469;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#30475;&#24453;&#24066;&#22330;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10481v1 Announce Type: cross  Abstract: In the burgeoning realm of cryptocurrency, social media platforms like Twitter have become pivotal in influencing market trends and investor sentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based BERT model for a multimodal sentiment analysis, focusing on the impact of emoji sentiment on cryptocurrency markets. By translating emojis into quantifiable sentiment data, we correlate these insights with key market indicators like BTC Price and the VCRIX index. This approach may be fed into the development of trading strategies aimed at utilizing social media elements to identify and forecast market trends. Crucially, our findings suggest that strategies based on emoji sentiment can facilitate the avoidance of significant market downturns and contribute to the stabilization of returns. This research underscores the practical benefits of integrating advanced AI-driven analyses into financial strategies, offering a nuan
&lt;/p&gt;</description></item><item><title>&#22270;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#26412;&#25991;&#21457;&#29616;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#36127;&#36801;&#31227;&#26222;&#36941;&#23384;&#22312;&#65292;&#21363;&#20351;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65292;&#23545;&#20110;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#65292;&#32467;&#26500;&#24046;&#24322;&#23545;&#23376;&#22270;&#23884;&#20837;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2402.08907</link><description>&lt;p&gt;
&#35299;&#20915;&#22270;&#19978;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Negative Transfer on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08907
&lt;/p&gt;
&lt;p&gt;
&#22270;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#26412;&#25991;&#21457;&#29616;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#36127;&#36801;&#31227;&#26222;&#36941;&#23384;&#22312;&#65292;&#21363;&#20351;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65292;&#23545;&#20110;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#65292;&#32467;&#26500;&#24046;&#24322;&#23545;&#23376;&#22270;&#23884;&#20837;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#20851;&#31995;&#19981;&#23494;&#20999;&#26102;&#65292;&#23398;&#20064;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#19981;&#21033;&#24433;&#21709;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36127;&#36801;&#31227;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#65292;&#19982;&#22270;&#20687;&#25110;&#25991;&#26412;&#19981;&#21516;&#65292;&#36127;&#36801;&#31227;&#32463;&#24120;&#21457;&#29983;&#65292;&#21363;&#20351;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#22312;&#35821;&#20041;&#19978;&#26377;&#30456;&#20284;&#20043;&#22788;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#32467;&#26500;&#24046;&#24322;&#20250;&#22823;&#22823;&#22686;&#24378;&#22270;&#20013;&#33410;&#28857;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24102;&#26469;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65306;&#23545;&#20110;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#65292;&#23613;&#31649;&#32467;&#26500;&#24046;&#24322;&#20250;&#23548;&#33268;&#33410;&#28857;&#23884;&#20837;&#30340;&#20998;&#24067;&#24046;&#24322;&#65292;&#20294;&#23427;&#20204;&#23545;&#23376;&#22270;&#23884;&#20837;&#30340;&#24433;&#21709;&#21487;&#33021;&#36739;&#23567;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;tw
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08907v1 Announce Type: cross Abstract: Transfer learning aims to boost the learning on the target task leveraging knowledge learned from other relevant tasks. However, when the source and target are not closely related, the learning performance may be adversely affected, a phenomenon known as negative transfer. In this paper, we investigate the negative transfer in graph transfer learning, which is important yet underexplored. We reveal that, unlike image or text, negative transfer commonly occurs in graph-structured data, even when source and target graphs share semantic similarities. Specifically, we identify that structural differences significantly amplify the dissimilarities in the node embeddings across graphs. To mitigate this, we bring a new insight: for semantically similar graphs, although structural differences lead to significant distribution shift in node embeddings, their impact on subgraph embeddings could be marginal. Building on this insight, we introduce tw
&lt;/p&gt;</description></item><item><title>CEHR-GPT&#26159;&#19968;&#31181;&#20351;&#29992;&#30149;&#20154;&#26102;&#38388;&#36724;&#29983;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;EHR&#25968;&#25454;&#30340;&#21512;&#25104;&#12289;&#30142;&#30149;&#36827;&#23637;&#20998;&#26512;&#31561;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.04400</link><description>&lt;p&gt;
&#29983;&#25104;&#24102;&#26377;&#30149;&#20154;&#26102;&#38388;&#36724;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;CEHR-GPT
&lt;/p&gt;
&lt;p&gt;
CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04400
&lt;/p&gt;
&lt;p&gt;
CEHR-GPT&#26159;&#19968;&#31181;&#20351;&#29992;&#30149;&#20154;&#26102;&#38388;&#36724;&#29983;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;EHR&#25968;&#25454;&#30340;&#21512;&#25104;&#12289;&#30142;&#30149;&#36827;&#23637;&#20998;&#26512;&#31561;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#24050;&#25104;&#20026;&#25512;&#36827;&#21307;&#30103;&#24212;&#29992;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27809;&#26377;&#30452;&#25509;&#35775;&#38382;&#21307;&#30103;&#25968;&#25454;&#30340;&#30740;&#31350;&#20154;&#21592;&#32780;&#35328;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#30495;&#23454;&#19990;&#30028;EHR&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#20351;&#29992;&#34920;&#26684;&#26684;&#24335;&#65292;&#24573;&#30053;&#20102;&#30149;&#20154;&#21382;&#21490;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#38480;&#21046;&#20102;&#25968;&#25454;&#22797;&#21046;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#26469;&#22788;&#29702;EHR&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#30142;&#30149;&#36827;&#23637;&#20998;&#26512;&#12289;&#20154;&#21475;&#20272;&#35745;&#12289;&#21453;&#20107;&#23454;&#25512;&#29702;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#28436;&#31034;&#20102;&#20351;&#29992;&#28304;&#33258;CEHR-BERT&#30340;&#29305;&#23450;&#30149;&#20154;&#34920;&#31034;&#35757;&#32451;GPT&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#33021;&#22815;&#29983;&#25104;&#21487;&#26080;&#32541;&#36716;&#25442;&#30340;&#30149;&#20154;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in advancing healthcare applications and machine learning models, particularly for researchers without direct access to healthcare data. Although existing methods, like rule-based approaches and generative adversarial networks (GANs), generate synthetic data that resembles real-world EHR data, these methods often use a tabular format, disregarding temporal dependencies in patient histories and limiting data replication. Recently, there has been a growing interest in leveraging Generative Pre-trained Transformers (GPT) for EHR data. This enables applications like disease progression analysis, population estimation, counterfactual reasoning, and synthetic data generation. In this work, we focus on synthetic data generation and demonstrate the capability of training a GPT model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences that can be seamlessly converted 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17791</link><description>&lt;p&gt;
&#19981;&#24102;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Transformers without Positional Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20301;&#32622;&#32534;&#30721;&#30340;&#22270;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#21464;&#21387;&#22120;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#21333;&#29420;&#20351;&#29992;&#36824;&#26159;&#19982;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MP-GNN&#65289;&#32467;&#21512;&#12290;&#23558;&#22270;&#24402;&#32435;&#20559;&#35265;&#34701;&#20837;&#22825;&#28982;&#19982;&#32467;&#26500;&#26080;&#20851;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20197;&#32467;&#26500;&#25110;&#20301;&#32622;&#32534;&#30721;&#65288;PEs&#65289;&#30340;&#24418;&#24335;&#65292;&#26159;&#23454;&#29616;&#36825;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#36825;&#26679;&#30340;&#32534;&#30721;&#26159;&#26840;&#25163;&#30340;&#65292;&#20154;&#20204;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#23581;&#35797;&#26469;&#35774;&#35745;&#36825;&#26679;&#30340;&#32534;&#30721;&#65292;&#21253;&#25324;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#12289;&#30456;&#23545;&#38543;&#26426;&#34892;&#36208;&#27010;&#29575;&#65288;RRWP&#65289;&#12289;&#31354;&#38388;&#32534;&#30721;&#12289;&#20013;&#24515;&#24230;&#32534;&#30721;&#12289;&#36793;&#32536;&#32534;&#30721;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#32534;&#30721;&#21487;&#33021;&#26681;&#26412;&#19981;&#38656;&#35201;&#65292;&#21482;&#35201;&#27880;&#24847;&#26426;&#21046;&#26412;&#36523;&#21253;&#21547;&#26377;&#20851;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Eigenformer&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#35889;&#24863;&#30693;&#27880;&#24847;&#26426;&#21046;&#65292;&#20102;&#35299;&#22270;&#30340;&#25289;&#26222;&#25289;&#26031;&#35889;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs). Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results. However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc. In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure. We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;IBRL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#26679;&#26412;-efficient RL&#65292;&#36890;&#36807;&#20808;&#22312;&#25552;&#20379;&#30340;&#31034;&#33539;&#19978;&#35757;&#32451;IL&#31574;&#30053;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#25552;&#20986;&#26367;&#20195;&#21160;&#20316;&#36827;&#34892;&#22312;&#32447;&#25506;&#32034;&#21644;&#24341;&#23548;&#30446;&#26631;&#20540;&#12290;</title><link>https://arxiv.org/abs/2311.02198</link><description>&lt;p&gt;
&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Bootstrapped Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02198
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;IBRL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#26679;&#26412;-efficient RL&#65292;&#36890;&#36807;&#20808;&#22312;&#25552;&#20379;&#30340;&#31034;&#33539;&#19978;&#35757;&#32451;IL&#31574;&#30053;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#25552;&#20986;&#26367;&#20195;&#21160;&#20316;&#36827;&#34892;&#22312;&#32447;&#25506;&#32034;&#21644;&#24341;&#23548;&#30446;&#26631;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#20027;&#35201;&#20381;&#36182;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#26159;&#22240;&#20026;&#20854;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#33021;&#20351;IL&#25512;&#24191;&#21040;&#25152;&#26377;&#21487;&#33021;&#22330;&#26223;&#30340;&#20840;&#38754;&#19987;&#23478;&#28436;&#31034;&#26159;&#26114;&#36149;&#30340;&#65292;&#20219;&#20309;&#20998;&#24067;&#30340;&#36716;&#21464;&#37117;&#38656;&#35201;&#37325;&#26032;&#25910;&#38598;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;RL&#21487;&#20197;&#24314;&#31435;&#22312;IL&#30340;&#22522;&#30784;&#19978;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#25105;&#25913;&#36827;&#31243;&#24207;&#65292;&#37027;&#20040;&#23427;&#23558;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;IBRL&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20855;&#26377;&#31034;&#33539;&#30340;&#39640;&#25928;&#25277;&#26679;RL&#65292;&#39318;&#20808;&#22312;&#25552;&#20379;&#30340;&#31034;&#33539;&#19978;&#35757;&#32451;IL&#31574;&#30053;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#25552;&#20986;&#26367;&#20195;&#21160;&#20316;&#36827;&#34892;&#22312;&#32447;&#25506;&#32034;&#21644;&#24341;&#23548;&#30446;&#26631;&#20540;&#12290;&#19982;&#20808;&#21069;&#36807;&#24230;&#37319;&#26679;&#31034;&#33539;&#25110;&#29992;&#39069;&#22806;&#30340;&#27169;&#20223;&#25439;&#22833;&#23545;RL&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;IBRL&#33021;&#22815;&#21033;&#29992;&#26469;&#33258;IL&#30340;&#39640;&#36136;&#37327;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02198v4 Announce Type: replace-cross  Abstract: Despite the considerable potential of reinforcement learning (RL), robotic control tasks predominantly rely on imitation learning (IL) due to its better sample efficiency. However, it is costly to collect comprehensive expert demonstrations that enable IL to generalize to all possible scenarios, and any distribution shift would require recollecting data for finetuning. Therefore, RL is appealing if it can build upon IL as an efficient autonomous self-improvement procedure. We propose imitation bootstrapped reinforcement learning (IBRL), a novel framework for sample-efficient RL with demonstrations that first trains an IL policy on the provided demonstrations and then uses it to propose alternative actions for both online exploration and bootstrapping target values. Compared to prior works that oversample the demonstrations or regularize RL with an additional imitation loss, IBRL is able to utilize high quality actions from IL p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#25429;&#25417;&#20102;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2204.04510</link><description>&lt;p&gt;
&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#35753;&#31616;&#21333;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#19978;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.04510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#25429;&#25417;&#20102;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#24120;&#20351;&#29992;&#19987;&#38376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#22823;&#22411;&#20840;&#23616;&#22270;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#20294;&#25361;&#25112;&#23376;&#22270;&#30340;&#23618;&#27425;&#32467;&#26500;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23376;&#22270;&#21040;&#33410;&#28857;&#65288;S2N&#65289;&#36716;&#25442;&#30340;&#26032;&#39062;&#20844;&#24335;&#65292;&#29992;&#20110;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#20840;&#23616;&#22270;&#20013;&#30340;&#19968;&#32452;&#23376;&#22270;&#65292;&#25105;&#20204;&#36890;&#36807;&#31895;&#30053;&#22320;&#23558;&#23376;&#22270;&#36716;&#25442;&#25104;&#33410;&#28857;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#22270;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;S2N&#19981;&#20165;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#19988;&#36890;&#36807;&#25429;&#25417;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#20063;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#23427;&#20204;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#31895;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20063;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35843;&#25972;&#27169;&#22411;&#21518;&#25928;&#26524;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#12290;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;MOCO&#24494;&#35843;&#30340;ResNet50&#30340;&#25928;&#26524;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.09243</link><description>&lt;p&gt;
DiffClone: &#20351;&#29992;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#26426;&#22120;&#20154;&#34892;&#20026;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven Policy Learning. (arXiv:2401.09243v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#12290;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;MOCO&#24494;&#35843;&#30340;ResNet50&#30340;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#20219;&#21153;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#23494;&#38598;&#19988;&#30828;&#20214;&#29305;&#23450;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#25805;&#20316;&#20195;&#29702;&#65292;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#24335;&#38750;&#24120;&#21560;&#24341;&#20154;&#12290;Train-Offline-Test-Online&#65288;TOTO&#65289;&#22522;&#20934;&#25552;&#20379;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#24320;&#28304;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#30001;&#19987;&#23478;&#25968;&#25454;&#32452;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#24120;&#35265;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#22522;&#20934;&#20998;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#65292;&#37319;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#36825;&#20063;&#26159;&#25105;&#20204;&#22312;NeurIPS 2023&#20030;&#21150;&#30340;Train-Offline-Test-Online&#65288;TOTO&#65289;&#22522;&#20934;&#25361;&#25112;&#36187;&#20013;&#30340;&#23448;&#26041;&#25552;&#20132;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#20195;&#29702;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MOCO&#24494;&#35843;&#30340;ResNet50&#30456;&#27604;&#20854;&#20182;&#24494;&#35843;&#26041;&#27861;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning tasks are extremely compute-intensive and hardware-specific. Thus the avenues of tackling these challenges, using a diverse dataset of offline demonstrations that can be used to train robot manipulation agents, is very appealing. The Train-Offline-Test-Online (TOTO) Benchmark provides a well-curated open-source dataset for offline training comprised mostly of expert data and also benchmark scores of the common offline-RL and behaviour cloning agents. In this paper, we introduce DiffClone, an offline algorithm of enhanced behaviour cloning agent with diffusion-based policy learning, and measured the efficacy of our method on real online physical robots at test time. This is also our official submission to the Train-Offline-Test-Online (TOTO) Benchmark Challenge organized at NeurIPS 2023. We experimented with both pre-trained visual representation and agent policies. In our experiments, we find that MOCO finetuned ResNet50 performs the best in comparison to other finetuned
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#35270;&#35273;&#35821;&#35328;&#23884;&#20837;&#22522;&#30784;&#27169;&#22411;&#21040;3D&#39640;&#26031;&#20998;&#21106;&#20013;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.01970</link><description>&lt;p&gt;
FMGS&#65306;&#22522;&#20110;&#23884;&#20837;&#24335;3D&#39640;&#26031;&#20998;&#21106;&#30340;&#20840;&#38754;3D&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding. (arXiv:2401.01970v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01970
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#35270;&#35273;&#35821;&#35328;&#23884;&#20837;&#22522;&#30784;&#27169;&#22411;&#21040;3D&#39640;&#26031;&#20998;&#21106;&#20013;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#24863;&#30693;&#29616;&#23454;&#19990;&#30028;3D&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#35821;&#20041;&#29305;&#24615;&#23545;&#20110;&#22686;&#24378;&#29616;&#23454;&#21644;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#25345;&#32493;&#36827;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FMGS&#65288;Foundation Model Embedded 3D Gaussian Splatting&#65289;&#65292;&#23558;&#35270;&#35273;&#35821;&#35328;&#23884;&#20837;&#22522;&#30784;&#27169;&#22411;&#21040;3D&#39640;&#26031;&#20998;&#21106;&#20013;&#12290;&#26412;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#21644;&#34920;&#31034;3D&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#22522;&#20110;&#22270;&#20687;&#30340;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#30340;&#29305;&#24449;&#22270;&#34701;&#21512;&#21040;&#25105;&#20204;&#30340;3D&#27169;&#22411;&#20013;&#28210;&#26579;&#23454;&#29616;&#30340;&#12290;&#20026;&#20102;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#28210;&#26579;&#21644;&#24555;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#23558;GS&#21644;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32534;&#30721;&#65288;MHE&#65289;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26377;&#25928;&#35757;&#32451;&#36807;&#31243;&#36824;&#24341;&#20837;&#20102;&#20687;&#32032;&#23545;&#40784;&#25439;&#22833;&#65292;&#20351;&#30456;&#21516;&#35821;&#20041;&#23454;&#20307;&#30340;&#28210;&#26579;&#29305;&#24449;&#36317;&#31163;&#25509;&#36817;&#65292;&#36981;&#24490;&#20687;&#32032;&#32423;&#35821;&#20041;&#36793;&#30028;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#22810;&#35270;&#22270;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present \algfull{} (\algname{}), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, faci
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;</title><link>http://arxiv.org/abs/2312.01185</link><description>&lt;p&gt;
&#26102;&#38388;&#20013;&#30340;&#28063;&#28458;&#65306;&#32654;&#22269;&#21382;&#21490;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#22269;&#24773;&#21672;&#25991;&#25968;&#25454;&#38598;&#23545;&#32654;&#22269;&#21382;&#21490;&#30340;&#24635;&#20307;&#26102;&#38388;&#32447;&#21450;&#21672;&#25991;&#26412;&#36523;&#30340;&#29305;&#28857;&#21644;&#24615;&#36136;&#36827;&#34892;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#65288;&#20063;&#26377;&#20123;&#19981;&#37027;&#20040;&#20196;&#20154;&#24778;&#35766;&#65289;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#65292;&#22914;BERT&#65288;DistilBERT&#65289;&#21644;GPT-2&#12290;&#34429;&#28982;&#24191;&#27867;&#35748;&#20026;BERT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#26368;&#36866;&#21512;NLP&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;GPT-2&#32467;&#21512;UMAP&#31561;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#26356;&#24378;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;&#36825;&#20351;&#24471;GPT-2 + UMAP&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#23601;&#36275;&#22815;&#22909;&#29992;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#26469;&#26816;&#27979;&#21738;&#20301;&#24635;&#32479;&#21457;&#34920;&#20102;&#21738;&#31687;&#28436;&#35762;&#65292;&#24182;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#65288;&#20934;&#30830;&#29575;&#20026;93\% - 95\%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#36816;&#34892;&#24773;&#20917;&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;&#20889;&#20316;&#24180;&#20221;&#65292;&#25105;&#20204;&#36824;&#25191;&#34892;&#20102;&#19968;&#20010;&#31867;&#20284;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#32465;&#23450;ID&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#23558;&#23454;&#20307;&#19982;&#23646;&#24615;&#36827;&#34892;&#26377;&#25928;&#22320;&#32465;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#22914;&#20309;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.17191</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23558;&#23454;&#20307;&#32465;&#23450;&#21040;&#19978;&#19979;&#25991;&#20013;?
&lt;/p&gt;
&lt;p&gt;
How do Language Models Bind Entities in Context?. (arXiv:2310.17191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17191
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#32465;&#23450;ID&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#23558;&#23454;&#20307;&#19982;&#23646;&#24615;&#36827;&#34892;&#26377;&#25928;&#22320;&#32465;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#22914;&#20309;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#27491;&#30830;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24517;&#39035;&#23558;&#23454;&#20307;&#19982;&#20854;&#23646;&#24615;&#36827;&#34892;&#32465;&#23450;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#25551;&#36848;&#8220;&#32511;&#33394;&#26041;&#22359;&#8221;&#21644;&#8220;&#34013;&#33394;&#22278;&#24418;&#8221;&#30340;&#19978;&#19979;&#25991;&#65292;LMs&#24517;&#39035;&#23558;&#24418;&#29366;&#19982;&#23427;&#20204;&#23545;&#24212;&#30340;&#39068;&#33394;&#36827;&#34892;&#32465;&#23450;&#12290;&#25105;&#20204;&#20998;&#26512;LM&#34920;&#31034;&#24182;&#30830;&#23450;&#32465;&#23450;ID&#26426;&#21046;&#65306;&#36825;&#26159;&#19968;&#31181;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#25105;&#20204;&#22312;Pythia&#21644;LLaMA&#23478;&#26063;&#30340;&#27599;&#20010;&#36275;&#22815;&#22823;&#30340;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#12290;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LMs&#20869;&#37096;&#28608;&#27963;&#36890;&#36807;&#23558;&#32465;&#23450;ID&#21521;&#37327;&#38468;&#21152;&#21040;&#30456;&#24212;&#30340;&#23454;&#20307;&#21644;&#23646;&#24615;&#19978;&#26469;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#32465;&#23450;ID&#21521;&#37327;&#24418;&#25104;&#36830;&#32493;&#30340;&#23376;&#31354;&#38388;&#65292;&#22312;&#36825;&#20010;&#23376;&#31354;&#38388;&#20013;&#65292;&#32465;&#23450;ID&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#21453;&#26144;&#20102;&#23427;&#20204;&#30340;&#21306;&#21035;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;LMs&#22312;&#19978;&#19979;&#25991;&#20013;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#30340;&#21487;&#35299;&#37322;&#31574;&#30053;&#65292;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;LMs&#20013;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#36830;&#25509;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19988;&#20855;&#26377;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616;&#30340;&#28508;&#21147;&#12290;&#26080;&#35770;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#22914;&#20309;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15872</link><description>&lt;p&gt;
KirchhoffNet&#65306;&#19968;&#31181;&#36830;&#25509;&#28040;&#24687;&#20256;&#36882;&#21644;&#36830;&#32493;&#28145;&#24230;&#27169;&#22411;&#30340;&#30005;&#36335;&#26725;&#25509;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models. (arXiv:2310.15872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#36830;&#25509;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19988;&#20855;&#26377;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616;&#30340;&#28508;&#21147;&#12290;&#26080;&#35770;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#22914;&#20309;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#27169;&#25311;&#30005;&#36335;&#30340;&#22522;&#26412;&#21407;&#29702;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#65292;&#24341;&#20837;&#20102;&#19968;&#31867;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#12290;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#20102;&#23494;&#20999;&#32852;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#20309;&#20256;&#32479;&#23618;&#65288;&#22914;&#21367;&#31215;&#12289;&#27744;&#21270;&#25110;&#32447;&#24615;&#23618;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#35753;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#26356;&#21152;&#26377;&#36259;&#30340;&#26159;&#20854;&#22312;&#30828;&#20214;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#24403;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#37096;&#32626;&#22312;GPU&#19978;&#12290;&#30456;&#21453;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#30005;&#36335;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26080;&#35770;&#22312;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#20869;&#26377;&#22810;&#23569;&#21442;&#25968;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20854;&#20013;f&#34920;&#31034;&#30828;&#20214;&#30340;&#26102;&#38047;&#39057;&#29575;&#12290;&#36825;&#31181;&#29305;&#24615;&#34920;&#26126;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#20855;&#26377;&#28508;&#21147;&#23454;&#29616;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we exploit a fundamental principle of analog electronic circuitry, Kirchhoff's current law, to introduce a unique class of neural network models that we refer to as KirchhoffNet. KirchhoffNet establishes close connections with message passing neural networks and continuous-depth networks. We demonstrate that even in the absence of any traditional layers (such as convolution, pooling, or linear layers), KirchhoffNet attains 98.86% test accuracy on the MNIST dataset, comparable with state of the art (SOTA) results. What makes KirchhoffNet more intriguing is its potential in the realm of hardware. Contemporary deep neural networks are conventionally deployed on GPUs. In contrast, KirchhoffNet can be physically realized by an analog electronic circuit. Moreover, we justify that irrespective of the number of parameters within a KirchhoffNet, its forward calculation can always be completed within 1/f seconds, with f representing the hardware's clock frequency. This characteris
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;LLM-SS&#65292;&#36890;&#36807;&#21512;&#25104;&#23398;&#29983;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#65292;&#20026;&#23398;&#29983;&#24314;&#27169;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#25945;&#23398;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.10690</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#29983;&#24314;&#27169;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20174;&#19968;&#27425;&#24615;&#35266;&#23519;&#20013;&#21512;&#25104;&#35270;&#35273;&#32534;&#31243;&#20013;&#23398;&#29983;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation. (arXiv:2310.10690v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;LLM-SS&#65292;&#36890;&#36807;&#21512;&#25104;&#23398;&#29983;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#65292;&#20026;&#23398;&#29983;&#24314;&#27169;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#24314;&#27169;&#23545;&#20110;&#35768;&#22810;&#25945;&#32946;&#25216;&#26415;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#23398;&#20064;&#32467;&#26524;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#22240;&#20026;&#23398;&#29983;&#34920;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#19988;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#30340;&#23398;&#20064;&#25216;&#33021;&#38598;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LLM-SS&#65292;&#21033;&#29992;LLMs&#21512;&#25104;&#23398;&#29983;&#30340;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#29305;&#23450;&#23398;&#29983;&#22312;&#21442;&#32771;&#20219;&#21153;&#19978;&#30340;&#35299;&#20915;&#23581;&#35797;&#20316;&#20026;&#35266;&#23519;&#65292;&#30446;&#26631;&#26159;&#21512;&#25104;&#35813;&#23398;&#29983;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#19982;&#19981;&#21516;&#30340;LLMs&#32467;&#21512;&#20351;&#29992;&#65307;&#32780;&#19988;&#65292;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#23427;&#20204;&#23545;&#39046;&#22495;&#32972;&#26223;&#21644;&#23398;&#29983;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#20855;&#20307;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Student modeling is central to many educational technologies as it enables the prediction of future learning outcomes and targeted instructional strategies. However, open-ended learning environments pose challenges for accurately modeling students due to the diverse behaviors exhibited by students and the absence of a well-defined set of learning skills. To approach these challenges, we explore the application of Large Language Models (LLMs) for in-context student modeling in open-ended learning environments. We introduce a novel framework, LLM-SS, that leverages LLMs for synthesizing student's behavior. More concretely, given a particular student's solving attempt on a reference task as observation, the goal is to synthesize the student's attempt on a target task. Our framework can be combined with different LLMs; moreover, we fine-tune LLMs using domain-specific expertise to boost their understanding of domain background and student behaviors. We evaluate several concrete methods bas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#27604;&#20363;&#65292;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#22810;&#31181;&#20010;&#20154;&#23646;&#24615;&#65292;&#36825;&#24341;&#21457;&#20102;&#38544;&#31169;&#27844;&#38706;&#30340;&#26032;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2310.07298</link><description>&lt;p&gt;
&#36229;&#36234;&#35760;&#24518;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#26469;&#20405;&#29359;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Beyond Memorization: Violating Privacy Via Inference with Large Language Models. (arXiv:2310.07298v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07298
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#27604;&#20363;&#65292;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#22810;&#31181;&#20010;&#20154;&#23646;&#24615;&#65292;&#36825;&#24341;&#21457;&#20102;&#38544;&#31169;&#27844;&#38706;&#30340;&#26032;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38544;&#31169;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#21462;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#19978;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#24050;&#22823;&#24133;&#22686;&#24378;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#24403;&#21069;&#30340;LLMs&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#20174;&#25512;&#29702;&#26102;&#32473;&#20986;&#30340;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#26469;&#20405;&#29359;&#20010;&#20154;&#38544;&#31169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#39044;&#35757;&#32451;LLMs&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#20010;&#20154;&#23646;&#24615;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;Reddit&#20010;&#20154;&#36164;&#26009;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26174;&#31034;&#24403;&#21069;&#30340;LLMs&#21487;&#20197;&#25512;&#26029;&#20986;&#21508;&#31181;&#21508;&#26679;&#30340;&#20010;&#20154;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;&#20301;&#32622;&#12289;&#25910;&#20837;&#12289;&#24615;&#21035;&#65289;&#65292;&#22312;&#25104;&#26412;&#65288;100&#20493;&#65289;&#21644;&#26102;&#38388;&#65288;240&#20493;&#65289;&#19978;&#20165;&#38656;&#20154;&#31867;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#36798;&#21040;&#20102;&#26368;&#39640;1&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;85&#65285;&#65292;&#26368;&#39640;3&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;95.8&#65285;&#12290;&#38543;&#30528;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#30001;LLM&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20114;&#21160;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20405;&#29359;&#38544;&#31169;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#36890;&#36807;&#20284;&#20046;&#26080;&#20851;&#30340;&#23545;&#35805;&#35797;&#22270;&#25552;&#21462;&#20010;&#20154;&#20449;&#24687;&#30340;&#26032;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and $95.8\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time ($240\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06387</link><description>&lt;p&gt;
&#21482;&#38656;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#21363;&#21487;&#23454;&#29616;&#36234;&#29425;&#21644;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#33539;&#26469;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#31034;&#33539;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#21487;&#20197;&#22686;&#21152;&#25110;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#30340;&#25285;&#24551;&#20063;&#28014;&#29616;&#20986;&#26469;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30456;&#21516;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#25805;&#32437;LLM&#23545;&#40784;&#33021;&#21147;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#33539;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#23601;&#21487;&#20197;&#25805;&#32437;LLM&#22686;&#21152;&#25110;&#38477;&#20302;&#36234;&#29425;&#27010;&#29575;&#65292;&#21363;&#22238;&#31572;&#24694;&#24847;&#25552;&#31034;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30446;&#30340;&#30340;&#30456;&#21516;&#19978;&#19979;&#25991;&#25915;&#20987;&#65288;ICA&#65289;&#21644;&#30456;&#21516;&#19978;&#19979;&#25991;&#38450;&#24481;&#65288;ICD&#65289;&#26041;&#27861;&#12290;ICA&#36890;&#36807;&#26500;&#36896;&#24694;&#24847;&#19978;&#19979;&#25991;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#36755;&#20986;&#65292;&#32780;ICD&#36890;&#36807;&#25298;&#32477;&#22238;&#31572;&#26377;&#23475;&#25552;&#31034;&#30340;&#31034;&#33539;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ICA&#21644;ICD&#22312;&#22686;&#21152;&#25110;&#38477;&#20302;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;ICL&#22312;&#36234;&#29425;&#21644;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01558</link><description>&lt;p&gt;
&#20351;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#20855;&#26377;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Making Retrieval-Augmented Language Models Robust to Irrelevant Context. (arXiv:2310.01558v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;RALMs&#65289;&#26377;&#26395;&#20135;&#29983;&#20934;&#30830;&#12289;&#39640;&#25928;&#21644;&#26368;&#26032;&#30340;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#12290;RALMs&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#22312;&#30456;&#20851;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#19981;&#30456;&#20851;&#26102;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#36825;&#22312;&#22810;&#36339;&#25512;&#29702;&#22330;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#19981;&#30456;&#20851;&#35777;&#25454;&#30340;&#35823;&#29992;&#20250;&#23548;&#33268;&#36830;&#38145;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26816;&#32034;&#22686;&#24378;&#26377;&#26102;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#20116;&#20010;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#22522;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#65292;&#25551;&#36848;&#20102;&#26816;&#32034;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#32447;&#65292;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#31579;&#36873;&#20986;&#19981;&#28041;&#21450;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#26816;&#32034;&#27573;&#33853;&#12290;&#36825;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#20195;&#20215;&#26159;&#33293;&#24323;&#20102;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding releva
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;RA-DIT&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#19968;&#26159;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#20108;&#26159;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#27599;&#20010;&#27493;&#39588;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.01352</link><description>&lt;p&gt;
RA-DIT: &#26816;&#32034;&#22686;&#24378;&#30340;&#21452;&#37325;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
RA-DIT: Retrieval-Augmented Dual Instruction Tuning. (arXiv:2310.01352v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;RA-DIT&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#19968;&#26159;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#20108;&#26159;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#27599;&#20010;&#27493;&#39588;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RALMs&#65289;&#36890;&#36807;&#35775;&#38382;&#22806;&#37096;&#25968;&#25454;&#23384;&#20648;&#20013;&#30340;&#38271;&#23614;&#21644;&#26368;&#26032;&#30693;&#35782;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#26500;&#24314;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#26816;&#32034;&#29305;&#23450;&#20462;&#25913;&#26469;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#35201;&#20040;&#20351;&#29992;&#20107;&#21518;&#38598;&#25104;&#25968;&#25454;&#23384;&#20648;&#30340;&#26041;&#27861;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;&#8212;&#8212;&#26816;&#32034;&#22686;&#24378;&#30340;&#21452;&#37325;&#25351;&#20196;&#35843;&#20248;&#65288;RA-DIT&#65289;&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#24494;&#35843;&#27493;&#39588;&#65306;&#65288;1&#65289;&#19968;&#20010;&#26356;&#26032;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#65288;2&#65289;&#21478;&#19968;&#20010;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#65292;&#31526;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#38656;&#35201;&#30693;&#35782;&#21033;&#29992;&#21644;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#20010;&#38454;&#27573;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#26159;RA-DIT 65B&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Causal Inference with Attention (CInA)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#27880;&#24847;&#21147;&#30340;&#23545;&#20598;&#20851;&#31995;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2310.00809</link><description>&lt;p&gt;
&#25351;&#21521;&#22240;&#26524;&#22522;&#30784;&#27169;&#22411;: &#22240;&#26524;&#25512;&#26029;&#19982;&#27880;&#24847;&#21147;&#30340;&#23545;&#20598;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Towards Causal Foundation Model: on Duality between Causal Inference and Attention. (arXiv:2310.00809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00809
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Causal Inference with Attention (CInA)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#27880;&#24847;&#21147;&#30340;&#23545;&#20598;&#20851;&#31995;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#23545;&#20598;&#36830;&#25509;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Causal Inference with Attention (CInA)&#30340;&#29702;&#35770;&#19978;&#23436;&#22791;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#30417;&#30563;&#22240;&#26524;&#23398;&#20064;&#65292;&#24182;&#22312;&#26032;&#25968;&#25454;&#30340;&#26410;&#35265;&#20219;&#21153;&#19978;&#23454;&#29616;&#38646;&#26679;&#26412;&#22240;&#26524;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for complex tasks. We propose a novel, theoretically sound method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that our approach
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLM-grounded Video Diffusion (LVD)&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#29983;&#25104;&#21160;&#24577;&#22330;&#26223;&#24067;&#23616;&#65292;&#20877;&#36890;&#36807;&#36825;&#20123;&#24067;&#23616;&#25351;&#23548;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#26102;&#31354;&#25552;&#31034;&#21644;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#29983;&#25104;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.17444</link><description>&lt;p&gt;
LLM&#22522;&#20110;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM-grounded Video Diffusion Models. (arXiv:2309.17444v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17444
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLM-grounded Video Diffusion (LVD)&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#29983;&#25104;&#21160;&#24577;&#22330;&#26223;&#24067;&#23616;&#65292;&#20877;&#36890;&#36807;&#36825;&#20123;&#24067;&#23616;&#25351;&#23548;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#26102;&#31354;&#25552;&#31034;&#21644;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#29983;&#25104;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#31070;&#32463;&#35270;&#39057;&#29983;&#25104;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#26102;&#31354;&#25552;&#31034;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36890;&#24120;&#29983;&#25104;&#21463;&#38480;&#21046;&#25110;&#19981;&#27491;&#30830;&#30340;&#36816;&#21160;&#65288;&#20363;&#22914;&#65292;&#29978;&#33267;&#32570;&#20047;&#20174;&#24038;&#21521;&#21491;&#31227;&#21160;&#30340;&#29289;&#20307;&#30340;&#25552;&#31034;&#33021;&#21147;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLM&#22522;&#20110;&#35270;&#39057;&#25193;&#25955;&#65288;LVD&#65289;&#12290;LVD&#19981;&#30452;&#25509;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#29983;&#25104;&#35270;&#39057;&#65292;&#32780;&#26159;&#39318;&#20808;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26681;&#25454;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#21160;&#24577;&#22330;&#26223;&#24067;&#23616;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#30340;&#24067;&#23616;&#26469;&#25351;&#23548;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#33021;&#22815;&#20174;&#21333;&#32431;&#30340;&#25991;&#26412;&#20013;&#29702;&#35299;&#22797;&#26434;&#30340;&#26102;&#31354;&#21160;&#24577;&#65292;&#24182;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#20013;&#36890;&#24120;&#35266;&#23519;&#21040;&#30340;&#25552;&#31034;&#21644;&#29289;&#20307;&#36816;&#21160;&#27169;&#24335;&#23494;&#20999;&#23545;&#40784;&#30340;&#24067;&#23616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#35843;&#25972;&#27880;&#24847;&#21147;&#22270;&#26469;&#25351;&#23548;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#19982;&#36825;&#20123;&#24067;&#23616;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned diffusion models have emerged as a promising tool for neural video generation. However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion (e.g., even lacking the ability to be prompted for objects moving from left to right). To address these limitations, we introduce LLM-grounded Video Diffusion (LVD). Instead of directly generating videos from the text inputs, LVD first leverages a large language model (LLM) to generate dynamic scene layouts based on the text inputs and subsequently uses the generated layouts to guide a diffusion model for video generation. We show that LLMs are able to understand complex spatiotemporal dynamics from text alone and generate layouts that align closely with both the prompts and the object motion patterns typically observed in the real world. We then propose to guide video diffusion models with these layouts by adjusting the attention maps. Our approach is training-free 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26356;&#36866;&#24212;&#20110;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#35813;&#27169;&#22411;&#20998;&#20026;&#20010;&#20154;&#32423;&#21035;&#12289;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#23567;&#22411;&#21270;&#20197;&#36866;&#24212;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#65292;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.14726</link><description>&lt;p&gt;
PLMM&#65306;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PLMM: Personal Large Models on Mobile Devices. (arXiv:2309.14726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26356;&#36866;&#24212;&#20110;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#35813;&#27169;&#22411;&#20998;&#20026;&#20010;&#20154;&#32423;&#21035;&#12289;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#23567;&#22411;&#21270;&#20197;&#36866;&#24212;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#65292;&#24182;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#22823;&#22411;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26356;&#36866;&#24212;&#26412;&#22320;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#22914;&#25945;&#32946;&#32972;&#26223;&#21644;&#29233;&#22909;&#12290;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#20026;&#19977;&#20010;&#32423;&#21035;&#65306;&#20010;&#20154;&#32423;&#21035;&#65292;&#19987;&#23478;&#32423;&#21035;&#21644;&#20256;&#32479;&#32423;&#21035;&#12290;&#20010;&#20154;&#32423;&#21035;&#27169;&#22411;&#36866;&#24212;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#23545;&#29992;&#25143;&#30340;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#24182;&#20445;&#25252;&#20854;&#38544;&#31169;&#12290;&#19987;&#23478;&#32423;&#21035;&#27169;&#22411;&#19987;&#27880;&#20110;&#21512;&#24182;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#22914;&#37329;&#34701;&#12289;IT&#21644;&#33402;&#26415;&#12290;&#20256;&#32479;&#27169;&#22411;&#19987;&#27880;&#20110;&#26222;&#36941;&#30693;&#35782;&#30340;&#21457;&#29616;&#21644;&#25552;&#21319;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#36825;&#26679;&#30340;&#20998;&#31867;&#20013;&#65292;&#20010;&#20154;&#27169;&#22411;&#30452;&#25509;&#19982;&#29992;&#25143;&#20132;&#20114;&#12290;&#23545;&#20110;&#25972;&#20010;&#31995;&#32479;&#26469;&#35828;&#65292;&#20010;&#20154;&#27169;&#22411;&#20855;&#26377;&#29992;&#25143;&#30340;&#65288;&#21152;&#23494;&#30340;&#65289;&#20010;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#36275;&#22815;&#23567;&#20197;&#22312;&#20010;&#20154;&#35745;&#31639;&#26426;&#25110;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#12290;&#26368;&#21518;&#65292;&#23427;&#20204;&#36824;&#24517;&#39035;&#23454;&#26102;&#21709;&#24212;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by Federated Learning, in this paper, we propose personal large models that are distilled from traditional large language models but more adaptive to local users' personal information such as education background and hobbies. We classify the large language models into three levels: the personal level, expert level and traditional level. The personal level models are adaptive to users' personal information. They encrypt the users' input and protect their privacy. The expert level models focus on merging specific knowledge such as finance, IT and art. The traditional models focus on the universal knowledge discovery and upgrading the expert models. In such classifications, the personal models directly interact with the user. For the whole system, the personal models have users' (encrypted) personal information. Moreover, such models must be small enough to be performed on personal computers or mobile devices. Finally, they also have to response in real-time for better user exper
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24102;&#26377;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#25511;&#21046;&#26080;&#20154;&#26426;&#32676;&#20307;&#65292;&#24182;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36824;&#23454;&#26045;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#20851;&#27880;&#37051;&#36817;&#26426;&#22120;&#20154;&#21644;&#38556;&#30861;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.13285</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#32676;&#20307;&#30340;&#30896;&#25758;&#22238;&#36991;&#21644;&#23548;&#33322;&#26041;&#27861;&#65306;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collision Avoidance and Navigation for a Quadrotor Swarm Using End-to-end Deep Reinforcement Learning. (arXiv:2309.13285v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13285
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24102;&#26377;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#25511;&#21046;&#26080;&#20154;&#26426;&#32676;&#20307;&#65292;&#24182;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36824;&#23454;&#26045;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#20851;&#27880;&#37051;&#36817;&#26426;&#22120;&#20154;&#21644;&#38556;&#30861;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#30340;&#26080;&#20154;&#26426;&#25511;&#21046;&#26041;&#27861;&#20855;&#26377;&#26131;&#20110;&#37096;&#32626;&#12289;&#20219;&#21153;&#27867;&#21270;&#21644;&#23454;&#26102;&#25191;&#34892;&#33021;&#21147;&#31561;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#31471;&#21040;&#31471;DRL&#26041;&#27861;&#20027;&#35201;&#29992;&#20110;&#22312;&#31616;&#21333;&#12289;&#26080;&#38556;&#30861;&#29615;&#22659;&#20013;&#35757;&#32451;&#21333;&#20010;&#26080;&#20154;&#26426;&#25110;&#26080;&#20154;&#26426;&#22242;&#38431;&#30340;&#25511;&#21046;&#22120;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#38556;&#30861;&#29289;&#23545;&#35757;&#32451;RL&#31574;&#30053;&#30340;&#22256;&#38590;&#24615;&#22686;&#21152;&#36896;&#25104;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24102;&#26377;&#38556;&#30861;&#29289;&#29615;&#22659;&#20013;&#25511;&#21046;&#26080;&#20154;&#26426;&#32676;&#20307;&#30340;&#31471;&#21040;&#31471;DRL&#26041;&#27861;&#12290;&#25105;&#20204;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#20102;&#19968;&#20010;&#35838;&#31243;&#65288;curriculum&#65289;&#21644;&#19968;&#20010;&#22238;&#25918;&#32531;&#20914;&#21306;&#65288;replay buffer&#65289;&#65292;&#29992;&#20110;&#25913;&#21892;&#22312;&#20805;&#28385;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23454;&#26045;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#20851;&#27880;&#37051;&#36817;&#26426;&#22120;&#20154;&#21644;&#38556;&#30861;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992; - &#36825;&#26159;&#39318;&#27425;&#25104;&#21151;&#22320;&#22312;&#20005;&#37325;&#35745;&#31639;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#30340;&#32676;&#20307;&#34892;&#20026;&#31574;&#30053;&#20013;&#24212;&#29992;&#27492;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end deep reinforcement learning (DRL) for quadrotor control promises many benefits -- easy deployment, task generalization and real-time execution capability. Prior end-to-end DRL-based methods have showcased the ability to deploy learned controllers onto single quadrotors or quadrotor teams maneuvering in simple, obstacle-free environments. However, the addition of obstacles increases the number of possible interactions exponentially, thereby increasing the difficulty of training RL policies. In this work, we propose an end-to-end DRL approach to control quadrotor swarms in environments with obstacles. We provide our agents a curriculum and a replay buffer of the clipped collision episodes to improve performance in obstacle-rich environments. We implement an attention mechanism to attend to the neighbor robots and obstacle interactions - the first successful demonstration of this mechanism on policies for swarm behavior deployed on severely compute-constrained hardware. Our wor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39034;&#24207;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#36861;&#27714;&#30340;&#21019;&#26032;&#26694;&#26550;&#23558;&#29615;&#22659;&#20449;&#24687;&#21644;&#25935;&#24863;&#23646;&#24615;&#19982;&#20998;&#31867;&#29305;&#24449;&#30340;&#23884;&#20837;&#34920;&#31034;&#20998;&#24320;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#19981;&#21516;&#21644;&#38476;&#29983;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13005</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#22495;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#36861;&#27714;
&lt;/p&gt;
&lt;p&gt;
Pursuing Counterfactual Fairness via Sequential Autoencoder Across Domains. (arXiv:2309.13005v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39034;&#24207;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#36861;&#27714;&#30340;&#21019;&#26032;&#26694;&#26550;&#23558;&#29615;&#22659;&#20449;&#24687;&#21644;&#25935;&#24863;&#23646;&#24615;&#19982;&#20998;&#31867;&#29305;&#24449;&#30340;&#23884;&#20837;&#34920;&#31034;&#20998;&#24320;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#19981;&#21516;&#21644;&#38476;&#29983;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#39046;&#22495;&#36716;&#31227;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#21508;&#31181;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#22788;&#29702;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#26102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#22312;&#36830;&#32493;&#30340;&#24207;&#21015;&#39046;&#22495;&#20013;&#36880;&#28176;&#21464;&#21270;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#22312;&#36825;&#20123;&#26032;&#39046;&#22495;&#20869;&#30340;&#27169;&#22411;&#25928;&#26524;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#20855;&#26377;&#39034;&#24207;&#33258;&#32534;&#30721;&#22120;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#65288;CDSAE&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#29615;&#22659;&#20449;&#24687;&#21644;&#25935;&#24863;&#23646;&#24615;&#19982;&#20998;&#31867;&#29305;&#24449;&#30340;&#23884;&#20837;&#34920;&#31034;&#20998;&#24320;&#12290;&#36825;&#31181;&#24182;&#34892;&#20998;&#31163;&#19981;&#20165;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#21644;&#38476;&#29983;&#30340;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing the prevalence of domain shift as a common challenge in machine learning, various domain generalization (DG) techniques have been developed to enhance the performance of machine learning systems when dealing with out-of-distribution (OOD) data. Furthermore, in real-world scenarios, data distributions can gradually change across a sequence of sequential domains. While current methodologies primarily focus on improving model effectiveness within these new domains, they often overlook fairness issues throughout the learning process. In response, we introduce an innovative framework called Counterfactual Fairness-Aware Domain Generalization with Sequential Autoencoder (CDSAE). This approach effectively separates environmental information and sensitive attributes from the embedded representation of classification features. This concurrent separation not only greatly improves model generalization across diverse and unfamiliar domains but also effectively addresses challenges rela
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#26234;&#33021;&#20307;&#21046;&#20316;&#22810;&#20010;&#30456;&#21516;&#21103;&#26412;&#26469;&#26356;&#22909;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#26469;&#24179;&#34913;&#28155;&#21152;&#39069;&#22806;&#21103;&#26412;&#30340;&#20248;&#21183;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.10908</link><description>&lt;p&gt;
&#22810;&#21103;&#26412;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Multicopy Reinforcement Learning Agents. (arXiv:2309.10908v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#26234;&#33021;&#20307;&#21046;&#20316;&#22810;&#20010;&#30456;&#21516;&#21103;&#26412;&#26469;&#26356;&#22909;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#26469;&#24179;&#34913;&#28155;&#21152;&#39069;&#22806;&#21103;&#26412;&#30340;&#20248;&#21183;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#21046;&#20316;&#22810;&#20010;&#30456;&#21516;&#21103;&#26412;&#26469;&#26356;&#22909;&#25110;&#26356;&#39640;&#25928;&#22320;&#23436;&#25104;&#21333;&#20010;&#26234;&#33021;&#20307;&#20219;&#21153;&#12290;&#22914;&#26524;&#29615;&#22659;&#22024;&#26434;&#65292;&#24182;&#19988;&#21333;&#20010;&#26234;&#33021;&#20307;&#21103;&#26412;&#26377;&#26102;&#26080;&#27861;&#23436;&#25104;&#20219;&#21153;&#65292;&#21017;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22810;&#21103;&#26412;&#38382;&#39064;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#22914;&#20309;&#24179;&#34913;&#28155;&#21152;&#39069;&#22806;&#21103;&#26412;&#30340;&#20248;&#21183;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines a novel type of multi-agent problem, in which an agent makes multiple identical copies of itself in order to achieve a single agent task better or more efficiently. This strategy improves performance if the environment is noisy and the task is sometimes unachievable by a single agent copy. We propose a learning algorithm for this multicopy problem which takes advantage of the structure of the value function to efficiently learn how to balance the advantages and costs of adding additional copies.
&lt;/p&gt;</description></item><item><title>&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65288;MOGAN&#65289;&#27169;&#22411;&#21270;&#20102;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#39044;&#27979;&#20102;&#23558;&#26032;&#23545;&#35937;&#25918;&#32622;&#22312;&#22797;&#21512;&#23545;&#35937;&#19978;&#30340;&#25928;&#26524;&#12290;&#22312;&#26500;&#24314;&#20855;&#26377;&#29305;&#23450;&#39640;&#24230;&#25110;&#23646;&#24615;&#30340;&#22612;&#31561;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#35268;&#21010;&#25214;&#21040;&#20855;&#26377;&#36866;&#24403;&#21487;&#29992;&#24615;&#30340;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#27491;&#30830;&#24314;&#27169;&#38750;&#24120;&#22797;&#26434;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10426</link><description>&lt;p&gt;
&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65306;&#36890;&#36807;&#22797;&#21512;&#23545;&#35937;&#21487;&#29992;&#24615;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Multi-Object Graph Affordance Network: Enabling Goal-Oriented Planning through Compound Object Affordances. (arXiv:2309.10426v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10426
&lt;/p&gt;
&lt;p&gt;
&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65288;MOGAN&#65289;&#27169;&#22411;&#21270;&#20102;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#39044;&#27979;&#20102;&#23558;&#26032;&#23545;&#35937;&#25918;&#32622;&#22312;&#22797;&#21512;&#23545;&#35937;&#19978;&#30340;&#25928;&#26524;&#12290;&#22312;&#26500;&#24314;&#20855;&#26377;&#29305;&#23450;&#39640;&#24230;&#25110;&#23646;&#24615;&#30340;&#22612;&#31561;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#35268;&#21010;&#25214;&#21040;&#20855;&#26377;&#36866;&#24403;&#21487;&#29992;&#24615;&#30340;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#27491;&#30830;&#24314;&#27169;&#38750;&#24120;&#22797;&#26434;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#39046;&#22495;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#34429;&#28982;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22411;&#28145;&#20837;&#25506;&#35752;&#20102;&#21333;&#20010;&#25110;&#25104;&#23545;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#22312;&#35843;&#26597;&#30001;&#22797;&#26434;&#24418;&#29366;&#30340;&#20219;&#24847;&#25968;&#37327;&#30340;&#23545;&#35937;&#32452;&#25104;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23545;&#35937;&#22270;&#24418;&#21487;&#29992;&#24615;&#32593;&#32476;&#65288;MOGAN&#65289;&#65292;&#23427;&#24314;&#27169;&#20102;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#39044;&#27979;&#23558;&#26032;&#23545;&#35937;&#25918;&#32622;&#22312;&#29616;&#26377;&#22797;&#21512;&#23545;&#35937;&#19978;&#30340;&#25928;&#26524;&#12290;&#32473;&#23450;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#26500;&#24314;&#20855;&#26377;&#29305;&#23450;&#39640;&#24230;&#25110;&#23646;&#24615;&#30340;&#22612;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#35268;&#21010;&#25214;&#21040;&#20855;&#26377;&#36866;&#24403;&#21487;&#29992;&#24615;&#30340;&#23545;&#35937;&#22534;&#21472;&#25805;&#20316;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#27491;&#30830;&#24314;&#27169;&#21253;&#25324;&#22534;&#21472;&#30340;&#29699;&#20307;&#21644;&#26479;&#23376;&#12289;&#26438;&#21644;&#21253;&#22260;&#26438;&#30340;&#29615;&#31561;&#38750;&#24120;&#22797;&#26434;&#30340;&#22797;&#21512;&#23545;&#35937;&#30340;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning object affordances is an effective tool in the field of robot learning. While the data-driven models delve into the exploration of affordances of single or paired objects, there is a notable gap in the investigation of affordances of compound objects that are composed of an arbitrary number of objects with complex shapes. In this study, we propose Multi-Object Graph Affordance Network (MOGAN) that models compound object affordances and predicts the effect of placing new objects on top of the existing compound. Given different tasks, such as building towers of specific heights or properties, we used a search based planning to find the sequence of stack actions with the objects of suitable affordances. We showed that our system was able to correctly model the affordances of very complex compound objects that include stacked spheres and cups, poles, and rings that enclose the poles. We demonstrated the applicability of our system in both simulated and real-world environments, com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12517</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#22870;&#21169;&#65292;&#36824;&#26377;&#32422;&#26463;&#65306;&#29992;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#30340;&#19968;&#20123;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#24182;&#20351;&#29992;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20855;&#26377;&#33258;&#28982;&#21160;&#20316;&#39118;&#26684;&#21644;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#20986;&#33394;&#25511;&#21046;&#22120;&#26159;&#36890;&#36807;&#36827;&#34892;&#22823;&#37327;&#22870;&#21169;&#24037;&#31243;&#32780;&#24320;&#21457;&#30340;&#65292;&#35813;&#36807;&#31243;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#65292;&#38656;&#35201;&#35774;&#35745;&#22823;&#37327;&#22870;&#21169;&#39033;&#24182;&#30830;&#23450;&#21512;&#36866;&#30340;&#22870;&#21169;&#31995;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#21516;&#26102;&#21253;&#21547;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#35753;&#24037;&#31243;&#24072;&#33021;&#22815;&#36866;&#24403;&#22320;&#21453;&#26144;&#20182;&#20204;&#23545;&#32422;&#26463;&#30340;&#24847;&#22270;&#24182;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#22788;&#29702;&#23427;&#20204;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32422;&#26463;&#31867;&#22411;&#21644;&#19968;&#31181;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#35813;&#23398;&#20064;&#26694;&#26550;&#34987;&#24212;&#29992;&#20110;&#35757;&#32451;&#19981;&#21516;&#24418;&#24577;&#21644;&#29289;&#29702;&#23646;&#24615;&#30340;&#20960;&#20010;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attribu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#35270;&#35273;&#20282;&#26381;&#65292;&#36866;&#24212;&#19981;&#21516;&#22330;&#26223;&#19988;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#36827;&#34892;&#27169;&#22411;&#25913;&#36827;&#65292;&#21487;&#20197;&#22788;&#29702;100&#31859;&#39640;&#24230;&#30340;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.11471</link><description>&lt;p&gt;
&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#65288;DOVESEI&#65289;
&lt;/p&gt;
&lt;p&gt;
Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI). (arXiv:2308.11471v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#35270;&#35273;&#20282;&#26381;&#65292;&#36866;&#24212;&#19981;&#21516;&#22330;&#26223;&#19988;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#36827;&#34892;&#27169;&#22411;&#25913;&#36827;&#65292;&#21487;&#20197;&#22788;&#29702;100&#31859;&#39640;&#24230;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22478;&#24066;&#31354;&#20013;&#26426;&#22120;&#20154;&#30340;&#22522;&#30784;&#27493;&#39588;&#20043;&#19968;&#65292;&#21363;&#23433;&#20840;&#30528;&#38470;&#12290;&#25105;&#20204;&#20851;&#27880;&#23433;&#20840;&#30528;&#38470;&#24863;&#30693;&#22534;&#26632;&#20013;&#26368;&#20851;&#38190;&#30340;&#26041;&#38754;&#20043;&#19968;&#65292;&#21363;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#21453;&#24212;&#24335;&#26080;&#20154;&#26426;&#31995;&#32479;&#65292;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#35270;&#35273;&#20282;&#26381;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#20854;&#24320;&#25918;&#35789;&#27719;&#26041;&#27861;&#65292;&#26368;&#23567;&#21270;&#35843;&#25972;&#38656;&#27714;&#65292;&#32469;&#36807;&#23545;&#20869;&#37096;&#27169;&#22411;&#36827;&#34892;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#20197;&#36827;&#34892;&#25913;&#36827;&#30340;&#24517;&#35201;&#24615;&#12290;&#32771;&#34385;&#21040;&#24403;&#22320;&#24403;&#23616;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#20174;100&#31859;&#39640;&#24230;&#36215;&#39134;&#30340;&#25805;&#20316;&#12290;&#36825;&#20010;&#36873;&#25321;&#26159;&#26377;&#24847;&#30340;&#65292;&#22240;&#20026;&#35768;&#22810;&#20043;&#21069;&#30340;&#24037;&#20316;&#22788;&#29702;&#30340;&#39640;&#24230;&#20165;&#38480;&#20110;30&#31859;&#65292;&#19982;&#23567;&#22411;&#31435;&#20307;&#30456;&#26426;&#30340;&#33021;&#21147;&#30456;&#21563;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20256;&#32479;&#30340;&#19977;&#32500;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#26469;&#23548;&#33322;&#21097;&#19979;&#30340;20&#31859;&#12290;&#21033;&#29992;&#21333;&#30446;&#30456;&#26426;&#21644;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
This work targets what we consider to be the foundational step for urban airborne robots, a safe landing. Our attention is directed toward what we deem the most crucial aspect of the safe landing perception stack: segmentation. We present a streamlined reactive UAV system that employs visual servoing by harnessing the capabilities of open vocabulary image segmentation. This approach can adapt to various scenarios with minimal adjustments, bypassing the necessity for extensive data accumulation for refining internal models, thanks to its open vocabulary methodology. Given the limitations imposed by local authorities, our primary focus centers on operations originating from altitudes of 100 meters. This choice is deliberate, as numerous preceding works have dealt with altitudes up to 30 meters, aligning with the capabilities of small stereo cameras. Consequently, we leave the remaining 20m to be navigated using conventional 3D path planning methods. Utilizing monocular cameras and image 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#32780;&#23545;&#25239;&#24615;RCPG&#36890;&#36807;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#24335;&#30452;&#25509;&#21644;&#22686;&#37327;&#23398;&#20064;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2308.11267</link><description>&lt;p&gt;
&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#21644;&#23545;&#25239;&#24615;&#31574;&#30053;&#26799;&#24230;&#22312;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes. (arXiv:2308.11267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#32780;&#23545;&#25239;&#24615;RCPG&#36890;&#36807;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#24335;&#30452;&#25509;&#21644;&#22686;&#37327;&#23398;&#20064;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;RCMDP&#65289;&#26159;&#19968;&#20010;&#26368;&#36817;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#24314;&#27169;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#22312;&#36716;&#31227;&#21160;&#24577;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#23545;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#27169;&#25311;RCMDPs&#38656;&#35201;&#22522;&#20110;&#27599;&#20010;&#29366;&#24577;&#30340;&#20540;&#20272;&#35745;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#36825;&#31181;&#26041;&#27861;&#20043;&#21069;&#22312;&#40065;&#26834;&#32422;&#26463;&#31574;&#30053;&#26799;&#24230;&#65288;RCPG&#65289;&#20013;&#20351;&#29992;&#36807;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#31216;&#20026;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#21644;&#23545;&#25239;&#24615;RCPG&#12290;&#20855;&#26377;&#40065;&#26834;&#25289;&#26684;&#26391;&#26085;&#30340;RCPG&#36890;&#36807;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#32780;&#19981;&#26159;&#20540;&#25110;&#32422;&#26463;&#26469;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#20174;&#32780;&#20462;&#25913;RCPG&#12290;&#23545;&#25239;&#24615;RCPG&#20063;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#20844;&#24335;&#35745;&#31639;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#24577;&#65292;&#20294;&#26159;&#23558;&#20854;&#20316;&#20026;&#23545;&#25239;&#31574;&#30053;&#30452;&#25509;&#21644;&#22686;&#37327;&#22320;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robust constrained Markov decision process (RCMDP) is a recent task-modelling framework for reinforcement learning that incorporates behavioural constraints and that provides robustness to errors in the transition dynamics model through the use of an uncertainty set. Simulating RCMDPs requires computing the worst-case dynamics based on value estimates for each state, an approach which has previously been used in the Robust Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG such as not robustifying the full constrained objective and the lack of incremental learning, this paper introduces two algorithms, called RCPG with Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies RCPG by taking the worst-case dynamics based on the Lagrangian rather than either the value or the constraint. Adversarial RCPG also formulates the worst-case dynamics based on the Lagrangian but learns this directly and incrementally as an adversarial policy throug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#21453;&#39304;&#26469;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20248;&#21270;&#20854;&#24615;&#33021;&#12290;&#36825;&#31181;&#20195;&#29702;&#33021;&#22815;&#20174;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#23398;&#20064;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.02151</link><description>&lt;p&gt;
Retroformer&#65306;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization. (arXiv:2308.02151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#21453;&#39304;&#26469;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20248;&#21270;&#20854;&#24615;&#33021;&#12290;&#36825;&#31181;&#20195;&#29702;&#33021;&#22815;&#20174;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#23398;&#20064;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#20010;&#26376;&#65292;&#20986;&#29616;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26032;&#36235;&#21183;&#65292;&#21363;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#25104;&#33021;&#22815;&#33258;&#20027;&#23436;&#25104;&#30446;&#26631;&#23548;&#21521;&#22810;&#27493;&#39588;&#20219;&#21153;&#30340;&#35821;&#35328;&#20195;&#29702;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22238;&#31572;&#20154;&#31867;&#29992;&#25143;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#27809;&#26377;&#20351;&#29992;&#29615;&#22659;&#29305;&#23450;&#30340;&#22870;&#21169;&#36827;&#34892;&#20248;&#21270;&#12290;&#23613;&#31649;&#19968;&#20123;&#20195;&#29702;&#36890;&#36807;&#21475;&#22836;&#21453;&#39304;&#23454;&#29616;&#20102;&#36845;&#20195;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#20197;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#22870;&#21169;&#23398;&#20064;&#30456;&#20860;&#23481;&#30340;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22238;&#39038;&#27169;&#22411;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#33258;&#21160;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#29615;&#22659;&#21453;&#39304;&#20013;&#20248;&#21270;&#20195;&#29702;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20195;&#29702;&#26550;&#26500;&#36890;&#36807;&#23398;&#20064;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#30340;&#22870;&#21169;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20154;&#25165;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#32452;&#32455;&#21487;&#20197;&#20174;&#25968;&#25454;&#31185;&#23398;&#30340;&#35282;&#24230;&#29702;&#35299;&#32452;&#32455;&#34892;&#20026;&#24182;&#23454;&#26102;&#20570;&#20986;&#20915;&#31574;&#65292;&#20026;&#26377;&#25928;&#30340;&#20154;&#25165;&#31649;&#29702;&#25552;&#20379;&#26234;&#33021;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2307.03195</link><description>&lt;p&gt;
&#20154;&#25165;&#20998;&#26512;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Artificial Intelligence Techniques for Talent Analytics. (arXiv:2307.03195v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03195
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20154;&#25165;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#32452;&#32455;&#21487;&#20197;&#20174;&#25968;&#25454;&#31185;&#23398;&#30340;&#35282;&#24230;&#29702;&#35299;&#32452;&#32455;&#34892;&#20026;&#24182;&#23454;&#26102;&#20570;&#20986;&#20915;&#31574;&#65292;&#20026;&#26377;&#25928;&#30340;&#20154;&#25165;&#31649;&#29702;&#25552;&#20379;&#26234;&#33021;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#31454;&#20105;&#28608;&#28872;&#19988;&#24555;&#36895;&#21457;&#23637;&#30340;&#21830;&#19994;&#29615;&#22659;&#19979;&#65292;&#32452;&#32455;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#20197;&#37327;&#21270;&#26041;&#24335;&#20570;&#20986;&#20154;&#25165;&#30456;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#12290;&#22823;&#35268;&#27169;&#20154;&#25165;&#21644;&#31649;&#29702;&#30456;&#20851;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20026;&#20225;&#19994;&#39046;&#23548;&#32773;&#25552;&#20379;&#20102;&#20174;&#25968;&#25454;&#31185;&#23398;&#35282;&#24230;&#29702;&#35299;&#32452;&#32455;&#34892;&#20026;&#21644;&#33719;&#21462;&#23454;&#38469;&#30693;&#35782;&#30340;&#26080;&#19982;&#20262;&#27604;&#26426;&#20250;&#65292;&#36827;&#32780;&#20026;&#23454;&#26102;&#20915;&#31574;&#21644;&#26377;&#25928;&#30340;&#20154;&#25165;&#31649;&#29702;&#25552;&#20379;&#26234;&#33021;&#25903;&#25345;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20154;&#25165;&#20998;&#26512;&#24050;&#32463;&#25104;&#20026;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#22312;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#26041;&#38754;&#30340;&#26377;&#24076;&#26395;&#30340;&#39046;&#22495;&#65292;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#24182;&#28608;&#21457;&#20102;&#35768;&#22810;&#30740;&#31350;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#26032;&#12289;&#20840;&#38754;&#30340;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20154;&#25165;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's competitive and fast-evolving business environment, it is a critical time for organizations to rethink how to make talent-related decisions in a quantitative manner. Indeed, the recent development of Big Data and Artificial Intelligence (AI) techniques have revolutionized human resource management. The availability of large-scale talent and management-related data provides unparalleled opportunities for business leaders to comprehend organizational behaviors and gain tangible knowledge from a data science perspective, which in turn delivers intelligence for real-time decision-making and effective talent management at work for their organizations. In the last decade, talent analytics has emerged as a promising field in applied data science for human resource management, garnering significant attention from AI communities and inspiring numerous research efforts. To this end, we present an up-to-date and comprehensive survey on AI technologies used for talent analytics in the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BaBE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#35299;&#37322;&#21464;&#37327;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#20272;&#35745;&#32473;&#23450;Z&#30340;&#27599;&#20010;&#32676;&#20307;E&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.02891</link><description>&lt;p&gt;
BaBE:&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#35299;&#37322;&#21464;&#37327;&#22686;&#24378;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables. (arXiv:2307.02891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BaBE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#28508;&#22312;&#35299;&#37322;&#21464;&#37327;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#20272;&#35745;&#32473;&#23450;Z&#30340;&#27599;&#20010;&#32676;&#20307;E&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20004;&#20010;&#32676;&#20307;&#20043;&#38388;&#19981;&#20844;&#24179;&#27495;&#35270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#22788;&#29702;&#26041;&#27861;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#30340;BaBE (Bayesian Bias Elimination)&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#32473;&#23450;Z&#30340;&#27599;&#20010;&#32676;&#20307;&#30340;E&#30340;&#26368;&#21487;&#33021;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of unfair discrimination between two groups and propose a pre-processing method to achieve fairness. Corrective methods like statistical parity usually lead to bad accuracy and do not really achieve fairness in situations where there is a correlation between the sensitive attribute S and the legitimate attribute E (explanatory variable) that should determine the decision. To overcome these drawbacks, other notions of fairness have been proposed, in particular, conditional statistical parity and equal opportunity. However, E is often not directly observable in the data, i.e., it is a latent variable. We may observe some other variable Z representing E, but the problem is that Z may also be affected by S, hence Z itself can be biased. To deal with this problem, we propose BaBE (Bayesian Bias Elimination), an approach based on a combination of Bayes inference and the Expectation-Maximization method, to estimate the most likely value of E for a given Z for each grou
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#29992;&#25143;&#26500;&#24314;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#26159;&#21542;&#20173;&#33021;&#20445;&#25345;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25915;&#20987;&#25163;&#27861;&#19981;&#36275;&#20197;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.15447</link><description>&lt;p&gt;
&#23545;&#40784;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#23545;&#25239;&#23545;&#40784;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are aligned neural networks adversarially aligned?. (arXiv:2306.15447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15447
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#29992;&#25143;&#26500;&#24314;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#26159;&#21542;&#20173;&#33021;&#20445;&#25345;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25915;&#20987;&#25163;&#27861;&#19981;&#36275;&#20197;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29616;&#22312;&#34987;&#35843;&#25972;&#20026;&#19982;&#20854;&#21019;&#24314;&#32773;&#30340;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#65292;&#21363;"&#26377;&#30410;&#19988;&#26080;&#23475;"&#12290;&#36825;&#20123;&#27169;&#22411;&#24212;&#35813;&#23545;&#29992;&#25143;&#30340;&#38382;&#39064;&#32473;&#20986;&#26377;&#30410;&#30340;&#22238;&#31572;&#65292;&#20294;&#25298;&#32477;&#22238;&#31572;&#21487;&#33021;&#20250;&#36896;&#25104;&#20260;&#23475;&#30340;&#35831;&#27714;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#32469;&#36807;&#23545;&#40784;&#23581;&#35797;&#30340;&#36755;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19982;&#26500;&#36896;&#26368;&#22351;&#24773;&#20917;&#36755;&#20837;&#65288;&#23545;&#25239;&#24615;&#26679;&#26412;&#65289;&#30340;&#23545;&#25239;&#29992;&#25143;&#20132;&#20114;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20445;&#25345;&#22810;&#22823;&#31243;&#24230;&#30340;&#23545;&#40784;&#12290;&#36825;&#20123;&#36755;&#20837;&#34987;&#35774;&#35745;&#25104;&#23548;&#33268;&#27169;&#22411;&#21457;&#20986;&#26412;&#24212;&#31105;&#27490;&#30340;&#26377;&#23475;&#20869;&#23481;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20248;&#21270;&#25915;&#20987;&#25163;&#27861;&#22312;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#26041;&#38754;&#30340;&#19981;&#36275;&#20043;&#22788;&#65306;&#21363;&#20351;&#22312;&#24403;&#21069;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25915;&#20987;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#25915;&#20987;&#30340;&#22833;&#36133;&#19981;&#24212;&#34987;&#35270;&#20026;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#20173;&#28982;&#20445;&#25345;&#23545;&#40784;&#30340;&#35777;&#26126;&#12290;&#20294;&#26159;&#36817;&#26399;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#26159;&#22810;&#27169;&#24577;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs.  However the recent trend in large-scale ML models is multim
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#22914;&#20309;&#36816;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20013;&#25991;&#26032;&#38395;&#25991;&#26412;&#20449;&#24687;&#30340;&#24773;&#24863;&#22240;&#32032;&#65292;&#20197;&#26399;&#20419;&#36827;&#30693;&#24773;&#21644;&#39640;&#39057;&#30340;&#25237;&#36164;&#32452;&#21512;&#35843;&#25972;&#12290;&#36890;&#36807;&#24314;&#31435;&#20005;&#26684;&#21644;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#19982;&#26631;&#20934;&#21270;&#30340;&#22238;&#27979;&#26694;&#26550;&#65292;&#20316;&#32773;&#23545;&#19981;&#21516;&#31867;&#22411; LLMs &#22312;&#35813;&#39046;&#22495;&#20869;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#23458;&#35266;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.14222</link><description>&lt;p&gt;
&#25581;&#31034;&#24773;&#24863;&#30340;&#28508;&#21147;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#39044;&#27979;&#20013;&#22269;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#65311;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?. (arXiv:2306.14222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#22914;&#20309;&#36816;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20013;&#25991;&#26032;&#38395;&#25991;&#26412;&#20449;&#24687;&#30340;&#24773;&#24863;&#22240;&#32032;&#65292;&#20197;&#26399;&#20419;&#36827;&#30693;&#24773;&#21644;&#39640;&#39057;&#30340;&#25237;&#36164;&#32452;&#21512;&#35843;&#25972;&#12290;&#36890;&#36807;&#24314;&#31435;&#20005;&#26684;&#21644;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#19982;&#26631;&#20934;&#21270;&#30340;&#22238;&#27979;&#26694;&#26550;&#65292;&#20316;&#32773;&#23545;&#19981;&#21516;&#31867;&#22411; LLMs &#22312;&#35813;&#39046;&#22495;&#20869;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#23458;&#35266;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#24555;&#36895;&#21457;&#23637;&#24050;&#24341;&#21457;&#20102;&#24191;&#27867;&#30340;&#35752;&#35770;&#65292;&#20854;&#20013;&#21253;&#25324;&#23427;&#20204;&#23558;&#22914;&#20309;&#25552;&#39640;&#37327;&#21270;&#32929;&#31080;&#20132;&#26131;&#31574;&#30053;&#30340;&#22238;&#25253;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#35752;&#35770;&#20027;&#35201;&#22260;&#32469;&#30528;&#21033;&#29992; LLMs &#30340;&#20986;&#33394;&#29702;&#35299;&#33021;&#21147;&#26469;&#25552;&#21462;&#24773;&#24863;&#22240;&#32032;&#65292;&#20174;&#32780;&#20419;&#36827;&#30693;&#24773;&#21644;&#39640;&#39057;&#30340;&#25237;&#36164;&#32452;&#21512;&#35843;&#25972;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#20123; LLMs &#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20013;&#22269;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#21644;&#38543;&#21518;&#30340;&#20013;&#22269;&#32929;&#31080;&#24066;&#22330;&#20132;&#26131;&#31574;&#30053;&#24320;&#21457;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#26684;&#21644;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#22238;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#23458;&#35266;&#35780;&#20272;&#19981;&#21516;&#31867;&#22411; LLMs &#22312;&#20013;&#25991;&#26032;&#38395;&#25991;&#26412;&#25968;&#25454;&#30340;&#24773;&#24863;&#22240;&#32032;&#25552;&#21462;&#20013;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#30340;&#24037;&#20316;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#29992;&#20102;&#19977;&#20010;&#19981;&#21516;&#27169;&#22411;&#65306;1&#65289;&#29983;&#25104;&#24335; LLM (ChatGPT)&#65292;2&#65289;&#20013;&#25991;&#35821;&#35328;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451; LLM (&#20108;&#37070;&#31070; RoBERTa)&#65292;&#20197;&#21450;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of Large Language Models (LLMs) has led to extensive discourse regarding their potential to boost the return of quantitative stock trading strategies. This discourse primarily revolves around harnessing the remarkable comprehension capabilities of LLMs to extract sentiment factors which facilitate informed and high-frequency investment portfolio adjustments. To ensure successful implementations of these LLMs into the analysis of Chinese financial texts and the subsequent trading strategy development within the Chinese stock market, we provide a rigorous and encompassing benchmark as well as a standardized back-testing framework aiming at objectively assessing the efficacy of various types of LLMs in the specialized domain of sentiment factor extraction from Chinese news text data. To illustrate how our benchmark works, we reference three distinctive models: 1) the generative LLM (ChatGPT), 2) the Chinese language-specific pre-trained LLM (Erlangshen-RoBERTa), and 
&lt;/p&gt;</description></item><item><title>LMFlow&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#21253;&#65292;&#20026;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#23436;&#25972;&#30340;&#24494;&#35843;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#25903;&#25345;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#24182;&#25903;&#25345;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#24494;&#35843;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19987;&#19994;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.12420</link><description>&lt;p&gt;
LMFlow&#65306;&#29992;&#20110;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#21487;&#25193;&#23637;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. (arXiv:2306.12420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12420
&lt;/p&gt;
&lt;p&gt;
LMFlow&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#21253;&#65292;&#20026;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#23436;&#25972;&#30340;&#24494;&#35843;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#25903;&#25345;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#24182;&#25903;&#25345;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#24494;&#35843;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19987;&#19994;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#23637;&#29616;&#20986;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#25509;&#36817;&#20154;&#31867;&#26234;&#33021;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#27169;&#22411;&#22312;&#19987;&#19994;&#20219;&#21153;&#24212;&#29992;&#20013;&#20173;&#28982;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#32570;&#38519;&#65292;&#38656;&#35201;&#24494;&#35843;&#25165;&#33021;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#21487;&#29992;&#27169;&#22411;&#21644;&#19987;&#19994;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36890;&#29992;&#24494;&#35843;&#30340;&#24037;&#20316;&#21464;&#24471;&#38750;&#24120;&#26840;&#25163;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#31532;&#19968;&#27493;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36731;&#37327;&#32423;&#30340;&#24037;&#20855;&#21253;LMFlow&#65292;&#26088;&#22312;&#31616;&#21270;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#25512;&#29702;&#12290;LMFlow&#20026;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#24494;&#35843;&#24037;&#20316;&#27969;&#31243;&#65292;&#25903;&#25345;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#36827;&#34892;&#20010;&#24615;&#21270;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25903;&#25345;&#36830;&#32493;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#31561;&#21151;&#33021;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#19987;&#19994;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large foundation models have demonstrated a great ability to achieve general human-level intelligence far beyond traditional approaches. As the technique keeps attracting attention from the AI community, more and more large foundation models have become publically available. However, most of those models exhibit a major deficiency in specialized-task applications, where the step of finetuning is still required for obtaining satisfactory performance. As the number of available models and specialized tasks keeps growing, the job of general finetuning becomes highly nontrivial. In this paper, we take the first step to address this issue. We introduce an extensible and lightweight toolkit, LMFlow, which aims to simplify the finetuning and inference of general large foundation models. LMFlow offers a complete finetuning workflow for a large foundation model to support personalized training with limited computing resources. Furthermore, it supports continuous pretraining, instruction tuning,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Wanda&#30340;&#26032;&#39062;&#12289;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#36755;&#20986;&#19978;&#30340;&#26435;&#37325;&#25353;&#29031;&#26368;&#23567;&#24133;&#24230;&#20056;&#20197;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#26469;&#36827;&#34892;&#21098;&#26525;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2306.11695</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple and Effective Pruning Approach for Large Language Models. (arXiv:2306.11695v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Wanda&#30340;&#26032;&#39062;&#12289;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#36755;&#20986;&#19978;&#30340;&#26435;&#37325;&#25353;&#29031;&#26368;&#23567;&#24133;&#24230;&#20056;&#20197;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#26469;&#36827;&#34892;&#21098;&#26525;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#30340;&#33258;&#28982;&#20505;&#36873;&#23545;&#35937;&#65306;&#36825;&#20123;&#26041;&#27861;&#22312;&#21162;&#21147;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20002;&#24323;&#20102;&#32593;&#32476;&#26435;&#37325;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#21313;&#20159;&#32423;&#21035;&#30340;LLMs&#26469;&#35828;&#24456;&#23569;&#21487;&#34892;&#65292;&#35201;&#20040;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#20381;&#36182;&#20108;&#38454;&#20449;&#24687;&#30340;&#26435;&#37325;&#37325;&#26500;&#38382;&#39064;&#65292;&#36825;&#20063;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#31216;&#20026;Wanda&#65288;&#22522;&#20110;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;&#21098;&#26525;&#65289;&#65292;&#26088;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;LLMs&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#21463;&#21040;&#26368;&#36817;&#23545;LLMs&#20013;&#20986;&#29616;&#30340;&#22823;&#24133;&#29305;&#24449;&#30340;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#36755;&#20986;&#19978;&#25353;&#29031;&#26435;&#37325;&#21644;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#30456;&#20056;&#30340;&#26368;&#23567;&#24133;&#24230;&#26469;&#21098;&#26525;&#26435;&#37325;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Wanda&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#65292;&#21098;&#26525;&#21518;&#30340;LLM&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;LLaMA&#21644;LLaMA-2&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;Wanda&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across vari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SequenceMatch&#30340;&#24102;&#26377;&#22238;&#28335;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#23567;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#24207;&#21015;&#21644;&#25968;&#25454;&#38598;&#24207;&#21015;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#27495;&#26469;&#20943;&#23569;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22797;&#21512;&#35823;&#24046;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#22238;&#28335;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.05426</link><description>&lt;p&gt;
SequenceMatch&#65306;&#24102;&#22238;&#28335;&#30340;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking. (arXiv:2306.05426v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SequenceMatch&#30340;&#24102;&#26377;&#22238;&#28335;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#23567;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#24207;&#21015;&#21644;&#25968;&#25454;&#38598;&#24207;&#21015;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#27495;&#26469;&#20943;&#23569;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22797;&#21512;&#35823;&#24046;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#22238;&#28335;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#21487;&#20197;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#35266;&#27979;&#20540;&#30340;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#20284;&#28982;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26368;&#22823;&#20284;&#28982;&#65288;MLE&#65289;&#30446;&#26631;&#19981;&#19968;&#23450;&#19982;&#33258;&#22238;&#24402;&#29983;&#25104;&#39640;&#36136;&#37327;&#24207;&#21015;&#30340;&#19979;&#28216;&#29992;&#20363;&#30456;&#21305;&#37197;&#12290;MLE&#30446;&#26631;&#25353;&#29031;&#25968;&#25454;&#20998;&#24067;&#19979;&#24207;&#21015;&#30340;&#39057;&#29575;&#21152;&#26435;&#65292;&#19981;&#25552;&#20379;&#27169;&#22411;&#22312;&#20998;&#24067;&#20043;&#22806;&#34892;&#20026;&#30340;&#25351;&#23548;&#65292;&#36825;&#20250;&#23548;&#33268;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#20013;&#22797;&#21512;&#35823;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22797;&#21512;&#35823;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#24207;&#21015;&#29983;&#25104;&#23450;&#20026;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#38382;&#39064;&#12290;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#26368;&#23567;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#20998;&#24067;&#21644;&#25968;&#25454;&#38598;&#24207;&#21015;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#27495;&#65292;&#21253;&#25324;&#32771;&#34385;&#20986;&#20998;&#24067;&#24207;&#21015;&#30340;&#20998;&#27495;&#12290;IL&#26694;&#26550;&#36824;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#24341;&#20837;&#22238;&#26684;&#21160;&#20316;&#26469;&#24341;&#20837;&#22238;&#28335;&#12290;&#36825;&#36827;&#19968;&#27493;&#20943;&#36731;&#20102;&#22797;&#21512;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compound
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#23548;&#33322;&#21644;&#27604;&#36739;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.02786</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#36335;&#24452;&#20960;&#20309;&#23548;&#33322;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;
&lt;/p&gt;
&lt;p&gt;
Navigating Explanatory Multiverse Through Counterfactual Path Geometry. (arXiv:2306.02786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#23548;&#33322;&#21644;&#27604;&#36739;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#35299;&#37322;&#65288;&#19981;&#36879;&#26126;&#30340;&#65289;&#39044;&#27979;&#27169;&#22411;&#20915;&#31574;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#20854;&#29983;&#25104;&#24448;&#24448;&#21463;&#21040;&#31639;&#27861;&#21644;&#29305;&#23450;&#39046;&#22495;&#32422;&#26463;&#30340;&#24433;&#21709;&#65292;&#22914;&#22522;&#20110;&#23494;&#24230;&#30340;&#21487;&#34892;&#24615;&#21644;&#23646;&#24615;&#30340;&#65288;&#19981;&#65289;&#21487;&#21464;&#24615;&#25110;&#21464;&#21270;&#30340;&#26041;&#21521;&#24615;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20854;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#38500;&#20102;&#23545;&#21453;&#20107;&#23454;&#23454;&#20363;&#26412;&#36523;&#30340;&#35201;&#27714;&#20043;&#22806;&#65292;&#24050;&#30693;&#31639;&#27861;&#21487;&#34892;&#24615;&#36335;&#24452;&#19982;&#20107;&#23454;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#21363;&#31639;&#27861;&#21487;&#35785;&#27714;&#65292;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#25216;&#26415;&#32771;&#34385;&#22240;&#32032;&#12290;&#23613;&#31649;&#36825;&#20004;&#20010;&#35201;&#27714;&#30830;&#20445;&#20102;&#26053;&#31243;&#30340;&#27493;&#39588;&#21644;&#30446;&#30340;&#22320;&#30340;&#21512;&#29702;&#24615;&#65292;&#20294;&#30446;&#21069;&#30340;&#25991;&#29486;&#24573;&#30053;&#20102;&#36825;&#31181;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#27010;&#24565;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#26053;&#31243;&#65307;&#28982;&#21518;&#23637;&#31034;&#20102;&#22914;&#20309;&#23548;&#33322;&#12289;&#25512;&#29702;&#21644;&#27604;&#36739;&#36825;&#20123;&#36712;&#36857;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations are the de facto standard when tasked with interpreting decisions of (opaque) predictive models. Their generation is often subject to algorithmic and domain-specific constraints -- such as density-based feasibility and attribute (im)mutability or directionality of change -- that aim to maximise their real-life utility. In addition to desiderata with respect to the counterfactual instance itself, existence of a viable path connecting it with the factual data point, known as algorithmic recourse, has become an important technical consideration. While both of these requirements ensure that the steps of the journey as well as its destination are admissible, current literature neglects the multiplicity of such counterfactual paths. To address this shortcoming we introduce the novel concept of explanatory multiverse that encompasses all the possible counterfactual journeys; we then show how to navigate, reason about and compare the geometry of these trajectories -
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31867;&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#20154;&#26426;&#23545;&#35805;&#20219;&#21153;&#65292;&#20197;&#21450;&#22312;&#20250;&#35758;&#35770;&#25991;&#23457;&#31295;&#20154;&#20998;&#37197;&#12289;&#22478;&#24066;&#22810;&#27493;&#34892;&#31243;&#35268;&#21010;&#21644;&#26053;&#34892;&#35745;&#21010;&#21327;&#21830;&#31561;&#22330;&#26223;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#21644;&#29992;&#25143;&#19981;&#21516;&#30340;&#33021;&#21147;&#22914;&#20309;&#32467;&#21512;&#20197;&#36798;&#21040;&#26368;&#20339;&#20915;&#31574;&#12290;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#23545;&#35805;&#29615;&#22659;&#24182;&#36827;&#34892;&#20154;&#26426;&#23545;&#35805;&#25910;&#38598;&#25968;&#25454;&#65292;&#21457;&#29616;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22312;&#27492;&#31867;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.20076</link><description>&lt;p&gt;
&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#20154;&#26426;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Decision-Oriented Dialogue for Human-AI Collaboration. (arXiv:2305.20076v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20076
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31867;&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#20154;&#26426;&#23545;&#35805;&#20219;&#21153;&#65292;&#20197;&#21450;&#22312;&#20250;&#35758;&#35770;&#25991;&#23457;&#31295;&#20154;&#20998;&#37197;&#12289;&#22478;&#24066;&#22810;&#27493;&#34892;&#31243;&#35268;&#21010;&#21644;&#26053;&#34892;&#35745;&#21010;&#21327;&#21830;&#31561;&#22330;&#26223;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#21644;&#29992;&#25143;&#19981;&#21516;&#30340;&#33021;&#21147;&#22914;&#20309;&#32467;&#21512;&#20197;&#36798;&#21040;&#26368;&#20339;&#20915;&#31574;&#12290;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#23545;&#35805;&#29615;&#22659;&#24182;&#36827;&#34892;&#20154;&#26426;&#23545;&#35805;&#25910;&#38598;&#25968;&#25454;&#65292;&#21457;&#29616;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22312;&#27492;&#31867;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31867;&#20219;&#21153;&#65292;&#31216;&#20026;&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#23545;&#35805;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#24517;&#39035;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#19968;&#21517;&#25110;&#22810;&#21517;&#20154;&#31867;&#21512;&#20316;&#65292;&#24110;&#21161;&#20182;&#20204;&#20570;&#20986;&#22797;&#26434;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#39046;&#22495;&#20013;&#24418;&#24335;&#21270;&#29992;&#25143;&#38754;&#20020;&#26085;&#24120;&#20915;&#31574;&#30340;&#36807;&#31243;&#65306;&#65288;1&#65289;&#36873;&#25321;&#20250;&#35758;&#35770;&#25991;&#30340;&#23457;&#31295;&#20154;&#20998;&#37197;&#65292;&#65288;2&#65289;&#22312;&#22478;&#24066;&#20013;&#35268;&#21010;&#22810;&#27493;&#34892;&#31243;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20026;&#19968;&#32676;&#26379;&#21451;&#21327;&#21830;&#26053;&#34892;&#35745;&#21010;&#12290;&#22312;&#27599;&#20010;&#35774;&#32622;&#20013;&#65292;AI&#21161;&#25163;&#21644;&#29992;&#25143;&#20855;&#26377;&#19981;&#21516;&#30340;&#33021;&#21147;&#65292;&#20182;&#20204;&#24517;&#39035;&#32467;&#21512;&#36215;&#26469;&#24471;&#20986;&#26368;&#20339;&#20915;&#31574;&#65306;&#21161;&#25163;&#21487;&#20197;&#35775;&#38382;&#21644;&#22788;&#29702;&#22823;&#37327;&#20449;&#24687;&#65292;&#32780;&#29992;&#25143;&#20855;&#26377;&#31995;&#32479;&#22806;&#30340;&#20559;&#22909;&#21644;&#38480;&#21046;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23545;&#35805;&#29615;&#22659;&#65292;&#20854;&#20013;&#20195;&#29702;&#26681;&#25454;&#20182;&#20204;&#36798;&#21040;&#30340;&#26368;&#32456;&#20915;&#31574;&#30340;&#36136;&#37327;&#33719;&#24471;&#22870;&#21169;&#12290;&#20351;&#29992;&#36825;&#20123;&#29615;&#22659;&#65292;&#25105;&#20204;&#19982;&#20154;&#20204;&#25198;&#28436;&#21161;&#25163;&#30340;&#20154;&#36827;&#34892;&#20102;&#20154;&#26426;&#23545;&#35805;&#12290;&#20026;&#20102;&#27604;&#36739;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#30340;&#20132;&#27969;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;-&#20154;&#31867;&#23545;&#35805;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#20915;&#31574;&#23548;&#21521;&#23545;&#35805;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#20984;&#26174;&#20102;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a class of tasks called decision-oriented dialogues, in which AI assistants must collaborate with one or more humans via natural language to help them make complex decisions. We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends. In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: assistants can access and process large amounts of information, while users have preferences and constraints external to the system. For each task, we build a dialogue environment where agents receive a reward based on the quality of the final decision they reach. Using these environments, we collect human-human dialogues with humans playing the role of assistant. To compare how current AI assistants communicate in these settings, we present bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20197;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#24182;&#20026;&#20559;&#35265;&#32531;&#35299;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.18569</link><description>&lt;p&gt;
ChatGPT&#30340;&#20844;&#24179;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fairness of ChatGPT. (arXiv:2305.18569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20197;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#24182;&#20026;&#20559;&#35265;&#32531;&#35299;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#35299;&#20915;LLM&#20013;&#19981;&#20844;&#24179;&#30340;&#38382;&#39064;&#23545;&#20110;&#36127;&#36131;&#20219;&#30340;AI&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;LLM&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#39046;&#22495;&#26102;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#20844;&#24179;&#35780;&#20272;&#26041;&#38754;&#65292;&#25968;&#37327;&#20998;&#26512;&#21644;&#28145;&#20837;&#30740;&#31350;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20351;&#29992;ChatGPT&#20316;&#20026;&#30740;&#31350;&#26696;&#20363;&#30340;LLM&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35780;&#20272;ChatGPT&#22312;&#21253;&#25324;&#25945;&#32946;&#12289;&#29359;&#32618;&#23398;&#12289;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#36827;&#34892;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#21644;&#20010;&#20154;&#20844;&#24179;&#24615;&#65292;&#24182;&#35266;&#23519;&#20102;&#22312;&#19968;&#31995;&#21015;&#26377;&#20559;&#25110;&#26080;&#20559;&#25552;&#31034;&#19979;ChatGPT&#36755;&#20986;&#30340;&#24046;&#24322;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;LLM&#30340;&#20844;&#24179;&#34920;&#29616;&#65292;&#20415;&#20110;&#20559;&#35265;&#32531;&#35299;&#65292;&#20419;&#36827;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#20855;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To make thorough evaluation, we consider both group fairness and individual fairness and we also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BBT-RGB&#65292;&#19968;&#22871;&#29992;&#20110;&#22686;&#24378;&#40657;&#30418;&#20248;&#21270;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#30452;&#25509;&#19988;&#20114;&#34917;&#25216;&#26415;&#22871;&#20214;&#65292;&#21253;&#25324;&#20004;&#38454;&#27573;&#26080;&#23548;&#25968;&#20248;&#21270;&#31574;&#30053;&#12289;&#33258;&#21160;&#35821;&#35328;&#36716;&#21270;&#22120;&#26500;&#24314;&#21450;&#20854;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#26032;&#29992;&#27861;&#20197;&#21450;&#26356;&#22909;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.08088</link><description>&lt;p&gt;
&#20351;&#22522;&#20110;&#25552;&#31034;&#30340;&#40657;&#30418;&#35843;&#20248;&#26356;&#21152;&#20016;&#23500;&#22810;&#24425;&#65306;&#20174;&#19977;&#20010;&#27491;&#20132;&#35282;&#24230;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives. (arXiv:2305.08088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BBT-RGB&#65292;&#19968;&#22871;&#29992;&#20110;&#22686;&#24378;&#40657;&#30418;&#20248;&#21270;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#30452;&#25509;&#19988;&#20114;&#34917;&#25216;&#26415;&#22871;&#20214;&#65292;&#21253;&#25324;&#20004;&#38454;&#27573;&#26080;&#23548;&#25968;&#20248;&#21270;&#31574;&#30053;&#12289;&#33258;&#21160;&#35821;&#35328;&#36716;&#21270;&#22120;&#26500;&#24314;&#21450;&#20854;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#26032;&#29992;&#27861;&#20197;&#21450;&#26356;&#22909;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24050;&#32463;&#23637;&#29616;&#20986;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35843;&#25972;&#36825;&#20123;&#27169;&#22411;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#24040;&#39069;&#30340;&#20195;&#20215;&#25110;&#30001;&#20110;&#21830;&#19994;&#32771;&#34385;&#32780;&#19981;&#21487;&#29992;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#40657;&#30418;&#35843;&#20248;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#32780;&#19981;&#35775;&#38382;&#26799;&#24230;&#21644;&#38544;&#34255;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20316;&#21697;&#36824;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#19979;&#26080;&#26799;&#24230;&#20248;&#21270;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;BBT-RGB&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#40657;&#30418;&#20248;&#21270;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#30452;&#25509;&#19988;&#20114;&#34917;&#25216;&#26415;&#22871;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#32452;&#20214;&#65306;&#65288;1&#65289;&#20004;&#38454;&#27573;&#26080;&#23548;&#25968;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#25910;&#25947;&#24182;&#32531;&#35299;&#36807;&#25311;&#21512;&#65307;&#65288;2&#65289;&#33258;&#21160;&#35821;&#35328;&#36716;&#21270;&#22120;&#26500;&#24314;&#21450;&#20854;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#26032;&#29992;&#27861;&#65307;&#65288;3&#65289;&#26356;&#22909;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#65292;&#22522;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#35328;&#23398;&#21160;&#26426;&#21477;&#27861;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown increasing power on various natural language processing (NLP) tasks. However, tuning these models for downstream tasks usually needs exorbitant costs or is unavailable due to commercial considerations. Recently, black-box tuning has been proposed to address this problem by optimizing task-specific prompts without accessing the gradients and hidden representations. However, most existing works have yet fully exploited the potential of gradient-free optimization under the scenario of few-shot learning. In this paper, we describe BBT-RGB, a suite of straightforward and complementary techniques for enhancing the efficiency and performance of black-box optimization. Specifically, our method includes three plug-and-play components: (1) Two-stage derivative-free optimization strategy that facilitates fast convergence and mitigates overfitting; (2) Automatic verbalizer construction with its novel usage under few-shot settings; (3) Better prompt initializ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#20027;&#31649;&#29702;&#25506;&#32034;&#31574;&#30053;&#30340;&#22810;&#27169;&#24335;&#26234;&#33021;&#20307;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01322</link><description>&lt;p&gt;
&#22522;&#20110;Option&#26694;&#26550;&#30340;&#22810;&#27169;&#24335;&#25506;&#32034;&#33258;&#20027;&#38750;&#21333;&#20307;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework. (arXiv:2305.01322v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#20027;&#31649;&#29702;&#25506;&#32034;&#31574;&#30053;&#30340;&#22810;&#27169;&#24335;&#26234;&#33021;&#20307;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25506;&#32034;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#8220;&#22914;&#20309;&#25506;&#32034;&#8221;&#30340;&#25506;&#32034;&#26041;&#24335;&#65292;&#32780;&#8220;&#20309;&#26102;&#25506;&#32034;&#8221;&#30340;&#25506;&#32034;&#30740;&#31350;&#19968;&#30452;&#27809;&#26377;&#25104;&#20026;&#37325;&#28857;&#12290;&#20856;&#22411;&#30340;&#25506;&#32034;&#34892;&#20026;&#36890;&#24120;&#23558;&#25506;&#32034;&#34892;&#20026;&#19982;&#26234;&#33021;&#20307;&#30340;&#24320;&#21457;&#21033;&#29992;&#34892;&#20026;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#38750;&#21333;&#20307;&#25506;&#32034;&#34892;&#20026;&#30340;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#27169;&#24335;&#20999;&#25442;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#30340;&#26368;&#32456;&#30446;&#30340;&#26159;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#20027;&#20915;&#23450;&#20309;&#26102;&#25506;&#32034;&#25110;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;Option&#26694;&#26550;&#20013;&#25551;&#36848;&#20102;&#33258;&#20027;&#22810;&#27169;&#24335;&#25506;&#32034;&#30340;&#21021;&#22987;&#30740;&#31350;&#12290;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#30340;&#26356;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most exploration research on reinforcement learning (RL) has paid attention to `the way of exploration', which is `how to explore'. The other exploration research, `when to explore', has not been the main focus of RL exploration research. \textcolor{black}{The issue of `when' of a monolithic exploration in the usual RL exploration behaviour binds an exploratory action to an exploitational action of an agent. Recently, a non-monolithic exploration research has emerged to examine the mode-switching exploration behaviour of humans and animals.} The ultimate purpose of our research is to enable an agent to decide when to explore or exploit autonomously. We describe the initial research of an autonomous multi-mode exploration of non-monolithic behaviour in an options framework. The higher performance of our method is shown against the existing non-monolithic exploration method through comparative experimental results.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;3D&#22330;&#26223;&#29702;&#35299;&#30340;Regional Point-Language Contrastive Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23494;&#38598;&#30340;&#35270;&#35273;&#25552;&#31034;&#21644;&#28857;&#29420;&#31435;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#23545;&#26032;&#39062;&#31867;&#21035;&#30340;&#35782;&#21035;&#21644;&#23494;&#38598;&#22330;&#26223;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.00962</link><description>&lt;p&gt;
RegionPLC&#65306;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#21306;&#22495;&#28857;-&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding. (arXiv:2304.00962v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;3D&#22330;&#26223;&#29702;&#35299;&#30340;Regional Point-Language Contrastive Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23494;&#38598;&#30340;&#35270;&#35273;&#25552;&#31034;&#21644;&#28857;&#29420;&#31435;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#23545;&#26032;&#39062;&#31867;&#21035;&#30340;&#35782;&#21035;&#21644;&#23494;&#38598;&#22330;&#26223;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#22312;&#38381;&#38598;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#65292;&#20294;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#26080;&#27861;&#22788;&#29702;&#26032;&#39062;&#31867;&#21035;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RegionPLC&#30340;&#24320;&#25918;&#19990;&#30028;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#21306;&#22495;&#28857;-&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20351;&#32463;&#36807;&#23553;&#38381;&#38598;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#22791;&#24320;&#25918;&#35789;&#27719;&#35782;&#21035;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23494;&#38598;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#36890;&#36807;&#26631;&#39064;&#29983;&#25104;&#20174;2D&#22522;&#30784;&#27169;&#22411;&#20013;&#24341;&#21457;&#21306;&#22495;&#32423;&#35270;&#35273;-&#35821;&#35328;&#30693;&#35782;&#65292;&#36827;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#24314;&#31435;&#23494;&#38598;&#30340;&#21306;&#22495;&#28857;-&#35821;&#35328;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28857;&#21028;&#21035;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#20351;&#24471;&#20174;&#26631;&#39064;&#20013;&#36827;&#34892;&#28857;&#29420;&#31435;&#23398;&#20064;&#20197;&#23454;&#29616;&#23494;&#38598;&#22330;&#26223;&#29702;&#35299;&#12290;&#25105;&#20204;&#22312;ScanNet&#12289;ScanNet200&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#30456;&#27604;&#20043;&#21069;&#30340;&#22522;&#20110;&#27880;&#37322;&#30340;3D&#24320;&#25918;&#19990;&#30028;&#22330;&#26223;&#29702;&#35299;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;RegionPLC&#22312;&#35821;&#20041;&#21644;&#23454;&#20363;&#20998;&#21106;&#26041;&#38754;&#30340;&#24615;&#33021;&#24179;&#22343;&#25552;&#39640;&#20102;11.6%&#21644;6.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing 3D scene understanding tasks have achieved high performance on close-set benchmarks but fail to handle novel categories in real-world applications. To this end, we propose a Regional Point-Language Contrastive learning framework, namely RegionPLC, for open-world 3D scene understanding, which equips models trained on closed-set datasets with open-vocabulary recognition capabilities. We propose dense visual prompts to elicit region-level visual-language knowledge from 2D foundation models via captioning, which further allows us to build dense regional point-language associations. Then, we design a point-discriminative contrastive learning objective to enable point-independent learning from captions for dense scene understanding. We conduct extensive experiments on ScanNet, ScanNet200, and nuScenes datasets. Our RegionPLC significantly outperforms previous base-annotated 3D open-world scene understanding approaches by an average of 11.6\% and 6.6\% for semantic and instance segme
&lt;/p&gt;</description></item><item><title>AraSpot&#26159;&#19968;&#27454;&#20351;&#29992;ConformerGRU&#27169;&#22411;&#26550;&#26500;&#35757;&#32451;40&#20010;&#38463;&#25289;&#20271;&#35821;&#20851;&#38190;&#35789;&#30340;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;&#24037;&#20855;&#65292;&#20854;&#36890;&#36807;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#30340;&#35757;&#32451;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#20197;99.59%&#30340;&#20934;&#30830;&#29575;&#36229;&#20986;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16621</link><description>&lt;p&gt;
AraSpot&#65306;&#38463;&#25289;&#20271;&#35821;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
AraSpot: Arabic Spoken Command Spotting. (arXiv:2303.16621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16621
&lt;/p&gt;
&lt;p&gt;
AraSpot&#26159;&#19968;&#27454;&#20351;&#29992;ConformerGRU&#27169;&#22411;&#26550;&#26500;&#35757;&#32451;40&#20010;&#38463;&#25289;&#20271;&#35821;&#20851;&#38190;&#35789;&#30340;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;&#24037;&#20855;&#65292;&#20854;&#36890;&#36807;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#30340;&#35757;&#32451;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#20197;99.59%&#30340;&#20934;&#30830;&#29575;&#36229;&#20986;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#20851;&#38190;&#35782;&#21035;&#65288;KWS&#65289;&#26159;&#25351;&#22312;&#38899;&#39057;&#27969;&#20013;&#35782;&#21035;&#20851;&#38190;&#35789;&#65292;&#24191;&#27867;&#29992;&#20110;&#26234;&#33021;&#36793;&#32536;&#35774;&#22791;&#19978;&#65292;&#20197;&#21551;&#21160;&#35821;&#38899;&#21161;&#25163;&#21644;&#36827;&#34892;&#20813;&#25552;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#38754;&#20020;&#30528;&#39640;&#31934;&#24230;&#21644;&#22312;&#20302;&#21151;&#29575;&#21644;&#21487;&#33021;&#30340;&#26377;&#38480;&#35745;&#31639;&#33021;&#21147;&#35774;&#22791;&#19978;&#20445;&#25345;&#31995;&#32479;&#36816;&#34892;&#25928;&#29575;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#19981;&#21516;&#30340;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#24341;&#20837;ConformerGRU&#27169;&#22411;&#26550;&#26500;&#30340;AraSpot&#65292;&#29992;&#20110;&#35757;&#32451;40&#20010;&#38463;&#25289;&#20271;&#35821;&#20851;&#38190;&#35789;&#30340;&#35782;&#21035;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;AraSpot&#20197;SOTA 99.59&#65285;&#36229;&#36807;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken keyword spotting (KWS) is the task of identifying a keyword in an audio stream and is widely used in smart devices at the edge in order to activate voice assistants and perform hands-free tasks. The task is daunting as there is a need, on the one hand, to achieve high accuracy while at the same time ensuring that such systems continue to run efficiently on low power and possibly limited computational capabilities devices. This work presents AraSpot for Arabic keyword spotting trained on 40 Arabic keywords, using different online data augmentation, and introducing ConformerGRU model architecture. Finally, we further improve the performance of the model by training a text-to-speech model for synthetic data generation. AraSpot achieved a State-of-the-Art SOTA 99.59% result outperforming previous approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#24182;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#21644;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;PCP&#28040;&#34701;&#25216;&#26415;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#25105;&#20204;&#25104;&#21151;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.12461</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. (arXiv:2302.12461v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#24182;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#21644;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;PCP&#28040;&#34701;&#25216;&#26415;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#25105;&#20204;&#25104;&#21151;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#20013;&#30340;&#27602;&#21270;&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#23433;&#20840;&#23041;&#32961;&#65292;&#21487;&#33021;&#23548;&#33268;&#26263;&#34255;&#21518;&#38376;&#30340;&#27169;&#22411;&#12290;&#20851;&#20110;&#26263;&#34255;&#21518;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#22788;&#29702;&#35302;&#21457;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#20999;&#25442;&#33267;&#26377;&#27602;&#35821;&#35328;&#65289;&#30340;&#25551;&#36848;&#23578;&#26410;&#25214;&#21040;&#12290;&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;Transformer&#30340;&#26263;&#34255;&#21518;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#30830;&#23450;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#19982;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#32467;&#21512;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#29992;&#24037;&#31243;&#21270;&#26367;&#20195;&#29289;&#38477;&#20302;MLP&#27169;&#22359;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20027;&#35201;&#25104;&#20998;&#30340;&#20302;&#31209;&#30697;&#38453;&#30340;PCP&#28040;&#34701;&#25216;&#26415;&#65292;&#29992;&#20854;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#26263;&#34255;&#21518;&#38376;&#30340;&#29609;&#20855;&#27169;&#22411;&#12289;&#26263;&#34255;&#21518;&#38376;&#30340;&#22823;&#22411;&#27169;&#22411;&#21644;&#38750;&#26263;&#34255;&#21518;&#38376;&#30340;&#24320;&#28304;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#25913;&#21892;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.00808</link><description>&lt;p&gt;
&#24179;&#22343;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Average-Constrained Policy Optimization. (arXiv:2302.00808v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#21046;&#26465;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36890;&#24120;&#65292;&#24179;&#22343;&#26631;&#20934;&#27604;&#25240;&#25187;&#26631;&#20934;&#26356;&#21512;&#36866;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#24179;&#22343;&#38480;&#21046; CMDP &#30340;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#25240;&#25187;&#38480;&#21046; RL &#38382;&#39064;&#35774;&#35745;&#30340;&#31639;&#27861;&#36890;&#24120;&#22312;&#24179;&#22343; CMDP &#29615;&#22659;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#24179;&#22343;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;&#65288;ACPO&#65289;&#31639;&#27861;&#30340;&#28789;&#24863;&#26469;&#33258;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;&#30340;&#33879;&#21517; PPO &#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#22522;&#26412;&#30340;&#24179;&#22343; MDP &#25935;&#24863;&#24615;&#29702;&#35770;&#65292;&#28982;&#21518;&#22312;&#31639;&#27861;&#35774;&#35745;&#20013;&#20351;&#29992;&#30456;&#24212;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20854;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; MuJoCo &#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#24037;&#20316;&#65292;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#19982;&#20854;&#20182;&#24120;&#35268;&#31639;&#27861;&#30456;&#27604;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) with constraints is becoming an increasingly important problem for various applications. Often, the average criterion is more suitable than the discounted criterion. Yet, RL for average criterion-constrained MDPs remains a challenging problem. Algorithms designed for discounted constrained RL problems often do not perform well for the average CMDP setting. In this paper, we introduce a new policy optimization with function approximation algorithm for constrained MDPs with the average criterion. The Average-Constrained Policy Optimization (ACPO) algorithm is inspired by the famed PPO-type algorithms based on trust region methods. We develop basic sensitivity theory for average MDPs, and then use the corresponding bounds in the design of the algorithm. We provide theoretical guarantees on its performance, and through extensive experimental work in various challenging MuJoCo environments, show the superior performance of the algorithm when compared to other sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2212.03932</link><description>&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#26041;&#27861;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#34892;&#20026;&#31574;&#30053;&#31163;&#32447;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Low Variance Off-policy Evaluation with State-based Importance Sampling. (arXiv:2212.03932v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#32447;&#35780;&#20272;&#20013;&#65292;&#38656;&#35201;&#35780;&#20272;&#30446;&#26631;&#31574;&#30053;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#38656;&#35201;&#20351;&#29992;&#30001;&#34892;&#20026;&#31574;&#30053;&#37319;&#38598;&#30340;&#26679;&#26412;&#25968;&#25454;&#12290;&#20256;&#32479;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#26041;&#27861;&#30001;&#20110;&#35745;&#31639;&#21160;&#20316;&#27010;&#29575;&#27604;&#20540;&#30340;&#20056;&#31215;&#32780;&#23548;&#33268;&#26041;&#24046;&#22686;&#21152;&#65292;&#20174;&#32780;&#22312;&#28041;&#21450;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#20986;&#29616;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In off-policy reinforcement learning, a behaviour policy performs exploratory interactions with the environment to obtain state-action-reward samples which are then used to learn a target policy that optimises the expected return. This leads to a problem of off-policy evaluation, where one needs to evaluate the target policy from samples collected by the often unrelated behaviour policy. Importance sampling is a traditional statistical technique that is often applied to off-policy evaluation. While importance sampling estimators are unbiased, their variance increases exponentially with the horizon of the decision process due to computing the importance weight as a product of action probability ratios, yielding estimates with low accuracy for domains involving long-term planning. This paper proposes state-based importance sampling (SIS), which drops the action probability ratios of sub-trajectories with "negligible states" -- roughly speaking, those for which the chosen actions have no 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33041;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.06348</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#33041;&#20449;&#21495;&#25581;&#31034;&#20154;&#31867;&#35821;&#35328;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Brain Signals Reveal Inner Alignment with Human Languages?. (arXiv:2208.06348v4 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33041;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#20449;&#21495;&#65288;&#22914;&#33041;&#30005;&#22270;&#65289;&#21644;&#20154;&#31867;&#35821;&#35328;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20108;&#32773;&#20043;&#38388;&#30340;&#32852;&#31995;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33041;&#30005;&#22270;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20381;&#36182;&#24615;&#12290;&#22312;&#34920;&#31034;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#65288;Multimodal Transformer Alignment Model&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35266;&#23519;&#36825;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#21327;&#35843;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#31181;&#20851;&#31995;&#23545;&#40784;&#25216;&#26415;&#65292;&#22914;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#21644;Wasserstein&#36317;&#31163;&#65292;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#36716;&#25442;&#29305;&#24449;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#22312;ZuCo&#21644;K-EmoCon&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#20351;K-EmoCon&#25968;&#25454;&#38598;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#20102;1.7&#65285;&#65292;ZuCo&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;9.3&#65285;&#65292;&#22312;&#20851;&#31995;&#26816;&#27979;&#26041;&#38754;ZuCo&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;7.4&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22269;&#38469;&#19978;&#26368;&#22823;&#30340;&#20154;&#31867;&#31867;&#27604;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study at the representation level, we introduced \textbf{MTAM}, a \textbf{M}ultimodal \textbf{T}ransformer \textbf{A}lignment \textbf{M}odel, to observe coordinated representations between the two modalities. We used various relationship alignment-seeking techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to transfigure features. On downstream applications, sentiment analysis and relation detection, we achieved new state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition, we provide inter
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#39034;&#24207;&#20915;&#31574;&#32773;&#30340;&#25932;&#23545;&#25915;&#20987;&#26469;&#35828;&#65292;&#24369;&#28857;&#26159;&#32570;&#20047;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#20351;&#20854;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#65307;&#32780;R-attack&#26159;&#19968;&#31181;&#26082;&#26377;&#25928;&#21448;&#21487;&#35777;&#26126;&#26159;&#32479;&#35745;&#19981;&#21487;&#26816;&#27979;&#30340;&#25915;&#20987;&#65292;&#21487;&#20197;&#26356;&#38590;&#20197;&#20351;&#29992;&#33258;&#21160;&#21270;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#12290;</title><link>http://arxiv.org/abs/2207.10170</link><description>&lt;p&gt;
&#24187;&#35273;&#25915;&#20987;&#65306;&#23545;&#39034;&#24207;&#20915;&#31574;&#32773;&#30340;&#25932;&#23545;&#25915;&#20987;&#20013;&#21487;&#26816;&#27979;&#24615;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers. (arXiv:2207.10170v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10170
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#39034;&#24207;&#20915;&#31574;&#32773;&#30340;&#25932;&#23545;&#25915;&#20987;&#26469;&#35828;&#65292;&#24369;&#28857;&#26159;&#32570;&#20047;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#20351;&#20854;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#65307;&#32780;R-attack&#26159;&#19968;&#31181;&#26082;&#26377;&#25928;&#21448;&#21487;&#35777;&#26126;&#26159;&#32479;&#35745;&#19981;&#21487;&#26816;&#27979;&#30340;&#25915;&#20987;&#65292;&#21487;&#20197;&#26356;&#38590;&#20197;&#20351;&#29992;&#33258;&#21160;&#21270;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#37096;&#32626;&#30340;&#33258;&#20027;&#20195;&#29702;&#38656;&#35201;&#23545;&#24863;&#23448;&#36755;&#20837;&#30340;&#25932;&#23545;&#25915;&#20987;&#20855;&#22791;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;&#24378;&#21270;&#20195;&#29702;&#31574;&#30053;&#38656;&#35201;&#39044;&#27979;&#21487;&#33021;&#30340;&#26368;&#24378;&#25915;&#20987;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#35266;&#27979;&#31354;&#38388;&#25915;&#20987;&#20855;&#26377;&#20849;&#21516;&#30340;&#24369;&#28857;&#65306;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#33258;&#21160;&#21270;&#25163;&#27573;&#25110;&#20154;&#24037;&#26816;&#26597;&#26469;&#26816;&#27979;&#12290;&#23545;&#20110;&#25932;&#25163;&#26469;&#35828;&#65292;&#21487;&#26816;&#27979;&#24615;&#26159;&#19981;&#24076;&#26395;&#20986;&#29616;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#20250;&#24341;&#21457;&#23433;&#20840;&#20107;&#24577;&#21319;&#32423;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23436;&#32654;&#30340;&#24187;&#35273;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#39034;&#24207;&#20915;&#31574;&#32773;&#30340;&#25932;&#23545;&#25915;&#20987;&#65292;&#26082;&#26377;&#25928;&#21448;&#21487;&#35777;&#26126;&#26159;&#32479;&#35745;&#19981;&#21487;&#26816;&#27979;&#30340;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26356;&#21152;&#28789;&#27963;&#30340;R-attack&#65292;&#20854;&#29983;&#25104;&#30340;&#35266;&#27979;&#36716;&#25442;&#19982;&#26080;&#25932;&#23545;&#29615;&#22659;&#30340;&#29366;&#24577;&#36716;&#25442;&#20989;&#25968;&#19968;&#33268;&#19988;&#21487;&#20197;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#30340;&#25915;&#20987;&#30456;&#27604;&#65292;R-attack&#26356;&#38590;&#20197;&#20351;&#29992;&#33258;&#21160;&#21270;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents deployed in the real world need to be robust against adversarial attacks on sensory inputs. Robustifying agent policies requires anticipating the strongest attacks possible. We demonstrate that existing observation-space attacks on reinforcement learning agents have a common weakness: while effective, their lack of temporal consistency makes them detectable using automated means or human inspection. Detectability is undesirable to adversaries as it may trigger security escalations. We introduce perfect illusory attacks, a novel form of adversarial attack on sequential decision-makers that is both effective and provably statistically undetectable. We then propose the more versatile R-attacks, which result in observation transitions that are consistent with the state-transition function of the adversary-free environment and can be learned end-to-end. Compared to existing attacks, we empirically find R-attacks to be significantly harder to detect with automated methods, 
&lt;/p&gt;</description></item></channel></rss>