<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#21010;&#20998;&#20026;&#20803;&#23398;&#20064;&#12289;&#39044;&#35757;&#32451;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01440</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#65306;&#20174;&#20803;&#23398;&#20064;&#21040;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#21010;&#20998;&#20026;&#20803;&#23398;&#20064;&#12289;&#39044;&#35757;&#32451;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#22270;&#20013;&#24515;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#22312;&#36825;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26089;&#26399;&#30340;&#25216;&#26415;&#36890;&#24120;&#22312;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#20013;&#36816;&#34892;&#65292;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20805;&#36275;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#20010;&#38480;&#21046;&#24341;&#21457;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#21482;&#26377;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#26631;&#31614;&#21487;&#29992;&#12290;&#37492;&#20110;&#36825;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#25991;&#29486;&#65292;&#26412;&#32508;&#36848;&#35797;&#22270;&#32508;&#21512;&#26368;&#36817;&#30340;&#21457;&#23637;&#65292;&#25552;&#20379;&#27604;&#36739;&#24615;&#30340;&#35265;&#35299;&#65292;&#24182;&#30830;&#23450;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#31995;&#32479;&#22320;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20803;&#23398;&#20064;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#65292;&#20197;&#24110;&#21161;&#35835;&#32773;&#36827;&#34892;&#26041;&#27861;&#36873;&#25321;&#12290;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements. Earlier techniques often operate in an end-to-end setting, where performance heavily relies on the availability of ample labeled data. This constraint has spurred the emergence of few-shot learning on graphs, where only a few task-specific labels are available for each task. Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions. We systematically categorize existing studies into three major families: meta-learning approaches, pre-training approaches, and hybrid approaches, with a finer-grained classification in each family to aid readers in their method selection process. Within each category, we analyze the relationships among these methods and compare their strengths and limitations. Finally, we outline prospective future directions for few-shot learning on graphs to cata
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18571</link><description>&lt;p&gt;
&#29992;&#20110;&#28385;&#36275;&#22810;&#26679;&#29992;&#25143;&#20559;&#22909;&#30340;&#31639;&#26415;&#25511;&#21046;LLMs&#65306;&#20855;&#26377;&#22810;&#30446;&#26631;&#22870;&#21169;&#30340;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#27169;&#25311;&#19981;&#21516;&#20559;&#22909;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31934;&#32454;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#36866;&#24212;&#21508;&#31181;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26041;&#21521;&#20559;&#22909;&#23545;&#40784;&#65288;DPA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#22870;&#21169;&#24314;&#27169;&#26469;&#34920;&#31034;&#22810;&#26679;&#21270;&#30340;&#20559;&#22909;&#37197;&#32622;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#20026;&#22870;&#21169;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65288;&#21363;&#21333;&#20301;&#21521;&#37327;&#65289;&#20197;&#23454;&#29616;&#29992;&#25143;&#30456;&#20851;&#30340;&#20559;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18571v1 Announce Type: cross  Abstract: Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;LM&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#31454;&#20105;&#24615;&#39044;&#27979;&#24179;&#21488;&#25910;&#38598;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#21518;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#21457;&#29616;&#35813;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20154;&#31867;&#39044;&#27979;&#32773;&#12290;</title><link>https://arxiv.org/abs/2402.18563</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Approaching Human-Level Forecasting with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;LM&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#31454;&#20105;&#24615;&#39044;&#27979;&#24179;&#21488;&#25910;&#38598;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#21518;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#21457;&#29616;&#35813;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20154;&#31867;&#39044;&#27979;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#23545;&#25919;&#31574;&#21644;&#20915;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;(LMs)&#26159;&#21542;&#33021;&#22815;&#22312;&#31454;&#20105;&#24615;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#27700;&#24179;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;LM&#31995;&#32479;&#65292;&#26088;&#22312;&#33258;&#21160;&#25628;&#32034;&#30456;&#20851;&#20449;&#24687;&#12289;&#29983;&#25104;&#39044;&#27979;&#21644;&#32858;&#21512;&#39044;&#27979;&#12290;&#20026;&#20102;&#20419;&#36827;&#30740;&#31350;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#31454;&#20105;&#24615;&#39044;&#27979;&#24179;&#21488;&#30340;&#22823;&#37327;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;&#22312;LM&#30340;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#21457;&#24067;&#30340;&#27979;&#35797;&#38598;&#19979;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#19982;&#20154;&#31867;&#39044;&#27979;&#30340;&#32858;&#21512;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#35813;&#31995;&#32479;&#25509;&#36817;&#20110;&#31454;&#20105;&#39044;&#27979;&#32773;&#30340;&#32858;&#21512;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#23427;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21033;&#29992;LM&#26469;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#20250;&#25552;&#20379;&#20934;&#30830;&#30340;&#22823;&#35268;&#27169;&#39044;&#27979;&#65292;&#24182;&#26377;&#21161;&#20110;&#20026;&#26426;&#26500;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18563v1 Announce Type: cross  Abstract: Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it. Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#8220;&#32431;&#31929;&#35843;&#20248;&#65292;&#23433;&#20840;&#27979;&#35797;&#8221;&#65288;PTST&#65289;&#21407;&#21017;&#65292;&#21363;&#22312;&#24494;&#35843;&#26102;&#19981;&#21253;&#21547;&#23433;&#20840;&#25552;&#31034;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#21152;&#20837;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;LLMs&#20013;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18540</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#21518;&#20445;&#25345;LLMs&#30340;&#23545;&#40784;&#24615;:&#25552;&#31034;&#27169;&#26495;&#30340;&#20851;&#38190;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18540
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#8220;&#32431;&#31929;&#35843;&#20248;&#65292;&#23433;&#20840;&#27979;&#35797;&#8221;&#65288;PTST&#65289;&#21407;&#21017;&#65292;&#21363;&#22312;&#24494;&#35843;&#26102;&#19981;&#21253;&#21547;&#23433;&#20840;&#25552;&#31034;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#21152;&#20837;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;LLMs&#20013;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;LLMs&#65292;&#22914;Llama 2-Chat&#65292;&#25512;&#21160;&#20102;LLM&#30740;&#31350;&#30340;&#24040;&#22823;&#27963;&#21160;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#21382;&#20102;&#23545;&#40784;&#24615;&#35757;&#32451;&#65292;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#12290;&#26368;&#36817;&#65292;&#40784;&#31561;&#20154;&#65288;2023&#24180;&#65289;&#25253;&#21578;&#31216;&#65292;&#21363;&#20351;&#26159;&#33391;&#24615;&#30340;&#24494;&#35843;&#65288;&#20363;&#22914;&#65292;&#22312;&#30475;&#20284;&#23433;&#20840;&#30340;&#25968;&#25454;&#38598;&#19978;&#65289;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20943;&#36731;&#36825;&#31181;&#23545;&#40784;&#24615;&#20002;&#22833;&#30340;&#26041;&#27861;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;&#36890;&#36807;&#23545;&#20960;&#20010;&#32842;&#22825;&#27169;&#22411;&#65288;Meta&#30340;Llama 2-Chat&#65292;Mistral AI&#30340;Mistral 7B Instruct v0.2&#21644;OpenAI&#30340;GPT-3.5 Turbo&#65289;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#26412;&#25991;&#21457;&#29616;&#24494;&#35843;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#27169;&#26495;&#22312;&#20445;&#25345;&#23433;&#20840;&#23545;&#40784;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;&#32431;&#31929;&#35843;&#20248;&#65292;&#23433;&#20840;&#27979;&#35797;&#8221;&#65288;PTST&#65289;&#21407;&#21017; - &#22312;&#27979;&#35797;&#26102;&#19981;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#21253;&#21547;&#23427;&#12290;&#23545;GSM8K&#65292;ChatDoctor&#21644;OpenOrca&#36827;&#34892;&#30340;&#24494;&#35843;&#23454;&#39564;&#34920;&#26126;&#65292;PTST&#26174;&#30528;&#20943;&#23569;&#20102;&#19981;&#23433;&#20840;&#34892;&#20026;&#30340;&#22686;&#21152;&#65292;&#29978;&#33267;&#20960;&#20046;&#28040;&#38500;&#20102;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18540v1 Announce Type: cross  Abstract: Public LLMs such as the Llama 2-Chat have driven huge activity in LLM research. These models underwent alignment training and were considered safe. Recently Qi et al. (2023) reported that even benign fine-tuning (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the "Pure Tuning, Safe Testing" (PTST) principle -- fine-tune models without a safety prompt, but include it at test time. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors, and even almost elimin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#34920;&#36848;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;&#65292;&#36825;&#23545;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18496</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#34920;&#36798;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;
&lt;/p&gt;
&lt;p&gt;
Language Models Represent Beliefs of Self and Others
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18496
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#26512;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#34920;&#36848;&#33258;&#25105;&#21644;&#20182;&#20154;&#20449;&#24565;&#65292;&#36825;&#23545;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#22312;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#24402;&#22240;&#24515;&#29702;&#29366;&#24577;&#65292;&#21363;&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#65292;&#34987;&#35270;&#20026;&#20154;&#31867;&#31038;&#20250;&#25512;&#29702;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20284;&#20046;&#20855;&#26377;&#26576;&#20123;ToM&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#20196;&#20154;&#36153;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#28608;&#27963;&#32447;&#24615;&#35299;&#30721;&#21508;&#20010;&#20195;&#29702;&#20154;&#35266;&#28857;&#19979;&#30340;&#20449;&#24565;&#29366;&#24577;&#26159;&#21487;&#33021;&#30340;&#65292;&#36825;&#34920;&#26126;&#23384;&#22312;&#33258;&#25105;&#30340;&#20869;&#37096;&#34920;&#36848;&#21644;&#20182;&#20154;&#20449;&#24565;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#25805;&#32437;&#36825;&#20123;&#34920;&#24449;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;ToM&#24615;&#33021;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#31038;&#20250;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#24310;&#20280;&#21040;&#28041;&#21450;&#19981;&#21516;&#22240;&#26524;&#25512;&#29702;&#27169;&#24335;&#30340;&#22810;&#26679;&#31038;&#20250;&#25512;&#29702;&#20219;&#21153;&#65292;&#26263;&#31034;&#20102;&#36825;&#20123;&#34920;&#24449;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18496v1 Announce Type: new  Abstract: Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#20154;&#26412;&#20851;&#27880;&#22240;&#32032;&#22312;&#25628;&#32034;&#21644;&#25937;&#25588;&#20219;&#21153;&#20013;&#26080;&#20154;&#26426;&#36712;&#36857;&#35268;&#21010;&#20013;&#30340;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#12289;AHP&#21644;&#22522;&#20110;&#30456;&#20284;&#24615;&#32463;&#39564;&#22238;&#25918;&#30340;&#26032;&#26041;&#27861;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#65292;&#24179;&#34913;&#20102;&#36816;&#33829;&#30446;&#26631;&#19982;&#20154;&#31867;&#33298;&#36866;&#21644;&#23433;&#20840;&#32771;&#34385;&#12290;</title><link>https://arxiv.org/abs/2402.18487</link><description>&lt;p&gt;
&#20154;&#26412;&#20851;&#27880;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#19982;AHP&#21450;&#22522;&#20110;&#30456;&#20284;&#24615;&#32463;&#39564;&#22238;&#25918;&#30340;&#26080;&#20154;&#26426;&#36712;&#36857;&#35268;&#21010;&#22312;&#25628;&#32034;&#21644;&#25937;&#25588;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Human-Centric Aware UAV Trajectory Planning in Search and Rescue Missions Employing Multi-Objective Reinforcement Learning with AHP and Similarity-Based Experience Replay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18487
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#20154;&#26412;&#20851;&#27880;&#22240;&#32032;&#22312;&#25628;&#32034;&#21644;&#25937;&#25588;&#20219;&#21153;&#20013;&#26080;&#20154;&#26426;&#36712;&#36857;&#35268;&#21010;&#20013;&#30340;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#12289;AHP&#21644;&#22522;&#20110;&#30456;&#20284;&#24615;&#32463;&#39564;&#22238;&#25918;&#30340;&#26032;&#26041;&#27861;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#65292;&#24179;&#34913;&#20102;&#36816;&#33829;&#30446;&#26631;&#19982;&#20154;&#31867;&#33298;&#36866;&#21644;&#23433;&#20840;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26080;&#20154;&#26426;&#65288;UAVs&#65289;&#21040;&#25628;&#32034;&#21644;&#25937;&#25588;&#65288;SAR&#65289;&#20219;&#21153;&#20013;&#20026;&#25552;&#39640;&#25805;&#20316;&#25928;&#29575;&#21644;&#25928;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#25104;&#21151;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#26080;&#20154;&#26426;&#30340;&#25216;&#26415;&#33021;&#21147;&#65292;&#20063;&#21462;&#20915;&#20110;&#23427;&#20204;&#19982;&#22320;&#38754;&#20154;&#21592;&#30340;&#25509;&#21463;&#21644;&#20114;&#21160;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#26412;&#22240;&#32032;&#22312;SAR&#20219;&#21153;&#20013;&#26080;&#20154;&#26426;&#36712;&#36857;&#35268;&#21010;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#23618;&#27425;&#20998;&#26512;&#36807;&#31243;&#21644;&#26032;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#32463;&#39564;&#22238;&#25918;&#20248;&#21270;&#26080;&#20154;&#26426;&#36712;&#36857;&#65292;&#24179;&#34913;&#25805;&#20316;&#30446;&#26631;&#19982;&#20154;&#31867;&#33298;&#36866;&#24615;&#21644;&#23433;&#20840;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19968;&#39033;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26080;&#20154;&#26426;&#35774;&#35745;&#20013;&#24615;&#21035;&#26263;&#31034;&#21644;&#25311;&#20154;&#21270;&#23545;&#20844;&#20247;&#25509;&#21463;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#23545;SAR&#20013;&#26080;&#20154;&#26426;&#20114;&#21160;&#31574;&#30053;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18487v1 Announce Type: cross  Abstract: The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue (SAR) missions presents a promising avenue for enhancing operational efficiency and effectiveness. However, the success of these missions is not solely dependent on the technical capabilities of the drones but also on their acceptance and interaction with humans on the ground. This paper explores the effect of human-centric factor in UAV trajectory planning for SAR missions. We introduce a novel approach based on the reinforcement learning augmented with Analytic Hierarchy Process and novel similarity-based experience replay to optimize UAV trajectories, balancing operational objectives with human comfort and safety considerations. Additionally, through a comprehensive survey, we investigate the impact of gender cues and anthropomorphism in UAV design on public acceptance and trust, revealing significant implications for drone interaction strategies in SAR. Our c
&lt;/p&gt;</description></item><item><title>FinAgent&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#20195;&#29702;&#65292;&#36890;&#36807;&#24037;&#20855;&#22686;&#24378;&#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#65292;&#20855;&#26377;&#29420;&#29305;&#30340;&#21452;&#37325;&#21453;&#23556;&#27169;&#22359;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#24182;&#24555;&#36895;&#36866;&#24212;&#24066;&#22330;&#21160;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.18485</link><description>&lt;p&gt;
FinAgent: &#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#20195;&#29702;&#65306;&#24037;&#20855;&#22686;&#24378;&#12289;&#22810;&#26679;&#21270;&#21644;&#36890;&#29992;
&lt;/p&gt;
&lt;p&gt;
FinAgent: A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18485
&lt;/p&gt;
&lt;p&gt;
FinAgent&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#20195;&#29702;&#65292;&#36890;&#36807;&#24037;&#20855;&#22686;&#24378;&#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#65292;&#20855;&#26377;&#29420;&#29305;&#30340;&#21452;&#37325;&#21453;&#23556;&#27169;&#22359;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#24182;&#24555;&#36895;&#36866;&#24212;&#24066;&#22330;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20132;&#26131;&#26159;&#24066;&#22330;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21463;&#21040;&#26032;&#38395;&#12289;&#20215;&#26684;&#21644;K&#32447;&#22270;&#31561;&#22810;&#27169;&#24577;&#20449;&#24687;&#26500;&#25104;&#30340;&#20449;&#24687;&#26223;&#35266;&#30340;&#24433;&#21709;&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#37327;&#21270;&#20132;&#26131;&#21644;&#19981;&#21516;&#36164;&#20135;&#30340;&#39640;&#39057;&#20132;&#26131;&#31561;&#22810;&#26679;&#21270;&#20219;&#21153;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#20808;&#36827;AI&#25216;&#26415;&#22312;&#37329;&#34701;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#37329;&#34701;&#20132;&#26131;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#38754;&#20020;&#30528;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#19981;&#36275;&#21644;&#36328;&#19981;&#21516;&#20219;&#21153;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FinAgent&#65292;&#19968;&#20010;&#20855;&#26377;&#24037;&#20855;&#22686;&#24378;&#21151;&#33021;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#20195;&#29702;&#65292;&#29992;&#20110;&#37329;&#34701;&#20132;&#26131;&#12290;FinAgent&#30340;&#24066;&#22330;&#26234;&#33021;&#27169;&#22359;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;-&#25968;&#20540;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;-&#20197;&#20934;&#30830;&#20998;&#26512;&#37329;&#34701;&#24066;&#22330;&#12290;&#20854;&#29420;&#29305;&#30340;&#21452;&#37325;&#21453;&#23556;&#27169;&#22359;&#19981;&#20165;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#24066;&#22330;&#21160;&#24577;&#65292;&#36824;&#34701;&#21512;&#20102;&#22810;&#26679;&#21270;&#30340;&#35760;&#24518;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18485v1 Announce Type: cross  Abstract: Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#26680;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#20197;&#21450;&#24320;&#21457;&#20102;&#32422;&#26463;&#26465;&#20214;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.18477</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#31614;&#21517;&#26680;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#29992;&#20110;&#38543;&#26426;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#26680;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#20197;&#21450;&#24320;&#21457;&#20102;&#32422;&#26463;&#26465;&#20214;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#22312;&#31185;&#23398;&#12289;&#20581;&#24247;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#31614;&#21517;&#26680;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#30340;&#8220;&#36335;&#24452;&#31354;&#38388;&#8221;&#19978;&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#27979;&#35797;&#65292;&#29992;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#36335;&#24452;&#31354;&#38388;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;CI&#27979;&#35797;&#34920;&#29616;&#20986;&#20005;&#26684;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#38750;&#24490;&#29615;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#24320;&#21457;&#20102;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#26469;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;&#22312;&#20551;&#35774;&#24544;&#23454;&#24615;&#21644;CI&#39044;&#35328;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#23436;&#22791;&#19988;&#27491;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18477v1 Announce Type: cross  Abstract: Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via "which variables enter the differential of which other variables". In this paper, we develop a kernel-based test of conditional independence (CI) on "path-space" -- solutions to SDEs -- by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph. Assuming faithfulness and a CI oracle, our algorithm is sound and complete. We empirical
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;HOP&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#19977;&#20010;&#26041;&#21521;&#20197;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36328;&#20219;&#21153;&#21644;&#39046;&#22495;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18449</link><description>&lt;p&gt;
HOP&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36830;&#32493;&#23398;&#20064;&#30340;&#19979;&#19968;&#20010;&#20219;&#21153;&#21644;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
HOP to the Next Tasks and Domains for Continual Learning in NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18449
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;HOP&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#19977;&#20010;&#26041;&#21521;&#20197;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36328;&#20219;&#21153;&#21644;&#39046;&#22495;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26088;&#22312;&#36890;&#36807;&#36716;&#31227;&#20808;&#21069;&#38382;&#39064;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#26469;&#23398;&#20064;&#19968;&#31995;&#21015;&#38382;&#39064;&#65288;&#21363;&#20219;&#21153;&#21644;&#39046;&#22495;&#65289;&#65292;&#21516;&#26102;&#36991;&#20813;&#36951;&#24536;&#36807;&#21435;&#30340;&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#19987;&#27880;&#20110;&#29305;&#23450;&#29992;&#20363;&#20013;&#19968;&#20010;NLP&#20219;&#21153;&#25110;&#39046;&#22495;&#30340;CL&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;CL&#35774;&#32622;&#65292;&#20174;&#19968;&#20010;&#21807;&#19968;&#30340;&#26694;&#26550;&#20013;&#23398;&#20064;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;HOP&#36890;&#36807;&#27839;&#19977;&#20010;&#26041;&#21521;&#35299;&#20915;CL&#38382;&#39064;&#26469;&#20801;&#35768;&#22312;&#20219;&#21153;&#21644;&#39046;&#22495;&#20043;&#38388;&#36339;&#36291;&#65306;&#65288;i&#65289;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#36866;&#37197;&#22120;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25512;&#24191;&#21040;&#26410;&#35265;&#38382;&#39064;&#65292;&#65288;ii&#65289;&#25105;&#20204;&#35745;&#31639;&#23884;&#20837;&#34920;&#31034;&#20998;&#24067;&#19978;&#30340;&#39640;&#38454;&#30697;&#20197;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#20043;&#38388;&#30340;&#29420;&#31435;&#21644;&#30456;&#20851;&#32479;&#35745;&#25968;&#25454;&#65292;&#65288;iii&#65289;&#25105;&#20204;&#36890;&#36807;&#20026;&#27599;&#20010;&#26368;&#32456;&#38382;&#39064;&#19987;&#38376;&#35774;&#35745;&#30340;&#36741;&#21161;&#22836;&#22788;&#29702;&#36825;&#20123;&#20016;&#23500;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;4&#20010;NLP&#24212;&#29992;&#31243;&#24207;&#65292;5&#20010;&#22522;&#20934;&#27979;&#35797;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18449v1 Announce Type: cross  Abstract: Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and domains) by transferring knowledge acquired on previous problems, whilst avoiding forgetting of past ones. Different from previous approaches which focused on CL for one NLP task or domain in a specific use-case, in this paper, we address a more general CL setting to learn from a sequence of problems in a unique framework. Our method, HOP, permits to hop across tasks and domains by addressing the CL problem along three directions: (i) we employ a set of adapters to generalize a large pre-trained model to unseen problems, (ii) we compute high-order moments over the distribution of embedded representations to distinguish independent and correlated statistics across different tasks and domains, (iii) we process this enriched information with auxiliary heads specialized for each end problem. Extensive experimental campaign on 4 NLP applications, 5 benchmarks and 
&lt;/p&gt;</description></item><item><title>LeMo-NADe&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#23450;&#20041;&#30340;&#21442;&#25968;&#12289;&#19987;&#23478;&#31995;&#32479;&#21644;&#22823;&#37327;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#65292;&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#38750;AI&#19987;&#23478;&#65292;&#26080;&#38656;&#39044;&#35774;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#32771;&#34385;&#20102;&#22823;&#37327;&#36793;&#32536;&#35774;&#22791;&#29305;&#23450;&#30340;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.18443</link><description>&lt;p&gt;
LeMo-NADe: &#22522;&#20110;LLMs&#30340;&#22810;&#21442;&#25968;&#31070;&#32463;&#26550;&#26500;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18443
&lt;/p&gt;
&lt;p&gt;
LeMo-NADe&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#23450;&#20041;&#30340;&#21442;&#25968;&#12289;&#19987;&#23478;&#31995;&#32479;&#21644;&#22823;&#37327;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#65292;&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#38750;AI&#19987;&#23478;&#65292;&#26080;&#38656;&#39044;&#35774;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#32771;&#34385;&#20102;&#22823;&#37327;&#36793;&#32536;&#35774;&#22791;&#29305;&#23450;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21487;&#33021;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#38656;&#35201;&#24191;&#27867;&#19987;&#19994;&#30693;&#35782;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#36793;&#32536;&#35774;&#22791;&#26469;&#35828;&#65292;&#36825;&#39033;&#20219;&#21153;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#20204;&#24517;&#39035;&#32771;&#34385;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#21151;&#32791;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#25512;&#29702;&#36895;&#24230;&#21644;CO2&#25490;&#25918;&#37327;&#31561;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#23450;&#20041;&#30340;&#21442;&#25968;&#12289;&#19987;&#23478;&#31995;&#32479;&#21644;&#22312;&#22823;&#37327;&#24320;&#25918;&#39046;&#22495;&#30693;&#35782;&#19978;&#35757;&#32451;&#30340;LLM&#65292;&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#24341;&#20837;&#30340;&#26694;&#26550;&#65288;LeMo-NADe&#65289;&#26088;&#22312;&#20379;&#38750;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#20351;&#29992;&#65292;&#19981;&#38656;&#35201;&#39044;&#20808;&#30830;&#23450;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#32771;&#34385;&#20102;&#22823;&#37327;&#36793;&#32536;&#35774;&#22791;&#29305;&#23450;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;GPT-4 Turbo&#21644;Gemini&#20316;&#20026;LLM&#32452;&#20214;&#65292;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet16-120&#25968;&#25454;&#38598;&#19978;&#23454;&#26045;&#21644;&#39564;&#35777;&#20102;&#36825;&#19968;&#25552;&#20986;&#30340;&#31070;&#32463;&#26550;&#26500;&#21457;&#29616;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18443v1 Announce Type: cross  Abstract: Building efficient neural network architectures can be a time-consuming task requiring extensive expert knowledge. This task becomes particularly challenging for edge devices because one has to consider parameters such as power consumption during inferencing, model size, inferencing speed, and CO2 emissions. In this article, we introduce a novel framework designed to automatically discover new neural network architectures based on user-defined parameters, an expert system, and an LLM trained on a large amount of open-domain knowledge. The introduced framework (LeMo-NADe) is tailored to be used by non-AI experts, does not require a predetermined neural architecture search space, and considers a large set of edge device-specific parameters. We implement and validate this proposed neural architecture discovery framework using CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using GPT-4 Turbo and Gemini as the LLM component. We obser
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;LLMs&#22312;&#25512;&#29702;&#21644;&#27807;&#36890;&#20013;&#20351;&#29992;&#26367;&#20195;&#26684;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#25512;&#29702;&#25928;&#29575;&#30340;&#25552;&#21319;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26102;&#26631;&#35760;&#20351;&#29992;&#37327;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.18439</link><description>&lt;p&gt;
&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#65306;LLM&#21033;&#29992;&#26367;&#20195;&#26684;&#24335;&#36827;&#34892;&#22686;&#24378;&#25512;&#29702;&#21644;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18439
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;LLMs&#22312;&#25512;&#29702;&#21644;&#27807;&#36890;&#20013;&#20351;&#29992;&#26367;&#20195;&#26684;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#25512;&#29702;&#25928;&#29575;&#30340;&#25552;&#21319;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26102;&#26631;&#35760;&#20351;&#29992;&#37327;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#20154;&#31867;&#35748;&#30693;&#21644;&#27807;&#36890;&#30340;&#20027;&#35201;&#26684;&#24335;&#65292;&#22240;&#27492;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#20013;&#21516;&#26679;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;NL&#20043;&#22806;&#65292;LLMs&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30475;&#21040;&#20102;&#21508;&#31181;&#38750;NL&#26684;&#24335;&#65292;&#22914;&#20195;&#30721;&#21644;&#36923;&#36753;&#34920;&#36798;&#24335;&#12290;NL&#20316;&#20026;LLMs&#30340;&#26368;&#20339;&#26684;&#24335;&#65292;&#22312;&#21333;&#19968;LLM&#25512;&#29702;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#20013;&#30340;&#22320;&#20301;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#23457;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;NL&#65292;&#36890;&#36807;&#25506;&#32034;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#38750;NL&#26684;&#24335;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#20801;&#35768;LLMs&#22312;&#25512;&#29702;&#25110;&#27807;&#36890;&#20043;&#21069;&#33258;&#20027;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26684;&#24335;&#65292;&#21487;&#23548;&#33268;&#19981;&#21516;LLMs&#25512;&#29702;&#25928;&#29575;&#25552;&#39640;3.3&#33267;5.7&#65285;&#65292;&#24182;&#19988;&#22312;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#20013;&#26368;&#22810;&#21487;&#20943;&#23569;72.7&#65285;&#30340;&#26631;&#35760;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#27807;&#36890;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;LLMs&#21487;&#20197;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18439v1 Announce Type: cross  Abstract: Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs c
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#20851;&#31995;&#24402;&#32435;&#20559;&#22909;&#23545;&#32500;&#24230;&#25277;&#35937;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20851;&#31995;&#29942;&#39048;&#26426;&#21046;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#21644;&#23398;&#20064;&#25928;&#29575;&#65292;&#20351;&#32593;&#32476;&#34920;&#29616;&#19982;&#20154;&#31867;&#34892;&#20026;&#20559;&#22909;&#19968;&#33268;&#12290; - &#20851;&#31995;&#29942;&#39048;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#25277;&#35937;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20419;&#36827;&#32593;&#32476;&#22312;&#32500;&#24230;&#19978;&#36827;&#34892;&#32452;&#21512;&#32534;&#30721;&#65292;&#25552;&#39640;&#22788;&#29702;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18426</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#20851;&#31995;&#24402;&#32435;&#20559;&#22909;&#23545;&#32500;&#24230;&#25277;&#35937;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
A Relational Inductive Bias for Dimensional Abstraction in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18426
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#20851;&#31995;&#24402;&#32435;&#20559;&#22909;&#23545;&#32500;&#24230;&#25277;&#35937;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20851;&#31995;&#29942;&#39048;&#26426;&#21046;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#21644;&#23398;&#20064;&#25928;&#29575;&#65292;&#20351;&#32593;&#32476;&#34920;&#29616;&#19982;&#20154;&#31867;&#34892;&#20026;&#20559;&#22909;&#19968;&#33268;&#12290; - &#20851;&#31995;&#29942;&#39048;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#25277;&#35937;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20419;&#36827;&#32593;&#32476;&#22312;&#32500;&#24230;&#19978;&#36827;&#34892;&#32452;&#21512;&#32534;&#30721;&#65292;&#25552;&#39640;&#22788;&#29702;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35748;&#30693;&#31995;&#32479;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#28789;&#27963;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#20854;&#33021;&#22815;&#24418;&#25104;&#29615;&#22659;&#30340;&#20302;&#32500;&#12289;&#32452;&#21512;&#34920;&#31034;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24120;&#24120;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#12289;&#36807;&#25311;&#21512;&#21644;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20851;&#31995;&#29942;&#39048;&#30340;&#24433;&#21709; - &#36825;&#26159;&#19968;&#31181;&#23558;&#22788;&#29702;&#38598;&#20013;&#22312;&#36755;&#20837;&#20043;&#38388;&#20851;&#31995;&#19978;&#30340;&#26426;&#21046; - &#23545;&#23398;&#20064;&#26377;&#21033;&#20110;&#32452;&#25104;&#32534;&#30721;&#21644;&#30456;&#24212;&#22788;&#29702;&#28789;&#27963;&#24615;&#30340;&#20998;&#35299;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#29942;&#39048;&#19981;&#20165;&#25552;&#39640;&#20102;&#27867;&#21270;&#21644;&#23398;&#20064;&#25928;&#29575;&#65292;&#36824;&#20351;&#32593;&#32476;&#34920;&#29616;&#19982;&#31867;&#20284;&#20154;&#31867;&#30340;&#34892;&#20026;&#20559;&#22909;&#19968;&#33268;&#12290;&#32463;&#36807;&#20851;&#31995;&#29942;&#39048;&#35757;&#32451;&#30340;&#32593;&#32476;&#21457;&#23637;&#20986;&#20102;&#22312;&#25968;&#25454;&#38598;&#20013;&#28508;&#22312;&#30340;&#29305;&#24449;&#32500;&#24230;&#19978;&#27491;&#20132;&#30340;&#34920;&#31034;&#65292;&#21453;&#26144;&#20102;&#34987;&#35748;&#20026;&#23384;&#22312;&#30340;&#20998;&#35299;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18426v1 Announce Type: new  Abstract: The human cognitive system exhibits remarkable flexibility and generalization capabilities, partly due to its ability to form low-dimensional, compositional representations of the environment. In contrast, standard neural network architectures often struggle with abstract reasoning tasks, overfitting, and requiring extensive data for training. This paper investigates the impact of the relational bottleneck -- a mechanism that focuses processing on relations among inputs -- on the learning of factorized representations conducive to compositional coding and the attendant flexibility of processing. We demonstrate that such a bottleneck not only improves generalization and learning efficiency, but also aligns network performance with human-like behavioral biases. Networks trained with the relational bottleneck developed orthogonal representations of feature dimensions latent in the dataset, reflecting the factorized structure thought to unde
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#22312;&#20302;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#20013;&#23454;&#29616;&#24773;&#24863;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18424</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Emotion Classification in Low and Moderate Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18424
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#22312;&#20302;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#20013;&#23454;&#29616;&#24773;&#24863;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20998;&#26512;&#20840;&#29699;&#33539;&#22260;&#20869;&#20154;&#20204;&#24773;&#32490;&#29366;&#24577;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#20840;&#29699;&#26377;7100&#22810;&#31181;&#27963;&#36291;&#35821;&#35328;&#65292;&#20026;&#27599;&#31181;&#35821;&#35328;&#26500;&#24314;&#24773;&#24863;&#20998;&#31867;&#26159;&#19968;&#39033;&#21171;&#21160;&#23494;&#38598;&#22411;&#24037;&#20316;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#21644;&#28626;&#21361;&#35821;&#35328;&#65292;&#24314;&#31435;&#24773;&#24863;&#20998;&#31867;&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65288;&#20363;&#22914;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#30340;&#33521;&#35821;&#65289;&#19978;&#35757;&#32451;&#24773;&#24863;&#20998;&#31867;&#22120;&#65292;&#24182;&#23558;&#23398;&#20064;&#36801;&#31227;&#21040;&#20302;&#36164;&#28304;&#21644;&#20013;&#31561;&#36164;&#28304;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#27604;&#36739;&#24182;&#23545;&#27604;&#20102;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#25110;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#30340;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#19968;&#31181;&#26041;&#27861;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#26631;&#27880;&#25237;&#24433;&#21040;&#20302;&#36164;&#28304;&#21644;&#20013;&#31561;&#36164;&#28304;&#35821;&#35328;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#65292;&#21478;&#19968;&#31181;&#26041;&#27861;&#30452;&#25509;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#23398;&#20064;&#36801;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;6&#31181;&#35821;&#35328;&#19978;&#30340;&#26377;&#25928;&#24615;&#65306;Fa
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18424v1 Announce Type: cross  Abstract: It is important to be able to analyze the emotional state of people around the globe. There are 7100+ active languages spoken around the world and building emotion classification for each language is labor intensive. Particularly for low-resource and endangered languages, building emotion classification can be quite challenging. We present a cross-lingual emotion classifier, where we train an emotion classifier with resource-rich languages (i.e. \textit{English} in our work) and transfer the learning to low and moderate resource languages. We compare and contrast two approaches of transfer learning from a high-resource language to a low or moderate-resource language. One approach projects the annotation from a high-resource language to low and moderate-resource language in parallel corpora and the other one uses direct transfer from high-resource language to the other languages. We show the efficacy of our approaches on 6 languages: Fa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38382;&#31572;&#20219;&#21153;&#65292;GPT&#33021;&#22815;&#39564;&#35777;&#21307;&#30103;&#39046;&#22495;&#24739;&#32773;&#30340;PA&#35831;&#27714;&#65292;&#24110;&#21161;&#21355;&#29983;&#35745;&#21010;&#26356;&#24555;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.18419</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#22522;&#20110;&#25351;&#21335;&#30340;&#33258;&#21160;&#38382;&#31572;&#26469;&#25913;&#21892;GPT&#30340;&#20808;&#21069;&#25480;&#26435;&#29366;&#24577;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18419
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38382;&#31572;&#20219;&#21153;&#65292;GPT&#33021;&#22815;&#39564;&#35777;&#21307;&#30103;&#39046;&#22495;&#24739;&#32773;&#30340;PA&#35831;&#27714;&#65292;&#24110;&#21161;&#21355;&#29983;&#35745;&#21010;&#26356;&#24555;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#29983;&#20445;&#38505;&#20844;&#21496;&#26377;&#19968;&#20010;&#34987;&#31216;&#20026;&#20808;&#21069;&#25480;&#26435;&#65288;PA&#65289;&#30340;&#27969;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#21355;&#29983;&#35745;&#21010;&#25104;&#26412;&#25511;&#21046;&#27969;&#31243;&#65292;&#35201;&#27714;&#21307;&#29983;&#21644;&#20854;&#20182;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#22312;&#23545;&#24739;&#32773;&#25191;&#34892;&#29305;&#23450;&#31243;&#24207;&#20043;&#21069;&#24517;&#39035;&#20107;&#20808;&#33719;&#24471;&#21355;&#29983;&#35745;&#21010;&#30340;&#25209;&#20934;&#65292;&#20197;&#20415;&#26377;&#36164;&#26684;&#33719;&#24471;&#25903;&#20184;&#35206;&#30422;&#12290;&#23545;&#21355;&#29983;&#20445;&#38505;&#20844;&#21496;&#26469;&#35828;&#65292;&#25209;&#20934;&#21307;&#30103;&#39046;&#22495;&#24739;&#32773;&#30340;PA&#35831;&#27714;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20854;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#25361;&#25112;&#26159;&#39564;&#35777;&#35831;&#27714;&#26159;&#21542;&#31526;&#21512;&#26576;&#20123;&#26631;&#20934;&#65292;&#22914;&#24180;&#40836;&#12289;&#24615;&#21035;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT&#26159;&#21542;&#33021;&#39564;&#35777;&#22823;&#37327;&#20851;&#38190;&#22240;&#32032;&#65292;&#20174;&#32780;&#24110;&#21161;&#21355;&#29983;&#35745;&#21010;&#26356;&#24555;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;&#20854;&#26500;&#24314;&#20026;&#19968;&#20010;&#38382;&#31572;&#20219;&#21153;&#65292;&#20419;&#20351;GPT&#20174;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#22238;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#20256;&#32479;&#25552;&#31034;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#26032;&#39062;&#25552;&#31034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18419v1 Announce Type: cross  Abstract: Health insurance companies have a defined process called prior authorization (PA) which is a health plan cost-control process that requires doctors and other healthcare professionals to get clearance in advance from a health plan before performing a particular procedure on a patient in order to be eligible for payment coverage. For health insurance companies, approving PA requests for patients in the medical domain is a time-consuming and challenging task. One of those key challenges is validating if a request matches up to certain criteria such as age, gender, etc. In this work, we evaluate whether GPT can validate numerous key factors, in turn helping health plans reach a decision drastically faster. We frame it as a question answering task, prompting GPT to answer a question from patient electronic health record. We experiment with different conventional prompting techniques as well as introduce our own novel prompting technique. Mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.18409</link><description>&lt;p&gt;
&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22270;&#20687;&#25512;&#29702;&#21644;&#25551;&#36848;&#30340;&#35748;&#30693;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18409
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(LVLMs)&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#21463;&#21040;&#20840;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#27979;&#35797;&#12290;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#27979;&#35797;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#8220;&#20599;&#39292;&#24178;&#8221;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;&#20855;&#26377;&#20016;&#23500;&#35821;&#20041;&#30340;&#22270;&#20687;&#35780;&#20272;LVLMs&#30340;&#39640;&#32423;&#35748;&#30693;&#33021;&#21147;&#12290;&#23427;&#23450;&#20041;&#20102;&#20843;&#31181;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21253;&#25324;&#22270;&#20687;&#25551;&#36848;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;&#30693;&#21517;LVLMs&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;LVLMs&#21644;&#20154;&#31867;&#20043;&#38388;&#20173;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18409v1 Announce Type: new  Abstract: Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the "Cookie Theft" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30528;&#37325;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#20915;&#31574;&#36136;&#37327;&#65292;&#25552;&#20986;&#20102;&#26816;&#27979;&#38750;&#26368;&#20339;&#20915;&#31574;&#22330;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#21464;&#24418;&#20851;&#31995;&#26292;&#38706;&#26368;&#20339;&#20915;&#31574;&#36829;&#35268;&#12290;</title><link>https://arxiv.org/abs/2402.18393</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#24418;&#27979;&#35797;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#30340;&#20915;&#31574;&#26368;&#20339;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Decision Optimality of Autonomous Driving via Metamorphic Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#20915;&#31574;&#36136;&#37327;&#65292;&#25552;&#20986;&#20102;&#26816;&#27979;&#38750;&#26368;&#20339;&#20915;&#31574;&#22330;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#21464;&#24418;&#20851;&#31995;&#26292;&#38706;&#26368;&#20339;&#20915;&#31574;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18393v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65288;ADS&#65289;&#30340;&#27979;&#35797;&#22312;ADS&#24320;&#21457;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#30446;&#21069;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#38750;&#23433;&#20840;&#20851;&#38190;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;ADS&#21046;&#23450;&#26368;&#20339;&#20915;&#31574;&#24182;&#20026;&#33258;&#21160;&#36710;&#36742;&#65288;AV&#65289;&#29983;&#25104;&#26368;&#20339;&#36335;&#24452;&#30340;&#33021;&#21147;&#21516;&#26679;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;AV&#30340;&#26234;&#33021;&#24615;&#24182;&#38477;&#20302;&#39118;&#38505;&#12290;&#30446;&#21069;&#65292;&#40092;&#26377;&#24037;&#20316;&#33268;&#21147;&#20110;&#35780;&#20272;ADS&#30340;&#26368;&#20339;&#20915;&#31574;&#24615;&#33021;&#65292;&#22240;&#20026;&#32570;&#20047;&#30456;&#24212;&#30340;&#39044;&#35328;&#21644;&#29983;&#25104;&#26377;&#38750;&#26368;&#20339;&#20915;&#31574;&#30340;&#22330;&#26223;&#38590;&#24230;&#36739;&#22823;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#35780;&#20272;ADS&#30340;&#20915;&#31574;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#39318;&#20010;&#29992;&#20110;&#26816;&#27979;&#38750;&#26368;&#20339;&#20915;&#31574;&#22330;&#26223;&#65288;NoDSs&#65289;&#30340;&#26041;&#27861;&#65292;&#21363;ADS&#26410;&#35745;&#31639;AV&#30340;&#26368;&#20339;&#36335;&#24452;&#30340;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#20026;&#35299;&#20915;&#39044;&#35328;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#26292;&#38706;&#26368;&#20339;&#20915;&#31574;&#36829;&#35268;&#24773;&#20917;&#30340;&#26032;&#39062;&#21464;&#24418;&#20851;&#31995;&#65288;MR&#65289;&#12290;&#36825;&#20010;MR&#30830;&#23450;&#20102;&#24615;&#33021;&#26368;&#20339;&#20915;&#31574;&#30340;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18393v1 Announce Type: new  Abstract: Autonomous Driving System (ADS) testing is crucial in ADS development, with the current primary focus being on safety. However, the evaluation of non-safety-critical performance, particularly the ADS's ability to make optimal decisions and produce optimal paths for autonomous vehicles (AVs), is equally vital to ensure the intelligence and reduce risks of AVs. Currently, there is little work dedicated to assessing ADSs' optimal decision-making performance due to the lack of corresponding oracles and the difficulty in generating scenarios with non-optimal decisions. In this paper, we focus on evaluating the decision-making quality of an ADS and propose the first method for detecting non-optimal decision scenarios (NoDSs), where the ADS does not compute optimal paths for AVs. Firstly, to deal with the oracle problem, we propose a novel metamorphic relation (MR) aimed at exposing violations of optimal decisions. The MR identifies the propert
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#20581;&#22766;&#24230;&#37327;&#65288;DRM&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#29702;&#24819;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#20013;&#20581;&#22766;&#20272;&#35745;&#22120;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.18392</link><description>&lt;p&gt;
&#25581;&#31034;&#20581;&#22766;&#24615;&#22312;&#35780;&#20272;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Potential of Robustness in Evaluating Causal Inference Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18392
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#20581;&#22766;&#24230;&#37327;&#65288;DRM&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#29702;&#24819;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#20013;&#20581;&#22766;&#20272;&#35745;&#22120;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#23545;&#20010;&#24615;&#21270;&#20915;&#31574;&#21046;&#23450;&#30340;&#38656;&#27714;&#23548;&#33268;&#20154;&#20204;&#23545;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#20132;&#21449;&#39046;&#22495;&#24050;&#32463;&#20135;&#29983;&#20102;&#21508;&#31181;&#26377;&#25928;&#30340;CATE&#20272;&#35745;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#36825;&#20123;&#20272;&#35745;&#22120;&#36890;&#24120;&#21463;&#21046;&#20110;&#32570;&#20047;&#21453;&#20107;&#23454;&#26631;&#31614;&#65292;&#22240;&#27492;&#20351;&#29992;&#20256;&#32479;&#30340;&#20132;&#21449;&#39564;&#35777;&#31561;&#27169;&#22411;&#36873;&#25321;&#31243;&#24207;&#26469;&#36873;&#25321;&#29702;&#24819;&#30340;CATE&#20272;&#35745;&#22120;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;CATE&#20272;&#35745;&#22120;&#36873;&#25321;&#26041;&#27861;&#65292;&#22914;&#25554;&#20540;&#21644;&#20266;&#32467;&#26524;&#24230;&#37327;&#65292;&#38754;&#20020;&#30528;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#38656;&#35201;&#30830;&#23450;&#24230;&#37327;&#24418;&#24335;&#21644;&#25311;&#21512;&#24178;&#25200;&#21442;&#25968;&#25110;&#25554;&#20214;&#23398;&#20064;&#32773;&#30340;&#22522;&#30784;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#32570;&#20047;&#38024;&#23545;&#36873;&#25321;&#20581;&#22766;&#20272;&#35745;&#22120;&#30340;&#29305;&#23450;&#37325;&#28857;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20998;&#24067;&#24335;&#20581;&#22766;&#24230;&#37327;&#65288;DRM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18392v1 Announce Type: cross  Abstract: The growing demand for personalized decision-making has led to a surge of interest in estimating the Conditional Average Treatment Effect (CATE). The intersection of machine learning and causal inference has yielded various effective CATE estimators. However, deploying these estimators in practice is often hindered by the absence of counterfactual labels, making it challenging to select the desirable CATE estimator using conventional model selection procedures like cross-validation. Existing approaches for CATE estimator selection, such as plug-in and pseudo-outcome metrics, face two inherent challenges. Firstly, they are required to determine the metric form and the underlying machine learning models for fitting nuisance parameters or plug-in learners. Secondly, they lack a specific focus on selecting a robust estimator. To address these challenges, this paper introduces a novel approach, the Distributionally Robust Metric (DRM), for 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#24494;&#32593;&#20013;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#22312;&#27599;&#20010;&#33410;&#28857;&#19978;&#21327;&#21516;&#22312;&#32447;&#35757;&#32451;&#30340;&#31070;&#32463;&#24418;&#24577;&#23398;&#20064;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#20869;&#22312;&#26041;&#24335;&#20197;&#20248;&#21270;&#31995;&#32479;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18390</link><description>&lt;p&gt;
&#24494;&#32593;&#20013;&#31070;&#32463;&#24418;&#24577;&#20107;&#20214;&#39537;&#21160;&#30340;&#35821;&#20041;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Neuromorphic Event-Driven Semantic Communication in Microgrids
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18390
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#24494;&#32593;&#20013;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#22312;&#27599;&#20010;&#33410;&#28857;&#19978;&#21327;&#21516;&#22312;&#32447;&#35757;&#32451;&#30340;&#31070;&#32463;&#24418;&#24577;&#23398;&#20064;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#20869;&#22312;&#26041;&#24335;&#20197;&#20248;&#21270;&#31995;&#32479;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#36890;&#20449;&#12289;&#35745;&#31639;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#27491;&#22312;&#25581;&#31034;&#24494;&#32593;&#21327;&#35843;&#36816;&#34892;&#21644;&#38887;&#24615;&#30340;&#26032;&#26041;&#21521;&#12290;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#20301;&#32622;&#36827;&#34892;&#20998;&#24067;&#24335;&#12289;&#27880;&#37325;&#38544;&#31169;&#30340;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#28304;&#20043;&#38388;&#30340;&#21327;&#35843;&#65292;&#32780;&#21478;&#19968;&#26041;&#38754;&#65292;&#36825;&#20063;&#20026;&#23545;&#25163;&#21019;&#36896;&#20102;&#22806;&#29983;&#25968;&#25454;&#21040;&#36798;&#36335;&#24452;&#65292;&#21487;&#33021;&#23548;&#33268;&#36890;&#20449;&#23618;&#20013;&#30340;&#32593;&#32476;&#29289;&#29702;&#25915;&#20987;&#31561;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#36825;&#19968;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#38656;&#35201;&#26032;&#30340;&#20869;&#22312;&#26041;&#24335;&#65292;&#22312;&#21151;&#29575;&#32447;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#65292;&#20197;&#20248;&#21270;&#31995;&#32479;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#36229;&#36234;&#29616;&#26377;&#30340;&#21463;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#38480;&#21046;&#30340;&#21151;&#29575;&#21644;&#25968;&#25454;&#20849;&#36716;&#31227;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#22312;&#27599;&#20010;&#33410;&#28857;&#19978;&#21327;&#21516;&#22312;&#32447;&#35757;&#32451;&#30340;&#31070;&#32463;&#24418;&#24577;&#23398;&#20064;&#65292;&#26469;&#26893;&#20837;&#20132;&#27969;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18390v1 Announce Type: cross  Abstract: Synergies between advanced communications, computing and artificial intelligence are unraveling new directions of coordinated operation and resiliency in microgrids. On one hand, coordination among sources is facilitated by distributed, privacy-minded processing at multiple locations, whereas on the other hand, it also creates exogenous data arrival paths for adversaries that can lead to cyber-physical attacks amongst other reliability issues in the communication layer. This long-standing problem necessitates new intrinsic ways of exchanging information between converters through power lines to optimize the system's control performance. Going beyond the existing power and data co-transfer technologies that are limited by efficiency and scalability concerns, this paper proposes neuromorphic learning to implant communicative features using spiking neural networks (SNNs) at each node, which is trained collaboratively in an online manner s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;WSDM Cup 2024&#20013;&#33719;&#32988;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21331;&#36234;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#25913;&#32534;LLMs&#12289;&#35774;&#35745;&#28151;&#21512;&#35757;&#32451;&#31574;&#30053;&#12289;&#37319;&#29992;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12289;&#20197;&#21450;&#27169;&#22411;&#38598;&#25104;&#31561;&#22810;&#31181;&#25216;&#26415;&#65292;&#26368;&#32456;&#22312;&#20250;&#35805;&#24335;&#22810;&#25991;&#26723;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.18385</link><description>&lt;p&gt;
WSDM Cup 2024&#30340;&#31532;&#19968;&#21517;&#35299;&#20915;&#26041;&#26696;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20250;&#35805;&#24335;&#22810;&#25991;&#26723;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;WSDM Cup 2024&#20013;&#33719;&#32988;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21331;&#36234;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#25913;&#32534;LLMs&#12289;&#35774;&#35745;&#28151;&#21512;&#35757;&#32451;&#31574;&#30053;&#12289;&#37319;&#29992;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12289;&#20197;&#21450;&#27169;&#22411;&#38598;&#25104;&#31561;&#22810;&#31181;&#25216;&#26415;&#65292;&#26368;&#32456;&#22312;&#20250;&#35805;&#24335;&#22810;&#25991;&#26723;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#22810;&#25991;&#26723;&#38382;&#31572;&#26088;&#22312;&#26681;&#25454;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#20197;&#21450;&#19978;&#19979;&#25991;&#23545;&#35805;&#26469;&#22238;&#31572;&#29305;&#23450;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;WSDM Cup 2024&#8220;&#20250;&#35805;&#24335;&#22810;&#25991;&#26723;&#38382;&#31572;&#8221;&#25361;&#25112;&#20013;&#30340;&#33719;&#32988;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21331;&#36234;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;LLMs&#25913;&#32534;&#20026;&#27492;&#20219;&#21153;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#28151;&#21512;&#35757;&#32451;&#31574;&#30053;&#65292;&#20805;&#20998;&#21033;&#29992;&#39046;&#22495;&#20869;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#26469;&#36807;&#28388;&#25481;&#28508;&#22312;&#30340;&#19981;&#30456;&#20851;&#25991;&#26723;&#65292;&#24182;&#20026;&#27169;&#22411;&#38598;&#25104;&#35774;&#35745;&#21644;&#27604;&#36739;&#20102;&#20960;&#31181;&#26041;&#27861;&#12290;&#20973;&#20511;&#25152;&#26377;&#36825;&#20123;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26368;&#32456;&#22312;WSDM Cup 2024&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#22823;&#22823;&#36229;&#36234;&#20102;&#31454;&#20105;&#23545;&#25163;&#12290;&#28304;&#20195;&#30721;&#24050;&#21457;&#24067;&#22312;https://github.com/zhangzhao219/WSDM-Cup-2024&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18385v1 Announce Type: new  Abstract: Conversational multi-doc question answering aims to answer specific questions based on the retrieved documents as well as the contextual conversations. In this paper, we introduce our winning approach for the "Conversational Multi-Doc QA" challenge in WSDM Cup 2024, which exploits the superior natural language understanding and generation capability of Large Language Models (LLMs). We first adapt LLMs to the task, then devise a hybrid training strategy to make the most of in-domain unlabeled data. Moreover, an advanced text embedding model is adopted to filter out potentially irrelevant documents and several approaches are designed and compared for the model ensemble. Equipped with all these techniques, our solution finally ranked 1st place in WSDM Cup 2024, surpassing its rivals to a large extent. The source codes have been released at https://github.com/zhangzhao219/WSDM-Cup-2024.
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#22788;&#29702;&#40657;&#30418;&#20248;&#21270;&#20219;&#21153;&#20013;&#23454;&#29616;&#36827;&#21270;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;&#22343;&#20540;&#32479;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#40657;&#30418;&#37325;&#32452;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.18381</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36827;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As Evolution Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18381
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#22788;&#29702;&#40657;&#30418;&#20248;&#21270;&#20219;&#21153;&#20013;&#23454;&#29616;&#36827;&#21270;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;&#22343;&#20540;&#32479;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#40657;&#30418;&#37325;&#32452;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;Transformer&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#25152;&#35859;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#65292;&#21253;&#25324;&#26799;&#24230;&#19979;&#38477;&#12289;&#20998;&#31867;&#12289;&#24207;&#21015;&#23436;&#25104;&#12289;&#36716;&#25442;&#21644;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20174;&#26410;&#26126;&#30830;&#36935;&#21040;&#36807;&#40657;&#30418;&#20248;&#21270;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#22522;&#26412;&#19978;&#33021;&#22815;&#23454;&#29616;&#36827;&#21270;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#23545;&#31163;&#25955;&#21270;&#30340;&#31181;&#32676;&#25104;&#21592;&#36827;&#34892;&#20174;&#23569;&#21040;&#22810;&#30340;&#25490;&#24207;&#65292;&#24182;&#35810;&#38382;LLM&#25552;&#20986;&#23545;&#22343;&#20540;&#32479;&#35745;&#30340;&#25913;&#36827;&#65292;&#25191;&#34892;&#19968;&#31181;&#40657;&#30418;&#37325;&#32452;&#25805;&#20316;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#35774;&#32622;&#20801;&#35768;&#29992;&#25143;&#33719;&#24471;&#22522;&#20110;LLM&#30340;&#36827;&#21270;&#31574;&#30053;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;`EvoLL`&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18381v1 Announce Type: new  Abstract: Large Transformer models are capable of implementing a plethora of so-called in-context learning algorithms. These include gradient descent, classification, sequence completion, transformation, and improvement. In this work, we investigate whether large language models (LLMs), which never explicitly encountered the task of black-box optimization, are in principle capable of implementing evolutionary optimization algorithms. While previous works have solely focused on language-based task specification, we move forward and focus on the zero-shot application of LLMs to black-box optimization. We introduce a novel prompting strategy, consisting of least-to-most sorting of discretized population members and querying the LLM to propose an improvement to the mean statistic, i.e. perform a type of black-box recombination operation. Empirically, we find that our setup allows the user to obtain an LLM-based evolution strategy, which we call `EvoLL
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20998;&#35789;&#22120;PathPiece&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#23569;&#37327;&#26631;&#35760;&#24182;&#19981;&#33021;&#23548;&#33268;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110; Tokenization &#30340;&#26377;&#25928;&#24615;&#29702;&#35299;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;</title><link>https://arxiv.org/abs/2402.18376</link><description>&lt;p&gt;
Tokenization&#36229;&#36234;&#20102;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Tokenization Is More Than Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18376
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20998;&#35789;&#22120;PathPiece&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#23569;&#37327;&#26631;&#35760;&#24182;&#19981;&#33021;&#23548;&#33268;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110; Tokenization &#30340;&#26377;&#25928;&#24615;&#29702;&#35299;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tokenization&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#22522;&#30784;&#27493;&#39588;&#65292;&#23427;&#36830;&#25509;&#20102;&#21407;&#22987;&#25991;&#26412;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;Tokenization&#26041;&#27861;&#65292;&#22914;&#23383;&#33410;&#23545;&#32534;&#30721;&#65288;Byte-Pair Encoding&#65292;BPE&#65289;&#65292;&#28304;&#33258;&#25968;&#25454;&#21387;&#32553;&#39046;&#22495;&#65292;&#24182;&#26377;&#20154;&#35748;&#20026;BPE&#30340;&#26377;&#25928;&#24615;&#28304;&#20110;&#20854;&#23558;&#25991;&#26412;&#21387;&#32553;&#20026;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;PathPiece&#26469;&#27979;&#35797;&#8220;&#26356;&#23569;&#30340;&#26631;&#35760;&#26159;&#21542;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#8221;&#36825;&#19968;&#20551;&#35774;&#65292;PathPiece&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#35789;&#22120;&#65292;&#26681;&#25454;&#32473;&#23450;&#35789;&#27719;&#23558;&#25991;&#26723;&#25991;&#26412;&#21010;&#20998;&#20026;&#26368;&#23569;&#25968;&#37327;&#30340;&#26631;&#35760;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#19968;&#20551;&#35774;&#24182;&#38750;&#25104;&#31435;&#65292;&#23545;&#26377;&#25928;Tokenization&#21407;&#22240;&#30340;&#29702;&#35299;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#20026;&#20102;&#26816;&#26597;&#21738;&#20123;&#20854;&#20182;&#22240;&#32032;&#36215;&#21040;&#20316;&#29992;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Tokenization&#30340;&#25152;&#26377;&#19977;&#20010;&#38454;&#27573;&#65288;&#39044;&#20998;&#35789;&#12289;&#35789;&#27719;&#26500;&#36896;&#21644;&#20998;&#21106;&#65289;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#35774;&#35745;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18376v1 Announce Type: cross  Abstract: Tokenization is a foundational step in Natural Language Processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FedUV&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#27491;&#21017;&#21270;&#39033;&#65292;&#20419;&#20351;&#23616;&#37096;&#27169;&#22411;&#22312;&#24322;&#26500;&#20998;&#24067;&#25968;&#25454;&#20013;&#34920;&#29616;&#24471;&#26356;&#22343;&#21248;&#21644;&#31283;&#23450;</title><link>https://arxiv.org/abs/2402.18372</link><description>&lt;p&gt;
FedUV: &#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#22343;&#21248;&#24615;&#21644;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
FedUV: Uniformity and Variance for Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18372
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FedUV&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#27491;&#21017;&#21270;&#39033;&#65292;&#20419;&#20351;&#23616;&#37096;&#27169;&#22411;&#22312;&#24322;&#26500;&#20998;&#24067;&#25968;&#25454;&#20013;&#34920;&#29616;&#24471;&#26356;&#22343;&#21248;&#21644;&#31283;&#23450;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#24191;&#27867;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#20250;&#38543;&#30528;&#24322;&#26500;&#20998;&#24067;&#30340;&#25968;&#25454;&#32780;&#19979;&#38477;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#26159;&#30001;&#20110;&#32593;&#32476;&#30340;&#26368;&#32456;&#23618;&#26368;&#23481;&#26131;&#20986;&#29616;&#23616;&#37096;&#20559;&#24046;&#65292;&#19968;&#20123;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#23558;&#26368;&#32456;&#23618;&#20923;&#32467;&#20026;&#27491;&#20132;&#20998;&#31867;&#22120;&#21487;&#20197;&#21462;&#24471;&#25104;&#21151;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26435;&#37325;&#24212;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#30740;&#31350;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36825;&#26159;&#21463;&#21040;&#20923;&#32467;&#26435;&#37325;&#23548;&#33268;&#22855;&#24322;&#20540;&#24658;&#23450;&#30340;&#35266;&#23519;&#21551;&#21457;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;IID&#21644;&#38750;IID&#35774;&#32622;&#19979;&#35757;&#32451;&#26102;&#23384;&#22312;&#24046;&#24322;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20004;&#31181;&#23616;&#37096;&#35757;&#32451;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#20197;&#25345;&#32493;&#27169;&#25311;IID&#35774;&#32622;&#65306;&#65288;1&#65289;&#20998;&#31867;&#22120;&#30340;&#32500;&#24230;&#27010;&#29575;&#20998;&#24067;&#26041;&#24046;&#21644;&#65288;2&#65289;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;&#36229;&#29699;&#22343;&#21248;&#24615;&#12290;&#36825;&#20123;&#27491;&#21017;&#21270;&#20419;&#20351;&#23616;&#37096;&#27169;&#22411;&#34920;&#29616;&#24471;&#22909;&#20687;&#22312;IID&#35774;&#32622;&#20013;&#19968;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18372v1 Announce Type: cross  Abstract: Federated learning is a promising framework to train neural networks with widely distributed data. However, performance degrades heavily with heterogeneously distributed data. Recent work has shown this is due to the final layer of the network being most prone to local bias, some finding success freezing the final layer as an orthogonal classifier. We investigate the training dynamics of the classifier by applying SVD to the weights motivated by the observation that freezing weights results in constant singular values. We find that there are differences when training in IID and non-IID settings. Based on this finding, we introduce two regularization terms for local training to continuously emulate IID settings: (1) variance in the dimension-wise probability distribution of the classifier and (2) hyperspherical uniformity of representations of the encoder. These regularizations promote local models to act as if it were in an IID setting
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;AG-DDAD&#65292;&#26088;&#22312;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#25193;&#25955;&#27169;&#22411;&#35780;&#20272;&#20083;&#33146;&#25972;&#23481;&#65292;&#20174;&#32780;&#20811;&#26381;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#21644;&#29616;&#26377;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18362</link><description>&lt;p&gt;
&#20351;&#29992;&#27880;&#24847;&#21147;&#24341;&#23548;&#21435;&#22122;&#25193;&#25955;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#23458;&#35266;&#19988;&#21487;&#35299;&#37322;&#30340;&#20083;&#33146;&#25972;&#23481;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Objective and Interpretable Breast Cosmesis Evaluation with Attention Guided Denoising Diffusion Anomaly Detection Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;AG-DDAD&#65292;&#26088;&#22312;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#25193;&#25955;&#27169;&#22411;&#35780;&#20272;&#20083;&#33146;&#25972;&#23481;&#65292;&#20174;&#32780;&#20811;&#26381;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#21644;&#29616;&#26377;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20083;&#33146;&#30284;&#27835;&#30103;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#26415;&#21518;&#25972;&#23481;&#25928;&#26524;&#30340;&#35780;&#20272;&#22240;&#20854;&#23545;&#24739;&#32773;&#29983;&#27963;&#36136;&#37327;&#30340;&#37325;&#22823;&#24433;&#21709;&#32780;&#26085;&#30410;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19987;&#23478;&#26631;&#27880;&#30340;&#22266;&#26377;&#20027;&#35266;&#24615;&#65292;&#35780;&#20272;&#20083;&#33146;&#25972;&#23481;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;Attention-Guided Denoising Diffusion Anomaly Detection (AG-DDAD)&#65292;&#26088;&#22312;&#35780;&#20272;&#25163;&#26415;&#21518;&#30340;&#20083;&#33146;&#25972;&#23481;&#65292;&#24182;&#35299;&#20915;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#21644;&#29616;&#26377;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26080;&#26631;&#31614;&#33976;&#39311;&#65288;DINO&#65289;&#33258;&#30417;&#30563;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#21644;&#36776;&#21035;&#24615;&#21306;&#22495;&#31934;&#30830;&#21464;&#25442;&#12290;&#36890;&#36807;&#20027;&#35201;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18362v1 Announce Type: cross  Abstract: As advancements in the field of breast cancer treatment continue to progress, the assessment of post-surgical cosmetic outcomes has gained increasing significance due to its substantial impact on patients' quality of life. However, evaluating breast cosmesis presents challenges due to the inherently subjective nature of expert labeling. In this study, we present a novel automated approach, Attention-Guided Denoising Diffusion Anomaly Detection (AG-DDAD), designed to assess breast cosmesis following surgery, addressing the limitations of conventional supervised learning and existing anomaly detection models. Our approach leverages the attention mechanism of the distillation with no label (DINO) self-supervised Vision Transformer (ViT) in combination with a diffusion model to achieve high-quality image reconstruction and precise transformation of discriminative regions. By training the diffusion model on unlabeled data predominantly with
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#31867;&#27604;&#27604;&#20363;&#35268;&#23450;&#20026;&#30456;&#20284;&#24615;&#65292;&#24314;&#31435;&#20102;&#20174;&#30456;&#20284;&#24615;&#21040;&#31867;&#27604;&#27604;&#20363;&#30340;&#26725;&#26753;&#65292;&#20351;&#24471;&#23558;&#26469;&#30340;&#30456;&#20284;&#24615;&#32467;&#26524;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#31867;&#27604;&#27604;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.18360</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#31867;&#27604;&#27604;&#20363;
&lt;/p&gt;
&lt;p&gt;
Similarity-based analogical proportions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18360
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#31867;&#27604;&#27604;&#20363;&#35268;&#23450;&#20026;&#30456;&#20284;&#24615;&#65292;&#24314;&#31435;&#20102;&#20174;&#30456;&#20284;&#24615;&#21040;&#31867;&#27604;&#27604;&#20363;&#30340;&#26725;&#26753;&#65292;&#20351;&#24471;&#23558;&#26469;&#30340;&#30456;&#20284;&#24615;&#32467;&#26524;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#31867;&#27604;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#26368;&#36817;&#22312;&#36890;&#29992;&#20195;&#25968;&#30340;&#19968;&#33324;&#35774;&#32622;&#20869;&#24341;&#20837;&#20102;&#31867;&#27604;&#27604;&#20363;&#21644;&#30456;&#20284;&#24615;&#30340;&#25277;&#35937;&#20195;&#25968;&#26694;&#26550;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#29992;&#30456;&#20284;&#24615;&#26469;&#35268;&#23450;&#31867;&#27604;&#27604;&#20363;&#65292;&#20174;&#32780;&#24314;&#31435;&#20174;&#30456;&#20284;&#24615;&#21040;&#31867;&#27604;&#27604;&#20363;&#30340;&#26725;&#26753;&#12290;&#36825;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#30340;&#22909;&#22788;&#22312;&#20110;&#65292;&#27604;&#20363;&#21644;&#30456;&#20284;&#24615;&#30340;&#32852;&#31995;&#34987;&#26500;&#24314;&#21040;&#26694;&#26550;&#20013;&#65292;&#22240;&#27492;&#26174;&#32780;&#26131;&#35265;&#65292;&#36825;&#19968;&#28857;&#24456;&#21560;&#24341;&#20154;&#65292;&#22240;&#20026;&#27604;&#20363;&#21644;&#30456;&#20284;&#24615;&#37117;&#22788;&#20110;&#31867;&#27604;&#30340;&#20013;&#24515;&#65307;&#27492;&#22806;&#65292;&#23545;&#30456;&#20284;&#24615;&#30340;&#26410;&#26469;&#32467;&#26524;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#31867;&#27604;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18360v1 Announce Type: cross  Abstract: The author has recently introduced abstract algebraic frameworks of analogical proportions and similarity within the general setting of universal algebra. The purpose of this paper is to build a bridge from similarity to analogical proportions by formulating the latter in terms of the former. The benefit of this similarity-based approach is that the connection between proportions and similarity is built into the framework and therefore evident which is appealing since proportions and similarity are both at the center of analogy; moreover, future results on similarity can directly be applied to analogical proportions.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#31639;&#27861;&#36766;&#32844;&#20316;&#20026;&#31649;&#29702;&#32452;&#32455;&#20869;&#20351;&#29992;AI&#31995;&#32479;&#30340;&#25112;&#30053;&#26041;&#27861;&#65292;&#25552;&#20986;&#36890;&#36807;&#22312;AI&#31995;&#32479;&#20013;&#23884;&#20837;&#27835;&#29702;&#26426;&#21046;&#65292;&#26377;&#24847;&#35782;&#21644;&#26126;&#26234;&#22320;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33073;&#31163;AI&#36741;&#21161;&#65292;&#20197;&#25351;&#23548;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#25110;&#36991;&#20813;&#36825;&#20123;&#31995;&#32479;&#65292;&#20174;&#32780;&#33719;&#24471;&#22810;&#26041;&#38754;&#30340;&#21033;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.18326</link><description>&lt;p&gt;
&#31639;&#27861;&#20309;&#26102;&#35813;&#36766;&#32844;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Should Algorithms Resign?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18326
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#31639;&#27861;&#36766;&#32844;&#20316;&#20026;&#31649;&#29702;&#32452;&#32455;&#20869;&#20351;&#29992;AI&#31995;&#32479;&#30340;&#25112;&#30053;&#26041;&#27861;&#65292;&#25552;&#20986;&#36890;&#36807;&#22312;AI&#31995;&#32479;&#20013;&#23884;&#20837;&#27835;&#29702;&#26426;&#21046;&#65292;&#26377;&#24847;&#35782;&#21644;&#26126;&#26234;&#22320;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33073;&#31163;AI&#36741;&#21161;&#65292;&#20197;&#25351;&#23548;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#25110;&#36991;&#20813;&#36825;&#20123;&#31995;&#32479;&#65292;&#20174;&#32780;&#33719;&#24471;&#22810;&#26041;&#38754;&#30340;&#21033;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#31639;&#27861;&#36766;&#32844;&#65292;&#36825;&#26159;&#19968;&#31181;&#31649;&#29702;&#32452;&#32455;&#20869;&#20351;&#29992;AI&#31995;&#32479;&#30340;&#25112;&#30053;&#26041;&#27861;&#12290;&#31639;&#27861;&#36766;&#32844;&#28041;&#21450;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26377;&#24847;&#35782;&#21644;&#26126;&#26234;&#22320;&#33073;&#31163;AI&#36741;&#21161;&#65292;&#36890;&#36807;&#30452;&#25509;&#23558;&#27835;&#29702;&#26426;&#21046;&#23884;&#20837;&#21040;AI&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#19981;&#20165;&#20165;&#26159;&#20851;&#20110;&#19981;&#20351;&#29992;AI&#65292;&#36824;&#21253;&#25324;&#25351;&#23548;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#25110;&#36991;&#20813;&#36825;&#20123;&#31995;&#32479;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#31639;&#27861;&#36766;&#32844;&#30340;&#22810;&#26041;&#38754;&#21033;&#30410;&#65292;&#28085;&#30422;&#32463;&#27982;&#25928;&#29575;&#12289;&#22768;&#35465;&#22686;&#30410;&#21644;&#27861;&#24459;&#21512;&#35268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#36890;&#36807;&#31215;&#26497;&#21644;&#28040;&#26497;&#21161;&#25512;&#12289;&#21033;&#30410;&#30456;&#20851;&#32773;&#28608;&#21169;&#23545;&#40784;&#20197;&#21450;&#23457;&#24910;&#32771;&#34385;AI&#21442;&#19982;&#31243;&#24230;&#31561;&#21508;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36766;&#32844;&#12290;&#36890;&#36807;&#35832;&#22914;&#26377;&#36873;&#25321;&#22320;&#38459;&#27490;&#35775;&#38382;AI&#36755;&#20986;&#25110;&#23545;&#31995;&#32479;&#24615;&#33021;&#26126;&#30830;&#22768;&#26126;&#20813;&#36131;&#31561;&#25216;&#26415;&#25163;&#27573;&#65292;&#31639;&#27861;&#36766;&#32844;&#19981;&#20165;&#33021;&#38477;&#20302;&#19982;&#20043;&#30456;&#20851;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18326v1 Announce Type: cross  Abstract: This paper discusses algorithmic resignation, a strategic approach for managing the use of AI systems within organizations. Algorithmic resignation involves the deliberate and informed disengagement from AI assistance in certain scenarios, by embedding governance mechanisms directly into AI systems. Our proposal is not merely about disuse of AI but includes guiding when and how these systems should be used or avoided. We discuss the multifaceted benefits of algorithmic resignation, spanning economic efficiency, reputational gains, and legal compliance. Further, we outline the operationalization of resignation through various methods such as positive and negative nudges, stakeholder incentive alignment, and careful consideration of the level of AI engagement. Using techniques like barring access to AI outputs selectively or providing explicit disclaimers on system performance, algorithmic resignation not only mitigates risks associated 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22836;&#37096;&#20301;&#32622;&#20449;&#24687;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#40060;&#30524;&#22270;&#20687;&#36827;&#34892;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#31471;&#21040;&#31471;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#22836;&#37096;&#23039;&#24577;&#21644;&#22836;&#37096;&#20301;&#32622;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#19979;&#65292;&#30452;&#25509;&#20174;&#40060;&#30524;&#22270;&#20687;&#20013;&#20272;&#35745;&#22836;&#37096;&#23039;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.18320</link><description>&lt;p&gt;
&#22522;&#20110;&#20301;&#32622;&#24341;&#23548;&#30340;&#40060;&#30524;&#22270;&#20687;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Location-guided Head Pose Estimation for Fisheye Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18320
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22836;&#37096;&#20301;&#32622;&#20449;&#24687;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#40060;&#30524;&#22270;&#20687;&#36827;&#34892;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#31471;&#21040;&#31471;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#22836;&#37096;&#23039;&#24577;&#21644;&#22836;&#37096;&#20301;&#32622;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#19979;&#65292;&#30452;&#25509;&#20174;&#40060;&#30524;&#22270;&#20687;&#20013;&#20272;&#35745;&#22836;&#37096;&#23039;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#26377;&#40060;&#30524;&#25110;&#36229;&#24191;&#35282;&#38236;&#22836;&#30340;&#30456;&#26426;&#35206;&#30422;&#30340;&#35270;&#22330;&#24191;&#38420;&#65292;&#26080;&#27861;&#36890;&#36807;&#36879;&#35270;&#25237;&#24433;&#26469;&#24314;&#27169;&#12290;&#22270;&#20687;&#36793;&#32536;&#22788;&#20005;&#37325;&#30340;&#40060;&#30524;&#38236;&#22836;&#30072;&#21464;&#23548;&#33268;&#20102;&#22312;&#38750;&#30072;&#21464;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#29616;&#26377;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#20013;&#22836;&#37096;&#20301;&#32622;&#30340;&#30693;&#35782;&#26469;&#20943;&#23569;&#40060;&#30524;&#30072;&#21464;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22836;&#37096;&#23039;&#24577;&#21644;&#22836;&#37096;&#20301;&#32622;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#20272;&#35745;&#22836;&#37096;&#23039;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#21487;&#20197;&#30452;&#25509;&#20174;&#40060;&#30524;&#22270;&#20687;&#20013;&#20272;&#35745;&#22836;&#37096;&#23039;&#24577;&#65292;&#26080;&#38656;&#26657;&#27491;&#25110;&#26631;&#23450;&#25805;&#20316;&#12290;&#25105;&#20204;&#36824;&#20026;BIWI&#12289;300W-LP&#21644;AFLW2000&#36825;&#19977;&#20010;&#28909;&#38376;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#40060;&#30524;&#30072;&#21464;&#29256;&#26412;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18320v1 Announce Type: cross  Abstract: Camera with a fisheye or ultra-wide lens covers a wide field of view that cannot be modeled by the perspective projection. Serious fisheye \textcolor{blue}{lens} distortion in the peripheral region of the image leads to degraded performance of the \textcolor{blue}{existing} head pose estimation models trained on undistorted images. This paper presents a new approach for head pose estimation that uses the knowledge of head location in the image to reduce the negative effect of fisheye distortion. We develop an end-to-end convolutional neural network to estimate the head pose with the multi-task learning of head pose and head location. Our proposed network estimates the head pose directly from the fisheye image without the operation of rectification or calibration. We also created \textcolor{blue}{a} fisheye-\textcolor{blue}{distorted} version of the three popular head pose estimation datasets, BIWI, 300W-LP, and AFLW2000 for our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28857;&#20113;&#31639;&#27861;&#65292;&#21033;&#29992;LiDAR&#25216;&#26415;&#33258;&#21160;&#26816;&#27979;&#26641;&#26408;&#29983;&#38271;&#22312;&#34903;&#36947;&#19978;&#38656;&#35201;&#20462;&#21098;&#30340;&#37096;&#20998;&#65292;&#36825;&#26377;&#21161;&#20110;&#22686;&#24378;&#36947;&#36335;&#23433;&#20840;&#12290;</title><link>https://arxiv.org/abs/2402.18309</link><description>&lt;p&gt;
&#22686;&#24378;&#36947;&#36335;&#23433;&#20840;&#65306;&#22522;&#20110;LiDAR&#30340;&#26641;&#26408;&#38388;&#38553;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enhancing Roadway Safety: LiDAR-based Tree Clearance Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28857;&#20113;&#31639;&#27861;&#65292;&#21033;&#29992;LiDAR&#25216;&#26415;&#33258;&#21160;&#26816;&#27979;&#26641;&#26408;&#29983;&#38271;&#22312;&#34903;&#36947;&#19978;&#38656;&#35201;&#20462;&#21098;&#30340;&#37096;&#20998;&#65292;&#36825;&#26377;&#21161;&#20110;&#22686;&#24378;&#36947;&#36335;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33268;&#21147;&#20110;&#26356;&#23433;&#20840;&#36947;&#36335;&#30340;&#21162;&#21147;&#20013;&#65292;&#30830;&#20445;&#36947;&#36335;&#19978;&#26041;&#26377;&#36275;&#22815;&#30340;&#22402;&#30452;&#38388;&#38553;&#38750;&#24120;&#37325;&#35201;&#12290;&#32463;&#24120;&#24773;&#20917;&#19979;&#65292;&#26641;&#26408;&#25110;&#20854;&#20182;&#26893;&#34987;&#20250;&#29983;&#38271;&#22312;&#36947;&#36335;&#19978;&#26041;&#65292;&#36974;&#25377;&#20102;&#20132;&#36890;&#26631;&#24535;&#21644;&#28783;&#20809;&#30340;&#35270;&#32447;&#65292;&#24182;&#23545;&#20132;&#36890;&#21442;&#19982;&#32773;&#26500;&#25104;&#21361;&#38505;&#12290;&#21033;&#29992;&#31616;&#21333;&#30340;&#22270;&#20687;&#20934;&#30830;&#20272;&#35745;&#36825;&#20010;&#31354;&#38388;&#30001;&#20110;&#32570;&#20047;&#28145;&#24230;&#20449;&#24687;&#32780;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#23601;&#26159;LiDAR&#25216;&#26415;&#21457;&#25381;&#20316;&#29992;&#30340;&#22320;&#26041;&#65292;&#36825;&#26159;&#19968;&#31181;&#28608;&#20809;&#25195;&#25551;&#20256;&#24863;&#22120;&#65292;&#21487;&#20197;&#26174;&#31034;&#19977;&#32500;&#35270;&#35282;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#34903;&#36947;&#27700;&#24179;&#19978;&#30340;LiDAR&#28857;&#20113;&#20027;&#35201;&#34987;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25195;&#25551;&#36824;&#25171;&#24320;&#20102;&#22478;&#24066;&#31649;&#29702;&#39046;&#22495;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28857;&#20113;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#37027;&#20123;&#29983;&#38271;&#22312;&#34903;&#36947;&#19978;&#26041;&#38656;&#35201;&#20462;&#21098;&#30340;&#26641;&#26408;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21033;&#29992;&#35821;&#20041;&#20998;&#21106;&#26469;&#36807;&#28388;&#30456;&#20851;&#28857;&#65292;&#28982;&#21518;&#36827;&#34892;&#19979;&#28216;&#22788;&#29702;&#27493;&#39588;&#26469;&#21019;&#24314;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18309v1 Announce Type: cross  Abstract: In the efforts for safer roads, ensuring adequate vertical clearance above roadways is of great importance. Frequently, trees or other vegetation is growing above the roads, blocking the sight of traffic signs and lights and posing danger to traffic participants. Accurately estimating this space from simple images proves challenging due to a lack of depth information. This is where LiDAR technology comes into play, a laser scanning sensor that reveals a three-dimensional perspective. Thus far, LiDAR point clouds at the street level have mainly been used for applications in the field of autonomous driving. These scans, however, also open up possibilities in urban management. In this paper, we present a new point cloud algorithm that can automatically detect those parts of the trees that grow over the street and need to be trimmed. Our system uses semantic segmentation to filter relevant points and downstream processing steps to create t
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18292</link><description>&lt;p&gt;
FSL&#27169;&#22411;&#21487;&#20197;&#22240;&#20026;&#20854;&#20248;&#36234;&#24615;&#24471;&#20998;&#26356;&#39640;
&lt;/p&gt;
&lt;p&gt;
FSL Model can Score Higher as It Is
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18292
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#34987;&#27491;&#30830;&#35782;&#21035;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20542;&#21521;&#20110;&#38754;&#23545;&#38754;&#22320;&#30452;&#35270;&#38754;&#37096;&#35782;&#21035;&#26426;&#65292;&#32780;&#19981;&#26159;&#20391;&#30528;&#38754;&#23545;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#20998;&#31867;&#26412;&#36523;&#23601;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#23646;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#23545;&#25197;&#26354;&#21644;&#38750;&#20856;&#22411;&#30340;&#26597;&#35810;&#25110;&#25903;&#25345;&#22270;&#20687;&#20250;&#35753;&#27169;&#22411;&#26356;&#38590;&#27491;&#30830;&#39044;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;&#35757;&#32451;&#36807;&#30340;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;FSL&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#20855;&#26377;&#36275;&#22815;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20855;&#26377;&#23569;&#26679;&#26412;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#25429;&#25417;&#27979;&#35797;&#22270;&#20687;&#30340;&#39118;&#26684;&#25110;&#24418;&#29366;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#36866;&#24403;&#30340;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18292v1 Announce Type: cross  Abstract: In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable traine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#21516;&#26102;&#25351;&#20986;&#36739;&#20302;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22987;&#32456;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18286</link><description>&lt;p&gt;
&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65306;&#36808;&#21521;&#39640;&#32423;&#22270;&#20687;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#21516;&#26102;&#25351;&#20986;&#36739;&#20302;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22987;&#32456;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20174;&#26080;&#26631;&#31614;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#36808;&#20986;&#20102;&#26500;&#24314;&#35813;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#27493;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#20197;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#20998;&#21106;&#12289;&#21435;&#22122;&#12289;&#22122;&#22768;&#19982;&#32972;&#26223;&#21435;&#38500;&#20197;&#21450;&#36229;&#20998;&#36776;&#29575;&#12290;&#36890;&#36807;&#23454;&#39564;&#19981;&#21516;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#24863;&#21463;&#37326;&#22823;&#23567;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#26174;&#33879;&#30340;&#29616;&#35937;&#65292;&#21363;&#24494;&#35843;&#36807;&#30340;&#36739;&#20302;&#22797;&#26434;&#24230;&#27169;&#22411;&#22987;&#32456;&#32988;&#36807;&#20855;&#26377;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#26356;&#22797;&#26434;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#32972;&#26223;&#19979;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#22810;&#25165;&#22810;&#33402;&#65292;&#20351;&#24471;&#24555;&#36895;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#20652;&#21270;&#21058;&#65292;&#29305;&#21035;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#25968;&#25454;&#21487;&#29992;&#26102;&#21644; ef
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18286v1 Announce Type: cross  Abstract: In this work, we explore the potential of self-supervised learning from unlabeled electron microscopy datasets, taking a step toward building a foundation model in this field. We show how self-supervised pretraining facilitates efficient fine-tuning for a spectrum of downstream tasks, including semantic segmentation, denoising, noise &amp; background removal, and super-resolution. Experimentation with varying model complexities and receptive field sizes reveals the remarkable phenomenon that fine-tuned models of lower complexity consistently outperform more complex models with random weight initialization. We demonstrate the versatility of self-supervised pretraining across various downstream tasks in the context of electron microscopy, allowing faster convergence and better performance. We conclude that self-supervised pretraining serves as a powerful catalyst, being especially advantageous when limited annotated data are available and ef
&lt;/p&gt;</description></item><item><title>PiShield&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#23558;&#38656;&#27714;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#26694;&#26550;&#65292;&#26080;&#35770;&#36755;&#20837;&#22914;&#20309;&#37117;&#33021;&#30830;&#20445;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#24182;&#21487;&#26681;&#25454;&#20174;&#19994;&#32773;&#38656;&#27714;&#22312;&#25512;&#26029;&#21644;/&#25110;&#35757;&#32451;&#26102;&#38598;&#25104;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.18285</link><description>&lt;p&gt;
PiShield&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#20197;&#38656;&#27714;&#20026;&#22522;&#30784;&#23398;&#20064;&#30340;NeSy&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PiShield: A NeSy Framework for Learning with Requirements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18285
&lt;/p&gt;
&lt;p&gt;
PiShield&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#23558;&#38656;&#27714;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#26694;&#26550;&#65292;&#26080;&#35770;&#36755;&#20837;&#22914;&#20309;&#37117;&#33021;&#30830;&#20445;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#24182;&#21487;&#26681;&#25454;&#20174;&#19994;&#32773;&#38656;&#27714;&#22312;&#25512;&#26029;&#21644;/&#25110;&#35757;&#32451;&#26102;&#38598;&#25104;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#20854;&#20248;&#21183;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#38590;&#20197;&#28385;&#36275;&#20854;&#36755;&#20986;&#30340;&#23433;&#20840;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PiShield&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#23558;&#38656;&#27714;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#26694;&#26550;&#12290;PiShield&#30830;&#20445;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#26080;&#35770;&#36755;&#20837;&#22914;&#20309;&#12290;&#27492;&#22806;&#65292;&#23427;&#20801;&#35768;&#26681;&#25454;&#20174;&#19994;&#32773;&#30340;&#38656;&#27714;&#22312;&#25512;&#26029;&#21644;/&#25110;&#35757;&#32451;&#26102;&#38598;&#25104;&#38656;&#27714;&#12290;&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36843;&#20999;&#38656;&#35201;&#20801;&#35768;&#22312;&#21508;&#20010;&#39046;&#22495;&#38598;&#25104;&#38656;&#27714;&#30340;&#26694;&#26550;&#12290;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#20010;&#24212;&#29992;&#22330;&#26223;&#65306;&#21151;&#33021;&#22522;&#22240;&#32452;&#23398;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18285v1 Announce Type: cross  Abstract: Deep learning models have shown their strengths in various application domains, however, they often struggle to meet safety requirements for their outputs. In this paper, we introduce PiShield, the first framework ever allowing for the integration of the requirements into the neural networks' topology. PiShield guarantees compliance with these requirements, regardless of input. Additionally, it allows for integrating requirements both at inference and/or training time, depending on the practitioners' needs. Given the widespread application of deep learning, there is a growing need for frameworks allowing for the integration of the requirements across various domains. Here, we explore three application scenarios: functional genomics, autonomous driving, and tabular data generation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25991;&#26412;&#25490;&#24207;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#38656;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21508;&#39033;&#24471;&#20998;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;</title><link>https://arxiv.org/abs/2402.18284</link><description>&lt;p&gt;
&#20247;&#21253;&#26159;&#21542;&#35753;&#24744;&#30772;&#20135;&#20102;&#65311;&#20351;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25104;&#26412;&#25928;&#30410;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18284
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25991;&#26412;&#25490;&#24207;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#38656;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21508;&#39033;&#24471;&#20998;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24191;&#27867;&#20351;&#29992;&#20984;&#26174;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#35757;&#32451;&#27969;&#31243;&#20381;&#36182;&#20110;&#20154;&#24037;&#25490;&#24207;&#65292;&#36825;&#26159;&#19968;&#20010;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#38477;&#20302;&#21171;&#21160;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25991;&#26412;&#25490;&#24207;&#26041;&#27861;&#65292;&#29992;&#20110;&#24212;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#26469;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#27010;&#29575;&#25277;&#26679;&#24320;&#22987;&#65292;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#20026;&#27599;&#20010;&#36755;&#20837;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21709;&#24212;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;TextRank&#21644;ISODATA&#31639;&#27861;&#65292;&#22522;&#20110;&#35821;&#20041;&#23545;&#36825;&#20123;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#21644;&#32858;&#31867;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22870;&#21169;&#27169;&#22411;&#26469;&#23398;&#20064;&#25490;&#21517;&#24182;&#20248;&#21270;&#25105;&#20204;&#30340;&#29983;&#25104;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#20351;&#29992;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;BLEU&#12289;GLEU&#21644;METEOR&#24471;&#20998;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25163;&#21160;&#35780;&#20272;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18284v1 Announce Type: cross  Abstract: Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback. However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores. Furthermore, our manual evaluation show
&lt;/p&gt;</description></item><item><title>&#22810;&#26234;&#33021;&#20307;&#35752;&#35770;&#25552;&#21319;&#20102;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#26377;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#21333;&#26234;&#33021;&#20307;LLM&#36890;&#36807;&#24378;&#25552;&#31034;&#20960;&#20046;&#33021;&#36798;&#21040;&#19982;&#26368;&#20339;&#35752;&#35770;&#26041;&#27861;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18272</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;LLM&#25512;&#29702;&#30340;&#30028;&#38480;&#65306;&#22810;&#26234;&#33021;&#20307;&#35752;&#35770;&#26159;&#20851;&#38190;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18272
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#35752;&#35770;&#25552;&#21319;&#20102;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#26377;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#21333;&#26234;&#33021;&#20307;LLM&#36890;&#36807;&#24378;&#25552;&#31034;&#20960;&#20046;&#33021;&#36798;&#21040;&#19982;&#26368;&#20339;&#35752;&#35770;&#26041;&#27861;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#35752;&#35770;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#22810;&#26234;&#33021;&#20307;&#35752;&#35770;&#25552;&#21319;&#20102;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#23454;&#39564;&#23545;&#36825;&#19968;&#35828;&#27861;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23567;&#32452;&#35752;&#35770;&#26694;&#26550;&#65292;&#20016;&#23500;&#20102;&#35752;&#35770;&#26426;&#21046;&#38598;&#21512;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#19968;&#20010;&#24102;&#26377;&#24378;&#25552;&#31034;&#30340;&#21333;&#26234;&#33021;&#20307;LLM&#22312;&#24191;&#27867;&#30340;&#25512;&#29702;&#20219;&#21153;&#21644;&#22522;&#26412;LLM&#20013;&#20960;&#20046;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20339;&#29616;&#26377;&#35752;&#35770;&#26041;&#27861;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22810;&#26234;&#33021;&#20307;&#35752;&#35770;&#20165;&#22312;&#25552;&#31034;&#20013;&#27809;&#26377;&#28436;&#31034;&#26102;&#25165;&#20248;&#20110;&#21333;&#20010;&#26234;&#33021;&#20307;&#12290;&#36827;&#19968;&#27493;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#35752;&#35770;&#36807;&#31243;&#20013;&#30340;&#24120;&#35265;&#20132;&#20114;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18272v1 Announce Type: cross  Abstract: Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#30740;&#31350;&#20102;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#65288;NQG&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#20102;&#32972;&#26223;&#27010;&#36848;&#12289;&#19981;&#21516;&#31867;&#21035;&#30340;&#26041;&#27861;&#12289;&#20197;&#21450;&#26410;&#26469;&#23637;&#26395;</title><link>https://arxiv.org/abs/2402.18267</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#30340;&#35843;&#26597;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Neural Question Generation: Methods, Applications, and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#30740;&#31350;&#20102;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#65288;NQG&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#20102;&#32972;&#26223;&#27010;&#36848;&#12289;&#19981;&#21516;&#31867;&#21035;&#30340;&#26041;&#27861;&#12289;&#20197;&#21450;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#65288;NQG&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#36827;&#34892;&#20102;&#35814;&#32454;&#26816;&#26597;&#65292;&#36825;&#19968;&#39046;&#22495;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#20174;&#21508;&#31181;&#26469;&#28304;&#65292;&#22914;&#30693;&#35782;&#24211;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#29983;&#25104;&#30456;&#20851;&#38382;&#39064;&#12290;&#35843;&#26597;&#20174;NQG&#32972;&#26223;&#27010;&#36848;&#24320;&#22987;&#65292;&#21253;&#25324;&#20219;&#21153;&#30340;&#38382;&#39064;&#21046;&#23450;&#12289;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#24050;&#24314;&#31435;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#26174;&#33879;&#24212;&#29992;&#12290;&#28982;&#21518;&#65292;&#31995;&#32479;&#22320;&#23558;NQG&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#32467;&#26500;&#21270;NQG&#65292;&#21033;&#29992;&#26377;&#32452;&#32455;&#30340;&#25968;&#25454;&#28304;&#65292;&#38750;&#32467;&#26500;&#21270;NQG&#65292;&#19987;&#27880;&#20110;&#26356;&#26494;&#25955;&#32467;&#26500;&#30340;&#36755;&#20837;&#65292;&#22914;&#25991;&#26412;&#25110;&#35270;&#35273;&#20869;&#23481;&#65292;&#20197;&#21450;&#28151;&#21512;NQG&#65292;&#21033;&#29992;&#22810;&#26679;&#30340;&#36755;&#20837;&#27169;&#24335;&#12290;&#36825;&#19968;&#20998;&#31867;&#21518;&#26159;&#23545;&#20026;&#27599;&#20010;&#31867;&#21035;&#37327;&#36523;&#23450;&#21046;&#30340;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#35752;&#35770;&#23427;&#20204;&#22266;&#26377;&#30340;&#20248;&#21183;&#21644;&#28508;&#22312;&#23616;&#38480;&#24615;&#12290;&#35843;&#26597;&#20197;&#23637;&#26395;&#26410;&#26469;&#32467;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18267v1 Announce Type: cross  Abstract: In this survey, we present a detailed examination of the advancements in Neural Question Generation (NQG), a field leveraging neural network techniques to generate relevant questions from diverse inputs like knowledge bases, texts, and images. The survey begins with an overview of NQG's background, encompassing the task's problem formulation, prevalent benchmark datasets, established evaluation metrics, and notable applications. It then methodically classifies NQG approaches into three predominant categories: structured NQG, which utilizes organized data sources, unstructured NQG, focusing on more loosely structured inputs like texts or visual content, and hybrid NQG, drawing on diverse input modalities. This classification is followed by an in-depth analysis of the distinct neural network models tailored for each category, discussing their inherent strengths and potential limitations. The survey culminates with a forward-looking persp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#36890;&#29992;&#25552;&#31034;&#27010;&#24565;&#21644;&#21019;&#26032;&#30340;MeMo&#65288;&#24515;&#26234;&#27169;&#22411;&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#36873;&#25321;&#21644;&#23450;&#21046;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.18252</link><description>&lt;p&gt;
&#36890;&#36807;&#24515;&#26234;&#27169;&#22411;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#25552;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Generalist Prompting for Large Language Models by Mental Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18252
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#36890;&#29992;&#25552;&#31034;&#27010;&#24565;&#21644;&#21019;&#26032;&#30340;MeMo&#65288;&#24515;&#26234;&#27169;&#22411;&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#36873;&#25321;&#21644;&#23450;&#21046;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#20173;&#28982;&#38656;&#35201;&#29305;&#21035;&#35774;&#35745;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#38656;&#35201;&#19968;&#23450;&#39046;&#22495;&#30693;&#35782;&#30340;&#29305;&#23450;&#20219;&#21153;&#23569;&#37327;&#31034;&#20363;&#65292;&#35201;&#20040;&#34987;&#35774;&#35745;&#20026;&#31616;&#21333;&#65292;&#20294;&#21482;&#23545;&#23569;&#25968;&#31867;&#22411;&#20219;&#21153;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#24341;&#20837;&#36890;&#29992;&#25552;&#31034;&#30340;&#27010;&#24565;&#65292;&#23427;&#30340;&#35774;&#35745;&#21407;&#21017;&#26159;&#22312;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#20339;&#25110;&#25509;&#36817;&#26368;&#20339;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#25163;&#21160;&#36873;&#25321;&#21644;&#23450;&#21046;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MeMo&#65288;&#24515;&#26234;&#27169;&#22411;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#35774;&#35745;&#30340;&#21019;&#26032;&#25552;&#31034;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22320;&#23454;&#29616;&#36890;&#29992;&#25552;&#31034;&#30340;&#26631;&#20934;&#12290;MeMo&#23558;&#21508;&#31181;&#25552;&#31034;&#26041;&#27861;&#30340;&#26680;&#24515;&#31934;&#39635;&#25552;&#28860;&#20026;&#21333;&#20010;&#24515;&#26234;&#27169;&#22411;&#65292;&#24182;&#20801;&#35768;LLM&#33258;&#20027;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18252v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive performance on many tasks. However, to achieve optimal performance, specially designed prompting methods are still needed. These methods either rely on task-specific few-shot examples that require a certain level of domain knowledge, or are designed to be simple but only perform well on a few types of tasks. In this work, we attempt to introduce the concept of generalist prompting, which operates on the design principle of achieving optimal or near-optimal performance on a wide range of tasks while eliminating the need for manual selection and customization of prompts tailored to specific problems. Furthermore, we propose MeMo (Mental Models), an innovative prompting method that is simple-designed yet effectively fulfills the criteria of generalist prompting. MeMo distills the cores of various prompting methods into individual mental models and allows LLMs to autonomously select
&lt;/p&gt;</description></item><item><title>CogBench&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#19971;&#20010;&#35748;&#30693;&#24515;&#29702;&#23398;&#23454;&#39564;&#20013;&#34893;&#29983;&#20986;&#21313;&#20010;&#34892;&#20026;&#25351;&#26631;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#25552;&#20379;&#20102;&#24037;&#20855;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#24615;&#33021;&#25913;&#21892;&#21644;&#19982;&#20154;&#31867;&#34892;&#20026;&#19968;&#33268;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.18225</link><description>&lt;p&gt;
CogBench: &#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27493;&#20837;&#24515;&#29702;&#23454;&#39564;&#23460;
&lt;/p&gt;
&lt;p&gt;
CogBench: a large language model walks into a psychology lab
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18225
&lt;/p&gt;
&lt;p&gt;
CogBench&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#19971;&#20010;&#35748;&#30693;&#24515;&#29702;&#23398;&#23454;&#39564;&#20013;&#34893;&#29983;&#20986;&#21313;&#20010;&#34892;&#20026;&#25351;&#26631;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#25552;&#20379;&#20102;&#24037;&#20855;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#24615;&#33021;&#25913;&#21892;&#21644;&#19982;&#20154;&#31867;&#34892;&#20026;&#19968;&#33268;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#37096;&#20998;&#26159;&#30001;&#20110;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#24615;&#33021;&#25351;&#26631;&#30340;&#20027;&#35201;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CogBench&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20174;&#19971;&#20010;&#35748;&#30693;&#24515;&#29702;&#23398;&#23454;&#39564;&#20013;&#34893;&#29983;&#30340;&#21313;&#20010;&#34892;&#20026;&#25351;&#26631;&#12290;&#36825;&#31181;&#26032;&#39062;&#26041;&#27861;&#20026;&#34920;&#22411;&#21270;LLMs&#30340;&#34892;&#20026;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#21253;&#12290;&#25105;&#20204;&#23558;CogBench&#24212;&#29992;&#20110;35&#20010;LLMs&#65292;&#24471;&#21040;&#20016;&#23500;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#22810;&#23618;&#24314;&#27169;&#25216;&#26415;&#20998;&#26512;&#36825;&#20123;&#25968;&#25454;&#65292;&#32771;&#34385;&#21040;&#29305;&#23450;LLMs&#30340;&#24494;&#35843;&#29256;&#26412;&#20043;&#38388;&#30340;&#23884;&#22871;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#25913;&#21892;&#24615;&#33021;&#24182;&#19982;&#20154;&#31867;&#34892;&#20026;&#20445;&#25345;&#19968;&#33268;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24320;&#28304;&#27169;&#22411;&#27604;&#19987;&#26377;&#27169;&#22411;&#26356;&#23569;&#39118;&#38505;&#65292;&#24182;&#19988;&#31934;&#32454;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18225v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-t
&lt;/p&gt;</description></item><item><title>HearHere&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32593;&#32476;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20174;&#19981;&#21516;&#35270;&#35282;&#34701;&#21512;&#20449;&#24687;&#21644;&#35266;&#28857;&#65292;&#20197;&#20943;&#36731;&#26032;&#38395;&#28040;&#36153;&#20013;&#8220;&#22238;&#22768;&#23460;&#8221;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.18222</link><description>&lt;p&gt;
HearHere: &#36890;&#36807;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32593;&#32476;&#31995;&#32479;&#32531;&#35299;&#26032;&#38395;&#28040;&#36153;&#20013;&#30340;&#8220;&#22238;&#22768;&#23460;&#8221;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
HearHere: Mitigating Echo Chambers in News Consumption through an AI-based Web System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18222
&lt;/p&gt;
&lt;p&gt;
HearHere&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32593;&#32476;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20174;&#19981;&#21516;&#35270;&#35282;&#34701;&#21512;&#20449;&#24687;&#21644;&#35266;&#28857;&#65292;&#20197;&#20943;&#36731;&#26032;&#38395;&#28040;&#36153;&#20013;&#8220;&#22238;&#22768;&#23460;&#8221;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#27491;&#22312;&#22823;&#21147;&#21162;&#21147;&#20943;&#36731;&#8220;&#22238;&#22768;&#23460;&#8221;&#25152;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21253;&#25324;&#26356;&#23481;&#26131;&#21463;&#21040;&#34394;&#20551;&#26032;&#38395;&#30340;&#24433;&#21709;&#20197;&#21450;&#23545;&#25509;&#21463;&#31185;&#23398;&#35777;&#25454;&#30340;&#25239;&#25298;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HearHere&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32593;&#32476;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20174;&#19981;&#21516;&#35270;&#35282;&#34701;&#21512;&#20449;&#24687;&#21644;&#35266;&#28857;&#12290;HearHere&#36890;&#36807;&#20004;&#31181;&#21487;&#35270;&#21270;&#26041;&#24335;&#20419;&#36827;&#20102;&#26032;&#38395;&#20449;&#24687;&#28040;&#36153;&#30340;&#20851;&#38190;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18222v1 Announce Type: cross  Abstract: Considerable efforts are currently underway to mitigate the negative impacts of echo chambers, such as increased susceptibility to fake news and resistance towards accepting scientific evidence. Prior research has presented the development of computer systems that support the consumption of news information from diverse political perspectives to mitigate the echo chamber effect. However, existing studies still lack the ability to effectively support the key processes of news information consumption and quantitatively identify a political stance towards the information. In this paper, we present HearHere, an AI-based web system designed to help users accommodate information and opinions from diverse perspectives. HearHere facilitates the key processes of news information consumption through two visualizations. Visualization 1 provides political news with quantitative political stance information, derived from our graph-based political c
&lt;/p&gt;</description></item><item><title>Lemur&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65292;&#35299;&#20915;&#20102;&#26085;&#24535;&#35299;&#26512;&#20013;&#23384;&#22312;&#30340;&#20154;&#24037;&#35268;&#21017;&#20381;&#36182;&#21644;&#35821;&#20041;&#20449;&#24687;&#24573;&#30053;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18205</link><description>&lt;p&gt;
Lemur: &#20351;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#36827;&#34892;&#26085;&#24535;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18205
&lt;/p&gt;
&lt;p&gt;
Lemur&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65292;&#35299;&#20915;&#20102;&#26085;&#24535;&#35299;&#26512;&#20013;&#23384;&#22312;&#30340;&#20154;&#24037;&#35268;&#21017;&#20381;&#36182;&#21644;&#35821;&#20041;&#20449;&#24687;&#24573;&#30053;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#36719;&#20214;&#31995;&#32479;&#20135;&#29983;&#30340;&#26085;&#24535;&#23545;&#30417;&#35270;&#31995;&#32479;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#36827;&#30340;&#26085;&#24535;&#20998;&#26512;&#26377;&#21161;&#20110;&#26816;&#27979;&#12289;&#25253;&#35686;&#21644;&#35786;&#26029;&#31995;&#32479;&#25925;&#38556;&#12290;&#26085;&#24535;&#35299;&#26512;&#26159;&#26085;&#24535;&#20998;&#26512;&#33258;&#21160;&#21270;&#30340;&#20851;&#38190;&#38454;&#27573;&#65292;&#23427;&#28041;&#21450;&#23558;&#21407;&#22987;&#26085;&#24535;&#28040;&#24687;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#27169;&#26495;&#12290;&#29616;&#26377;&#30340;&#26085;&#24535;&#35299;&#26512;&#22120;&#30001;&#20110;&#20381;&#36182;&#20110;&#20154;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#32780;&#26080;&#27861;&#35782;&#21035;&#27491;&#30830;&#30340;&#27169;&#26495;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20391;&#37325;&#20110;&#32479;&#35745;&#29305;&#24449;&#65292;&#32780;&#24573;&#30053;&#20102;&#26085;&#24535;&#28040;&#24687;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65288;Lemur&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#25670;&#33073;&#32321;&#29712;&#30340;&#25163;&#21160;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20449;&#24687;&#29109;&#21551;&#21457;&#30340;&#26032;&#22411;&#25277;&#26679;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#20856;&#22411;&#26085;&#24535;&#36827;&#34892;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#26085;&#24535;&#27169;&#26495;&#30340;&#21512;&#24182;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24605;&#32500;&#38142;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18205v1 Announce Type: cross  Abstract: Logs produced by extensive software systems are integral to monitoring system behaviors. Advanced log analysis facilitates the detection, alerting, and diagnosis of system faults. Log parsing, which entails transforming raw log messages into structured templates, constitutes a critical phase in the automation of log analytics. Existing log parsers fail to identify the correct templates due to reliance on human-made rules. Besides, These methods focus on statistical features while ignoring semantic information in log messages. To address these challenges, we introduce a cutting-edge \textbf{L}og parsing framework with \textbf{E}ntropy sampling and Chain-of-Thought \textbf{M}erging (Lemur). Specifically, to discard the tedious manual rules. We propose a novel sampling method inspired by information entropy, which efficiently clusters typical logs. Furthermore, to enhance the merging of log templates, we design a chain-of-thought method f
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#23884;&#20837;&#65292;&#23637;&#31034;&#31616;&#21333;&#27169;&#22411;&#22312;&#23884;&#20837;&#22797;&#26434;&#34920;&#26684;&#25968;&#25454;&#26102;&#20248;&#20110;&#22797;&#26434;&#27169;&#22411;&#65292;&#24182;&#23558;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#34920;&#31034;AWS&#23458;&#25143;&#30340;&#23884;&#20837;&#65292;&#26174;&#33879;&#33410;&#30465;&#24320;&#21457;&#26102;&#38388;&#24182;&#35266;&#23519;&#21040;&#19979;&#28216;&#27169;&#22411;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.18164</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#36890;&#29992;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#23458;&#25143;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Autoencoder-based General Purpose Representation Learning for Customer Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18164
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#23884;&#20837;&#65292;&#23637;&#31034;&#31616;&#21333;&#27169;&#22411;&#22312;&#23884;&#20837;&#22797;&#26434;&#34920;&#26684;&#25968;&#25454;&#26102;&#20248;&#20110;&#22797;&#26434;&#27169;&#22411;&#65292;&#24182;&#23558;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#34920;&#31034;AWS&#23458;&#25143;&#30340;&#23884;&#20837;&#65292;&#26174;&#33879;&#33410;&#30465;&#24320;&#21457;&#26102;&#38388;&#24182;&#35266;&#23519;&#21040;&#19979;&#28216;&#27169;&#22411;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#39046;&#22495;&#29305;&#23450;&#22522;&#30784;&#32467;&#26500;&#21450;&#20854;&#29983;&#25104;&#22240;&#32032;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;&#21508;&#31181;&#29992;&#20363;&#26080;&#20851;&#24212;&#29992;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#34920;&#26684;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#20351;&#24471;&#36890;&#36807;&#22810;&#32500;&#21521;&#37327;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#34920;&#31034;&#36825;&#20123;&#32467;&#26500;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#23884;&#20837;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#31616;&#21333;&#27169;&#22411;&#22312;&#23884;&#20837;&#39640;&#24230;&#22797;&#26434;&#34920;&#26684;&#25968;&#25454;&#26102;&#20248;&#20110;&#22797;&#26434;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#25554;&#25300;&#24335;&#12289;&#20016;&#23500;&#21644;&#21311;&#21517;&#21270;&#30340;&#34920;&#31034;AWS&#23458;&#25143;&#30340;&#23884;&#20837;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#65292;&#33410;&#30465;&#24320;&#21457;&#26102;&#38388;&#39640;&#36798;45&#65285;&#65292;&#24182;&#35266;&#23519;&#21040;&#19979;&#28216;&#27169;&#22411;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#22810;&#23618;&#25910;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#37325;&#26500;&#25439;&#22833;&#35745;&#31639;&#30340;&#37325;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18164v1 Announce Type: cross  Abstract: In recent years, exploiting the domain-specific underlying structure of data and its generative factors for representation learning has shown success in various use-case agnostic applications. However, the diversity and complexity of tabular data have made it challenging to represent these structures in a latent space through multi-dimensional vectors. We design an autoencoder-based framework for building general purpose embeddings, we assess the performance of different autoencoder architectures, and show simpler models outperform complex ones in embedding highly complex tabular data. We apply our framework to produce plug-and-play, rich, and anonymized embeddings representing AWS customers for usage in any model, saving up to 45% of development time, and observe significant improvements in downstream models. Moreover, we propose a significant improvement to the calculation of reconstruction loss for multi-layer contractive autoencode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#30340;&#24433;&#21709;&#65292;&#20197;&#25351;&#23548;&#36873;&#25321;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#23545;11&#31181;&#27169;&#22411;&#31995;&#21015;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35206;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18158</link><description>&lt;p&gt;
&#35780;&#20272;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Quantized Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#30340;&#24433;&#21709;&#65292;&#20197;&#25351;&#23548;&#36873;&#25321;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#23545;11&#31181;&#27169;&#22411;&#31995;&#21015;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35206;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#32463;&#25104;&#20026;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#20855;&#20307;&#22320;&#65292;PTQ&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;LLMs&#20013;&#30340;&#20869;&#23384;&#28040;&#32791;&#24182;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#28385;&#36275;&#21508;&#31181;&#22330;&#26223;&#19979;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#35201;&#27714;&#65292;&#23545;&#37327;&#21270;LLMs&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#25351;&#23548;&#37327;&#21270;&#26041;&#27861;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;PTQ&#23545;11&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;&#21253;&#25324;OPT&#12289;LLaMA2&#12289;Falcon&#12289;Bloomz&#12289;Mistral&#12289;ChatGLM&#12289;Vicuna&#12289;LongChat&#12289;StableLM&#12289;Gemma&#21644;Mamba&#65289;&#30340;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#30340;&#24433;&#21709;&#65292;&#33539;&#22260;&#20174;125M&#21040;180B&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#22240;&#32032;&#12290;&#35780;&#20272;&#28085;&#30422;&#20102;&#20116;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#65306;&#22522;&#30784;NLP&#12289;&#31361;&#28982;&#20986;&#29616;&#30340;&#33021;&#21147;&#12289;&#21487;&#38752;&#24615;&#12289;&#23545;&#35805;&#21644;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#23637;&#31034;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18158v1 Announce Type: cross  Abstract: Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs). Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs. To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their appli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24037;&#20855;&#35843;&#29992;&#31649;&#36947;&#65292;&#26088;&#22312;&#25511;&#21046;&#22823;&#35268;&#27169;&#29616;&#23454;&#19990;&#30028;API&#65292;&#20174;&#32780;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18157</link><description>&lt;p&gt;
&#20174;&#24635;&#32467;&#21040;&#34892;&#21160;&#65306;&#21033;&#29992;&#24320;&#25918;&#19990;&#30028;API&#22686;&#24378;&#22797;&#26434;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24037;&#20855;&#35843;&#29992;&#31649;&#36947;&#65292;&#26088;&#22312;&#25511;&#21046;&#22823;&#35268;&#27169;&#29616;&#23454;&#19990;&#30028;API&#65292;&#20174;&#32780;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19982;&#21160;&#29289;&#20043;&#38388;&#30340;&#21306;&#21035;&#22312;&#20110;&#20154;&#31867;&#20855;&#26377;&#20351;&#29992;&#21644;&#21019;&#36896;&#24037;&#20855;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#24037;&#20855;&#20351;&#20154;&#31867;&#33021;&#22815;&#20811;&#26381;&#29983;&#29702;&#38480;&#21046;&#65292;&#20419;&#36827;&#20102;&#23439;&#20255;&#25991;&#26126;&#30340;&#21019;&#36896;&#12290;&#31867;&#20284;&#22320;&#65292;&#23558;&#20687;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36825;&#26679;&#30340;&#22522;&#30784;&#27169;&#22411;&#36171;&#20104;&#23398;&#20064;&#22806;&#37096;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#21487;&#33021;&#26159;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#26412;&#39046;&#22495;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#36861;&#27714;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;LLMs&#30340;&#24037;&#20855;&#35843;&#29992;&#33021;&#21147;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#24378;&#35843;&#26500;&#24314;&#29992;&#20110;&#27169;&#22411;&#24494;&#35843;&#30340;&#30456;&#20851;&#25968;&#25454;&#38598;&#12290;&#30456;&#21453;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#20805;&#20998;&#21033;&#29992;LLMs&#22266;&#26377;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#25511;&#21046;&#22823;&#22411;&#29616;&#23454;&#19990;&#30028;API&#30340;&#21019;&#26032;&#24037;&#20855;&#35843;&#29992;&#31649;&#36947;&#12290;&#36825;&#19968;&#31649;&#36947;&#21453;&#26144;&#20102;&#20154;&#31867;&#35299;&#20915;&#20219;&#21153;&#30340;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;c
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18157v1 Announce Type: new  Abstract: The distinction between humans and animals lies in the unique ability of humans to use and create tools. Tools empower humans to overcome physiological limitations, fostering the creation of magnificent civilizations. Similarly, enabling foundational models like Large Language Models (LLMs) with the capacity to learn external tool usage may serve as a pivotal step toward realizing artificial general intelligence. Previous studies in this field have predominantly pursued two distinct approaches to augment the tool invocation capabilities of LLMs. The first approach emphasizes the construction of relevant datasets for model fine-tuning. The second approach, in contrast, aims to fully exploit the inherent reasoning abilities of LLMs through in-context learning strategies. In this work, we introduce a novel tool invocation pipeline designed to control massive real-world APIs. This pipeline mirrors the human task-solving process, addressing c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#27969;&#30340;&#35270;&#35282;&#35299;&#37322;&#24182;&#24178;&#39044;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pruning Head via PatH&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#20914;&#31361;</title><link>https://arxiv.org/abs/2402.18154</link><description>&lt;p&gt;
&#20999;&#26029;&#22836;&#37096;&#32456;&#32467;&#20914;&#31361;&#65306;&#35299;&#37322;&#21644;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18154
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#27969;&#30340;&#35270;&#35282;&#35299;&#37322;&#24182;&#24178;&#39044;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pruning Head via PatH&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26816;&#32034;&#22686;&#24378;&#21644;&#24037;&#20855;&#22686;&#24378;&#23637;&#31034;&#20102;&#36890;&#36807;&#25552;&#20379;&#22806;&#37096;&#19978;&#19979;&#25991;&#26469;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#20869;&#37096;&#35760;&#24518;&#36793;&#30028;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#19978;&#19979;&#25991;&#19981;&#21487;&#36991;&#20813;&#22320;&#21457;&#29983;&#20914;&#31361;&#65292;&#23548;&#33268;LMs&#20869;&#37096;&#20986;&#29616;&#30693;&#35782;&#20914;&#31361;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20449;&#24687;&#27969;&#30340;&#35270;&#35282;&#35299;&#37322;&#30693;&#35782;&#20914;&#31361;&#30340;&#26426;&#21046;&#65292;&#28982;&#21518;&#36890;&#36807;&#22312;&#20851;&#38190;&#28857;&#36827;&#34892;&#31934;&#30830;&#24178;&#39044;&#26469;&#32531;&#35299;&#20914;&#31361;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#21518;&#32493;&#23618;&#20013;&#26377;&#19968;&#20123;&#20855;&#26377;&#30456;&#21453;&#25928;&#26524;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#20854;&#20013;&#20869;&#23384;&#22836;&#21487;&#20197;&#20174;&#20869;&#37096;&#35760;&#24518;&#20013;&#21484;&#22238;&#30693;&#35782;&#65292;&#32780;&#19978;&#19979;&#25991;&#22836;&#21487;&#20197;&#20174;&#22806;&#37096;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LMs&#20013;&#30693;&#35782;&#20914;&#31361;&#21457;&#29983;&#30340;&#20851;&#38190;&#28857;&#26159;&#20869;&#23384;&#22836;&#21644;&#19978;&#19979;&#25991;&#22836;&#25972;&#21512;&#19981;&#19968;&#33268;&#20449;&#24687;&#27969;&#30340;&#22320;&#26041;&#12290;&#21463;&#21040;&#36825;&#20123;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pruning Head via PatH&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18154v1 Announce Type: cross  Abstract: Recently, retrieval augmentation and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context. However, internal memory and external context inevitably clash, leading to knowledge conflicts within LMs. In this paper, we aim to interpret the mechanism of knowledge conflicts through the lens of information flow, and then mitigate conflicts by precise interventions at the pivotal point. We find there are some attention heads with opposite effects in the later layers, where memory heads can recall knowledge from internal memory, and context heads can retrieve knowledge from external context. Moreover, we reveal that the pivotal point at which knowledge conflicts emerge in LMs is the integration of inconsistent information flows by memory heads and context heads. Inspired by the insights, we propose a novel method called Pruning Head via PatH 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#25968;&#25454;&#38598;&#26465;&#20214;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#21892;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18153</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Neural Network Weights Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18153
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#25968;&#25454;&#38598;&#26465;&#20214;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#21892;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#26159;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#20855;&#26377;&#26174;&#33879;&#20852;&#36259;&#30340;&#35805;&#39064;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#22312;&#26032;&#20219;&#21153;&#19978;&#25913;&#21892;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36801;&#31227;&#23398;&#20064;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#28304;&#25968;&#25454;&#19982;&#30446;&#26631;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#65292;&#20294;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#25104;&#26412;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#24120;&#26159;&#30450;&#30446;&#36873;&#25321;&#65292;&#24182;&#24076;&#26395;&#23427;&#20204;&#33021;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27425;&#20248;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#38598;&#26465;&#20214;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#37319;&#26679;&#23454;&#29616;&#39640;&#25928;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#37325;&#24314;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#65292;&#20197;&#23398;&#20064;&#27599;&#20010;&#25968;&#25454;&#38598;&#26465;&#20214;&#19979;&#19968;&#32452;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#22312;&#26410;&#35265;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#24067;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18153v1 Announce Type: cross  Abstract: Transfer learning is a topic of significant interest in recent deep learning research because it enables faster convergence and improved performance on new tasks. While the performance of transfer learning depends on the similarity of the source data to the target data, it is costly to train a model on a large number of datasets. Therefore, pretrained models are generally blindly selected with the hope that they will achieve good performance on the given task. To tackle such suboptimality of the pretrained models, we propose an efficient and adaptive transfer learning scheme through dataset-conditioned pretrained weights sampling. Specifically, we use a latent diffusion model with a variational autoencoder that can reconstruct the neural network weights, to learn the distribution of a set of pretrained weights conditioned on each dataset for transfer learning on unseen datasets. By learning the distribution of a neural network on a var
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26465;&#20214;&#35299;&#30721;&#22120;&#21644;NeRV-like&#27169;&#22359;&#30340;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#38544;&#24335;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.18152</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#35299;&#30721;&#22120;&#22686;&#24378;&#35270;&#39057;&#30340;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Boosting Neural Representations for Videos with a Conditional Decoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18152
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26465;&#20214;&#35299;&#30721;&#22120;&#21644;NeRV-like&#27169;&#22359;&#30340;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#38544;&#24335;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#24050;&#32463;&#25104;&#20026;&#35270;&#39057;&#23384;&#20648;&#21644;&#22788;&#29702;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#35270;&#39057;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#26631;&#24103;&#35299;&#30721;&#36807;&#31243;&#20013;&#20013;&#38388;&#29305;&#24449;&#30340;&#19981;&#36275;&#23545;&#40784;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20854;&#34920;&#31034;&#33021;&#21147;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22686;&#24378;&#26694;&#26550;&#26469;&#21152;&#24378;&#24403;&#21069;&#30340;&#38544;&#24335;&#35270;&#39057;&#34920;&#31034;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#26102;&#38388;&#24863;&#30693;&#20223;&#23556;&#21464;&#25442;&#27169;&#22359;&#30340;&#26465;&#20214;&#35299;&#30721;&#22120;&#65292;&#35813;&#27169;&#22359;&#20351;&#29992;&#24103;&#32034;&#24341;&#20316;&#20026;&#20808;&#39564;&#26465;&#20214;&#65292;&#26377;&#25928;&#22320;&#23558;&#20013;&#38388;&#29305;&#24449;&#19982;&#30446;&#26631;&#24103;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#24358;NeRV-like&#27169;&#22359;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#20013;&#38388;&#29305;&#24449;&#65292;&#24182;&#23454;&#29616;&#26356;&#24179;&#34913;&#30340;&#21442;&#25968;&#20998;&#24067;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#20511;&#21161;&#39640;&#39057;&#20449;&#24687;&#20445;&#30041;&#30340;&#37325;&#26500;&#25439;&#22833;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22686;&#24378;&#20102;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18152v1 Announce Type: cross  Abstract: Implicit neural representations (INRs) have emerged as a promising approach for video storage and processing, showing remarkable versatility across various video tasks. However, existing methods often fail to fully leverage their representation capabilities, primarily due to inadequate alignment of intermediate features during target frame decoding. This paper introduces a universal boosting framework for current implicit video representation approaches. Specifically, we utilize a conditional decoder with a temporal-aware affine transform module, which uses the frame index as a prior condition to effectively align intermediate features with target frames. Besides, we introduce a sinusoidal NeRV-like block to generate diverse intermediate features and achieve a more balanced parameter distribution, thereby enhancing the model's capacity. With a high-frequency information-preserving reconstruction loss, our approach successfully boosts m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#35282;&#33394;&#23450;&#20041;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#25972;&#21512;&#26816;&#32034;&#20449;&#24687;&#20197;&#29983;&#25104;&#26356;&#21152;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18150</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#35282;&#33394;&#23450;&#20041;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#25972;&#21512;&#26816;&#32034;&#20449;&#24687;&#20197;&#29983;&#25104;&#26356;&#21152;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#23558;&#26469;&#33258;&#26816;&#32034;&#30340;&#39069;&#22806;&#20449;&#24687;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#26377;&#25928;&#21033;&#29992;&#26816;&#32034;&#20449;&#24687;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#65292;&#26377;&#26102;&#20250;&#24573;&#35270;&#25110;&#34987;&#38169;&#35823;&#24341;&#23548;&#12290;&#20854;&#20851;&#38190;&#21407;&#22240;&#22312;&#20110;LLMs&#30340;&#35757;&#32451;&#27809;&#26377;&#28165;&#26224;&#22320;&#35753;LLMs&#23398;&#20250;&#22914;&#20309;&#21033;&#29992;&#20855;&#26377;&#19981;&#21516;&#36136;&#37327;&#30340;&#26816;&#32034;&#25991;&#26412;&#36755;&#20837;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#35282;&#65292;&#23558;LLMs&#22312;RAG&#20013;&#30340;&#35282;&#33394;&#35270;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#36825;&#24847;&#21619;&#30528;&#26080;&#35770;&#26816;&#32034;&#25991;&#26412;&#30340;&#27491;&#30830;&#24615;&#12289;&#23436;&#25972;&#24615;&#25110;&#26377;&#29992;&#24615;&#22914;&#20309;&#65292;LLMs&#37117;&#33021;&#19968;&#33268;&#22320;&#25972;&#21512;&#26816;&#32034;&#25991;&#26412;&#20013;&#30340;&#30693;&#35782;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#29983;&#25104;&#27604;&#26816;&#32034;&#25991;&#26412;&#26356;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20248;&#21270;LLMs&#29992;&#20110;RAG&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18150v1 Announce Type: cross  Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#30789;&#25277;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#27169;&#25311;&#20154;&#31867;&#31181;&#32676;&#20122;&#32452;&#30340;&#24847;&#35265;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#22238;&#24212;&#20998;&#24067;&#19982;&#23454;&#38469;&#32654;&#22269;&#20844;&#20849;&#33286;&#35770;&#35843;&#26597;&#38750;&#24120;&#30456;&#20284;&#12290;</title><link>https://arxiv.org/abs/2402.18144</link><description>&lt;p&gt;
&#38543;&#26426;&#30789;&#25277;&#26679;&#65306;&#22522;&#20110;&#32676;&#20307;&#32423;&#21035;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#20122;&#20154;&#21475;&#24847;&#35265;
&lt;/p&gt;
&lt;p&gt;
Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18144
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#30789;&#25277;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#27169;&#25311;&#20154;&#31867;&#31181;&#32676;&#20122;&#32452;&#30340;&#24847;&#35265;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#22238;&#24212;&#20998;&#24067;&#19982;&#23454;&#38469;&#32654;&#22269;&#20844;&#20849;&#33286;&#35770;&#35843;&#26597;&#38750;&#24120;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30456;&#20851;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21253;&#25324;&#31181;&#26063;&#12289;&#24615;&#21035;&#31561;&#12290;&#20026;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22522;&#20110;&#20154;&#21475;&#25968;&#25454;&#30340;&#20010;&#24615;&#21487;&#20197;&#23454;&#29616;&#29983;&#25104;&#19982;&#20154;&#31867;&#35266;&#28857;&#19968;&#33268;&#30340;&#24847;&#35265;&#12290;&#22522;&#20110;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#38543;&#26426;&#30789;&#25277;&#26679;&#8221;&#65292;&#19968;&#31181;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#31867;&#31181;&#32676;&#20122;&#32452;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20998;&#26512;&#20102;&#65306;1&#65289;&#20165;&#22522;&#20110;&#20854;&#20154;&#21475;&#20998;&#24067;&#29983;&#25104;&#19982;&#20154;&#31867;&#32676;&#20307;&#23545;&#24212;&#30340;&#35843;&#26597;&#22238;&#24212;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;2&#65289;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#20154;&#21475;&#23376;&#32676;&#21644;&#20027;&#39064;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#38543;&#26426;&#30789;&#25277;&#26679;&#24182;&#20165;&#20351;&#29992;&#32676;&#20307;&#32423;&#21035;&#30340;&#20154;&#21475;&#20449;&#24687;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19982;&#23454;&#38469;&#32654;&#22269;&#20844;&#20849;&#33286;&#35770;&#35843;&#26597;&#38750;&#24120;&#30456;&#20284;&#30340;&#22238;&#24212;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#22797;&#21046;&#24615;&#21462;&#20915;&#20110;&#19981;&#21516;&#22240;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18144v1 Announce Type: new  Abstract: Large language models exhibit societal biases associated with demographic information, including race, gender, and others. Endowing such language models with personalities based on demographic data can enable generating opinions that align with those of humans. Building on this idea, we propose "random silicon sampling," a method to emulate the opinions of the human population sub-group. Our study analyzed 1) a language model that generates the survey responses that correspond with a human group based solely on its demographic distribution and 2) the applicability of our methodology across various demographic subgroups and thematic questions. Through random silicon sampling and using only group-level demographic information, we discovered that language models can generate response distributions that are remarkably similar to the actual U.S. public opinion polls. Moreover, we found that the replicability of language models varies dependin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CARE CA&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#26174;&#24335;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#12289;&#20197;&#21450;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18139</link><description>&lt;p&gt;
&#22240;&#26524;&#20851;&#31995;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30495;&#27491;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Cause and Effect: Can Large Language Models Truly Understand Causality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CARE CA&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#32467;&#21512;&#26174;&#24335;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#12289;&#20197;&#21450;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#35299;&#35835;&#21644;&#35299;&#37322;&#35821;&#35328;&#25152;&#28041;&#21450;&#30340;&#22797;&#26434;&#22240;&#26524;&#20851;&#31995;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#26126;&#30830;&#25110;&#38544;&#21547;&#30340;&#22240;&#26524;&#25512;&#29702;&#65292;&#28982;&#32780;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#23558;&#20004;&#32773;&#32467;&#21512;&#36215;&#26469;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#22240;&#26524;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20855;&#26377;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#25512;&#29702;&#22686;&#24378;&#65288;CARE CA&#65289;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#22240;&#26524;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558; ConceptNet &#21644;&#21453;&#20107;&#23454;&#38472;&#36848;&#20013;&#30340;&#26126;&#30830;&#22240;&#26524;&#26816;&#27979;&#27169;&#22359;&#20197;&#21450;&#36890;&#36807;LLMs&#36827;&#34892;&#30340;&#38544;&#21547;&#22240;&#26524;&#26816;&#27979;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#19968;&#23618;&#21453;&#20107;&#23454;&#35299;&#37322;&#36827;&#19968;&#27493;&#31361;&#20986;LLMs&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;ConceptNet &#20013;&#30340;&#30693;&#35782;&#25552;&#39640;&#20102;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18139v1 Announce Type: cross  Abstract: With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs. Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality. The knowledge from ConceptNet enhances the performance of multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; DecisionNCE &#26694;&#26550;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#21644;&#19982;&#35821;&#35328;&#25351;&#20196;&#23545;&#40784;&#30340;&#26377;&#25928;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.18137</link><description>&lt;p&gt;
DecisionNCE: &#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DecisionNCE &#26694;&#26550;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#23398;&#20064;&#23454;&#20307;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#21644;&#19982;&#35821;&#35328;&#25351;&#20196;&#23545;&#40784;&#30340;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#26159;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#19977;&#22823;&#30446;&#26631;&#65306;1&#65289;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#65307;2&#65289;&#24378;&#21270;&#35270;&#35273;&#34920;&#31034;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65307;3&#65289;&#25429;&#33719;&#36712;&#36857;&#32423;&#35821;&#35328;&#22522;&#30784;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#22823;&#37096;&#20998;&#24050;&#26377;&#26041;&#27861;&#36890;&#36807;&#19981;&#21516;&#30340;&#30446;&#26631;&#26469;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#65292;&#24448;&#24448;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#32479;&#19968;&#30446;&#26631;&#65292;&#21487;&#20197;&#21516;&#26102;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20219;&#21153;&#36827;&#23637;&#20449;&#24687;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#35821;&#35328;&#25351;&#20196;&#26080;&#32541;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#38544;&#24335;&#20559;&#22909;&#65292;&#22312;&#35270;&#35273;&#36712;&#36857;&#19982;&#20854;&#23545;&#24212;&#30340;&#35821;&#35328;&#25351;&#20196;&#30456;&#27604;&#19981;&#21305;&#37197;&#23545;&#26356;&#22909;&#22320;&#23545;&#40784;&#26102;&#65292;&#27969;&#34892;&#30340; Bradley-Terry &#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#22870;&#21169;&#37325;&#26032;&#21442;&#25968;&#21270;&#32780;&#21464;&#20026;&#34920;&#31034;&#23398;&#20064;&#12290;&#32467;&#26524;&#20135;&#29983;&#30340; DecisionNCE &#26694;&#26550;&#65292;&#31867;&#20284;&#20110; InfoNC
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18137v1 Announce Type: cross  Abstract: Multimodal pretraining has emerged as an effective strategy for the trinity of goals of representation learning in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual representation; 3) capturing trajectory-level language grounding. Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNC
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#26631;&#20934; DP &#22522;&#30784;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#32473;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#39044;&#27979;&#26631;&#31614;&#26465;&#20214;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#26041;&#27861;&#26469;&#25511;&#21046;&#24402;&#32435;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.18129</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#30340;&#20844;&#24179;&#23398;&#20064;&#31639;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18129
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#26631;&#20934; DP &#22522;&#30784;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#32473;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#39044;&#27979;&#26631;&#31614;&#26465;&#20214;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#26041;&#27861;&#26469;&#25511;&#21046;&#24402;&#32435;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#30340;&#30417;&#30563;&#24335;&#23398;&#20064;&#31639;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#22791;&#21463;&#20851;&#27880;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20998;&#37197;&#26631;&#31614;&#26102;&#24456;&#23569;&#20381;&#36182;&#25935;&#24863;&#23646;&#24615;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#26631;&#20934;DP&#65288;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#65289;&#22522;&#30784;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#32473;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#39044;&#27979;&#26631;&#31614;&#26465;&#20214;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#38750;&#22343;&#21248;&#20998;&#24067;&#25935;&#24863;&#23646;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#20998;&#31867;&#35268;&#21017;&#20559;&#21521;&#21344;&#25454;&#22823;&#22810;&#25968;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#23646;&#24615;&#32467;&#26524;&#12290;&#20026;&#20102;&#25511;&#21046;DP-based&#20844;&#24179;&#23398;&#20064;&#20013;&#30340;&#36825;&#31181;&#24402;&#32435;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#65288;SA&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18129v1 Announce Type: cross  Abstract: Fair supervised learning algorithms assigning labels with little dependence on a sensitive attribute have attracted great attention in the machine learning community. While the demographic parity (DP) notion has been frequently used to measure a model's fairness in training fair classifiers, several studies in the literature suggest potential impacts of enforcing DP in fair learning algorithms. In this work, we analytically study the effect of standard DP-based regularization methods on the conditional distribution of the predicted label given the sensitive attribute. Our analysis shows that an imbalanced training dataset with a non-uniform distribution of the sensitive attribute could lead to a classification rule biased toward the sensitive attribute outcome holding the majority of training data. To control such inductive biases in DP-based fair learning, we propose a sensitive attribute-based distributionally robust optimization (SA
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#27688;&#22522;&#37240;&#35821;&#35328;&#20013;&#30340;&#36866;&#24212;&#24615;&#12289;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.18121</link><description>&lt;p&gt;
&#25327;&#25937;&#33521;&#38596;&#20234;&#24052;&#20160;&#30340;&#36951;&#20135;&#65306;&#35780;&#20272;&#22235;&#31181;&#35821;&#35328;&#27169;&#22411;&#22312;&#27688;&#22522;&#37240;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#27688;&#22522;&#37240;&#35821;&#35328;&#20013;&#30340;&#36866;&#24212;&#24615;&#12289;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#21069;&#27839;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#24320;&#21457;&#30340;&#27688;&#22522;&#37240;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#35780;&#20272;&#65292;&#26412;&#30740;&#31350;&#23457;&#26597;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#29983;&#25104;&#12289;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#26041;&#38754;&#30340;&#36866;&#24212;&#24615;&#65292;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#25581;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#65292;&#20026;&#24357;&#21512;&#35821;&#35328;&#24046;&#36317;&#24320;&#36767;&#20102;&#36947;&#36335;&#12290;&#36890;&#36807;&#25552;&#20379;&#22522;&#20934;&#21644;&#29702;&#35299;&#25361;&#25112;&#65292;&#20026;&#26410;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#26088;&#22312;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#22312;&#31867;&#20284;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#26631;&#24535;&#30528;&#35821;&#35328;&#25216;&#26415;&#21253;&#23481;&#24615;&#21644;&#36827;&#27493;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18121v1 Announce Type: cross  Abstract: This study assesses four cutting-edge language models in the underexplored Aminoacian language. Through evaluation, it scrutinizes their adaptability, effectiveness, and limitations in text generation, semantic coherence, and contextual understanding. Uncovering insights into these models' performance in a low-resourced language, this research pioneers pathways to bridge linguistic gaps. By offering benchmarks and understanding challenges, it lays groundwork for future advancements in natural language processing, aiming to elevate the applicability of language models in similar linguistic landscapes, marking a significant step toward inclusivity and progress in language technology.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#37197;&#20026;&#8220;&#25945;&#24072;&#8221;&#29983;&#25104;&#25968;&#25454;&#21644;&#8220;&#35780;&#35770;&#23478;&#8221;&#35780;&#20272;&#23398;&#29983;&#34920;&#29616;&#30340;&#21452;&#37325;&#35282;&#33394;&#65292;&#30740;&#31350;&#34920;&#26126;&#36825;&#31181;&#22522;&#20110;&#21453;&#39304;&#30340;&#26041;&#27861;&#22312;&#24189;&#40664;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18113</link><description>&lt;p&gt;
&#23567;&#32780;&#26377;&#36259;&#65306;&#19968;&#31181;&#22522;&#20110;&#21453;&#39304;&#30340;&#24189;&#40664;&#31934;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Small But Funny: A Feedback-Driven Approach to Humor Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18113
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#37197;&#20026;&#8220;&#25945;&#24072;&#8221;&#29983;&#25104;&#25968;&#25454;&#21644;&#8220;&#35780;&#35770;&#23478;&#8221;&#35780;&#20272;&#23398;&#29983;&#34920;&#29616;&#30340;&#21452;&#37325;&#35282;&#33394;&#65292;&#30740;&#31350;&#34920;&#26126;&#36825;&#31181;&#22522;&#20110;&#21453;&#39304;&#30340;&#26041;&#27861;&#22312;&#24189;&#40664;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#25581;&#31034;&#20102;&#26377;&#28508;&#21147;&#30340;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25191;&#34892;&#35832;&#22914;&#22797;&#26434;&#25512;&#29702;&#21644;&#21019;&#24847;&#20889;&#20316;&#20043;&#31867;&#30340;&#20219;&#21153;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#27169;&#20223;&#25945;&#24072;&#22238;&#31572;&#30340;&#26041;&#24335;&#36827;&#34892;&#31934;&#39311;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;LLMs&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#26356;&#26131;&#35775;&#38382;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#20013;&#12290;&#34429;&#28982;&#36825;&#23545;&#20110;&#31616;&#21333;&#20219;&#21153;&#25928;&#26524;&#24456;&#22909;&#65292;&#20294;&#22312;&#38656;&#35201;&#22797;&#26434;&#35821;&#35328;&#29702;&#35299;&#21644;&#21019;&#36896;&#21147;&#30340;&#20219;&#21153;&#19978;&#23384;&#22312;&#23454;&#36136;&#24615;&#30340;&#34920;&#29616;&#24046;&#36317;&#65292;&#27604;&#22914;&#24189;&#40664;&#29983;&#25104;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#31181;&#24046;&#36317;&#21487;&#33021;&#28304;&#33258;&#20110;&#21019;&#36896;&#24615;&#20219;&#21153;&#21487;&#33021;&#21333;&#20973;&#27169;&#20223;&#23398;&#20064;&#26159;&#24456;&#38590;&#30340;&#65292;&#24182;&#25506;&#35752;&#19968;&#31181;&#28041;&#21450;&#21040;&#25945;&#24072;&#39069;&#22806;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#33021;&#21542;&#20135;&#29983;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;LLM&#20998;&#37197;&#21452;&#37325;&#35282;&#33394;&#30340;&#25928;&#26524;-&#20316;&#20026;&#29983;&#25104;&#25968;&#25454;&#30340;&#8220;&#25945;&#24072;&#8221;&#65292;&#20197;&#21450;&#20316;&#20026;&#35780;&#20272;&#23398;&#29983;&#34920;&#29616;&#30340;&#8220;&#35780;&#35770;&#23478;&#8221;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#24189;&#40664;&#29983;&#25104;&#20219;&#21153;&#19978;&#30830;&#23454;&#20135;&#29983;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18113v1 Announce Type: cross  Abstract: The emergence of Large Language Models (LLMs) has brought to light promising language generation capabilities, particularly in performing tasks like complex reasoning and creative writing. Consequently, distillation through imitation of teacher responses has emerged as a popular technique to transfer knowledge from LLMs to more accessible, Small Language Models (SLMs). While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation. We hypothesize that this gap may stem from the fact that creative tasks might be hard to learn by imitation alone and explore whether an approach, involving supplementary guidance from the teacher, could yield higher performance. To address this, we study the effect of assigning a dual role to the LLM - as a "teacher" generating data, as well as a "critic" evaluating the student's performance. Our ex
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20266;&#35013;&#21644;&#37325;&#26500;&#25915;&#20987;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36234;&#29425;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25104;&#21151;&#38544;&#34255;&#26377;&#23475;&#25351;&#20196;&#24182;&#24341;&#23548;&#27169;&#22411;&#37325;&#26500;&#21407;&#25351;&#20196;&#65292;&#21462;&#24471;&#20102;90%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18104</link><description>&lt;p&gt;
&#36890;&#36807;&#20266;&#35013;&#21644;&#37325;&#26500;&#22312;&#23569;&#37327;&#26597;&#35810;&#20013;&#36234;&#29425;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18104
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20266;&#35013;&#21644;&#37325;&#26500;&#25915;&#20987;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36234;&#29425;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25104;&#21151;&#38544;&#34255;&#26377;&#23475;&#25351;&#20196;&#24182;&#24341;&#23548;&#27169;&#22411;&#37325;&#26500;&#21407;&#25351;&#20196;&#65292;&#21462;&#24471;&#20102;90%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;LLMs&#30340;&#21487;&#20449;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#20854;&#20013;&#19968;&#20010;&#29305;&#23450;&#30340;&#23041;&#32961;&#26159;&#21487;&#33021;&#29983;&#25104;&#26377;&#27602;&#25110;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#25915;&#20987;&#32773;&#21487;&#20197;&#21046;&#20316;&#26377;&#38024;&#23545;&#24615;&#30340;&#25552;&#31034;&#65292;&#35825;&#20351;LLMs&#29983;&#25104;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#22312;&#23433;&#20840;&#24494;&#35843;&#20013;&#30340;&#20559;&#35265;&#28431;&#27934;&#65292;&#24320;&#21019;&#20102;LLMs&#23433;&#20840;&#39046;&#22495;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;DRA&#65288;&#20266;&#35013;&#21644;&#37325;&#26500;&#25915;&#20987;&#65289;&#30340;&#40657;&#30418;&#36234;&#29425;&#26041;&#27861;&#65292;&#36890;&#36807;&#20266;&#35013;&#26469;&#38544;&#34255;&#26377;&#23475;&#25351;&#20196;&#65292;&#24182;&#25552;&#31034;&#27169;&#22411;&#22312;&#23436;&#25104;&#36807;&#31243;&#20013;&#37325;&#26500;&#21407;&#22987;&#26377;&#23475;&#25351;&#20196;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;DRA&#22312;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25104;&#21151;&#29575;&#21644;&#25915;&#20987;&#25928;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DRA&#22312;LLM&#32842;&#22825;&#26426;&#22120;&#20154;GPT-4&#19978;&#25317;&#26377;90\%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18104v1 Announce Type: cross  Abstract: In recent years, large language models (LLMs) have demonstrated notable success across various tasks, but the trustworthiness of LLMs is still an open problem. One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial prompts that induce harmful responses from LLMs. In this work, we pioneer a theoretical foundation in LLMs security by identifying bias vulnerabilities within the safety fine-tuning and design a black-box jailbreak method named DRA (Disguise and Reconstruction Attack), which conceals harmful instructions through disguise and prompts the model to reconstruct the original harmful instruction within its completion. We evaluate DRA across various open-source and close-source models, showcasing state-of-the-art jailbreak success rates and attack efficiency. Notably, DRA boasts a 90\% attack success rate on LLM chatbots GPT-4.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Layer-wise Scalable Adapter&#31574;&#30053;MedLaSA&#65292;&#29992;&#20110;&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#31934;&#30830;&#20462;&#25913;&#21307;&#23398;&#30693;&#35782;&#24182;&#35299;&#37322;&#20107;&#23454;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.18099</link><description>&lt;p&gt;
&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#30693;&#35782;&#21644;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18099
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Layer-wise Scalable Adapter&#31574;&#30053;MedLaSA&#65292;&#29992;&#20110;&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#31934;&#30830;&#20462;&#25913;&#21307;&#23398;&#30693;&#35782;&#24182;&#35299;&#37322;&#20107;&#23454;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26088;&#22312;&#31934;&#30830;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#29305;&#23450;&#30693;&#35782;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#19981;&#21464;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;LLMs&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#38382;&#39064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;LLMs&#22312;&#35768;&#22810;&#20851;&#38190;&#39046;&#22495;&#65288;&#20363;&#22914;&#21307;&#23398;&#39046;&#22495;&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#24187;&#35273;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20004;&#39033;&#27169;&#22411;&#32534;&#36753;&#30740;&#31350;&#65292;&#24182;&#22312;&#21307;&#23398;&#39046;&#22495;&#39564;&#35777;&#23427;&#20204;&#65306;&#65288;1&#65289;&#30452;&#25509;&#32534;&#36753;&#21307;&#23398;&#20107;&#23454;&#30693;&#35782;&#21644;&#65288;2&#65289;&#32534;&#36753;&#23545;&#20107;&#23454;&#30340;&#35299;&#37322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#30340;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MedLaSA&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#36866;&#29992;&#20110;&#21307;&#23398;&#27169;&#22411;&#32534;&#36753;&#30340;&#20998;&#23618;&#21487;&#25193;&#23637;&#36866;&#37197;&#22120;&#31574;&#30053;&#12290;&#23427;&#37319;&#29992;&#22240;&#26524;&#36861;&#36394;&#26469;&#35782;&#21035;&#31070;&#32463;&#20803;&#20013;&#30693;&#35782;&#30340;&#31934;&#30830;&#20301;&#32622;&#65292;&#28982;&#21518;&#23558;&#21487;&#25193;&#23637;&#36866;&#37197;&#22120;&#24341;&#20837;&#21040;LLMs&#30340;&#23494;&#38598;&#23618;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18099v1 Announce Type: cross  Abstract: Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged. It has been proven effective in resolving hallucination and out-of-date issues in LLMs. As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable. In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts. Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LL
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#35201;&#24615;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;KV&#32531;&#23384;&#21387;&#32553;&#20013;&#19981;&#20002;&#24323;&#20196;&#29260;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20445;&#30041;&#34987;&#39537;&#36880;KV&#23545;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#20449;&#24687;&#21487;&#20197;&#36991;&#20813;&#23433;&#20840;&#28431;&#27934;&#12289;&#24187;&#35273;&#21644;&#19978;&#19979;&#25991;&#20002;&#22833;&#12290;</title><link>https://arxiv.org/abs/2402.18096</link><description>&lt;p&gt;
&#19981;&#20002;&#24323;&#20219;&#20309;&#20196;&#29260;: &#36890;&#36807;&#37325;&#35201;&#24615;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#23454;&#29616;&#21487;&#38752;&#30340;KV&#32531;&#23384;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18096
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;KV&#32531;&#23384;&#21387;&#32553;&#20013;&#19981;&#20002;&#24323;&#20196;&#29260;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20445;&#30041;&#34987;&#39537;&#36880;KV&#23545;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#20449;&#24687;&#21487;&#20197;&#36991;&#20813;&#23433;&#20840;&#28431;&#27934;&#12289;&#24187;&#35273;&#21644;&#19978;&#19979;&#25991;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#24050;&#25104;&#20026;&#21152;&#36895;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#36895;&#24230;&#21644;&#21534;&#21520;&#37327;&#30340;&#22522;&#26412;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25209;&#37327;&#22823;&#23567;&#21644;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#38271;&#65292;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#21344;&#29992;&#25104;&#20026;LLM&#37096;&#32626;&#20013;&#30340;&#20851;&#38190;&#29942;&#39048;&#65292;&#24120;&#24120;&#36229;&#36807;&#27169;&#22411;&#26412;&#36523;&#30340;&#22823;&#23567;&#12290;&#23613;&#31649;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#36873;&#25321;&#21644;&#39537;&#36880;&#32531;&#23384;&#20013;&#30340;&#19981;&#37325;&#35201;KV&#23545;&#20197;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#65292;&#20294;&#39537;&#36880;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#28508;&#22312;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#26816;&#39564;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#32531;&#23384;&#39537;&#36880;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#30001;&#20110;KV&#23545;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#34987;&#24443;&#24213;&#20002;&#24323;&#32780;&#23548;&#33268;&#23433;&#20840;&#28431;&#27934;&#12289;&#24187;&#35273;&#21644;&#19978;&#19979;&#25991;&#20002;&#22833;&#30340;&#19981;&#33391;&#21518;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#36890;&#36807;&#38477;&#20302;&#31934;&#24230;&#20445;&#30041;&#34987;&#39537;&#36880;KV&#23545;&#20013;&#21253;&#21547;&#30340;&#19968;&#23567;&#37096;&#20998;&#20449;&#24687;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18096v1 Announce Type: cross  Abstract: Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced prec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#26694;&#26550;&#35757;&#32451;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;Polos&#65292;&#26088;&#22312;&#26377;&#25928;&#24320;&#21457;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18091</link><description>&lt;p&gt;
Polos&#65306;&#20174;&#20154;&#31867;&#21453;&#39304;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#24230;&#37327;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Polos: Multimodal Metric Learning from Human Feedback for Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18091
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#26694;&#26550;&#35757;&#32451;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;Polos&#65292;&#26088;&#22312;&#26377;&#25928;&#24320;&#21457;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#19968;&#20010;&#19982;&#20154;&#31867;&#21028;&#26029;&#32039;&#23494;&#23545;&#40784;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#23545;&#20110;&#26377;&#25928;&#24320;&#21457;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#39537;&#21160;&#25351;&#26631;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#25351;&#26631;&#22914;CIDEr&#26356;&#24378;&#30340;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#32570;&#20047;&#36275;&#22815;&#30340;&#33021;&#21147;&#26469;&#22788;&#29702;&#24187;&#35273;&#65292;&#24182;&#19988;&#36328;&#21508;&#31181;&#22270;&#20687;&#21644;&#25991;&#26412;&#27867;&#21270;&#37096;&#20998;&#26159;&#22240;&#20026;&#23427;&#20204;&#20165;&#20165;&#20351;&#29992;&#20174;&#19982;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#35780;&#20272;&#26080;&#20851;&#30340;&#20219;&#21153;&#23398;&#20064;&#30340;&#23884;&#20837;&#35745;&#31639;&#26631;&#37327;&#30456;&#20284;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Polos&#65292;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#30340;&#30417;&#30563;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#12290;Polos&#20174;&#22810;&#27169;&#24335;&#36755;&#20837;&#20013;&#35745;&#31639;&#24471;&#20998;&#65292;&#20351;&#29992;&#19968;&#20010;&#24182;&#34892;&#29305;&#24449;&#25552;&#21462;&#26426;&#21046;&#65292;&#21033;&#29992;&#36890;&#36807;&#22823;&#35268;&#27169;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#23884;&#20837;&#12290;&#20026;&#20102;&#35757;&#32451;Polos&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#22810;&#27169;&#24577;&#24230;&#37327;&#23398;&#20064;&#65288;M$^2$LHF&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18091v1 Announce Type: cross  Abstract: Establishing an automatic evaluation metric that closely aligns with human judgments is essential for effectively developing image captioning models. Recent data-driven metrics have demonstrated a stronger correlation with human judgments than classic metrics such as CIDEr; however they lack sufficient capabilities to handle hallucinations and generalize across diverse images and texts partially because they compute scalar similarities merely using embeddings learned from tasks unrelated to image captioning evaluation. In this study, we propose Polos, a supervised automatic evaluation metric for image captioning models. Polos computes scores from multimodal inputs, using a parallel feature extraction mechanism that leverages embeddings trained through large-scale contrastive learning. To train Polos, we introduce Multimodal Metric Learning from Human Feedback (M$^2$LHF), a framework for developing metrics based on human feedback. We co
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#35299;&#20915;&#26080;&#20154;&#36710;&#32676;&#20307;&#25361;&#25112;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#23545;&#20854;&#22312;&#24212;&#29992;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;</title><link>https://arxiv.org/abs/2402.18062</link><description>&lt;p&gt;
&#26080;&#20154;&#36710;&#32676;&#20307;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#25361;&#25112;&#12289;&#24212;&#29992;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Unmanned Vehicle Swarms: Challenges, Applications and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18062
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#35299;&#20915;&#26080;&#20154;&#36710;&#32676;&#20307;&#25361;&#25112;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#23545;&#20854;&#22312;&#24212;&#29992;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#26080;&#20154;&#36710;&#32676;&#20307;&#22240;&#20854;&#28508;&#22312;&#25552;&#20379;&#20154;&#31867;&#38590;&#20197;&#23436;&#25104;&#21644;&#21361;&#38505;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20174;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#33719;&#24471;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#23398;&#20064;&#21644;&#21327;&#35843;&#22823;&#37327;&#26080;&#20154;&#36710;&#30340;&#31227;&#21160;&#21644;&#34892;&#21160;&#65292;&#32473;&#20256;&#32479;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#24341;&#20837;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20197;&#20854;&#22312;&#22797;&#26434;&#25968;&#25454;&#29305;&#24449;&#25552;&#21462;&#12289;&#36716;&#25442;&#21644;&#22686;&#24378;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#35299;&#20915;&#26080;&#20154;&#36710;&#32676;&#20307;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#26080;&#20154;&#36710;&#32676;&#20307;&#20013;&#30340;&#24212;&#29992;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#26080;&#20154;&#36710;&#21644;&#26080;&#20154;&#36710;&#32676;&#20307;&#20197;&#21450;&#23427;&#20204;&#30340;&#29992;&#20363;&#21644;&#29616;&#26377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18062v1 Announce Type: cross  Abstract: With recent advances in artificial intelligence (AI) and robotics, unmanned vehicle swarms have received great attention from both academia and industry due to their potential to provide services that are difficult and dangerous to perform by humans. However, learning and coordinating movements and actions for a large number of unmanned vehicles in complex and dynamic environments introduce significant challenges to conventional AI methods. Generative AI (GAI), with its capabilities in complex data feature extraction, transformation, and enhancement, offers great potential in solving these challenges of unmanned vehicle swarms. For that, this paper aims to provide a comprehensive survey on applications, challenges, and opportunities of GAI in unmanned vehicle swarms. Specifically, we first present an overview of unmanned vehicles and unmanned vehicle swarms as well as their use cases and existing issues. Then, an in-depth background of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;Clean-LaVe&#65292;&#26088;&#22312;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#26469;&#22686;&#24378;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18061</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#20449;&#24687;&#25552;&#21462;&#20013;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#36827;&#34892;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;Clean-LaVe&#65292;&#26088;&#22312;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#26469;&#22686;&#24378;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#39046;&#22495;&#65292;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#20854;&#20182;NLP&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#25991;&#26412;&#34164;&#28085;&#65289;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;NLP&#20219;&#21153;&#30340;&#29616;&#25104;&#27169;&#22411;&#30452;&#25509;&#23545;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#22823;&#37327;&#30340;IE&#27880;&#37322;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#28508;&#22312;&#26377;&#20215;&#20540;&#30340;&#21103;&#20135;&#21697;&#26159;&#22823;&#35268;&#27169;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#65292;&#21363;&#20854;&#20182;NLP&#20219;&#21153;&#30340;&#29616;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20266;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#21033;&#29992;&#24182;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;Clean-LaVe&#65292;&#26088;&#22312;&#21033;&#29992;&#38134;&#26631;&#20934;&#25968;&#25454;&#26469;&#22686;&#24378;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;Clean-LaVe&#21253;&#25324;&#22235;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#33719;&#21462;&#38134;&#26631;&#20934;&#25968;&#25454;&#65307;&#65288;2&#65289;&#20174;&#38134;&#26631;&#20934;&#25968;&#25454;&#20013;&#35782;&#21035;&#30456;&#23545;&#24178;&#20928;&#30340;&#25968;&#25454;&#65307;&#65288;3&#65289;&#20351;&#29992;&#24178;&#20928;&#25968;&#25454;&#24494;&#35843;&#29616;&#25104;&#27169;&#22411;&#65307;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18061v1 Announce Type: cross  Abstract: The superior performance of supervised classification methods in the information extraction (IE) area heavily relies on a large amount of gold standard data. Recent zero-shot classification methods converted the task to other NLP tasks (e.g., textual entailment) and used off-the-shelf models of these NLP tasks to directly perform inference on the test data without using a large amount of IE annotation data. A potentially valuable by-product of these methods is the large-scale silver standard data, i.e., pseudo-labeled data by the off-the-shelf models of other NLP tasks. However, there is no further investigation into the use of these data. In this paper, we propose a new framework, Clean-LaVe, which aims to utilize silver standard data to enhance the zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining silver data; (2) Identifying relatively clean data from silver data; (3) Finetuning the off-the-shelf model using clea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#31867;&#22411;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.18041</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#38598;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Datasets for Large Language Models: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#31867;&#22411;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;LLMs&#30340;&#26174;&#30528;&#36827;&#23637;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25968;&#25454;&#38598;&#31867;&#20284;&#20110;&#32500;&#25345;&#21644;&#22521;&#32946;LLMs&#21457;&#23637;&#30340;&#26681;&#31995;&#22522;&#30784;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#26816;&#26597;&#36825;&#20123;&#25968;&#25454;&#38598;&#25104;&#20026;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20027;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#32570;&#20047;&#23545;LLM&#25968;&#25454;&#38598;&#20840;&#38754;&#27010;&#36848;&#21644;&#24443;&#24213;&#20998;&#26512;&#30340;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#26377;&#20851;&#23427;&#20204;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#36235;&#21183;&#30340;&#35265;&#35299;&#65292;&#26412;&#35843;&#26597;&#20174;&#20116;&#20010;&#35282;&#24230;&#25972;&#21512;&#21644;&#20998;&#31867;LLM&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#26041;&#38754;&#65306;&#65288;1&#65289;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#65307;&#65288;2&#65289;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#38598;&#65307;&#65288;3&#65289;&#20559;&#22909;&#25968;&#25454;&#38598;&#65307;&#65288;4&#65289;&#35780;&#20272;&#25968;&#25454;&#38598;&#65307;&#65288;5&#65289;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#12290;&#35843;&#26597;&#38416;&#26126;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18041v1 Announce Type: cross  Abstract: This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a compre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#37325;&#26032;&#21457;&#29616;&#22522;&#26412;&#25968;&#23398;&#27010;&#24565;&#65306;&#31215;&#20998;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18040</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#21457;&#29616;&#31215;&#20998;
&lt;/p&gt;
&lt;p&gt;
Automated Discovery of Integral with Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#37325;&#26032;&#21457;&#29616;&#22522;&#26412;&#25968;&#23398;&#27010;&#24565;&#65306;&#31215;&#20998;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#22797;&#26434;&#25968;&#23398;&#38382;&#39064;&#25110;&#35299;&#20915;&#32534;&#31243;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#35299;&#20915;&#26126;&#30830;&#23450;&#20041;&#38382;&#39064;&#30340;&#33021;&#21147;&#19982;&#36827;&#34892;&#31185;&#23398;&#21457;&#29616;&#30340;&#24494;&#22937;&#36807;&#31243;&#26377;&#30528;&#26174;&#33879;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#37325;&#26032;&#21457;&#29616;&#22522;&#26412;&#25968;&#23398;&#27010;&#24565;&#65306;&#31215;&#20998; &#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#23558;&#31215;&#20998;&#23450;&#20041;&#20026;&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#25512;&#23548;&#32473;&#23450;&#20989;&#25968;&#30340;&#31215;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18040v1 Announce Type: new  Abstract: Recent advancements in the realm of deep learning, particularly in the development of large language models (LLMs), have demonstrated AI's ability to tackle complex mathematical problems or solving programming challenges. However, the capability to solve well-defined problems based on extensive training data differs significantly from the nuanced process of making scientific discoveries. Trained on almost all human knowledge available, today's sophisticated LLMs basically learn to predict sequences of tokens. They generate mathematical derivations and write code in a similar way as writing an essay, and do not have the ability to pioneer scientific discoveries in the manner a human scientist would do.   In this study we delve into the potential of using deep learning to rediscover a fundamental mathematical concept: integrals. By defining integrals as area under the curve, we illustrate how AI can deduce the integral of a given function,
&lt;/p&gt;</description></item><item><title>ResLoRA&#25552;&#20986;&#20102;&#22312;&#35757;&#32451;&#20013;&#28155;&#21152;&#27531;&#20313;&#36335;&#24452;&#24182;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#28040;&#38500;&#36825;&#20123;&#39069;&#22806;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#27604;LoRA&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.18039</link><description>&lt;p&gt;
ResLoRA&#65306;&#20302;&#31209;&#36866;&#24212;&#20013;&#30340;&#36523;&#20221;&#27531;&#24046;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
ResLoRA: Identity Residual Mapping in Low-Rank Adaption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18039
&lt;/p&gt;
&lt;p&gt;
ResLoRA&#25552;&#20986;&#20102;&#22312;&#35757;&#32451;&#20013;&#28155;&#21152;&#27531;&#20313;&#36335;&#24452;&#24182;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#28040;&#38500;&#36825;&#20123;&#39069;&#22806;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#27604;LoRA&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26368;&#27969;&#34892;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#20043;&#19968;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#36890;&#24120;&#24212;&#29992;&#20110;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#21407;&#22987;&#27169;&#22411;&#20013;&#30001;&#20110;&#38271;&#35745;&#31639;&#36335;&#24452;&#65292;&#22312;&#26377;&#25928;&#32780;&#36805;&#36895;&#22320;&#26356;&#26032;LoRA&#22359;&#30340;&#26435;&#37325;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ResLoRA&#65292;&#36825;&#26159;LoRA&#30340;&#25913;&#36827;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#27531;&#20313;&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#21512;&#24182;&#26041;&#27861;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#28040;&#38500;&#36825;&#20123;&#39069;&#22806;&#36335;&#24452;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#35757;&#32451;&#27493;&#39588;&#20869;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#32780;&#19982;LoRA&#30456;&#27604;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25110;&#25512;&#26029;&#25104;&#26412;&#12290;&#23545; NLG&#12289;NLU &#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;ResLoRA&#26159;&#39318;&#20010;&#23558;&#27531;&#20313;&#36335;&#24452;&#19982;LoRA&#32467;&#21512;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/microsoft/LMOps/tree/main/reslora &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18039v1 Announce Type: cross  Abstract: As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs). However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA. The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method. To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA. The code of our method is available at https://github.com/microsoft/LMOps/tree/main/reslora .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#32852;&#31995;&#36215;&#26469;&#65292;&#35780;&#20272;LLMs&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.18023</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21453;&#26144;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models Mirror Cognitive Language Processing?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#31034;&#19982;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#32852;&#31995;&#36215;&#26469;&#65292;&#35780;&#20272;LLMs&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#23637;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#35768;&#22810;&#35748;&#30693;&#20219;&#21153;&#20013;&#23454;&#29616;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#30001;&#20110;LLMs&#26159;&#20174;&#20154;&#31867;&#35821;&#35328;&#35748;&#30693;&#30340;&#22823;&#37327;&#25991;&#26412;&#20135;&#20986;&#20013;&#35757;&#32451;&#20986;&#26469;&#30340;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#20250;&#38382;LLMs&#26159;&#21542;&#21453;&#26144;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#65292;&#25110;LLMs&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#31867;&#20284;&#20110;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36830;&#25509;LLMs&#34920;&#24449;&#21644;&#20154;&#31867;&#35748;&#30693;&#20449;&#21495;&#65292;&#20197;&#35780;&#20272;LLMs&#22914;&#20309;&#26377;&#25928;&#22320;&#27169;&#25311;&#35748;&#30693;&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#37319;&#29992;&#34920;&#24449;&#30456;&#20284;&#24615;&#20998;&#26512;&#65288;RSA&#65289;&#26469;&#34913;&#37327;16&#31181;&#20027;&#27969;LLMs&#19982;&#22823;&#33041;fMRI&#20449;&#21495;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#25506;&#35752;&#20102;&#21508;&#31181;&#22240;&#32032;&#65288;&#20363;&#22914;&#27169;&#22411;&#35268;&#27169;&#12289;&#23545;&#40784;&#35757;&#32451;&#12289;&#25351;&#23548;&#38468;&#21152;&#65289;&#23545;LLM-&#22823;&#33041;&#23545;&#40784;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#19982;&#27491;&#30456;&#20851;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18023v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in text comprehension and logical reasoning, achiving or even surpassing human-level performance in numerous cognition tasks. As LLMs are trained from massive textual outputs of human language cognition, it is natural to ask whether LLMs mirror cognitive language processing. Or to what extend LLMs resemble cognitive language processing? In this paper, we propose a novel method that bridge between LLM representations and human cognition signals to evaluate how effectively LLMs simulate cognitive language processing. We employ Representational Similarity Analysis (RSA) to mearsure the alignment between 16 mainstream LLMs and fMRI signals of the brain. We empirically investigate the impact of a variety of factors (e.g., model scaling, alignment training, instruction appending) on such LLM-brain alignment. Experimental results indicate that model scaling is positively cor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-Selector&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#35299;&#37322;&#65292;&#39044;&#27979;&#19981;&#21516;&#35299;&#37322;&#32452;&#21512;&#23545;&#29992;&#25143;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24341;&#23548;&#29992;&#25143;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.18016</link><description>&lt;p&gt;
&#21160;&#24577;&#35299;&#37322;&#36873;&#25321;&#65306;&#23454;&#29616;&#21487;&#35299;&#37322;AI&#30340;&#25104;&#21151;&#29992;&#25143;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Dynamic Explanation Selection Towards Successful User-Decision Support with Explainable AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-Selector&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36873;&#25321;&#35299;&#37322;&#65292;&#39044;&#27979;&#19981;&#21516;&#35299;&#37322;&#32452;&#21512;&#23545;&#29992;&#25143;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24341;&#23548;&#29992;&#25143;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22914;&#20309;&#20026;&#22522;&#20110;&#21487;&#35299;&#37322;AI&#30340;&#26234;&#33021;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;(IDSSs)&#36873;&#25321;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;IDSSs&#36890;&#36807;&#21487;&#35299;&#37322;AI&#29983;&#25104;&#30340;&#35299;&#37322;&#20197;&#21450;AI&#39044;&#27979;&#23637;&#31034;&#20102;&#25552;&#39640;&#29992;&#25143;&#20915;&#31574;&#30340;&#28508;&#21147;&#12290;&#30001;&#20110;&#21487;&#35299;&#37322;AI&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#21508;&#31181;&#35299;&#37322;&#65292;&#25105;&#20204;&#35748;&#20026;&#22914;&#26524;&#33021;&#22815;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#25351;&#23548;&#29992;&#25143;&#20570;&#20986;&#26356;&#22909;&#20915;&#31574;&#30340;&#35299;&#37322;&#65292;IDSSs &#30340;&#24615;&#33021;&#23558;&#24471;&#21040;&#26497;&#22823;&#25552;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;X-Selector&#65292;&#19968;&#31181;&#21160;&#24577;&#36873;&#25321;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;X-Selector&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#39044;&#27979;&#19981;&#21516;&#35299;&#37322;&#32452;&#21512;&#23545;&#29992;&#25143;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24341;&#23548;&#29992;&#25143;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;X-Selector&#30340;&#24615;&#33021;&#19982;&#20004;&#31181;&#26420;&#32032;&#31574;&#30053;&#65288;&#25152;&#26377;&#21487;&#33021;&#30340;&#35299;&#37322;&#21644;&#20165;&#38024;&#23545;&#26368;&#21487;&#33021;&#39044;&#27979;&#30340;&#35299;&#37322;&#65289;&#12289;&#20197;&#21450;&#20004;&#31181;&#22522;&#32447;&#26041;&#27861;&#65288;&#26080;&#35299;&#37322;&#21644;&#26080;AI&#25903;&#25345;&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;X-Selector&#26377;&#28508;&#21147;&#24341;&#23548;&#29992;&#25143;&#20570;&#20986;&#25512;&#33616;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18016v1 Announce Type: cross  Abstract: This paper addresses the problem of how to select explanations for XAI (Explainable AI)-based Intelligent Decision Support Systems (IDSSs). IDSSs have shown promise in improving user decisions through XAI-generated explanations along with AI predictions. As the development of XAI made various explanations available, we believe that IDSSs can be greatly improved if they can strategically select explanations that guide users to better decisions. This paper proposes X-Selector, a method for dynamically selecting explanations. X-Selector aims to guide users to better decisions by predicting the impact of different combinations of explanations on user decisions. We compared X-Selector's performance with two naive strategies (all possible explanations and explanations only for the most likely prediction) and two baselines (no explanation and no AI support). The results suggest the potential of X-Selector to guide users to recommended decisio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#32508;&#36848;&#20102;&#22522;&#20110;LLM&#30340;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;LLMs&#30340;&#24212;&#29992;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#23545;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#20102;&#28085;&#30422;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18013</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#26368;&#26032;&#36827;&#23637;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18013
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#32508;&#36848;&#20102;&#22522;&#20110;LLM&#30340;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;LLMs&#30340;&#24212;&#29992;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#23545;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#20102;&#28085;&#30422;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#20840;&#38754;&#22238;&#39038;&#20102;&#20851;&#20110;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#20391;&#37325;&#20110;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#12290;&#26412;&#25991;&#26088;&#22312;&#65288;a&#65289;&#24635;&#32467;&#29616;&#26377;&#30340;LLMs&#21644;&#36866;&#24212;LLMs&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#26041;&#27861;&#65307;&#65288;b&#65289;&#35814;&#32454;&#38416;&#36848;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#22522;&#20110;LLM&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#65288;ODD&#65289;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#31995;&#32479;&#65292;&#20197;&#21450;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;c&#65289;&#35752;&#35770;&#30001;&#20110;LLMs&#30340;&#21457;&#23637;&#21644;&#23545;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#19981;&#26029;&#22686;&#21152;&#30340;&#38656;&#27714;&#32780;&#20135;&#29983;&#30340;&#26410;&#26469;&#37325;&#28857;&#21644;&#26368;&#26032;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18013v1 Announce Type: cross  Abstract: This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#19978;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18012</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#32422;&#26463;&#25277;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18012
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#19978;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#38382;&#39064;&#22312;&#20998;&#26512;&#23458;&#35266;&#20989;&#25968;&#25110;&#32422;&#26463;&#19981;&#21487;&#29992;&#26102;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#26410;&#30693;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#20294;&#26377;&#38480;&#30740;&#31350;&#20851;&#27880;&#20102;&#32422;&#26463;&#26465;&#20214;&#26410;&#26126;&#30830;&#32473;&#20986;&#30340;&#24773;&#20917;&#12290;&#24573;&#30053;&#36825;&#20123;&#32422;&#26463;&#21487;&#33021;&#23548;&#33268;&#22312;&#23454;&#36341;&#20013;&#19981;&#29616;&#23454;&#30340;&#34394;&#20551;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#26410;&#30693;&#32422;&#26463;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#12290;&#20026;&#20102;&#23558;&#20248;&#21270;&#36807;&#31243;&#38480;&#21046;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#36890;&#36807;&#23458;&#35266;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#30340;&#25277;&#26679;&#38382;&#39064;&#12290;&#20026;&#20102;&#22686;&#24378;&#25277;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20197;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#39044;&#28909;&#65292;&#28982;&#21518;&#26159;Langevin&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18012v1 Announce Type: cross  Abstract: Addressing real-world optimization problems becomes particularly challenging when analytic objective functions or constraints are unavailable. While numerous studies have addressed the issue of unknown objectives, limited research has focused on scenarios where feasibility constraints are not given explicitly. Overlooking these constraints can lead to spurious solutions that are unrealistic in practice. To deal with such unknown constraints, we propose to perform optimization within the data manifold using diffusion models. To constrain the optimization process to the data manifold, we reformulate the original optimization problem as a sampling problem from the product of the Boltzmann distribution defined by the objective function and the data distribution learned by the diffusion model. To enhance sampling efficiency, we propose a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dyna
&lt;/p&gt;</description></item><item><title>Mixer&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#36890;&#36947;&#21644;&#20196;&#29260;&#20449;&#24687;&#34701;&#21512;&#65292;&#20195;&#34920;&#20102;&#20449;&#24687;&#25552;&#21462;&#33539;&#24335;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#38656;&#27714;&#21019;&#24314;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#28151;&#21512;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.18007</link><description>&lt;p&gt;
Mixer&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixer is more than just a model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18007
&lt;/p&gt;
&lt;p&gt;
Mixer&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#36890;&#36947;&#21644;&#20196;&#29260;&#20449;&#24687;&#34701;&#21512;&#65292;&#20195;&#34920;&#20102;&#20449;&#24687;&#25552;&#21462;&#33539;&#24335;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#38656;&#27714;&#21019;&#24314;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#28151;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;MLP&#32467;&#26500;&#37325;&#26032;&#21463;&#21040;&#20851;&#27880;&#65292;&#20854;&#20013;MLP-Mixer&#20197;&#20854;&#31361;&#20986;&#30340;&#34920;&#29616;&#33073;&#39062;&#32780;&#20986;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;MLP-Mixer&#20197;&#20174;&#36890;&#36947;&#21644;&#20196;&#29260;&#20004;&#20010;&#35282;&#24230;&#25552;&#21462;&#25968;&#25454;&#20449;&#24687;&#30340;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#26377;&#25928;&#22320;&#20316;&#20026;&#36890;&#36947;&#20449;&#24687;&#21644;&#20196;&#29260;&#20449;&#24687;&#30340;&#34701;&#21512;&#12290;&#20107;&#23454;&#19978;&#65292;Mixer&#20195;&#34920;&#20102;&#19968;&#31181;&#20449;&#24687;&#25552;&#21462;&#33539;&#24335;&#65292;&#23558;&#36890;&#36947;&#21644;&#20196;&#29260;&#20449;&#24687;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;Mixer&#30340;&#31934;&#39635;&#22312;&#20110;&#23427;&#33021;&#22815;&#20174;&#22810;&#20803;&#35270;&#35282;&#34701;&#21512;&#20449;&#24687;&#65292;&#20856;&#22411;&#22320;&#20307;&#29616;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#39046;&#22495;&#30340;&#8220;&#28151;&#21512;&#8221;&#30495;&#27491;&#27010;&#24565;&#12290;&#38500;&#20102;&#32771;&#34385;&#36890;&#36947;&#21644;&#20196;&#29260;&#20197;&#22806;&#65292;&#21487;&#20197;&#20174;&#21508;&#31181;&#35282;&#24230;&#21019;&#36896;&#26356;&#36148;&#21512;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#30340;&#28151;&#21512;&#22120;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#38899;&#39057;&#35782;&#21035;&#39046;&#22495;&#65292;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;&#24102;Roll-Time&#21644;Hermit FFT&#30340;&#38899;&#39057;&#39057;&#35889;&#28151;&#21512;&#22120;(ASM-RH)&#30340;&#21019;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#23545;&#26102;&#38388;&#21644;&#39057;&#29575;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18007v1 Announce Type: cross  Abstract: Recently, MLP structures have regained popularity, with MLP-Mixer standing out as a prominent example. In the field of computer vision, MLP-Mixer is noted for its ability to extract data information from both channel and token perspectives, effectively acting as a fusion of channel and token information. Indeed, Mixer represents a paradigm for information extraction that amalgamates channel and token information. The essence of Mixer lies in its ability to blend information from diverse perspectives, epitomizing the true concept of "mixing" in the realm of neural network architectures. Beyond channel and token considerations, it is possible to create more tailored mixers from various perspectives to better suit specific task requirements. This study focuses on the domain of audio recognition, introducing a novel model named Audio Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates insights from both time and freq
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#31867;&#20803;&#23457;&#38405;&#32773;&#30340;&#24773;&#24863;&#25972;&#21512;&#26694;&#26550;&#65292;&#25552;&#20986;&#35780;&#20272;&#25351;&#26631;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#65292;&#25351;&#23548;LLMs&#29983;&#25104;&#31185;&#23398;&#20803;&#23457;&#38405;&#30340;&#36923;&#36753;&#34987;&#39564;&#35777;&#21487;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.18005</link><description>&lt;p&gt;
&#25506;&#32034;&#31185;&#23398;&#24773;&#24863;&#24635;&#32467;&#30340;&#22810;&#25991;&#26723;&#20449;&#24687;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#20803;&#23457;&#38405;&#32773;&#30340;&#24773;&#24863;&#25972;&#21512;&#26694;&#26550;&#65292;&#25552;&#20986;&#35780;&#20272;&#25351;&#26631;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#65292;&#25351;&#23548;LLMs&#29983;&#25104;&#31185;&#23398;&#20803;&#23457;&#38405;&#30340;&#36923;&#36753;&#34987;&#39564;&#35777;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#20855;&#26377;&#29983;&#25104;&#22810;&#20010;&#25991;&#26723;&#30340;&#21512;&#29702;&#25688;&#35201;&#30340;&#33021;&#21147;&#65307;&#28982;&#32780;&#65292;&#29616;&#22312;&#23578;&#19981;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#20855;&#26377;&#25972;&#21512;&#20449;&#24687;&#30340;&#33021;&#21147;&#26469;&#29983;&#25104;&#24635;&#32467;&#65292;&#23588;&#20854;&#26159;&#23545;&#37027;&#20123;&#21253;&#21547;&#20010;&#20154;&#24847;&#35265;&#20449;&#24687;&#30340;&#28304;&#25991;&#26723;&#12290;&#20026;&#20102;&#20351;&#31185;&#23398;&#24773;&#24863;&#24635;&#32467;&#26356;&#21152;&#25166;&#23454;&#65292;&#25105;&#20204;&#20551;&#35774;&#22312;&#21516;&#34892;&#35780;&#23457;&#20013;&#65292;&#20154;&#31867;&#20803;&#23457;&#38405;&#32773;&#36981;&#24490;&#24773;&#24863;&#25972;&#21512;&#30340;&#19977;&#23618;&#26694;&#26550;&#26469;&#25776;&#20889;&#20803;&#23457;&#38405;&#65292;&#24182;&#19988;&#36825;&#20195;&#34920;&#20102;&#22312;&#20803;&#23457;&#38405;&#29983;&#25104;&#36807;&#31243;&#20013;&#24635;&#32467;&#31185;&#23398;&#24773;&#24863;&#30340;&#36923;&#36753;&#12290;&#36890;&#36807;&#20154;&#31867;&#27880;&#37322;&#65292;&#39564;&#35777;&#20102;&#36825;&#19968;&#26694;&#26550;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#20803;&#23457;&#38405;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24403;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;LLMs&#29983;&#25104;&#20803;&#23457;&#38405;&#30340;&#25552;&#31034;&#26102;&#65292;&#24773;&#24863;&#25972;&#21512;&#26694;&#26550;&#30340;&#20551;&#35774;&#22312;&#32463;&#39564;&#19978;&#26159;&#34892;&#24471;&#36890;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18005v1 Announce Type: cross  Abstract: Modern natural language generation systems with LLMs exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if models truly possess the ability of information consolidation to generate summaries, especially on those source documents with opinionated information. To make scientific sentiment summarization more grounded, we hypothesize that in peer review human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews and it represents the logic of summarizing scientific sentiments in meta-review generation. The framework is validated via human annotation. Based on the framework, we propose evaluation metrics to assess the quality of generated meta-reviews, and we find that the hypothesis of the sentiment consolidation framework works out empirically when we incorporate it as prompts for LLMs to generate meta-reviews in extensive experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22788;&#29702;&#26426;&#22120;&#20154;&#35013;&#37197;&#20013;&#30340;&#38646;&#20214;&#35013;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18002</link><description>&lt;p&gt;
&#32771;&#34385;&#23545;&#31216;&#24615;&#30340;&#36719;&#33109;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19979;&#26426;&#22120;&#20154;&#35013;&#37197;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial Observability with a Soft Wrist
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22788;&#29702;&#26426;&#22120;&#20154;&#35013;&#37197;&#20013;&#30340;&#38646;&#20214;&#35013;&#37197;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#36719;&#33109;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#35013;&#37197;&#20013;&#30340;&#20195;&#34920;&#24615;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23500;&#25509;&#35302;PEG-IN-HOLE&#20219;&#21153;&#65292;&#35813;&#36719;&#33109;&#21487;&#20197;&#27604;&#21018;&#24615;&#33109;&#37096;&#26356;&#23433;&#20840;&#22320;&#25805;&#20316;&#24182;&#23481;&#24525;&#36739;&#20302;&#39057;&#29575;&#30340;&#25511;&#21046;&#20449;&#21495;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#36890;&#24120;&#20351;&#29992;&#23436;&#20840;&#21487;&#35266;&#27979;&#20844;&#24335;&#19981;&#21516;&#65292;&#35813;&#20844;&#24335;&#38656;&#35201;&#22806;&#37096;&#35774;&#32622;&#25110;&#20272;&#35745;&#22120;&#26469;&#33719;&#21462;PEG-TO-HOLE&#23039;&#24577;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#20844;&#24335;&#21644;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31034;&#33539;&#26469;&#23398;&#20064;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#23436;&#20840;&#22522;&#20110;&#35302;&#35273;&#21644;&#26412;&#20307;&#24863;&#30693;&#20449;&#21495;&#34892;&#21160;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#26410;&#34701;&#21512;&#28508;&#22312;&#39046;&#22495;&#23545;&#31216;&#24615;&#65292;&#22240;&#27492;&#24517;&#39035;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#24182;&#26500;&#24314;&#36741;&#21161;&#25439;&#22833;&#26469;&#24378;&#36843;&#20195;&#29702;&#36981;&#23432;&#23545;&#31216;&#24615;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#20116;&#31181;&#19981;&#21516;&#23545;&#31216;PEG&#24418;&#29366;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20195;&#29702;&#21487;&#20197;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18002v1 Announce Type: cross  Abstract: This study tackles the representative yet challenging contact-rich peg-in-hole task of robotic assembly, using a soft wrist that can operate more safely and tolerate lower-frequency control signals than a rigid one. Previous studies often use a fully observable formulation, requiring external setups or estimators for the peg-to-hole pose. In contrast, we use a partially observable formulation and deep reinforcement learning from demonstrations to learn a memory-based agent that acts purely on haptic and proprioceptive signals. Moreover, previous works do not incorporate potential domain symmetry and thus must search for solutions in a bigger space. Instead, we propose to leverage the symmetry for sample efficiency by augmenting the training data and constructing auxiliary losses to force the agent to adhere to the symmetry. Results in simulation with five different symmetric peg shapes show that our proposed agent can be comparable to 
&lt;/p&gt;</description></item><item><title>FlattenQuant&#26041;&#27861;&#36890;&#36807;&#23637;&#24179;&#24352;&#37327;&#20013;&#30340;&#22823;&#36890;&#36947;&#65292;&#23454;&#29616;&#20102;&#20302;&#27604;&#29305;&#27599;&#24352;&#37327;&#37327;&#21270;&#65292;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;&#25439;&#22833;</title><link>https://arxiv.org/abs/2402.17985</link><description>&lt;p&gt;
FlattenQuant: &#20351;&#29992;&#20998;&#24352;&#37327;&#37327;&#21270;&#25171;&#30772;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#35745;&#31639;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17985
&lt;/p&gt;
&lt;p&gt;
FlattenQuant&#26041;&#27861;&#36890;&#36807;&#23637;&#24179;&#24352;&#37327;&#20013;&#30340;&#22823;&#36890;&#36947;&#65292;&#23454;&#29616;&#20102;&#20302;&#27604;&#29305;&#27599;&#24352;&#37327;&#37327;&#21270;&#65292;&#38477;&#20302;&#20102;&#20934;&#30830;&#24615;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#25512;&#26029;&#30340;&#24310;&#36831;&#21644;LLMs&#30340;&#22823;GPU&#20869;&#23384;&#28040;&#32791;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#37096;&#32626;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlattenQuant&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24352;&#37327;&#20013;&#30340;&#22823;&#36890;&#36947;&#36827;&#34892;&#23637;&#24179;&#26469;&#26174;&#33879;&#38477;&#20302;&#24352;&#37327;&#30340;&#26368;&#22823;&#20540;&#65292;&#23454;&#29616;&#20102;&#20302;&#27604;&#29305;&#27599;&#24352;&#37327;&#37327;&#21270;&#65292;&#20943;&#23567;&#20102;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17985v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated state-of-the-art performance across various tasks. However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance. Recently, there have been some efficient attempts to quantize LLMs, yet inference with large batch size or long sequence still has the issue of being compute-bound. Fine-grained quantization methods have showcased their proficiency in achieving low-bit quantization for LLMs, while requiring FP16 data type for linear layer computations, which is time-consuming when dealing with large batch size or long sequence. In this paper, we introduce a method called FlattenQuant, which significantly reduces the maximum value of the tensor by flattening the large channels in the tensor, to achieve low bit per-tensor quantization with minimal accuracy loss. Our experiments show that FlattenQuant can directly use 4 bits to achieve 48.29% 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#26041;&#27861;&#26694;&#26550;&#65292;&#21253;&#25324;LightGBM&#12289;XGBoost&#21644;LocalEnsemble&#27169;&#22359;&#65292;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.17979</link><description>&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#35770;&#65306;&#20351;&#29992;LightGBM&#12289;XGBoost&#21644;LocalEnsemble&#36827;&#34892;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
Ensemble Methodology:Innovations in Credit Default Prediction Using LightGBM, XGBoost, and LocalEnsemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#26041;&#27861;&#26694;&#26550;&#65292;&#21253;&#25324;LightGBM&#12289;XGBoost&#21644;LocalEnsemble&#27169;&#22359;&#65292;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28040;&#36153;&#20449;&#36151;&#39046;&#22495;&#65292;&#20934;&#30830;&#30340;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#26159;&#39118;&#38505;&#32531;&#35299;&#21644;&#36151;&#27454;&#20915;&#31574;&#20248;&#21270;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#39046;&#22495;&#30340;&#19981;&#26029;&#28436;&#21464;&#65292;&#25361;&#25112;&#20256;&#32479;&#27169;&#22411;&#65292;&#24341;&#20837;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#31215;&#32047;&#22522;&#30784;&#30740;&#31350;&#21644;&#26368;&#26032;&#21019;&#26032;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;&#20449;&#29992;&#36829;&#32422;&#39044;&#27979;&#20934;&#30830;&#24615;&#26631;&#20934;&#65292;&#20026;&#35813;&#34892;&#19994;&#35774;&#31435;&#26032;&#30340;&#22522;&#20934;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;LightGBM&#12289;XGBoost&#21644;LocalEnsemble&#27169;&#22359;&#30340;&#38598;&#25104;&#26041;&#27861;&#26694;&#26550;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#25552;&#20379;&#29420;&#29305;&#30340;&#36129;&#29486;&#65292;&#20197;&#22686;&#24378;&#22810;&#26679;&#24615;&#21644;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#30340;&#29305;&#24449;&#38598;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#35299;&#20915;&#20102;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17979v1 Announce Type: cross  Abstract: In the realm of consumer lending, accurate credit default prediction stands as a critical element in risk mitigation and lending decision optimization. Extensive research has sought continuous improvement in existing models to enhance customer experiences and ensure the sound economic functioning of lending institutions. This study responds to the evolving landscape of credit default prediction, challenging conventional models and introducing innovative approaches. By building upon foundational research and recent innovations, our work aims to redefine the standards of accuracy in credit default prediction, setting a new benchmark for the industry. To overcome these challenges, we present an Ensemble Methods framework comprising LightGBM, XGBoost, and LocalEnsemble modules, each making unique contributions to amplify diversity and improve generalization. By utilizing distinct feature sets, our methodology directly tackles limitations i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Imagine, Initialize, and Explore (IIE)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.17978</link><description>&lt;p&gt;
&#24819;&#35937;&#12289;&#21021;&#22987;&#21270;&#21644;&#25506;&#32034;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Imagine, Initialize, and Explore (IIE)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#25506;&#32034;&#23545;&#20110;&#22312;&#22797;&#26434;&#21327;&#35843;&#20219;&#21153;&#20013;&#21457;&#29616;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#26368;&#20339;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#20869;&#22312;&#22870;&#21169;&#26469;&#23454;&#29616;&#25215;&#35834;&#30340;&#25506;&#32034;&#65292;&#25110;&#32773;&#20351;&#29992;&#22522;&#20110;&#35282;&#33394;&#30340;&#23398;&#20064;&#26469;&#20998;&#35299;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#22312;&#25972;&#20010;&#21160;&#20316;-&#35266;&#23519;&#31354;&#38388;&#20013;&#36827;&#34892;&#38598;&#20307;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#20013;&#65292;&#20182;&#20204;&#24448;&#24448;&#38754;&#20020;&#33719;&#21462;&#29305;&#23450;&#32852;&#21512;&#21160;&#20316;&#24207;&#21015;&#20197;&#36798;&#21040;&#25104;&#21151;&#29366;&#24577;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Imagine, Initialize, and Explore (IIE)&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#22810;&#26234;&#33021;&#20307;&#25506;&#32034;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;IIE&#21033;&#29992;&#19968;&#20010;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#24819;&#35937;&#26234;&#33021;&#20307;&#22914;&#20309;&#36798;&#21040;&#21487;&#20197;&#24433;&#21709;&#24444;&#27492;&#36716;&#31227;&#20989;&#25968;&#30340;&#20851;&#38190;&#29366;&#24577;&#12290;&#28982;&#21518;&#65292;&#22312;&#25506;&#32034;&#38454;&#27573;&#20043;&#21069;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#22120;&#22312;&#36825;&#20010;&#29366;&#24577;&#19979;&#21021;&#22987;&#21270;&#29615;&#22659;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#23454;&#29616;&#36825;&#31181;&#24819;&#35937;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17978v1 Announce Type: cross  Abstract: Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imag
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#24863;&#30693;&#22870;&#21169;&#20989;&#25968;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#21482;&#38656;50&#20010;&#20559;&#22909;&#26631;&#31614;&#21363;&#21487;&#36798;&#21040;&#19982;&#20256;&#32479;&#26041;&#27861;500&#20010;&#20559;&#22909;&#26631;&#31614;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#26356;&#22909;&#22320;&#24674;&#22797;&#22320;&#38754;&#30495;&#20540;&#22870;&#21169;&#31574;&#30053;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17975</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#26377;&#25928;&#24615;&#21450;&#21160;&#24577;&#24863;&#30693;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17975
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#24863;&#30693;&#22870;&#21169;&#20989;&#25968;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#21482;&#38656;50&#20010;&#20559;&#22909;&#26631;&#31614;&#21363;&#21487;&#36798;&#21040;&#19982;&#20256;&#32479;&#26041;&#27861;500&#20010;&#20559;&#22909;&#26631;&#31614;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#26356;&#22909;&#22320;&#24674;&#22797;&#22320;&#38754;&#30495;&#20540;&#22870;&#21169;&#31574;&#30053;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#36890;&#36807;&#20174;&#23545;&#20195;&#29702;&#34892;&#20026;&#30340;&#20108;&#36827;&#21046;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#23558;&#26426;&#22120;&#20154;&#34892;&#20026;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21160;&#24577;&#24863;&#30693;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#23558;PbRL&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#23398;&#20064;&#21160;&#24577;&#24863;&#30693;&#29366;&#24577;-&#21160;&#20316;&#34920;&#31034;&#65288;z^{sa&#65289;&#65289;&#21644;&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#36845;&#20195;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#21487;&#20197;&#21152;&#24555;&#31574;&#30053;&#23398;&#20064;&#24182;&#25552;&#39640;&#26368;&#32456;&#31574;&#30053;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#22312;&#22235;&#36275;&#34892;&#36208;&#12289;&#27493;&#34892;&#21644;&#29454;&#35961;&#22868;&#36305;&#31561;&#39046;&#22495;&#65292;&#20351;&#29992;50&#20010;&#20559;&#22909;&#26631;&#31614;&#30340;&#24615;&#33021;&#19982;&#20351;&#29992;500&#20010;&#20559;&#22909;&#26631;&#31614;&#30340;&#29616;&#26377;&#26041;&#27861;&#30456;&#21516;&#65292;&#24182;&#19988;&#25105;&#20204;&#24674;&#22797;&#20102;83\%&#21644;66\%&#30340;&#22320;&#38754;&#30495;&#20540;&#22870;&#21169;&#31574;&#30053;&#24615;&#33021;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#21482;&#26377;38\%&#21644;21\%&#12290;&#36825;&#20123;&#24615;&#33021;&#25552;&#21319;&#23637;&#31034;&#20102;&#26126;&#30830;&#23398;&#20064;&#21160;&#24577;&#24863;&#30693;&#22870;&#21169;&#27169;&#22411;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17975v1 Announce Type: new  Abstract: Preference-based reinforcement learning (PbRL) aligns a robot behavior with human preferences via a reward function learned from binary feedback over agent behaviors. We show that dynamics-aware reward functions improve the sample efficiency of PbRL by an order of magnitude. In our experiments we iterate between: (1) learning a dynamics-aware state-action representation (z^{sa}) via a self-supervised temporal consistency task, and (2) bootstrapping the preference-based reward function from (z^{sa}), which results in faster policy learning and better final policy performance. For example, on quadruped-walk, walker-walk, and cheetah-run, with 50 preference labels we achieve the same performance as existing approaches with 500 preference labels, and we recover 83\% and 66\% of ground truth reward policy performance versus only 38\% and 21\%. The performance gains demonstrate the benefits of explicitly learning a dynamics-aware reward model.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#29255;&#20869;&#23398;&#20064;&#65288;I$^2$L&#65289;&#30340;&#26032;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#26426;&#21046;&#65292;&#23558;&#28436;&#31034;&#31034;&#20363;&#12289;&#35270;&#35273;&#32447;&#32034;&#21644;&#25351;&#20196;&#21512;&#24182;&#21040;&#19968;&#20010;&#22270;&#29255;&#20013;&#65292;&#20197;&#25552;&#21319;GPT-4V&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#22270;&#20687;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#26469;&#21462;&#24471;&#22810;&#20010;&#20248;&#28857;</title><link>https://arxiv.org/abs/2402.17971</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#29255;&#25630;&#23450;&#65306;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#22270;&#29255;&#20869;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
All in a Single Image: Large Multimodal Models are In-Image Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17971
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#29255;&#20869;&#23398;&#20064;&#65288;I$^2$L&#65289;&#30340;&#26032;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#26426;&#21046;&#65292;&#23558;&#28436;&#31034;&#31034;&#20363;&#12289;&#35270;&#35273;&#32447;&#32034;&#21644;&#25351;&#20196;&#21512;&#24182;&#21040;&#19968;&#20010;&#22270;&#29255;&#20013;&#65292;&#20197;&#25552;&#21319;GPT-4V&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#22270;&#20687;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#26469;&#21462;&#24471;&#22810;&#20010;&#20248;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#29255;&#20869;&#23398;&#20064;&#65288;I$^2$L&#65289;&#30340;&#26032;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26426;&#21046;&#65292;&#23558;&#28436;&#31034;&#31034;&#20363;&#12289;&#35270;&#35273;&#32447;&#32034;&#21644;&#25351;&#20196;&#21512;&#24182;&#21040;&#19968;&#24352;&#22270;&#29255;&#20013;&#65292;&#20197;&#22686;&#24378;GPT-4V&#30340;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#20381;&#36182;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#25110;&#23558;&#35270;&#35273;&#36755;&#20837;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;I$^2$L&#23558;&#25152;&#26377;&#20449;&#24687;&#25972;&#21512;&#21040;&#19968;&#24352;&#22270;&#29255;&#20013;&#65292;&#20027;&#35201;&#21033;&#29992;&#22270;&#20687;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#36991;&#20813;&#20102;&#23545;&#22797;&#26434;&#22270;&#20687;&#30340;&#19981;&#20934;&#30830;&#25991;&#26412;&#25551;&#36848;&#65292;&#25552;&#20379;&#20102;&#22312;&#23450;&#20301;&#28436;&#31034;&#31034;&#20363;&#26102;&#30340;&#28789;&#27963;&#24615;&#65292;&#20943;&#23569;&#20102;&#36755;&#20837;&#36127;&#25285;&#65292;&#24182;&#36890;&#36807;&#28040;&#38500;&#23545;&#22810;&#20010;&#22270;&#29255;&#21644;&#20887;&#38271;&#25991;&#26412;&#30340;&#38656;&#27714;&#26469;&#36991;&#20813;&#36229;&#36807;&#36755;&#20837;&#38480;&#21046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#32467;&#21512;&#19981;&#21516;ICL&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#31574;&#30053;&#65292;&#29992;&#20110;&#36873;&#25321;&#32473;&#23450;&#20219;&#21153;&#20013;&#25968;&#25454;&#31034;&#20363;&#30340;&#36866;&#24403;ICL&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;MathVi&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17971v1 Announce Type: cross  Abstract: This paper introduces a new in-context learning (ICL) mechanism called In-Image Learning (I$^2$L) that combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of GPT-4V. Unlike previous approaches that rely on converting images to text or incorporating visual input into language models, I$^2$L consolidates all information into one image and primarily leverages image processing, understanding, and reasoning abilities. This has several advantages: it avoids inaccurate textual descriptions of complex images, provides flexibility in positioning demonstration examples, reduces the input burden, and avoids exceeding input limits by eliminating the need for multiple images and lengthy text. To further combine the strengths of different ICL methods, we introduce an automatic strategy to select the appropriate ICL method for a data example in a given task. We conducted experiments on MathVi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#21644;&#32452;&#32455;&#35270;&#35273;&#19978;&#19979;&#25991;&#26469;&#26367;&#20195;&#20154;&#24037;&#21442;&#32771;&#25991;&#29486;&#65292;&#20174;&#32780;&#25552;&#21319;&#22270;&#20687;&#26631;&#39064;&#30340;&#35780;&#20272;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17969</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;&#19978;&#19979;&#25991;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17969
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#21644;&#32452;&#32455;&#35270;&#35273;&#19978;&#19979;&#25991;&#26469;&#26367;&#20195;&#20154;&#24037;&#21442;&#32771;&#25991;&#29486;&#65292;&#20174;&#32780;&#25552;&#21319;&#22270;&#20687;&#26631;&#39064;&#30340;&#35780;&#20272;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#30340;&#21152;&#36895;&#36827;&#23637;&#65292;&#20934;&#30830;&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#30340;&#22270;&#20687;&#26631;&#39064;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#26356;&#36148;&#36817;&#20154;&#31867;&#20559;&#22909;&#22320;&#35780;&#20272;&#26631;&#39064;&#65292;&#24230;&#37327;&#26631;&#20934;&#38656;&#35201;&#21306;&#20998;&#19981;&#21516;&#36136;&#37327;&#21644;&#20869;&#23481;&#30340;&#26631;&#39064;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#24230;&#37327;&#26631;&#20934;&#20165;&#38480;&#20110;&#27604;&#36739;&#21333;&#35789;&#25110;&#23884;&#20837;&#30456;&#20284;&#24615;&#30340;&#34920;&#38754;&#21305;&#37197;&#65292;&#22240;&#27492;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#39064;&#35780;&#20272;&#26041;&#27861;VisCE$^2$&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#35270;&#35273;&#19978;&#19979;&#25991;&#65292;&#21363;&#22270;&#20687;&#30340;&#35814;&#32454;&#20869;&#23481;&#65292;&#21253;&#25324;&#23545;&#35937;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#12290;&#36890;&#36807;&#25552;&#21462;&#24182;&#32452;&#32455;&#25104;&#32467;&#26500;&#21270;&#26684;&#24335;&#65292;&#25105;&#20204;&#29992;&#35270;&#35273;&#19978;&#19979;&#25991;&#26367;&#25442;&#20154;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25991;&#29486;&#65292;&#24110;&#21161;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22270;&#20687;&#65292;&#25552;&#21319;&#35780;&#20272;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#20803;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;VisCE$^2$&#20248;&#20110;&#20256;&#32113;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17969v1 Announce Type: cross  Abstract: Given the accelerating progress of vision and language modeling, accurate evaluation of machine-generated image captions remains critical. In order to evaluate captions more closely to human preferences, metrics need to discriminate between captions of varying quality and content. However, conventional metrics fail short of comparing beyond superficial matches of words or embedding similarities; thus, they still need improvement. This paper presents VisCE$^2$, a vision language model-based caption evaluation method. Our method focuses on visual context, which refers to the detailed content of images, including objects, attributes, and relationships. By extracting and organizing them into a structured format, we replace the human-written references with visual contexts and help VLMs better understand the image, enhancing evaluation performance. Through meta-evaluation on multiple datasets, we validated that VisCE$^2$ outperforms the con
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLix&#30340;&#26032;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#35843;&#25972;&#65292;&#36890;&#36807;&#20851;&#32852;&#27599;&#20010;&#29420;&#29305;&#25968;&#25454;&#38598;&#29305;&#24449;&#19982;&#20854;&#20302;&#31209;&#26435;&#37325;&#26356;&#26032;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17934</link><description>&lt;p&gt;
&#29992;&#29305;&#24449;&#21270;&#20302;&#31209;&#28151;&#21512;&#36827;&#34892;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17934
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLix&#30340;&#26032;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#35843;&#25972;&#65292;&#36890;&#36807;&#20851;&#32852;&#27599;&#20010;&#29420;&#29305;&#25968;&#25454;&#38598;&#29305;&#24449;&#19982;&#20854;&#20302;&#31209;&#26435;&#37325;&#26356;&#26032;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#25968;&#21313;&#29978;&#33267;&#25968;&#30334;&#31181;&#20154;&#31867;&#35821;&#35328;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#36890;&#36807;&#21482;&#35843;&#25972;&#23569;&#37327;&#21442;&#25968;&#26174;&#33879;&#20943;&#23569;&#20102;&#36866;&#24212;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#20687; LoRA&#65288;Hu &#31561;&#20154;&#65292;2022&#65289;&#36825;&#26679;&#30340; PEFT &#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#28151;&#21512;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#65292;&#21407;&#22240;&#22312;&#20110;&#26377;&#38480;&#30340;&#21442;&#25968;&#23481;&#37327;&#21644;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36127;&#38754;&#20114;&#30456;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#21270;&#20302;&#31209;&#28151;&#21512;&#65288;FLix&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#26377;&#25928;&#30340;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#35843;&#25972;&#30340;&#26032;&#22411; PEFT &#26041;&#27861;&#12290;FLix&#23558;&#27599;&#20010;&#29420;&#29305;&#25968;&#25454;&#38598;&#29305;&#24449;&#65288;&#20363;&#22914;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#25110;&#20219;&#21153;&#65289;&#19982;&#20854;&#33258;&#24049;&#30340;&#20302;&#31209;&#26435;&#37325;&#26356;&#26032;&#21442;&#25968;&#30456;&#20851;&#32852;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#32452;&#21512;&#29305;&#23450;&#20110;&#29305;&#24449;&#30340;&#21442;&#25968;&#65292;FLix&#33021;&#22815;&#36866;&#24212;&#22810;&#31181;&#25968;&#25454;&#38598;&#28151;&#21512;&#65292;&#24182;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FLix &#21487;&#20197;&#22312;&#25552;&#20379;&#26356;&#22909;&#24615;&#33021;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#36866;&#24212;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17934v1 Announce Type: cross  Abstract: Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive. Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation cost, by tuning only a small amount of parameters. However, directly applying PEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could lead to suboptimal performance due to limited parameter capacity and negative interference among different datasets. In this work, we propose Featurized Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask multilingual tuning. FLix associates each unique dataset feature, such as the dataset's language or task, with its own low-rank weight update parameters. By composing feature-specific parameters for each dataset, FLix can accommodate diverse dataset mixtures and generalize better to unseen datasets. Our experiments show that FLix leads t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#24341;&#23548;&#36870;&#21521;&#35745;&#21010;&#25628;&#32034;&#65288;CLIPS&#65289;&#30340;&#36125;&#21494;&#26031;&#20195;&#29702;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#29616;&#23454;&#29992;&#25351;&#20196;&#36319;&#38543;&#21644;&#30446;&#26631;&#36741;&#21161;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25351;&#20196;&#30340;&#21487;&#33021;&#24615;&#20197;&#23454;&#29616;&#23454;&#29992;&#30446;&#26631;&#36798;&#25104;&#25104;&#26412;&#26368;&#23567;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.17930</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#35821;&#35328;&#24341;&#23548;&#36870;&#21521;&#35268;&#21010;&#23454;&#29616;&#23454;&#29992;&#25351;&#20196;&#36319;&#38543;&#21644;&#30446;&#26631;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#24341;&#23548;&#36870;&#21521;&#35745;&#21010;&#25628;&#32034;&#65288;CLIPS&#65289;&#30340;&#36125;&#21494;&#26031;&#20195;&#29702;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#29616;&#23454;&#29992;&#25351;&#20196;&#36319;&#38543;&#21644;&#30446;&#26631;&#36741;&#21161;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25351;&#20196;&#30340;&#21487;&#33021;&#24615;&#20197;&#23454;&#29616;&#23454;&#29992;&#30446;&#26631;&#36798;&#25104;&#25104;&#26412;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#32463;&#24120;&#32473;&#20986;&#22312;&#32570;&#20047;&#36827;&#19968;&#27493;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#24847;&#20041;&#27169;&#31946;&#30340;&#25351;&#20196;&#65292;&#26399;&#26395;&#20182;&#20204;&#30340;&#34892;&#21160;&#25110;&#30446;&#26631;&#33021;&#28040;&#38500;&#19981;&#26126;&#30830;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#22914;&#20309;&#26500;&#24314;&#33021;&#22815;&#20197;&#28789;&#27963;&#12289;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26041;&#24335;&#36981;&#24490;&#36825;&#31867;&#25351;&#20196;&#30340;&#36741;&#21161;&#20195;&#29702;&#21602;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#24341;&#23548;&#36870;&#21521;&#35745;&#21010;&#25628;&#32034;&#65288;CLIPS&#65289;&#30340;&#36125;&#21494;&#26031;&#20195;&#29702;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#29616;&#23454;&#29992;&#25351;&#20196;&#36319;&#38543;&#21644;&#30446;&#26631;&#36741;&#21161;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#36890;&#36807;&#23558;&#20154;&#31867;&#24314;&#27169;&#20026;&#19968;&#20010;&#21512;&#20316;&#35268;&#21010;&#32773;&#65292;&#23558;&#20849;&#21516;&#35745;&#21010;&#19982;&#21161;&#25163;&#36827;&#34892;&#36890;&#20449;&#65292;&#28982;&#21518;&#36890;&#36807;&#21160;&#20316;&#21644;&#35821;&#35328;&#25191;&#34892;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#22312;&#20551;&#35774;&#35745;&#21010;&#19979;&#32473;&#20986;&#30340;&#25351;&#20196;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#33719;&#24471;&#36825;&#19968;&#21518;&#39564;&#20998;&#24067;&#21518;&#65292;&#25105;&#20204;&#30340;&#21161;&#25163;&#36890;&#36807;&#34892;&#21160;&#26469;&#26368;&#23567;&#21270;&#26399;&#26395;&#30446;&#26631;&#23454;&#29616;&#25104;&#26412;&#65292;&#20351;&#20854;&#33021;&#22815;&#23454;&#29992;&#22320;&#36981;&#24490;&#21547;&#31946;&#30340;&#25351;&#20196;&#65292;&#24182;&#21363;&#20351;&#22312;&#23545;&#25351;&#20196;&#19981;&#30830;&#23450;&#26102;&#20063;&#33021;&#25552;&#20379;&#26377;&#25928;&#30340;&#36741;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17930v1 Announce Type: new  Abstract: People often give instructions whose meaning is ambiguous without further context, expecting that their actions or goals will disambiguate their intentions. How can we build assistive agents that follow such instructions in a flexible, context-sensitive manner? This paper introduces cooperative language-guided inverse plan search (CLIPS), a Bayesian agent architecture for pragmatic instruction following and goal assistance. Our agent assists a human by modeling them as a cooperative planner who communicates joint plans to the assistant, then performs multimodal Bayesian inference over the human's goal from actions and language, using large language models (LLMs) to evaluate the likelihood of an instruction given a hypothesized plan. Given this posterior, our assistant acts to minimize expected goal achievement cost, enabling it to pragmatically follow ambiguous instructions and provide effective assistance even when uncertain about the g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20445;&#30041;&#21407;&#38382;&#39064;&#32467;&#26500;&#38590;&#24230;&#20294;&#38024;&#23545;LLMs&#26080;&#35299;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;LLMs&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17916</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#23454;&#29616;&#25239;LLM&#30340;&#25968;&#23398;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLM-Resistant Math Word Problem Generation via Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20445;&#30041;&#21407;&#38382;&#39064;&#32467;&#26500;&#38590;&#24230;&#20294;&#38024;&#23545;LLMs&#26080;&#35299;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;LLMs&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25913;&#21464;&#20102;&#25945;&#32946;&#39046;&#22495;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#35780;&#20272;&#65292;&#36825;&#20123;&#31034;&#20363;&#20445;&#30041;&#20102;&#21407;&#22987;&#38382;&#39064;&#30340;&#32467;&#26500;&#21644;&#38590;&#24230;&#65292;&#20294;LLMs&#26080;&#27861;&#35299;&#20915;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25968;&#23398;&#24212;&#29992;&#39046;&#22495;&#30340;&#35789;&#38382;&#39064;&#65292;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#32467;&#26500;&#29983;&#25104;&#23545;&#25239;&#31034;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#32534;&#36753;&#38382;&#39064;&#20013;&#30340;&#25968;&#23383;&#20540;&#65292;&#23548;&#33268;LLMs&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17916v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs. In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs. Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#35789;&#27719;&#29305;&#24449;&#65292;&#25104;&#21151;&#35782;&#21035;&#20102;&#26377;&#21161;&#20110;&#26041;&#35328;&#21464;&#21270;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#23450;&#35789;&#27719;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.17914</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#35789;&#27719;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Extracting Lexical Features from Dialects via Interpretable Dialect Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17914
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#35789;&#27719;&#29305;&#24449;&#65292;&#25104;&#21151;&#35782;&#21035;&#20102;&#26377;&#21161;&#20110;&#26041;&#35328;&#21464;&#21270;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#23450;&#35789;&#27719;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#19968;&#31181;&#35821;&#35328;&#30340;&#26041;&#35328;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#36890;&#24120;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#21644;&#32454;&#33268;&#30340;&#20154;&#31867;&#20998;&#26512;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#30740;&#31350;&#21508;&#31181;&#26041;&#35328;&#28041;&#21450;&#21040;&#22797;&#26434;&#24615;&#21644;&#24494;&#22937;&#20043;&#22788;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#21306;&#20998;&#24615;&#35789;&#27719;&#29305;&#24449;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20154;&#31867;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20107;&#21518;&#21644;&#20869;&#22312;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23545;&#26222;&#36890;&#35805;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#20302;&#22320;&#33832;&#20811;&#26862;&#35821;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#26377;&#21161;&#20110;&#26041;&#35328;&#21464;&#21270;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#23450;&#35789;&#27719;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17914v1 Announce Type: cross  Abstract: Identifying linguistic differences between dialects of a language often requires expert knowledge and meticulous human analysis. This is largely due to the complexity and nuance involved in studying various dialects. We present a novel approach to extract distinguishing lexical features of dialects by utilizing interpretable dialect classifiers, even in the absence of human experts. We explore both post-hoc and intrinsic approaches to interpretability, conduct experiments on Mandarin, Italian, and Low Saxon, and experimentally demonstrate that our method successfully identifies key language-specific lexical features that contribute to dialectal variations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;AI&#36719;&#20214;&#21644;&#30828;&#20214;&#24212;&#29992;&#20110;&#25968;&#20540;&#24314;&#27169;&#39046;&#22495;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;AI&#26041;&#27861;&#65292;&#22914;CNN&#65292;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26631;&#20934;&#25805;&#20316;&#65292;&#24102;&#26469;&#39640;&#24615;&#33021;&#12289;&#26550;&#26500;&#19981;&#21487;&#30693;&#24615;&#21644;&#26131;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17913</link><description>&lt;p&gt;
&#20351;&#29992;AI&#24211;&#36827;&#34892;&#19981;&#21487;&#21387;&#32553;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Using AI libraries for Incompressible Computational Fluid Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;AI&#36719;&#20214;&#21644;&#30828;&#20214;&#24212;&#29992;&#20110;&#25968;&#20540;&#24314;&#27169;&#39046;&#22495;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;AI&#26041;&#27861;&#65292;&#22914;CNN&#65292;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26631;&#20934;&#25805;&#20316;&#65292;&#24102;&#26469;&#39640;&#24615;&#33021;&#12289;&#26550;&#26500;&#19981;&#21487;&#30693;&#24615;&#21644;&#26131;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#39640;&#25928;&#24320;&#28304;&#24211;&#65292;&#20197;&#22312;&#19981;&#21516;&#30340;&#35745;&#31639;&#26426;&#26550;&#26500;&#65288;&#20363;&#22914;CPU&#12289;GPU&#21644;&#26032;&#30340;AI&#22788;&#29702;&#22120;&#65289;&#19978;&#25191;&#34892;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30456;&#20851;&#30340;&#35745;&#31639;&#12290;&#36825;&#19981;&#20165;&#20351;&#22522;&#20110;&#36825;&#20123;&#24211;&#30340;&#31639;&#27861;&#39640;&#25928;&#32780;&#19988;&#22312;&#19981;&#21516;&#26550;&#26500;&#20043;&#38388;&#21487;&#31227;&#26893;&#65292;&#36824;&#22823;&#22823;&#31616;&#21270;&#20102;&#20351;&#29992;AI&#24320;&#21457;&#26041;&#27861;&#30340;&#38376;&#27099;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#35770;&#65292;&#23558;AI&#36719;&#20214;&#21644;&#30828;&#20214;&#30340;&#24378;&#22823;&#21151;&#33021;&#24102;&#20837;&#25968;&#20540;&#24314;&#27169;&#39046;&#22495;&#65292;&#23558;AI&#26041;&#27861;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;CNN&#65289;&#37325;&#26032;&#29992;&#20110;&#25968;&#20540;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26631;&#20934;&#25805;&#20316;&#12290;&#26412;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#23558;&#39640;&#24615;&#33021;&#12289;&#26550;&#26500;&#19981;&#21487;&#30693;&#24615;&#21644;&#26131;&#29992;&#24615;&#24341;&#20837;&#25968;&#20540;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#20915;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17913v1 Announce Type: cross  Abstract: Recently, there has been a huge effort focused on developing highly efficient open source libraries to perform Artificial Intelligence (AI) related computations on different computer architectures (for example, CPUs, GPUs and new AI processors). This has not only made the algorithms based on these libraries highly efficient and portable between different architectures, but also has substantially simplified the entry barrier to develop methods using AI. Here, we present a novel methodology to bring the power of both AI software and hardware into the field of numerical modelling by repurposing AI methods, such as Convolutional Neural Networks (CNNs), for the standard operations required in the field of the numerical solution of Partial Differential Equations (PDEs). The aim of this work is to bring the high performance, architecture agnosticism and ease of use into the field of the numerical solution of PDEs. We use the proposed methodol
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#30740;&#31350;&#24615;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#38750;&#20107;&#23454;&#22411;&#12289;&#22810;&#36879;&#35270;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#25361;&#25112;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17896</link><description>&lt;p&gt;
&#30740;&#31350;&#24615;&#38382;&#39064;&#65306;LLM&#32593;&#32476;&#29305;&#24037;&#30340;&#22810;&#36879;&#35270;&#12289;&#20998;&#35299;&#38382;&#39064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17896
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#30740;&#31350;&#24615;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#38750;&#20107;&#23454;&#22411;&#12289;&#22810;&#36879;&#35270;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#25361;&#25112;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#23545;&#20110;&#22823;&#22810;&#25968;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35828;&#19981;&#20877;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#30340;QA&#22522;&#20934;&#22914;TriviaQA&#12289;NaturalQuestions&#12289;ELI5&#21644;HotpotQA&#20027;&#35201;&#30740;&#31350;&#26126;&#30830;&#25351;&#31034;&#20102;&#32570;&#23569;&#21738;&#20123;&#20449;&#24687;&#20197;&#21450;&#22914;&#20309;&#25214;&#21040;&#36825;&#20123;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#30340;&#8220;&#24050;&#30693;&#26410;&#30693;s&#8221;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20123;&#22522;&#20934;&#30340;&#20248;&#31168;&#34920;&#29616;&#25552;&#20379;&#20102;&#19968;&#31181;&#34394;&#20551;&#30340;&#23433;&#20840;&#24863;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#23578;&#26410;&#28385;&#36275;&#30340;&#38656;&#27714;&#26159;&#19968;&#20010;&#38750;&#20107;&#23454;&#22411;&#12289;&#22810;&#36879;&#35270;&#38382;&#39064;&#30340;&#38134;&#34892;&#65292;&#28041;&#21450;&#22823;&#37327;&#19981;&#26126;&#30830;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#21363;&#8220;&#26410;&#30693;&#30340;&#26410;&#30693;s&#8221;&#12290;&#25105;&#20204;&#22768;&#31216;&#21487;&#20197;&#22312;&#25628;&#32034;&#24341;&#25806;&#26085;&#24535;&#20013;&#25214;&#21040;&#36825;&#26679;&#30340;&#38382;&#39064;&#65292;&#36825;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#38382;&#31572;&#24847;&#22270;&#26597;&#35810;&#23454;&#38469;&#19978;&#26159;&#20107;&#23454;&#22411;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Researchy Questions&#65292;&#19968;&#20010;&#32463;&#36807;&#32321;&#29712;&#36807;&#28388;&#20197;&#21464;&#20026;&#38750;&#20107;&#23454;&#22411;&#12289;&#8220;&#20998;&#35299;&#24335;&#8221;&#21644;&#22810;&#36879;&#35270;&#30340;&#25628;&#32034;&#24341;&#25806;&#26597;&#35810;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#25143;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#25237;&#20837;&#20102;&#22823;&#37327;&#8220;&#21162;&#21147;&#8221;&#65292;&#36825;&#31181;&#21162;&#21147;&#34920;&#29616;&#20026;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17896v1 Announce Type: cross  Abstract: Existing question answering (QA) datasets are no longer challenging to most powerful Large Language Models (LLMs). Traditional QA benchmarks like TriviaQA, NaturalQuestions, ELI5 and HotpotQA mainly study ``known unknowns'' with clear indications of both what information is missing, and how to find it to answer the question. Hence, good performance on these benchmarks provides a false sense of security. A yet unmet need of the NLP community is a bank of non-factoid, multi-perspective questions involving a great deal of unclear information needs, i.e. ``unknown uknowns''. We claim we can find such questions in search engine logs, which is surprising because most question-intent queries are indeed factoid. We present Researchy Questions, a dataset of search engine queries tediously filtered to be non-factoid, ``decompositional'' and multi-perspective. We show that users spend a lot of ``effort'' on these questions in terms of signals lik
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#22522;&#20110;Bregman&#25955;&#24230;&#65292;&#36890;&#36807;&#24341;&#20837;&#20849;&#36717;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;\textsc{ConjNorm}&#26041;&#27861;&#65292;&#20197;&#22312;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#25628;&#32034;&#26368;&#20339;&#35268;&#33539;&#31995;&#25968;$p$&#26469;&#37325;&#26032;&#26500;&#24819;&#23494;&#24230;&#20989;&#25968;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.17888</link><description>&lt;p&gt;
ConjNorm&#65306;&#29992;&#20110;&#24322;&#24120;&#20998;&#24067;&#26816;&#27979;&#30340;&#21487;&#22788;&#29702;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17888
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#22522;&#20110;Bregman&#25955;&#24230;&#65292;&#36890;&#36807;&#24341;&#20837;&#20849;&#36717;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;\textsc{ConjNorm}&#26041;&#27861;&#65292;&#20197;&#22312;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#25628;&#32034;&#26368;&#20339;&#35268;&#33539;&#31995;&#25968;$p$&#26469;&#37325;&#26032;&#26500;&#24819;&#23494;&#24230;&#20989;&#25968;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#32493;&#24322;&#24120;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#22312;&#21487;&#38752;&#26426;&#22120;&#23398;&#20064;&#20013;&#21463;&#21040;&#23494;&#20999;&#20851;&#27880;&#12290;&#35768;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;&#25512;&#23548;&#22522;&#20110;logits&#12289;&#36317;&#31163;&#25110;&#20005;&#26684;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#20197;&#35782;&#21035;&#24471;&#20998;&#20302;&#30340;OOD&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20272;&#35745;&#24471;&#20998;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#30495;&#23454;&#25968;&#25454;&#23494;&#24230;&#25110;&#26045;&#21152;&#19981;&#20999;&#23454;&#38469;&#30340;&#32422;&#26463;&#12290;&#20026;&#20102;&#22312;&#22522;&#20110;&#23494;&#24230;&#24471;&#20998;&#35774;&#35745;&#26041;&#38754;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;Bregman&#25955;&#24230;&#20026;&#22522;&#30784;&#30340;&#26032;&#39062;&#29702;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#20998;&#24067;&#32771;&#34385;&#25193;&#23637;&#21040;&#28085;&#30422;&#19968;&#31995;&#21015;&#25351;&#25968;&#26063;&#20998;&#24067;&#12290;&#21033;&#29992;&#25105;&#20204;&#23450;&#29702;&#20013;&#25581;&#31034;&#30340;&#20849;&#36717;&#32422;&#26463;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;\textsc{ConjNorm}&#26041;&#27861;&#65292;&#23558;&#23494;&#24230;&#20989;&#25968;&#35774;&#35745;&#37325;&#26032;&#26500;&#24819;&#20026;&#38024;&#23545;&#32473;&#23450;&#25968;&#25454;&#38598;&#25628;&#32034;&#26368;&#20339;&#35268;&#33539;&#31995;&#25968;$p$&#30340;&#36807;&#31243;&#12290;&#37492;&#20110;&#24402;&#19968;&#21270;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#20559;&#21644;&#35299;&#26512;&#21487;&#36861;&#36394;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17888v1 Announce Type: cross  Abstract: Post-hoc out-of-distribution (OOD) detection has garnered intensive attention in reliable machine learning. Many efforts have been dedicated to deriving score functions based on logits, distances, or rigorous data distribution assumptions to identify low-scoring OOD samples. Nevertheless, these estimate scores may fail to accurately reflect the true data density or impose impractical constraints. To provide a unified perspective on density-based score design, we propose a novel theoretical framework grounded in Bregman divergence, which extends distribution considerations to encompass an exponential family of distributions. Leveraging the conjugation constraint revealed in our theorem, we introduce a \textsc{ConjNorm} method, reframing density function design as a search for the optimal norm coefficient $p$ against the given dataset. In light of the computational challenges of normalization, we devise an unbiased and analytically tract
&lt;/p&gt;</description></item><item><title>REPrune&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#27169;&#25311;&#26680;&#20462;&#21098;&#65292;&#24182;&#32467;&#21512;&#32858;&#31867;&#21644;&#28388;&#27874;&#22120;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#26356;&#31934;&#32454;&#20294;&#32467;&#26500;&#21270;&#30340;&#20462;&#21098;&#31890;&#24230;&#65292;&#20419;&#36827;&#20102;&#22312;&#35757;&#32451;CNNs&#26399;&#38388;&#30340;&#39640;&#25928;&#12289;&#28176;&#36827;&#24335;&#20462;&#21098;&#12290;</title><link>https://arxiv.org/abs/2402.17862</link><description>&lt;p&gt;
REPrune&#65306;&#36890;&#36807;&#26680;&#20195;&#34920;&#36873;&#25321;&#36827;&#34892;&#36890;&#36947;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
REPrune: Channel Pruning via Kernel Representative Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17862
&lt;/p&gt;
&lt;p&gt;
REPrune&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#27169;&#25311;&#26680;&#20462;&#21098;&#65292;&#24182;&#32467;&#21512;&#32858;&#31867;&#21644;&#28388;&#27874;&#22120;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#26356;&#31934;&#32454;&#20294;&#32467;&#26500;&#21270;&#30340;&#20462;&#21098;&#31890;&#24230;&#65292;&#20419;&#36827;&#20102;&#22312;&#35757;&#32451;CNNs&#26399;&#38388;&#30340;&#39640;&#25928;&#12289;&#28176;&#36827;&#24335;&#20462;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36947;&#20462;&#21098;&#34987;&#24191;&#27867;&#35748;&#21487;&#20026;&#21152;&#36895;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;&#20462;&#21098;&#27169;&#22411;&#21487;&#20197;&#31435;&#21363;&#37096;&#32626;&#22312;&#36890;&#29992;&#36719;&#20214;&#21644;&#30828;&#20214;&#36164;&#28304;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#21367;&#31215;&#28388;&#27874;&#22120;&#36825;&#20010;&#21333;&#20803;&#19978;&#30340;&#22823;&#20462;&#21098;&#31890;&#24230;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#19981;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#36825;&#26159;&#30001;&#20110;&#22312;CNNs&#20013;&#20915;&#23450;&#22914;&#20309;&#20197;&#21450;&#22312;&#20309;&#22788;&#24341;&#20837;&#31232;&#30095;&#24615;&#30340;&#28789;&#27963;&#24615;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REPrune&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#20462;&#21098;&#25216;&#26415;&#65292;&#27169;&#25311;&#20102;&#26680;&#20462;&#21098;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#26356;&#32454;&#20294;&#26377;&#32467;&#26500;&#30340;&#31890;&#24230;&#12290;REPrune&#20351;&#29992;&#20957;&#32858;&#32858;&#31867;&#35782;&#21035;&#27599;&#20010;&#36890;&#36947;&#20869;&#30340;&#30456;&#20284;&#26680;&#12290;&#28982;&#21518;&#65292;&#23427;&#36873;&#25321;&#26368;&#22823;&#21270;&#21253;&#21547;&#26680;&#20195;&#34920;&#30340;&#28388;&#27874;&#22120;&#65292;&#21516;&#26102;&#20248;&#21270;&#26368;&#22823;&#32858;&#31867;&#35206;&#30422;&#38382;&#39064;&#12290;&#36890;&#36807;&#19982;&#21516;&#26102;&#35757;&#32451;-&#20462;&#21098;&#33539;&#24335;&#30456;&#32467;&#21512;&#65292;REPrune&#20419;&#36827;&#20102;&#22312;&#35757;&#32451;CNNs&#26399;&#38388;&#30340;&#39640;&#25928;&#12289;&#28176;&#36827;&#24335;&#20462;&#21098;&#65292;&#36991;&#20813;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;&#30340;&#35823;&#24046;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17862v1 Announce Type: cross  Abstract: Channel pruning is widely accepted to accelerate modern convolutional neural networks (CNNs). The resulting pruned model benefits from its immediate deployment on general-purpose software and hardware resources. However, its large pruning granularity, specifically at the unit of a convolution filter, often leads to undesirable accuracy drops due to the inflexibility of deciding how and where to introduce sparsity to the CNNs. In this paper, we propose REPrune, a novel channel pruning technique that emulates kernel pruning, fully exploiting the finer but structured granularity. REPrune identifies similar kernels within each channel using agglomerative clustering. Then, it selects filters that maximize the incorporation of kernel representatives while optimizing the maximum cluster coverage problem. By integrating with a simultaneous training-pruning paradigm, REPrune promotes efficient, progressive pruning throughout training CNNs, avoi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65288;LNS&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#24182;&#20351;&#29992;&#36739;&#31895;&#31961;&#30340;&#31163;&#25955;&#21270;&#65292;&#21487;&#20197;&#22823;&#22823;&#31616;&#21270;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.17853</link><description>&lt;p&gt;
&#28508;&#22312;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65306;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38477;&#38454;&#24314;&#27169;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Latent Neural PDE Solver: a reduced-order modelling framework for partial differential equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65288;LNS&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#24182;&#20351;&#29992;&#36739;&#31895;&#31961;&#30340;&#31163;&#25955;&#21270;&#65292;&#21487;&#20197;&#22823;&#22823;&#31616;&#21270;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#21152;&#36895;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25511;&#21046;&#30340;&#31995;&#32479;&#30340;&#25968;&#20540;&#27169;&#25311;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#19982;&#35768;&#22810;&#29616;&#26377;&#30340;&#22312;&#39640;&#32500;&#31163;&#25955;&#21270;&#22330;&#19978;&#25805;&#20316;&#30340;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#35758;&#22312;&#28508;&#22312;&#31354;&#38388;&#23398;&#20064;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#20351;&#29992;&#26356;&#31895;&#31961;&#30340;&#31163;&#25955;&#21270;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550; - &#28508;&#22312;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65288;LNS&#65289;&#20013;&#65292;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23558;&#31995;&#32479;&#30340;&#20840;&#38454;&#34920;&#31034;&#25237;&#24433;&#21040;&#32593;&#26684;&#20943;&#23569;&#30340;&#31354;&#38388;&#20013;&#65292;&#25509;&#30528;&#35757;&#32451;&#19968;&#20010;&#26102;&#38388;&#27169;&#22411;&#26469;&#39044;&#27979;&#36825;&#20010;&#32593;&#26684;&#20943;&#23569;&#30340;&#31354;&#38388;&#20013;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#36825;&#31181;&#38477;&#38454;&#36807;&#31243;&#36890;&#36807;&#22823;&#22823;&#20943;&#23569;&#20276;&#38543;&#32454;&#31890;&#24230;&#31163;&#25955;&#21270;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#31616;&#21270;&#20102;&#26102;&#38388;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#20197;&#21450;&#20960;&#31181;&#20854;&#20182;&#27969;&#34892;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#31995;&#32479;&#19978;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#21333;&#30456;&#21644;&#22810;&#30456;&#27969;&#20307;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17853v1 Announce Type: cross  Abstract: Neural networks have shown promising potential in accelerating the numerical simulation of systems governed by partial differential equations (PDEs). Different from many existing neural network surrogates operating on high-dimensional discretized fields, we propose to learn the dynamics of the system in the latent space with much coarser discretizations. In our proposed framework - Latent Neural PDE Solver (LNS), a non-linear autoencoder is first trained to project the full-order representation of the system onto the mesh-reduced space, then a temporal model is trained to predict the future state in this mesh-reduced space. This reduction process simplifies the training of the temporal model by greatly reducing the computational cost accompanying a fine discretization. We study the capability of the proposed framework and several other popular neural PDE solvers on various types of systems including single-phase and multi-phase flows a
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;</title><link>https://arxiv.org/abs/2402.17840</link><description>&lt;p&gt;
&#36981;&#24490;&#25105;&#30340;&#25351;&#31034;&#24182;&#35828;&#20986;&#30495;&#30456;&#65306;&#26469;&#33258;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17840
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#23558;&#22806;&#37096;&#30693;&#35782;&#32435;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#21046;&#36866;&#24212;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;Retrieval-In-Context RAG&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23545;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LMs&#26500;&#24314;&#30340;RAG&#31995;&#32479;&#36827;&#34892;&#25552;&#31034;&#27880;&#20837;&#26102;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#12290;&#36825;&#31181;&#28431;&#27934;&#23384;&#22312;&#20110;&#35206;&#30422;Llama2&#12289;Mistral/Mixtral&#12289;Vicuna&#12289;SOLAR&#12289;WizardLM&#12289;Qwen1.5&#21644;Platypus2&#31561;&#22810;&#31181;&#29616;&#20195;LMs&#30340;&#24191;&#27867;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#21033;&#29992;&#33021;&#21147;&#21152;&#21095;&#12290;&#23558;&#30740;&#31350;&#25193;&#23637;&#21040;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25915;&#20987;&#65292;&#21487;&#20197;&#22312;&#23545;25&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#23450;&#21046;GPTs&#26045;&#21152;&#26368;&#22810;2&#20010;&#26597;&#35810;&#26102;&#20197;100%&#25104;&#21151;&#29575;&#23548;&#33268;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#65292;&#24182;&#19988;&#25105;&#20204;&#33021;&#22815;&#20197;77,000&#23383;&#30340;&#20070;&#31821;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;41%&#65292;&#20197;&#21450;&#22312;&#21547;&#26377;1,569,00&#35789;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;3%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17840v1 Announce Type: cross  Abstract: Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,00
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.17826</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Prediction-Powered Ranking of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#26681;&#25454;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#36827;&#34892;&#25490;&#21517;--&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#21463;&#20154;&#31867;&#20559;&#22909;&#65292;&#37027;&#20040;&#23427;&#23601;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#26469;&#24357;&#21512;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#21487;&#33021;&#24341;&#20837;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17826v1 Announce Type: cross  Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TruthX&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;TruthX&#24179;&#22343;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17811</link><description>&lt;p&gt;
TruthX: &#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20943;&#36731;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TruthX&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;TruthX&#24179;&#22343;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26377;&#26102;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#21487;&#33021;&#29983;&#25104;&#19981;&#30495;&#23454;&#30340;&#22238;&#24212;&#65292;&#23613;&#31649;&#25317;&#26377;&#27491;&#30830;&#30340;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TruthX&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;LLMs&#20869;&#37096;&#34920;&#31034;&#20197;&#33719;&#21462;&#20854;&#30495;&#23454;&#24615;&#30340;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#12290;TruthX&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;LLM&#30340;&#34920;&#31034;&#20998;&#21035;&#26144;&#23556;&#21040;&#35821;&#20041;&#21644;&#30495;&#23454;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#35782;&#21035;&#30495;&#23454;&#30340;&#32534;&#36753;&#26041;&#21521;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#31354;&#38388;&#20013;&#32534;&#36753;LLM&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;TruthX&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;LLMs&#30340;&#30495;&#23454;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TruthX&#36890;&#36807;20%&#30340;&#24179;&#22343;&#20540;&#25552;&#39640;&#20102;13&#31181;&#20808;&#36827;LLMs&#22312;TruthfulQA&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#30495;&#23454;&#24615;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30495;&#23454;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17811v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge. In this paper, we propose TruthX, an inference-time method to elicit the truthfulness of LLMs by editing their internal representations in truthful space. TruthX employs an auto-encoder to map LLM's representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM's internal representations in truthful space, TruthX effectively enhances the truthfulness of LLMs. Experiments show that TruthX effectively improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that the truthful space
&lt;/p&gt;</description></item><item><title>BioT5+&#26159;BioT5&#26694;&#26550;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#25972;&#21512;IUPAC&#21517;&#31216;&#12289;&#21253;&#21547;&#24191;&#27867;&#29983;&#29289;&#25991;&#26412;&#21644;&#20998;&#23376;&#25968;&#25454;&#12289;&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;&#20197;&#21450;&#26032;&#39062;&#30340;&#25968;&#20540;&#26631;&#35760;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#34920;&#31034;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.17810</link><description>&lt;p&gt;
BioT5+: &#36890;&#36807;IUPAC&#38598;&#25104;&#21644;&#22810;&#20219;&#21153;&#35843;&#25972;&#23454;&#29616;&#24191;&#20041;&#29983;&#29289;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17810
&lt;/p&gt;
&lt;p&gt;
BioT5+&#26159;BioT5&#26694;&#26550;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#25972;&#21512;IUPAC&#21517;&#31216;&#12289;&#21253;&#21547;&#24191;&#27867;&#29983;&#29289;&#25991;&#26412;&#21644;&#20998;&#23376;&#25968;&#25454;&#12289;&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;&#20197;&#21450;&#26032;&#39062;&#30340;&#25968;&#20540;&#26631;&#35760;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#34920;&#31034;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35745;&#31639;&#29983;&#29289;&#23398;&#30340;&#30740;&#31350;&#36235;&#21183;&#36234;&#26469;&#36234;&#38598;&#20013;&#20110;&#25972;&#21512;&#25991;&#26412;&#21644;&#29983;&#29289;&#23454;&#20307;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#30340;&#32972;&#26223;&#19979;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;BioT5&#30340;&#20808;&#21069;&#24037;&#20316;&#22312;&#36328;&#36234;&#22810;&#26679;&#21270;&#20219;&#21153;&#21644;&#32570;&#20047;&#23545;&#20998;&#23376;&#32467;&#26500;&#30340;&#32454;&#33268;&#29702;&#35299;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#30340;&#25991;&#26412;&#34920;&#31034;&#65288;&#20363;&#22914;IUPAC&#65289;&#26041;&#38754;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BioT5+&#65292;&#36825;&#26159;BioT5&#26694;&#26550;&#30340;&#19968;&#20010;&#25193;&#23637;&#65292;&#26088;&#22312;&#22686;&#24378;&#29983;&#29289;&#30740;&#31350;&#21644;&#33647;&#29289;&#21457;&#29616;&#12290; BioT5+&#21253;&#21547;&#20960;&#20010;&#26032;&#39062;&#30340;&#29305;&#24615;&#65306;&#25972;&#21512;IUPAC&#21517;&#31216;&#20197;&#21152;&#28145;&#23545;&#20998;&#23376;&#30340;&#29702;&#35299;&#65292;&#21253;&#25324;&#26469;&#33258;bioRxiv&#21644;PubChem&#31561;&#28304;&#30340;&#24191;&#27867;&#29983;&#29289;&#25991;&#26412;&#21644;&#20998;&#23376;&#25968;&#25454;&#65292;&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;&#20197;&#36328;&#36234;&#22810;&#20010;&#20219;&#21153;&#65292;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#25968;&#23383;&#25968;&#25454;&#22788;&#29702;&#30340;&#26032;&#39062;&#25968;&#20540;&#26631;&#35760;&#25216;&#26415;&#12290; &#36825;&#20123;&#22686;&#24378;&#21151;&#33021;&#20351;BioT5+&#33021;&#22815;&#24357;&#21512;&#20998;&#23376;&#34920;&#31034;&#21644;&#23427;&#20204;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17810v1 Announce Type: cross  Abstract: Recent research trends in computational biology have increasingly focused on integrating text and bio-entity modeling, especially in the context of molecules and proteins. However, previous efforts like BioT5 faced challenges in generalizing across diverse tasks and lacked a nuanced understanding of molecular structures, particularly in their textual representations (e.g., IUPAC). This paper introduces BioT5+, an extension of the BioT5 framework, tailored to enhance biological research and drug discovery. BioT5+ incorporates several novel features: integration of IUPAC names for molecular understanding, inclusion of extensive bio-text and molecule data from sources like bioRxiv and PubChem, the multi-task instruction tuning for generality across tasks, and a novel numerical tokenization technique for improved processing of numerical data. These enhancements allow BioT5+ to bridge the gap between molecular representations and their text
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#22312;&#26412;&#25991;&#20013;&#24314;&#31435;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#31561;&#20215;&#20110;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.17805</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks and Arithmetic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17805
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#22312;&#26412;&#25991;&#20013;&#24314;&#31435;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#31561;&#20215;&#20110;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#34920;&#24449;&#20102;&#36981;&#24490;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#19981;&#38480;&#20110;&#32858;&#21512;-&#32452;&#21512;GNN&#25110;&#20854;&#20182;&#29305;&#23450;&#31867;&#22411;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20351;&#29992;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#20934;&#30830;&#23545;&#24212;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#32467;&#26524;&#20013;&#65292;&#32593;&#32476;&#30340;&#28608;&#27963;&#20989;&#25968;&#25104;&#20026;&#30005;&#36335;&#20013;&#30340;&#38376;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#24120;&#25968;&#28145;&#24230;&#30005;&#36335;&#21644;&#32593;&#32476;&#23478;&#26063;&#22343;&#25104;&#31435;&#65292;&#26080;&#35770;&#26159;&#22312;&#19968;&#33268;&#36824;&#26159;&#38750;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#25152;&#26377;&#24120;&#35265;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17805v1 Announce Type: cross  Abstract: We characterize the computational power of neural networks that follow the graph neural network (GNN) architecture, not restricted to aggregate-combine GNNs or other particular types. We establish an exact correspondence between the expressivity of GNNs using diverse activation functions and arithmetic circuits over real numbers. In our results the activation function of the network becomes a gate type in the circuit. Our result holds for families of constant depth circuits and networks, both uniformly and non-uniformly, for all common activation functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#23545;&#21019;&#24847;&#20135;&#19994;&#24102;&#26469;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20844;&#24179;&#20351;&#29992;&#26631;&#20934;&#21644;AI-&#29256;&#26435;&#24615;&#23545;AI&#21457;&#23637;&#21644;&#20844;&#21496;&#21033;&#28070;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2402.17801</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#29256;&#26435;&#65306;&#19968;&#20010;&#21160;&#24577;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Generative AI and Copyright: A Dynamic Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#23545;&#21019;&#24847;&#20135;&#19994;&#24102;&#26469;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20844;&#24179;&#20351;&#29992;&#26631;&#20934;&#21644;AI-&#29256;&#26435;&#24615;&#23545;AI&#21457;&#23637;&#21644;&#20844;&#21496;&#21033;&#28070;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#21363;&#23558;&#39072;&#35206;&#21019;&#24847;&#20135;&#19994;&#12290;&#22312;&#23545;&#36825;&#39033;&#26032;&#25216;&#26415;&#30340;&#24040;&#22823;&#20852;&#22859;&#20013;&#65292;&#20854;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#21644;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#30340;&#20004;&#20010;&#29256;&#26435;&#38382;&#39064;&#26159;&#65306;1) &#34917;&#20607;&#37027;&#20123;&#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#21019;&#20316;&#32773;&#65288;&#20844;&#24179;&#20351;&#29992;&#26631;&#20934;&#65289;&#65307;&#21644;2) AI&#29983;&#25104;&#30340;&#20869;&#23481;&#26159;&#21542;&#26377;&#36164;&#26684;&#33719;&#24471;&#29256;&#26435;&#20445;&#25252;&#65288;AI-&#29256;&#26435;&#24615;&#65289;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#38382;&#39064;&#24341;&#21457;&#20102;&#23398;&#26415;&#30028;&#21644;&#23454;&#36341;&#32773;&#20043;&#38388;&#28608;&#28872;&#30340;&#20105;&#35770;&#65292;&#20294;&#22823;&#22810;&#25968;&#20998;&#26512;&#37117;&#38598;&#20013;&#22312;&#23427;&#20204;&#23545;&#29616;&#26377;&#29256;&#26435;&#21407;&#21017;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20004;&#20010;&#30417;&#31649;&#38382;&#39064;&#21450;&#20854;&#20114;&#21160;&#23545;&#32463;&#27982;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#20869;&#29983;&#20869;&#23481;&#21019;&#20316;&#21644;AI&#27169;&#22411;&#21457;&#23637;&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20844;&#24179;&#20351;&#29992;&#26631;&#20934;&#21644;AI-&#29256;&#26435;&#24615;&#23545;AI&#21457;&#23637;&#12289;AI&#20844;&#21496;&#21033;&#28070;&#12289;cr&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17801v1 Announce Type: cross  Abstract: The rapid advancement of generative AI is poised to disrupt the creative industry. Amidst the immense excitement for this new technology, its future development and applications in the creative industry hinge crucially upon two copyright issues: 1) the compensation to creators whose content has been used to train generative AI models (the fair use standard); and 2) the eligibility of AI-generated content for copyright protection (AI-copyrightability). While both issues have ignited heated debates among academics and practitioners, most analysis has focused on their challenges posed to existing copyright doctrines. In this paper, we aim to better understand the economic implications of these two regulatory issues and their interactions. By constructing a dynamic model with endogenous content creation and AI model development, we unravel the impacts of the fair use standard and AI-copyrightability on AI development, AI company profit, cr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;NLVR&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#38656;&#35201;&#32452;&#21512;&#21644;&#31354;&#38388;&#25512;&#29702;&#12289;&#23545;&#35821;&#20041;&#21644;&#31995;&#32479;&#24615;&#20559;&#35265;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.17793</link><description>&lt;p&gt;
&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#65311;&#22810;&#27169;LLMs&#21644;NLVR&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Surprising Failure? Multimodal LLMs and the NLVR Challenge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17793
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;NLVR&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#38656;&#35201;&#32452;&#21512;&#21644;&#31354;&#38388;&#25512;&#29702;&#12289;&#23545;&#35821;&#20041;&#21644;&#31995;&#32479;&#24615;&#20559;&#35265;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;MLLMs&#8212;&#8212;GPT-4V&#12289;Gemini Pro&#21644;&#24320;&#28304;&#27169;&#22411;IDEFICS&#8212;&#8212;&#23545;&#20110;&#32452;&#21512;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;NLVR&#30340;&#34920;&#29616;&#12290;NLVR&#35201;&#27714;&#27169;&#22411;&#26681;&#25454;&#19968;&#20010;&#20154;&#31867;&#20070;&#20889;&#30340;&#21477;&#23376;&#21644;&#19968;&#20010;&#21512;&#25104;&#22270;&#20687;&#26469;&#30830;&#23450;&#21477;&#23376;&#30456;&#23545;&#20110;&#22270;&#20687;&#30340;&#30495;&#20551;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;NLVR&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#38656;&#35201;&#32452;&#21512;&#21644;&#31354;&#38388;&#25512;&#29702;&#65292;&#24182;&#19988;&#23545;&#35821;&#20041;&#21644;&#31995;&#32479;&#24615;&#20559;&#35265;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17793v1 Announce Type: new  Abstract: This study evaluates three state-of-the-art MLLMs -- GPT-4V, Gemini Pro, and the open-source model IDEFICS -- on the compositional natural language vision reasoning task NLVR. Given a human-written sentence paired with a synthetic image, this task requires the model to determine the truth value of the sentence with respect to the image. Despite the strong performance demonstrated by these models, we observe they perform poorly on NLVR, which was constructed to require compositional and spatial reasoning, and to be robust for semantic and systematic biases.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#28436;&#21270;&#39063;&#31890;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#22312;&#20998;&#31867;&#24369;&#30417;&#30563;EEG&#25968;&#25454;&#27969;&#26102;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#23545;&#24773;&#32490;&#30456;&#20851;&#27169;&#24335;&#30340;&#20998;&#31867;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17792</link><description>&lt;p&gt;
EGNN-C+: &#21487;&#35299;&#37322;&#30340;&#28436;&#21270;&#39063;&#31890;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#22312;&#24369;&#30417;&#30563;EEG&#25968;&#25454;&#27969;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EGNN-C+: Interpretable Evolving Granular Neural Network and Application in Classification of Weakly-Supervised EEG Data Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17792
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#28436;&#21270;&#39063;&#31890;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#22312;&#20998;&#31867;&#24369;&#30417;&#30563;EEG&#25968;&#25454;&#27969;&#26102;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#23545;&#24773;&#32490;&#30456;&#20851;&#27169;&#24335;&#30340;&#20998;&#31867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#28436;&#21270;&#39063;&#31890;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65288;eGNN-C+&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#21452;&#36793;&#30028;&#36229;&#31435;&#26041;&#20307;&#26469;&#34920;&#31034;&#39063;&#31890;&#65292;&#24182;&#23450;&#21046;&#20102;&#36866;&#24212;&#24615;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#22806;&#37096;&#30418;&#23376;&#23545;&#25968;&#25454;&#35206;&#30422;&#21644;&#22122;&#22768;&#25233;&#21046;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#20869;&#37096;&#30418;&#23376;&#20445;&#25345;&#28789;&#27963;&#20197;&#25429;&#33719;&#28418;&#31227;&#12290;&#20998;&#31867;&#22120;&#20174;&#38646;&#24320;&#22987;&#28436;&#21270;&#65292;&#22312;&#36816;&#34892;&#36807;&#31243;&#20013;&#32467;&#21512;&#26032;&#30340;&#31867;&#21035;&#65292;&#24182;&#25191;&#34892;&#23616;&#37096;&#22686;&#37327;&#29305;&#24449;&#21152;&#26435;&#12290;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#38598;&#20013;&#22312;&#23545;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#20013;&#19982;&#24773;&#32490;&#30456;&#20851;&#30340;&#27169;&#24335;&#36827;&#34892;&#20998;&#31867;&#12290;&#24773;&#32490;&#35782;&#21035;&#23545;&#20110;&#22686;&#24378;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#36924;&#30495;&#24615;&#21644;&#20114;&#21160;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20174;28&#21517;&#21442;&#19982;&#30005;&#33041;&#28216;&#25103;&#30340;&#20010;&#20307;&#33719;&#24471;&#30340;EEG&#20449;&#21495;&#30340;&#20613;&#31435;&#21494;&#35889;&#20013;&#25552;&#21462;&#29305;&#24449; -- &#36825;&#26159;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#28216;&#25103;&#24341;&#21457;&#19981;&#21516;&#30340;&#20027;&#23548;&#24773;&#32490;&#65306;&#26080;&#32842;&#12289;&#24179;&#38745;&#12289;&#24656;&#24598;&#25110;&#24555;&#20048;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20010;&#20307;&#38388;&#19981;&#21516;&#30340;&#24773;&#32490;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17792v1 Announce Type: cross  Abstract: We introduce a modified incremental learning algorithm for evolving Granular Neural Network Classifiers (eGNN-C+). We use double-boundary hyper-boxes to represent granules, and customize the adaptation procedures to enhance the robustness of outer boxes for data coverage and noise suppression, while ensuring that inner boxes remain flexible to capture drifts. The classifier evolves from scratch, incorporates new classes on the fly, and performs local incremental feature weighting. As an application, we focus on the classification of emotion-related patterns within electroencephalogram (EEG) signals. Emotion recognition is crucial for enhancing the realism and interactivity of computer systems. We extract features from the Fourier spectrum of EEG signals obtained from 28 individuals engaged in playing computer games -- a public dataset. Each game elicits a different predominant emotion: boredom, calmness, horror, or joy. We analyze indi
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26631;&#31614;&#20449;&#24687;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#38382;&#39064;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#65288;LICAP&#65289;&#65292;&#21033;&#29992;&#36830;&#32493;&#26631;&#31614;&#29983;&#25104;&#23545;&#27604;&#26679;&#26412;&#26469;&#26356;&#22909;&#22320;&#20102;&#35299;&#39640;&#37325;&#35201;&#24615;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.17791</link><description>&lt;p&gt;
&#26631;&#31614;&#20449;&#24687;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Label Informed Contrastive Pretraining for Node Importance Estimation on Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17791
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26631;&#31614;&#20449;&#24687;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#38382;&#39064;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#65288;LICAP&#65289;&#65292;&#21033;&#29992;&#36830;&#32493;&#26631;&#31614;&#29983;&#25104;&#23545;&#27604;&#26679;&#26412;&#26469;&#26356;&#22909;&#22320;&#20102;&#35299;&#39640;&#37325;&#35201;&#24615;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#65288;NIE&#65289;&#26159;&#25512;&#26029;&#22270;&#20013;&#33410;&#28857;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20852;&#36259;&#24050;&#32463;&#36716;&#21521;&#30693;&#35782;&#22270;&#35889;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#25110;&#32570;&#22833;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#30001;&#20110;&#25968;&#25454;&#21644;&#30693;&#35782;&#26356;&#20026;&#20016;&#23500;&#12290;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;NIE&#26041;&#27861;&#36890;&#36807;&#21487;&#29992;&#26631;&#31614;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#35757;&#32451;&#20043;&#21069;&#24179;&#31561;&#22320;&#32771;&#34385;&#27599;&#20010;&#24863;&#20852;&#36259;&#30340;&#33410;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#26356;&#37325;&#35201;&#30340;&#33410;&#28857;&#36890;&#24120;&#38656;&#35201;&#25110;&#20250;&#24471;&#21040;&#26356;&#22810;&#20851;&#27880;&#65292;&#20363;&#22914;&#65292;&#20154;&#20204;&#21487;&#33021;&#26356;&#20851;&#24515;&#20855;&#26377;&#26356;&#39640;&#37325;&#35201;&#24615;&#30340;&#30005;&#24433;&#25110;&#32593;&#39029;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#31614;&#20449;&#24687;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#38382;&#39064;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#65288;LICAP&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20855;&#26377;&#39640;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#33410;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LICAP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#36830;&#32493;&#26631;&#31614;&#29983;&#25104;&#39044;&#35757;&#32451;&#23884;&#20837;&#30340;&#23545;&#27604;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17791v1 Announce Type: new  Abstract: Node Importance Estimation (NIE) is a task of inferring importance scores of the nodes in a graph. Due to the availability of richer data and knowledge, recent research interests of NIE have been dedicating to knowledge graphs for predicting future or missing node importance scores. Existing state-of-the-art NIE methods train the model by available labels, and they consider every interested node equally before training. However, the nodes with higher importance often require or receive more attention in real-world scenarios, e.g., people may care more about the movies or webpages with higher importance. To this end, we introduce Label Informed ContrAstive Pretraining (LICAP) to the NIE problem for being better aware of the nodes with high importance scores. Specifically, LICAP is a novel type of contrastive learning framework that aims to fully utilize the continuous labels to generate contrastive samples for pretraining embeddings. Cons
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSC-CoT&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20013;&#38388;&#27493;&#39588;&#30340;&#31574;&#30053;&#21644;&#26597;&#35810;&#30693;&#35782;&#22270;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25968;&#23398;&#25512;&#29702;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.17786</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36880;&#27493;&#33258;&#27965;&#30340;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Stepwise Self-Consistent Mathematical Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17786
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSC-CoT&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20013;&#38388;&#27493;&#39588;&#30340;&#31574;&#30053;&#21644;&#26597;&#35810;&#30693;&#35782;&#22270;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25968;&#23398;&#25512;&#29702;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25968;&#23398;&#25512;&#29702;&#26159;&#22256;&#38590;&#30340;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;Stepwise Self-Consistent Chain-of-Thought&#65288;SSC-CoT&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;SSC-CoT&#21033;&#29992;&#36873;&#25321;&#22522;&#20110;&#19981;&#21516;&#25512;&#29702;&#38142;&#20132;&#38598;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#26597;&#35810;&#21253;&#21547;&#30456;&#20851;&#39046;&#22495;&#30693;&#35782;&#30340;&#30693;&#35782;&#22270;&#26469;&#21457;&#29616;&#20851;&#38190;&#30340;&#20013;&#38388;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17786v1 Announce Type: new  Abstract: Using Large Language Models for complex mathematical reasoning is difficult, primarily due to the complexity of multi-step reasoning. The main challenges of this process include (1) selecting critical intermediate results to advance the procedure, and (2) limited exploration of potential solutions. To address these issues, we introduce a novel algorithm, namely Stepwise Self-Consistent Chain-of-Thought (SSC-CoT). SSC-CoT employs a strategy of selecting intermediate steps based on the intersection of various reasoning chains. Additionally, SSC-CoT enables the model to discover critical intermediate steps by querying a knowledge graph comprising relevant domain knowledge. To validate SSC-CoT, we present a new dataset, TriMaster100, tailored for complex trigonometry problems. This dataset contains 100 questions, with each solution broken down into scored intermediate steps, facilitating a comprehensive evaluation of the mathematical reasoni
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ByteComposer&#20195;&#29702;&#26694;&#26550;&#65292;&#22312;&#27169;&#25311;&#20154;&#31867;&#21019;&#20316;&#27969;&#31243;&#30340;&#22522;&#30784;&#19978;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#21487;&#19982;&#20154;&#31867;&#21019;&#20316;&#32773;&#30456;&#25552;&#24182;&#35770;&#30340;&#26059;&#24459;&#21019;&#20316;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.17785</link><description>&lt;p&gt;
ByteComposer&#65306;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#31867;&#20154;&#26059;&#24459;&#21019;&#20316;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ByteComposer: a Human-like Melody Composition Method based on Language Model Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17785
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ByteComposer&#20195;&#29702;&#26694;&#26550;&#65292;&#22312;&#27169;&#25311;&#20154;&#31867;&#21019;&#20316;&#27969;&#31243;&#30340;&#22522;&#30784;&#19978;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#21487;&#19982;&#20154;&#31867;&#21019;&#20316;&#32773;&#30456;&#25552;&#24182;&#35770;&#30340;&#26059;&#24459;&#21019;&#20316;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLM&#65289;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#20154;&#31867;&#23545;&#40784;&#19988;&#21487;&#35299;&#37322;&#30340;&#26059;&#24459;&#21019;&#20316;&#31995;&#32479;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ByteComposer&#65292;&#19968;&#20010;&#20195;&#29702;&#26694;&#26550;&#65292;&#27169;&#25311;&#20154;&#31867;&#21019;&#20316;&#30340;&#22235;&#20010;&#29420;&#31435;&#27493;&#39588;&#65306;&#8220;&#27010;&#24565;&#20998;&#26512; - &#36215;&#33609;&#20316;&#21697; - &#33258;&#25105;&#35780;&#20272;&#21644;&#20462;&#25913; - &#32654;&#23398;&#36873;&#25321;&#8221;&#12290;&#35813;&#26694;&#26550;&#23558;LLM&#30340;&#20132;&#20114;&#21644;&#30693;&#35782;&#29702;&#35299;&#29305;&#24615;&#19982;&#29616;&#26377;&#30340;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#26080;&#32541;&#34701;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#20010;&#21487;&#19982;&#20154;&#31867;&#21019;&#20316;&#32773;&#23218;&#32654;&#30340;&#26059;&#24459;&#21019;&#20316;&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;GPT4&#21644;&#20960;&#20010;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#23454;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#19987;&#19994;&#38899;&#20048;&#20316;&#26354;&#23478;&#21442;&#19982;&#20102;&#22810;&#32500;&#24230;&#35780;&#20272;&#65292;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#20010;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17785v1 Announce Type: cross  Abstract: Large Language Models (LLM) have shown encouraging progress in multimodal understanding and generation tasks. However, how to design a human-aligned and interpretable melody composition system is still under-explored. To solve this problem, we propose ByteComposer, an agent framework emulating a human's creative pipeline in four separate steps : "Conception Analysis - Draft Composition - Self-Evaluation and Modification - Aesthetic Selection". This framework seamlessly blends the interactive and knowledge-understanding features of LLMs with existing symbolic music generation models, thereby achieving a melody composition agent comparable to human creators. We conduct extensive experiments on GPT4 and several open-source large language models, which substantiate our framework's effectiveness. Furthermore, professional music composers were engaged in multi-dimensional evaluations, the final results demonstrated that across various facets
&lt;/p&gt;</description></item><item><title>BagStacking&#26159;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#30340;&#33258;&#20030;&#26679;&#26412;&#19978;&#35757;&#32451;&#19968;&#32452;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#22522;&#30784;&#27169;&#22411;&#36755;&#20986;&#21644;&#30495;&#23454;&#26631;&#31614;&#19978;&#35757;&#32451;&#20803;&#23398;&#20064;&#22120;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#32858;&#21512;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#27493;&#24577;&#20923;&#32467;&#26816;&#27979;&#30340;&#26174;&#30528;&#25913;&#36827;</title><link>https://arxiv.org/abs/2402.17783</link><description>&lt;p&gt;
BagStacking&#65306;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#27493;&#24577;&#20923;&#32467;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BagStacking: An Integrated Ensemble Learning Approach for Freezing of Gait Detection in Parkinson's Disease
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17783
&lt;/p&gt;
&lt;p&gt;
BagStacking&#26159;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#30340;&#33258;&#20030;&#26679;&#26412;&#19978;&#35757;&#32451;&#19968;&#32452;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#22522;&#30784;&#27169;&#22411;&#36755;&#20986;&#21644;&#30495;&#23454;&#26631;&#31614;&#19978;&#35757;&#32451;&#20803;&#23398;&#20064;&#22120;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#32858;&#21512;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#27493;&#24577;&#20923;&#32467;&#26816;&#27979;&#30340;&#26174;&#30528;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BagStacking&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#19979;&#32972;&#20256;&#24863;&#22120;&#36319;&#36394;&#21152;&#36895;&#24230;&#26469;&#22686;&#24378;&#23545;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#27493;&#24577;&#20923;&#32467;&#65288;FOG&#65289;&#30340;&#26816;&#27979;&#12290;BagStacking&#24314;&#31435;&#22312;&#35013;&#34955;&#21644;&#22534;&#21472;&#30340;&#21407;&#21017;&#20043;&#19978;&#65292;&#26088;&#22312;&#23454;&#29616;&#35013;&#34955;&#30340;&#33258;&#20030;&#37319;&#26679;&#30340;&#26041;&#24046;&#20943;&#23569;&#22909;&#22788;&#65292;&#21516;&#26102;&#36890;&#36807;&#22534;&#21472;&#23398;&#20064;&#22797;&#26434;&#30340;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17783v1 Announce Type: cross  Abstract: This paper introduces BagStacking, a novel ensemble learning method designed to enhance the detection of Freezing of Gait (FOG) in Parkinson's Disease (PD) by using a lower-back sensor to track acceleration. Building on the principles of bagging and stacking, BagStacking aims to achieve the variance reduction benefit of bagging's bootstrap sampling while also learning sophisticated blending through stacking. The method involves training a set of base models on bootstrap samples from the training data, followed by a meta-learner trained on the base model outputs and true labels to find an optimal aggregation scheme. The experimental evaluation demonstrates significant improvements over other state-of-the-art machine learning methods on the validation set. Specifically, BagStacking achieved a MAP score of 0.306, outperforming LightGBM (0.234) and classic Stacking (0.286). Additionally, the run-time of BagStacking was measured at 3828 sec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#36229;&#23485;&#24102;&#26080;&#26631;&#31614;&#38376;&#30340;&#21160;&#24577;&#38170;&#28857;&#36873;&#25321;&#21644;DS-TWR&#30340;&#23454;&#26102;&#23039;&#24577;&#39044;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;MD&#20934;&#30830;&#20301;&#32622;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#23450;&#20301;&#21644;&#23039;&#24577;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.17778</link><description>&lt;p&gt;
&#20026;&#36229;&#23485;&#24102;&#26080;&#26631;&#31614;&#38376;&#23454;&#26102;&#23039;&#24577;&#39044;&#27979;&#25552;&#20986;&#21160;&#24577;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic Anchor Selection and Real-Time Pose Prediction for Ultra-wideband Tagless Gate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#36229;&#23485;&#24102;&#26080;&#26631;&#31614;&#38376;&#30340;&#21160;&#24577;&#38170;&#28857;&#36873;&#25321;&#21644;DS-TWR&#30340;&#23454;&#26102;&#23039;&#24577;&#39044;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;MD&#20934;&#30830;&#20301;&#32622;&#38656;&#27714;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#23450;&#20301;&#21644;&#23039;&#24577;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#23485;&#24102;&#65288;Ultra-wideband&#65292;UWB&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#26395;&#23454;&#29616;&#25509;&#35302;&#24615;&#26381;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;UWB&#26080;&#26631;&#31614;&#38376;&#65288;UTG&#65289;&#65292;&#24471;&#30410;&#20110;&#22522;&#20110;&#19979;&#34892;&#21040;&#36798;&#26102;&#38388;&#24046;&#65288;DL-TDoA&#65289;&#21644;&#21452;&#36793;&#21452;&#21521;&#27979;&#36317;&#65288;DS-TWR&#65289;&#31561;&#20004;&#31181;&#19981;&#21516;&#27979;&#36317;&#26041;&#27861;&#25552;&#20379;&#30340;&#21400;&#31859;&#32423;&#23450;&#20301;&#31934;&#24230;&#65292;&#27491;&#22312;&#23835;&#36215;&#12290;UTG&#26159;&#19968;&#31181;&#22522;&#20110;UWB&#30340;&#25509;&#35302;&#24615;&#26381;&#21153;&#65292;&#25552;&#20379;&#26080;&#38656;&#23454;&#26102;&#31227;&#21160;&#35774;&#22791;&#65288;MD&#65289;&#25970;&#20987;&#21363;&#21487;&#23454;&#29616;&#30340;&#26080;&#32541;&#38376;&#31105;&#31995;&#32479;&#12290;MD&#30340;&#20301;&#32622;&#36890;&#36807;DL-TDoA&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;DS-TWR&#19982;&#26368;&#36817;&#30340;UTG&#36890;&#20449;&#20197;&#25171;&#24320;&#22823;&#38376;&#12290;&#22240;&#27492;&#65292;MD&#30340;&#20934;&#30830;&#20301;&#32622;&#26159;UTG&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20379;&#20102;&#38024;&#23545;DL-TDoA&#21644;DS-TWR&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26497;&#20854;&#20934;&#30830;&#30340;DL-TDoA&#23450;&#20301;&#30340;&#21160;&#24577;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#21644;DS-TWR&#30340;&#23039;&#24577;&#39044;&#27979;&#65292;&#31216;&#20026;DynaPose&#12290;&#23039;&#24577;&#23450;&#20041;&#20026;MD&#22312;&#20154;&#20307;&#19978;&#30340;&#23454;&#38469;&#20301;&#32622;&#65292;&#24433;&#21709;&#30528;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17778v1 Announce Type: cross  Abstract: Ultra-wideband (UWB) is emerging as a promising solution that can realize proximity services, such as UWB tagless gate (UTG), thanks to centimeter-level localization accuracy based on two different ranging methods such as downlink time-difference of arrival (DL-TDoA) and double-sided two-way ranging (DS-TWR). The UTG is a UWB-based proximity service that provides a seamless gate pass system without requiring real-time mobile device (MD) tapping. The location of MD is calculated using DL-TDoA, and the MD communicates with the nearest UTG using DS-TWR to open the gate. Therefore, the knowledge about the exact location of MD is the main challenge of UTG, and hence we provide the solutions for both DL-TDoA and DS-TWR. In this paper, we propose dynamic anchor selection for extremely accurate DL-TDoA localization and pose prediction for DS-TWR, called DynaPose. The pose is defined as the actual location of MD on the human body, which affects
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#21644;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17775</link><description>&lt;p&gt;
Wavelet&#25955;&#23556;&#21464;&#25442;&#22312;&#29983;&#29289;&#22768;&#23398;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Wavelet Scattering Transform for Bioacustics: Application to Watkins Marine Mammal Sound Database
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#21644;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#30340;&#20132;&#27969;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#21463;&#21040;&#40483;&#21483;&#30340;&#22810;&#26679;&#24615;&#21644;&#29615;&#22659;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#65288;WMMD&#65289;&#26159;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#37325;&#28857;&#20171;&#32461;&#20102;&#35813;&#25968;&#25454;&#38598;&#19978;&#26368;&#26032;&#30340;&#22522;&#20934;&#35760;&#24405;&#65292;&#30528;&#37325;&#28548;&#28165;&#25968;&#25454;&#20934;&#22791;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;STFT&#22522;&#30784;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#28145;&#23618;&#26550;&#26500;&#21644;&#27531;&#24046;&#23618;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20934;&#30830;&#29575;&#19978;&#20351;&#29992;WST&#27604;&#29616;&#26377;&#20998;&#31867;&#26550;&#26500;&#25552;&#39640;&#20102;6&#65285;&#65292;&#20351;&#29992;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#25552;&#39640;&#20102;8&#65285;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17775v1 Announce Type: cross  Abstract: Marine mammal communication is a complex field, hindered by the diversity of vocalizations and environmental factors. The Watkins Marine Mammal Sound Database (WMMD) is an extensive labeled dataset used in machine learning applications. However, the methods for data preparation, preprocessing, and classification found in the literature are quite disparate. This study first focuses on a brief review of the state-of-the-art benchmarks on the dataset, with an emphasis on clarifying data preparation and preprocessing methods. Subsequently, we propose the application of the Wavelet Scattering Transform (WST) in place of standard methods based on the Short-Time Fourier Transform (STFT). The study also tackles a classification task using an ad-hoc deep architecture with residual layers. We outperform the existing classification architecture by $6\%$ in accuracy using WST and $8\%$ using Mel spectrogram preprocessing, effectively reducing by h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#30340;&#28436;&#36827;&#36712;&#36857;&#65292;&#20174;&#22522;&#30784;&#21407;&#29702;&#36861;&#28335;&#21040;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#38416;&#26126;&#20102;AI&#22312;&#22609;&#36896;&#36710;&#36742;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#20013;&#30340;&#22522;&#30784;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17690</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65306;&#20154;&#24037;&#26234;&#33021;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#28436;&#36827;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicles: Evolution of Artificial Intelligence and Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#30340;&#28436;&#36827;&#36712;&#36857;&#65292;&#20174;&#22522;&#30784;&#21407;&#29702;&#36861;&#28335;&#21040;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#38416;&#26126;&#20102;AI&#22312;&#22609;&#36896;&#36710;&#36742;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#20013;&#30340;&#22522;&#30784;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#20132;&#36890;&#36816;&#36755;&#39046;&#22495;&#36814;&#26469;&#20102;&#19968;&#20010;&#21464;&#38761;&#26102;&#20195;&#65292;&#36890;&#36807;&#23574;&#31471;&#25216;&#26415;&#37325;&#22609;&#20102;&#31227;&#21160;&#24615;&#30340;&#26684;&#23616;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#25972;&#21512;&#26159;&#36825;&#19968;&#36827;&#21270;&#30340;&#26680;&#24515;&#65292;&#23558;&#36710;&#36742;&#25512;&#21521;&#21069;&#25152;&#26410;&#26377;&#30340;&#33258;&#20027;&#39046;&#22495;&#12290;&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#30340;&#28436;&#36827;&#36712;&#36857;&#65292;&#20174;&#22522;&#30784;&#21407;&#29702;&#36861;&#28335;&#21040;&#26368;&#26032;&#36827;&#23637;&#12290;&#20174;&#24403;&#21069;&#26223;&#35266;&#27010;&#36848;&#24320;&#22987;&#65292;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;AI&#22312;&#22609;&#36896;&#36710;&#36742;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#20013;&#30340;&#22522;&#30784;&#20316;&#29992;&#12290;&#38416;&#26126;&#20102;AI&#39537;&#21160;&#30340;&#36710;&#36742;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#20013;&#28041;&#21450;&#30340;&#27493;&#39588;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#39537;&#21160;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#20262;&#29702;&#32771;&#34385;&#21644;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;AI/&#23398;&#20064;&#30340;&#20351;&#29992;&#21644;&#31867;&#22411;&#30340;&#32479;&#35745;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17690v1 Announce Type: cross  Abstract: The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies. Central to this evolu- tion is the integration of Artificial Intelligence (AI) and learning algorithms, propelling vehicles into realms of unprecedented autonomy. This paper provides a comprehensive exploration of the evolutionary trajectory of AI within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements. Commencing with a current landscape overview, the paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles. It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing ethical considerations and bias in AI-driven software development for autonomous vehicles. The study presents statis- tical insights into the usage and types of AI/learning
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Navigator&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;GPU&#20869;&#23384;&#31649;&#29702;&#21644;&#20219;&#21153;&#25918;&#32622;&#21151;&#33021;&#65292;&#20197;&#20943;&#23569;&#20316;&#19994;&#24310;&#36831;&#65292;&#21516;&#26102;&#39640;&#25928;&#21033;&#29992;&#36164;&#28304;&#65292;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35843;&#24230;&#22120;&#34920;&#29616;&#20986;&#26356;&#26174;&#33879;&#30340;&#23436;&#25104;&#26102;&#38388;&#32553;&#30701;&#12290;</title><link>https://arxiv.org/abs/2402.17652</link><description>&lt;p&gt;
&#23548;&#33322;&#22120;&#65306;&#38754;&#21521;&#24310;&#36831;&#25935;&#24863;ML&#24037;&#20316;&#27969;&#30340;&#20998;&#25955;&#24335;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
Navigator: A Decentralized Scheduler for Latency-Sensitive ML Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17652
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Navigator&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;GPU&#20869;&#23384;&#31649;&#29702;&#21644;&#20219;&#21153;&#25918;&#32622;&#21151;&#33021;&#65292;&#20197;&#20943;&#23569;&#20316;&#19994;&#24310;&#36831;&#65292;&#21516;&#26102;&#39640;&#25928;&#21033;&#29992;&#36164;&#28304;&#65292;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35843;&#24230;&#22120;&#34920;&#29616;&#20986;&#26356;&#26174;&#33879;&#30340;&#23436;&#25104;&#26102;&#38388;&#32553;&#30701;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#36827;&#34892;ML&#26597;&#35810;&#22788;&#29702;&#65292;&#20854;&#20013;&#21551;&#29992;GPU&#30340;&#24037;&#20316;&#20154;&#21592;&#21327;&#35843;&#25191;&#34892;&#22797;&#26434;&#26597;&#35810;&#65306;&#36825;&#31181;&#35745;&#31639;&#39118;&#26684;&#32463;&#24120;&#20986;&#29616;&#22312;&#19982;&#29992;&#25143;&#20114;&#21160;&#25903;&#25345;&#22270;&#20687;&#22788;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;GPU&#20869;&#23384;&#31649;&#29702;&#21644;&#20219;&#21153;&#25918;&#32622;&#30340;&#21327;&#21516;&#35843;&#24230;&#20195;&#34920;&#30528;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Navigator&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#36825;&#20123;&#21151;&#33021;&#65292;&#20197;&#20943;&#23569;&#20316;&#19994;&#24310;&#36831;&#65292;&#21516;&#26102;&#39640;&#25928;&#21033;&#29992;&#36164;&#28304;&#65292;&#23558;&#20219;&#21153;&#25918;&#32622;&#22312;&#25968;&#25454;&#20381;&#36182;&#20851;&#31995;&#23558;&#24471;&#21040;&#28385;&#36275;&#30340;&#22320;&#26041;&#65292;&#23558;&#26469;&#33258;&#21516;&#19968;&#20316;&#19994;&#30340;&#20219;&#21153;&#25918;&#22312;&#19968;&#36215;&#65288;&#24403;&#36825;&#19981;&#20250;&#20351;&#20027;&#26426;&#25110;&#20854;GPU&#36229;&#36733;&#26102;&#65289;&#65292;&#24182;&#39640;&#25928;&#22320;&#31649;&#29702;GPU&#20869;&#23384;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35843;&#24230;&#22120;&#27604;&#36739;&#26174;&#31034;&#65292;&#22312;&#38656;&#35201;&#30456;&#21516;&#37327;&#29978;&#33267;&#26356;&#23569;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#23436;&#25104;&#26102;&#38388;&#26174;&#33879;&#20943;&#23569;&#12290;&#22312;&#19968;&#20010;&#26696;&#20363;&#20013;&#65292;&#20165;&#38656;&#35201;&#19968;&#21322;&#30340;&#26381;&#21153;&#22120;&#26469;&#22788;&#29702;&#30456;&#21516;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17652v1 Announce Type: cross  Abstract: We consider ML query processing in distributed systems where GPU-enabled workers coordinate to execute complex queries: a computing style often seen in applications that interact with users in support of image processing and natural language processing. In such systems, coscheduling of GPU memory management and task placement represents a promising opportunity. We propose Navigator, a novel framework that unifies these functions to reduce job latency while using resources efficiently, placing tasks where data dependencies will be satisfied, collocating tasks from the same job (when this will not overload the host or its GPU), and efficiently managing GPU memory. Comparison with other state of the art schedulers shows a significant reduction in completion times while requiring the same amount or even fewer resources. In one case, just half the servers were needed for processing the same workload.
&lt;/p&gt;</description></item><item><title>DAGnosis&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#26469;&#35299;&#20915;&#25968;&#25454;&#19968;&#33268;&#24615;&#26816;&#27979;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#23450;&#20301;&#20026;&#20309;&#26679;&#26412;&#20250;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2402.17599</link><description>&lt;p&gt;
DAGnosis&#65306;&#20351;&#29992;&#32467;&#26500;&#36827;&#34892;&#25968;&#25454;&#19981;&#19968;&#33268;&#24615;&#30340;&#23616;&#37096;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DAGnosis: Localized Identification of Data Inconsistencies using Structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17599
&lt;/p&gt;
&lt;p&gt;
DAGnosis&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#26469;&#35299;&#20915;&#25968;&#25454;&#19968;&#33268;&#24615;&#26816;&#27979;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#23450;&#20301;&#20026;&#20309;&#26679;&#26412;&#20250;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#26102;&#35782;&#21035;&#21644;&#36866;&#24403;&#22788;&#29702;&#25968;&#25454;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#23545;&#21487;&#38752;&#22320;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#26399;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#33021;&#22815;&#35782;&#21035;&#19982;&#35757;&#32451;&#38598;&#30456;&#20851;&#30340;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#65288;1&#65289;&#22312;&#29305;&#24449;&#23637;&#29616;&#32479;&#35745;&#29420;&#31435;&#24615;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#21387;&#32553;&#34920;&#31034;&#65307;&#65288;2&#65289;&#32570;&#20047;&#23616;&#37096;&#21270;&#65292;&#26080;&#27861;&#20934;&#30830;&#23450;&#20301;&#26679;&#26412;&#20026;&#20309;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#65292;&#36825;&#23545;&#25351;&#23548;&#26410;&#26469;&#25968;&#25454;&#25910;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#26469;&#32534;&#30721;&#35757;&#32451;&#38598;&#30340;&#29305;&#24449;&#27010;&#29575;&#20998;&#24067;&#21644;&#29420;&#31435;&#24615;&#20316;&#20026;&#32467;&#26500;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#22522;&#26412;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;DAGnosis&#65292;&#21033;&#29992;&#36825;&#20123;&#32467;&#26500;&#20132;&#20114;&#24102;&#26469;&#26377;&#20215;&#20540;&#30340;&#12289;&#28145;&#21051;&#30340;&#25968;&#25454;&#20013;&#24515;&#32467;&#35770;&#12290;DAGnosis&#35299;&#38145;&#20102;&#22312;DAG&#19978;&#23450;&#20301;&#19981;&#19968;&#33268;&#24615;&#21407;&#22240;&#30340;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17599v1 Announce Type: cross  Abstract: Identification and appropriate handling of inconsistencies in data at deployment time is crucial to reliably use machine learning models. While recent data-centric methods are able to identify such inconsistencies with respect to the training set, they suffer from two key limitations: (1) suboptimality in settings where features exhibit statistical independencies, due to their usage of compressive representations and (2) lack of localization to pin-point why a sample might be flagged as inconsistent, which is important to guide future data collection. We solve these two fundamental limitations using directed acyclic graphs (DAGs) to encode the training set's features probability distribution and independencies as a structure. Our method, called DAGnosis, leverages these structural interactions to bring valuable and insightful data-centric conclusions. DAGnosis unlocks the localization of the causes of inconsistencies on a DAG, an aspec
&lt;/p&gt;</description></item><item><title>OmniACT&#26159;&#19968;&#20010;&#38024;&#23545;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17553</link><description>&lt;p&gt;
OmniACT&#65306;&#29992;&#20110;&#21551;&#29992;&#26700;&#38754;&#21644;Web&#22810;&#27169;&#24335;&#36890;&#29992;&#20027;&#21160;&#26234;&#33021;&#20307;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17553
&lt;/p&gt;
&lt;p&gt;
OmniACT&#26159;&#19968;&#20010;&#38024;&#23545;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#21313;&#24180;&#26469;&#65292;&#20154;&#26426;&#20132;&#20114;&#20174;&#26681;&#26412;&#19978;&#19968;&#30452;&#26159;&#25163;&#21160;&#30340;&#12290;&#21363;&#20351;&#22312;&#20170;&#22825;&#65292;&#20960;&#20046;&#25152;&#26377;&#22312;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39640;&#25928;&#24037;&#20316;&#37117;&#38656;&#35201;&#20154;&#31867;&#22312;&#27599;&#19968;&#27493;&#37117;&#25552;&#20379;&#36755;&#20837;&#12290;&#34394;&#25311;&#20027;&#21160;&#26234;&#33021;&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#35768;&#22810;&#36825;&#20123;&#29712;&#30862;&#20219;&#21153;&#30340;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#27493;&#39588;&#12290;&#34394;&#25311;&#20195;&#29702;&#23558;&#20351;&#25216;&#26415;&#33021;&#21147;&#26377;&#38480;&#30340;&#29992;&#25143;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#22320;&#31616;&#21270;&#35768;&#22810;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#20174;&#26085;&#21382;&#31649;&#29702;&#21040;&#22797;&#26434;&#30340;&#26053;&#34892;&#39044;&#35746;&#65292;&#20943;&#23569;&#20154;&#31867;&#24178;&#39044;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; OmniACT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#26469;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#33539;&#22260;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#35832;&#22914;"&#25773;&#25918;&#19979;&#19968;&#39318;&#27468;"&#20043;&#31867;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20197;&#21450;&#26356;&#20026;&#38271;&#26399;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17553v1 Announce Type: new  Abstract: For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as "Play the next song", as well as longer horizon tasks such
&lt;/p&gt;</description></item><item><title>Sora&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#25361;&#25112;&#65292;&#26410;&#26469;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.17177</link><description>&lt;p&gt;
Sora: &#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#32972;&#26223;&#12289;&#25216;&#26415;&#12289;&#23616;&#38480;&#24615;&#21644;&#26426;&#36935;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17177
&lt;/p&gt;
&lt;p&gt;
Sora&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#25361;&#25112;&#65292;&#26410;&#26469;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sora&#26159;&#30001;OpenAI&#20110;2024&#24180;2&#26376;&#21457;&#24067;&#30340;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#20196;&#29983;&#25104;&#36924;&#30495;&#25110;&#24819;&#35937;&#30340;&#22330;&#26223;&#35270;&#39057;&#65292;&#24182;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#25991;&#22522;&#20110;&#20844;&#24320;&#30340;&#25216;&#26415;&#25253;&#21578;&#21644;&#36870;&#21521;&#24037;&#31243;&#65292;&#23545;&#36825;&#20010;&#27169;&#22411;&#30340;&#32972;&#26223;&#12289;&#30456;&#20851;&#25216;&#26415;&#12289;&#24212;&#29992;&#12289;&#23578;&#23384;&#30340;&#25361;&#25112;&#20197;&#21450;&#25991;&#26412;&#21040;&#35270;&#39057;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#26410;&#26469;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#39318;&#20808;&#25105;&#20204;&#36861;&#28335;&#20102;Sora&#30340;&#21457;&#23637;&#21382;&#31243;&#65292;&#24182;&#35843;&#26597;&#20102;&#29992;&#20110;&#26500;&#24314;&#36825;&#20010;"&#19990;&#30028;&#27169;&#25311;&#22120;"&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;Sora&#22312;&#20174;&#30005;&#24433;&#21046;&#20316;&#21644;&#25945;&#32946;&#21040;&#33829;&#38144;&#31561;&#22810;&#20010;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#21644;&#28508;&#22312;&#24433;&#21709;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#20415;&#24191;&#27867;&#37096;&#32626;Sora&#65292;&#22914;&#30830;&#20445;&#23433;&#20840;&#21644;&#26080;&#20559;&#35265;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;Sora&#20197;&#21450;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17177v1 Announce Type: cross  Abstract: Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this "world simulator". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video gene
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;RCML&#65289;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;Evidential Conflictive Multi-view Learning (ECML)&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#20914;&#31361;&#20449;&#24687;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.16897</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reliable Conflictive Multi-View Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;RCML&#65289;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;Evidential Conflictive Multi-view Learning (ECML)&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#20914;&#31361;&#20449;&#24687;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23398;&#20064;&#26088;&#22312;&#32467;&#21512;&#22810;&#20010;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#26356;&#20840;&#38754;&#25551;&#36848;&#12290;&#20043;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#20551;&#35774;&#22810;&#20010;&#35270;&#22270;&#26159;&#20005;&#26684;&#23545;&#40784;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#20302;&#36136;&#37327;&#30340;&#20914;&#31361;&#23454;&#20363;&#65292;&#21363;&#22312;&#19981;&#21516;&#35270;&#22270;&#20013;&#26174;&#31034;&#20914;&#31361;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;RCML&#65289;&#38382;&#39064;&#65292;&#35201;&#27714;&#27169;&#22411;&#20026;&#20914;&#31361;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#25552;&#20379;&#20915;&#31574;&#32467;&#26524;&#21644;&#38468;&#21152;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#38382;&#39064;&#24320;&#21457;&#20102;&#19968;&#31181;Evidential Conflictive Multi-view Learning (ECML)&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16897v1 Announce Type: cross  Abstract: Multi-view learning aims to combine multiple features to achieve more comprehensive descriptions of data. Most previous works assume that multiple views are strictly aligned. However, real-world multi-view data may contain low-quality conflictive instances, which show conflictive information in different views. Previous methods for this problem mainly focus on eliminating the conflictive data instances by removing them or replacing conflictive views. Nevertheless, real-world applications usually require making decisions for conflictive instances rather than only eliminating them. To solve this, we point out a new Reliable Conflictive Multi-view Learning (RCML) problem, which requires the model to provide decision results and attached reliabilities for conflictive multi-view data. We develop an Evidential Conflictive Multi-view Learning (ECML) method for this problem. ECML first learns view-specific evidence, which could be termed as th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102; GPT-4 &#21644; MTurk &#31649;&#36947;&#30340;&#25968;&#25454;&#26631;&#27880;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#23613;&#31649; MTurk &#37319;&#29992;&#20102;&#26368;&#20339;&#23454;&#36341;&#65292;&#20294; GPT-4 &#30340;&#20934;&#30830;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#32467;&#21512; GPT-4 &#21644;&#20247;&#21253;&#26631;&#31614;&#20351;&#29992;&#32858;&#21512;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.16795</link><description>&lt;p&gt;
&#22914;&#26524;&#22312;&#19968;&#20010;&#20247;&#21253;&#25968;&#25454;&#26631;&#27880;&#31649;&#36947;&#20013;&#65292;GPT-4
&lt;/p&gt;
&lt;p&gt;
If in a Crowdsourced Data Annotation Pipeline, a GPT-4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102; GPT-4 &#21644; MTurk &#31649;&#36947;&#30340;&#25968;&#25454;&#26631;&#27880;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#23613;&#31649; MTurk &#37319;&#29992;&#20102;&#26368;&#20339;&#23454;&#36341;&#65292;&#20294; GPT-4 &#30340;&#20934;&#30830;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#32467;&#21512; GPT-4 &#21644;&#20247;&#21253;&#26631;&#31614;&#20351;&#29992;&#32858;&#21512;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;GPT-4&#22312;&#25968;&#25454;&#26631;&#27880;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#22312;&#32447;&#20247;&#21253;&#24037;&#20316;&#32773;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#65288;MTurk&#65289;&#30340;&#24037;&#20316;&#32773;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22240;&#20559;&#31163;&#26631;&#20934;&#20247;&#21253;&#23454;&#36341;&#24182;&#24378;&#35843;&#20010;&#21035;&#24037;&#20316;&#32773;&#30340;&#34920;&#29616;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#25968;&#25454;&#26631;&#27880;&#36807;&#31243;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;GPT-4&#21644;&#19968;&#20010;&#36947;&#24503;&#19988;&#25191;&#34892;&#33391;&#22909;&#30340;MTurk&#31649;&#36947;&#65292;&#20351;&#29992;415&#21517;&#24037;&#20316;&#32773;&#26631;&#27880;&#20102;&#26469;&#33258;200&#31687;&#23398;&#26415;&#25991;&#31456;&#30340;3,177&#20010;&#21477;&#27573;&#65292;&#20351;&#29992;&#20102;CODA-19&#26041;&#26696;&#12290;&#20004;&#20010;&#24037;&#20316;&#32773;&#30028;&#38754;&#20135;&#29983;&#20102;127,080&#20010;&#26631;&#31614;&#65292;&#28982;&#21518;&#36890;&#36807;&#20843;&#31181;&#26631;&#31614;&#32858;&#21512;&#31639;&#27861;&#25512;&#26029;&#20986;&#26368;&#32456;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#37319;&#29992;&#20102;&#26368;&#20339;&#23454;&#36341;&#65292;MTurk&#31649;&#36947;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#20026;81.5%&#65292;&#32780;GPT-4&#36798;&#21040;&#20102;83.6%&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#23558;GPT-4&#30340;&#26631;&#31614;&#19982;&#36890;&#36807;&#20808;&#36827;&#24037;&#20316;&#32773;&#30028;&#38754;&#25910;&#38598;&#30340;&#20247;&#21253;&#26631;&#31614;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#32858;&#21512;&#26102;&#65292;8&#31181;&#31639;&#27861;&#20013;&#26377;2&#31181;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16795v1 Announce Type: cross  Abstract: Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk). However, these studies were criticized for deviating from standard crowdsourcing practices and emphasizing individual workers' performances over the whole data-annotation process. This paper compared GPT-4 and an ethical and well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces yielded 127,080 labels, which were then used to infer the final labels through eight label-aggregation algorithms. Our evaluation showed that despite best practices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved 83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected via an advanced worker interface for aggregation, 2 out of the 8 algorithms achieved an even higher accuracy (
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#24211;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#29983;&#25104;NL-GQL&#25968;&#25454;&#23545;&#24182;&#24494;&#35843;LLMs&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.16567</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models to a Domain-specific Graph Database
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16567
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#24211;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#29983;&#25104;NL-GQL&#25968;&#25454;&#23545;&#24182;&#24494;&#35843;LLMs&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#24211;&#65288;Graph DB&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#37329;&#34701;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#21307;&#33647;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#36716;&#25442;&#20026;&#22270;&#26597;&#35810;&#35821;&#35328;&#65288;GQL&#65289;&#65292;&#36890;&#24120;&#31216;&#20026;NL2GQL&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#19987;&#19994;&#21270;&#29305;&#24615;&#32780;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#20123;&#26041;&#27861;&#35797;&#22270;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#31867;&#20284;&#30340;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#36716;SQL&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;NL2GQL&#20219;&#21153;&#20013;&#65292;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;NL-GQL&#25968;&#25454;&#23545;&#20351;&#24471;&#38590;&#20197;&#24314;&#31435;LLMs&#21644;&#22270;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#27969;&#27700;&#32447;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;ChatGPT&#22522;&#20110;&#32473;&#23450;&#30340;&#22270;&#25968;&#25454;&#24211;&#33258;&#25105;&#29983;&#25104;NL-GQL&#25968;&#25454;&#23545;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21019;&#24314;&#30340;&#25968;&#25454;&#26469;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;LLMs&#19982;&#22270;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16567v1 Announce Type: new  Abstract: Graph Databases (Graph DB) are widely applied in various fields, including finance, social networks, and medicine. However, translating Natural Language (NL) into the Graph Query Language (GQL), commonly known as NL2GQL, proves to be challenging due to its inherent complexity and specialized nature. Some approaches have sought to utilize Large Language Models (LLMs) to address analogous tasks like text2SQL. Nevertheless, when it comes to NL2GQL taskson a particular domain, the absence of domain-specific NL-GQL data pairs makes it difficult to establish alignment between LLMs and the graph DB. To address this challenge, we propose a well-defined pipeline. Specifically, we utilize ChatGPT to create NL-GQL data pairs based on the given graph DB with self-instruct. Then, we use the created data to fine-tune LLMs, thereby achieving alignment between LLMs and the graph DB. Additionally, during inference, we propose a method that extracts relev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22235;&#21313;&#22810;&#24180;&#21069;&#30340;&#22270;&#23572;&#25991;&#27979;&#35797;&#26694;&#26550;&#26159;&#21542;&#23545;LLM&#30340;&#35760;&#24518;&#34892;&#20026;&#26377;&#25152;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2402.16505</link><description>&lt;p&gt;
&#35760;&#24518;&#24046;&#36317;&#65306;LLM &#26159;&#21542;&#33021;&#36890;&#36807;&#22270;&#23572;&#25991;&#27979;&#35797;&#65311;
&lt;/p&gt;
&lt;p&gt;
Memory GAPS: Would LLM pass the Tulving Test?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22235;&#21313;&#22810;&#24180;&#21069;&#30340;&#22270;&#23572;&#25991;&#27979;&#35797;&#26694;&#26550;&#26159;&#21542;&#23545;LLM&#30340;&#35760;&#24518;&#34892;&#20026;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16505v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#22270;&#23572;&#25991;&#27979;&#35797;&#26088;&#22312;&#30740;&#31350;&#35748;&#30693;&#21644;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#35760;&#24518;&#34920;&#29616;&#12290;&#20854;&#32467;&#26524;&#26377;&#21161;&#20110;&#35780;&#20272;&#35760;&#24518;&#30340;&#8220;&#21327;&#21516;&#24341;&#23548;&#27169;&#22411;&#8221;&#21450;&#31867;&#20284;RK&#33539;&#20363;&#22312;&#20154;&#31867;&#34920;&#29616;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#30528;&#25163;&#30740;&#31350;&#36825;&#20010;&#24050;&#26377;&#22235;&#21313;&#22810;&#24180;&#21382;&#21490;&#30340;&#26694;&#26550;&#26159;&#21542;&#33021;&#20026;LLM&#30340;&#35760;&#24518;&#34892;&#20026;&#24102;&#26469;&#19968;&#20123;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16505v1 Announce Type: new  Abstract: The Tulving Test was designed to investigate memory performance in recognition and recall tasks. Its results help assess the relevance of the "Synergistic Ecphory Model" of memory and similar RK paradigms in human performance. This paper starts investigating whether the more than forty-year-old framework sheds some light on LLMs' acts of remembering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.16363</link><description>&lt;p&gt;
LLM&#25512;&#26029;&#25581;&#31034;&#65306;&#35843;&#26597;&#19982;Roofline&#27169;&#22411;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLM Inference Unveiled: Survey and Roofline Model Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#26029;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#25552;&#20379;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#29420;&#29305;&#32467;&#21512;&#12290;&#34429;&#28982;&#35813;&#39046;&#22495;&#24050;&#32463;&#25193;&#23637;&#24182;&#20805;&#28385;&#27963;&#21147;&#65292;&#20294;&#33267;&#20170;&#36824;&#27809;&#26377;&#19968;&#20010;&#31616;&#26126;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;LLM&#25512;&#26029;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#28165;&#26224;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#19981;&#20165;&#24635;&#32467;&#20102;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#36824;&#22522;&#20110;Roofline&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#12290;&#36825;&#19968;&#26694;&#26550;&#33021;&#22815;&#24110;&#21161;&#35782;&#21035;LLM&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#20174;&#32780;&#20026;&#37096;&#32626;LLM&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27719;&#24635;&#20102;&#39640;&#25928;LLM&#25512;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20851;&#38190;&#39046;&#22495;&#65292;&#27604;&#22914;&#26435;&#37325;&#20248;&#21270;&#65288;&#22914;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16363v1 Announce Type: cross  Abstract: The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantizatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.16278</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#27880;&#37322;&#23884;&#20837;&#27169;&#22411;&#30340;&#26412;&#20307;&#21253;&#21547;&#20851;&#31995;&#39044;&#27979;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16278
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#34920;&#31034;&#23454;&#20307;&#30340;&#26412;&#20307;&#23884;&#20837;&#65292;&#29992;&#20110;&#26412;&#20307;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#26412;&#20307;&#23884;&#20837;&#26410;&#35299;&#20915;&#31867;&#20284;&#21644;&#23396;&#31435;&#23454;&#20307;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#26410;&#25552;&#21462;&#26412;&#20307;&#20013;&#27880;&#37322;&#20844;&#29702;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#30340;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65306;Inverted-index Matrix Embedding (InME) &#21644; Co-occurrence Matrix Embedding (CoME)&#12290;&#36825;&#20004;&#31181;&#23884;&#20837;&#36890;&#36807;&#27599;&#20010;&#21333;&#35789;&#22312;&#19968;&#32452;&#20844;&#29702;&#20013;&#20986;&#29616;&#30340;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#20844;&#29702;&#20013;&#21333;&#35789;&#30340;&#20849;&#29616;&#26469;&#25429;&#33719;&#27880;&#37322;&#20844;&#29702;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#12290;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#39044;&#27979;&#30340;&#36229;&#31867;&#19982;&#23376;&#31867;&#30456;&#20284;&#19988;&#23396;&#31435;&#20110;&#26412;&#20307;&#20013;&#30340;&#20854;&#20182;&#23454;&#20307;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16278v1 Announce Type: new  Abstract: Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#24320;&#21457;&#32773;&#35770;&#22363;&#21644;&#24179;&#21488;&#19978;&#30340;&#24086;&#23376;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;&#19982;&#26426;&#22120;&#23398;&#20064;&#36164;&#20135;&#31649;&#29702;&#25361;&#25112;&#30456;&#20851;&#30340;133&#20010;&#20027;&#39064;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#21253;&#25324;&#36719;&#20214;&#20381;&#36182;&#12289;&#27169;&#22411;&#37096;&#32626;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.15990</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36164;&#20135;&#31649;&#29702;&#25361;&#25112;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Challenges in Machine Learning Asset Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15990
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#24320;&#21457;&#32773;&#35770;&#22363;&#21644;&#24179;&#21488;&#19978;&#30340;&#24086;&#23376;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;&#19982;&#26426;&#22120;&#23398;&#20064;&#36164;&#20135;&#31649;&#29702;&#25361;&#25112;&#30456;&#20851;&#30340;133&#20010;&#20027;&#39064;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#21253;&#25324;&#36719;&#20214;&#20381;&#36182;&#12289;&#27169;&#22411;&#37096;&#32626;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#65292;&#39640;&#25928;&#30340;&#36164;&#20135;&#31649;&#29702;&#65292;&#21253;&#25324;ML&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#12289;&#31639;&#27861;&#21644;&#24037;&#20855;&#65292;&#23545;&#20110;&#36164;&#28304;&#20248;&#21270;&#12289;&#25345;&#32493;&#24615;&#33021;&#21644;&#31616;&#21270;&#30340;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20351;&#24471;&#24555;&#36895;&#36845;&#20195;&#12289;&#36866;&#24212;&#24615;&#12289;&#20943;&#23569;&#24320;&#21457;&#21040;&#37096;&#32626;&#26102;&#38388;&#20197;&#21450;&#21487;&#38752;&#30340;&#36755;&#20986;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#23384;&#22312;&#30740;&#31350;&#65292;&#20294;&#22312;&#35832;&#22914;&#27169;&#22411;&#29256;&#26412;&#25511;&#21046;&#12289;&#25968;&#25454;&#21487;&#36861;&#28335;&#24615;&#21644;&#21327;&#20316;&#31561;&#25805;&#20316;&#25361;&#25112;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#35201;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#36825;&#23545;ML&#39033;&#30446;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#26469;&#33258;&#24320;&#21457;&#32773;&#35770;&#22363;&#21644;&#24179;&#21488;&#30340;15,065&#20010;&#24086;&#23376;&#65292;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#26469;&#20998;&#31867;&#26597;&#35810;&#65292;&#21033;&#29992;BERTopic&#25552;&#21462;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#24320;&#25918;&#24335;&#21345;&#29255;&#25490;&#24207;&#21644;BERTopic&#32858;&#31867;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;133&#20010;&#19982;&#36164;&#20135;&#31649;&#29702;&#25361;&#25112;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#20998;&#25104;16&#20010;&#23439;&#20027;&#39064;&#65292;&#20854;&#20013;&#36719;&#20214;&#20381;&#36182;&#12289;&#27169;&#22411;&#37096;&#32626;&#21644;&#27169;&#22411;&#35757;&#32451;&#26159;&#26368;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15990v1 Announce Type: cross  Abstract: In machine learning (ML), efficient asset management, including ML models, datasets, algorithms, and tools, is vital for resource optimization, consistent performance, and a streamlined development lifecycle. This enables quicker iterations, adaptability, reduced development-to-deployment time, and reliable outputs. Despite existing research, a significant knowledge gap remains in operational challenges like model versioning, data traceability, and collaboration, which are crucial for the success of ML projects. Our study aims to address this gap by analyzing 15,065 posts from developer forums and platforms, employing a mixed-method approach to classify inquiries, extract challenges using BERTopic, and identify solutions through open card sorting and BERTopic clustering. We uncover 133 topics related to asset management challenges, grouped into 16 macro-topics, with software dependency, model deployment, and model training being the mo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340; GAN &#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#32593;&#32476;&#23433;&#20840;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#21512;&#25104;&#25915;&#20987;&#22330;&#26223;&#26469;&#25913;&#36827;&#23041;&#32961;&#35782;&#21035;&#21644;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15945</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340; GAN &#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#31649;&#29702;&#30340;&#21069;&#27839;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attention-GAN for Anomaly Detection: A Cutting-Edge Approach to Cybersecurity Threat Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340; GAN &#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#32593;&#32476;&#23433;&#20840;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#21512;&#25104;&#25915;&#20987;&#22330;&#26223;&#26469;&#25913;&#36827;&#23041;&#32961;&#35782;&#21035;&#21644;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340; GAN &#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#32593;&#32476;&#23433;&#20840;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#24322;&#24120;&#26816;&#27979;&#12290;&#38024;&#23545;&#32593;&#32476;&#23041;&#32961;&#19981;&#26029;&#28436;&#21464;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#21512;&#25104;&#25915;&#20987;&#22330;&#26223;&#65292;&#20174;&#32780;&#20016;&#23500;&#25968;&#25454;&#38598;&#24182;&#25913;&#36827;&#23041;&#32961;&#35782;&#21035;&#12290;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30456;&#32467;&#21512;&#26159;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#29305;&#28857;&#12290;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#20102;&#27169;&#22411;&#32858;&#28966;&#20110;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#26816;&#27979;&#24494;&#22937;&#21644;&#22797;&#26434;&#30340;&#25915;&#20987;&#27169;&#24335;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;GANs&#36890;&#36807;&#29983;&#25104;&#39069;&#22806;&#22810;&#26679;&#30340;&#25915;&#20987;&#25968;&#25454;&#65292;&#28085;&#30422;&#24050;&#30693;&#21644;&#26032;&#20852;&#30340;&#23041;&#32961;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#36825;&#31181;&#21452;&#37325;&#26041;&#27861;&#30830;&#20445;&#31995;&#32479;&#33021;&#22815;&#24212;&#23545;&#19981;&#26029;&#28436;&#21464;&#30340;&#32593;&#32476;&#25915;&#20987;&#20445;&#25345;&#30456;&#20851;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;KDD Cup &#21644; CICIDS2017 &#25968;&#25454;&#38598;&#29992;&#20110;&#39564;&#35777;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15945v1 Announce Type: cross  Abstract: This paper proposes an innovative Attention-GAN framework for enhancing cybersecurity, focusing on anomaly detection. In response to the challenges posed by the constantly evolving nature of cyber threats, the proposed approach aims to generate diverse and realistic synthetic attack scenarios, thereby enriching the dataset and improving threat identification. Integrating attention mechanisms with Generative Adversarial Networks (GANs) is a key feature of the proposed method. The attention mechanism enhances the model's ability to focus on relevant features, essential for detecting subtle and complex attack patterns. In addition, GANs address the issue of data scarcity by generating additional varied attack data, encompassing known and emerging threats. This dual approach ensures that the system remains relevant and effective against the continuously evolving cyberattacks. The KDD Cup and CICIDS2017 datasets were used to validate this m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20449;&#24687;&#35770;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#22320;&#27719;&#24635;&#29616;&#26377;&#25506;&#27979;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#28040;&#38500;&#20102;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.15808</link><description>&lt;p&gt;
&#22810;&#33218;&#25915;&#20987;&#30340;&#26368;&#20339;&#38646;&#23556;&#20987;&#25506;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Optimal Zero-Shot Detector for Multi-Armed Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20449;&#24687;&#35770;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#22320;&#27719;&#24635;&#29616;&#26377;&#25506;&#27979;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#28040;&#38500;&#20102;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24694;&#24847;&#21442;&#19982;&#32773;&#37319;&#29992;&#22810;&#33218;&#25915;&#20987;&#31574;&#30053;&#25805;&#32437;&#25968;&#25454;&#26679;&#26412;&#30340;&#24773;&#20917;&#65292;&#20026;&#20854;&#25552;&#20379;&#20102;&#21508;&#31181;&#26041;&#24335;&#21521;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;&#22122;&#38899;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#26816;&#27979;&#20219;&#20309;&#23545;&#36755;&#20837;&#30340;&#26356;&#25913;&#26469;&#20445;&#25252;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#38450;&#24481;&#31574;&#30053;&#20013;&#26497;&#24230;&#35880;&#24910;&#65292;&#25805;&#20316;&#22312;&#38450;&#23432;&#32773;&#25317;&#26377;&#20449;&#24687;&#26126;&#26174;&#23569;&#20110;&#25915;&#20987;&#32773;&#30340;&#29615;&#22659;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38450;&#23432;&#32773;&#26080;&#27861;&#21033;&#29992;&#20219;&#20309;&#25968;&#25454;&#26679;&#26412;&#26469;&#35757;&#32451;&#38450;&#24481;&#27169;&#22411;&#25110;&#39564;&#35777;&#20449;&#36947;&#30340;&#23436;&#25972;&#24615;&#12290;&#30456;&#21453;&#65292;&#38450;&#23432;&#32773;&#23436;&#20840;&#20381;&#36182;&#19968;&#32452;&#29616;&#25104;&#30340;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#25506;&#27979;&#22120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20449;&#24687;&#35770;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#22320;&#27719;&#24635;&#36825;&#20123;&#25506;&#27979;&#22120;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#20351;&#29992;&#26696;&#20363;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15808v1 Announce Type: cross  Abstract: This paper explores a scenario in which a malicious actor employs a multi-armed attack strategy to manipulate data samples, offering them various avenues to introduce noise into the dataset. Our central objective is to protect the data by detecting any alterations to the input. We approach this defensive strategy with utmost caution, operating in an environment where the defender possesses significantly less information compared to the attacker. Specifically, the defender is unable to utilize any data samples for training a defense model or verifying the integrity of the channel. Instead, the defender relies exclusively on a set of pre-existing detectors readily available ``off the shelf''. To tackle this challenge, we derive an innovative information-theoretic defense approach that optimally aggregates the decisions made by these detectors, eliminating the need for any training data. We further explore a practical use-case scenario fo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23558;&#22823;&#35268;&#27169;&#22810;&#27169;&#25968;&#25454;&#36716;&#21270;&#20026;&#36830;&#36143;&#27969;&#30021;&#25991;&#26412;&#65292;&#39318;&#27425;&#25512;&#20986;&#20102;&#29992;&#20110;&#20307;&#32946;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;AI&#35780;&#35770;&#31995;&#32479;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.15514</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25991;&#26412;&#22312;&#20307;&#32946;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Scale Generative AI Text Applied to Sports and Music
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15514
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23558;&#22823;&#35268;&#27169;&#22810;&#27169;&#25968;&#25454;&#36716;&#21270;&#20026;&#36830;&#36143;&#27969;&#30021;&#25991;&#26412;&#65292;&#39318;&#27425;&#25512;&#20986;&#20102;&#29992;&#20110;&#20307;&#32946;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;AI&#35780;&#35770;&#31995;&#32479;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#23186;&#20307;&#20869;&#23481;&#65288;&#21253;&#25324;&#35780;&#35770;&#21644;&#20010;&#24615;&#21270;&#26032;&#38395;&#25253;&#36947;&#65289;&#25193;&#23637;&#21040;&#20840;&#29699;&#22823;&#22411;&#20307;&#32946;&#21644;&#38899;&#20048;&#27963;&#21160;&#30340;&#29983;&#20135;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23558;&#22823;&#37327;&#22810;&#27169;&#25968;&#25454;&#65288;&#20363;&#22914;&#35270;&#39057;&#12289;&#25991;&#31456;&#12289;&#23454;&#26102;&#27604;&#20998;&#12289;&#32479;&#35745;&#25968;&#25454;&#21644;&#36164;&#26009;&#65289;&#36716;&#25442;&#20026;&#36830;&#36143;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#22522;&#20110;&#36825;&#19968;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#27425;&#25512;&#20986;&#20102;&#19968;&#27454;&#20154;&#24037;&#26234;&#33021;&#35780;&#35770;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#34987;&#37096;&#32626;&#29992;&#20110;&#20026;2023&#24180;&#32654;&#22269;&#20844;&#24320;&#36187;&#12289;&#28201;&#24067;&#23572;&#30331;&#20844;&#24320;&#36187;&#21644;&#22823;&#24072;&#36187;&#30340;&#31934;&#24425;&#29255;&#27573;&#21046;&#20316;&#33258;&#21160;&#21270;&#21465;&#36848;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36824;&#34987;&#25193;&#23637;&#29992;&#20110;&#20026;ESPN&#26790;&#24187;&#27204;&#27012;&#29699;&#21644;&#26684;&#33713;&#32654;&#22870;&#38899;&#20048;&#33402;&#26415;&#23478;&#25925;&#20107;&#21019;&#36896;&#20010;&#24615;&#21270;&#20869;&#23481;&#12290;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#37319;&#29992;&#20102;&#30456;&#21516;&#30340;&#36719;&#20214;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;15&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#24179;&#22343;Rouge-L&#20026;82.00&#65292;&#22256;&#24785;&#24230;&#20026;6.6&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15514v1 Announce Type: cross  Abstract: We address the problem of scaling up the production of media content, including commentary and personalized news stories, for large-scale sports and music events worldwide. Our approach relies on generative AI models to transform a large volume of multimodal data (e.g., videos, articles, real-time scoring feeds, statistics, and fact sheets) into coherent and fluent text. Based on this approach, we introduce, for the first time, an AI commentary system, which was deployed to produce automated narrations for highlight packages at the 2023 US Open, Wimbledon, and Masters tournaments. In the same vein, our solution was extended to create personalized content for ESPN Fantasy Football and stories about music artists for the Grammy awards. These applications were built using a common software architecture achieved a 15x speed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Our work was successfully deployed at the aforeme
&lt;/p&gt;</description></item><item><title>Text2Pic Swift&#26694;&#26550;&#38024;&#23545;&#22823;&#35268;&#27169;&#24211;&#20013;&#25991;&#26412;&#25551;&#36848;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#31574;&#30053;&#35299;&#20915;&#20102;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15276</link><description>&lt;p&gt;
Text2Pic Swift&#65306;&#22686;&#24378;&#22823;&#35268;&#27169;&#24211;&#20013;&#38271;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale Libraries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15276
&lt;/p&gt;
&lt;p&gt;
Text2Pic Swift&#26694;&#26550;&#38024;&#23545;&#22823;&#35268;&#27169;&#24211;&#20013;&#25991;&#26412;&#25551;&#36848;&#21040;&#22270;&#20687;&#30340;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#31574;&#30053;&#35299;&#20915;&#20102;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15276v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#26816;&#32034;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#25968;&#23383;&#22270;&#20070;&#39302;&#12289;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#21644;&#22810;&#23186;&#20307;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#26597;&#35810;&#26469;&#25628;&#32034;&#22270;&#20687;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21462;&#24471;&#20102;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#27169;&#31946;&#30340;&#26816;&#32034;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#26174;&#30528;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#29983;&#25104;&#21487;&#27880;&#20837;&#30340;&#23884;&#20837;&#25152;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Text2Pic Swift&#26694;&#26550;&#65292;&#19987;&#20026;&#22312;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#26377;&#25928;&#21644;&#31283;&#20581;&#22320;&#26816;&#32034;&#19982;&#24191;&#27867;&#25991;&#26412;&#25551;&#36848;&#23545;&#24212;&#30340;&#22270;&#20687;&#32780;&#35774;&#35745;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#21021;&#22987;&#22522;&#20110;&#23454;&#20307;&#30340;&#25490;&#24207;&#65288;ER&#65289;&#38454;&#27573;&#36890;&#36807;&#22810;&#26597;&#35810;&#23545;&#22810;&#30446;&#26631;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#38271;&#25991;&#26412;&#26597;&#35810;&#20013;&#22266;&#26377;&#30340;&#27495;&#20041;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#21487;&#33021;&#30340;&#20505;&#36873;&#39033;&#65292;&#20197;&#20415;&#36827;&#34892;&#21518;&#32493;&#20998;&#26512;&#12290;&#25509;&#19979;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15276v1 Announce Type: cross  Abstract: Text-to-image retrieval plays a crucial role across various applications, including digital libraries, e-commerce platforms, and multimedia databases, by enabling the search for images using text queries. Despite the advancements in Multimodal Large Language Models (MLLMs), which offer leading-edge performance, their applicability in large-scale, varied, and ambiguous retrieval scenarios is constrained by significant computational demands and the generation of injective embeddings. This paper introduces the Text2Pic Swift framework, tailored for efficient and robust retrieval of images corresponding to extensive textual descriptions in sizable datasets. The framework employs a two-tier approach: the initial Entity-based Ranking (ER) stage addresses the ambiguity inherent in lengthy text queries through a multiple-queries-to-multiple-targets strategy, effectively narrowing down potential candidates for subsequent analysis. Following thi
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#26469;&#20197;&#30446;&#26631;&#23548;&#21521;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#21487;&#33021;&#20250;&#38754;&#20020;&#22870;&#21169;&#23849;&#28291;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15194</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#20316;&#20026;&#29109;&#27491;&#21017;&#21270;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15194
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#26469;&#20197;&#30446;&#26631;&#23548;&#21521;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#21487;&#33021;&#20250;&#38754;&#20020;&#22870;&#21169;&#23849;&#28291;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;&#33258;&#28982;&#22270;&#20687;&#21644;&#34507;&#30333;&#36136;&#30340;&#20998;&#24067;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21487;&#20195;&#34920;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#65292;&#20294;&#25105;&#20204;&#36890;&#24120;&#26356;&#20851;&#27880;&#20854;&#20182;&#23646;&#24615;&#65292;&#20363;&#22914;&#29983;&#25104;&#22270;&#20687;&#30340;&#32654;&#23398;&#36136;&#37327;&#25110;&#29983;&#25104;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#23646;&#24615;&#12290;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#26576;&#20123;&#22870;&#21169;&#20989;&#25968;&#30340;&#20215;&#20540;&#65288;&#20363;&#22914;&#22270;&#20687;&#30340;&#32654;&#23398;&#36136;&#37327;&#65289;&#20197;&#30446;&#26631;&#23548;&#21521;&#30340;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#26679;&#26412;&#22810;&#26679;&#24615;&#20943;&#23569;&#65292;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#20986;&#29616;&#26174;&#33879;&#20559;&#24046;&#65292;&#29978;&#33267;&#30001;&#20110;&#21033;&#29992;&#19981;&#23436;&#32654;&#30340;&#22870;&#21169;&#20989;&#25968;&#32780;&#23548;&#33268;&#26679;&#26412;&#36136;&#37327;&#36739;&#24046;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#22870;&#21169;&#20989;&#25968;&#26159;&#29992;&#20110;&#36817;&#20284;&#30495;&#23454;&#8220;&#30495;&#23454;&#8221;&#22870;&#21169;&#30340;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#26368;&#21518;&#19968;&#20010;&#38382;&#39064;&#32463;&#24120;&#20250;&#20135;&#29983;&#12290;&#36825;&#20123;&#25361;&#25112;&#24635;&#31216;&#20026;&#8220;&#22870;&#21169;&#23849;&#28291;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15194v1 Announce Type: cross  Abstract: Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth "genuine" reward, as is the case in many practical applications. These challenges, collectively termed "reward collapse," pose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15160</link><description>&lt;p&gt;
&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#30340;&#21464;&#21387;&#22120;&#35760;&#24518;&#20307;&#29992;&#20110;&#20307;&#39564;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Spatially-Aware Transformer Memory for Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#33410;&#35760;&#24518;&#22312;&#21508;&#31181;&#35748;&#30693;&#36807;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#27604;&#22914;&#33021;&#22815;&#22312;&#22836;&#33041;&#20013;&#22238;&#24518;&#36807;&#21435;&#20107;&#20214;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#35748;&#30693;&#31185;&#23398;&#24378;&#35843;&#31354;&#38388;&#19978;&#19979;&#25991;&#22312;&#24773;&#33410;&#35760;&#24518;&#30340;&#24418;&#25104;&#21644;&#26816;&#32034;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24403;&#21069;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#24773;&#33410;&#35760;&#24518;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#36890;&#36807;&#23384;&#20648;&#26102;&#38388;&#39034;&#24207;&#20307;&#39564;&#30340;&#21464;&#21387;&#22120;&#65292;&#36825;&#24573;&#30053;&#20102;&#31354;&#38388;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#23558;&#22522;&#30784;&#32467;&#26500;&#25193;&#23637;&#21040;&#38500;&#20102;&#20165;&#26377;&#26102;&#38388;&#39034;&#24207;&#20043;&#22806;&#30340;&#31354;&#38388;&#36724;&#65292;&#24182;&#30001;&#27492;&#33021;&#22815;&#33719;&#24471;&#21738;&#20123;&#22909;&#22788;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#32771;&#34385;&#26102;&#38388;&#21644;&#31354;&#38388;&#32500;&#24230;&#30340;&#22330;&#25152;&#20013;&#24515;&#24773;&#33410;&#35760;&#24518;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#65292;&#23548;&#33268;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15160v1 Announce Type: cross  Abstract: Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#35270;&#35282;&#25506;&#35752;&#20102;&#25945;&#32946;&#20013;&#19981;&#21516;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#21487;&#25509;&#21463;&#24615;&#65292;&#20851;&#27880;&#25968;&#25454;&#38544;&#31169;&#12289;AI&#20195;&#29702;&#12289;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36947;&#24503;&#37096;&#32626;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15027</link><description>&lt;p&gt;
&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#35270;&#35282;&#19979;&#30340;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#19982;&#25945;&#32946;&#21487;&#25509;&#21463;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-stakeholder Perspective on Responsible Artificial Intelligence and Acceptability in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#35270;&#35282;&#25506;&#35752;&#20102;&#25945;&#32946;&#20013;&#19981;&#21516;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#21487;&#25509;&#21463;&#24615;&#65292;&#20851;&#27880;&#25968;&#25454;&#38544;&#31169;&#12289;AI&#20195;&#29702;&#12289;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36947;&#24503;&#37096;&#32626;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35270;&#35282;&#65292;&#21253;&#25324;&#23398;&#29983;&#12289;&#25945;&#24072;&#21644;&#23478;&#38271;&#65292;&#35843;&#26597;&#20102;&#25945;&#32946;&#20013;&#19981;&#21516;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#30340;&#21487;&#25509;&#21463;&#24615;&#12290;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#20013;&#30340;&#21464;&#38761;&#28508;&#21147;&#65292;&#23427;&#20851;&#27880;&#20102;&#19982;&#25968;&#25454;&#38544;&#31169;&#12289;AI&#20195;&#29702;&#12289;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#36947;&#24503;&#37096;&#32626;&#26377;&#20851;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21019;&#26223;&#26041;&#27861;&#65292;&#21442;&#19982;&#32773;&#34987;&#21576;&#29616;&#20102;&#22235;&#20010;&#22330;&#26223;&#65292;&#20854;&#20013;AI&#30340;&#20195;&#29702;&#12289;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#38544;&#31169;&#36973;&#21040;&#25805;&#32437;&#12290;&#22312;&#27599;&#20010;&#22330;&#26223;&#21518;&#65292;&#21442;&#19982;&#32773;&#23436;&#25104;&#20102;&#19968;&#20010;&#35843;&#26597;&#38382;&#21367;&#65292;&#20854;&#20013;&#25429;&#25417;&#20102;&#20182;&#20204;&#23545;AI&#30340;&#20840;&#29699;&#25928;&#29992;&#12289;&#20010;&#20154;&#23454;&#29992;&#24615;&#12289;&#20844;&#27491;&#24615;&#12289;&#20449;&#24515;&#12289;&#39118;&#38505;&#20197;&#21450;&#33509;&#27599;&#20010;&#22330;&#26223;&#30340;AI&#21487;&#29992;&#35805;&#65292;&#20182;&#20204;&#25171;&#31639;&#20351;&#29992;&#30340;&#24847;&#22270;&#12290;&#25968;&#25454;&#25910;&#38598;&#28085;&#30422;&#20102;&#26368;&#32456;&#26679;&#26412;&#37327;&#20026;1198&#21517;&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#21442;&#19982;&#32773;&#65292;&#36890;&#36807;&#19968;&#20010;&#21512;&#20316;&#26426;&#26500;&#21644;&#31038;&#20132;&#23186;&#20307;&#27963;&#21160;&#20998;&#21457;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#23545;&#22235;&#20010;&#22330;&#26223;&#30340;&#20010;&#20307;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15027v1 Announce Type: cross  Abstract: This study investigates the acceptability of different artificial intelligence (AI) applications in education from a multi-stakeholder perspective, including students, teachers, and parents. Acknowledging the transformative potential of AI in education, it addresses concerns related to data privacy, AI agency, transparency, explainability and the ethical deployment of AI. Through a vignette methodology, participants were presented with four scenarios where AI's agency, transparency, explainability, and privacy were manipulated. After each scenario, participants completed a survey that captured their perceptions of AI's global utility, individual usefulness, justice, confidence, risk, and intention to use each scenario's AI if available. The data collection comprising a final sample of 1198 multi-stakeholder participants was distributed through a partner institution and social media campaigns and focused on individual responses to four 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24418;&#25104;RDTE&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#35299;&#20915;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26377;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.14798</link><description>&lt;p&gt;
&#21033;&#29992;&#38750;&#27491;&#24335;&#36923;&#36753;&#22686;&#24378;&#31995;&#32479;&#21270;&#20998;&#35299;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24418;&#25104;RDTE&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#35299;&#20915;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26377;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#20026;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20363;&#22914;&#22312;&#19981;&#20381;&#36182;&#33030;&#24369;&#30340;&#24418;&#24335;&#36923;&#36753;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#21644;&#35780;&#20272;&#30452;&#35266;&#30340;&#12289;&#31867;&#20284;&#35777;&#26126;&#30340;&#25991;&#26412;&#34164;&#28085;&#26641;&#12290;&#28982;&#32780;&#65292;&#27839;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#36827;&#23637;&#21463;&#21040;&#19968;&#20010;&#38271;&#26399;&#20197;&#26469;&#32570;&#20047;&#26126;&#30830;&#30340;&#30830;&#23450;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#28165;&#26224;&#21327;&#35758;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25968;&#25454;&#38598;RDTE (Recognizing Decompositional Textual Entailment) &#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#27604;&#20808;&#21069;&#30340;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#39640;&#24471;&#22810;&#65288;+9%&#65289;&#65292;&#34920;&#26126;RDTE&#22312;&#38271;&#26399;&#23384;&#22312;&#30340;&#20851;&#20110;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14798v1 Announce Type: cross  Abstract: Contemporary language models enable new opportunities for structured reasoning with text, such as the construction and evaluation of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what valid compositional entailment is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment datasets, and evaluate its impact on LLM-based textual inference. We find that our resulting dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency (+9%) than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of for
&lt;/p&gt;</description></item><item><title>OpenCodeInterpreter&#26159;&#19968;&#31181;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#25191;&#34892;&#12289;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#30340;&#21151;&#33021;&#65292;&#24182;&#22312;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#19982;GPT-4&#30456;&#23218;&#32654;&#12290;</title><link>https://arxiv.org/abs/2402.14658</link><description>&lt;p&gt;
OpenCodeInterpreter&#65306;&#38598;&#25104;&#20195;&#30721;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14658
&lt;/p&gt;
&lt;p&gt;
OpenCodeInterpreter&#26159;&#19968;&#31181;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#25191;&#34892;&#12289;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#30340;&#21151;&#33021;&#65292;&#24182;&#22312;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#19982;GPT-4&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24341;&#20837;&#26174;&#33879;&#25512;&#21160;&#20102;&#20195;&#30721;&#29983;&#25104;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#24320;&#28304;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#31867;&#20284;GPT-4 Code Interpreter&#36825;&#26679;&#30340;&#39640;&#32423;&#31995;&#32479;&#30340;&#25191;&#34892;&#33021;&#21147;&#21644;&#36845;&#20195;&#32454;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OpenCodeInterpreter&#65292;&#36825;&#26159;&#19968;&#26063;&#26088;&#22312;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#36845;&#20195;&#32454;&#21270;&#20195;&#30721;&#30340;&#24320;&#28304;&#20195;&#30721;&#31995;&#32479;&#12290;&#36890;&#36807;Code-Feedback&#25903;&#25345;&#65292;&#35813;&#31995;&#32479;&#38598;&#25104;&#20102;&#25191;&#34892;&#21644;&#20154;&#31867;&#21453;&#39304;&#65292;&#29992;&#20110;&#21160;&#24577;&#20195;&#30721;&#32454;&#21270;&#12290;&#25105;&#20204;&#23545;OpenCodeInterpreter&#22312;&#35832;&#22914;HumanEval&#12289;MBPP&#20197;&#21450;&#23427;&#20204;&#26469;&#33258;EvalPlus&#30340;&#22686;&#24378;&#29256;&#26412;&#31561;&#20851;&#38190;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#35777;&#23454;&#20102;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;OpenCodeInterpreter-33B&#22312;HumanEval&#21644;MBPP&#30340;&#24179;&#22343;&#20540;&#65288;&#20197;&#21450;&#20854;&#22686;&#24378;&#29256;&#26412;&#65289;&#19978;&#21462;&#24471;&#20102;83.2&#65288;76.4&#65289;&#30340;&#20934;&#30830;&#29575;&#65292;&#19982;GPT-4&#30340;84.2&#65288;76.2&#65289;&#32039;&#23494;&#21305;&#25932;&#65292;&#24182;&#19988;&#36890;&#36807;&#21512;&#25104;hum
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14658v1 Announce Type: cross  Abstract: The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized hum
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14095</link><description>&lt;p&gt;
&#36328;&#26550;&#26500;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-shot generalization across architectures for visual classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14095
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20854;&#19982;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#20851;&#31995;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26497;&#31616;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#27867;&#21270;&#24230;&#37327;&#65292;&#23637;&#31034;&#20102;&#20174;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65288;CNNs&#65289;&#21040;transformers&#30340;&#27969;&#34892;&#32593;&#32476;&#22312;&#36890;&#36807;&#23618;&#21644;&#26550;&#26500;&#27867;&#21270;&#21040;&#26410;&#35265;&#31867;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#12290;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#24182;&#19988;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/dyballa/zero-shot-generalization &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14095v1 Announce Type: cross  Abstract: Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear. Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep convolutional networks (CNNs) to transformers, vary in their power to extrapolate to unseen classes both across layers and across architectures. Accuracy is not a good predictor of generalizability, and generalization varies non-monotonically with layer depth. Code is available at https://github.com/dyballa/zero-shot-generalization.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13754</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning-assisted quantum architecture search for variational quantum algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13754
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22024;&#26434;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#65292;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#26159;&#30830;&#23450;&#21151;&#33021;&#24615;&#37327;&#23376;&#30005;&#36335;&#12290;&#36825;&#20123;&#30005;&#36335;&#24517;&#39035;&#21516;&#26102;&#31526;&#21512;&#24403;&#21069;&#37327;&#23376;&#30828;&#20214;&#38480;&#21046;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#12290;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQA&#65289;&#26159;&#19968;&#31867;&#37327;&#23376;-&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#21487;&#29992;&#37327;&#23376;&#35774;&#22791;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#35770;&#25991;&#20391;&#37325;&#20110;&#30005;&#36335;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20248;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;&#35770;&#25991;&#20869;&#36890;&#36807;&#35780;&#20272;&#30005;&#36335;&#30340;&#28145;&#24230;&#12289;&#38376;&#21644;&#21442;&#25968;&#30340;&#24635;&#25968;&#20197;&#21450;&#20934;&#30830;&#24615;&#26469;&#30830;&#23450;&#30005;&#36335;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13754v1 Announce Type: cross  Abstract: A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits. These circuits must also adhere to the constraints imposed by current quantum hardware limitations. Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices. However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function. Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL). Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;-free &#26041;&#27861; ToDo&#65292;&#36890;&#36807;&#20196;&#29260;&#19979;&#37319;&#26679;&#21152;&#36895; Stable Diffusion &#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#39640;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.13573</link><description>&lt;p&gt;
&#20219;&#21153;&#24453;&#21150;&#65306;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#39640;&#25928;&#29983;&#25104;&#30340;&#20196;&#29260;&#19979;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
ToDo: Token Downsampling for Efficient Generation of High-Resolution Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13573
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;-free &#26041;&#27861; ToDo&#65292;&#36890;&#36807;&#20196;&#29260;&#19979;&#37319;&#26679;&#21152;&#36895; Stable Diffusion &#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#39640;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#25105;&#20204;&#21487;&#20197;&#22312;&#21512;&#29702;&#26102;&#38388;&#21644;&#20869;&#23384;&#38480;&#21046;&#20869;&#22788;&#29702;&#30340;&#22270;&#20687;&#22823;&#23567;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#23494;&#38598;&#27880;&#24847;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21253;&#21547;&#20887;&#20313;&#29305;&#24449;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#31232;&#30095;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861; ToDo&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#20851;&#38190;&#21644;&#20540;&#20196;&#29260;&#30340;&#20196;&#29260;&#19979;&#37319;&#26679;&#65292;&#21487;&#23558;&#24120;&#35265;&#22823;&#23567;&#30340; Stable Diffusion &#25512;&#29702;&#21152;&#36895;&#33267;&#22810;&#36798;2&#20493;&#65292;&#23545;&#20110;2048x2048&#31561;&#39640;&#20998;&#36776;&#29575;&#65292;&#21152;&#36895;&#27604;&#21487;&#36798;4.5&#20493;&#25110;&#26356;&#39640;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24179;&#34913;&#39640;&#25928;&#21534;&#21520;&#37327;&#21644;&#20445;&#30495;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13573v1 Announce Type: cross  Abstract: Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38899;&#20048;&#29983;&#25104;&#30340;&#32467;&#26500;&#24863;&#30693;&#20301;&#32622;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#32477;&#23545;&#12289;&#30456;&#23545;&#21644;&#38750;&#24179;&#31283;&#20301;&#32622;&#20449;&#24687;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#29983;&#25104;&#20316;&#21697;&#30340;&#26059;&#24459;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13301</link><description>&lt;p&gt;
&#38024;&#23545;&#38899;&#20048;&#29983;&#25104;&#30340;&#32467;&#26500;&#24863;&#30693;&#20301;&#32622;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Structure-informed Positional Encoding for Music Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13301
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38899;&#20048;&#29983;&#25104;&#30340;&#32467;&#26500;&#24863;&#30693;&#20301;&#32622;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#32477;&#23545;&#12289;&#30456;&#23545;&#21644;&#38750;&#24179;&#31283;&#20301;&#32622;&#20449;&#24687;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#29983;&#25104;&#20316;&#21697;&#30340;&#26059;&#24459;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#38899;&#20048;&#24448;&#24448;&#32570;&#20047;&#36830;&#36143;&#24615;&#21644;&#38271;&#26399;&#32452;&#32455;&#65292;&#32780;&#22810;&#23610;&#24230;&#23618;&#27425;&#32467;&#26500;&#26159;&#38899;&#20048;&#20449;&#21495;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#19968;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;Transformer&#38899;&#20048;&#29983;&#25104;&#30340;&#32467;&#26500;&#24863;&#30693;&#20301;&#32622;&#32534;&#30721;&#26694;&#26550;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#21464;&#20307;&#65292;&#28041;&#21450;&#32477;&#23545;&#12289;&#30456;&#23545;&#21644;&#38750;&#24179;&#31283;&#20301;&#32622;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20219;&#21153;&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#27979;&#35797;&#65306;&#19979;&#19968;&#20010;&#26102;&#38388;&#27493;&#39044;&#27979;&#21644;&#20276;&#22863;&#29983;&#25104;&#12290;&#20316;&#20026;&#23545;&#27604;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#25991;&#29486;&#20013;&#30340;&#22810;&#20010;&#22522;&#32447;&#65292;&#20351;&#29992;&#20960;&#20010;&#20197;&#38899;&#20048;&#20026;&#21160;&#26426;&#30340;&#35780;&#20272;&#25351;&#26631;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29983;&#25104;&#20316;&#21697;&#30340;&#26059;&#24459;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13301v1 Announce Type: cross  Abstract: Music generated by deep learning methods often suffers from a lack of coherence and long-term organization. Yet, multi-scale hierarchical structure is a distinctive feature of music signals. To leverage this information, we propose a structure-informed positional encoding framework for music generation with Transformers. We design three variants in terms of absolute, relative and non-stationary positional information. We comprehensively test them on two symbolic music generation tasks: next-timestep prediction and accompaniment generation. As a comparison, we choose multiple baselines from the literature and demonstrate the merits of our methods using several musically-motivated evaluation metrics. In particular, our methods improve the melodic and structural consistency of the generated pieces.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#36807;&#31243;&#20013;&#30452;&#25509;&#24314;&#31435;&#20107;&#20214;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#22240;&#26524;&#36129;&#29486;&#30340;&#20004;&#20010;&#20851;&#38190;&#24341;&#29702;&#65292;&#21487;&#20197;&#25581;&#31034;&#21644;&#37327;&#21270;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.10240</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#38382;&#39064;&#30340;&#21160;&#24577;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Dynamical View of the Question of Why
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10240
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#36807;&#31243;&#20013;&#30452;&#25509;&#24314;&#31435;&#20107;&#20214;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#22240;&#26524;&#36129;&#29486;&#30340;&#20004;&#20010;&#20851;&#38190;&#24341;&#29702;&#65292;&#21487;&#20197;&#25581;&#31034;&#21644;&#37327;&#21270;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#30001;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#25512;&#29702;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#23616;&#38480;&#20110;&#38745;&#24577;&#35774;&#32622;&#65292;&#24573;&#30053;&#20102;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#24615;&#21644;&#21464;&#21270;&#30340;&#21457;&#23556;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#33539;&#24335;&#65292;&#30452;&#25509;&#22312;&#26102;&#38388;&#36807;&#31243;&#20013;&#24314;&#31435;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#24341;&#29702;&#26469;&#35745;&#31639;&#22240;&#26524;&#36129;&#29486;&#65292;&#24182;&#23558;&#20854;&#26500;&#36896;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#25581;&#31034;&#21644;&#37327;&#21270;&#25193;&#25955;&#36807;&#31243;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#24418;&#24335;&#21270;&#21644;&#35745;&#31639;&#24037;&#20855;&#65292;&#21253;&#25324;&#21508;&#31181;&#37325;&#35201;&#35774;&#32622;&#65292;&#22914;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#30456;&#24403;&#22797;&#26434;&#30340;&#23454;&#39564;&#21644;&#36890;&#36807;&#32431;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25581;&#31034;&#21644;&#37327;&#21270;&#20102;&#22240;&#26524;&#32852;&#31995;&#65292;&#21542;&#21017;&#30475;&#20284;&#33707;&#21517;&#20854;&#22937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10240v1 Announce Type: cross  Abstract: We address causal reasoning in multivariate time series data generated by stochastic processes. Existing approaches are largely restricted to static settings, ignoring the continuity and emission of variations across time. In contrast, we propose a learning paradigm that directly establishes causation between events in the course of time. We present two key lemmas to compute causal contributions and frame them as reinforcement learning problems. Our approach offers formal and computational tools for uncovering and quantifying causal relationships in diffusion processes, subsuming various important settings such as discrete-time Markov decision processes. Finally, in fairly intricate experiments and through sheer learning, our framework reveals and quantifies causal links, which otherwise seem inexplicable.
&lt;/p&gt;</description></item><item><title>HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10228</link><description>&lt;p&gt;
HyperAgent&#65306;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#29992;&#20110;&#22797;&#26434;&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10228
&lt;/p&gt;
&lt;p&gt;
HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#19981;&#26029;&#31215;&#32047;&#30340;&#20132;&#20114;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyperAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36229;&#27169;&#22411;&#12289;&#32034;&#24341;&#25277;&#26679;&#26041;&#26696;&#21644;&#22686;&#37327;&#26356;&#26032;&#26426;&#21046;&#30340;RL&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#20013;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#39034;&#24207;&#21518;&#39564;&#36924;&#36817;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#36229;&#36234;&#20102;&#20849;&#36717;&#24615;&#12290;HyperAgent&#30340;&#23454;&#29616;&#31616;&#21333;&#65292;&#21482;&#38656;&#35201;&#22312;DDQN&#20013;&#28155;&#21152;&#19968;&#20010;&#27169;&#22359;&#21644;&#19968;&#34892;&#39069;&#22806;&#20195;&#30721;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;HyperAgent&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#25968;&#25454;&#36824;&#26159;&#35745;&#31639;&#26041;&#38754;&#37117;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#22312;&#23454;&#38469;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#20013;&#65292;HyperAgent&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
&lt;/p&gt;</description></item><item><title>"GraphTranslator"&#26159;&#19968;&#20010;&#26088;&#22312;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#32763;&#35793;&#22120;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#39044;&#23450;&#20041;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#36825;&#20004;&#31181;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#26356;&#20855;&#21019;&#26032;&#24615;&#21644;&#28789;&#27963;&#24615;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.07197</link><description>&lt;p&gt;
GraphTranslator&#65306;&#23558;&#22270;&#27169;&#22411;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#29992;&#20110;&#24320;&#25918;&#24335;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07197
&lt;/p&gt;
&lt;p&gt;
"GraphTranslator"&#26159;&#19968;&#20010;&#26088;&#22312;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#32763;&#35793;&#22120;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#39044;&#23450;&#20041;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#36825;&#20004;&#31181;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#26356;&#20855;&#21019;&#26032;&#24615;&#21644;&#28789;&#27963;&#24615;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#65292;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#21457;&#20102;&#19968;&#22330;&#38761;&#21629;&#24615;&#30340;&#36716;&#21464;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#24320;&#25918;&#24335;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#22270;&#39046;&#22495;&#20013;&#36825;&#20010;&#24819;&#27861;&#36739;&#23569;&#34987;&#25506;&#32034;&#65292;&#23613;&#31649;&#26377;&#35768;&#22810;&#24378;&#22823;&#30340;&#22270;&#27169;&#22411;&#65288;GMs&#65289;&#21487;&#29992;&#65292;&#20294;&#23427;&#20204;&#34987;&#38480;&#21046;&#22312;&#39044;&#23450;&#20041;&#24418;&#24335;&#30340;&#20219;&#21153;&#20013;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#23558;LLMs&#24212;&#29992;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#21516;&#26102;&#22788;&#29702;&#39044;&#23450;&#20041;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#20219;&#21153;&#65292;&#26080;&#35770;&#26159;&#23558;LLMs&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#22120;&#36824;&#26159;&#20316;&#20026;&#29420;&#31435;&#39044;&#27979;&#22120;&#12290;&#20026;&#20102;&#25171;&#30772;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphTranslator&#30340;&#32763;&#35793;&#22120;&#65292;&#26088;&#22312;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;GM&#21644;LLMs&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#39044;&#23450;&#20041;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;LLMs&#30340;&#25193;&#23637;&#25509;&#21475;&#20026;GM&#25552;&#20379;&#21508;&#31181;&#24320;&#25918;&#24335;&#20219;&#21153;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#30340;&#32763;&#35793;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Producer&#30340;&#26500;&#24314;&#22270;&#25991;&#23545;&#40784;&#25968;&#25454;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse research fields of artificial intelligence, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.05119</link><description>&lt;p&gt;
&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Limitations of Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#65288;IT&#65289;&#26159;&#20351;&#29992;&#25351;&#20196;-&#22238;&#24212;&#23545;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36807;&#31243;&#65292;&#24050;&#25104;&#20026;&#23558;&#22522;&#30784;&#39044;&#35757;&#32451;LLM&#36716;&#21270;&#20026;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#34429;&#28982;IT&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#24182;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#23616;&#38480;&#24615;&#21644;&#19981;&#36275;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#23545;LLM&#36890;&#36807;IT&#21457;&#29983;&#30340;&#21464;&#21270;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;IT&#30340;&#22810;&#31181;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;IT&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#25110;&#25216;&#33021;&#12290;LoRA&#24494;&#35843;&#20165;&#38480;&#20110;&#23398;&#20064;&#22238;&#24212;&#30340;&#21551;&#21160;&#21644;&#26679;&#24335;&#20196;&#29260;&#65292;&#32780;&#20840;&#21442;&#25968;&#24494;&#35843;&#20250;&#23548;&#33268;&#30693;&#35782;&#36864;&#21270;&#12290;&#65288;2&#65289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;IT&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#20250;&#23548;&#33268;&#22238;&#24212;&#36136;&#37327;&#19979;&#38477;&#12290;&#65288;3&#65289;&#20840;&#21442;&#25968;&#24494;&#35843;&#36890;&#36807;&#19981;&#20934;&#30830;&#22320;&#20174;IT&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#27010;&#24565;&#19978;&#30456;&#20284;&#23454;&#20363;&#30340;&#26631;&#35760;&#65292;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;LS-MCPP&#65292;&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#30452;&#25509;&#22312;&#20998;&#35299;&#22270;&#19978;&#25805;&#20316;&#65292;&#20197;&#25506;&#32034;&#22914;&#20309;&#31995;&#32479;&#22320;&#25628;&#32034;&#22320;&#24418;&#22270;&#19978;&#30340;&#33391;&#22909;&#35206;&#30422;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2312.10797</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#26426;&#22120;&#20154;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#30340;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Multi-Robot Coverage Path Planning via Local Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;LS-MCPP&#65292;&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#30452;&#25509;&#22312;&#20998;&#35299;&#22270;&#19978;&#25805;&#20316;&#65292;&#20197;&#25506;&#32034;&#22914;&#20309;&#31995;&#32479;&#22320;&#25628;&#32034;&#22320;&#24418;&#22270;&#19978;&#30340;&#33391;&#22909;&#35206;&#30422;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#30340;&#22810;&#26426;&#22120;&#20154;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#65288;MCPP&#65289;&#65292;&#26088;&#22312;&#20026;&#22810;&#20010;&#26426;&#22120;&#20154;&#35745;&#31639;&#35206;&#30422;&#32473;&#23450;2D&#32593;&#26684;&#22320;&#24418;&#22270;$G$&#30340;&#25152;&#26377;&#39030;&#28857;&#30340;&#35206;&#30422;&#36335;&#24452;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#30340;MCPP&#31639;&#27861;&#39318;&#20808;&#22312;$G$&#19978;&#35745;&#31639;&#26641;&#35206;&#30422;&#65292;&#21363;&#35206;&#30422;&#25152;&#26377;&#39030;&#28857;&#30340;&#22810;&#26869;&#26641;&#65292;&#28982;&#21518;&#37319;&#29992;&#29983;&#25104;&#35206;&#30422;&#36335;&#24452;&#30340;&#36328;&#36234;&#26641;&#35206;&#30422;&#65288;STC&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#32469;&#34892;&#35745;&#31639;&#26641;&#30340;&#36793;&#22312;&#22320;&#24418;&#22270;$G$&#30340;&#20998;&#35299;&#22270;$D$&#19978;&#29983;&#25104;&#35206;&#30422;&#36335;&#24452;&#65292;&#26088;&#22312;&#20248;&#21270;&#26368;&#22823;&#35206;&#30422;&#36335;&#24452;&#25104;&#26412;&#65288;&#21363;&#25152;&#26377;&#26426;&#22120;&#20154;&#20013;&#30340;&#26368;&#22823;&#35206;&#30422;&#36335;&#24452;&#25104;&#26412;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10797v2 Announce Type: replace-cross  Abstract: We study graph-based Multi-Robot Coverage Path Planning (MCPP) that aims to compute coverage paths for multiple robots to cover all vertices of a given 2D grid terrain graph $G$. Existing graph-based MCPP algorithms first compute a tree cover on $G$ -- a forest of multiple trees that cover all vertices -- and then employ the Spanning Tree Coverage (STC) paradigm to generate coverage paths on the decomposed graph $D$ of the terrain graph $G$ by circumnavigating the edges of the computed trees, aiming to optimize the makespan (i.e., the maximum coverage path cost among all robots). In this paper, we take a different approach by exploring how to systematically search for good coverage paths directly on $D$. We introduce a new algorithmic framework, called LS-MCPP, which leverages a local search to operate directly on $D$. We propose a novel standalone paradigm, Extended-STC (ESTC), that extends STC to achieve complete coverage for
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#65292;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.05440</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#21644;&#24555;&#36895;&#27169;&#25311;&#25512;&#26029;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Consistency Models for Scalable and Fast Simulation-Based Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05440
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#65292;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#25512;&#26029;&#65288;SBI&#65289;&#19981;&#26029;&#23547;&#25214;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#31639;&#27861;&#65292;&#20197;&#20934;&#30830;&#25512;&#26029;&#22797;&#26434;&#27169;&#22411;&#30340;&#21442;&#25968;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CMPE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#30340;&#26032;&#33258;&#30001;&#24418;&#24335;&#26465;&#20214;&#37319;&#26679;&#22120;&#65292;&#21033;&#29992;&#29983;&#25104;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;CMPE&#23558;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#21040;&#21333;&#20010;&#29983;&#25104;&#26550;&#26500;&#20013;&#65306;&#23427;&#26412;&#36136;&#19978;&#25552;&#28860;&#20102;&#36830;&#32493;&#27010;&#29575;&#27969;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#26080;&#32422;&#26463;&#30340;&#32467;&#26500;&#24555;&#36895;&#36827;&#34892;&#23569;&#23556;&#25512;&#26029;&#65292;&#35813;&#32467;&#26500;&#21487;&#20197;&#23450;&#21046;&#21040;&#20272;&#35745;&#38382;&#39064;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;CMPE&#19981;&#20165;&#22312;&#19977;&#20010;&#22256;&#38590;&#30340;&#20302;&#32500;&#38382;&#39064;&#19978;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#65292;&#32780;&#19988;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#21435;&#22122;&#23454;&#39564;&#21644;&#20272;&#35745;&#35745;&#31639;&#23494;&#38598;&#22411;&#22810;&#23610;&#24230;&#20013;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05440v2 Announce Type: replace-cross  Abstract: Simulation-based inference (SBI) is constantly in search of more expressive algorithms for accurately inferring the parameters of complex models from noisy data. We present consistency models for neural posterior estimation (CMPE), a new free-form conditional sampler for scalable, fast, and amortized SBI with generative neural networks. CMPE combines the advantages of normalizing flows and flow matching methods into a single generative architecture: It essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be tailored to the structure of the estimation problem. Our empirical evaluation demonstrates that CMPE not only outperforms current state-of-the-art algorithms on three hard low-dimensional problems but also achieves competitive performance in a high-dimensional Bayesian denoising experiment and in estimating a computationally demanding multi-scale 
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#29983;&#25104;&#39044;&#22823;&#23398;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20026;&#22635;&#34917;&#29616;&#26377;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#36275;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2312.01661</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#25968;&#23398;&#25552;&#38382;&#32773;&#65311;&#35780;&#20272;ChatGPT&#22312;&#29983;&#25104;&#39044;&#22823;&#23398;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01661
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#29983;&#25104;&#39044;&#22823;&#23398;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20026;&#22635;&#34917;&#29616;&#26377;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#36275;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25552;&#38382;&#23545;&#20110;&#35780;&#20272;&#23398;&#29983;&#30340;&#35299;&#20915;&#38382;&#39064;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#25163;&#21160;&#21019;&#24314;&#36825;&#26679;&#30340;&#38382;&#39064;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#65292;&#22240;&#27492;&#20154;&#20204;&#24050;&#32463;&#25506;&#32034;&#20102;&#33258;&#21160;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20381;&#36182;&#20110;&#24494;&#35843;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#28041;&#21450;&#22810;&#27493;&#36923;&#36753;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;&#38382;&#39064;&#26102;&#24456;&#38590;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35832;&#22914;ChatGPT&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#28041;&#21450;&#36923;&#36753;&#21644;&#31639;&#26415;&#25512;&#29702;&#30340;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#25104;&#25945;&#32946;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#23398;&#39046;&#22495;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#65292;&#23545;ChatGPT&#22312;&#29983;&#25104;&#39044;&#22823;&#23398;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#35774;&#32622;&#65306;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#19981;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01661v2 Announce Type: replace-cross  Abstract: Mathematical questioning is crucial for assessing students problem-solving skills. Since manually creating such questions requires substantial effort, automatic methods have been explored. Existing state-of-the-art models rely on fine-tuning strategies and struggle to generate questions that heavily involve multiple steps of logical and arithmetic reasoning. Meanwhile, large language models(LLMs) such as ChatGPT have excelled in many NLP tasks involving logical and arithmetic reasoning. Nonetheless, their applications in generating educational questions are underutilized, especially in the field of mathematics. To bridge this gap, we take the first step to conduct an in-depth analysis of ChatGPT in generating pre-university math questions. Our analysis is categorized into two main settings: context-aware and context-unaware. In the context-aware setting, we evaluate ChatGPT on existing math question-answering benchmarks coverin
&lt;/p&gt;</description></item><item><title>TFMQ-DM&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Temporal Feature Maintenance Quantization (TFMQ)&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.16503</link><description>&lt;p&gt;
TFMQ-DM&#65306;&#38754;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#29305;&#24449;&#32500;&#25345;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16503
&lt;/p&gt;
&lt;p&gt;
TFMQ-DM&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Temporal Feature Maintenance Quantization (TFMQ)&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16503v2 &#36890;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449; &#25688;&#35201;&#65306;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#20294;&#30001;&#20110;&#20854;&#36739;&#38271;&#30340;&#25512;&#29702;&#26102;&#38388;&#21644;&#22823;&#37327;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#22312;&#24191;&#27867;&#36866;&#29992;&#24615;&#26041;&#38754;&#36935;&#21040;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#39640;&#25928;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#65292;&#25193;&#25955;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#26102;&#38388;&#27493;&#38271; $t$ &#26469;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#22810;&#36718;&#21435;&#22122;&#12290;&#36890;&#24120;&#65292;&#20174;&#26377;&#38480;&#38598;&#21512; $\{1, \ldots, T\}$ &#20013;&#30340; $t$&#20250;&#34987;&#20960;&#20010;&#27169;&#22359;&#32534;&#30721;&#20026;&#19968;&#20010;&#26102;&#38388;&#29305;&#24449;&#65292;&#36825;&#23436;&#20840;&#19981;&#32771;&#34385;&#37319;&#26679;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#24182;&#19981;&#20998;&#21035;&#20248;&#21270;&#36825;&#20123;&#27169;&#22359;&#12290;&#23427;&#20204;&#37319;&#29992;&#19981;&#24688;&#24403;&#30340;&#37325;&#26500;&#30446;&#26631;&#21644;&#22797;&#26434;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#23548;&#33268;&#26102;&#38388;&#29305;&#24449;&#21644;&#21435;&#22122;&#36712;&#36857;&#20005;&#37325;&#21463;&#21040;&#24178;&#25200;&#65292;&#21516;&#26102;&#21387;&#32553;&#25928;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Temporal Feature Maintenance Quantization (TFMQ)&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16503v2 Announce Type: replace-cross  Abstract: The Diffusion model, a prevalent framework for image generation, encounters significant challenges in terms of broad applicability due to its extended inference times and substantial memory requirements. Efficient Post-training Quantization (PTQ) is pivotal for addressing these issues in traditional models. Different from traditional models, diffusion models heavily depend on the time-step $t$ to achieve satisfactory multi-round denoising. Usually, $t$ from the finite set $\{1, \ldots, T\}$ is encoded to a temporal feature by a few modules totally irrespective of the sampling data. However, existing PTQ methods do not optimize these modules separately. They adopt inappropriate reconstruction targets and complex calibration methods, resulting in a severe disturbance of the temporal feature and denoising trajectory, as well as a low compression efficiency. To solve these, we propose a Temporal Feature Maintenance Quantization (TF
&lt;/p&gt;</description></item><item><title>MetaCloak&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#26694;&#26550;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#39069;&#22806;&#36716;&#25442;&#25277;&#26679;&#36807;&#31243;&#26469;&#21046;&#36896;&#21487;&#36716;&#31227;&#21644;&#40065;&#26834;&#30340;&#25200;&#21160;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.13127</link><description>&lt;p&gt;
&#38024;&#23545;&#26410;&#32463;&#25480;&#26435;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#21512;&#25104;&#30340;&#40065;&#26834;&#24615;&#19981;&#21487;&#23519;&#35273;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Toward Robust Imperceptible Perturbation against Unauthorized Text-to-image Diffusion-based Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13127
&lt;/p&gt;
&lt;p&gt;
MetaCloak&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#26694;&#26550;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#39069;&#22806;&#36716;&#25442;&#25277;&#26679;&#36807;&#31243;&#26469;&#21046;&#36896;&#21487;&#36716;&#31227;&#21644;&#40065;&#26834;&#30340;&#25200;&#21160;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13127v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#20195;&#20132;&#21449; &#25688;&#35201;: &#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20801;&#35768;&#20174;&#23569;&#37327;&#21442;&#32771;&#29031;&#29255;&#26080;&#32541;&#29983;&#25104;&#20010;&#24615;&#21270;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#22914;&#26524;&#33853;&#20837;&#38169;&#35823;&#30340;&#25163;&#20013;&#65292;&#21487;&#33021;&#21046;&#36896;&#35823;&#23548;&#24615;&#25110;&#26377;&#23475;&#20869;&#23481;&#65292;&#21361;&#23475;&#20010;&#20154;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25237;&#27602;&#30340;&#26041;&#27861;&#20197;&#19968;&#31181;&#19981;&#21487;&#23519;&#35273;&#30340;&#26041;&#24335;&#25200;&#21160;&#29992;&#25143;&#22270;&#20687;&#65292;&#20197;&#20351;&#20854;&#26080;&#27861;&#34987;&#24694;&#24847;&#20351;&#29992;&#32773;&#23398;&#20064;&#12290;&#25105;&#20204;&#30830;&#23450;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;i) &#30001;&#20110;&#25163;&#24037;&#21551;&#21457;&#24335;&#35299;&#20915;&#38590;&#35299;&#21452;&#23618;&#20248;&#21270;&#32780;&#23548;&#33268;&#27425;&#20248;&#65307;ii) &#32570;&#20047;&#23545;&#31616;&#21333;&#25968;&#25454;&#36716;&#25442;&#65288;&#22914;&#39640;&#26031;&#28388;&#27874;&#65289;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaCloak&#65292;&#23427;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#35299;&#20915;&#21452;&#32423;&#25237;&#27602;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#39069;&#22806;&#30340;&#36716;&#25442;&#25277;&#26679;&#36807;&#31243;&#26469;&#21046;&#36896;&#21487;&#36716;&#31227;&#21644;&#40065;&#26834;&#30340;&#25200;&#21160;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#32452;&#26367;&#20195;&#25193;&#25955;&#27169;&#22411;&#26469;&#21046;&#36896;&#21487;&#36716;&#31227;&#21644;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13127v2 Announce Type: replace-cross  Abstract: Text-to-image diffusion models allow seamless generation of personalized images from scant reference photos. Yet, these tools, in the wrong hands, can fabricate misleading or harmful content, endangering individuals. To address this problem, existing poisoning-based approaches perturb user images in an imperceptible way to render them "unlearnable" from malicious uses. We identify two limitations of these defending approaches: i) sub-optimal due to the hand-crafted heuristics for solving the intractable bilevel optimization and ii) lack of robustness against simple data transformations like Gaussian filtering. To solve these challenges, we propose MetaCloak, which solves the bi-level poisoning problem with a meta-learning framework with an additional transformation sampling process to craft transferable and robust perturbation. Specifically, we employ a pool of surrogate diffusion models to craft transferable and model-agnostic
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22522;&#30784;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#38024;&#23545;&#24615;&#24494;&#35843;&#65292;&#29978;&#33267;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#21319;&#20102;&#30456;&#20851;&#27861;&#24459;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2311.09693</link><description>&lt;p&gt;
BLT: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22788;&#29702;&#22522;&#30784;&#27861;&#24459;&#25991;&#26412;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
BLT: Can Large Language Models Handle Basic Legal Text?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09693
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22522;&#30784;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#38024;&#23545;&#24615;&#24494;&#35843;&#65292;&#29978;&#33267;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#21319;&#20102;&#30456;&#20851;&#27861;&#24459;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#20687;GPT-4&#12289;Claude&#21644;{PaLM 2}&#36825;&#26679;&#30340;&#26368;&#22909;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#22312;&#22788;&#29702;&#22522;&#30784;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#24459;&#24072;&#21644;&#27861;&#24459;&#21161;&#29702;&#26399;&#26395;LLM&#38646;-shot&#22788;&#29702;&#30340;&#20219;&#21153;&#65292;&#27604;&#22914;&#26597;&#25214;&#35777;&#35789;&#25991;&#20214;&#30340;&#26576;&#19968;&#34892;&#25110;&#21512;&#21516;&#30340;&#26576;&#20010;&#23376;&#37096;&#20998;&#30340;&#25991;&#26412;&#12290;LLM&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#24046;&#21170;&#34920;&#29616;&#23545;&#23427;&#20204;&#22312;&#27861;&#24459;&#23454;&#36341;&#20013;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#36825;&#20123;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#29978;&#33267;&#20351;&#19968;&#20010;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#25509;&#36817;&#23436;&#32654;&#65292;&#24182;&#19988;&#36824;&#25552;&#21319;&#20102;&#30456;&#20851;&#27861;&#24459;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#35768;&#22810;&#39046;&#22495;&#25152;&#38656;&#30340;&#31616;&#21333;&#34892;&#20026;&#22312;&#22522;&#30784;LLM&#20013;&#21487;&#33021;&#19981;&#23384;&#22312;&#65292;&#38500;&#38750;&#26377;&#39046;&#22495;&#19987;&#23478;&#30340;&#39069;&#22806;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09693v2 Announce Type: replace-cross  Abstract: We find that the best publicly available LLMs like GPT-4, Claude, and {PaLM 2} currently perform poorly at basic legal text handling. We introduce a benchmark consisting of tasks that lawyers and paralegals would expect LLMs to handle zero-shot, such as looking up the text at a line of a witness deposition or at a subsection of a contract. LLMs' poor performance on this benchmark casts into doubt their reliability as-is for legal practice. However, fine-tuning for these tasks brings even a smaller model to near-perfect performance on our test set and also raises performance on a related legal task. These results suggest that many simple behaviors needed for a domain may not be present in foundational LLMs, without additional engagement from subject matter experts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32570;&#22833;&#19981;&#26159;&#38543;&#26426;&#30340;&#25968;&#25454;&#38598;&#19982;&#38543;&#26426;&#32570;&#22833;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#23545;&#24863;&#20852;&#36259;&#21442;&#25968;&#30340;&#35782;&#21035;&#21644;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2311.09015</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#38543;&#26426;&#32570;&#22833;&#25968;&#25454;&#30340;&#35782;&#21035;&#21644;&#20272;&#35745;&#65306;&#19968;&#31181;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32570;&#22833;&#19981;&#26159;&#38543;&#26426;&#30340;&#25968;&#25454;&#38598;&#19982;&#38543;&#26426;&#32570;&#22833;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#23545;&#24863;&#20852;&#36259;&#21442;&#25968;&#30340;&#35782;&#21035;&#21644;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#25968;&#25454;&#32570;&#22833;&#19981;&#26159;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#21644;&#20272;&#35745;&#24863;&#20852;&#36259;&#21442;&#25968;&#30340;&#20219;&#21153;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36825;&#31181;&#21442;&#25968;&#22312;&#27809;&#26377;&#23545;&#32570;&#22833;&#25968;&#25454;&#27169;&#22411;&#20570;&#20986;&#24378;&#28872;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#35782;&#21035;&#30340;&#12290;&#26412;&#25991;&#37319;&#29992;&#19968;&#31181;&#21463;&#25968;&#25454;&#34701;&#21512;&#21551;&#21457;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#22312;&#32570;&#22833;&#19981;&#26159;&#38543;&#26426;&#30340;&#25968;&#25454;&#38598;&#20013;&#22686;&#21152;&#30001;&#38543;&#26426;&#32570;&#22833;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#21333;&#29420;&#30340;&#25968;&#25454;&#38598;&#20013;&#26080;&#27861;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#21442;&#25968;&#65292;&#36890;&#36807;&#27719;&#38598;&#25968;&#25454;&#65292;&#22312;&#20004;&#32452;&#20114;&#34917;&#30340;&#20551;&#35774;&#19979;&#21487;&#20197;&#35782;&#21035;&#21442;&#25968;&#12290;&#25105;&#20204;&#20026;&#24050;&#30830;&#23450;&#30340;&#21442;&#25968;&#25512;&#23548;&#20102;&#19968;&#31181;&#21453;&#21521;&#27010;&#29575;&#21152;&#26435;&#65288;IPW&#65289;&#20272;&#35745;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#21644;&#25968;&#25454;&#24212;&#29992;&#35780;&#20272;&#20102;&#25105;&#20204;&#20272;&#35745;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09015v2 Announce Type: replace-cross  Abstract: We consider the task of identifying and estimating a parameter of interest in settings where data is missing not at random (MNAR). In general, such parameters are not identified without strong assumptions on the missing data model. In this paper, we take an alternative approach and introduce a method inspired by data fusion, where information in an MNAR dataset is augmented by information in an auxiliary dataset subject to missingness at random (MAR). We show that even if the parameter of interest cannot be identified given either dataset alone, it can be identified given pooled data, under two complementary sets of assumptions. We derive an inverse probability weighted (IPW) estimator for identified parameters, and evaluate the performance of our estimation strategies via simulation studies, and a data application.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LINK&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#31995;&#32479;&#24615;&#22320;&#29983;&#25104;&#38271;&#23614;&#25512;&#29702;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#25512;&#29702;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2311.07237</link><description>&lt;p&gt;
&#22312;&#25628;&#32034;&#38271;&#23614;&#20013;&#65306;&#36890;&#36807;&#36923;&#36753;&#35268;&#21017;&#24341;&#23548;&#25628;&#32034;&#31995;&#32479;&#24615;&#29983;&#25104;&#38271;&#23614;&#25512;&#29702;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LINK&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#31995;&#32479;&#24615;&#22320;&#29983;&#25104;&#38271;&#23614;&#25512;&#29702;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#25512;&#29702;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#35832;&#22914;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#25512;&#29702;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#12290;&#26368;&#36817;&#35780;&#20272;LLMs&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#22312;&#26469;&#33258;&#20302;&#27010;&#29575;&#20998;&#24067;&#8212;&#8212;&#21363;&#38271;&#23614;&#30340;&#36755;&#20837;&#25968;&#25454;&#19978;&#34920;&#29616;&#22823;&#24133;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#31995;&#32479;&#29983;&#25104;&#28041;&#21450;&#38271;&#23614;&#25512;&#29702;&#30693;&#35782;&#30340;&#35821;&#21477;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#25512;&#29702;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;Logic-Induced-Knowledge-Search&#65288;LINK&#65289;&#65292;&#35813;&#26694;&#26550;&#29983;&#25104;&#22522;&#20110;&#31526;&#21495;&#35268;&#21017;&#27169;&#26495;&#30340;&#20107;&#23454;&#27491;&#30830;&#19988;&#38271;&#23614;&#30693;&#35782;&#35821;&#21477;&#65307;LINK&#26377;&#25928;&#22320;&#29983;&#25104;&#38271;&#23614;&#20998;&#24067;&#25968;&#25454;&#65292;&#38646;-shot&#25552;&#31034;&#30340;LLMs&#26080;&#27861;&#21040;&#36798;&#65292;&#24182;&#19988;&#22312;&#20107;&#23454;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#38646;-shot GPT4&#36798;&#21040;5%&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;LINK&#29983;&#25104;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Logic-Induced-Long-Tail&#65288;LINT&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#38271;&#23614;&#20998;&#24067;&#19978;&#30340;&#19979;&#28216;&#27169;&#22411;&#65307;LINT&#21253;&#21547;108K&#20010;&#30693;&#35782;&#26465;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07237v2 Announce Type: replace-cross  Abstract: State-of-the-art LLMs outperform humans on reasoning tasks such as Natural Language Inference. Recent works evaluating LLMs note a marked performance drop on input data from the low-probability distribution, i.e., the longtail. Therefore, we focus on systematically generating statements involving long-tail inferential knowledge for more effective evaluation of LLMs in the reasoning space. We first propose a novel framework Logic-Induced- Knowledge-Search (LINK) that generates factually correct and long-tail knowledge statements grounded on symbolic rule templates; LINK effectively generates data in the longtail distribution that zero-shot prompted LLMs are unable to reach, and outperforms zero-shot GPT4 on factual correctness by 5%. We further use the data generated by LINK to construct a dataset Logic-Induced-Long-Tail (LINT) that can be used to evaluate downstream models on the long-tail distribution; LINT contains 108K knowl
&lt;/p&gt;</description></item><item><title>&#22823;&#35268;&#27169;&#36712;&#36857;&#27169;&#22411;&#65288;LTMs&#65289;&#37319;&#29992;State Transformer (STR)&#27169;&#22411;&#65292;&#23558;&#36816;&#21160;&#39044;&#27979;&#21644;&#35268;&#21010;&#38382;&#39064;&#32479;&#19968;&#24314;&#27169;&#65292;&#26377;&#25928;&#24212;&#23545;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2310.19620</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#36712;&#36857;&#27169;&#22411;&#26159;&#21487;&#25193;&#23637;&#30340;&#36816;&#21160;&#39044;&#27979;&#21644;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Trajectory Models are Scalable Motion Predictors and Planners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19620
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#36712;&#36857;&#27169;&#22411;&#65288;LTMs&#65289;&#37319;&#29992;State Transformer (STR)&#27169;&#22411;&#65292;&#23558;&#36816;&#21160;&#39044;&#27979;&#21644;&#35268;&#21010;&#38382;&#39064;&#32479;&#19968;&#24314;&#27169;&#65292;&#26377;&#25928;&#24212;&#23545;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#39044;&#27979;&#21644;&#35268;&#21010;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#36716;&#21521;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#38754;&#20020;&#30340;&#25361;&#25112;&#21253;&#25324;&#29702;&#35299;&#22810;&#26679;&#21270;&#30340;&#36947;&#36335;&#25299;&#25169;&#65292;&#25512;&#29702;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#20132;&#36890;&#21160;&#24577;&#65292;&#35299;&#37322;&#24322;&#36136;&#34892;&#20026;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#29983;&#25104;&#31574;&#30053;&#12290;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#27169;&#22411;&#25193;&#23637;&#35299;&#20915;&#31867;&#20284;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#36712;&#36857;&#27169;&#22411; State Transformer (STR)&#12290;STR&#36890;&#36807;&#23558;&#35266;&#23519;&#12289;&#29366;&#24577;&#21644;&#21160;&#20316;&#25490;&#21015;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#26469;&#37325;&#26032;&#23450;&#20041;&#36816;&#21160;&#39044;&#27979;&#21644;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#36712;&#36857;&#29983;&#25104;&#38382;&#39064;&#19982;&#20854;&#20182;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#32467;&#21512;&#36215;&#26469;&#65292;&#20511;&#21161;&#39046;&#22495;&#38388;&#30340;&#31361;&#30772;&#24615;&#36827;&#23637;&#65288;&#22914;&#35821;&#35328;&#24314;&#27169;&#65289;&#65292;&#23454;&#29616;&#24555;&#36895;&#36845;&#20195;&#12290;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#35268;&#27169;&#36712;&#36857;&#27169;&#22411;&#65288;&#22914;STR&#65289;&#36981;&#24490;&#25193;&#23637;&#23450;&#24459;&#65292;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
Motion prediction and planning are vital tasks in autonomous driving, and recent efforts have shifted to machine learning-based approaches. The challenges include understanding diverse road topologies, reasoning traffic dynamics over a long time horizon, interpreting heterogeneous behaviors, and generating policies in a large continuous state space. Inspired by the success of large language models in addressing similar complexities through model scaling, we introduce a scalable trajectory model called State Transformer (STR). STR reformulates the motion prediction and motion planning problems by arranging observations, states, and actions into one unified sequence modeling task. Our approach unites trajectory generation problems with other sequence modeling problems, powering rapid iterations with breakthroughs in neighbor domains such as language modeling. Remarkably, experimental results reveal that large trajectory models (LTMs), such as STR, adhere to the scaling laws by presenting
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;UWB&#30340;&#38745;&#24577;&#25163;&#21183;&#35782;&#21035;&#30340;&#31283;&#20581;&#26694;&#26550;&#65292;&#32463;&#36807;&#25968;&#25454;&#38598;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#35757;&#32451;&#27169;&#22411;&#36798;&#21040;96.78%&#20934;&#30830;&#29575;&#65292;&#24182;&#24320;&#21457;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;GUI&#26694;&#26550;&#65292;&#20026;&#23558;UWB&#25216;&#26415;&#24212;&#29992;&#20110;&#38745;&#24577;&#25163;&#21183;&#35782;&#21035;&#24102;&#26469;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2310.15036</link><description>&lt;p&gt;
&#22522;&#20110;UWB&#30340;&#38745;&#24577;&#25163;&#21183;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
UWB Based Static Gesture Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.15036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;UWB&#30340;&#38745;&#24577;&#25163;&#21183;&#35782;&#21035;&#30340;&#31283;&#20581;&#26694;&#26550;&#65292;&#32463;&#36807;&#25968;&#25454;&#38598;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#35757;&#32451;&#27169;&#22411;&#36798;&#21040;96.78%&#20934;&#30830;&#29575;&#65292;&#24182;&#24320;&#21457;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;GUI&#26694;&#26550;&#65292;&#20026;&#23558;UWB&#25216;&#26415;&#24212;&#29992;&#20110;&#38745;&#24577;&#25163;&#21183;&#35782;&#21035;&#24102;&#26469;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;UWB&#30340;&#38745;&#24577;&#25163;&#21183;&#35782;&#21035;&#65292;&#21033;&#29992;&#19987;&#26377;&#30340;UWB&#38647;&#36798;&#20256;&#24863;&#22120;&#25216;&#26415;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#24037;&#20316;&#65292;&#32534;&#21046;&#20102;&#21253;&#21547;&#20116;&#31181;&#24120;&#29992;&#25163;&#21183;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20840;&#38754;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#27969;&#31243;&#65292;&#21253;&#25324;&#24322;&#24120;&#20540;&#22788;&#29702;&#12289;&#20445;&#25345;&#38271;&#23485;&#27604;&#30340;&#35843;&#25972;&#22823;&#23567;&#21644;&#20266;&#24425;&#33394;&#22270;&#20687;&#36716;&#25442;&#12290;&#25105;&#20204;&#23545;&#22788;&#29702;&#21518;&#30340;&#22270;&#20687;&#35757;&#32451;&#20102;CNN&#21644;MobileNet&#27169;&#22411;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;96.78%&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;GUI&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#31995;&#32479;&#36164;&#28304;&#20351;&#29992;&#24773;&#20917;&#21644;&#22788;&#29702;&#26102;&#38388;&#65292;&#26174;&#31034;&#20986;&#20302;&#20869;&#23384;&#21033;&#29992;&#21644;&#22312;&#19981;&#21040;&#19968;&#31186;&#30340;&#23454;&#26102;&#20219;&#21153;&#23436;&#25104;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#22312;&#20351;&#29992;UWB&#25216;&#26415;&#22686;&#24378;&#38745;&#24577;&#25163;&#21183;&#35782;&#21035;&#26041;&#38754;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#65292;&#20855;&#26377;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.15036v2 Announce Type: replace-cross  Abstract: Our paper presents a robust framework for UWB-based static gesture recognition, leveraging proprietary UWB radar sensor technology. Extensive data collection efforts were undertaken to compile datasets containing five commonly used gestures. Our approach involves a comprehensive data pre-processing pipeline that encompasses outlier handling, aspect ratio-preserving resizing, and false-color image transformation. Both CNN and MobileNet models were trained on the processed images. Remarkably, our best-performing model achieved an accuracy of 96.78%. Additionally, we developed a user-friendly GUI framework to assess the model's system resource usage and processing times, which revealed low memory utilization and real-time task completion in under one second. This research marks a significant step towards enhancing static gesture recognition using UWB technology, promising practical applications in various domains.
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.02174</link><description>&lt;p&gt;
&#35753;&#24490;&#29615;&#30340;&#35810;&#38382;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#20013;&#30340;&#25671;&#25670;
&lt;/p&gt;
&lt;p&gt;
Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02174
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20250;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24448;&#24448;&#22312;&#20854;&#21028;&#26029;&#19978;&#25671;&#25670;&#19981;&#23450;&#65292;&#21363;&#20351;&#21407;&#22987;&#21028;&#26029;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#31181;&#25671;&#25670;&#23545;&#20110;&#29983;&#25104;&#21487;&#38752;&#22238;&#22797;&#21644;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#20197;&#21450;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#30830;&#35748;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#29992;&#20110;&#38381;&#28304;&#27169;&#22411;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#35757;&#32451;&#30340;&#26694;&#26550;Unwavering-FQ&#65292;&#36890;&#36807;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#20445;&#25345;&#20854;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#22686;&#24378;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#32771;&#34385;&#20102;&#26356;&#19968;&#33324;&#30340;&#32047;&#31215;&#30693;&#35782;&#36807;&#31243;&#23478;&#26063;&#65292;&#25506;&#35752;&#20102;&#22312;&#31038;&#20250;&#30693;&#35782;&#31215;&#32047;&#36807;&#31243;&#20013;&#22914;&#20309;&#30830;&#20445;&#19968;&#23450;&#27604;&#20363;&#30340;&#30693;&#35782;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2309.05638</link><description>&lt;p&gt;
&#38169;&#35823;&#22312;&#32047;&#31215;&#30693;&#35782;&#36807;&#31243;&#20013;&#34987;&#26377;&#25928;&#22320;&#39535;&#26381;&#20102;
&lt;/p&gt;
&lt;p&gt;
Errors are Robustly Tamed in Cumulative Knowledge Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05638
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#32771;&#34385;&#20102;&#26356;&#19968;&#33324;&#30340;&#32047;&#31215;&#30693;&#35782;&#36807;&#31243;&#23478;&#26063;&#65292;&#25506;&#35752;&#20102;&#22312;&#31038;&#20250;&#30693;&#35782;&#31215;&#32047;&#36807;&#31243;&#20013;&#22914;&#20309;&#30830;&#20445;&#19968;&#23450;&#27604;&#20363;&#30340;&#30693;&#35782;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31038;&#20250;&#30693;&#35782;&#31215;&#32047;&#36807;&#31243;&#65292;&#20854;&#20013;&#26032;&#30693;&#35782;&#21333;&#20803;&#30340;&#26377;&#25928;&#24615;&#26082;&#21462;&#20915;&#20110;&#20854;&#25512;&#23548;&#30340;&#27491;&#30830;&#24615;&#65292;&#21448;&#21462;&#20915;&#20110;&#23427;&#25152;&#20381;&#36182;&#30340;&#21333;&#20803;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#31181;&#24773;&#22659;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#65306;&#22914;&#26524;&#26032;&#25512;&#23548;&#30340;&#24658;&#23450;&#27604;&#20363;&#26159;&#38169;&#35823;&#30340;&#65292;&#37027;&#20040;&#25237;&#20837;&#19968;&#20010;&#24658;&#23450;&#27604;&#20363;&#24182;&#36828;&#31163;&#19968;&#30340;&#21162;&#21147;&#26159;&#21542;&#33021;&#30830;&#20445;&#31038;&#20250;&#20013;&#30340;&#30693;&#35782;&#30340;&#19968;&#20010;&#24658;&#23450;&#27604;&#20363;&#26159;&#26377;&#25928;&#30340;&#65311;Ben-Eliezer, Mikulincer, Mossel&#21644;Sudan (ITCS 2023)&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#27010;&#29575;&#27169;&#22411;&#26469;&#20998;&#26512;&#36825;&#31867;&#38382;&#39064;&#65292;&#24182;&#23545;&#36825;&#20010;&#38382;&#39064;&#20316;&#20986;&#20102;&#32943;&#23450;&#30340;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#19968;&#20010;&#31616;&#21333;&#24773;&#20917;&#65292;&#21363;&#27599;&#20010;&#26032;&#21333;&#20803;&#21482;&#20381;&#36182;&#20110;&#19968;&#20010;&#29616;&#26377;&#21333;&#20803;&#65292;&#24182;&#19988;&#21333;&#20803;&#26681;&#25454;$\textit{&#20248;&#20808;&#38468;&#21152;&#35268;&#21017;}$&#38468;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26356;&#19968;&#33324;&#30340;&#32047;&#31215;&#30693;&#35782;&#36807;&#31243;&#23478;&#26063;&#65292;&#20854;&#20013;&#26032;&#21333;&#20803;&#21487;&#33021;&#26681;&#25454;&#19981;&#21516;&#30340;&#38468;&#21152;&#26426;&#21046;&#36827;&#34892;&#38468;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05638v2 Announce Type: replace  Abstract: We study processes of societal knowledge accumulation, where the validity of a new unit of knowledge depends both on the correctness of its derivation and on the validity of the units it depends on. A fundamental question in this setting is: If a constant fraction of the new derivations is wrong, can investing a constant fraction, bounded away from one, of effort ensure that a constant fraction of knowledge in society is valid? Ben-Eliezer, Mikulincer, Mossel, and Sudan (ITCS 2023) introduced a concrete probabilistic model to analyze such questions and showed an affirmative answer to this question. Their study, however, focuses on the simple case where each new unit depends on just one existing unit, and units attach according to a $\textit{preferential attachment rule}$.   In this work, we consider much more general families of cumulative knowledge processes, where new units may attach according to varied attachment mechanisms and d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35270;&#39057;&#24322;&#24120;&#26816;&#32034;(VAR)&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#36328;&#27169;&#24577;&#26816;&#32034;&#30456;&#20851;&#24322;&#24120;&#35270;&#39057;&#12290;</title><link>https://arxiv.org/abs/2307.12545</link><description>&lt;p&gt;
&#20174;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#36208;&#21521;&#35270;&#39057;&#24322;&#24120;&#26816;&#32034;&#65306;&#26032;&#30340;&#22522;&#20934;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Video Anomaly Retrieval from Video Anomaly Detection: New Benchmarks and Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.12545
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35270;&#39057;&#24322;&#24120;&#26816;&#32034;(VAR)&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#36328;&#27169;&#24577;&#26816;&#32034;&#30456;&#20851;&#24322;&#24120;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;(VAD)&#22240;&#20854;&#28508;&#22312;&#24212;&#29992;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#30446;&#21069;&#30340;&#20027;&#35201;&#20219;&#21153;&#38598;&#20013;&#22312;&#22312;&#32447;&#26816;&#27979;&#24103;&#32423;&#21035;&#30340;&#24322;&#24120;&#65292;&#36825;&#21487;&#20197;&#31895;&#30053;&#22320;&#35299;&#37322;&#20026;&#20108;&#36827;&#21046;&#25110;&#22810;&#31867;&#20107;&#20214;&#20998;&#31867;&#12290;&#19982;&#27492;&#31867;&#20284;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#35270;&#39057;&#24322;&#24120;&#26816;&#32034;(VAR)&#30340;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#36328;&#27169;&#24577;&#65292;&#20363;&#22914;&#35821;&#35328;&#25551;&#36848;&#21644;&#21516;&#27493;&#38899;&#39057;&#65292;&#23454;&#29616;&#23454;&#29992;&#30340;&#26816;&#32034;&#30456;&#20851;&#24322;&#24120;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.12545v2 Announce Type: replace-cross  Abstract: Video anomaly detection (VAD) has been paid increasing attention due to its potential applications, its current dominant tasks focus on online detecting anomalies% at the frame level, which can be roughly interpreted as the binary or multiple event classification. However, such a setup that builds relationships between complicated anomalous events and single labels, e.g., ``vandalism'', is superficial, since single labels are deficient to characterize anomalous events. In reality, users tend to search a specific video rather than a series of approximate videos. Therefore, retrieving anomalous events using detailed descriptions is practical and positive but few researches focus on this. In this context, we propose a novel task called Video Anomaly Retrieval (VAR), which aims to pragmatically retrieve relevant anomalous videos by cross-modalities, e.g., language descriptions and synchronous audios. Unlike the current video retrie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#25351;&#26631;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#21306;&#38388;&#20256;&#25773;&#36793;&#30028;&#65288;IBP&#65289;&#35757;&#32451;&#25104;&#21151;&#30340;&#26426;&#21046;&#12290;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#23545;&#20110;&#28145;&#24230;&#32447;&#24615;&#27169;&#22411;&#65292;IBP&#35757;&#32451;&#33021;&#22815;&#22312;&#36275;&#22815;&#23485;&#24230;&#30340;&#26465;&#20214;&#19979;&#25913;&#21892;&#36793;&#30028;&#32039;&#23494;&#24230;&#12290;</title><link>https://arxiv.org/abs/2306.10426</link><description>&lt;p&gt;
&#20102;&#35299;&#20351;&#29992;&#21306;&#38388;&#20256;&#25773;&#36793;&#30028;&#36827;&#34892;&#35748;&#35777;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Understanding Certified Training with Interval Bound Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.10426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#25351;&#26631;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#21306;&#38388;&#20256;&#25773;&#36793;&#30028;&#65288;IBP&#65289;&#35757;&#32451;&#25104;&#21151;&#30340;&#26426;&#21046;&#12290;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#23545;&#20110;&#28145;&#24230;&#32447;&#24615;&#27169;&#22411;&#65292;IBP&#35757;&#32451;&#33021;&#22815;&#22312;&#36275;&#22815;&#23485;&#24230;&#30340;&#26465;&#20214;&#19979;&#25913;&#21892;&#36793;&#30028;&#32039;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#40065;&#26834;&#24615;&#39564;&#35777;&#26041;&#27861;&#21464;&#24471;&#26356;&#21152;&#31934;&#30830;&#65292;&#23545;&#35757;&#32451;&#20855;&#26377;&#35748;&#35777;&#40065;&#26834;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#35745;&#31639;&#24182;&#20248;&#21270;&#20102;&#23545;&#40065;&#26834;&#24615;&#35268;&#33539;&#19979;&#26368;&#22351;&#24773;&#20917;&#25439;&#22833;&#30340;&#19978;&#30028;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22522;&#20110;&#19981;&#31934;&#30830;&#30340;&#21306;&#38388;&#20256;&#25773;&#36793;&#30028;&#65288;IBP&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#19968;&#30452;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#21033;&#29992;&#26356;&#31934;&#30830;&#36793;&#30028;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#23545;&#20351;IBP&#22914;&#27492;&#25104;&#21151;&#30340;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;&#34913;&#37327;IBP&#36793;&#30028;&#32039;&#23494;&#24230;&#30340;&#26032;&#39062;&#25351;&#26631;&#65292;&#24443;&#24213;&#30740;&#31350;&#20102;&#36825;&#20123;&#26426;&#21046;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#23545;&#20110;&#28145;&#24230;&#32447;&#24615;&#27169;&#22411;&#65292;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#32039;&#23494;&#24230;&#38543;&#30528;&#23485;&#24230;&#21644;&#28145;&#24230;&#20943;&#23567;&#65292;&#20294;&#22312;&#32473;&#23450;&#36275;&#22815;&#32593;&#32476;&#23485;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;IBP&#35757;&#32451;&#20250;&#24471;&#21040;&#25913;&#36827;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20351;IBP&#36793;&#30028;&#21464;&#24471;&#31934;&#30830;&#30340;&#26435;&#37325;&#30697;&#38453;&#30340;&#20805;&#20998;&#21644;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#20102;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.10426v2 Announce Type: replace-cross  Abstract: As robustness verification methods are becoming more precise, training certifiably robust neural networks is becoming ever more relevant. To this end, certified training methods compute and then optimize an upper bound on the worst-case loss over a robustness specification. Curiously, training methods based on the imprecise interval bound propagation (IBP) consistently outperform those leveraging more precise bounding methods. Still, we lack an understanding of the mechanisms making IBP so successful.   In this work, we thoroughly investigate these mechanisms by leveraging a novel metric measuring the tightness of IBP bounds. We first show theoretically that, for deep linear models, tightness decreases with width and depth at initialization, but improves with IBP training, given sufficient network width. We, then, derive sufficient and necessary conditions on weight matrices for IBP bounds to become exact and demonstrate that t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#23567;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;KLD&#26367;&#25442;&#26631;&#20934;KD&#26041;&#27861;&#20013;&#30340;&#21069;&#21521;KLD&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#23398;&#29983;&#27169;&#22411;&#39640;&#20272;&#25945;&#24072;&#20998;&#24067;&#30340;&#20302;&#27010;&#29575;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2306.08543</link><description>&lt;p&gt;
MiniLLM&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MiniLLM: Knowledge Distillation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#23567;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;KLD&#26367;&#25442;&#26631;&#20934;KD&#26041;&#27861;&#20013;&#30340;&#21069;&#21521;KLD&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#23398;&#29983;&#27169;&#22411;&#39640;&#20272;&#25945;&#24072;&#20998;&#24067;&#30340;&#20302;&#27010;&#29575;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#35745;&#31639;&#38656;&#27714;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;KD&#26041;&#27861;&#20027;&#35201;&#24212;&#29992;&#20110;&#30333;&#30418;&#20998;&#31867;&#27169;&#22411;&#25110;&#35757;&#32451;&#23567;&#27169;&#22411;&#26469;&#27169;&#20223;&#22914;ChatGPT&#20043;&#31867;&#30340;&#40657;&#30418;&#27169;&#22411;API&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#30333;&#30418;LLMs&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#23567;&#27169;&#22411;&#20013;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#65292;&#38543;&#30528;&#24320;&#28304;LLMs&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#36825;&#21464;&#24471;&#26356;&#20026;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;KD&#26041;&#27861;&#65292;&#23558;LLMs&#33976;&#39311;&#21040;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08543v2 Announce Type: replace-cross  Abstract: Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named Mi
&lt;/p&gt;</description></item><item><title>V2X-INCOP&#26159;&#19968;&#20010;&#38024;&#23545;V2X&#36890;&#20449;&#36741;&#21161;&#33258;&#21160;&#39550;&#39542;&#30340;&#21512;&#20316;&#24863;&#30693;&#31995;&#32479;&#65292;&#21033;&#29992;&#21382;&#21490;&#21512;&#20316;&#20449;&#24687;&#26469;&#24674;&#22797;&#30001;&#20013;&#26029;&#24341;&#36215;&#30340;&#20002;&#22833;&#20449;&#24687;&#65292;&#24182;&#32531;&#35299;&#23433;&#20840;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2304.11821</link><description>&lt;p&gt;
&#22522;&#20110;&#20013;&#26029;&#24863;&#30693;&#30340;V2X&#36890;&#20449;&#36741;&#21161;&#33258;&#21160;&#39550;&#39542;&#30340;&#21512;&#20316;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Interruption-Aware Cooperative Perception for V2X Communication-Aided Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.11821
&lt;/p&gt;
&lt;p&gt;
V2X-INCOP&#26159;&#19968;&#20010;&#38024;&#23545;V2X&#36890;&#20449;&#36741;&#21161;&#33258;&#21160;&#39550;&#39542;&#30340;&#21512;&#20316;&#24863;&#30693;&#31995;&#32479;&#65292;&#21033;&#29992;&#21382;&#21490;&#21512;&#20316;&#20449;&#24687;&#26469;&#24674;&#22797;&#30001;&#20013;&#26029;&#24341;&#36215;&#30340;&#20002;&#22833;&#20449;&#24687;&#65292;&#24182;&#32531;&#35299;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#24863;&#30693;&#36890;&#36807;V2X&#36890;&#20449;&#19982;&#37051;&#36817;&#36710;&#36742;&#20132;&#25442;&#20449;&#24687;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#33258;&#21160;&#36710;&#36742;&#30340;&#24863;&#30693;&#24615;&#33021;&#65292;&#36229;&#36234;&#20010;&#20307;&#36710;&#36742;&#30340;&#26377;&#38480;&#24863;&#30693;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20551;&#35774;&#21508;&#36710;&#36742;&#20043;&#38388;&#36890;&#20449;&#29702;&#24819;&#65292;&#24573;&#35270;&#20102;V2X&#36890;&#20449;&#19981;&#23436;&#21892;&#24102;&#26469;&#30340;&#26174;&#33879;&#30340;&#24120;&#35265;\textit{&#20013;&#26029;&#38382;&#39064;}&#65292;&#23548;&#33268;&#21512;&#20316;&#36710;&#36742;&#26080;&#27861;&#25104;&#21151;&#25509;&#25910;&#21512;&#20316;&#20449;&#24687;&#65292;&#20174;&#32780;&#26080;&#27861;&#23454;&#29616;&#21512;&#20316;&#24863;&#30693;&#65292;&#24102;&#26469;&#23433;&#20840;&#39118;&#38505;&#12290;&#20026;&#20102;&#22312;&#23454;&#36341;&#20013;&#20805;&#20998;&#21033;&#29992;&#21512;&#20316;&#24863;&#30693;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; V2X&#36890;&#20449;&#20013;&#26029;&#24863;&#30693;&#30340;&#21512;&#20316;&#24863;&#30693; (V2X-INCOP)&#65292;&#19968;&#20010;&#38024;&#23545;V2X&#36890;&#20449;&#36741;&#21161;&#33258;&#21160;&#39550;&#39542;&#30340;&#40065;&#26834;&#24615;&#21512;&#20316;&#24863;&#30693;&#31995;&#32479;&#65292;&#21033;&#29992;&#21382;&#21490;&#21512;&#20316;&#20449;&#24687;&#26469;&#24674;&#22797;&#30001;&#20013;&#26029;&#23548;&#33268;&#30340;&#20002;&#22833;&#20449;&#24687;&#65292;&#24182;&#32531;&#35299;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.11821v2 Announce Type: replace-cross  Abstract: Cooperative perception can significantly improve the perception performance of autonomous vehicles beyond the limited perception ability of individual vehicles by exchanging information with neighbor agents through V2X communication. However, most existing work assume ideal communication among agents, ignoring the significant and common \textit{interruption issues} caused by imperfect V2X communication, where cooperation agents can not receive cooperative messages successfully and thus fail to achieve cooperative perception, leading to safety risks. To fully reap the benefits of cooperative perception in practice, we propose V2X communication INterruption-aware COoperative Perception (V2X-INCOP), a cooperative perception system robust to communication interruption for V2X communication-aided autonomous driving, which leverages historical cooperation information to recover missing information due to the interruptions and allevia
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#32422;&#26463;&#31574;&#30053;&#25913;&#36827;&#26041;&#21521;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2210.07484</link><description>&lt;p&gt;
&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mutual Information Regularized Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.07484
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#32422;&#26463;&#31574;&#30053;&#25913;&#36827;&#26041;&#21521;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#24403;&#36229;&#20986;&#20998;&#24067;&#30340;&#21160;&#20316;&#34987;&#26597;&#35810;&#26102;&#20986;&#29616;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36825;&#20351;&#24471;&#31574;&#30053;&#25913;&#36827;&#26041;&#21521;&#21463;&#21040;&#22806;&#25512;&#35823;&#24046;&#30340;&#20559;&#32622;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#24809;&#32602;&#22312;&#31574;&#30053;&#25913;&#36827;&#25110;&#35780;&#20272;&#36807;&#31243;&#20013;&#20559;&#31163;&#34892;&#20026;&#31574;&#30053;&#30340;&#31574;&#30053;&#25110;&#20215;&#20540;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;MISA&#26694;&#26550;&#65292;&#20174;&#25968;&#25454;&#38598;&#20013;&#29366;&#24577;&#21644;&#34892;&#20026;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#30452;&#25509;&#32422;&#26463;&#31574;&#30053;&#25913;&#36827;&#26041;&#21521;&#65292;&#20197;&#24212;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.07484v3 Announce Type: replace-cross  Abstract: The major challenge of offline RL is the distribution shift that appears when out-of-distribution actions are queried, which makes the policy improvement direction biased by extrapolation errors. Most existing methods address this problem by penalizing the policy or value for deviating from the behavior policy during policy improvement or evaluation. In this work, we propose a novel MISA framework to approach offline RL from the perspective of Mutual Information between States and Actions in the dataset by directly constraining the policy improvement direction. MISA constructs lower bounds of mutual information parameterized by the policy and Q-values. We show that optimizing this lower bound is equivalent to maximizing the likelihood of a one-step improved policy on the offline dataset. Hence, we constrain the policy improvement direction to lie in the data manifold. The resulting algorithm simultaneously augments the policy e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2401.12258</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#26032;&#20852;&#25903;&#37197;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33021;&#22815;&#32988;&#36807;&#20154;&#31867;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35774;&#32622;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#25104;&#21151;&#30340;&#28151;&#21512;&#21160;&#26426;&#20195;&#29702;&#21327;&#20316;&#21462;&#20915;&#20110;&#20010;&#20307;&#21644;&#32676;&#20307;&#30446;&#26631;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;&#31038;&#20250;&#20064;&#24815;&#21644;&#35268;&#33539;&#65292;&#24448;&#24448;&#21463;&#21040;&#20154;&#31867;&#26426;&#26500;&#30340;&#21551;&#21457;&#65292;&#34987;&#29992;&#20316;&#23454;&#29616;&#36825;&#31181;&#24179;&#34913;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#26412;&#19988;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#31038;&#20250;&#20064;&#24815;&#65292;&#21363;&#25903;&#37197;&#31561;&#32423;&#65292;&#23427;&#22312;&#21160;&#29289;&#21644;&#20154;&#31867;&#31038;&#20250;&#20013;&#37117;&#23384;&#22312;&#12290;&#25105;&#20204;&#23558;&#25903;&#37197;&#31561;&#32423;&#30340;&#34892;&#20026;&#29702;&#35770;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#24182;&#23613;&#21487;&#33021;&#23569;&#22320;&#20462;&#25913;&#29616;&#26377;&#30340;&#26415;&#35821;&#21644;&#23450;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#25110;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#32676;&#20307;&#33021;&#22815;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;&#25152;&#20135;&#29983;&#30340;&#25903;&#37197;&#31561;&#32423;&#26377;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: Dominance hierarchies.  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ELLE&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#20943;&#36731;&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;&#23427;&#33021;&#22815;&#20445;&#25345;&#25439;&#22833;&#20989;&#25968;&#22312;&#36755;&#20837;&#19978;&#30340;&#23616;&#37096;&#32447;&#24615;&#24615;&#65292;&#19982;&#20256;&#32479;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;ELLE&#26356;&#21152;&#39640;&#25928;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22823;&#23545;&#25239;&#24615;&#25200;&#21160;&#21644;&#38271;&#35757;&#32451;&#35745;&#21010;&#31561;&#22256;&#38590;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.11618</link><description>&lt;p&gt;
&#29992;&#20110;&#20811;&#26381;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#30340;&#39640;&#25928;&#26412;&#22320;&#32447;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient local linearity regularization to overcome catastrophic overfitting. (arXiv:2401.11618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ELLE&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#20943;&#36731;&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;&#23427;&#33021;&#22815;&#20445;&#25345;&#25439;&#22833;&#20989;&#25968;&#22312;&#36755;&#20837;&#19978;&#30340;&#23616;&#37096;&#32447;&#24615;&#24615;&#65292;&#19982;&#20256;&#32479;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;ELLE&#26356;&#21152;&#39640;&#25928;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22823;&#23545;&#25239;&#24615;&#25200;&#21160;&#21644;&#38271;&#35757;&#32451;&#35745;&#21010;&#31561;&#22256;&#38590;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512; (CO) &#23548;&#33268;&#23545;&#25239;&#24615;&#27979;&#35797;&#20934;&#30830;&#29575;&#31361;&#28982;&#19979;&#38477;&#65288;&#29978;&#33267;&#38477;&#33267;0%&#65289;&#12290;&#23545;&#20110;&#20351;&#29992;&#22810;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24050;&#35266;&#23519;&#21040;&#25439;&#22833;&#20989;&#25968;&#22312;&#36755;&#20837;&#19978;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#24615;&#65292;&#20294;&#36825;&#31181;&#29305;&#24615;&#22312;&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#21333;&#27493;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#30340;CO&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#24378;&#21046;&#25439;&#22833;&#20989;&#25968;&#23616;&#37096;&#32447;&#24615;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21452;&#37325;&#21453;&#21521;&#20256;&#25773;&#65292;&#36825;&#20123;&#27491;&#21017;&#21270;&#39033;&#20250;&#26174;&#33879;&#20943;&#24930;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;ELLE&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#20197;&#22312;&#32463;&#20856;&#23545;&#25239;&#24615;&#35757;&#32451;&#35780;&#20272;&#20013;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#20943;&#36731;CO&#38382;&#39064;&#65292;&#22312;&#19968;&#20123;&#26356;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#36215;&#20316;&#29992;&#65292;&#20363;&#22914;&#22823;&#23545;&#25239;&#24615;&#25200;&#21160;&#21644;&#38271;&#35757;&#32451;&#35745;&#21010;&#12290;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#39033;&#22312;&#29702;&#35770;&#19978;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#26354;&#29575;&#26377;&#32852;&#31995;&#65292;&#24182;&#19988;&#36890;&#36807;&#36991;&#20813;&#21452;&#37325;&#21453;&#21521;&#20256;&#25773;&#32780;&#20855;&#26377;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#24443;&#24213;&#30340;&#23454;&#39564;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Catastrophic overfitting (CO) in single-step adversarial training (AT) results in abrupt drops in the adversarial test accuracy (even down to 0%). For models trained with multi-step AT, it has been observed that the loss function behaves locally linearly with respect to the input, this is however lost in single-step AT. To address CO in single-step AT, several methods have been proposed to enforce local linearity of the loss via regularization. However, these regularization terms considerably slow down training due to Double Backpropagation. Instead, in this work, we introduce a regularization term, called ELLE, to mitigate CO effectively and efficiently in classical AT evaluations, as well as some more difficult regimes, e.g., large adversarial perturbations and long training schedules. Our regularization term can be theoretically linked to curvature of the loss function and is computationally cheaper than previous methods by avoiding Double Backpropagation. Our thorough experimental 
&lt;/p&gt;</description></item><item><title>BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02982</link><description>&lt;p&gt;
BIBench: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#20998;&#26512;&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BIBench: Benchmarking Data Analysis Knowledge of Large Language Models. (arXiv:2401.02982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02982
&lt;/p&gt;
&lt;p&gt;
BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#29087;&#32451;&#24230;&#21644;&#21487;&#38752;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#24605;&#32500;&#20026;&#37325;&#28857;&#30340;&#39046;&#22495;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BIBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#30340;&#32972;&#26223;&#19979;&#30340;&#25968;&#25454;&#20998;&#26512;&#33021;&#21147;&#12290;BIBench&#36890;&#36807;&#19977;&#20010;&#32500;&#24230;&#35780;&#20272;LLMs&#65306;1&#65289;BI&#22522;&#30784;&#30693;&#35782;&#65292;&#35780;&#20272;&#27169;&#22411;&#30340;&#25968;&#20540;&#25512;&#29702;&#33021;&#21147;&#21644;&#23545;&#37329;&#34701;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65307;2&#65289;BI&#30693;&#35782;&#24212;&#29992;&#65292;&#30830;&#23450;&#27169;&#22411;&#24555;&#36895;&#29702;&#35299;&#25991;&#26412;&#20449;&#24687;&#24182;&#20174;&#22810;&#20010;&#35270;&#35282;&#29983;&#25104;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65307;3&#65289;BI&#25216;&#26415;&#25216;&#33021;&#65292;&#26816;&#26597;&#27169;&#22411;&#20351;&#29992;&#25216;&#26415;&#30693;&#35782;&#35299;&#20915;&#29616;&#23454;&#25968;&#25454;&#20998;&#26512;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;BIBench&#21253;&#25324;11&#20010;&#23376;&#20219;&#21153;&#65292;&#28085;&#30422;&#20998;&#31867;&#12289;&#25552;&#21462;&#21644;&#29983;&#25104;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. However, their proficiency and reliability in the specialized domain of Data Analysis, particularly with a focus on data-driven thinking, remain uncertain. To bridge this gap, we introduce BIBench, a comprehensive benchmark designed to evaluate the data analysis capabilities of LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs across three dimensions: 1) BI foundational knowledge, evaluating the models' numerical reasoning and familiarity with financial concepts; 2) BI knowledge application, determining the models' ability to quickly comprehend textual information and generate analysis questions from multiple views; and 3) BI technical skills, examining the models' use of technical knowledge to address real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning three categories of task types: classification, extraction, and generation. Additi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MVRE&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#22810;&#35270;&#35282;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.17267</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35270;&#35282;&#35299;&#32806;&#23398;&#20064;&#25913;&#36827;&#20302;&#36164;&#28304;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#20851;&#31995;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving Low-resource Prompt-based Relation Representation with Multi-view Decoupling Learning. (arXiv:2312.17267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17267
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MVRE&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#22810;&#35270;&#35282;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#33021;&#21147;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#20219;&#21153;&#30340;&#22686;&#24378;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#23545;&#20851;&#31995;&#30340;&#34920;&#23618;&#29702;&#35299;&#65292;&#20808;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#33021;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24378;&#35843;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#20851;&#31995;&#34920;&#31034;&#23545;&#20110;RE&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#20851;&#31995;&#34920;&#31034;&#26041;&#27861;&#65292;&#21517;&#20026;MVRE&#65288;&#22810;&#35270;&#35282;&#20851;&#31995;&#25277;&#21462;&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;PLMs&#30340;&#33021;&#21147;&#26469;&#25913;&#21892;&#20302;&#36164;&#28304;&#25552;&#31034;&#35843;&#25972;&#33539;&#24335;&#19979;&#30340;RE&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MVRE&#23558;&#27599;&#20010;&#20851;&#31995;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#20197;&#21253;&#21547;&#22810;&#35270;&#35282;&#30340;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#31995;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#20284;&#28982;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#23616;&#24615;&#30340;&#20302;&#39046;&#22495;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20851;&#31995;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, prompt-tuning with pre-trained language models (PLMs) has demonstrated the significantly enhancing ability of relation extraction (RE) tasks. However, in low-resource scenarios, where the available training data is scarce, previous prompt-based methods may still perform poorly for prompt-based representation learning due to a superficial understanding of the relation. To this end, we highlight the importance of learning high-quality relation representation in low-resource scenarios for RE, and propose a novel prompt-based relation representation method, named MVRE (\underline{M}ulti-\underline{V}iew \underline{R}elation \underline{E}xtraction), to better leverage the capacity of PLMs to improve the performance of RE within the low-resource prompt-tuning paradigm. Specifically, MVRE decouples each relation into different perspectives to encompass multi-view relation representations for maximizing the likelihood during relation inference. Furthermore, we also design a Global-Lo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#21327;&#21516;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.11730</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#30340;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation. (arXiv:2310.11730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#21327;&#21516;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#36890;&#36807;&#20803;&#36335;&#24452;&#25551;&#36848;&#20016;&#23500;&#30340;&#35821;&#20041;&#65292;&#24050;&#25104;&#20026;&#32531;&#35299;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;HIN&#30340;&#25512;&#33616;&#31995;&#32479;&#25345;&#26377;&#25968;&#25454;&#30340;&#38598;&#20013;&#23384;&#20648;&#20551;&#35774;&#65292;&#24182;&#36827;&#34892;&#38598;&#20013;&#24335;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24448;&#24448;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#23384;&#20648;&#65292;&#23548;&#33268;&#38598;&#20013;&#24335;HIN&#25512;&#33616;&#26080;&#27861;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;HIN&#20998;&#20026;&#23458;&#25143;&#31471;&#23384;&#20648;&#30340;&#31169;&#26377;HIN&#21644;&#26381;&#21153;&#22120;&#31471;&#30340;&#20849;&#20139;HIN&#12290;&#22312;&#27492;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;HIN&#19978;&#21327;&#20316;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#27844;&#38706;&#29992;&#25143;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#22522;&#20110;HIN&#30340;&#32852;&#21512;&#25512;&#33616;&#65292;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#20809;&#19979;&#30830;&#23450;&#20102;&#38544;&#31169;&#23450;&#20041;&#65292;&#26088;&#22312;&#20445;&#25252;&#31169;&#26377;HIN&#30340;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#65292;&#20197;&#21450;&#29992;&#25143;&#30340;&#38544;&#31169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has become a powerful tool to alleviate data sparsity in recommender systems. Existing HIN-based recommendations hold the data centralized storage assumption and conduct centralized model training. However, the real-world data is often stored in a distributed manner for privacy concerns, resulting in the failure of centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored in the client side and shared HINs in the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which can collaboratively train a recommendation model on distributed HINs without leaking user privacy. Specifically, we first formalize the privacy definition in the light of differential privacy for HIN-based federated recommendation, which aims to protect user-item interactions of private HIN as well as user's 
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.05227</link><description>&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#38761;&#21629;&#31185;&#23398;&#33539;&#24335;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#36807;&#31243;&#30340;&#27700;&#25991;&#23398;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology. (arXiv:2310.05227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05227
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#23545;&#20110;&#35299;&#20915;&#27700;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#31185;&#23398;&#21644;&#31038;&#20250;&#25361;&#25112;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#30340;&#21160;&#24577;&#24433;&#21709;&#19979;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#28982;&#32780;&#27700;&#25991;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#29420;&#31435;&#30340;&#33539;&#24335;&#23384;&#22312;&#26126;&#26174;&#30340;&#21306;&#21035;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20197;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#36825;&#31181;&#35748;&#30693;&#38556;&#30861;&#65292;&#24182;&#38761;&#26032;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#35780;&#35770;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#31038;&#21306;&#65288;PaML&#65289;&#65292;&#23558;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#25110;&#22522;&#20110;&#29289;&#29702;&#30340;&#24314;&#27169;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20174;&#29289;&#29702;&#25968;&#25454;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#20449;&#24687;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#29702;&#24863;&#30693;&#28151;&#21512;&#23398;&#20064;&#22235;&#20010;&#26041;&#38754;&#20998;&#26512;&#20102;&#36825;&#20123;PaML&#26041;&#27861;&#12290;PaML&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20551;&#35774;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate hydrological understanding and water cycle prediction are crucial for addressing scientific and societal challenges associated with the management of water resources, particularly under the dynamic influence of anthropogenic climate change. Existing reviews predominantly concentrate on the development of machine learning (ML) in this field, yet there is a clear distinction between hydrology and ML as separate paradigms. Here, we introduce physics-aware ML as a transformative approach to overcome the perceived barrier and revolutionize both fields. Specifically, we present a comprehensive review of the physics-aware ML methods, building a structured community (PaML) of existing methodologies that integrate prior physical knowledge or physics-based modeling into ML. We systematically analyze these PaML methodologies with respect to four aspects: physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning. PaML facilitates ML-aided hypothe
&lt;/p&gt;</description></item><item><title>MindShift&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#22522;&#20110;&#24515;&#24577;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#36866;&#24212;&#29992;&#25143;&#29615;&#22659;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#39640;&#36136;&#37327;&#35828;&#26381;&#20869;&#23481;&#26469;&#24110;&#21161;&#29992;&#25143;&#35299;&#20915;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#30340;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2309.16639</link><description>&lt;p&gt;
MindShift: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#24515;&#24577;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention. (arXiv:2309.16639v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16639
&lt;/p&gt;
&lt;p&gt;
MindShift&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#22522;&#20110;&#24515;&#24577;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#36866;&#24212;&#29992;&#25143;&#29615;&#22659;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#39640;&#36136;&#37327;&#35828;&#26381;&#20869;&#23481;&#26469;&#24110;&#21161;&#29992;&#25143;&#35299;&#20915;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#30340;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#23545;&#36523;&#20307;&#21644;&#24515;&#29702;&#20581;&#24247;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#20808;&#21069;&#30740;&#31350;&#65292;&#29616;&#26377;&#30340;&#35828;&#26381;&#25216;&#24039;&#19981;&#36275;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#36523;&#20307;&#29615;&#22659;&#21644;&#24515;&#29702;&#29366;&#24577;&#25552;&#20379;&#21160;&#24577;&#35828;&#26381;&#20869;&#23481;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#20026;&#25805;&#20316;&#30740;&#31350;&#65288;N = 12&#65289;&#21644;&#19968;&#39033;&#35775;&#35848;&#30740;&#31350;&#65288;N = 10&#65289;&#65292;&#24635;&#32467;&#20102;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#32972;&#21518;&#30340;&#24515;&#24577;&#65306;&#26080;&#32842;&#12289;&#21387;&#21147;&#21644;&#24815;&#24615;&#12290;&#36825;&#20026;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#31181;&#35828;&#26381;&#31574;&#30053;&#65306;&#29702;&#35299;&#12289;&#23433;&#25242;&#12289;&#21796;&#36215;&#21644;&#25903;&#25345;&#20064;&#24815;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23454;&#29616;&#20102;&#26377;&#25928;&#35828;&#26381;&#20869;&#23481;&#30340;&#33258;&#21160;&#21644;&#21160;&#24577;&#29983;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#25216;&#26415;&#39537;&#21160;&#30340;&#38382;&#39064;&#24615;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;&#24178;&#39044;&#25216;&#26415;MindShift&#12290;MindShift&#26681;&#25454;&#29992;&#25143;&#24403;&#19979;&#30340;&#36523;&#20307;&#29615;&#22659;&#12289;&#24515;&#24577;&#12289;&#24212;&#29992;&#20351;&#29992;&#34892;&#20026;&#12289;&#29992;&#25143;&#30340;&#30446;&#26631;&#19982;&#20064;&#24815;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#36866;&#24403;&#35828;&#26381;&#31574;&#30053;&#30340;&#39640;&#36136;&#37327;&#21644;&#28789;&#27963;&#30340;&#35828;&#26381;&#20869;&#23481;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;5-
&lt;/p&gt;
&lt;p&gt;
Problematic smartphone use negatively affects physical and mental health. Despite the wide range of prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users' physical contexts and mental states. We first conduct a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leverage large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We develop MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users' in-the-moment physical contexts, mental states, app usage behaviors, users' goals &amp; habits as input, and generates high-quality and flexible persuasive content with appropriate persuasion strategies. We conduct a 5-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.08646</link><description>&lt;p&gt;
&#36890;&#36807;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#35299;&#20915;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cure the headache of Transformers via Collinear Constrained Attention. (arXiv:2309.08646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08646
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#25512;&#26029;&#24615;&#33021;&#30340;&#22806;&#25512;&#21464;&#24471;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#34987;&#20043;&#21069;&#24573;&#35270;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#23548;&#33268;&#20102;&#26368;&#25509;&#36817;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#28151;&#20081;&#65292;&#36825;&#20123;&#26631;&#35760;&#25658;&#24102;&#20102;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#21457;&#29616;&#31216;&#20026;&#8220;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;&#8221;&#12290;&#20026;&#20102;&#20174;&#26681;&#26412;&#19978;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#32467;&#26500;&#65292;&#21629;&#21517;&#20026;Collinear Constrained Attention&#65288;CoCA&#65289;&#12290;&#36825;&#20010;&#32467;&#26500;&#21487;&#20197;&#26080;&#32541;&#22320;&#19982;&#29616;&#26377;&#30340;&#25512;&#26029;&#12289;&#25554;&#20540;&#26041;&#27861;&#21644;&#20854;&#20182;&#38024;&#23545;&#20256;&#32479;Transformer&#27169;&#22411;&#35774;&#35745;&#30340;&#20248;&#21270;&#31574;&#30053;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#21363;&#20351;&#26159;16&#21040;24&#20493;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#32780;&#19988;&#27809;&#26377;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#25105;&#20204;&#36824;&#22686;&#24378;&#20102;CoCA&#30340;&#35745;&#31639;&#21644;&#31354;&#38388;&#25928;&#29575;&#65292;&#20197;&#30830;&#20445;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35745;&#21010;...
&lt;/p&gt;
&lt;p&gt;
As the rapid progression of practical applications based on Large Language Models continues, the importance of extrapolating performance has grown exponentially in the research domain. In our study, we identified an anomalous behavior in Transformer models that had been previously overlooked, leading to a chaos around closest tokens which carried the most important information. We've coined this discovery the "headache of Transformers". To address this at its core, we introduced a novel self-attention structure named Collinear Constrained Attention (CoCA). This structure can be seamlessly integrated with existing extrapolation, interpolation methods, and other optimization strategies designed for traditional Transformer models. We have achieved excellent extrapolating performance even for 16 times to 24 times of sequence lengths during inference without any fine-tuning on our model. We have also enhanced CoCA's computational and spatial efficiency to ensure its practicality. We plan to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;CL-MAE&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#36974;&#32617;&#27169;&#22359;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#26469;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.16572</link><description>&lt;p&gt;
CL-MAE: &#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
CL-MAE: Curriculum-Learned Masked Autoencoders. (arXiv:2308.16572v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;CL-MAE&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#36974;&#32617;&#27169;&#22359;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#26469;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#29992;&#20110;&#29983;&#25104;&#33021;&#22815;&#26377;&#25928;&#27867;&#21270;&#21040;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;&#36890;&#24120;&#65292;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#38543;&#26426;&#36974;&#32617;&#34917;&#19969;&#65288;&#26631;&#35760;&#65289;&#65292;&#24182;&#19988;&#36974;&#32617;&#31574;&#30053;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#20197;&#25345;&#32493;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#36974;&#32617;&#27169;&#22359;&#65292;&#20855;&#26377;&#29983;&#25104;&#19981;&#21516;&#22797;&#26434;&#24230;&#36974;&#32617;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#35813;&#27169;&#22359;&#19982;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#19982;MAE&#19968;&#21516;&#35757;&#32451;&#65292;&#21516;&#26102;&#35843;&#25972;&#20854;&#34892;&#20026;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;MAE&#30340;&#21442;&#19982;&#32773;&#36807;&#28193;&#21040;MAE&#65288;&#20248;&#21270;&#30456;&#21516;&#30340;&#37325;&#26500;&#30446;&#26631;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked image modeling has been demonstrated as a powerful pretext task for generating robust representations that can be effectively generalized across multiple downstream tasks. Typically, this approach involves randomly masking patches (tokens) in input images, with the masking strategy remaining unchanged during training. In this paper, we propose a curriculum learning approach that updates the masking strategy to continually increase the complexity of the self-supervised reconstruction task. We conjecture that, by gradually increasing the task complexity, the model can learn more sophisticated and transferable representations. To facilitate this, we introduce a novel learnable masking module that possesses the capability to generate masks of different complexities, and integrate the proposed module into masked autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting its behavior during training, transitioning from a partner to the MAE (optimizing the same rec
&lt;/p&gt;</description></item><item><title>FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12532</link><description>&lt;p&gt;
FedSoL: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12532
&lt;/p&gt;
&lt;p&gt;
FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#20010;&#20307;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#24403;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#26102;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;FL&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#36817;&#20284;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#32422;&#26463;&#26088;&#22312;&#36890;&#36807;&#38480;&#21046;&#23616;&#37096;&#23398;&#20064;&#19982;&#20840;&#23616;&#30446;&#26631;&#30340;&#20559;&#31163;&#26469;&#20419;&#36827;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#36890;&#36807;&#24178;&#25200;&#21407;&#22987;&#30340;&#23616;&#37096;&#30446;&#26631;&#32780;&#38480;&#21046;&#20102;&#23616;&#37096;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#25913;&#21892;&#26412;&#22320;&#23398;&#20064;&#30340;&#19968;&#33324;&#24615;&#12290;&#36890;&#36807;&#22312;&#24179;&#28369;&#30340;&#25439;&#22833;&#31354;&#38388;&#20013;&#33719;&#24471;&#26412;&#22320;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#20943;&#36731;&#20102;&#23458;&#25143;&#31471;&#19981;&#21516;&#26412;&#22320;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#23427;&#19981;&#33021;&#30830;&#20445;&#31283;&#23450;&#30340;&#20840;&#23616;&#23545;&#40784;&#65292;&#22240;&#20026;&#26412;&#22320;&#23398;&#20064;&#19981;&#32771;&#34385;&#20840;&#23616;&#30446;&#26631;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;(FedSoL)&#26041;&#27861;&#26469;&#22312;FL&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (Fed
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PISF&#30340;&#29289;&#29702;&#20449;&#24687;&#21512;&#25104;&#25968;&#25454;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#24471;&#24555;&#36895;MRI&#37325;&#24314;&#20013;&#30340;&#22810;&#22330;&#26223;&#21487;&#25512;&#24191;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13220</link><description>&lt;p&gt;
&#19968;&#27425;&#22810;&#29992;&#65306;&#29289;&#29702;&#20449;&#24687;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#20102;&#24555;&#36895;MRI&#37325;&#24314;&#20013;&#30340;&#21487;&#25512;&#24191;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One for Multiple: Physics-informed Synthetic Data Boosts Generalizable Deep Learning for Fast MRI Reconstruction. (arXiv:2307.13220v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13220
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PISF&#30340;&#29289;&#29702;&#20449;&#24687;&#21512;&#25104;&#25968;&#25454;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#24471;&#24555;&#36895;MRI&#37325;&#24314;&#20013;&#30340;&#22810;&#22330;&#26223;&#21487;&#25512;&#24191;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#26159;&#19968;&#31181;&#25552;&#20379;&#26080;&#36752;&#23556;&#12289;&#20016;&#23500;&#21644;&#22810;&#26679;&#21270;&#26377;&#20851;&#25972;&#20010;&#20154;&#20307;&#21307;&#23398;&#35786;&#26029;&#20449;&#24687;&#30340;&#20027;&#35201;&#25918;&#23556;&#23398;&#26041;&#27861;&#65292;&#20294;&#20854;&#25195;&#25551;&#26102;&#38388;&#36739;&#38271;&#12290;&#36890;&#36807;k&#31354;&#38388;&#27424;&#37319;&#26679;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25195;&#25551;&#26102;&#38388;&#65292;&#20294;&#38656;&#35201;&#22312;&#22270;&#20687;&#37325;&#24314;&#20013;&#21435;&#38500;&#24341;&#20837;&#30340;&#20266;&#24433;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24050;&#32463;&#25104;&#20026;&#24555;&#36895;MRI&#22270;&#20687;&#37325;&#24314;&#30340;&#26377;&#21147;&#24037;&#20855;&#65292;&#20294;&#20854;&#22312;&#22810;&#31181;&#25104;&#20687;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290;&#36825;&#26159;&#22240;&#20026;&#19981;&#20165;&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26114;&#36149;&#19988;&#21463;&#38480;&#20110;&#38544;&#31169;&#65292;&#29616;&#26377;&#30340;DL&#26041;&#27861;&#36824;&#38590;&#20197;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#23454;&#38469;&#19978;&#19981;&#21487;&#36991;&#20813;&#30340;&#19981;&#21305;&#37197;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#21512;&#25104;&#25968;&#25454;&#23398;&#20064;&#26694;&#26550;&#65288;PISF&#65289;&#29992;&#20110;&#24555;&#36895;MRI&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20165;&#20351;&#29992;&#19968;&#20010;&#35757;&#32451;&#27169;&#22411;&#23454;&#29616;&#21487;&#25512;&#24191;&#30340;&#22810;&#22330;&#26223;MRI&#37325;&#24314;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnetic resonance imaging (MRI) is a principal radiological modality that provides radiation-free, abundant, and diverse information about the whole human body for medical diagnosis, but suffers from prolonged scan time. The scan time can be significantly reduced through k-space undersampling but the introduced artifacts need to be removed in image reconstruction. Although deep learning (DL) has emerged as a powerful tool for image reconstruction in fast MRI, its potential in multiple imaging scenarios remains largely untapped. This is because not only collecting large-scale and diverse realistic training data is generally costly and privacy-restricted, but also existing DL methods are hard to handle the practically inevitable mismatch between training and target data. Here, we present a Physics-Informed Synthetic data learning framework for Fast MRI, called PISF, which is the first to enable generalizable DL for multi-scenario MRI reconstruction using solely one trained model. For a 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#22312;Poincar&#233;&#21452;&#26354;&#29699;&#27169;&#22411;&#20013;&#36816;&#29992;&#36229;bolic&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#21306;&#22495;&#20869;&#20687;&#32032;&#23884;&#20837;&#30340;&#21322;&#24452;&#21464;&#21270;&#20316;&#20026;&#26032;&#30340;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#65292;&#20197;&#25552;&#21319;&#22495;&#36716;&#31227;&#19979;&#35821;&#20041;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11180</link><description>&lt;p&gt;
&#36229;bolic&#27963;&#36291;&#23398;&#20064;&#22312;&#22495;&#36716;&#31227;&#19979;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Active Learning for Semantic Segmentation under Domain Shift. (arXiv:2306.11180v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11180
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#22312;Poincar&#233;&#21452;&#26354;&#29699;&#27169;&#22411;&#20013;&#36816;&#29992;&#36229;bolic&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#21306;&#22495;&#20869;&#20687;&#32032;&#23884;&#20837;&#30340;&#21322;&#24452;&#21464;&#21270;&#20316;&#20026;&#26032;&#30340;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#65292;&#20197;&#25552;&#21319;&#22495;&#36716;&#31227;&#19979;&#35821;&#20041;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22495;&#36716;&#31227;&#19979;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#22522;&#20110;&#22270;&#20687;&#21306;&#22495;&#21644;&#20266;&#26631;&#31614;&#30340;&#20027;&#21160;&#23398;&#20064;&#33719;&#21462;&#31574;&#30053;&#26159;&#26368;&#20808;&#36827;&#30340;&#12290;&#22312;&#21306;&#22495;&#20869;&#23384;&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#21487;&#20197;&#35782;&#21035;&#20986;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#20687;&#32032;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#20027;&#21160;&#23398;&#20064;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#35745;&#38480;&#21046;&#65292;&#20266;&#26631;&#31614;&#30340;&#21464;&#21270;&#20165;&#38480;&#20110;&#36873;&#25321;&#31867;&#21035;&#30340;&#36718;&#24275;&#65292;&#38480;&#21046;&#20102;&#26368;&#32456;&#30340;&#20027;&#21160;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;Poincar&#233;&#21452;&#26354;&#29699;&#27169;&#22411;&#20013;&#20351;&#29992;&#36229;bolic&#26041;&#27861;&#26469;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#21306;&#22495;&#20869;&#20687;&#32032;&#23884;&#20837;&#30340;&#21322;&#24452;&#21464;&#21270;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#12290;&#36825;&#28304;&#20110;&#19968;&#31181;&#26080;&#23618;&#27425;&#32422;&#26463;&#35757;&#32451;&#30340;&#36229;bolic&#31354;&#38388;&#30340;&#26032;&#39062;&#20960;&#20309;&#29305;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#31867;&#21035;&#34987;&#26144;&#23556;&#21040;&#20855;&#26377;&#30456;&#24403;&#20869;&#31867;&#21322;&#24452;&#26041;&#24046;&#30340;&#32039;&#20945;&#36229;bolic&#21306;&#22495;&#65292;&#22240;&#20026;&#27169;&#22411;&#23558;&#38590;&#20197;&#35299;&#37322;&#30340;&#31867;&#21035;&#25918;&#32622;&#22312;&#26356;&#23494;&#38598;&#30340;&#36229;bolic&#21306;&#22495;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the task of semantic segmentation (SS) under domain shift, active learning (AL) acquisition strategies based on image regions and pseudo labels are state-of-the-art (SoA). The presence of diverse pseudo-labels within a region identifies pixels between different classes, which is a labeling efficient active learning data acquisition strategy. However, by design, pseudo-label variations are limited to only select the contours of classes, limiting the final AL performance. We approach AL for SS in the Poincar\'e hyperbolic ball model for the first time and leverage the variations of the radii of pixel embeddings within regions as a novel data acquisition strategy. This stems from a novel geometric property of a hyperbolic space trained without enforced hierarchies, which we experimentally prove. Namely, classes are mapped into compact hyperbolic areas with a comparable intra-class radii variance, as the model places classes of increasing explainable difficulty at denser hyperbolic are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04366</link><description>&lt;p&gt;
&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction. (arXiv:2306.04366v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GCN&#21487;&#20449;&#24230;&#39044;&#27979;&#30340;&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#30340;&#39640;&#25928;&#25307;&#21215;&#31574;&#30053;&#65292;&#36890;&#36807;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#21644;&#24037;&#20154;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#31227;&#21160;&#32676;&#24863;&#30693;&#21487;&#20197;&#36890;&#36807;&#20419;&#36827;&#20219;&#21153;&#24863;&#30693;&#30340;&#22242;&#38431;&#21512;&#20316;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#32780;&#24037;&#20154;&#25307;&#21215;&#21017;&#20195;&#34920;&#30528;&#19968;&#20010;&#22797;&#26434;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#31574;&#30053;&#20027;&#35201;&#20851;&#27880;&#24037;&#20154;&#26412;&#36523;&#30340;&#29305;&#24449;&#65292;&#24573;&#30053;&#20102;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20219;&#21153;&#25928;&#29992;&#35780;&#20272;&#30340;&#21512;&#29702;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;Mini-Batch K-Means&#32858;&#31867;&#31639;&#27861;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#24037;&#20154;&#25307;&#21215;&#12290;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#21644;&#20219;&#21153;&#35201;&#27714;&#33719;&#24471;&#24037;&#20154;&#30340;&#33021;&#21147;&#31867;&#22411;&#21644;&#36317;&#31163;&#12290;&#20351;&#29992;&#24037;&#20154;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20449;&#20219;&#23548;&#21521;&#22270;&#36755;&#20837;&#33267;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#65292;&#25429;&#33719;&#24037;&#20154;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20449;&#20219;&#20851;&#31995;&#12290;&#36890;&#36807;&#24037;&#20154;&#20043;&#38388;&#30340;&#39640;&#20449;&#20219;&#20540;&#65292;&#38450;&#27490;CMCS&#22330;&#26223;&#19979;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26368;&#32456;&#65292;&#21033;&#29992;&#39044;&#27979;&#30340;&#20449;&#20219;&#21644;&#24037;&#20154;&#33021;&#21147;&#26500;&#24314;&#20102;&#19968;&#20010;&#26080;&#21521;&#25307;&#21215;&#22270;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20219;&#21153;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#25307;&#21215;&#26041;&#27861;&#22312;&#25307;&#21215;&#20934;&#30830;&#24230;&#12289;&#20219;&#21153;&#23436;&#25104;&#26102;&#38388;&#21644;&#33021;&#37327;&#28040;&#32791;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Mobile Crowd Sensing (CMCS) enhances data quality and coverage by promoting teamwork in task sensing, with worker recruitment representing a complex multi-objective optimization problem. Existing strategies mainly focus on the characteristics of workers themselves, neglecting the asymmetric trust relationships between them, which affects the rationality of task utility evaluation. To address this, this paper first employs the Mini-Batch K-Means clustering algorithm and deploys edge servers to enable efficient distributed worker recruitment. Historical data and task requirements are utilized to obtain workers' ability types and distances. A trust-directed graph in the worker's social network is input into the Graph Convolutional Network (GCN) framework for training, capturing asymmetric trustworthiness between worker pairs. Privacy leakage is prevented in CMCS scenarios through high trust values between workers. Ultimately, an undirected recruitment graph is constructed us
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;&#65292;&#20197;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.18400</link><description>&lt;p&gt;
&#19968;&#31181;&#20803;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning. (arXiv:2305.18400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18400
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;&#65292;&#20197;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#65288;TFL&#65289;&#36890;&#24120;&#21033;&#29992;&#20445;&#25252;&#26426;&#21046;&#26469;&#20445;&#35777;&#38544;&#31169;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#20445;&#25252;&#26426;&#21046;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#24341;&#20837;&#25928;&#29992;&#25439;&#22833;&#25110;&#25928;&#29575;&#38477;&#20302;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#22240;&#27492;&#65292;&#20445;&#25252;&#26426;&#21046;&#21450;&#20854;&#21442;&#25968;&#24212;&#35813;&#20180;&#32454;&#36873;&#25321;&#65292;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;&#20026;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#20174;&#19994;&#32773;&#38656;&#35201;&#24037;&#20855;&#26469;&#34913;&#37327;&#36825;&#19977;&#20010;&#22240;&#32032;&#65292;&#24182;&#20248;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#25163;&#22836;&#24212;&#29992;&#30340;&#20445;&#25252;&#26426;&#21046;&#12290;&#22522;&#20110;&#36825;&#20010;&#35201;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;(1)&#23558;TFL&#23450;&#20041;&#20026;&#25214;&#21040;&#20445;&#25252;&#26426;&#21046;&#26469;&#20248;&#21270;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#19977;&#32773;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#38382;&#39064;&#65307;(2)&#27491;&#24335;&#23450;&#20041;&#20102;&#36825;&#19977;&#20010;&#22240;&#32032;&#30340;&#26377;&#30028;&#27979;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#27492;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Federated Learning (TFL) typically leverages protection mechanisms to guarantee privacy. However, protection mechanisms inevitably introduce utility loss or efficiency reduction while protecting data privacy. Therefore, protection mechanisms and their parameters should be carefully chosen to strike an optimal tradeoff between \textit{privacy leakage}, \textit{utility loss}, and \textit{efficiency reduction}. To this end, federated learning practitioners need tools to measure the three factors and optimize the tradeoff between them to choose the protection mechanism that is most appropriate to the application at hand. Motivated by this requirement, we propose a framework that (1) formulates TFL as a problem of finding a protection mechanism to optimize the tradeoff between privacy leakage, utility loss, and efficiency reduction and (2) formally defines bounded measurements of the three factors. We then propose a meta-learning algorithm to approximate this optimization proble
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ConvBN&#22359;&#20013;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Tune&#27169;&#24335;&#65292;&#20197;&#20415;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#26082;&#33021;&#20445;&#25345;&#31283;&#23450;&#24615;&#21448;&#33021;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11624</link><description>&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#32780;&#35843;&#25972;&#27169;&#24335;&#30340;ConvBN&#22359;
&lt;/p&gt;
&lt;p&gt;
Tune-Mode ConvBN Blocks For Efficient Transfer Learning. (arXiv:2305.11624v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ConvBN&#22359;&#20013;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Tune&#27169;&#24335;&#65292;&#20197;&#20415;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#26082;&#33021;&#20445;&#25345;&#31283;&#23450;&#24615;&#21448;&#33021;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;-&#25209;&#24402;&#19968;&#21270;&#65288;ConvBN&#65289;&#22359;&#26159;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;ConvBN&#22359;&#21487;&#20197;&#22312;&#19977;&#31181;&#27169;&#24335;&#19979;&#36816;&#34892;&#65306;Train&#12289;Eval&#21644;Deploy&#12290;&#34429;&#28982;Train&#27169;&#24335;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20294;Eval&#27169;&#24335;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#27169;&#22411;&#39564;&#35777;&#65292;&#32780;Deploy&#27169;&#24335;&#21017;&#36866;&#29992;&#20110;&#27169;&#22411;&#37096;&#32626;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;ConvBN&#22359;&#20013;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;Deploy&#27169;&#24335;&#25928;&#29575;&#39640;&#20294;&#35757;&#32451;&#19981;&#31283;&#23450;&#65307;Eval&#27169;&#24335;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#32570;&#20047;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;Deploy&#27169;&#24335;&#19979;&#31283;&#23450;&#24615;&#19979;&#38477;&#30340;&#21407;&#22240;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Tune&#27169;&#24335;&#65292;&#20197;&#24357;&#21512;Eval&#27169;&#24335;&#21644;Deploy&#27169;&#24335;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25152;&#25552;&#20986;&#30340;Tune&#27169;&#24335;&#19982;Eval&#27169;&#24335;&#19968;&#26679;&#31283;&#23450;&#65292;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#65292;&#32780;&#20854;&#35745;&#31639;&#25928;&#29575;&#19982;Deploy&#27169;&#24335;&#38750;&#24120;&#25509;&#36817;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Tune&#27169;&#24335;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolution-BatchNorm (ConvBN) blocks are integral components in various computer vision tasks and other domains. A ConvBN block can operate in three modes: Train, Eval, and Deploy. While the Train mode is indispensable for training models from scratch, the Eval mode is suitable for transfer learning and model validation, and the Deploy mode is designed for the deployment of models. This paper focuses on the trade-off between stability and efficiency in ConvBN blocks: Deploy mode is efficient but suffers from training instability; Eval mode is widely used in transfer learning but lacks efficiency. To solve the dilemma, we theoretically reveal the reason behind the diminished training stability observed in the Deploy mode. Subsequently, we propose a novel Tune mode to bridge the gap between Eval mode and Deploy mode. The proposed Tune mode is as stable as Eval mode for transfer learning, and its computational efficiency closely matches that of the Deploy mode. Through extensive experime
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#20013;&#23545;&#25239;&#24694;&#24847;&#21407;&#21578;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#24120;&#35265;&#30340;MOR&#26041;&#26696;&#21487;&#20197;&#34987;&#24694;&#24847;&#21407;&#21578;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;</title><link>http://arxiv.org/abs/2304.06607</link><description>&lt;p&gt;
&#27169;&#22411;&#25152;&#26377;&#26435;&#20105;&#35758;&#20013;&#30340;&#34394;&#20551;&#25351;&#25511;
&lt;/p&gt;
&lt;p&gt;
False Claims against Model Ownership Resolution. (arXiv:2304.06607v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06607
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#20013;&#23545;&#25239;&#24694;&#24847;&#21407;&#21578;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#24120;&#35265;&#30340;MOR&#26041;&#26696;&#21487;&#20197;&#34987;&#24694;&#24847;&#21407;&#21578;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26159;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#26377;&#20215;&#20540;&#30693;&#35782;&#20135;&#26435;&#65292;&#26500;&#25104;&#20102;&#31454;&#20105;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20445;&#25252;&#27169;&#22411;&#19981;&#34987;&#30423;&#29992;&#30340;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#65288;MOR&#65289;&#26159;&#19968;&#31867;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#34987;&#30423;&#30340;&#25216;&#26415;&#12290;MOR&#26041;&#26696;&#20351;&#24471;&#21407;&#21578;&#26041;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#65288;&#22914;&#27700;&#21360;&#25110;&#25351;&#32441;&#65289;&#26469;&#26029;&#35328;&#23545;&#28041;&#23244;&#30423;&#29992;&#27169;&#22411;&#30340;&#34987;&#21578;&#26041;&#22768;&#31216;&#25152;&#26377;&#26435;&#65292;&#35777;&#26126;&#28041;&#23244;&#27169;&#22411;&#26159;&#34987;&#30423;&#25110;&#32773;&#28304;&#33258;&#20110;&#21407;&#21578;&#26041;&#25317;&#26377;&#30340;&#28304;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968; MOR &#26041;&#26696;&#37325;&#28857;&#25918;&#22312;&#38450;&#33539;&#24694;&#24847;&#28041;&#23244;&#26041;&#26041;&#38754;&#65292;&#30830;&#20445;&#22914;&#26524;&#28041;&#23244;&#27169;&#22411;&#30830;&#23454;&#26159;&#34987;&#30423;&#29256;&#65292;&#21017;&#21407;&#21578;&#26041;&#23558;&#33719;&#32988;&#12290;&#20294;&#26159;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24120;&#35265; MOR &#26041;&#26696;&#23384;&#22312;&#30528;&#21478;&#19968;&#20010;&#21516;&#31561;&#37325;&#35201;&#20294;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65306;&#24694;&#24847;&#21407;&#21578;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#22320;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network (DNN) models are valuable intellectual property of model owners, constituting a competitive advantage. Therefore, it is crucial to develop techniques to protect against model theft. Model ownership resolution (MOR) is a class of techniques that can deter model theft. A MOR scheme enables an accuser to assert an ownership claim for a suspect model by presenting evidence, such as a watermark or fingerprint, to show that the suspect model was stolen or derived from a source model owned by the accuser. Most of the existing MOR schemes prioritize robustness against malicious suspects, ensuring that the accuser will win if the suspect model is indeed a stolen model.  In this paper, we show that common MOR schemes in the literature are vulnerable to a different, equally important but insufficiently explored, robustness concern: a malicious accuser. We show how malicious accusers can successfully make false claims against independent suspect models that were not stolen. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21516;&#26102;&#32771;&#34385;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#25910;&#30410;&#65292;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#31639;&#27861;&#23545;&#20110;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05836</link><description>&lt;p&gt;
&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Game-theoretic Framework for Federated Learning. (arXiv:2304.05836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21516;&#26102;&#32771;&#34385;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#25910;&#30410;&#65292;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#31639;&#27861;&#23545;&#20110;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#33391;&#24615;&#21442;&#19982;&#32773;&#26088;&#22312;&#21327;&#21516;&#20248;&#21270;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#21322;&#35802;&#23454;&#30340;&#23545;&#25163;&#26102;&#65292;\textit{&#38544;&#31169;&#27844;&#28431;}&#30340;&#39118;&#38505;&#26159;&#19981;&#21487;&#24573;&#35270;&#30340;&#12290;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#19987;&#27880;&#20110;&#35774;&#35745;&#20445;&#25252;&#26426;&#21046;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#21457;&#26126;&#25915;&#20987;&#26426;&#21046;&#12290;&#34429;&#28982;&#20445;&#25252;&#32773;&#19982;&#25915;&#20987;&#32773;&#20043;&#38388;&#30340;&#26007;&#20105;&#20284;&#20046;&#27704;&#26080;&#27490;&#22659;&#65292;&#20294;&#25105;&#20204;&#20851;&#24515;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#20107;&#20808;&#39044;&#38450;&#28508;&#22312;&#30340;&#25915;&#20987;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;FL&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#30456;&#24212;&#25910;&#30410;&#65292;&#20854;&#20013;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;&#27492;&#28216;&#25103;&#31216;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#65292;&#22312;&#20854;&#20013;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#37117;&#19981;&#30693;&#36947;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#25910;&#30410;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#22266;&#26377;&#30340;\textit{&#19981;&#23436;&#20840;&#20449;&#24687;}&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;FLSG&#19982;&#19968;&#20010;\textit{oracle}&#30456;&#20851;&#32852;&#65292;&#35813;oracle&#20855;&#26377;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#25910;&#30410;&#30693;&#35782;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#21508;&#31181;&#25928;&#29992;&#20989;&#25968;&#21644;&#25915;&#20987;&#27169;&#22411;&#32452;&#21512;&#19979;FLSG&#30340;&#32435;&#20160;&#22343;&#34913;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#23454;&#39564;&#32467;&#26524;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;FL&#22330;&#26223;&#20013;&#30340;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning, benign participants aim to optimize a global model collaboratively. However, the risk of \textit{privacy leakage} cannot be ignored in the presence of \textit{semi-honest} adversaries. Existing research has focused either on designing protection mechanisms or on inventing attacking mechanisms. While the battle between defenders and attackers seems never-ending, we are concerned with one critical question: is it possible to prevent potential attacks in advance? To address this, we propose the first game-theoretic framework that considers both FL defenders and attackers in terms of their respective payoffs, which include computational costs, FL model utilities, and privacy leakage risks. We name this game the Federated Learning Security Game (FLSG), in which neither defenders nor attackers are aware of all participants' payoffs.  To handle the \textit{incomplete information} inherent in this situation, we propose associating the FLSG with an \textit{oracle} that ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05055</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Graph Representation Learning. (arXiv:2304.05055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#39640;&#32500;&#31232;&#30095;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#26377;&#25928;&#22320;&#32534;&#30721;&#25104;&#20302;&#32500;&#23494;&#38598;&#21521;&#37327;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#39046;&#22495;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#20256;&#32479;&#22270;&#23884;&#20837;&#26041;&#27861;&#36981;&#24490;&#36825;&#26679;&#19968;&#31181;&#22522;&#26412;&#24605;&#24819;&#65292;&#21363;&#22270;&#20013;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#30340;&#23884;&#20837;&#30690;&#37327;&#20173;&#28982;&#33021;&#22815;&#20445;&#25345;&#30456;&#23545;&#25509;&#36817;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;i&#65289;&#20256;&#32479;&#26041;&#27861;&#30340;&#27169;&#22411;&#23481;&#37327;&#21463;&#38480;&#65292;&#38480;&#21046;&#20102;&#23398;&#20064;&#24615;&#33021;; &#65288;ii&#65289;&#29616;&#26377;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#26080;&#27861;&#19982;&#26368;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#30456;&#32467;&#21512;&#65307;&#65288;iii&#65289;&#34920;&#31034;&#23398;&#20064;&#21644;&#19979;&#28216;&#20219;&#21153;&#30456;&#20114;&#20381;&#23384;&#65292;&#24212;&#20849;&#21516;&#21152;&#24378;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26174;&#30528;&#25104;&#21151;&#65292;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#22823;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#65292;BNN&#30340;&#21518;&#39564;&#20855;&#26377;&#26799;&#24230;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#33030;&#24369;&#24615;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2207.06154</link><description>&lt;p&gt;
&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Bayesian Neural Networks to Adversarial Attacks. (arXiv:2207.06154v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#22823;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#65292;BNN&#30340;&#21518;&#39564;&#20855;&#26377;&#26799;&#24230;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#33030;&#24369;&#24615;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#26159;&#28145;&#24230;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#21644;&#29702;&#35770;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#35757;&#32451;&#20986;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#20960;&#20309;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#20010;&#26497;&#38480;&#19979;&#65292;&#26799;&#24230;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#26159;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#36864;&#21270;&#23548;&#33268;&#30340;&#65292;&#20063;&#23601;&#26159;&#24403;&#25968;&#25454;&#20301;&#20110;&#29615;&#22659;&#31354;&#38388;&#30340;&#19968;&#20010;&#20302;&#32500;&#23376;&#27969;&#24418;&#19978;&#26102;&#12290;&#20316;&#20026;&#30452;&#25509;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36825;&#20010;&#26497;&#38480;&#19979;&#65292;BNN&#30340;&#21518;&#39564;&#23545;&#26799;&#24230;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#20174;&#21518;&#39564;&#20013;&#37319;&#26679;&#30340;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#23545;&#26799;&#24230;&#25915;&#20987;&#37117;&#20855;&#26377;&#33030;&#24369;&#24615;&#65292;&#25439;&#22833;&#20989;&#25968;&#23545;BNN&#21518;&#39564;&#20998;&#24067;&#30340;&#26399;&#26395;&#26799;&#24230;&#20173;&#28982;&#36235;&#20110;&#38646;&#12290;&#22312;t&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vulnerability to adversarial attacks is one of the principal hurdles to the adoption of deep learning in safety-critical applications. Despite significant efforts, both practical and theoretical, training deep learning models robust to adversarial attacks is still an open problem. In this paper, we analyse the geometry of adversarial attacks in the large-data, overparameterized limit for Bayesian Neural Networks (BNNs). We show that, in the limit, vulnerability to gradient-based attacks arises as a result of degeneracy in the data distribution, i.e., when the data lies on a lower-dimensional submanifold of the ambient space. As a direct consequence, we demonstrate that in this limit BNN posteriors are robust to gradient-based adversarial attacks. Crucially, we prove that the expected gradient of the loss with respect to the BNN posterior distribution is vanishing, even when each neural network sampled from the posterior is vulnerable to gradient-based attacks. Experimental results on t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30524;&#30555;&#35270;&#22270;&#35270;&#39057;&#12289;&#27880;&#35270;&#30524;&#21160;&#21644;&#32908;&#30005;&#36827;&#34892;&#25569;&#25345;&#24847;&#22270;&#25512;&#29702;&#30340;&#36125;&#21494;&#26031;&#35777;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#20551;&#32930;&#25163;&#25511;&#21046;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2104.03893</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20551;&#32930;&#25163;&#25511;&#21046;&#20013;&#30340;&#32908;&#30005;&#21644;&#35270;&#35273;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in Prosthetic Hand Control. (arXiv:2104.03893v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.03893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30524;&#30555;&#35270;&#22270;&#35270;&#39057;&#12289;&#27880;&#35270;&#30524;&#21160;&#21644;&#32908;&#30005;&#36827;&#34892;&#25569;&#25345;&#24847;&#22270;&#25512;&#29702;&#30340;&#36125;&#21494;&#26031;&#35777;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#20551;&#32930;&#25163;&#25511;&#21046;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#23545;&#20110;&#19979;&#32930;&#25130;&#32930;&#32773;&#65292;&#20351;&#29992;&#26426;&#22120;&#20154;&#20551;&#32930;&#25163;&#21487;&#20197;&#24674;&#22797;&#36827;&#34892;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#22522;&#20110;&#29983;&#29702;&#20449;&#21495;&#65288;&#22914;&#32908;&#30005;&#65289;&#30340;&#25511;&#21046;&#26041;&#27861;&#23481;&#26131;&#22240;&#20026;&#36816;&#21160;&#20266;&#36857;&#12289;&#32908;&#32905;&#30130;&#21171;&#31561;&#21407;&#22240;&#23548;&#33268;&#25512;&#29702;&#32467;&#26524;&#19981;&#20339;&#12290;&#35270;&#35273;&#20256;&#24863;&#22120;&#26159;&#20851;&#20110;&#29615;&#22659;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#26469;&#28304;&#65292;&#21487;&#20197;&#22312;&#25512;&#26029;&#21487;&#34892;&#21644;&#39044;&#26399;&#25163;&#21183;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#35777;&#25454;&#20063;&#23481;&#26131;&#21463;&#21040;&#33258;&#36523;&#20266;&#36857;&#30340;&#24433;&#21709;&#65292;&#26368;&#24120;&#35265;&#30340;&#21407;&#22240;&#26159;&#29289;&#20307;&#36974;&#25377;&#12289;&#20809;&#29031;&#21464;&#21270;&#31561;&#12290;&#20351;&#29992;&#29983;&#29702;&#21644;&#35270;&#35273;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22810;&#27169;&#24577;&#35777;&#25454;&#34701;&#21512;&#26159;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#24577;&#20855;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#12290;&#26041;&#27861;&#65306;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#35777;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#30524;&#30555;&#35270;&#22270;&#35270;&#39057;&#12289;&#27880;&#35270;&#30524;&#21160;&#21644;&#32908;&#30005;&#20174;&#21069;&#33218;&#36827;&#34892;&#25569;&#25345;&#24847;&#22270;&#25512;&#29702;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20010;&#20307;&#21644;&#34701;&#21512;&#24615;&#33021;&#19982;&#26576;&#20123;&#22240;&#32032;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: For lower arm amputees, robotic prosthetic hands promise to regain the capability to perform daily living activities. Current control methods based on physiological signals such as electromyography (EMG) are prone to yielding poor inference outcomes due to motion artifacts, muscle fatigue, and many more. Vision sensors are a major source of information about the environment state and can play a vital role in inferring feasible and intended gestures. However, visual evidence is also susceptible to its own artifacts, most often due to object occlusion, lighting changes, etc. Multimodal evidence fusion using physiological and vision sensor measurements is a natural approach due to the complementary strengths of these modalities. Methods: In this paper, we present a Bayesian evidence fusion framework for grasp intent inference using eye-view video, eye-gaze, and EMG from the forearm processed by neural network models. We analyze individual and fused performance as a function of 
&lt;/p&gt;</description></item></channel></rss>