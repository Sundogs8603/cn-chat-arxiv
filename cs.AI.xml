<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>AdaptiX&#26159;&#19968;&#20010;&#36807;&#28193;&#24615;&#30340;XR&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#21161;&#21160;&#21147;&#26426;&#22120;&#20154;&#20013;&#30340;&#20849;&#21516;&#25511;&#21046;&#24212;&#29992;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#20223;&#30495;&#29615;&#22659;&#65292;&#24182;&#32467;&#21512;&#20102;&#29992;&#25143;&#33258;&#20027;&#24615;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#12290;</title><link>http://arxiv.org/abs/2310.15887</link><description>&lt;p&gt;
AdaptiX - &#19968;&#20010;&#29992;&#20110;&#21161;&#21160;&#21147;&#26426;&#22120;&#20154;&#20013;&#24320;&#21457;&#21644;&#35780;&#20272;&#20849;&#21516;&#25511;&#21046;&#24212;&#29992;&#30340;&#36807;&#28193;&#24615;XR&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AdaptiX -- A Transitional XR Framework for Development and Evaluation of Shared Control Applications in Assistive Robotics. (arXiv:2310.15887v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15887
&lt;/p&gt;
&lt;p&gt;
AdaptiX&#26159;&#19968;&#20010;&#36807;&#28193;&#24615;&#30340;XR&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#21161;&#21160;&#21147;&#26426;&#22120;&#20154;&#20013;&#30340;&#20849;&#21516;&#25511;&#21046;&#24212;&#29992;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#20223;&#30495;&#29615;&#22659;&#65292;&#24182;&#32467;&#21512;&#20102;&#29992;&#25143;&#33258;&#20027;&#24615;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#20204;&#25480;&#26435;&#34892;&#21160;&#21463;&#38480;&#21644;&#25216;&#26415;&#25509;&#21463;&#24230;&#30340;&#25552;&#39640;&#65292;&#22914;&#21512;&#20316;&#26426;&#22120;&#33218;&#31561;&#21161;&#21160;&#25216;&#26415;&#27491;&#22312;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26222;&#21450;&#25104;&#21151;&#21463;&#21040;&#21487;&#29992;&#24615;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#29992;&#25143;&#36755;&#20837;&#19982;&#36719;&#20214;&#25511;&#21046;&#22312;&#33258;&#20027;&#24615;&#36830;&#32493;&#24615;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20849;&#21516;&#25511;&#21046;&#27010;&#24565;&#25552;&#20379;&#20102;&#23558;&#26377;&#38024;&#23545;&#24615;&#22320;&#22686;&#21152;&#29992;&#25143;&#33258;&#20027;&#24615;&#19982;&#19968;&#23450;&#31243;&#24230;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#30456;&#32467;&#21512;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;AdaptiX&#30340;&#20813;&#36153;&#24320;&#28304;XR&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39640;&#20998;&#36776;&#29575;&#20223;&#30495;&#29615;&#22659;&#20013;&#24320;&#21457;&#21644;&#35780;&#20272;&#20849;&#21516;&#25511;&#21046;&#24212;&#29992;&#12290;&#21021;&#22987;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#20013;&#30340;&#31034;&#20363;&#24773;&#26223;&#19979;&#30340;&#27169;&#25311;&#26426;&#22120;&#33218;&#12289;&#22810;&#31181;&#26631;&#20934;&#25511;&#21046;&#25509;&#21475;&#21644;&#19968;&#20010;&#19987;&#38376;&#30340;&#35760;&#24405;/&#22238;&#25918;&#31995;&#32479;&#12290;AdaptiX&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20197;&#28385;&#36275;&#29305;&#23450;&#30340;&#30740;&#31350;&#38656;&#27714;&#65292;&#20801;&#35768;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#65288;HRI&#65289;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the ongoing efforts to empower people with mobility impairments and the increase in technological acceptance by the general public, assistive technologies, such as collaborative robotic arms, are gaining popularity. Yet, their widespread success is limited by usability issues, specifically the disparity between user input and software control along the autonomy continuum. To address this, shared control concepts provide opportunities to combine the targeted increase of user autonomy with a certain level of computer assistance. This paper presents the free and open-source AdaptiX XR framework for developing and evaluating shared control applications in a high-resolution simulation environment. The initial framework consists of a simulated robotic arm with an example scenario in Virtual Reality (VR), multiple standard control interfaces, and a specialized recording/replay system. AdaptiX can easily be extended for specific research needs, allowing Human-Robot Interaction (HRI) resea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#36830;&#25509;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19988;&#20855;&#26377;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616;&#30340;&#28508;&#21147;&#12290;&#26080;&#35770;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#22914;&#20309;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15872</link><description>&lt;p&gt;
KirchhoffNet&#65306;&#19968;&#31181;&#36830;&#25509;&#28040;&#24687;&#20256;&#36882;&#21644;&#36830;&#32493;&#28145;&#24230;&#27169;&#22411;&#30340;&#30005;&#36335;&#26725;&#25509;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models. (arXiv:2310.15872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#36830;&#25509;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19988;&#20855;&#26377;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616;&#30340;&#28508;&#21147;&#12290;&#26080;&#35770;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#22914;&#20309;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#27169;&#25311;&#30005;&#36335;&#30340;&#22522;&#26412;&#21407;&#29702;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#65292;&#24341;&#20837;&#20102;&#19968;&#31867;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#12290;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#20102;&#23494;&#20999;&#32852;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#20309;&#20256;&#32479;&#23618;&#65288;&#22914;&#21367;&#31215;&#12289;&#27744;&#21270;&#25110;&#32447;&#24615;&#23618;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#35753;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#26356;&#21152;&#26377;&#36259;&#30340;&#26159;&#20854;&#22312;&#30828;&#20214;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#24403;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#37096;&#32626;&#22312;GPU&#19978;&#12290;&#30456;&#21453;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#30005;&#36335;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26080;&#35770;&#22312;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#20869;&#26377;&#22810;&#23569;&#21442;&#25968;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20854;&#20013;f&#34920;&#31034;&#30828;&#20214;&#30340;&#26102;&#38047;&#39057;&#29575;&#12290;&#36825;&#31181;&#29305;&#24615;&#34920;&#26126;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#20855;&#26377;&#28508;&#21147;&#23454;&#29616;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we exploit a fundamental principle of analog electronic circuitry, Kirchhoff's current law, to introduce a unique class of neural network models that we refer to as KirchhoffNet. KirchhoffNet establishes close connections with message passing neural networks and continuous-depth networks. We demonstrate that even in the absence of any traditional layers (such as convolution, pooling, or linear layers), KirchhoffNet attains 98.86% test accuracy on the MNIST dataset, comparable with state of the art (SOTA) results. What makes KirchhoffNet more intriguing is its potential in the realm of hardware. Contemporary deep neural networks are conventionally deployed on GPUs. In contrast, KirchhoffNet can be physically realized by an analog electronic circuit. Moreover, we justify that irrespective of the number of parameters within a KirchhoffNet, its forward calculation can always be completed within 1/f seconds, with f representing the hardware's clock frequency. This characteris
&lt;/p&gt;</description></item><item><title>&#38754;&#21521;&#25512;&#33616;&#30340;&#25299;&#25169;&#24863;&#30693;&#21435;&#20559;&#21521;&#33258;&#30417;&#30563;&#22270;&#23398;&#20064;&#65288;TDSGL&#65289;&#36890;&#36807;&#26500;&#24314;&#23545;&#27604;&#23545;&#65292;&#32771;&#34385;&#29992;&#25143;&#65288;&#29289;&#21697;&#65289;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#36127;&#37319;&#26679;&#31574;&#30053;&#23548;&#33268;&#30340;&#38169;&#35823;&#36127;&#26679;&#26412;&#21644;&#24573;&#30053;&#27491;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15858</link><description>&lt;p&gt;
&#38754;&#21521;&#25512;&#33616;&#30340;&#25299;&#25169;&#24863;&#30693;&#21435;&#20559;&#21521;&#33258;&#30417;&#30563;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Topology-aware Debiased Self-supervised Graph Learning for Recommendation. (arXiv:2310.15858v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15858
&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#25512;&#33616;&#30340;&#25299;&#25169;&#24863;&#30693;&#21435;&#20559;&#21521;&#33258;&#30417;&#30563;&#22270;&#23398;&#20064;&#65288;TDSGL&#65289;&#36890;&#36807;&#26500;&#24314;&#23545;&#27604;&#23545;&#65292;&#32771;&#34385;&#29992;&#25143;&#65288;&#29289;&#21697;&#65289;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#36127;&#37319;&#26679;&#31574;&#30053;&#23548;&#33268;&#30340;&#38169;&#35823;&#36127;&#26679;&#26412;&#21644;&#24573;&#30053;&#27491;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#22522;&#20110;&#22270;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#22270;&#23545;&#27604;&#23398;&#20064;&#26469;&#32531;&#35299;&#25968;&#25454;&#31232;&#30095;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#20013;&#30340;&#38543;&#26426;&#36127;&#37319;&#26679;&#31574;&#30053;&#24573;&#35270;&#20102;&#29992;&#25143;&#65288;&#29289;&#21697;&#65289;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#36825;&#19981;&#20165;&#24341;&#20837;&#20102;&#38169;&#35823;&#30340;&#36127;&#26679;&#26412;&#65288;&#19982;&#38170;&#23450;&#29992;&#25143;&#65288;&#29289;&#21697;&#65289;&#30456;&#20284;&#30340;&#36127;&#26679;&#26412;&#65289;&#65292;&#36824;&#24573;&#30053;&#20102;&#28508;&#22312;&#30340;&#27491;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38754;&#21521;&#25512;&#33616;&#30340;&#25299;&#25169;&#24863;&#30693;&#21435;&#20559;&#21521;&#33258;&#30417;&#30563;&#22270;&#23398;&#20064;&#65288;TDSGL&#65289;&#65292;&#26681;&#25454;&#29992;&#25143;&#65288;&#29289;&#21697;&#65289;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26500;&#24314;&#23545;&#27604;&#23545;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#21407;&#22987;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#24456;&#22909;&#22320;&#21453;&#26144;&#20102;&#29992;&#25143;&#30340;&#36141;&#20080;&#24847;&#22270;&#21644;&#29289;&#21697;&#30340;&#26576;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#22312;&#20132;&#20114;&#25968;&#25454;&#19978;&#35745;&#31639;&#29992;&#25143;&#65288;&#29289;&#21697;&#65289;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#28982;&#21518;&#65292;&#32473;&#23450;&#19968;&#20010;&#29992;&#25143;&#65288;&#29289;&#21697;&#65289;&#65292;&#25105;&#20204;&#36890;&#36807;&#36873;&#25321;&#23884;&#20837;&#19981;&#21516;&#35821;&#20041;&#32467;&#26500;&#30340;&#29992;&#25143;&#65288;&#29289;&#21697;&#65289;&#26469;&#26500;&#24314;&#20854;&#36127;&#26679;&#26412;&#23545;&#65292;&#20197;&#30830;&#20445;&#21435;&#20559;&#21521;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recommendation, graph-based Collaborative Filtering (CF) methods mitigate the data sparsity by introducing Graph Contrastive Learning (GCL). However, the random negative sampling strategy in these GCL-based CF models neglects the semantic structure of users (items), which not only introduces false negatives (negatives that are similar to anchor user (item)) but also ignores the potential positive samples. To tackle the above issues, we propose Topology-aware Debiased Self-supervised Graph Learning (TDSGL) for recommendation, which constructs contrastive pairs according to the semantic similarity between users (items). Specifically, since the original user-item interaction data commendably reflects the purchasing intent of users and certain characteristics of items, we calculate the semantic similarity between users (items) on interaction data. Then, given a user (item), we construct its negative pairs by selecting users (items) which embed different semantic structures to ensure the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20154;&#24037;&#27861;&#35821;&#25968;&#25454;&#29983;&#25104;&#30340;&#35821;&#26009;&#24211;&#65292;&#25506;&#32034;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21457;&#29616;&#21333;&#35789;&#30340;&#24615;&#21035;&#23646;&#24615;&#20197;&#21450;&#20854;&#20351;&#29992;&#35268;&#21017;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#26159;&#21542;&#27491;&#30830;&#25429;&#25417;&#21040;&#24615;&#21035;&#20449;&#24687;&#25110;&#34920;&#29616;&#20986;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.15852</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#27861;&#35821;&#25968;&#25454;&#65292;&#20102;&#35299;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models. (arXiv:2310.15852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20154;&#24037;&#27861;&#35821;&#25968;&#25454;&#29983;&#25104;&#30340;&#35821;&#26009;&#24211;&#65292;&#25506;&#32034;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21457;&#29616;&#21333;&#35789;&#30340;&#24615;&#21035;&#23646;&#24615;&#20197;&#21450;&#20854;&#20351;&#29992;&#35268;&#21017;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#26159;&#21542;&#27491;&#30830;&#25429;&#25417;&#21040;&#24615;&#21035;&#20449;&#24687;&#25110;&#34920;&#29616;&#20986;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#30452;&#25509;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#23398;&#20064;&#21508;&#31181;&#35821;&#35328;&#23646;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#30528;&#25163;&#25506;&#32034;&#31070;&#32463;&#27169;&#22411;&#22914;&#20309;&#21457;&#29616;&#21333;&#35789;&#30340;&#35821;&#35328;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#65289;&#20197;&#21450;&#35268;&#21017;&#30340;&#20351;&#29992;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#30456;&#23545;&#23569;&#26377;&#30740;&#31350;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30001;&#22522;&#20110;&#27861;&#35821;&#30340;PCFG&#29983;&#25104;&#30340;&#20154;&#24037;&#35821;&#26009;&#24211;&#65292;&#31934;&#30830;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#24615;&#21035;&#20998;&#24067;&#65292;&#24182;&#30830;&#23450;&#27169;&#22411;&#22312;&#21738;&#31181;&#26465;&#20214;&#19979;&#33021;&#27491;&#30830;&#25429;&#25417;&#21040;&#24615;&#21035;&#20449;&#24687;&#65292;&#25110;&#32773;&#30456;&#21453;&#65292;&#26174;&#31034;&#20986;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous studies have demonstrated the ability of neural language models to learn various linguistic properties without direct supervision. This work takes an initial step towards exploring the less researched topic of how neural models discover linguistic properties of words, such as gender, as well as the rules governing their usage. We propose to use an artificial corpus generated by a PCFG based on French to precisely control the gender distribution in the training data and determine under which conditions a model correctly captures gender information or, on the contrary, appears gender-biased.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26377;&#25928;&#20272;&#35745;&#21160;&#24577;PET&#25104;&#20687;&#20013;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#23545;&#25239;&#21069;&#21521;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#20351;&#29992;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20248;&#21270;&#65292;&#25105;&#20204;&#33021;&#22815;&#29992;&#31616;&#21333;&#30340;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#25512;&#26029;&#20986;&#21518;&#39564;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.15850</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#21160;&#24577;PET&#25104;&#20687;&#30340;&#21518;&#39564;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Posterior Estimation for Dynamic PET imaging using Conditional Variational Inference. (arXiv:2310.15850v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26377;&#25928;&#20272;&#35745;&#21160;&#24577;PET&#25104;&#20687;&#20013;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#23545;&#25239;&#21069;&#21521;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#20351;&#29992;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20248;&#21270;&#65292;&#25105;&#20204;&#33021;&#22815;&#29992;&#31616;&#21333;&#30340;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#25512;&#26029;&#20986;&#21518;&#39564;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#26377;&#25928;&#22320;&#20272;&#35745;&#21160;&#24577;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25104;&#20687;&#65288;PET&#65289;&#20013;&#21160;&#21147;&#23398;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#32473;&#23450;&#26102;&#38388;-&#27963;&#24615;&#26354;&#32447;&#30340;&#27979;&#37327;&#12290;&#32771;&#34385;&#21040;&#20351;&#29992;&#21069;&#21521;&#21160;&#24577;&#27169;&#22411;&#20174;&#21442;&#25968;&#25104;&#20687;&#21040;&#27979;&#37327;&#31354;&#38388;&#20013;&#30340;&#22266;&#26377;&#20449;&#24687;&#25439;&#22833;&#65292;&#21453;&#21521;&#26144;&#23556;&#26159;&#27169;&#31946;&#30340;&#12290;&#20256;&#32479;&#65288;&#20294;&#26114;&#36149;&#65289;&#30340;&#35299;&#20915;&#26041;&#27861;&#21487;&#20197;&#26159;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#37319;&#26679;&#65292;&#24050;&#30693;&#21487;&#20135;&#29983;&#26080;&#20559;&#30340;&#28176;&#36817;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#21518;&#39564;&#20272;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#23545;&#25239;&#21069;&#21521;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#24182;&#20248;&#21270;&#20854;&#35777;&#25454;&#19979;&#30028;&#12290;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#35299;&#30721;&#22120;&#33021;&#22815;&#26681;&#25454;&#32473;&#23450;&#30340;&#27979;&#37327;&#21644;&#37319;&#26679;&#30340;&#28508;&#22312;&#21464;&#37327;&#25512;&#26029;&#20986;&#21518;&#39564;&#27010;&#29575;&#65292;&#21518;&#39564;&#27010;&#29575;&#36981;&#24490;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#26080;&#20559;&#30340;MCMC&#39564;&#35777;&#20102;&#25105;&#20204;CVAE&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims efficiently estimating the posterior distribution of kinetic parameters for dynamic positron emission tomography (PET) imaging given a measurement of time of activity curve. Considering the inherent information loss from parametric imaging to measurement space with the forward kinetic model, the inverse mapping is ambiguous. The conventional (but expensive) solution can be the Markov Chain Monte Carlo (MCMC) sampling, which is known to produce unbiased asymptotical estimation. We propose a deep-learning-based framework for efficient posterior estimation. Specifically, we counteract the information loss in the forward process by introducing latent variables. Then, we use a conditional variational autoencoder (CVAE) and optimize its evidence lower bound. The well-trained decoder is able to infer the posterior with a given measurement and the sampled latent variables following a simple multivariate Gaussian distribution. We validate our CVAE-based method using unbiased MCMC
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion Weighted Graph Framework (DWGF)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#32467;&#26500;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20805;&#20998;&#21644;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#26032;&#24847;&#22270;&#21457;&#29616;&#20013;&#26080;&#27861;&#24179;&#34913;&#25968;&#37327;&#21644;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15836</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26032;&#24847;&#22270;&#21457;&#29616;&#30340;&#25193;&#25955;&#21152;&#26435;&#22270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Diffusion Weighted Graph Framework for New Intent Discovery. (arXiv:2310.15836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion Weighted Graph Framework (DWGF)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#32467;&#26500;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20805;&#20998;&#21644;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#26032;&#24847;&#22270;&#21457;&#29616;&#20013;&#26080;&#27861;&#24179;&#34913;&#25968;&#37327;&#21644;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#24847;&#22270;&#21457;&#29616;&#26088;&#22312;&#36890;&#36807;&#26377;&#38480;&#30340;&#24102;&#26377;&#24050;&#30693;&#24847;&#22270;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#24110;&#21161;&#65292;&#35782;&#21035;&#20986;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#26032;&#24847;&#22270;&#21644;&#24050;&#30693;&#24847;&#22270;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#26410;&#32771;&#34385;&#26679;&#26412;&#20043;&#38388;&#30340;&#32467;&#26500;&#20851;&#31995;&#65292;&#29983;&#25104;&#30340;&#22122;&#22768;&#30417;&#30563;&#20449;&#21495;&#26080;&#27861;&#22312;&#25968;&#37327;&#21644;&#36136;&#37327;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#38459;&#30861;&#20102;&#26032;&#24847;&#22270;&#32858;&#31867;&#30340;&#24418;&#25104;&#21644;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#26377;&#25928;&#20256;&#36882;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#21152;&#26435;&#22270;&#26694;&#26550;&#65288;DWGF&#65289;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#32467;&#26500;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20805;&#20998;&#21644;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#27599;&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#27839;&#30528;&#30001;&#26368;&#36817;&#37051;&#25351;&#23548;&#30340;&#35821;&#20041;&#36335;&#24452;&#25193;&#25955;&#37051;&#22495;&#20851;&#31995;&#65292;&#20197;&#37492;&#21035;&#22320;&#21051;&#30011;&#20854;&#23616;&#37096;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#23616;&#37096;&#32467;&#26500;&#23545;&#20854;&#27491;&#26679;&#26412;&#36827;&#34892;&#25277;&#26679;&#21644;&#21152;&#26435;&#65292;&#29992;&#20110;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
New Intent Discovery (NID) aims to recognize both new and known intents from unlabeled data with the aid of limited labeled data containing only known intents. Without considering structure relationships between samples, previous methods generate noisy supervisory signals which cannot strike a balance between quantity and quality, hindering the formation of new intent clusters and effective transfer of the pre-training knowledge. To mitigate this limitation, we propose a novel Diffusion Weighted Graph Framework (DWGF) to capture both semantic similarities and structure relationships inherent in data, enabling more sufficient and reliable supervisory signals. Specifically, for each sample, we diffuse neighborhood relationships along semantic paths guided by the nearest neighbors for multiple hops to characterize its local structure discriminately. Then, we sample its positive keys and weigh them based on semantic similarities and local structures for contrastive learning. During inferen
&lt;/p&gt;</description></item><item><title>&#37325;&#24230;&#22686;&#24378;&#12289;&#39640;&#20998;&#36776;&#29575;3D ResUNet&#33258;&#21160;&#20027;&#21160;&#33033;&#20998;&#21106;&#22312;SEG.A&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25104;&#32489;&#65292;&#36798;&#21040;&#20102;&#25152;&#26377;&#27979;&#35797;&#26696;&#20363;0.9&#20197;&#19978;&#30340;Dice&#20998;&#25968;&#65292;&#24182;&#22312;&#31283;&#23450;&#24615;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#21442;&#19982;&#32773;&#12290;&#23427;&#22312;&#20020;&#24202;&#35780;&#20272;&#12289;&#23450;&#37327;&#32467;&#26524;&#21644;&#20307;&#31215;&#32593;&#26684;&#36136;&#37327;&#26041;&#38754;&#21517;&#21015;&#21069;&#33541;&#12290;</title><link>http://arxiv.org/abs/2310.15827</link><description>&lt;p&gt;
&#12298;&#37325;&#24230;&#22686;&#24378;&#12289;&#39640;&#20998;&#36776;&#29575;3D ResUNet&#33258;&#21160;&#20027;&#21160;&#33033;&#20998;&#21106;&#65306;&#23545;SEG.A&#25361;&#25112;&#30340;&#36129;&#29486;&#12299;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Automatic Aorta Segmentation with Heavily Augmented, High-Resolution 3-D ResUNet: Contribution to the SEG.A Challenge. (arXiv:2310.15827v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15827
&lt;/p&gt;
&lt;p&gt;
&#37325;&#24230;&#22686;&#24378;&#12289;&#39640;&#20998;&#36776;&#29575;3D ResUNet&#33258;&#21160;&#20027;&#21160;&#33033;&#20998;&#21106;&#22312;SEG.A&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25104;&#32489;&#65292;&#36798;&#21040;&#20102;&#25152;&#26377;&#27979;&#35797;&#26696;&#20363;0.9&#20197;&#19978;&#30340;Dice&#20998;&#25968;&#65292;&#24182;&#22312;&#31283;&#23450;&#24615;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#21442;&#19982;&#32773;&#12290;&#23427;&#22312;&#20020;&#24202;&#35780;&#20272;&#12289;&#23450;&#37327;&#32467;&#26524;&#21644;&#20307;&#31215;&#32593;&#26684;&#36136;&#37327;&#26041;&#38754;&#21517;&#21015;&#21069;&#33541;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;3D&#21307;&#23398;&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#21106;&#20027;&#21160;&#33033;&#26159;&#19968;&#39033;&#37325;&#35201;&#20294;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#35768;&#22810;&#22240;&#32032;&#20351;&#24471;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#27604;&#22914;&#20027;&#21160;&#33033;&#22841;&#23618;&#30340;&#21487;&#33021;&#24615;&#25110;&#32773;&#23545;&#23567;&#20998;&#25903;&#36827;&#34892;&#20998;&#21106;&#21644;&#27880;&#37322;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MedGIFT&#22242;&#38431;&#22312;MICCAI 2023&#20250;&#35758;&#26399;&#38388;&#32452;&#32455;&#30340;SEG.A&#25361;&#25112;&#20013;&#30340;&#19968;&#39033;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32534;&#30721;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#20840;&#33258;&#21160;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20027;&#35201;&#20551;&#35774;&#26159;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#22686;&#24378;&#27604;&#28145;&#24230;&#26550;&#26500;&#26356;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#20256;&#32479;&#30340;&#21367;&#31215;U-Net&#30340;&#21464;&#20307;&#12290;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#25152;&#26377;&#27979;&#35797;&#26696;&#20363;&#20013;&#37117;&#23454;&#29616;&#20102;0.9&#20197;&#19978;&#30340;Dice&#20998;&#25968;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#21442;&#19982;&#32773;&#20013;&#20855;&#26377;&#26368;&#39640;&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#20020;&#24202;&#35780;&#20272;&#12289;&#23450;&#37327;&#32467;&#26524;&#21644;&#20307;&#31215;&#32593;&#26684;&#36136;&#37327;&#26041;&#38754;&#20998;&#21035;&#25490;&#21517;&#31532;&#19968;&#12289;&#31532;&#22235;&#21644;&#31532;&#19977;&#12290;&#25105;&#20204;&#20813;&#36153;&#20844;&#24320;&#28304;&#20195;&#30721;&#65292;
&lt;/p&gt;
&lt;p&gt;
Automatic aorta segmentation from 3-D medical volumes is an important yet difficult task. Several factors make the problem challenging, e.g. the possibility of aortic dissection or the difficulty with segmenting and annotating the small branches. This work presents a contribution by the MedGIFT team to the SEG.A challenge organized during the MICCAI 2023 conference. We propose a fully automated algorithm based on deep encoder-decoder architecture. The main assumption behind our work is that data preprocessing and augmentation are much more important than the deep architecture, especially in low data regimes. Therefore, the solution is based on a variant of traditional convolutional U-Net. The proposed solution achieved a Dice score above 0.9 for all testing cases with the highest stability among all participants. The method scored 1st, 4th, and 3rd in terms of the clinical evaluation, quantitative results, and volumetric meshing quality, respectively. We freely release the source code,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;Rosetta Stone&#30340;&#24212;&#29992;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#24212;&#29992;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#20013;&#12290;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.15823</link><description>&lt;p&gt;
Rosetta Stone&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;&#65306;&#20174;&#35821;&#35328;&#24314;&#27169;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#30340;&#36291;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word--Definition Alignment. (arXiv:2310.15823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;Rosetta Stone&#30340;&#24212;&#29992;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#24212;&#29992;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#20013;&#12290;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#35789;&#20856;&#26159;&#19968;&#31181;&#24037;&#20855;&#65292;&#21487;&#26681;&#25454;&#25552;&#20379;&#30340;&#23450;&#20041;&#12289;&#21547;&#20041;&#25110;&#25551;&#36848;&#26469;&#21457;&#29616;&#19968;&#20010;&#35789;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#37117;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#21487;&#20197;&#24110;&#21161;&#25484;&#25569;&#19968;&#20010;&#35789;&#30340;&#25551;&#36848;&#32780;&#19981;&#30693;&#20854;&#36523;&#20221;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#65292;&#24182;&#20351;&#23547;&#27714;&#31934;&#30830;&#26415;&#35821;&#30340;&#20889;&#20316;&#32773;&#21463;&#30410;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#28085;&#30422;&#34987;&#31216;&#20026;&#8220;&#33292;&#23574;&#19978;&#30340;&#35789;&#8221;&#29616;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#25105;&#20204;&#22312;&#38463;&#25289;&#20271;&#35821;&#21453;&#21521;&#35789;&#20856;&#20849;&#20139;&#20219;&#21153;&#20013;&#33719;&#32988;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#20219;&#21153;&#30340;&#37325;&#28857;&#26159;&#20174;&#20276;&#38543;&#30340;&#25551;&#36848;&#20013;&#25512;&#23548;&#20986;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#20849;&#20139;&#20219;&#21153;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153;&#65306;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#28041;&#21450;&#19968;&#20010;&#38463;&#25289;&#20271;&#23450;&#20041;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780;&#31532;&#20108;&#20010;&#23376;&#20219;&#21153;&#21017;&#20351;&#29992;&#19968;&#20010;&#33521;&#25991;&#23450;&#20041;&#12290;&#23545;&#20110;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#32463;&#36807;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#65292;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#12290;&#26368;&#32456;&#34920;&#31034;&#26159;&#36890;&#36807;&#23545;&#27599;&#20010;&#27169;&#22411;&#36755;&#20986;&#30340;&#23884;&#20837;&#36827;&#34892;&#24179;&#22343;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Reverse Dictionary is a tool enabling users to discover a word based on its provided definition, meaning, or description. Such a technique proves valuable in various scenarios, aiding language learners who possess a description of a word without its identity, and benefiting writers seeking precise terminology. These scenarios often encapsulate what is referred to as the "Tip-of-the-Tongue" (TOT) phenomena. In this work, we present our winning solution for the Arabic Reverse Dictionary shared task. This task focuses on deriving a vector representation of an Arabic word from its accompanying description. The shared task encompasses two distinct subtasks: the first involves an Arabic definition as input, while the second employs an English definition. For the first subtask, our approach relies on an ensemble of finetuned Arabic BERT-based models, predicting the word embedding for a given definition. The final representation is obtained through averaging the output embeddings from each m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#21028;&#21035;&#22120;&#24341;&#23548;&#65292;&#29992;&#20110;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#21028;&#21035;&#22120;&#26469;&#32416;&#27491;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#26469;&#24212;&#23545;&#20351;&#29992;&#27425;&#20248;&#21028;&#21035;&#22120;&#30340;&#24773;&#20917;&#12290;&#22312;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#20219;&#21153;&#20013;&#65292;&#21028;&#21035;&#22120;&#24341;&#23548;&#26377;&#21161;&#20110;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15817</link><description>&lt;p&gt;
&#21028;&#21035;&#22120;&#24341;&#23548;&#19979;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discriminator Guidance for Autoregressive Diffusion Models. (arXiv:2310.15817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#21028;&#21035;&#22120;&#24341;&#23548;&#65292;&#29992;&#20110;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#21028;&#21035;&#22120;&#26469;&#32416;&#27491;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#26469;&#24212;&#23545;&#20351;&#29992;&#27425;&#20248;&#21028;&#21035;&#22120;&#30340;&#24773;&#20917;&#12290;&#22312;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#20219;&#21153;&#20013;&#65292;&#21028;&#21035;&#22120;&#24341;&#23548;&#26377;&#21161;&#20110;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#21028;&#21035;&#22120;&#24341;&#23548;&#12290;&#22312;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#21028;&#21035;&#22120;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#20351;&#29992;&#36807;&#65292;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#22312;&#31163;&#25955;&#24773;&#20917;&#19979;&#20351;&#29992;&#21028;&#21035;&#22120;&#21644;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#26368;&#20248;&#21028;&#21035;&#22120;&#23558;&#32416;&#27491;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#33021;&#22815;&#20174;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#20013;&#31934;&#30830;&#37319;&#26679;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#24212;&#23545;&#20351;&#29992;&#27425;&#20248;&#21028;&#21035;&#22120;&#30340;&#23454;&#38469;&#24773;&#20917;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#36845;&#20195;&#22320;&#23558;&#21028;&#21035;&#22120;&#30340;&#39044;&#27979;&#32435;&#20837;&#32771;&#34385;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#21028;&#21035;&#22120;&#30456;&#36739;&#20110;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26102;&#30340;&#29983;&#25104;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce discriminator guidance in the setting of Autoregressive Diffusion Models. The use of a discriminator to guide a diffusion process has previously been used for continuous diffusion models, and in this work we derive ways of using a discriminator together with a pretrained generative model in the discrete case. First, we show that using an optimal discriminator will correct the pretrained model and enable exact sampling from the underlying data distribution. Second, to account for the realistic scenario of using a sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which iteratively takes the predictions from the discrimiator into account during the generation process. We test these approaches on the task of generating molecular graphs and show how the discriminator improves the generative performance over using only the pretrained model.
&lt;/p&gt;</description></item><item><title>DALE&#26159;&#19968;&#20010;&#29992;&#20110;&#20302;&#36164;&#28304;&#27861;&#24459;NLP&#30340;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#36873;&#25321;&#24615;&#25513;&#30721;&#30340;&#26080;&#30417;&#30563;&#25991;&#26412;&#21435;&#22122;&#30446;&#26631;&#39044;&#35757;&#32451;&#65292;&#22312;&#35299;&#20915;&#27861;&#24459;&#35821;&#35328;&#29305;&#24322;&#24615;&#30340;&#21516;&#26102;&#29983;&#25104;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2310.15799</link><description>&lt;p&gt;
DALE: &#29992;&#20110;&#20302;&#36164;&#28304;&#27861;&#24459;NLP&#30340;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
DALE: Generative Data Augmentation for Low-Resource Legal NLP. (arXiv:2310.15799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15799
&lt;/p&gt;
&lt;p&gt;
DALE&#26159;&#19968;&#20010;&#29992;&#20110;&#20302;&#36164;&#28304;&#27861;&#24459;NLP&#30340;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#36873;&#25321;&#24615;&#25513;&#30721;&#30340;&#26080;&#30417;&#30563;&#25991;&#26412;&#21435;&#22122;&#30446;&#26631;&#39044;&#35757;&#32451;&#65292;&#22312;&#35299;&#20915;&#27861;&#24459;&#35821;&#35328;&#29305;&#24322;&#24615;&#30340;&#21516;&#26102;&#29983;&#25104;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DALE&#65292;&#19968;&#20010;&#38024;&#23545;&#20302;&#36164;&#28304;&#27861;&#24459;NLP&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;DALE&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#22312;&#29983;&#25104;&#27861;&#24459;&#25991;&#20214;&#30340;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#26041;&#38754;&#23384;&#22312;&#30340;&#25361;&#25112; - &#27861;&#24459;&#35821;&#35328;&#20855;&#26377;&#19987;&#38376;&#30340;&#35789;&#27719;&#21644;&#22797;&#26434;&#30340;&#35821;&#20041;&#12289;&#24418;&#24577;&#21644;&#21477;&#27861;&#65292;&#24182;&#19981;&#33021;&#20174;&#20165;&#20165;&#23545;&#28304;&#21477;&#23376;&#36827;&#34892;&#25913;&#36848;&#30340;&#25968;&#25454;&#22686;&#24378;&#20013;&#21463;&#30410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;DALE&#26159;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#65292;&#23427;&#22312;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25991;&#26412;&#21435;&#22122;&#30446;&#26631;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#35813;&#30446;&#26631;&#22522;&#20110;&#36873;&#25321;&#24615;&#25513;&#30721; - &#25105;&#20204;&#30340;&#25513;&#30721;&#31574;&#30053;&#21033;&#29992;&#27169;&#26495;&#21270;&#27861;&#24459;&#25991;&#20214;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#29305;&#24449;&#26469;&#25513;&#30422;&#25991;&#26412;&#30340;&#36830;&#32493;&#33539;&#22260;&#12290;&#21435;&#22122;&#36825;&#20123;&#33539;&#22260;&#26377;&#21161;&#20110;DALE&#33719;&#21462;&#20851;&#20110;&#27861;&#24459;&#27010;&#24565;&#12289;&#21407;&#21017;&#21644;&#35821;&#35328;&#20351;&#29992;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#23427;&#20855;&#22791;&#20102;&#29983;&#25104;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#20197;&#21450;&#26032;&#39062;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;DALE&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DALE, a novel and effective generative Data Augmentation framework for low-resource LEgal NLP. DALE addresses the challenges existing frameworks pose in generating effective data augmentations of legal documents - legal language, with its specialized vocabulary and complex semantics, morphology, and syntax, does not benefit from data augmentations that merely rephrase the source sentence. To address this, DALE, built on an Encoder-Decoder Language Model, is pre-trained on a novel unsupervised text denoising objective based on selective masking - our masking strategy exploits the domain-specific language characteristics of templatized legal documents to mask collocated spans of text. Denoising these spans helps DALE acquire knowledge about legal concepts, principles, and language usage. Consequently, it develops the ability to generate coherent and diverse augmentations with novel contexts. Finally, DALE performs conditional generation to generate synthetic augmentations for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#32452;&#21512;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#24403;&#21069;&#31574;&#30053;&#31867;&#20284;&#30340;&#25928;&#26524;&#65292;&#36825;&#26159;&#22240;&#20026;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#19979;&#65292;&#23454;&#20307;&#30721;&#26377;&#26356;&#39640;&#30340;&#29109;&#21644;&#30721;&#23383;&#32423;&#21035;&#30340;Jaccard&#36317;&#31163;&#65292;&#20351;&#24471;&#19981;&#21516;&#23454;&#20307;&#26356;&#23481;&#26131;&#21306;&#20998;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#12290;</title><link>http://arxiv.org/abs/2310.15797</link><description>&lt;p&gt;
&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#32452;&#21512;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation. (arXiv:2310.15797v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#32452;&#21512;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#24403;&#21069;&#31574;&#30053;&#31867;&#20284;&#30340;&#25928;&#26524;&#65292;&#36825;&#26159;&#22240;&#20026;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#19979;&#65292;&#23454;&#20307;&#30721;&#26377;&#26356;&#39640;&#30340;&#29109;&#21644;&#30721;&#23383;&#32423;&#21035;&#30340;Jaccard&#36317;&#31163;&#65292;&#20351;&#24471;&#19981;&#21516;&#23454;&#20307;&#26356;&#23481;&#26131;&#21306;&#20998;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20027;&#23548;&#26041;&#27861;KG&#23884;&#20837;&#65288;KGE&#65289;&#36890;&#36807;&#29420;&#31435;&#21521;&#37327;&#34920;&#31034;&#23454;&#20307;&#65292;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#25928;&#29575;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#39044;&#23450;&#20041;&#30340;&#23567;&#35268;&#27169;&#30721;&#20070;&#20013;&#21305;&#37197;&#23454;&#20307;&#23545;&#24212;&#30340;&#30721;&#23383;&#26469;&#34920;&#31034;&#23454;&#20307;&#12290;&#25105;&#20204;&#23558;&#33719;&#21462;&#27599;&#20010;&#23454;&#20307;&#23545;&#24212;&#30721;&#23383;&#30340;&#36807;&#31243;&#31216;&#20026;&#23454;&#20307;&#37327;&#21270;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#22797;&#26434;&#30340;&#31574;&#30053;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26412;&#25991;&#34920;&#26126;&#31616;&#21333;&#30340;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#21487;&#20197;&#23454;&#29616;&#19982;&#24403;&#21069;&#31574;&#30053;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#29616;&#35937;&#24182;&#25581;&#31034;&#20102;&#22312;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#19979;&#65292;&#34920;&#31034;&#23454;&#20307;&#30340;&#37327;&#21270;&#32467;&#26524;-&#23454;&#20307;&#30721;&#20855;&#26377;&#26356;&#39640;&#30340;&#29109;&#21644;&#30721;&#23383;&#32423;&#21035;&#30340;Jaccard&#36317;&#31163;&#12290;&#22240;&#27492;&#65292;&#19981;&#21516;&#23454;&#20307;&#26356;&#23481;&#26131;&#21306;&#20998;&#65292;&#26377;&#21161;&#20110;&#26377;&#25928;&#30340;KG&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation Learning on Knowledge Graphs (KGs) is essential for downstream tasks. The dominant approach, KG Embedding (KGE), represents entities with independent vectors and faces the scalability challenge. Recent studies propose an alternative way for parameter efficiency, which represents entities by composing entity-corresponding codewords matched from predefined small-scale codebooks. We refer to the process of obtaining corresponding codewords of each entity as entity quantization, for which previous works have designed complicated strategies. Surprisingly, this paper shows that simple random entity quantization can achieve similar results to current strategies. We analyze this phenomenon and reveal that entity codes, the quantization outcomes for expressing entities, have higher entropy at the code level and Jaccard distance at the codeword level under random entity quantization. Therefore, different entities become more easily distinguished, facilitating effective KG represen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#21069;&#32512;&#23376;&#31354;&#38388;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25972;&#20010;&#21333;&#32431;&#24418;&#27169;&#22411;&#65292;&#22312;&#31232;&#32570;&#25968;&#25454;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#24191;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#20860;&#23481;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15793</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#21069;&#32512;&#23376;&#31354;&#38388;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving generalization in large language models by learning prefix subspaces. (arXiv:2310.15793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#21069;&#32512;&#23376;&#31354;&#38388;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25972;&#20010;&#21333;&#32431;&#24418;&#27169;&#22411;&#65292;&#22312;&#31232;&#32570;&#25968;&#25454;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#24191;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#20860;&#23481;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31232;&#32570;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#24494;&#35843;&#65288;&#20063;&#34987;&#31216;&#20026;&#8220;&#23569;&#26679;&#26412;&#8221;&#23398;&#20064;&#35774;&#32622;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#23376;&#31354;&#38388;&#30340;&#26041;&#27861;&#26469;&#22686;&#21152;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#20248;&#21270;&#26041;&#27861;&#26368;&#36817;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#24341;&#20837;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25972;&#20010;&#21333;&#32431;&#24418;&#27169;&#22411;&#30340;&#32852;&#21512;&#20248;&#21270;&#65292;&#35782;&#21035;&#26356;&#24191;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#20854;&#36866;&#24212;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#21017;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#22823;&#37327;&#30340;&#21442;&#25968;&#20351;&#24471;&#32852;&#21512;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#65292;&#20854;&#27425;&#65292;&#23427;&#20204;&#30340;&#30830;&#23450;&#24615;&#21442;&#25968;&#21021;&#22987;&#21270;&#26041;&#26696;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#26368;&#21021;&#30340;&#23376;&#31354;&#38388;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23637;&#31034;&#65292;&#8220;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#8221;&#65288;PEFT&#65289;&#26041;&#27861;&#19982;&#26368;&#21021;&#30340;&#26041;&#27861;&#23436;&#20840;&#20860;&#23481;&#65292;&#24182;&#25552;&#20986;&#23398;&#20064;&#25972;&#20010;&#36830;&#32493;&#21069;&#32512;&#30340;&#21333;&#32431;&#24418;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article focuses on large language models (LLMs) fine-tuning in the scarce data regime (also known as the "few-shot" learning setting). We propose a method to increase the generalization capabilities of LLMs based on neural network subspaces. This optimization method, recently introduced in computer vision, aims to improve model generalization by identifying wider local optima through the joint optimization of an entire simplex of models in parameter space. Its adaptation to massive, pretrained transformers, however, poses some challenges. First, their considerable number of parameters makes it difficult to train several models jointly, and second, their deterministic parameter initialization schemes make them unfit for the subspace method as originally proposed. We show in this paper that "Parameter Efficient Fine-Tuning" (PEFT) methods, however, are perfectly compatible with this original approach, and propose to learn entire simplex of continuous prefixes. We test our method on 
&lt;/p&gt;</description></item><item><title>SequenceMatch&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20013;&#31561;&#22686;&#24378;&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#19968;&#33268;&#24615;&#32422;&#26463;&#65292;&#24110;&#21161;&#20943;&#23567;&#20102;&#27169;&#22411;&#23545;&#24369;&#22686;&#24378;&#21644;&#24378;&#22686;&#24378;&#31034;&#20363;&#30340;&#39044;&#27979;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.15787</link><description>&lt;p&gt;
SequenceMatch: &#37325;&#26032;&#32771;&#34385;&#24378;&#24369;&#22686;&#24378;&#35774;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SequenceMatch: Revisiting the design of weak-strong augmentations for Semi-supervised learning. (arXiv:2310.15787v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15787
&lt;/p&gt;
&lt;p&gt;
SequenceMatch&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20013;&#31561;&#22686;&#24378;&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#19968;&#33268;&#24615;&#32422;&#26463;&#65292;&#24110;&#21161;&#20943;&#23567;&#20102;&#27169;&#22411;&#23545;&#24369;&#22686;&#24378;&#21644;&#24378;&#22686;&#24378;&#31034;&#20363;&#30340;&#39044;&#27979;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22240;&#20854;&#21487;&#20197;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;SSL&#26041;&#27861;&#38754;&#20020;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#30830;&#35748;&#20559;&#24046;&#65292;&#21363;&#24403;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#23567;&#22411;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#38598;&#24182;&#20135;&#29983;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SequenceMatch&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;SSL&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22810;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;SequenceMatch&#30340;&#20851;&#38190;&#26159;&#20026;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#24341;&#20837;&#20102;&#20013;&#31561;&#22686;&#24378;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#30340;&#22686;&#24378;&#26041;&#27861;&#21644;&#27599;&#23545;&#22686;&#24378;&#31034;&#20363;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#32422;&#26463;&#65292;SequenceMatch&#24110;&#21161;&#32553;&#23567;&#20102;&#27169;&#22411;&#23545;&#24369;&#22686;&#24378;&#21644;&#24378;&#22686;&#24378;&#31034;&#20363;&#30340;&#39044;&#27979;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;SequenceMatch&#20026;&#39640;&#32622;&#20449;&#24230;&#21644;&#20302;&#32622;&#20449;&#24230;&#30340;&#39044;&#27979;&#23450;&#20041;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#19968;&#33268;&#24615;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;SequenceMatch&#27604;ReMixMatch&#26356;&#20855;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) has become popular in recent years because it allows the training of a model using a large amount of unlabeled data. However, one issue that many SSL methods face is the confirmation bias, which occurs when the model is overfitted to the small labeled training dataset and produces overconfident, incorrect predictions. To address this issue, we propose SequenceMatch, an efficient SSL method that utilizes multiple data augmentations. The key element of SequenceMatch is the inclusion of a medium augmentation for unlabeled data. By taking advantage of different augmentations and the consistency constraints between each pair of augmented examples, SequenceMatch helps reduce the divergence between the prediction distribution of the model for weakly and strongly augmented examples. In addition, SequenceMatch defines two different consistency constraints for high and low-confidence predictions. As a result, SequenceMatch is more data-efficient than ReMixMatch, an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CP-MAE&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38754;&#37096;&#36974;&#32617;&#26469;&#23454;&#29616;MRI&#25195;&#25551;&#20013;&#30340;&#20154;&#33080;&#21435;&#35782;&#21035;&#65292;&#25552;&#39640;&#20102;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.15778</link><description>&lt;p&gt;
&#22686;&#24378;MRI&#25195;&#25551;&#38544;&#31169;&#30340;3D&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
3D Masked Autoencoders for Enhanced Privacy in MRI Scans. (arXiv:2310.15778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CP-MAE&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38754;&#37096;&#36974;&#32617;&#26469;&#23454;&#29616;MRI&#25195;&#25551;&#20013;&#30340;&#20154;&#33080;&#21435;&#35782;&#21035;&#65292;&#25552;&#39640;&#20102;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MRI&#25195;&#25551;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21307;&#23398;&#20449;&#24687;&#65292;&#20294;&#20063;&#21253;&#21547;&#25935;&#24863;&#21644;&#21487;&#35782;&#21035;&#20010;&#20154;&#20449;&#24687;&#65288;PII&#65289;&#65292;&#38656;&#35201;&#20445;&#25252;&#12290;&#20256;&#32479;&#30340;MRI&#25968;&#25454;&#21435;&#35782;&#21035;&#26041;&#27861;&#36890;&#36807;&#21024;&#38500;&#38544;&#31169;&#25935;&#24863;&#37096;&#20301;&#65288;&#22914;&#30524;&#30555;&#12289;&#40763;&#23376;&#31561;&#65289;&#26469;&#23454;&#29616;&#65292;&#20294;&#20250;&#24341;&#20837;&#39046;&#22495;&#36716;&#25442;&#65292;&#24433;&#21709;&#19979;&#28216;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CP-MAE&#27169;&#22411;&#65292;&#36890;&#36807;&#38754;&#37096;&#36974;&#32617;&#26469;&#23454;&#29616;&#20154;&#33080;&#21435;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
MRI scans provide valuable medical information, however they also contain sensitive and personally identifiable information (PII) that needs to be protected. Whereas MRI metadata is easily sanitized, MRI image data is a privacy risk because it contains information to render highly-realistic 3D visualizations of a patient's head, enabling malicious actors to possibly identify the subject by cross-referencing a database. Data anonymization and de-identification is concerned with ensuring the privacy and confidentiality of individuals' personal information. Traditional MRI de-identification methods remove privacy-sensitive parts (e.g. eyes, nose etc.) from a given scan. This comes at the expense of introducing a domain shift that can throw off downstream analyses. Recently, a GAN-based approach was proposed to de-identify a patient's scan by remodeling it (e.g. changing the face) rather than by removing parts. In this work, we propose CP-MAE, a model that de-identifies the face using mask
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;MindLLM&#65292;&#36890;&#36807;&#25552;&#20379;1.3&#20159;&#21644;3&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20943;&#36731;&#20102;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#26412;&#21644;&#36164;&#28304;&#31232;&#32570;&#24615;&#30340;&#21387;&#21147;&#12290;MindLLM&#22312;&#21508;&#20010;&#27493;&#39588;&#20013;&#32473;&#20986;&#20102;&#32463;&#39564;&#25945;&#35757;&#65292;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#35780;&#20272;&#21644;&#24212;&#29992;&#65292;&#23545;&#23398;&#26415;&#30028;&#21644;&#24320;&#21457;&#32773;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.15777</link><description>&lt;p&gt;
MindLLM: &#20174;&#38646;&#24320;&#22987;&#39044;&#35757;&#32451;&#36731;&#37327;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35780;&#20272;&#21644;&#39046;&#22495;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications. (arXiv:2310.15777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;MindLLM&#65292;&#36890;&#36807;&#25552;&#20379;1.3&#20159;&#21644;3&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20943;&#36731;&#20102;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#26412;&#21644;&#36164;&#28304;&#31232;&#32570;&#24615;&#30340;&#21387;&#21147;&#12290;MindLLM&#22312;&#21508;&#20010;&#27493;&#39588;&#20013;&#32473;&#20986;&#20102;&#32463;&#39564;&#25945;&#35757;&#65292;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#35780;&#20272;&#21644;&#24212;&#29992;&#65292;&#23545;&#23398;&#26415;&#30028;&#21644;&#24320;&#21457;&#32773;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#26631;&#24535;&#30528;&#36890;&#24448;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#34429;&#28982;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#26159;&#36890;&#36807;&#24320;&#21457;&#36234;&#26469;&#36234;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#30340;&#65292;&#20294;&#36824;&#26377;&#21478;&#19968;&#31181;&#20998;&#25903;&#65292;&#21363;&#24320;&#21457;&#36731;&#37327;&#32423;&#23450;&#21046;&#27169;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#26381;&#21153;&#26576;&#20123;&#39046;&#22495;&#65292;&#32771;&#34385;&#21040;&#35757;&#32451;&#21644;&#37096;&#32626;LLM&#30340;&#39640;&#25104;&#26412;&#21644;&#36164;&#28304;&#30340;&#31232;&#32570;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;MindLLM&#65292;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#21452;&#35821;&#36731;&#37327;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#65292;&#36890;&#36807;&#25552;&#20379;13&#20159;&#21644;30&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#26469;&#20943;&#36731;&#36825;&#20123;&#36127;&#25285;&#12290;&#32473;&#20986;&#20102;&#22312;&#22823;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#31215;&#32047;&#30340;&#32463;&#39564;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#35780;&#20272;&#21644;&#24212;&#29992;&#12290;&#36825;&#20123;&#35265;&#35299;&#23545;&#23398;&#32773;&#21644;&#24320;&#21457;&#32773;&#26469;&#35828;&#26377;&#20215;&#20540;&#12290;MindLLM&#22987;&#32456;&#33021;&#22815;&#19982;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is leveraged by developing increasingly large-scale models, there could be another branch to develop lightweight custom models that better serve certain domains, taking into account the high cost of training and deploying LLMs and the scarcity of resources. In this paper, we present MindLLM, a novel series of bilingual lightweight large language models, trained from scratch, alleviating such burdens by offering models with 1.3 billion and 3 billion parameters. A thorough account of experiences accrued during large model development is given, covering every step of the process, including data construction, model architecture, evaluation, and applications. Such insights are hopefully valuable for fellow academics and developers. MindLLM consistently matches or surpasses the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#20026;&#20309;&#20998;&#20139;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#25311;&#29992;&#25143;&#33030;&#24369;&#24615;&#26469;&#25581;&#31034;&#24433;&#21709;&#29992;&#25143;&#20998;&#20139;&#34892;&#20026;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.15772</link><description>&lt;p&gt;
&#29992;&#25143;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20998;&#20139;&#20167;&#24680;&#35328;&#35770;&#30340;&#22240;&#26524;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Causal Understanding of Why Users Share Hate Speech on Social Media. (arXiv:2310.15772v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#20026;&#20309;&#20998;&#20139;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#25968;&#25454;&#20559;&#24046;&#21644;&#27169;&#25311;&#29992;&#25143;&#33030;&#24369;&#24615;&#26469;&#25581;&#31034;&#24433;&#21709;&#29992;&#25143;&#20998;&#20139;&#34892;&#20026;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#23041;&#32961;&#21040;&#20010;&#20154;&#30340;&#24515;&#29702;&#21644;&#36523;&#20307;&#20581;&#24247;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#23548;&#33268;&#29616;&#23454;&#20013;&#30340;&#26292;&#21147;&#20107;&#20214;&#12290;&#20167;&#24680;&#35328;&#35770;&#20256;&#25773;&#32972;&#21518;&#30340;&#37325;&#35201;&#39537;&#21160;&#22240;&#32032;&#26159;&#36716;&#21457;&#65292;&#20294;&#26159;&#20154;&#20204;&#24456;&#23569;&#20102;&#35299;&#20026;&#20160;&#20040;&#29992;&#25143;&#20250;&#36716;&#21457;&#20167;&#24680;&#35328;&#35770;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#12289;&#22240;&#26524;&#20998;&#26512;&#30340;&#29992;&#25143;&#23646;&#24615;&#26694;&#26550;&#65292;&#30740;&#31350;&#29992;&#25143;&#20026;&#20309;&#20998;&#20139;&#20167;&#24680;&#35328;&#35770;&#12290;&#28982;&#32780;&#65292;&#22312;&#20174;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#26102;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#31867;&#25968;&#25454;&#24456;&#21487;&#33021;&#23384;&#22312;&#36873;&#25321;&#20559;&#24046;&#65292;&#24182;&#19988;&#29992;&#25143;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#33030;&#24369;&#24615;&#23384;&#22312;&#28151;&#28102;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19977;&#27493;&#22240;&#26524;&#26694;&#26550;&#65306;&#65288;1&#65289;&#25105;&#20204;&#36890;&#36807;&#36870;&#21521;&#20542;&#21521;&#35780;&#20998;&#26469;&#28040;&#38500;&#35266;&#23519;&#24615;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#20559;&#24046;&#12290;&#65288;2&#65289;&#25105;&#20204;&#20351;&#29992;&#28040;&#38500;&#20559;&#24046;&#30340;&#20542;&#21521;&#35780;&#20998;&#26469;&#27169;&#25311;&#29992;&#25143;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#28508;&#22312;&#33030;&#24369;&#24615;&#20316;&#20026;&#28508;&#22312;&#23884;&#20837;&#12290;&#65288;3&#65289;&#25105;&#20204;&#24314;&#31435;&#20102;&#29992;&#25143;&#23646;&#24615;&#23545;&#29992;&#25143;&#20998;&#20139;&#20167;&#24680;&#35328;&#35770;&#27010;&#29575;&#30340;&#22240;&#26524;&#25928;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech on social media threatens the mental and physical well-being of individuals and is further responsible for real-world violence. An important driver behind the spread of hate speech and thus why hateful posts can go viral are reshares, yet little is known about why users reshare hate speech. In this paper, we present a comprehensive, causal analysis of the user attributes that make users reshare hate speech. However, causal inference from observational social media data is challenging, because such data likely suffer from selection bias, and there is further confounding due to differences in the vulnerability of users to hate speech. We develop a novel, three-step causal framework: (1) We debias the observational social media data by applying inverse propensity scoring. (2) We use the debiased propensity scores to model the latent vulnerability of users to hate speech as a latent embedding. (3) We model the causal effects of user attributes on users' probability of sharing h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25237;&#24433;&#22120;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#25913;&#36827;&#20102;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15764</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#30340;&#38598;&#25104;&#25237;&#24433;&#22120;&#28040;&#38500;&#20559;&#35265;&#12289;&#26657;&#20934;&#21644;&#25913;&#36827;&#21322;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Debiasing, calibrating, and improving Semi-supervised Learning performance via simple Ensemble Projector. (arXiv:2310.15764v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25237;&#24433;&#22120;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#25913;&#36827;&#20102;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20542;&#21521;&#20110;&#36890;&#36807;&#24341;&#20837;&#26356;&#22810;&#30340;&#32593;&#32476;&#32452;&#20214;&#21644;&#39069;&#22806;&#30340;&#35757;&#32451;&#36807;&#31243;&#26469;&#33719;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#35774;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#38598;&#25104;&#25237;&#24433;&#22120;&#36741;&#21161;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;EPASS&#65289;&#65292;&#20027;&#35201;&#20851;&#27880;&#20110;&#25913;&#36827;&#23398;&#20064;&#30340;&#23884;&#20837;&#20197;&#25552;&#39640;&#29616;&#26377;&#23545;&#27604;&#32852;&#21512;&#35757;&#32451;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;&#19982;&#26631;&#20934;&#26041;&#27861;&#19981;&#21516;&#65292;EPASS&#23558;&#22810;&#20010;&#25237;&#24433;&#22120;&#30340;&#38598;&#21512;&#23884;&#20837;&#23384;&#20648;&#22312;&#23384;&#20648;&#22120;&#20013;&#65292;&#20197;&#20379;&#23545;&#27604;&#23398;&#20064;&#26102;&#20351;&#29992;&#12290;&#32467;&#26524;&#65292;EPASS&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#21152;&#24378;&#20102;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;EPASS&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24378;&#22522;&#32447;&#30340;top-1&#38169;&#35823;&#29575;&#25552;&#39640;&#20102;39.47\%/31.39\%/24.70\%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on semi-supervised learning (SSL) have achieved great success. Despite their promising performance, current state-of-the-art methods tend toward increasingly complex designs at the cost of introducing more network components and additional training procedures. In this paper, we propose a simple method named Ensemble Projectors Aided for Semi-supervised Learning (EPASS), which focuses mainly on improving the learned embeddings to boost the performance of the existing contrastive joint-training semi-supervised learning frameworks. Unlike standard methods, where the learned embeddings from one projector are stored in memory banks to be used with contrastive learning, EPASS stores the ensemble embeddings from multiple projectors in memory banks. As a result, EPASS improves generalization, strengthens feature representation, and boosts performance. For instance, EPASS improves strong baselines for semi-supervised learning by 39.47\%/31.39\%/24.70\% top-1 error rate, while usi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#32447;&#24615;&#21464;&#25442;&#22120;&#20316;&#20026;transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;transformers&#22312;&#22788;&#29702;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#25512;&#26029;&#25104;&#26412;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15719</link><description>&lt;p&gt;
&#24490;&#29615;&#32447;&#24615;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Recurrent Linear Transformers. (arXiv:2310.15719v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#32447;&#24615;&#21464;&#25442;&#22120;&#20316;&#20026;transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;transformers&#22312;&#22788;&#29702;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#25512;&#26029;&#25104;&#26412;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
transformer&#26550;&#26500;&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#33021;&#22815;&#25429;&#25417;&#38271;&#36317;&#31163;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#20063;&#26159;&#20854;&#22312;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#26102;&#26377;&#25928;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#25104;&#21151;&#65292;transformers&#20173;&#28982;&#26377;&#20004;&#20010;&#37325;&#22823;&#32570;&#28857;&#65292;&#38480;&#21046;&#20102;&#20854;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65306;(1)&#20026;&#20102;&#35760;&#20303;&#36807;&#21435;&#30340;&#20449;&#24687;&#65292;&#33258;&#27880;&#24847;&#26426;&#21046;&#38656;&#35201;&#35775;&#38382;&#25972;&#20010;&#21382;&#21490;&#20449;&#24687;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;(2)transformers&#30340;&#25512;&#26029;&#25104;&#26412;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#24490;&#29615;&#26367;&#20195;&#26041;&#26696;&#65292;&#20854;&#20855;&#26377;&#29420;&#31435;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#26029;&#25104;&#26412;&#24182;&#26377;&#25928;&#22320;&#21033;&#29992;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#19978;&#36848;&#35745;&#31639;&#38480;&#21046;&#20960;&#20046;&#20351;&#24471;transformers&#30340;&#24212;&#29992;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#35786;&#26029;&#29615;&#22659;&#20013;&#37327;&#21270;&#20102;&#25105;&#20204;&#26550;&#26500;&#20013;&#19981;&#21516;&#37096;&#20998;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-attention mechanism in the transformer architecture is capable of capturing long-range dependencies and it is the main reason behind its effectiveness in processing sequential data. Nevertheless, despite their success, transformers have two significant drawbacks that still limit their broader applicability: (1) In order to remember past information, the self-attention mechanism requires access to the whole history to be provided as context. (2) The inference cost in transformers is expensive. In this paper we introduce recurrent alternatives to the transformer self-attention mechanism that offer a context-independent inference cost, leverage long-range dependencies effectively, and perform well in practice. We evaluate our approaches in reinforcement learning problems where the aforementioned computational limitations make the application of transformers nearly infeasible. We quantify the impact of the different components of our architecture in a diagnostic environment and as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#35843;&#24230;&#31574;&#30053;&#26469;&#35299;&#20915;&#22823;&#22411;&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#35843;&#24230;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.15706</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#22810;&#26679;&#21270;&#35843;&#24230;&#31574;&#30053;&#26469;&#35299;&#20915;&#22823;&#22411;&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
Solving large flexible job shop scheduling instances by generating a diverse set of scheduling policies with deep reinforcement learning. (arXiv:2310.15706v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#35843;&#24230;&#31574;&#30053;&#26469;&#35299;&#20915;&#22823;&#22411;&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#35843;&#24230;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;FJSSP&#65289;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#21551;&#21457;&#24335;&#12289;&#31934;&#30830;&#21644;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24037;&#19994;&#23545;&#23454;&#26102;&#21709;&#24212;&#31361;&#21457;&#20107;&#20214;&#30340;&#38656;&#27714;&#20135;&#29983;&#20102;&#22312;&#20960;&#31186;&#20869;&#29983;&#25104;&#26032;&#35843;&#24230;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#21482;&#26377;&#35843;&#24230;&#35268;&#21017;&#65288;DRs&#65289;&#33021;&#22815;&#22312;&#32422;&#26463;&#19979;&#29983;&#25104;&#35843;&#24230;&#65292;&#23613;&#31649;&#20854;&#36136;&#37327;&#21487;&#20197;&#24471;&#21040;&#25913;&#36827;&#12290;&#20026;&#20102;&#25913;&#21892;&#32467;&#26524;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;FJSSP&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#19968;&#20010;&#31574;&#30053;&#65292;&#23558;&#25805;&#20316;&#20998;&#37197;&#21040;&#26426;&#22120;&#19978;&#29983;&#25104;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22411;&#30340;FJSSP&#23454;&#20363;&#20013;&#20173;&#28982;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#24456;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#33021;&#22815;&#31283;&#20581;&#35299;&#20915;&#22823;&#22411;FJSSP&#23454;&#20363;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Flexible Job Shop Scheduling Problem (FJSSP) has been extensively studied in the literature, and multiple approaches have been proposed within the heuristic, exact, and metaheuristic methods. However, the industry's demand to be able to respond in real-time to disruptive events has generated the necessity to be able to generate new schedules within a few seconds. Among these methods, under this constraint, only dispatching rules (DRs) are capable of generating schedules, even though their quality can be improved. To improve the results, recent methods have been proposed for modeling the FJSSP as a Markov Decision Process (MDP) and employing reinforcement learning to create a policy that generates an optimal solution assigning operations to machines. Nonetheless, there is still room for improvement, particularly in the larger FJSSP instances which are common in real-world scenarios. Therefore, the objective of this paper is to propose a method capable of robustly solving large insta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#26080;&#32447;&#32593;&#32476;&#35843;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#26631;&#20934;&#30340;&#36172;&#21338;&#26426;&#31574;&#30053;&#30340;&#24615;&#33021;&#65292;&#20197;&#25552;&#39640;&#20449;&#24687;&#20934;&#30830;&#24615;&#21644;&#26032;&#40092;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.15705</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#26080;&#32447;&#32593;&#32476;&#20449;&#24687;&#20934;&#30830;&#24615;&#21644;&#26032;&#40092;&#24230;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Learning-based Scheduling for Information Accuracy and Freshness in Wireless Networks. (arXiv:2310.15705v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#26080;&#32447;&#32593;&#32476;&#35843;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#26631;&#20934;&#30340;&#36172;&#21338;&#26426;&#31574;&#30053;&#30340;&#24615;&#33021;&#65292;&#20197;&#25552;&#39640;&#20449;&#24687;&#20934;&#30830;&#24615;&#21644;&#26032;&#40092;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#21333;&#36890;&#20449;&#20449;&#36947;&#21644;&#21333;&#20010;&#30417;&#27979;&#31449;&#30340;&#31995;&#32479;&#12290;&#27599;&#20010;&#28304;&#37117;&#20197;&#19981;&#21516;&#20934;&#30830;&#24230;&#27979;&#37327;&#19968;&#20010;&#26102;&#38388;&#21464;&#21270;&#30340;&#25968;&#37327;&#65292;&#20854;&#20013;&#19968;&#20010;&#28304;&#36890;&#36807;&#20449;&#36947;&#23558;&#20854;&#26356;&#26032;&#21457;&#36865;&#21040;&#30417;&#27979;&#31449;&#12290;&#27599;&#27425;&#23581;&#35797;&#36890;&#20449;&#30340;&#25104;&#21151;&#27010;&#29575;&#21462;&#20915;&#20110;&#34987;&#35843;&#24230;&#29992;&#20110;&#20256;&#36755;&#26356;&#26032;&#30340;&#28304;&#12290;&#35843;&#24230;&#22120;&#19981;&#30693;&#36947;&#25152;&#26377;&#28304;&#30340;&#27979;&#37327;&#27491;&#30830;&#24615;&#21644;&#20256;&#36755;&#25104;&#21151;&#27010;&#29575;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#24230;&#37327;&#26159;&#31995;&#32479;&#25509;&#25910;&#21040;&#30340;&#22870;&#21169;&#65292;&#35813;&#22870;&#21169;&#21462;&#20915;&#20110;&#30446;&#30340;&#22320;&#25910;&#21040;&#30340;&#26368;&#21518;&#19968;&#27425;&#26356;&#26032;&#30340;&#20934;&#30830;&#24615;&#21644;&#20449;&#24687;&#24180;&#40836;&#65288;Age-of-Information&#65292;AoI&#65289;&#12290;&#25105;&#20204;&#23558;&#35843;&#24230;&#38382;&#39064;&#24314;&#27169;&#20026;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20854;&#20013;&#21508;&#20010;&#28304;&#20316;&#20026;&#19981;&#21516;&#30340;&#33218;&#12290;&#36890;&#36807;&#27169;&#25311;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#25152;&#26377;&#22235;&#31181;&#26631;&#20934;&#30340;&#36172;&#21338;&#26426;&#31574;&#30053;&#65292;&#21363;ETC&#12289;epsilon-greedy&#12289;UCB&#21644;TS&#22312;&#25105;&#20204;&#30340;&#31995;&#32479;&#27169;&#22411;&#19979;&#36866;&#24403;&#35843;&#25972;&#21518;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a system of multiple sources, a single communication channel, and a single monitoring station. Each source measures a time-varying quantity with varying levels of accuracy and one of them sends its update to the monitoring station via the channel. The probability of success of each attempted communication is a function of the source scheduled for transmitting its update. Both the probability of correct measurement and the probability of successful transmission of all the sources are unknown to the scheduler. The metric of interest is the reward received by the system which depends on the accuracy of the last update received by the destination and the Age-of-Information (AoI) of the system. We model our scheduling problem as a variant of the multi-arm bandit problem with sources as different arms. We compare the performance of all $4$ standard bandit policies, namely, ETC, $\epsilon$-greedy, UCB, and TS suitably adjusted to our system model via simulations. In addition, we p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;3A2M+&#30340;&#33258;&#21160;&#33756;&#35889;&#31867;&#22411;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#25193;&#23637;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21015;&#34920;&#23545;&#28921;&#39274;&#39135;&#35889;&#36827;&#34892;&#20998;&#31867;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#20004;&#30334;&#19975;&#20010;&#24102;&#26377;&#21508;&#31181;&#29305;&#24449;&#21644;&#20061;&#20010;&#19981;&#21516;&#31867;&#22411;&#26631;&#31614;&#30340;&#28921;&#39274;&#39135;&#35889;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15693</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#23454;&#29616;&#33258;&#21160;&#33756;&#35889;&#31867;&#22411;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Automated Recipe Genre Classification using Semi-Supervised Learning. (arXiv:2310.15693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15693
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;3A2M+&#30340;&#33258;&#21160;&#33756;&#35889;&#31867;&#22411;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#25193;&#23637;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21015;&#34920;&#23545;&#28921;&#39274;&#39135;&#35889;&#36827;&#34892;&#20998;&#31867;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#20004;&#30334;&#19975;&#20010;&#24102;&#26377;&#21508;&#31181;&#29305;&#24449;&#21644;&#20061;&#20010;&#19981;&#21516;&#31867;&#22411;&#26631;&#31614;&#30340;&#28921;&#39274;&#39135;&#35889;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#20139;&#28921;&#39274;&#39135;&#35889;&#26159;&#20132;&#27969;&#28921;&#39274;&#21019;&#24847;&#21644;&#25552;&#20379;&#39135;&#29289;&#21046;&#20316;&#25351;&#24341;&#30340;&#22909;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23558;&#22312;&#32447;&#25214;&#21040;&#30340;&#21407;&#22987;&#39135;&#35889;&#20998;&#31867;&#20026;&#36866;&#24403;&#30340;&#39135;&#29289;&#31867;&#22411;&#21487;&#33021;&#20250;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#36275;&#22815;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#28151;&#21512;&#12289;&#20856;&#22411;&#21644;&#27880;&#37322;&#30340;&#25193;&#23637;&#20004;&#30334;&#19975;&#65288;3A2M+&#65289;&#28921;&#39274;&#39135;&#35889;&#25968;&#25454;&#38598;&#8221;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#30334;&#19975;&#20010;&#34987;&#26631;&#35760;&#20026;&#30456;&#24212;&#31867;&#21035;&#30340;&#28921;&#39274;&#39135;&#35889;&#65292;&#24182;&#25552;&#21462;&#20102;&#39135;&#35889;&#25551;&#36848;&#20013;&#30340;&#25193;&#23637;&#21629;&#21517;&#23454;&#20307;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#26631;&#39064;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#27493;&#39588;&#21644;&#25193;&#23637;&#21629;&#21517;&#23454;&#20307;&#31561;&#21508;&#31181;&#29305;&#24449;&#65292;&#20197;&#21450;&#20195;&#34920;&#31957;&#28857;&#12289;&#39278;&#26009;&#12289;&#38750;&#32032;&#39135;&#12289;&#34092;&#33756;&#12289;&#24555;&#39184;&#12289;&#35895;&#29289;&#12289;&#20027;&#39135;&#12289;&#37197;&#33756;&#21644;&#34701;&#21512;&#30340;&#20061;&#20010;&#19981;&#21516;&#31867;&#22411;&#30340;&#26631;&#31614;&#12290;&#25152;&#25552;&#20986;&#30340;&#21517;&#20026;3A2M+&#30340;&#27969;&#31243;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24037;&#20855;&#65292;&#25193;&#23637;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21015;&#34920;&#30340;&#22823;&#23567;&#65292;&#20197;&#35299;&#20915;&#20174;&#39135;&#35889;&#27493;&#39588;&#20013;&#32570;&#23569;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#22914;&#21152;&#28909;&#12289;&#26102;&#38388;&#25110;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharing cooking recipes is a great way to exchange culinary ideas and provide instructions for food preparation. However, categorizing raw recipes found online into appropriate food genres can be challenging due to a lack of adequate labeled data. In this study, we present a dataset named the ``Assorted, Archetypal, and Annotated Two Million Extended (3A2M+) Cooking Recipe Dataset" that contains two million culinary recipes labeled in respective categories with extended named entities extracted from recipe descriptions. This collection of data includes various features such as title, NER, directions, and extended NER, as well as nine different labels representing genres including bakery, drinks, non-veg, vegetables, fast food, cereals, meals, sides, and fusions. The proposed pipeline named 3A2M+ extends the size of the Named Entity Recognition (NER) list to address missing named entities like heat, time or process from the recipe directions using two NER extraction tools. 3A2M+ dataset
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#32858;&#21512;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#24341;&#25991;&#35770;&#25991;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#24635;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15684</link><description>&lt;p&gt;
&#21033;&#29992;&#24341;&#25991;&#25991;&#29486;&#20013;&#30340;&#30693;&#35782;&#32858;&#21512;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers. (arXiv:2310.15684v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15684
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#32858;&#21512;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#24341;&#25991;&#35770;&#25991;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#24635;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#30340;&#25688;&#35201;&#20855;&#26377;&#29305;&#23450;&#30340;&#39046;&#22495;&#29305;&#24449;&#65292;&#21253;&#25324;&#19987;&#38376;&#30340;&#20889;&#20316;&#39118;&#26684;&#21644;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#65292;&#36825;&#35201;&#27714;&#23545;&#30456;&#20851;&#25991;&#29486;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#19982;&#29983;&#29289;&#21307;&#23398;&#19987;&#23478;&#30456;&#23218;&#32654;&#30340;&#25216;&#26415;&#25688;&#35201;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#32858;&#21512;&#28304;&#35770;&#25991;&#20013;&#24341;&#29992;&#30340;&#22806;&#37096;&#35770;&#25991;&#20013;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#24635;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24341;&#25991;&#32858;&#21512;&#27169;&#22411;&#65292;&#23427;&#23558;&#24341;&#25991;&#35770;&#25991;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#19982;&#35770;&#25991;&#20869;&#23481;&#30456;&#32467;&#21512;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#21033;&#29992;&#35770;&#25991;&#20869;&#23481;&#21644;&#24341;&#25991;&#35770;&#25991;&#20013;&#30340;&#30456;&#20851;&#30693;&#35782;&#29983;&#25104;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#24635;&#32467;&#25968;&#25454;&#38598;&#20316;&#20026;&#25105;&#20204;&#30740;&#31350;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstracts derived from biomedical literature possess distinct domain-specific characteristics, including specialised writing styles and biomedical terminologies, which necessitate a deep understanding of the related literature. As a result, existing language models struggle to generate technical summaries that are on par with those produced by biomedical experts, given the absence of domain-specific background knowledge. This paper aims to enhance the performance of language models in biomedical abstractive summarisation by aggregating knowledge from external papers cited within the source article. We propose a novel attention-based citation aggregation model that integrates domain-specific knowledge from citation papers, allowing neural networks to generate summaries by leveraging both the paper content and relevant knowledge from citation papers. Furthermore, we construct and release a large-scale biomedical summarisation dataset that serves as a foundation for our research. Extensiv
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;3D&#22330;&#26223;&#29702;&#35299;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#23545;&#22810;&#27169;&#24577;3D&#26041;&#27861;&#30340;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.15676</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;3D&#22330;&#26223;&#29702;&#35299;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Multi-modal 3D Scene Understanding: A Comprehensive Survey and Evaluation. (arXiv:2310.15676v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15676
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;3D&#22330;&#26223;&#29702;&#35299;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#23545;&#22810;&#27169;&#24577;3D&#26041;&#27861;&#30340;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;3D&#22330;&#26223;&#29702;&#35299;&#22240;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#30340;&#21333;&#27169;&#24577;3D&#29702;&#35299;&#30456;&#27604;&#65292;&#24341;&#20837;&#39069;&#22806;&#30340;&#27169;&#24577;&#19981;&#20165;&#25552;&#21319;&#20102;&#22330;&#26223;&#35299;&#37322;&#30340;&#20016;&#23500;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#36824;&#30830;&#20445;&#20102;&#26356;&#31283;&#20581;&#21644;&#24377;&#24615;&#30340;&#29702;&#35299;&#12290;&#36825;&#22312;&#22810;&#26679;&#21270;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#23588;&#20026;&#20851;&#38190;&#65292;&#20165;&#20381;&#38752;3D&#25968;&#25454;&#21487;&#33021;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#23613;&#31649;&#22312;&#36807;&#21435;&#19977;&#24180;&#20013;&#20986;&#29616;&#20102;&#35768;&#22810;&#22810;&#27169;&#24577;3D&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#25972;&#21512;&#22810;&#25668;&#20687;&#22836;&#22270;&#20687;&#65288;3D+2D&#65289;&#21644;&#25991;&#26412;&#25551;&#36848;&#65288;3D+&#35821;&#35328;&#65289;&#30340;&#26041;&#27861;&#65292;&#20294;&#32570;&#20047;&#20840;&#38754;&#28145;&#20837;&#30340;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#26368;&#36817;&#30340;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#35843;&#26597;&#65292;&#20197;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;&#19968;&#20010;&#32972;&#26223;&#65292;&#27491;&#24335;&#23450;&#20041;&#20102;&#21508;&#31181;3D&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal 3D scene understanding has gained considerable attention due to its wide applications in many areas, such as autonomous driving and human-computer interaction. Compared to conventional single-modal 3D understanding, introducing an additional modality not only elevates the richness and precision of scene interpretation but also ensures a more robust and resilient understanding. This becomes especially crucial in varied and challenging environments where solely relying on 3D data might be inadequate. While there has been a surge in the development of multi-modal 3D methods over past three years, especially those integrating multi-camera images (3D+2D) and textual descriptions (3D+language), a comprehensive and in-depth review is notably absent. In this article, we present a systematic survey of recent progress to bridge this gap. We begin by briefly introducing a background that formally defines various 3D multi-modal tasks and summarizes their inherent challenges. After that
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26159;&#20851;&#20110;LLMs&#29983;&#25104;&#20869;&#23481;&#26816;&#27979;&#30340;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#29616;&#26377;&#31574;&#30053;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20513;&#37319;&#29992;&#26356;&#28789;&#27963;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#24378;&#35843;&#20351;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#19981;&#21516;&#25915;&#20987;&#12290;&#36825;&#26159;&#39318;&#20010;&#32508;&#21512;&#35843;&#26597;LLMs&#26102;&#20195;&#26816;&#27979;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.15654</link><description>&lt;p&gt;
LLMs&#29983;&#25104;&#20869;&#23481;&#26816;&#27979;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Detection of LLMs-Generated Content. (arXiv:2310.15654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15654
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26159;&#20851;&#20110;LLMs&#29983;&#25104;&#20869;&#23481;&#26816;&#27979;&#30340;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#29616;&#26377;&#31574;&#30053;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20513;&#37319;&#29992;&#26356;&#28789;&#27963;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#24378;&#35843;&#20351;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#19981;&#21516;&#25915;&#20987;&#12290;&#36825;&#26159;&#39318;&#20010;&#32508;&#21512;&#35843;&#26597;LLMs&#26102;&#20195;&#26816;&#27979;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#23548;&#33268;&#21512;&#25104;&#20869;&#23481;&#29983;&#25104;&#19981;&#26029;&#22686;&#21152;&#65292;&#28041;&#21450;&#23186;&#20307;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#20844;&#20849;&#35805;&#35821;&#21644;&#25945;&#32946;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;LLMs&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#29616;&#26377;&#26816;&#27979;&#31574;&#30053;&#21644;&#22522;&#20934;&#30340;&#35814;&#32454;&#27010;&#36848;&#65292;&#23457;&#26597;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#24182;&#30830;&#23450;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#21069;&#26223;&#65292;&#25552;&#20513;&#37319;&#29992;&#26356;&#28789;&#27963;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#20027;&#24352;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#24212;&#23545;&#19981;&#21516;&#25915;&#20987;&#65292;&#20197;&#25269;&#24481;LLMs&#19981;&#26029;&#21457;&#23637;&#30340;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;LLMs&#26102;&#20195;&#26816;&#27979;&#30340;&#39318;&#20010;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#24076;&#26395;&#23427;&#33021;&#22815;&#25552;&#20379;&#23545;LLMs&#29983;&#25104;&#20869;&#23481;&#26816;&#27979;&#24403;&#21069;&#24773;&#20917;&#30340;&#24191;&#27867;&#29702;&#35299;&#65292;&#24182;&#20026;&#30740;&#31350;&#35813;&#39046;&#22495;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning capabilities of advanced large language models (LLMs) such as ChatGPT have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. As such, the ability to detect LLMs-generated content has become of paramount importance. We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. We also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs. To the best of our knowledge, this work is the first comprehensive survey on the detection in the era of LLMs. We hope it will provide a broad understanding of the current landscape of LLMs-generated content detection, offering a guiding reference for res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#32844;&#19994;&#36335;&#24452;&#39044;&#27979;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31616;&#21382;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#20110;&#25216;&#33021;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#25991;&#26412;&#25551;&#36848;&#37096;&#20998;&#26469;&#39044;&#27979;&#19979;&#19968;&#27493;&#30340;&#32844;&#19994;&#21160;&#21521;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15636</link><description>&lt;p&gt;
&#20351;&#29992;&#31616;&#21382;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#20110;&#25216;&#33021;&#21305;&#37197;&#30340;&#32844;&#19994;&#36335;&#24452;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Career Path Prediction using Resume Representation Learning and Skill-based Matching. (arXiv:2310.15636v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#32844;&#19994;&#36335;&#24452;&#39044;&#27979;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31616;&#21382;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#20110;&#25216;&#33021;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#25991;&#26412;&#25551;&#36848;&#37096;&#20998;&#26469;&#39044;&#27979;&#19979;&#19968;&#27493;&#30340;&#32844;&#19994;&#21160;&#21521;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;-&#32844;&#19994;&#21305;&#37197;&#23545;&#24037;&#20316;&#28385;&#24847;&#24230;&#21644;&#24037;&#20316;&#32489;&#25928;&#30340;&#24433;&#21709;&#34987;&#24191;&#27867;&#25215;&#35748;&#65292;&#36825;&#20984;&#26174;&#20102;&#22312;&#32844;&#19994;&#29983;&#28079;&#20013;&#20026;&#24037;&#20316;&#32773;&#25552;&#20379;&#19979;&#19968;&#27493;&#34892;&#21160;&#30340;&#37325;&#35201;&#24615;&#12290;&#32844;&#19994;&#36335;&#24452;&#39044;&#27979;&#26159;&#39044;&#27979;&#32844;&#19994;&#29983;&#28079;&#20013;&#30340;&#19979;&#19968;&#27493;&#34892;&#21160;&#30340;&#20219;&#21153;&#65292;&#24182;&#20855;&#26377;&#21592;&#24037;&#27969;&#22833;&#39044;&#38450;&#21644;&#20869;&#37096;&#23703;&#20301;&#27969;&#21160;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;&#29616;&#26377;&#30340;&#32844;&#19994;&#36335;&#24452;&#39044;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#31169;&#20154;&#32844;&#19994;&#21382;&#21490;&#25968;&#25454;&#26469;&#24314;&#27169;&#32844;&#20301;&#21644;&#20844;&#21496;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31616;&#21382;&#20013;&#30340;&#24037;&#20316;&#32463;&#21382;&#37096;&#20998;&#30340;&#26410;&#24320;&#21457;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;2,164&#20010;&#21311;&#21517;&#21270;&#32844;&#19994;&#32463;&#21382;&#32452;&#25104;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#65292;&#24182;&#24102;&#26377;ESCO&#32844;&#19994;&#26631;&#31614;&#12290;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#24037;&#20316;&#21382;&#21490;&#25968;&#25454;&#35774;&#35745;&#30340;&#26032;&#39062;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;CareerBERT&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#25216;&#33021;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#26469;&#36827;&#34892;&#32844;&#19994;&#36335;&#24452;&#39044;&#27979;&#65292;&#22312;@10&#19979;&#65292;&#25216;&#33021;&#27169;&#22411;&#21644;&#25991;&#26412;&#27169;&#22411;&#23454;&#29616;&#20102;35.24%&#21644;39.61%&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impact of person-job fit on job satisfaction and performance is widely acknowledged, which highlights the importance of providing workers with next steps at the right time in their career. This task of predicting the next step in a career is known as career path prediction, and has diverse applications such as turnover prevention and internal job mobility. Existing methods to career path prediction rely on large amounts of private career history data to model the interactions between job titles and companies. We propose leveraging the unexplored textual descriptions that are part of work experience sections in resumes. We introduce a structured dataset of 2,164 anonymized career histories, annotated with ESCO occupation labels. Based on this dataset, we present a novel representation learning approach, CareerBERT, specifically designed for work history data. We develop a skill-based model and a text-based model for career path prediction, which achieve 35.24% and 39.61% recall@10 r
&lt;/p&gt;</description></item><item><title>Slisemap&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#27969;&#24418;&#21487;&#35270;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22312;&#29289;&#29702;&#25968;&#25454;&#20013;&#25214;&#21040;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.15610</link><description>&lt;p&gt;
&#20351;&#29992;Slisemap&#35299;&#37322;&#29289;&#29702;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Using Slisemap to interpret physical data. (arXiv:2310.15610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15610
&lt;/p&gt;
&lt;p&gt;
Slisemap&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#27969;&#24418;&#21487;&#35270;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22312;&#29289;&#29702;&#25968;&#25454;&#20013;&#25214;&#21040;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#21487;&#35270;&#21270;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#22312;&#29289;&#29702;&#31185;&#23398;&#20013;&#21487;&#35270;&#21270;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#23558;&#19968;&#31181;&#26368;&#36817;&#20171;&#32461;&#30340;&#27969;&#24418;&#21487;&#35270;&#21270;&#26041;&#27861;Slise&#24212;&#29992;&#20110;&#29289;&#29702;&#21644;&#21270;&#23398;&#25968;&#25454;&#38598;&#12290;Slisemap&#23558;&#27969;&#24418;&#21487;&#35270;&#21270;&#19982;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#30740;&#31350;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22797;&#26434;&#27169;&#25311;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36890;&#36807;Slisemap&#65292;&#25105;&#20204;&#25214;&#21040;&#19968;&#31181;&#23884;&#20837;&#65292;&#20351;&#24471;&#20855;&#26377;&#31867;&#20284;&#23616;&#37096;&#35299;&#37322;&#30340;&#25968;&#25454;&#39033;&#34987;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;Slisemap&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#40657;&#30418;&#27169;&#22411;&#19981;&#21516;&#34892;&#20026;&#30340;&#27010;&#35272;&#12290;&#36825;&#20351;&#24471;Slisemap&#25104;&#20026;&#19968;&#31181;&#26377;&#30417;&#30563;&#30340;&#27969;&#24418;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#23884;&#20837;&#30340;&#27169;&#24335;&#21453;&#26144;&#20102;&#30446;&#26631;&#23646;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#29289;&#29702;&#25968;&#25454;&#19978;&#20351;&#29992;&#21644;&#35780;&#20272;Slisemap&#65292;&#24182;&#35777;&#26126;Slisemap&#22312;&#25214;&#21040;&#20998;&#31867;&#21644;&#22238;&#24402;&#27169;&#22411;&#30340;&#26377;&#24847;&#20041;&#20449;&#24687;&#26041;&#38754;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manifold visualisation techniques are commonly used to visualise high-dimensional datasets in physical sciences. In this paper we apply a recently introduced manifold visualisation method, called Slise, on datasets from physics and chemistry. Slisemap combines manifold visualisation with explainable artificial intelligence. Explainable artificial intelligence is used to investigate the decision processes of black box machine learning models and complex simulators. With Slisemap we find an embedding such that data items with similar local explanations are grouped together. Hence, Slisemap gives us an overview of the different behaviours of a black box model. This makes Slisemap into a supervised manifold visualisation method, where the patterns in the embedding reflect a target property. In this paper we show how Slisemap can be used and evaluated on physical data and that Slisemap is helpful in finding meaningful information on classification and regression models trained on these data
&lt;/p&gt;</description></item><item><title>tagE&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#25552;&#21462;&#20219;&#21153;&#30340;&#20855;&#36523;&#20195;&#29702;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#26234;&#33021;&#20195;&#29702;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#26102;&#30340;&#27495;&#20041;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15605</link><description>&lt;p&gt;
tagE: &#35753;&#20855;&#36523;&#20195;&#29702;&#29702;&#35299;&#20154;&#31867;&#25351;&#20196;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
tagE: Enabling an Embodied Agent to Understand Human Instructions. (arXiv:2310.15605v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15605
&lt;/p&gt;
&lt;p&gt;
tagE&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#25552;&#21462;&#20219;&#21153;&#30340;&#20855;&#36523;&#20195;&#29702;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#26234;&#33021;&#20195;&#29702;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#26102;&#30340;&#27495;&#20041;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#20855;&#26377;&#29289;&#29702;&#23384;&#22312;&#30340;&#26234;&#33021;&#20195;&#29702;&#19982;&#20154;&#31867;&#20132;&#27969;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#30340;&#30740;&#31350;&#65292;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24847;&#22270;&#39044;&#27979;&#12289;&#38382;&#39064;&#22238;&#31572;&#21644;&#25688;&#35201;&#65292;&#20294;&#38754;&#21521;&#20855;&#36523;&#20195;&#29702;&#38656;&#35201;&#23454;&#38469;&#34892;&#21160;&#30340;&#24773;&#22659;&#30340;NLU&#30340;&#33539;&#22260;&#20173;&#28982;&#26377;&#38480;&#12290;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#27495;&#20041;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#32473;&#26234;&#33021;&#20195;&#29702;&#35299;&#35835;&#20154;&#31867;&#24847;&#22270;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#31216;&#20026;&#20855;&#36523;&#20195;&#29702;&#30340;&#20219;&#21153;&#21644;&#21442;&#25968;&#22522;&#30784;&#65288;tagE&#65289;&#12290;&#22312;&#26680;&#24515;&#37096;&#20998;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#37319;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20197;&#20174;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22797;&#26434;&#20219;&#21153;&#25351;&#20196;&#20013;&#25552;&#21462;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#21152;&#24378;&#20102;&#23884;&#22871;&#35299;&#30721;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language serves as the primary mode of communication when an intelligent agent with a physical presence engages with human beings. While a plethora of research focuses on natural language understanding (NLU), encompassing endeavors such as sentiment analysis, intent prediction, question answering, and summarization, the scope of NLU directed at situations necessitating tangible actions by an embodied agent remains limited. The inherent ambiguity and incompleteness inherent in natural language present challenges for intelligent agents striving to decipher human intention. To tackle this predicament head-on, we introduce a novel system known as task and argument grounding for Embodied agents (tagE). At its core, our system employs an inventive neural network model designed to extract a series of tasks from complex task instructions expressed in natural language. Our proposed model adopts an encoder-decoder framework enriched with nested decoding to effectively extract tasks and t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#32039;&#24613;&#27807;&#36890;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#32032;&#25551;&#38382;&#31572;&#20219;&#21153;&#65292;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#23454;&#29616;&#20102;&#26234;&#33021;agent&#20043;&#38388;&#30340;&#26377;&#38024;&#23545;&#24615;&#12289;&#39640;&#25928;&#30340;&#27807;&#36890;&#65292;&#24182;&#20855;&#26377;&#36739;&#22909;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15597</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#32032;&#25551;&#38382;&#31572;&#20013;&#30340;&#32039;&#24613;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Emergent Communication in Interactive Sketch Question Answering. (arXiv:2310.15597v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#32039;&#24613;&#27807;&#36890;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#32032;&#25551;&#38382;&#31572;&#20219;&#21153;&#65292;&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#23454;&#29616;&#20102;&#26234;&#33021;agent&#20043;&#38388;&#30340;&#26377;&#38024;&#23545;&#24615;&#12289;&#39640;&#25928;&#30340;&#27807;&#36890;&#65292;&#24182;&#20855;&#26377;&#36739;&#22909;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#32039;&#24613;&#27807;&#36890;&#65288;EC&#65289;&#26088;&#22312;&#36890;&#36807;&#32032;&#25551;&#23398;&#20064;&#27807;&#36890;&#65292;&#24182;&#25581;&#31034;&#20154;&#31867;&#27807;&#36890;&#30340;&#28436;&#21464;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#22810;&#36718;&#20132;&#20114;&#65292;&#22312;&#20154;&#31867;&#27807;&#36890;&#20013;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#32032;&#25551;&#38382;&#31572;&#65288;ISQA&#65289;&#20219;&#21153;&#65292;&#20854;&#20013;&#20004;&#20010;&#21512;&#20316;&#29609;&#23478;&#36890;&#36807;&#32032;&#25551;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#65292;&#20197;&#22238;&#31572;&#20851;&#20110;&#19968;&#24352;&#22270;&#29255;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#25928;&#30340;&#20132;&#20114;&#24335;EC&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#22312;&#38382;&#39064;&#22238;&#31572;&#20934;&#30830;&#24615;&#12289;&#32472;&#30011;&#22797;&#26434;&#24230;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#36825;&#19977;&#20010;&#35780;&#20272;&#22240;&#32032;&#20043;&#38388;&#21462;&#24471;&#26377;&#25928;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#21253;&#25324;&#20154;&#31867;&#35780;&#20272;&#65292;&#34920;&#26126;&#22810;&#36718;&#20132;&#20114;&#26426;&#21046;&#33021;&#22815;&#20419;&#36827;&#26234;&#33021;agent&#20043;&#38388;&#30340;&#26377;&#38024;&#23545;&#24615;&#12289;&#39640;&#25928;&#30340;&#27807;&#36890;&#65292;&#24182;&#20855;&#26377;&#36739;&#22909;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-based emergent communication (EC) aims to learn to communicate through sketches and demystify the evolution of human communication. Ironically, previous works neglect multi-round interaction, which is indispensable in human communication. To fill this gap, we first introduce a novel Interactive Sketch Question Answering (ISQA) task, where two collaborative players are interacting through sketches to answer a question about an image in a multi-round manner. To accomplish this task, we design a new and efficient interactive EC system, which can achieve an effective balance among three evaluation factors, including the question answering accuracy, drawing complexity and human interpretability. Our experimental results including human evaluation demonstrate that multi-round interactive mechanism facilitates targeted and efficient communication between intelligent agents with decent human interpretability.
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#24335;&#30693;&#35782;&#36716;&#31227;&#65288;RetriKT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#33539;&#20363;&#65292;&#23427;&#36890;&#36807;&#25552;&#21462;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24182;&#21033;&#29992; retrieval-based &#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#30693;&#35782;&#24212;&#29992;&#20110;&#26497;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26497;&#31471;&#30340;&#27169;&#22411;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15594</link><description>&lt;p&gt;
&#26816;&#32034;&#24335;&#30693;&#35782;&#36716;&#31227;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#26497;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression. (arXiv:2310.15594v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15594
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#24335;&#30693;&#35782;&#36716;&#31227;&#65288;RetriKT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#33539;&#20363;&#65292;&#23427;&#36890;&#36807;&#25552;&#21462;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24182;&#21033;&#29992; retrieval-based &#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#30693;&#35782;&#24212;&#29992;&#20110;&#26497;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26497;&#31471;&#30340;&#27169;&#22411;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24040;&#22823;&#35268;&#27169;&#32473;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#20294;&#23545;&#20110;&#22312;&#27169;&#22411;&#35268;&#27169;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#26102;&#23454;&#29616;&#26497;&#31471;&#27169;&#22411;&#21387;&#32553;&#24182;&#19981;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#33539;&#20363;&#65292;&#31216;&#20026;&#26816;&#32034;&#24335;&#30693;&#35782;&#36716;&#31227;&#65288;RetriKT&#65289;&#65292;&#23427;&#23558;LLM&#30340;&#30693;&#35782;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#26497;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;1%&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;LLM&#20013;&#25552;&#21462;&#30693;&#35782;&#26500;&#24314;&#30693;&#35782;&#23384;&#20648;&#65292;&#24182;&#20174;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#65292;&#21033;&#29992;&#23427;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36719;&#25552;&#31034;&#35843;&#25972;&#21644;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#12290;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, the massive size of these models poses huge challenges for their deployment in real-world applications. While numerous model compression techniques have been proposed, most of them are not well-suited for achieving extreme model compression when there is a significant gap in model scale. In this paper, we introduce a novel compression paradigm called Retrieval-based Knowledge Transfer (RetriKT), which effectively transfers the knowledge of LLMs to extremely small-scale models (e.g., 1%). In particular, our approach extracts knowledge from LLMs to construct a knowledge store, from which the small-scale model can retrieve relevant information and leverage it for effective inference. To improve the quality of the model, soft prompt tuning and Proximal Policy Optimization (PPO) reinforcement learning techniques are employed. Extensive experim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#24320;&#25918;&#28023;&#22495;&#28023;&#19978;&#30417;&#35270;&#20013;&#30340;&#26377;&#24847;AIS&#20851;&#38381;&#12290;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#39044;&#27979;&#32467;&#26524;&#26469;&#25253;&#21578;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.15586</link><description>&lt;p&gt;
&#26816;&#27979;&#24320;&#25918;&#28023;&#22495;&#28023;&#19978;&#30417;&#35270;&#20013;&#30340;&#26377;&#24847;AIS&#20851;&#38381;&#20351;&#29992;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Detecting Intentional AIS Shutdown in Open Sea Maritime Surveillance Using Self-Supervised Deep Learning. (arXiv:2310.15586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#24320;&#25918;&#28023;&#22495;&#28023;&#19978;&#30417;&#35270;&#20013;&#30340;&#26377;&#24847;AIS&#20851;&#38381;&#12290;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#39044;&#27979;&#32467;&#26524;&#26469;&#25253;&#21578;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28023;&#19978;&#20132;&#36890;&#30417;&#35270;&#20013;&#65292;&#26816;&#27979;&#38750;&#27861;&#27963;&#21160;&#65292;&#22914;&#38750;&#27861;&#25429;&#40060;&#25110;&#38750;&#27861;&#36135;&#29289;&#36716;&#33337;&#26159;&#27839;&#28023;&#31649;&#29702;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22312;&#24320;&#25918;&#28023;&#22495;&#20013;&#65292;&#20154;&#20204;&#24517;&#39035;&#20381;&#36182;&#33337;&#19978;&#30340;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;&#65288;AIS&#65289;&#21457;&#20986;&#30340;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#34987;&#30417;&#35270;&#21355;&#26143;&#25429;&#33719;&#12290;&#28982;&#32780;&#65292;&#19981;&#35802;&#23454;&#30340;&#33337;&#21482;&#36890;&#24120;&#20250;&#26377;&#24847;&#20851;&#38381;&#20854;AIS&#21457;&#23556;&#26426;&#65292;&#20197;&#38544;&#34255;&#38750;&#27861;&#27963;&#21160;&#12290;&#22312;&#24320;&#25918;&#28023;&#19978;&#65292;&#24456;&#38590;&#23558;&#26377;&#24847;&#30340;AIS&#20851;&#38381;&#19982;&#30001;&#20110;&#21327;&#35758;&#38480;&#21046;&#65292;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#25110;&#38480;&#21046;&#21355;&#26143;&#20301;&#32622;&#32780;&#23548;&#33268;&#30340;&#25509;&#25910;&#32570;&#22833;&#21306;&#20998;&#24320;&#26469;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#24322;&#24120;AIS&#25509;&#25910;&#20002;&#22833;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;&#39044;&#27979;&#26410;&#26469;&#19968;&#20998;&#38047;&#26159;&#21542;&#24212;&#25509;&#25910;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#39044;&#27979;&#32467;&#26524;&#26469;&#25253;&#21578;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In maritime traffic surveillance, detecting illegal activities, such as illegal fishing or transshipment of illicit products is a crucial task of the coastal administration. In the open sea, one has to rely on Automatic Identification System (AIS) message transmitted by on-board transponders, which are captured by surveillance satellites. However, insincere vessels often intentionally shut down their AIS transponders to hide illegal activities. In the open sea, it is very challenging to differentiate intentional AIS shutdowns from missing reception due to protocol limitations, bad weather conditions or restricting satellite positions. This paper presents a novel approach for the detection of abnormal AIS missing reception based on self-supervised deep learning techniques and transformer models. Using historical data, the trained model predicts if a message should be received in the upcoming minute or not. Afterwards, the model reports on detected anomalies by comparing the prediction w
&lt;/p&gt;</description></item><item><title>CONTRASTE&#26159;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#26041;&#38754;&#30340;&#25552;&#31034;&#24182;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#22686;&#24378;&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25277;&#21462;&#65288;ASTE&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20854;&#20182;ABSA&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.15577</link><description>&lt;p&gt;
CONTRASTE: &#19968;&#31181;&#24102;&#26377;&#22522;&#20110;&#26041;&#38754;&#30340;&#25552;&#31034;&#30340;&#30417;&#30563;&#23545;&#27604;&#39044;&#35757;&#32451;&#29992;&#20110;&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction. (arXiv:2310.15577v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15577
&lt;/p&gt;
&lt;p&gt;
CONTRASTE&#26159;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#26041;&#38754;&#30340;&#25552;&#31034;&#24182;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#22686;&#24378;&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25277;&#21462;&#65288;ASTE&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20854;&#20182;ABSA&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25277;&#21462;&#65288;ASTE&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#24320;&#21457;&#26356;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#22810;&#20010;ABSA&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CONTRASTE&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#22686;&#24378;ASTE&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#38500;&#20102;&#20027;&#35201;&#20851;&#27880;ASTE&#20043;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#22312;&#20854;&#20182;ABSA&#20219;&#21153;&#65288;&#22914;ACOS&#65292;TASD&#21644;AESC&#65289;&#19978;&#30340;&#20248;&#21183;&#12290;&#32473;&#23450;&#19968;&#20010;&#21477;&#23376;&#21450;&#20854;&#30456;&#20851;&#30340;&#65288;&#26041;&#38754;&#65292;&#35266;&#28857;&#65292;&#24773;&#24863;&#65289;&#19977;&#20803;&#32452;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#26041;&#38754;&#30340;&#25552;&#31034;&#65292;&#24182;&#23631;&#34109;&#20102;&#30456;&#24212;&#30340;&#24773;&#24863;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#35299;&#30721;&#22120;&#29983;&#25104;&#30340;&#26041;&#38754;&#24863;&#30693;&#24773;&#24863;&#34920;&#31034;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#26469;&#65288;&#39044;&#65289;&#35757;&#32451;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#12290;&#20026;&#20102;&#24494;&#35843;&#24471;&#21040;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#26041;&#27861;&#65292;&#20854;&#20013;&#22522;&#30784;&#32534;&#30721;&#35299;&#30721;&#27169;&#22411;&#21516;&#26102;&#34987;&#29992;&#20110;ASTG&#21644;&#20854;&#20182;ABSA&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing works on Aspect Sentiment Triplet Extraction (ASTE) explicitly focus on developing more efficient fine-tuning techniques for the task. Instead, our motivation is to come up with a generic approach that can improve the downstream performances of multiple ABSA tasks simultaneously. Towards this, we present CONTRASTE, a novel pre-training strategy using CONTRastive learning to enhance the ASTE performance. While we primarily focus on ASTE, we also demonstrate the advantage of our proposed technique on other ABSA tasks such as ACOS, TASD, and AESC. Given a sentence and its associated (aspect, opinion, sentiment) triplets, first, we design aspect-based prompts with corresponding sentiments masked. We then (pre)train an encoder-decoder model by applying contrastive learning on the decoder-generated aspect-aware sentiment representations of the masked terms. For fine-tuning the model weights thus obtained, we then propose a novel multi-task approach where the base encoder-decoder mod
&lt;/p&gt;</description></item><item><title>SteloCoder&#26159;&#19968;&#20010;&#20165;&#35299;&#30721;&#30340;&#22522;&#20110;StarCoder&#30340;LLM&#65292;&#22312;&#22810;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23427;&#37319;&#29992;Mixture-of-Experts&#65288;MoE&#65289;&#25216;&#26415;&#21644;&#38376;&#25511;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;StarCoder&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#19987;&#23478;&#65292;&#24182;&#20351;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;LoRA&#65289;&#25216;&#26415;&#26469;&#38480;&#21046;&#27599;&#20010;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.15539</link><description>&lt;p&gt;
SteloCoder:&#19968;&#31181;&#20165;&#35299;&#30721;&#30340;&#29992;&#20110;&#22810;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation. (arXiv:2310.15539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15539
&lt;/p&gt;
&lt;p&gt;
SteloCoder&#26159;&#19968;&#20010;&#20165;&#35299;&#30721;&#30340;&#22522;&#20110;StarCoder&#30340;LLM&#65292;&#22312;&#22810;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23427;&#37319;&#29992;Mixture-of-Experts&#65288;MoE&#65289;&#25216;&#26415;&#21644;&#38376;&#25511;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;StarCoder&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#19987;&#23478;&#65292;&#24182;&#20351;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;LoRA&#65289;&#25216;&#26415;&#26469;&#38480;&#21046;&#27599;&#20010;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#27880;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;StarCoder&#21644;Code Llama&#20998;&#21035;&#23637;&#31034;&#20102;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#20195;&#30721;&#32763;&#35793;&#21151;&#33021;&#19978;&#20173;&#28982;&#38656;&#35201;&#25913;&#36827;&#21644;&#26377;&#25928;&#35757;&#32451;&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SteloCoder&#65292;&#19968;&#31181;&#20165;&#35299;&#30721;&#30340;&#22522;&#20110;StarCoder&#30340;LLM&#65292;&#19987;&#20026;&#22810;&#32534;&#31243;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#32780;&#35774;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SteloCoder&#23454;&#29616;&#20102;C ++&#65292;C&#65283;&#65292;JavaScript&#65292;Java&#25110;PHP&#21040;Python&#20195;&#30721;&#32763;&#35793;&#65292;&#32780;&#26080;&#38656;&#25351;&#23450;&#36755;&#20837;&#32534;&#31243;&#35821;&#35328;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#32452;&#28151;&#21512;&#65288;Mixture-of-Experts&#65292;MoE&#65289;&#25216;&#26415;&#21644;&#19968;&#20010;&#25511;&#21046;&#22810;&#20219;&#21153;&#30340;&#38376;&#25511;&#32593;&#32476;&#26469;&#20462;&#25913;StarCoder&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;StarCoder&#36827;&#34892;&#24494;&#35843;&#26469;&#33719;&#24471;&#19987;&#23478;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;Low-Rank Adaptive Method&#65292;LoRA&#65289;&#25216;&#26415;&#65292;&#23558;&#27599;&#20010;&#19987;&#23478;&#30340;&#22823;&#23567;&#38480;&#21046;&#20026;StarCoder&#21442;&#25968;&#25968;&#37327;&#30340;&#20165;0.06&#65285;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#22686;&#24378;tr
&lt;/p&gt;
&lt;p&gt;
With the recent focus on Large Language Models (LLMs), both StarCoder (Li et al., 2023) and Code Llama (Rozi\`ere et al., 2023) have demonstrated remarkable performance in code generation. However, there is still a need for improvement in code translation functionality with efficient training techniques. In response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM designed specifically for multi-programming language-to-Python code translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or PHP-to-Python code translation without specifying the input programming language. We modified StarCoder model architecture by incorporating a Mixture-of-Experts (MoE) technique featuring five experts and a gating network for multi-task handling. Experts are obtained by StarCoder fine-tuning. Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each expert size as only 0.06% of number of StarCoder's parameters. At the same time, to enhance tr
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#21644;&#23545;&#27604;&#24335;&#33539;&#24335;&#22312;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#26159;&#20114;&#34917;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#23545;&#27604;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;GCMAE&#65289;&#26694;&#26550;&#26469;&#32479;&#19968;&#23427;&#20204;&#65292;GCMAE&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#20840;&#23616;&#20449;&#24687;&#26469;&#24357;&#34917;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#22312;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.15523</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#21644;&#23545;&#27604;&#24335;&#33539;&#24335;&#22312;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#20114;&#34917;
&lt;/p&gt;
&lt;p&gt;
Generative and Contrastive Paradigms Are Complementary for Graph Self-Supervised Learning. (arXiv:2310.15523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15523
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#21644;&#23545;&#27604;&#24335;&#33539;&#24335;&#22312;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#26159;&#20114;&#34917;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#23545;&#27604;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;GCMAE&#65289;&#26694;&#26550;&#26469;&#32479;&#19968;&#23427;&#20204;&#65292;GCMAE&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#20840;&#23616;&#20449;&#24687;&#26469;&#24357;&#34917;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#22312;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;GSSL&#65289;&#65292;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#36981;&#24490;&#29983;&#25104;&#24335;&#33539;&#24335;&#65292;&#24182;&#23398;&#20064;&#37325;&#26500;&#25513;&#30721;&#22270;&#30340;&#36793;&#32536;&#25110;&#33410;&#28857;&#29305;&#24449;&#12290;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#36890;&#36807;&#26368;&#22823;&#21270;&#21516;&#19968;&#22270;&#30340;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#24191;&#27867;&#29992;&#20110;GSSL&#12290;&#28982;&#32780;&#65292;MAE&#21644;CL&#22312;&#29616;&#26377;&#30340;GSSL&#24037;&#20316;&#20013;&#34987;&#21333;&#29420;&#32771;&#34385;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;MAE&#21644;CL&#30340;&#33539;&#24335;&#26159;&#20114;&#34917;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#22270;&#23545;&#27604;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;GCMAE&#65289;&#26694;&#26550;&#26469;&#32479;&#19968;&#23427;&#20204;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#23616;&#37096;&#36793;&#32536;&#25110;&#33410;&#28857;&#29305;&#24449;&#65292;MAE&#19981;&#33021;&#25429;&#25417;&#21040;&#22270;&#30340;&#20840;&#23616;&#20449;&#24687;&#65292;&#24182;&#23545;&#29305;&#23450;&#30340;&#36793;&#32536;&#21644;&#29305;&#24449;&#25935;&#24863;&#12290;&#30456;&#21453;&#65292;&#22312;&#25552;&#21462;&#20840;&#23616;&#20449;&#24687;&#26041;&#38754;&#65292;CL&#34920;&#29616;&#20986;&#33394;&#65292;&#22240;&#20026;&#23427;&#32771;&#34385;&#20102;&#22270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;GCMAE&#35013;&#22791;&#20102;&#19968;&#20010;MAE&#20998;&#25903;&#21644;&#19968;&#20010;CL&#20998;&#25903;&#65292;&#24182;&#19988;&#36825;&#20004;&#20010;&#20998;&#25903;&#20849;&#20139;&#19968;&#20010;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#65292;&#36825;&#20351;&#24471;MAE&#20998;&#25903;&#33021;&#22815;&#21033;&#29992;CL&#20998;&#25903;&#25552;&#21462;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#20026;&#20102;&#24378;&#21046;GCMAE&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;...
&lt;/p&gt;
&lt;p&gt;
For graph self-supervised learning (GSSL), masked autoencoder (MAE) follows the generative paradigm and learns to reconstruct masked graph edges or node features. Contrastive Learning (CL) maximizes the similarity between augmented views of the same graph and is widely used for GSSL. However, MAE and CL are considered separately in existing works for GSSL. We observe that the MAE and CL paradigms are complementary and propose the graph contrastive masked autoencoder (GCMAE) framework to unify them. Specifically, by focusing on local edges or node features, MAE cannot capture global information of the graph and is sensitive to particular edges and features. On the contrary, CL excels in extracting global information because it considers the relation between graphs. As such, we equip GCMAE with an MAE branch and a CL branch, and the two branches share a common encoder, which allows the MAE branch to exploit the global information extracted by the CL branch. To force GCMAE to capture glob
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#22238;&#31572;&#32422;&#26463;&#28385;&#36275;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;KITAB&#26469;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#28385;&#36275;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15511</link><description>&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#35780;&#20272;&#22522;&#20110;&#32422;&#26463;&#28385;&#36275;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval. (arXiv:2310.15511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#22238;&#31572;&#32422;&#26463;&#28385;&#36275;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;KITAB&#26469;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#28385;&#36275;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#22238;&#31572;&#32422;&#26463;&#28385;&#36275;&#26597;&#35810;&#65288;&#20363;&#22914;&#65292;&#8220;&#22307;&#22320;&#20122;&#21733;&#30340;&#20912;&#28103;&#28107;&#24215;&#21015;&#34920;&#8221;&#65289;&#30340;&#33021;&#21147;&#12290;&#36807;&#21435;&#65292;&#36825;&#26679;&#30340;&#26597;&#35810;&#34987;&#35748;&#20026;&#21482;&#33021;&#36890;&#36807;&#32593;&#32476;&#25628;&#32034;&#25110;&#30693;&#35782;&#24211;&#26469;&#35299;&#20915;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21021;&#27493;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24403;&#21069;&#30340;&#26816;&#32034;&#22522;&#20934;&#35201;&#20040;&#24050;&#39281;&#21644;&#65292;&#35201;&#20040;&#19981;&#33021;&#34913;&#37327;&#32422;&#26463;&#28385;&#36275;&#12290;&#21463;&#21040;&#23545;LLMs&#20107;&#23454;&#19981;&#27491;&#30830;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#26085;&#30410;&#20851;&#27880;&#30340;&#39537;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KITAB&#65292;&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#32422;&#26463;&#28385;&#36275;&#33021;&#21147;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;KITAB&#21253;&#21547;600&#22810;&#20301;&#20316;&#32773;&#21644;13,000&#20010;&#26597;&#35810;&#30340;&#19982;&#20070;&#31821;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#32852;&#30340;&#21160;&#24577;&#25968;&#25454;&#25910;&#38598;&#21644;&#32422;&#26463;&#39564;&#35777;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#20854;&#20182;&#20316;&#32773;&#30340;&#31867;&#20284;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;GPT4&#21644;GPT3.5&#36827;&#34892;&#20102;&#25193;&#23637;&#23454;&#39564;&#65292;&#23545;&#24120;&#35265;&#30340;&#22833;&#36133;&#27169;&#24335;&#36827;&#34892;&#20102;&#34920;&#24449;&#21644;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the ability of state-of-the art models to answer constraint satisfaction queries for information retrieval (e.g., 'a list of ice cream shops in San Diego'). In the past, such queries were considered to be tasks that could only be solved via web-search or knowledge bases. More recently, large language models (LLMs) have demonstrated initial emergent abilities in this task. However, many current retrieval benchmarks are either saturated or do not measure constraint satisfaction. Motivated by rising concerns around factual incorrectness and hallucinations of LLMs, we present KITAB, a new dataset for measuring constraint satisfaction abilities of language models. KITAB consists of book-related data across more than 600 authors and 13,000 queries, and also offers an associated dynamic data collection and constraint verification approach for acquiring similar test data for other authors. Our extended experiments on GPT4 and GPT3.5 characterize and decouple common failure modes acros
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#40065;&#26834;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32479;&#19968;&#22312;&#32447;Top-K&#25512;&#33616;&#65292;&#22312;&#22823;&#35268;&#27169;&#24037;&#19994;&#30005;&#23376;&#21830;&#21153;&#20013;&#35299;&#20915;&#29289;&#21697;&#24191;&#21578;&#21644;&#20869;&#23481;&#24191;&#21578;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#23454;&#20307;&#24191;&#21578;&#30340;&#26816;&#32034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15492</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#32479;&#19968;&#22312;&#32447;Top-K&#25512;&#33616;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Representation Learning for Unified Online Top-K Recommendation. (arXiv:2310.15492v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#40065;&#26834;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32479;&#19968;&#22312;&#32447;Top-K&#25512;&#33616;&#65292;&#22312;&#22823;&#35268;&#27169;&#24037;&#19994;&#30005;&#23376;&#21830;&#21153;&#20013;&#35299;&#20915;&#29289;&#21697;&#24191;&#21578;&#21644;&#20869;&#23481;&#24191;&#21578;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#23454;&#20307;&#24191;&#21578;&#30340;&#26816;&#32034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24037;&#19994;&#30005;&#23376;&#21830;&#21153;&#20013;&#65292;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#30340;&#25928;&#29575;&#23545;&#20110;&#25552;&#20379;&#39640;&#24230;&#30456;&#20851;&#30340;&#29289;&#21697;/&#20869;&#23481;&#24191;&#21578;&#20197;&#28385;&#36275;&#22810;&#26679;&#21270;&#30340;&#19994;&#21153;&#22330;&#26223;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#20165;&#20851;&#27880;&#29289;&#21697;&#24191;&#21578;&#65292;&#24573;&#35270;&#20102;&#20869;&#23481;&#24191;&#21578;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#31181;&#30095;&#24573;&#23548;&#33268;&#20102;&#22810;&#23454;&#20307;&#32467;&#26500;&#20869;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#19981;&#20844;&#24179;&#26816;&#32034;&#12290;&#27492;&#22806;&#65292;&#20174;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#23454;&#20307;&#24191;&#21578;&#20013;&#26816;&#32034;Top-K&#24191;&#21578;&#30340;&#25361;&#25112;&#20063;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;&#19981;&#21516;&#39046;&#22495;&#20869;&#30340;&#29992;&#25143;-&#23454;&#20307;&#34892;&#20026;&#34920;&#29616;&#20986;&#24046;&#24322;&#24615;&#21644;&#21516;&#36136;&#24615;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#22810;&#39046;&#22495;&#21305;&#37197;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#20855;&#26377;&#39046;&#22495;&#19981;&#21464;&#21644;&#39046;&#22495;&#29305;&#23450;&#34920;&#31034;&#30340;&#28151;&#21512;&#19987;&#23478;&#26694;&#26550;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20248;&#21270;&#19981;&#21516;&#19987;&#23478;&#30340;&#32452;&#21512;&#27169;&#24335;&#65292;&#26410;&#33021;&#35299;&#20915;&#20248;&#21270;&#20013;&#22266;&#26377;&#30340;&#22256;&#38590;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In large-scale industrial e-commerce, the efficiency of an online recommendation system is crucial in delivering highly relevant item/content advertising that caters to diverse business scenarios. However, most existing studies focus solely on item advertising, neglecting the significance of content advertising. This oversight results in inconsistencies within the multi-entity structure and unfair retrieval. Furthermore, the challenge of retrieving top-k advertisements from multi-entity advertisements across different domains adds to the complexity. Recent research proves that user-entity behaviors within different domains exhibit characteristics of differentiation and homogeneity. Therefore, the multi-domain matching models typically rely on the hybrid-experts framework with domain-invariant and domain-specific representations. Unfortunately, most approaches primarily focus on optimizing the combination mode of different experts, failing to address the inherent difficulty in optimizin
&lt;/p&gt;</description></item><item><title>NuTrea&#26159;&#19968;&#20010;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;GNN&#27169;&#22411;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#24341;&#23548;&#30340;&#22810;&#36339;&#30693;&#35782;&#22270;&#38382;&#31572;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26469;&#22686;&#24378;&#36807;&#21435;&#23548;&#21521;&#30340;&#23884;&#20837;&#65292;&#24182;&#24341;&#20837;&#20102;RF-IEF&#33410;&#28857;&#23884;&#20837;&#26469;&#26356;&#22909;&#22320;&#34920;&#24449;&#27169;&#31946;&#30340;&#30693;&#35782;&#22270;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.15484</link><description>&lt;p&gt;
NuTrea&#65306;&#29992;&#20110;&#19978;&#19979;&#25991;&#24341;&#23548;&#30340;&#22810;&#36339;&#30693;&#35782;&#22270;&#38382;&#31572;&#30340;&#31070;&#32463;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA. (arXiv:2310.15484v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15484
&lt;/p&gt;
&lt;p&gt;
NuTrea&#26159;&#19968;&#20010;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;GNN&#27169;&#22411;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#24341;&#23548;&#30340;&#22810;&#36339;&#30693;&#35782;&#22270;&#38382;&#31572;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26469;&#22686;&#24378;&#36807;&#21435;&#23548;&#21521;&#30340;&#23884;&#20837;&#65292;&#24182;&#24341;&#20837;&#20102;RF-IEF&#33410;&#28857;&#23884;&#20837;&#26469;&#26356;&#22909;&#22320;&#34920;&#24449;&#27169;&#31946;&#30340;&#30693;&#35782;&#22270;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36339;&#30340;&#30693;&#35782;&#22270;&#38382;&#31572;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#28041;&#21450;&#20174;&#30693;&#35782;&#22270;&#20013;&#26816;&#32034;&#33410;&#28857;&#20197;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#26368;&#36817;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#23558;&#27492;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#30693;&#35782;&#22270;&#36335;&#24452;&#25628;&#32034;&#38382;&#39064;&#65292;&#20854;&#20013;&#28040;&#24687;&#20174;&#31181;&#23376;&#33410;&#28857;&#27839;&#30528;&#36335;&#24452;&#20381;&#27425;&#20256;&#25773;&#21040;&#31572;&#26696;&#33410;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28040;&#24687;&#37117;&#26159;&#36807;&#21435;&#23548;&#21521;&#30340;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#23436;&#25972;&#30340;&#30693;&#35782;&#22270;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;GNN&#27169;&#22411;NuTrea&#65292;&#23427;&#23558;&#26356;&#24191;&#27867;&#30340;&#30693;&#35782;&#22270;&#19978;&#19979;&#25991;&#32435;&#20837;&#32771;&#34385;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#25506;&#32034;&#26410;&#21040;&#36798;&#30340;&#23376;&#26641;&#21306;&#22495;&#20197;&#25552;&#21319;&#36807;&#21435;&#23548;&#21521;&#30340;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20851;&#31995;&#39057;&#29575;-&#36870;&#23454;&#20307;&#39057;&#29575;&#65288;RF-IEF&#65289;&#33410;&#28857;&#23884;&#20837;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#24449;&#27169;&#31946;&#30340;&#30693;&#35782;&#22270;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-hop Knowledge Graph Question Answering (KGQA) is a task that involves retrieving nodes from a knowledge graph (KG) to answer natural language questions. Recent GNN-based approaches formulate this task as a KG path searching problem, where messages are sequentially propagated from the seed node towards the answer nodes. However, these messages are past-oriented, and they do not consider the full KG context. To make matters worse, KG nodes often represent proper noun entities and are sometimes encrypted, being uninformative in selecting between paths. To address these problems, we propose Neural Tree Search (NuTrea), a tree search-based GNN model that incorporates the broader KG context. Our model adopts a message-passing scheme that probes the unreached subtree regions to boost the past-oriented embeddings. In addition, we introduce the Relation Frequency-Inverse Entity Frequency (RF-IEF) node embedding that considers the global KG context to better characterize ambiguous KG nodes
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#30340;AutoDiff&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#21512;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24322;&#26500;&#29305;&#24449;&#21644;&#29305;&#24449;&#38388;&#30456;&#20851;&#24615;&#30340;&#25361;&#25112;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#22312;&#32479;&#35745;&#19978;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.15479</link><description>&lt;p&gt;
AutoDiff:&#32467;&#21512;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
AutoDiff: combining Auto-encoder and Diffusion model for tabular data synthesizing. (arXiv:2310.15479v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15479
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#32467;&#21512;&#30340;AutoDiff&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#21512;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24322;&#26500;&#29305;&#24449;&#21644;&#29305;&#24449;&#38388;&#30456;&#20851;&#24615;&#30340;&#25361;&#25112;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#22312;&#32479;&#35745;&#19978;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#35768;&#22810;&#23376;&#39046;&#22495;&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#20027;&#35201;&#33539;&#24335;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#35328;&#27169;&#22411;&#25110;&#35821;&#38899;&#21512;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#21147;&#37327;&#26469;&#29983;&#25104;&#21512;&#25104;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24322;&#26500;&#29305;&#24449;&#19968;&#30452;&#26159;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#21512;&#25104;&#22120;&#30456;&#27604;&#65292;&#25105;&#20204;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#34920;&#26684;&#22312;&#32479;&#35745;&#19978;&#19982;&#30495;&#23454;&#25968;&#25454;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#22312;15&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#28789;&#27963;&#22320;&#25429;&#25417;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#26159;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#22914;&#33509;&#25509;&#32435;&#20102;&#35770;&#25991;&#65292;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#26681;&#25454;&#35201;&#27714;&#25552;&#20379;&#65292;&#24182;&#19988;&#23558;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model has become a main paradigm for synthetic data generation in many subfields of modern machine learning, including computer vision, language model, or speech synthesis. In this paper, we leverage the power of diffusion model for generating synthetic tabular data. The heterogeneous features in tabular data have been main obstacles in tabular data synthesis, and we tackle this problem by employing the auto-encoder architecture. When compared with the state-of-the-art tabular synthesizers, the resulting synthetic tables from our model show nice statistical fidelities to the real data, and perform well in downstream tasks for machine learning utilities. We conducted the experiments over 15 publicly available datasets. Notably, our model adeptly captures the correlations among features, which has been a long-standing challenge in tabular data synthesis. Our code is available upon request and will be publicly released if paper is accepted.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#38598;&#20013;&#21040;&#20998;&#25955;&#30340;&#26041;&#27861;&#22312;&#30005;&#21147;&#34892;&#19994;&#20013;&#30340;&#36716;&#21464;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#23545;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#36171;&#33021;&#21644;&#30005;&#32593;&#31649;&#29702;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#20135;&#29983;&#21644;&#28040;&#32791;&#26041;&#38754;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23558;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#26234;&#33021;&#30005;&#32593;&#21487;&#20197;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12289;&#26356;&#22909;&#22320;&#21709;&#24212;&#38656;&#27714;&#21644;&#26356;&#22909;&#22320;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#35299;&#20915;&#22788;&#29702;&#22823;&#25968;&#25454;&#37327;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#19987;&#19994;&#30693;&#35782;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.15468</link><description>&lt;p&gt;
&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#31995;&#32479;&#21644;&#30005;&#32593;&#20248;&#21270;&#20013;&#36171;&#33021;&#20998;&#24067;&#24335;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Empowering Distributed Solutions in Renewable Energy Systems and Grid Optimization. (arXiv:2310.15468v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#38598;&#20013;&#21040;&#20998;&#25955;&#30340;&#26041;&#27861;&#22312;&#30005;&#21147;&#34892;&#19994;&#20013;&#30340;&#36716;&#21464;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#23545;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#36171;&#33021;&#21644;&#30005;&#32593;&#31649;&#29702;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#20135;&#29983;&#21644;&#28040;&#32791;&#26041;&#38754;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23558;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#26234;&#33021;&#30005;&#32593;&#21487;&#20197;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12289;&#26356;&#22909;&#22320;&#21709;&#24212;&#38656;&#27714;&#21644;&#26356;&#22909;&#22320;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#35299;&#20915;&#22788;&#29702;&#22823;&#25968;&#25454;&#37327;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#19987;&#19994;&#30693;&#35782;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30005;&#21147;&#34892;&#19994;&#20174;&#38598;&#20013;&#24335;&#21521;&#20998;&#25955;&#24335;&#26041;&#27861;&#30340;&#36716;&#21464;&#65292;&#29305;&#21035;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#36827;&#23637;&#22312;&#36171;&#33021;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#25913;&#21892;&#30005;&#32593;&#31649;&#29702;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;ML&#27169;&#22411;&#22312;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#20135;&#29983;&#21644;&#28040;&#32791;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20351;&#29992;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#20915;&#31574;&#26641;&#31561;&#21508;&#31181;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#22914;&#25968;&#25454;&#20998;&#21106;&#12289;&#24402;&#19968;&#21270;&#12289;&#20998;&#35299;&#21644;&#31163;&#25955;&#21270;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23558;&#22823;&#25968;&#25454;&#21644;ML&#34701;&#20837;&#26234;&#33021;&#30005;&#32593;&#20855;&#26377;&#22810;&#31181;&#20248;&#21183;&#65292;&#21253;&#25324;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#65292;&#26356;&#26377;&#25928;&#22320;&#24212;&#23545;&#38656;&#27714;&#65292;&#26356;&#22909;&#22320;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#12290;&#28982;&#32780;&#65292;&#36824;&#38656;&#35201;&#35299;&#20915;&#22788;&#29702;&#22823;&#25968;&#25454;&#37327;&#12289;&#30830;&#20445;&#32593;&#32476;&#23433;&#20840;&#21644;&#33719;&#21462;&#19987;&#19994;&#30693;&#35782;&#31561;&#25361;&#25112;&#12290;&#30740;&#31350;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study delves into the shift from centralized to decentralized approaches in the electricity industry, with a particular focus on how machine learning (ML) advancements play a crucial role in empowering renewable energy sources and improving grid management. ML models have become increasingly important in predicting renewable energy generation and consumption, utilizing various techniques like artificial neural networks, support vector machines, and decision trees. Furthermore, data preprocessing methods, such as data splitting, normalization, decomposition, and discretization, are employed to enhance prediction accuracy.  The incorporation of big data and ML into smart grids offers several advantages, including heightened energy efficiency, more effective responses to demand, and better integration of renewable energy sources. Nevertheless, challenges like handling large data volumes, ensuring cybersecurity, and obtaining specialized expertise must be addressed. The research inves
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;LLMs&#29983;&#25104;UI&#24067;&#23616;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;UI&#35821;&#27861;&#26469;&#25913;&#36827;&#29983;&#25104;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#26377;&#28508;&#21147;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29992;&#25143;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.15455</link><description>&lt;p&gt;
LLM&#24341;&#23548;&#19979;&#22522;&#20110;UI&#35821;&#27861;&#30340;UI&#24067;&#23616;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
UI Layout Generation with LLMs Guided by UI Grammar. (arXiv:2310.15455v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;LLMs&#29983;&#25104;UI&#24067;&#23616;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;UI&#35821;&#27861;&#26469;&#25913;&#36827;&#29983;&#25104;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#26377;&#28508;&#21147;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29992;&#25143;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#34892;&#19994;&#19987;&#19994;&#20154;&#21592;&#30340;&#20852;&#36259;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#31227;&#21160;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#24212;&#29992;LLMs&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;UI&#24067;&#23616;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;UI&#35821;&#27861;&#20316;&#20026;&#25105;&#20204;&#25552;&#20986;&#30340;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#26469;&#34920;&#31034;UI&#23631;&#24149;&#20013;&#22266;&#26377;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#30340;&#30446;&#30340;&#26159;&#26356;&#26377;&#25928;&#22320;&#25351;&#23548;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25552;&#39640;&#29983;&#25104;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26377;&#21487;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29992;&#25143;&#30028;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#27604;&#36739;&#30740;&#31350;&#34920;&#26126;&#22522;&#20110;&#35821;&#27861;&#30340;&#26041;&#27861;&#22312;&#29305;&#23450;&#26041;&#38754;&#26377;&#25913;&#36827;&#29983;&#25104;&#32467;&#26524;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in Large Language Models (LLMs) have stimulated interest among researchers and industry professionals, particularly in their application to tasks concerning mobile user interfaces (UIs). This position paper investigates the use of LLMs for UI layout generation. Central to our exploration is the introduction of UI grammar -- a novel approach we proposed to represent the hierarchical structure inherent in UI screens. The aim of this approach is to guide the generative capacities of LLMs more effectively and improve the explainability and controllability of the process. Initial experiments conducted with GPT-4 showed the promising capability of LLMs to produce high-quality user interfaces via in-context learning. Furthermore, our preliminary comparative study suggested the potential of the grammar-based approach in improving the quality of generative results in specific aspects.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;AI&#21644;UI&#35774;&#35745;&#32039;&#23494;&#32467;&#21512;&#65292;&#24320;&#21457;&#20102;PromptInfuser&#25554;&#20214;&#65292;&#23558;&#25552;&#31034;&#19982;UI&#20803;&#32032;&#30456;&#36830;&#65292;&#36890;&#36807;&#19982;&#35774;&#35745;&#24072;&#29616;&#26377;&#30340;AI&#21407;&#22411;&#21270;&#24037;&#20316;&#27969;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;PromptInfuser&#22312;&#20256;&#36798;&#20135;&#21697;&#29702;&#24565;&#12289;&#21407;&#22411;&#21046;&#20316;&#25928;&#29575;&#21644;&#38382;&#39064;&#39044;&#27979;&#26041;&#38754;&#26356;&#26377;&#29992;&#65292;&#40723;&#21169;&#35774;&#35745;&#24072;&#21516;&#26102;&#36845;&#20195;&#25552;&#31034;&#21644;UI&#12290;</title><link>http://arxiv.org/abs/2310.15435</link><description>&lt;p&gt;
PromptInfuser: &#22914;&#20309;&#32039;&#23494;&#32467;&#21512;AI&#21644;UI&#35774;&#35745;&#24433;&#21709;&#35774;&#35745;&#24072;&#30340;&#24037;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
PromptInfuser: How Tightly Coupling AI and UI Design Impacts Designers' Workflows. (arXiv:2310.15435v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;AI&#21644;UI&#35774;&#35745;&#32039;&#23494;&#32467;&#21512;&#65292;&#24320;&#21457;&#20102;PromptInfuser&#25554;&#20214;&#65292;&#23558;&#25552;&#31034;&#19982;UI&#20803;&#32032;&#30456;&#36830;&#65292;&#36890;&#36807;&#19982;&#35774;&#35745;&#24072;&#29616;&#26377;&#30340;AI&#21407;&#22411;&#21270;&#24037;&#20316;&#27969;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;PromptInfuser&#22312;&#20256;&#36798;&#20135;&#21697;&#29702;&#24565;&#12289;&#21407;&#22411;&#21046;&#20316;&#25928;&#29575;&#21644;&#38382;&#39064;&#39044;&#27979;&#26041;&#38754;&#26356;&#26377;&#29992;&#65292;&#40723;&#21169;&#35774;&#35745;&#24072;&#21516;&#26102;&#36845;&#20195;&#25552;&#31034;&#21644;UI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#21270;AI&#24212;&#29992;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25552;&#31034;&#22823;&#22823;&#38477;&#20302;&#20102;AI&#21407;&#22411;&#21270;&#30340;&#38556;&#30861;&#65292;&#20294;&#35774;&#35745;&#24072;&#20173;&#28982;&#29420;&#31435;&#22320;&#21407;&#22411;&#21270;AI&#21151;&#33021;&#21644;UI&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#31034;&#21644;UI&#35774;&#35745;&#30340;&#32806;&#21512;&#22914;&#20309;&#24433;&#21709;&#35774;&#35745;&#24072;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;PromptInfuser&#65292;&#19968;&#20010;Figma&#25554;&#20214;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#25143;&#36890;&#36807;&#23558;UI&#20803;&#32032;&#36830;&#25509;&#21040;&#25552;&#31034;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#21019;&#24314;&#21322;&#21151;&#33021;&#30340;&#27169;&#22411;&#12290;&#22312;&#19982;14&#20301;&#35774;&#35745;&#24072;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;PromptInfuser&#19982;&#35774;&#35745;&#24072;&#24403;&#21069;&#30340;AI&#21407;&#22411;&#21270;&#24037;&#20316;&#27969;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;PromptInfuser&#34987;&#35748;&#20026;&#22312;&#20256;&#36798;&#20135;&#21697;&#29702;&#24565;&#26041;&#38754;&#26356;&#26377;&#29992;&#65292;&#33021;&#22815;&#26356;&#30495;&#23454;&#22320;&#21576;&#29616;&#35774;&#24819;&#30340;&#35774;&#35745;&#21407;&#22411;&#65292;&#21407;&#22411;&#21046;&#20316;&#26356;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;UI&#38382;&#39064;&#21644;&#25216;&#26415;&#38480;&#21046;&#26041;&#38754;&#26356;&#26377;&#24110;&#21161;&#12290;PromptInfuser&#40723;&#21169;&#21516;&#26102;&#36845;&#20195;&#25552;&#31034;&#21644;UI&#65292;&#36825;&#26377;&#21161;&#20110;&#35774;&#35745;&#24072;&#21457;&#29616;UI&#21644;&#25552;&#31034;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prototyping AI applications is notoriously difficult. While large language model (LLM) prompting has dramatically lowered the barriers to AI prototyping, designers are still prototyping AI functionality and UI separately. We investigate how coupling prompt and UI design affects designers' workflows. Grounding this research, we developed PromptInfuser, a Figma plugin that enables users to create semi-functional mockups, by connecting UI elements to the inputs and outputs of prompts. In a study with 14 designers, we compare PromptInfuser to designers' current AI-prototyping workflow. PromptInfuser was perceived to be significantly more useful for communicating product ideas, more capable of producing prototypes that realistically represent the envisioned artifact, more efficient for prototyping, and more helpful for anticipating UI issues and technical constraints. PromptInfuser encouraged iteration over prompt and UI together, which helped designers identify UI and prompt incompatibilit
&lt;/p&gt;</description></item><item><title>ConstitutionMaker&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#36890;&#36807;&#24110;&#21161;&#29992;&#25143;&#23558;&#20182;&#20204;&#30340;&#21453;&#39304;&#36716;&#21270;&#20026;&#19968;&#32452;&#21407;&#21017;&#65292;&#20174;&#32780;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25209;&#35780;&#21644;&#35843;&#25972;&#65292;&#20197;&#25913;&#21892;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2310.15428</link><description>&lt;p&gt;
ConstitutionMaker:&#36890;&#36807;&#23558;&#29992;&#25143;&#21453;&#39304;&#36716;&#21270;&#20026;&#21407;&#21017;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#25209;&#35780;
&lt;/p&gt;
&lt;p&gt;
ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles. (arXiv:2310.15428v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15428
&lt;/p&gt;
&lt;p&gt;
ConstitutionMaker&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#36890;&#36807;&#24110;&#21161;&#29992;&#25143;&#23558;&#20182;&#20204;&#30340;&#21453;&#39304;&#36716;&#21270;&#20026;&#19968;&#32452;&#21407;&#21017;&#65292;&#20174;&#32780;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25209;&#35780;&#21644;&#35843;&#25972;&#65292;&#20197;&#25913;&#21892;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#31034;&#26159;&#19968;&#31181;&#35753;&#29992;&#25143;&#21019;&#24314;&#21644;&#23450;&#21046;&#33258;&#24049;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26032;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#24341;&#23548;&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20986;&#30340;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#65292;&#19981;&#25903;&#25345;&#29992;&#25143;&#23558;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#33258;&#28982;&#21453;&#39304;&#36716;&#25442;&#20026;&#25552;&#31034;&#25110;&#27169;&#22411;&#30340;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#24110;&#21161;&#29992;&#25143;&#23558;&#20182;&#20204;&#30340;&#21453;&#39304;&#36716;&#21270;&#20026;&#19968;&#32452;&#21407;&#21017;&#65288;&#21363;&#23466;&#27861;&#65289;&#26469;&#20351;&#29992;&#25143;&#33021;&#22815;&#36890;&#36807;&#21453;&#39304;&#19982;&#25913;&#36827;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#20132;&#20114;&#12290;&#20174;&#19968;&#39033;&#24418;&#25104;&#24615;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65288;1&#65289;&#29992;&#25143;&#38656;&#35201;&#25903;&#25345;&#23558;&#20182;&#20204;&#30340;&#21453;&#39304;&#36716;&#21270;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21407;&#21017;&#65292;&#24182;&#19988;&#65288;2&#65289;&#23545;&#29992;&#25143;&#25152;&#38656;&#30340;&#19981;&#21516;&#21407;&#21017;&#31867;&#22411;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ConstitutionMaker&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#29992;&#25143;&#21453;&#39304;&#36716;&#21270;&#20026;&#21407;&#21017;&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#29992;&#20110;&#24341;&#23548;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM) prompting is a promising new approach for users to create and customize their own chatbots. However, current methods for steering a chatbot's outputs, such as prompt engineering and fine-tuning, do not support users in converting their natural feedback on the model's outputs to changes in the prompt or model. In this work, we explore how to enable users to interactively refine model outputs through their feedback, by helping them convert their feedback into a set of principles (i.e. a constitution) that dictate the model's behavior. From a formative study, we (1) found that users needed support converting their feedback into principles for the chatbot and (2) classified the different principle types desired by users. Inspired by these findings, we developed ConstitutionMaker, an interactive tool for converting user feedback into principles, to steer LLM-based chatbots. With ConstitutionMaker, users can provide either positive or negative feedback in natural l
&lt;/p&gt;</description></item><item><title>FANToM&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#21387;&#21147;&#27979;&#35797;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;&#27169;&#22411;&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#24471;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.15421</link><description>&lt;p&gt;
FANToM: &#22312;&#20132;&#20114;&#20013;&#23545;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions. (arXiv:2310.15421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15421
&lt;/p&gt;
&lt;p&gt;
FANToM&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#21387;&#21147;&#27979;&#35797;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;&#27169;&#22411;&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#24471;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#30340;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#32570;&#20047;&#20114;&#21160;&#24615;&#30340;&#34987;&#21160;&#25925;&#20107;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FANToM&#65292;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#24515;&#26234;&#29702;&#35770;&#30340;&#21387;&#21147;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#32467;&#21512;&#20102;&#24515;&#29702;&#23398;&#20013;&#30340;&#37325;&#35201;&#29702;&#35770;&#35201;&#27714;&#21644;&#23545;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#24517;&#35201;&#30340;&#32463;&#39564;&#32771;&#34385;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#22810;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#35201;&#27714;&#30456;&#21516;&#30340;&#22522;&#26412;&#25512;&#29702;&#26469;&#35782;&#21035;LLM&#20013;&#19981;&#23384;&#22312;&#25110;&#34394;&#20551;&#30340;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FANToM&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;LLM&#20063;&#34920;&#29616;&#27604;&#20154;&#31867;&#24046;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25919;&#31574;&#20248;&#21270;&#36807;&#31243;&#20013;&#38750;&#24179;&#28369;&#25110;&#20998;&#24418;&#30340;&#20248;&#21270;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35299;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22266;&#26377;&#38480;&#21046;&#30340;&#26694;&#26550;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#26469;&#35782;&#21035;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#36935;&#21040;&#20998;&#24418;&#26223;&#35266;&#12290;</title><link>http://arxiv.org/abs/2310.15418</link><description>&lt;p&gt;
&#25919;&#31574;&#20248;&#21270;&#20013;&#30340;&#20998;&#24418;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
Fractal Landscapes in Policy Optimization. (arXiv:2310.15418v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15418
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25919;&#31574;&#20248;&#21270;&#36807;&#31243;&#20013;&#38750;&#24179;&#28369;&#25110;&#20998;&#24418;&#30340;&#20248;&#21270;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35299;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22266;&#26377;&#38480;&#21046;&#30340;&#26694;&#26550;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#26469;&#35782;&#21035;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#36935;&#21040;&#20998;&#24418;&#26223;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#26799;&#24230;&#26159;&#36830;&#32493;&#39046;&#22495;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26680;&#24515;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#23454;&#36341;&#20013;&#32463;&#24120;&#35266;&#23519;&#21040;&#21033;&#29992;&#31574;&#30053;&#26799;&#24230;&#36827;&#34892;RL&#35757;&#32451;&#21487;&#33021;&#22240;&#20026;&#22810;&#31181;&#21407;&#22240;&#32780;&#22833;&#36133;&#65292;&#29978;&#33267;&#22312;&#24050;&#30693;&#35299;&#30340;&#26631;&#20934;&#25511;&#21046;&#38382;&#39064;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#29702;&#35299;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#19968;&#20010;&#22266;&#26377;&#38480;&#21046;&#65306;&#23545;&#20110;&#26576;&#20123;&#31867;&#21035;&#30340;MDPs&#65292;&#31574;&#30053;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#26223;&#35266;&#21487;&#20197;&#38750;&#24120;&#38750;&#24179;&#28369;&#25110;&#20998;&#24418;&#65292;&#20197;&#33267;&#20110;&#26681;&#26412;&#19981;&#23384;&#22312;&#38656;&#35201;&#20272;&#35745;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#20511;&#37492;&#28151;&#27788;&#29702;&#35770;&#21644;&#38750;&#24179;&#28369;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#31574;&#30053;&#20248;&#21270;&#30446;&#26631;&#30340;&#26368;&#22823;Lyapunov&#25351;&#25968;&#21644;H&#246;lder&#25351;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26679;&#26412;&#20013;&#20272;&#35745;&#30446;&#26631;&#20989;&#25968;&#30340;&#23616;&#37096;&#24179;&#28369;&#24615;&#65292;&#20197;&#20415;&#35782;&#21035;&#35757;&#32451;&#36807;&#31243;&#26159;&#21542;&#36935;&#21040;&#20998;&#24418;&#26223;&#35266;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23454;&#39564;&#26469;&#35828;&#26126;&#19968;&#20123;&#31574;&#30053;&#20248;&#21270;&#22833;&#36133;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy gradient lies at the core of deep reinforcement learning (RL) in continuous domains. Despite much success, it is often observed in practice that RL training with policy gradient can fail for many reasons, even on standard control problems with known solutions. We propose a framework for understanding one inherent limitation of the policy gradient approach: the optimization landscape in the policy space can be extremely non-smooth or fractal for certain classes of MDPs, such that there does not exist gradient to be estimated in the first place. We draw on techniques from chaos theory and non-smooth analysis, and analyze the maximal Lyapunov exponents and H\"older exponents of the policy optimization objectives. Moreover, we develop a practical method that can estimate the local smoothness of objective function from samples to identify when the training process has encountered fractal landscapes. We show experiments to illustrate how some failure cases of policy optimization can b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#28857;/&#39034;&#24207;&#37325;&#26500;&#27169;&#22411;&#30340;&#26694;&#26550;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#37325;&#26500;&#35823;&#24046;&#30340;&#32452;&#21512;&#20540;&#20043;&#27604;&#24471;&#21040;&#21629;&#21517;&#20998;&#25968;, &#36827;&#19968;&#27493;&#32467;&#21512;&#21629;&#21517;&#20998;&#25968;&#21644;&#24322;&#24120;&#20998;&#25968;&#23548;&#20986;&#24863;&#24212;&#24322;&#24120;&#20998;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#28857;&#24322;&#24120;&#21644;&#19978;&#19979;&#25991;&#24322;&#24120;&#30340;&#37327;&#21270;&#21644;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.15416</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;/&#39034;&#24207;&#37325;&#26500;&#30340;&#21629;&#21517;&#20998;&#25968;&#26465;&#20214;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Nominality Score Conditioned Time Series Anomaly Detection by Point/Sequential Reconstruction. (arXiv:2310.15416v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#28857;/&#39034;&#24207;&#37325;&#26500;&#27169;&#22411;&#30340;&#26694;&#26550;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#37325;&#26500;&#35823;&#24046;&#30340;&#32452;&#21512;&#20540;&#20043;&#27604;&#24471;&#21040;&#21629;&#21517;&#20998;&#25968;, &#36827;&#19968;&#27493;&#32467;&#21512;&#21629;&#21517;&#20998;&#25968;&#21644;&#24322;&#24120;&#20998;&#25968;&#23548;&#20986;&#24863;&#24212;&#24322;&#24120;&#20998;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#28857;&#24322;&#24120;&#21644;&#19978;&#19979;&#25991;&#24322;&#24120;&#30340;&#37327;&#21270;&#21644;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#27169;&#24335;&#30340;&#23384;&#22312;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#22256;&#38590;&#22312;&#20110;&#24314;&#27169;&#26102;&#38388;&#30456;&#20851;&#30340;&#20851;&#31995;&#20197;&#23547;&#25214;&#19978;&#19979;&#25991;&#24322;&#24120;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#28857;&#24322;&#24120;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#22522;&#20110;&#28857;&#21644;&#24207;&#21015;&#30340;&#37325;&#26500;&#27169;&#22411;&#12290;&#22522;&#20110;&#28857;&#30340;&#27169;&#22411;&#35797;&#22270;&#37327;&#21270;&#28857;&#24322;&#24120;&#65292;&#32780;&#22522;&#20110;&#24207;&#21015;&#30340;&#27169;&#22411;&#35797;&#22270;&#37327;&#21270;&#28857;&#24322;&#24120;&#21644;&#19978;&#19979;&#25991;&#24322;&#24120;&#12290;&#22312;&#20551;&#35774;&#35266;&#23519;&#21040;&#30340;&#26102;&#38388;&#28857;&#26159;&#20174;&#19968;&#31181;&#26631;&#20934;&#26102;&#38388;&#28857;&#24320;&#22987;&#30340;&#20004;&#20010;&#38454;&#27573;&#30340;&#20559;&#31163;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;&#37325;&#26500;&#35823;&#24046;&#30340;&#32452;&#21512;&#20540;&#20043;&#27604;&#35745;&#31639;&#24471;&#20986;&#30340;&#21629;&#21517;&#20998;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#19968;&#27493;&#25972;&#21512;&#21629;&#21517;&#20998;&#25968;&#21644;&#24322;&#24120;&#20998;&#25968;&#26469;&#23548;&#20986;&#24863;&#24212;&#24322;&#24120;&#20998;&#25968;&#65292;&#28982;&#21518;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#24863;&#24212;&#24322;&#24120;&#20998;&#25968;&#20248;&#20110;&#21407;&#22987;&#24322;&#24120;&#20998;&#25968;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection is challenging due to the complexity and variety of patterns that can occur. One major difficulty arises from modeling time-dependent relationships to find contextual anomalies while maintaining detection accuracy for point anomalies. In this paper, we propose a framework for unsupervised time series anomaly detection that utilizes point-based and sequence-based reconstruction models. The point-based model attempts to quantify point anomalies, and the sequence-based model attempts to quantify both point and contextual anomalies. Under the formulation that the observed time point is a two-stage deviated value from a nominal time point, we introduce a nominality score calculated from the ratio of a combined value of the reconstruction errors. We derive an induced anomaly score by further integrating the nominality score and anomaly score, then theoretically prove the superiority of the induced anomaly score over the original anomaly score under certain condi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#23545;&#35805;&#27169;&#22411;&#24847;&#35782;&#21040;&#26102;&#38388;&#30340;&#24819;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27425;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;GapChat&#65292;&#26174;&#31034;&#20986;&#26102;&#38388;&#24863;&#30693;&#30340;&#27169;&#22411;&#22312;&#21028;&#26029;&#35805;&#39064;&#30456;&#20851;&#24615;&#21644;&#20174;&#23545;&#35805;&#20013;&#33719;&#21462;&#20449;&#24687;&#30340;&#24230;&#37327;&#26631;&#20934;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.15415</link><description>&lt;p&gt;
&#27880;&#24847;&#23545;&#35805;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#25552;&#39640;&#38271;&#26399;&#23545;&#35805;&#29983;&#25104;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation. (arXiv:2310.15415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#23545;&#35805;&#27169;&#22411;&#24847;&#35782;&#21040;&#26102;&#38388;&#30340;&#24819;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27425;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;GapChat&#65292;&#26174;&#31034;&#20986;&#26102;&#38388;&#24863;&#30693;&#30340;&#27169;&#22411;&#22312;&#21028;&#26029;&#35805;&#39064;&#30456;&#20851;&#24615;&#21644;&#20174;&#23545;&#35805;&#20013;&#33719;&#21462;&#20449;&#24687;&#30340;&#24230;&#37327;&#26631;&#20934;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#36947;&#22914;&#20309;&#32467;&#26463;&#21644;&#24674;&#22797;&#23545;&#35805;&#26159;&#20132;&#27969;&#30340;&#33258;&#28982;&#37096;&#20998;&#65292;&#20801;&#35768;&#35752;&#35770;&#36328;&#36234;&#25968;&#21608;&#12289;&#25968;&#26376;&#25110;&#25968;&#24180;&#12290;&#23545;&#35805;&#20043;&#38388;&#30340;&#38388;&#38548;&#25345;&#32493;&#26102;&#38388;&#20915;&#23450;&#20102;&#21738;&#20123;&#35805;&#39064;&#26159;&#30456;&#20851;&#30340;&#65292;&#20197;&#21450;&#35201;&#38382;&#21738;&#20123;&#38382;&#39064;&#65292;&#32780;&#19981;&#26126;&#30830;&#27169;&#25311;&#26102;&#38388;&#30340;&#23545;&#35805;&#31995;&#32479;&#21487;&#33021;&#29983;&#25104;&#19981;&#33258;&#28982;&#30340;&#22238;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#23545;&#35805;&#27169;&#22411;&#24847;&#35782;&#21040;&#26102;&#38388;&#30340;&#24819;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;GapChat&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27425;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#23545;&#35805;&#20043;&#38388;&#30340;&#26102;&#38388;&#19981;&#21516;&#12290;&#34429;&#28982;&#25968;&#25454;&#38598;&#26159;&#23454;&#26102;&#26500;&#24314;&#30340;&#65292;&#20294;&#28436;&#35762;&#32773;&#29983;&#27963;&#20013;&#20107;&#20214;&#30340;&#36827;&#23637;&#26159;&#27169;&#25311;&#30340;&#65292;&#20197;&#21019;&#24314;&#21457;&#29983;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#20869;&#30340;&#29616;&#23454;&#23545;&#35805;&#12290;&#25105;&#20204;&#23558;&#26102;&#38388;&#20449;&#24687;&#26292;&#38706;&#32473;&#27169;&#22411;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#30340;&#26102;&#38388;&#21644;&#20107;&#20214;&#36827;&#23637;&#34920;&#31034;&#12290;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26102;&#38388;&#24863;&#30693;&#27169;&#22411;&#22312;&#21028;&#26029;&#36873;&#25321;&#35805;&#39064;&#30340;&#30456;&#20851;&#24615;&#21644;&#20174;&#23545;&#35805;&#20013;&#33719;&#24471;&#30340;&#20449;&#24687;&#30340;&#24230;&#37327;&#26631;&#20934;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowing how to end and resume conversations over time is a natural part of communication, allowing for discussions to span weeks, months, or years. The duration of gaps between conversations dictates which topics are relevant and which questions to ask, and dialogue systems which do not explicitly model time may generate responses that are unnatural. In this work we explore the idea of making dialogue models aware of time, and present GapChat, a multi-session dialogue dataset in which the time between each session varies. While the dataset is constructed in real-time, progress on events in speakers' lives is simulated in order to create realistic dialogues occurring across a long timespan. We expose time information to the model and compare different representations of time and event progress. In human evaluation we show that time-aware models perform better in metrics that judge the relevance of the chosen topics and the information gained from the conversation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26368;&#22823;&#21270;&#33258;&#25105;&#23545;&#24328;&#30340;&#22870;&#21169;&#24182;&#26368;&#23567;&#21270;&#19982;&#20808;&#21069;&#21457;&#29616;&#30340;&#32422;&#23450;&#20132;&#20114;&#26102;&#30340;&#22870;&#21169;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#32422;&#23450;&#65292;&#30830;&#20445;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#20132;&#21449;&#23545;&#24328;&#30340;&#23545;&#25239;&#24615;&#20248;&#21270;&#36807;&#31243;&#20013;&#36981;&#23432;&#21892;&#24847;&#34892;&#20107;</title><link>http://arxiv.org/abs/2310.15414</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#22810;&#26679;&#21270;&#32422;&#23450;
&lt;/p&gt;
&lt;p&gt;
Diverse Conventions for Human-AI Collaboration. (arXiv:2310.15414v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26368;&#22823;&#21270;&#33258;&#25105;&#23545;&#24328;&#30340;&#22870;&#21169;&#24182;&#26368;&#23567;&#21270;&#19982;&#20808;&#21069;&#21457;&#29616;&#30340;&#32422;&#23450;&#20132;&#20114;&#26102;&#30340;&#22870;&#21169;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#32422;&#23450;&#65292;&#30830;&#20445;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#20132;&#21449;&#23545;&#24328;&#30340;&#23545;&#25239;&#24615;&#20248;&#21270;&#36807;&#31243;&#20013;&#36981;&#23432;&#21892;&#24847;&#34892;&#20107;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#20307;&#28216;&#25103;&#20013;&#65292;&#32422;&#23450;&#23545;&#20110;&#24378;&#22823;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#29609;&#23478;&#22312;&#27809;&#26377;&#26126;&#30830;&#20132;&#27969;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20849;&#21516;&#25112;&#30053;&#30340;&#21327;&#35843;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#22914;&#33258;&#25105;&#23545;&#24328;&#65292;&#20250;&#25910;&#25947;&#21040;&#20219;&#24847;&#21644;&#38750;&#22810;&#26679;&#21270;&#30340;&#32422;&#23450;&#65292;&#23548;&#33268;&#22312;&#19982;&#26032;&#30340;&#21512;&#20316;&#20249;&#20276;&#20114;&#21160;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#33258;&#25105;&#23545;&#24328;&#36807;&#31243;&#20013;&#26368;&#22823;&#21270;&#20854;&#22870;&#21169;&#65292;&#24182;&#22312;&#19982;&#20808;&#21069;&#21457;&#29616;&#30340;&#32422;&#23450;&#36827;&#34892;&#20132;&#20114;&#26102;&#26368;&#23567;&#21270;&#20854;&#22870;&#21169;&#65288;&#20132;&#21449;&#23545;&#24328;&#65289;&#65292;&#20197;&#21050;&#28608;&#32422;&#23450;&#22312;&#35821;&#20041;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#32422;&#23450;&#12290;&#20026;&#20102;&#30830;&#20445;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#20132;&#21449;&#23545;&#24328;&#30340;&#23545;&#25239;&#24615;&#20248;&#21270;&#36807;&#31243;&#20013;&#22987;&#32456;&#36981;&#23432;&#21892;&#24847;&#34892;&#20107;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28151;&#21512;&#23545;&#24328;&#65288;mixed-play&#65289;&#30340;&#27010;&#24565;&#65292;&#21363;&#36890;&#36807;&#20174;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#30340;&#36716;&#25442;&#20013;&#38543;&#26426;&#29983;&#25104;&#21021;&#22987;&#29366;&#24577;&#65292;&#24182;&#23398;&#20064;&#22312;&#27492;&#21021;&#22987;&#29366;&#24577;&#19979;&#26368;&#22823;&#21270;&#33258;&#25105;&#23545;&#24328;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Conventions are crucial for strong performance in cooperative multi-agent games, because they allow players to coordinate on a shared strategy without explicit communication. Unfortunately, standard multi-agent reinforcement learning techniques, such as self-play, converge to conventions that are arbitrary and non-diverse, leading to poor generalization when interacting with new partners. In this work, we present a technique for generating diverse conventions by (1) maximizing their rewards during self-play, while (2) minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different. To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce \emph{mixed-play}, where an initial state is randomly generated by sampling self-play and cross-play transitions and the player learns to maximize the self-play reward from this initial state. We analyze the benefits
&lt;/p&gt;</description></item><item><title>DoGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27867;&#21270;&#20272;&#35745;&#30340;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#27599;&#20010;&#39046;&#22495;&#23545;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#37325;&#26032;&#35843;&#25972;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21516;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15393</link><description>&lt;p&gt;
DoGE: &#20351;&#29992;&#27867;&#21270;&#20272;&#35745;&#36827;&#34892;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
DoGE: Domain Reweighting with Generalization Estimation. (arXiv:2310.15393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15393
&lt;/p&gt;
&lt;p&gt;
DoGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27867;&#21270;&#20272;&#35745;&#30340;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#27599;&#20010;&#39046;&#22495;&#23545;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#37325;&#26032;&#35843;&#25972;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21516;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#35821;&#26009;&#24211;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#32452;&#25104;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#20256;&#32479;&#19978;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30001;&#21508;&#31181;&#26469;&#28304;&#39046;&#22495;&#65288;&#22914;CommonCrawl&#12289;Wikipedia&#12289;Github&#31561;&#65289;&#25353;&#29031;&#29305;&#23450;&#30340;&#37319;&#26679;&#27010;&#29575;&#65288;&#39046;&#22495;&#26435;&#37325;&#65289;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#19968;&#31181;&#22522;&#20110;&#26368;&#32456;&#27867;&#21270;&#30446;&#26631;&#20248;&#21270;&#39046;&#22495;&#26435;&#37325;&#30340;&#21407;&#21017;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DOmain reweighting with Generalization Estimation&#65288;DoGE&#65289;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#37325;&#26032;&#35843;&#25972;&#20102;&#27599;&#20010;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#65292;&#26681;&#25454;&#23427;&#23545;&#26368;&#32456;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#36827;&#34892;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#27867;&#21270;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#35757;&#32451;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#33719;&#21462;&#37325;&#26032;&#21152;&#26435;&#30340;&#39046;&#22495;&#26435;&#37325;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#36890;&#36807;&#38236;&#20687;&#19979;&#38477;&#27861;&#26356;&#26032;&#39046;&#22495;&#26435;&#37325;&#20197;&#26368;&#22823;&#21270;&#25972;&#20307;&#30340;&#27867;&#21270;&#22686;&#30410;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#33719;&#24471;&#30340;&#39046;&#22495;&#26435;&#37325;&#26469;&#35757;&#32451;&#19968;&#20010;&#35268;&#27169;&#26356;&#22823;&#30340;&#23436;&#25972;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The coverage and composition of the pretraining data corpus significantly impacts the generalization ability of large language models. Conventionally, the pretraining corpus is composed of various source domains (e.g. CommonCrawl, Wikipedia, Github etc.) according to certain sampling probabilities (domain weights). However, current methods lack a principled way to optimize domain weights for ultimate goal for generalization. We propose DOmain reweighting with Generalization Estimation (DoGE), where we reweigh the sampling probability from each domain based on its contribution to the final generalization objective assessed by a gradient-based generalization estimation function. First, we train a small-scale proxy model with a min-max optimization to obtain the reweighted domain weights. At each step, the domain weights are updated to maximize the overall generalization gain by mirror descent. Finally we use the obtained domain weights to train a larger scale full-size language model. On
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21487;&#32422;&#35838;&#31243;&#31639;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#26356;&#39640;&#23398;&#20064;&#33021;&#21147;&#30340;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#23567;&#35268;&#27169;&#20195;&#29702;&#27169;&#22411;&#27169;&#25311;&#26679;&#26412;&#20002;&#22833;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#31639;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15389</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#19981;&#21487;&#32422;&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Irreducible Curriculum for Language Model Pretraining. (arXiv:2310.15389v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21487;&#32422;&#35838;&#31243;&#31639;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#26356;&#39640;&#23398;&#20064;&#33021;&#21147;&#30340;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#23567;&#35268;&#27169;&#20195;&#29702;&#27169;&#22411;&#27169;&#25311;&#26679;&#26412;&#20002;&#22833;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#31639;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25968;&#25454;&#36873;&#25321;&#21644;&#35838;&#31243;&#35774;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21482;&#26377;&#23569;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#35757;&#32451;&#19978;&#26174;&#31034;&#20986;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#26041;&#26696;&#26356;&#20851;&#27880;&#39046;&#22495;&#32423;&#21035;&#30340;&#36873;&#25321;&#65292;&#24573;&#35270;&#20102;&#27599;&#20010;&#21333;&#29420;&#35757;&#32451;&#28857;&#30340;&#26356;&#32454;&#31890;&#24230;&#30340;&#36129;&#29486;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#24212;&#29992;&#20256;&#32479;&#30340;&#25968;&#25454;&#28857;&#36873;&#25321;&#26041;&#27861;&#24456;&#22256;&#38590;&#65306;&#22823;&#22810;&#25968;&#22312;&#32447;&#25209;&#36873;&#25321;&#26041;&#27861;&#25191;&#34892;&#20004;&#27425;&#21069;&#21521;&#25110;&#21518;&#21521;&#20256;&#36882;&#65292;&#36825;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#39069;&#22806;&#25104;&#26412;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21487;&#32422;&#35838;&#31243;&#20316;&#20026;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#35838;&#31243;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#26356;&#39640;&#23398;&#20064;&#33021;&#21147;&#30340;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#36991;&#20813;&#36807;&#39640;&#30340;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#20351;&#29992;&#23567;&#35268;&#27169;&#20195;&#29702;&#27169;&#22411;&#27169;&#25311;&#26679;&#26412;&#20002;&#22833;&#27839;&#20027;&#27169;&#22411;&#35757;&#32451;&#36712;&#36857;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#22312;RedPajama-1B&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35838;&#31243;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#27169;&#22411;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic data selection and curriculum design for training large language models is challenging, with only a few existing methods showing improvements over standard training. Furthermore, current schemes focus on domain-level selection, overlooking the more fine-grained contributions of each individual training point. It is difficult to apply traditional datapoint selection methods on large language models: most online batch selection methods perform two-times forward or backward passes, which introduces considerable extra costs with large-scale models. To mitigate these obstacles, we propose irreducible curriculum as a curriculum learning algorithm for language model pretraining, which prioritizes samples with higher learnability. Specifically, to avoid prohibitive extra computation overhead, we simulate the sample loss along the main model's training trajectory using a small-scale proxy model. Our experiments on the RedPajama-1B dataset demonstrate a consistent improvement on valida
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20462;&#27491;&#20102;Koopman&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21608;&#26399;&#37325;&#26032;&#32534;&#30721;&#8221;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.15386</link><description>&lt;p&gt;
&#20462;&#27491;Koopman&#34920;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Course Correcting Koopman Representations. (arXiv:2310.15386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20462;&#27491;&#20102;Koopman&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21608;&#26399;&#37325;&#26032;&#32534;&#30721;&#8221;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20934;&#30830;&#25429;&#25417;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#34920;&#31034;&#26088;&#22312;&#23398;&#20064;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#20013;&#23548;&#33268;&#28508;&#22312;&#31354;&#38388;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#31616;&#21270;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#24314;&#27169;&#21644;&#25511;&#21046;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27492;&#38382;&#39064;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#24314;&#27169;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#19981;&#21516;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21608;&#26399;&#37325;&#26032;&#32534;&#30721;&#8221;&#30340;&#25512;&#29702;&#26102;&#38388;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#38271;&#26399;&#21160;&#24577;&#30340;&#20934;&#30830;&#25429;&#25417;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Koopman representations aim to learn features of nonlinear dynamical systems (NLDS) which lead to linear dynamics in the latent space. Theoretically, such features can be used to simplify many problems in modeling and control of NLDS. In this work we study autoencoder formulations of this problem, and different ways they can be used to model dynamics, specifically for future state prediction over long horizons. We discover several limitations of predicting future states in the latent space and propose an inference-time mechanism, which we refer to as Periodic Reencoding, for faithfully capturing long term dynamics. We justify this method both analytically and empirically via experiments in low and high dimensional NLDS.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#28246;&#26159;&#31649;&#29702;&#22823;&#37327;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#20026;&#20102;&#38450;&#27490;&#25968;&#25454;&#28246;&#25104;&#20026;&#26080;&#27861;&#25805;&#20316;&#30340;&#25968;&#25454;&#27836;&#27901;&#65292;&#25105;&#20204;&#21487;&#20197;&#37319;&#29992;&#35821;&#20041;&#25968;&#25454;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20803;&#25968;&#25454;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#38142;&#25509;&#65292;&#20026;&#25968;&#25454;&#25552;&#20379;&#26356;&#22810;&#30340;&#24847;&#20041;&#21644;&#35821;&#20041;&#12290;&#36825;&#31181;&#35821;&#20041;&#23618;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#25968;&#25454;&#31649;&#29702;&#65292;&#36824;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#25972;&#21512;&#38382;&#39064;&#65292;&#20351;&#25968;&#25454;&#35775;&#38382;&#26356;&#20855;&#34920;&#36798;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15373</link><description>&lt;p&gt;
&#25968;&#25454;&#28246;&#20013;&#30340;&#35821;&#20041;&#25968;&#25454;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Semantic Data Management in Data Lakes. (arXiv:2310.15373v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15373
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#28246;&#26159;&#31649;&#29702;&#22823;&#37327;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#20026;&#20102;&#38450;&#27490;&#25968;&#25454;&#28246;&#25104;&#20026;&#26080;&#27861;&#25805;&#20316;&#30340;&#25968;&#25454;&#27836;&#27901;&#65292;&#25105;&#20204;&#21487;&#20197;&#37319;&#29992;&#35821;&#20041;&#25968;&#25454;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20803;&#25968;&#25454;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#38142;&#25509;&#65292;&#20026;&#25968;&#25454;&#25552;&#20379;&#26356;&#22810;&#30340;&#24847;&#20041;&#21644;&#35821;&#20041;&#12290;&#36825;&#31181;&#35821;&#20041;&#23618;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#25968;&#25454;&#31649;&#29702;&#65292;&#36824;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#25972;&#21512;&#38382;&#39064;&#65292;&#20351;&#25968;&#25454;&#35775;&#38382;&#26356;&#20855;&#34920;&#36798;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#28246;&#20316;&#20026;&#31649;&#29702;&#22823;&#37327;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#30340;&#19968;&#31181;&#26041;&#27861;&#20986;&#29616;&#12290;&#38450;&#27490;&#25968;&#25454;&#28246;&#21464;&#25104;&#26080;&#27861;&#25805;&#20316;&#30340;&#25968;&#25454;&#27836;&#27901;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#35821;&#20041;&#25968;&#25454;&#31649;&#29702;&#12290;&#19968;&#20123;&#26041;&#27861;&#25552;&#35758;&#22522;&#20110;&#38142;&#25509;&#25968;&#25454;&#21407;&#21017;&#23558;&#20803;&#25968;&#25454;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#38142;&#25509;&#65292;&#20197;&#20026;&#28246;&#20013;&#30340;&#25968;&#25454;&#25552;&#20379;&#26356;&#22810;&#30340;&#24847;&#20041;&#21644;&#35821;&#20041;&#12290;&#36825;&#26679;&#30340;&#35821;&#20041;&#23618;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#25968;&#25454;&#31649;&#29702;&#65292;&#36824;&#21487;&#20197;&#35299;&#20915;&#26469;&#33258;&#24322;&#26500;&#26469;&#28304;&#30340;&#25968;&#25454;&#25972;&#21512;&#38382;&#39064;&#65292;&#20197;&#20351;&#25968;&#25454;&#35775;&#38382;&#26356;&#20855;&#34920;&#36798;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#22312;&#25968;&#25454;&#28246;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#21644;&#22823;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22238;&#39038;&#20102;&#26368;&#36817;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65306;(i)&#22522;&#26412;&#30340;&#35821;&#20041;&#25968;&#25454;&#31649;&#29702;&#65292;(ii)&#22312;&#25968;&#25454;&#28246;&#20013;&#20016;&#23500;&#20803;&#25968;&#25454;&#30340;&#35821;&#20041;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#21450;(iii)&#22522;&#20110;&#26412;&#20307;&#30340;&#25968;&#25454;&#35775;&#38382;&#26041;&#27861;&#12290;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;&#20027;&#35201;&#25216;&#26415;&#21450;&#20854;&#32972;&#26223;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, data lakes emerged as away to manage large amounts of heterogeneous data for modern data analytics. One way to prevent data lakes from turning into inoperable data swamps is semantic data management. Some approaches propose the linkage of metadata to knowledge graphs based on the Linked Data principles to provide more meaning and semantics to the data in the lake. Such a semantic layer may be utilized not only for data management but also to tackle the problem of data integration from heterogeneous sources, in order to make data access more expressive and interoperable. In this survey, we review recent approaches with a specific focus on the application within data lake systems and scalability to Big Data. We classify the approaches into (i) basic semantic data management, (ii) semantic modeling approaches for enriching metadata in data lakes, and (iii) methods for ontologybased data access. In each category, we cover the main techniques and their background, and compa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;EpiK-Eval&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#20998;&#21106;&#30340;&#21465;&#36848;&#20013;&#26500;&#24314;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#35757;&#32451;&#30446;&#26631;&#23384;&#22312;&#22266;&#26377;&#30340;&#32570;&#38519;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#25913;&#36827;&#30693;&#35782;&#25972;&#21512;&#26041;&#27861;&#30340;&#24314;&#35758;&#65292;&#20197;&#22823;&#24133;&#25552;&#39640;LLMs&#30340;&#25972;&#20307;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15372</link><description>&lt;p&gt;
EpiK-Eval&#65306;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#35782;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
EpiK-Eval: Evaluation for Language Models as Epistemic Models. (arXiv:2310.15372v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15372
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;EpiK-Eval&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#20998;&#21106;&#30340;&#21465;&#36848;&#20013;&#26500;&#24314;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#35757;&#32451;&#30446;&#26631;&#23384;&#22312;&#22266;&#26377;&#30340;&#32570;&#38519;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#25913;&#36827;&#30693;&#35782;&#25972;&#21512;&#26041;&#27861;&#30340;&#24314;&#35758;&#65292;&#20197;&#22823;&#24133;&#25552;&#39640;LLMs&#30340;&#25972;&#20307;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20316;&#29992;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#26085;&#30410;&#26222;&#21450;&#65292;&#20294;&#23427;&#20204;&#22312;&#20174;&#19981;&#21516;&#35757;&#32451;&#25991;&#26723;&#20013;&#25972;&#21512;&#30693;&#35782;&#30340;&#33021;&#21147;&#8212;&#8212;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#26159;&#20851;&#38190;&#33021;&#21147;&#8212;&#8212;&#20173;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;LLMs&#22312;&#20854;&#21442;&#25968;&#31354;&#38388;&#20869;&#26377;&#25928;&#22320;&#32467;&#21512;&#36825;&#31181;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;EpiK-Eval&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#20174;&#20998;&#21106;&#30340;&#21465;&#36848;&#20013;&#26500;&#24314;&#19968;&#31181;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23545;&#21508;&#31181;LLMs&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#23384;&#22312;&#30340;&#26174;&#33879;&#24369;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#32570;&#28857;&#28304;&#20110;&#29616;&#26377;&#35757;&#32451;&#30446;&#26631;&#30340;&#22266;&#26377;&#24615;&#36136;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#25913;&#36827;&#30693;&#35782;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#36825;&#26377;&#28508;&#21147;&#26174;&#33879;&#25552;&#39640;LLMs&#30340;&#25972;&#20307;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents - a crucial ability in numerous applications - remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs' proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#36817;&#29305;&#24449;&#32479;&#35745;&#22686;&#24378;&#30340;&#32852;&#37030;&#19977;&#32500;&#21307;&#23398;&#20307;&#31215;&#20998;&#21106;&#26041;&#26696;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;FL&#20998;&#21106;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#21644;&#25968;&#25454;&#20998;&#24067;&#24322;&#26500;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15371</link><description>&lt;p&gt;
&#22522;&#20110;&#37051;&#36817;&#29305;&#24449;&#32479;&#35745;&#22686;&#24378;&#30340;&#32852;&#37030;&#19977;&#32500;&#21307;&#23398;&#20307;&#31215;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Vicinal Feature Statistics Augmentation for Federated 3D Medical Volume Segmentation. (arXiv:2310.15371v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#36817;&#29305;&#24449;&#32479;&#35745;&#22686;&#24378;&#30340;&#32852;&#37030;&#19977;&#32500;&#21307;&#23398;&#20307;&#31215;&#20998;&#21106;&#26041;&#26696;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;FL&#20998;&#21106;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#21644;&#25968;&#25454;&#20998;&#24067;&#24322;&#26500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#22810;&#20010;&#21307;&#30103;&#26426;&#26500;&#33021;&#22815;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#21069;&#25552;&#19979;&#65292;&#20849;&#21516;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23567;&#22411;&#26426;&#26500;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#24322;&#26500;&#30340;&#25968;&#25454;&#20998;&#24067;&#65288;&#21363;&#38750;i.i.d.&#65289;&#21487;&#33021;&#38480;&#21046;FL&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#25968;&#25454;&#22686;&#24378;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#20256;&#32479;&#38598;&#20013;&#24335;DL&#27867;&#21270;&#33021;&#21147;&#30340;&#26377;&#25928;&#25216;&#26415;&#65292;&#20294;&#20854;&#22312;FL&#20013;&#30340;&#24212;&#29992;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21463;&#21040;&#26114;&#36149;&#30340;&#26631;&#27880;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;3D&#21307;&#23398;&#20998;&#21106;&#36890;&#24120;&#20381;&#36182;&#20110;&#25968;&#25454;&#22686;&#24378;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#37051;&#36817;&#29305;&#24449;&#23618;&#32423;&#25968;&#25454;&#22686;&#24378;&#65288;VFDA&#65289;&#26041;&#26696;&#65292;&#20197;&#26377;&#25928;&#20943;&#36731;&#23616;&#37096;&#29305;&#24449;&#28418;&#31227;&#65292;&#24182;&#20419;&#36827;&#38544;&#31169;&#24863;&#30693;&#30340;FL&#20998;&#21106;&#30340;&#21327;&#21516;&#35757;&#32451;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20869;&#37096;&#21644;&#36328;&#26426;&#26500;&#30340;&#24046;&#24322;&#65292;&#32780;&#26080;&#38656;&#36328;&#26426;&#26500;&#36716;&#31227;&#21407;&#22987;&#25968;&#25454;&#25110;&#28151;&#21512;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple client medical institutes collaboratively train a deep learning (DL) model with privacy protection. However, the performance of FL can be constrained by the limited availability of labeled data in small institutes and the heterogeneous (i.e., non-i.i.d.) data distribution across institutes. Though data augmentation has been a proven technique to boost the generalization capabilities of conventional centralized DL as a "free lunch", its application in FL is largely underexplored. Notably, constrained by costly labeling, 3D medical segmentation generally relies on data augmentation. In this work, we aim to develop a vicinal feature-level data augmentation (VFDA) scheme to efficiently alleviate the local feature shift and facilitate collaborative training for privacy-aware FL segmentation. We take both the inner- and inter-institute divergence into consideration, without the need for cross-institute transfer of raw data or their mixup. Specifically
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;LLMs&#20135;&#29983;&#24187;&#35273;&#30340;&#21407;&#22240;&#26159;&#22240;&#20026;&#23427;&#20204;&#30340;&#36755;&#20986;&#27809;&#26377;&#21463;&#21040;&#35777;&#25454;&#25903;&#25345;&#30340;&#20027;&#24352;&#30340;&#32422;&#26463;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#24863;&#30693;&#12289;&#20869;&#28085;&#21644;&#22806;&#24310;&#23398;&#20064;&#26469;&#32422;&#26463;LLMs&#20197;&#29983;&#25104;&#28385;&#36275;&#35777;&#25454;&#38381;&#21512;&#24615;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2310.15355</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;LLM&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#20197;&#21450;&#22914;&#20309;&#33719;&#24471;&#65288;&#35777;&#25454;&#24615;&#30340;&#65289;&#38381;&#21512;&#24615;&#65306;&#29992;&#20110;&#24544;&#23454;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#24863;&#30693;&#12289;&#20869;&#28085;&#21644;&#22806;&#24310;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation. (arXiv:2310.15355v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;LLMs&#20135;&#29983;&#24187;&#35273;&#30340;&#21407;&#22240;&#26159;&#22240;&#20026;&#23427;&#20204;&#30340;&#36755;&#20986;&#27809;&#26377;&#21463;&#21040;&#35777;&#25454;&#25903;&#25345;&#30340;&#20027;&#24352;&#30340;&#32422;&#26463;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#24863;&#30693;&#12289;&#20869;&#28085;&#21644;&#22806;&#24310;&#23398;&#20064;&#26469;&#32422;&#26463;LLMs&#20197;&#29983;&#25104;&#28385;&#36275;&#35777;&#25454;&#38381;&#21512;&#24615;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#20026;&#20160;&#20040;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#36755;&#20986;&#27809;&#26377;&#21463;&#21040;&#20855;&#22791;&#35777;&#25454;&#25903;&#25345;&#30340;&#20027;&#24352;&#30340;&#32422;&#26463;&#65292;&#36825;&#31181;&#26465;&#20214;&#34987;&#31216;&#20026;&#35777;&#25454;&#38381;&#21512;&#12290;&#22312;&#26631;&#20934;&#30340;&#31070;&#32463;&#27010;&#29575;&#35821;&#35328;&#27169;&#22411;&#35774;&#32622;&#20013;&#65292;&#24182;&#19981;&#33021;&#20174;&#32479;&#35745;&#19978;&#36776;&#21035;&#20986;&#20851;&#20110;&#21477;&#23376;&#30495;&#20266;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#19981;&#33021;&#20197;&#27492;&#20026;&#26465;&#20214;&#29983;&#25104;&#26032;&#30340;&#23383;&#31526;&#20018;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#32422;&#26463;LLMs&#20197;&#20135;&#29983;&#28385;&#36275;&#35777;&#25454;&#38381;&#21512;&#24615;&#30340;&#36755;&#20986;&#12290;&#22810;&#27169;&#24577;LLM&#24517;&#39035;&#23398;&#20064;&#22806;&#37096;&#19990;&#30028;&#65288;&#24863;&#30693;&#23398;&#20064;&#65289;&#65307;&#23427;&#24517;&#39035;&#23398;&#20064;&#20174;&#23383;&#31526;&#20018;&#21040;&#19990;&#30028;&#29366;&#24577;&#30340;&#26144;&#23556;&#65288;&#22806;&#24310;&#23398;&#20064;&#65289;&#65307;&#24182;&#19988;&#65292;&#20026;&#20102;&#22312;&#36229;&#36234;&#19968;&#32452;&#35777;&#25454;&#26102;&#23454;&#29616;&#27969;&#30021;&#24615;&#65292;&#23427;&#24517;&#39035;&#23398;&#20064;&#20174;&#23383;&#31526;&#20018;&#21040;&#23427;&#20204;&#30340;&#21516;&#20041;&#35789;&#30340;&#26144;&#23556;&#65288;&#20869;&#28085;&#23398;&#20064;&#65289;&#12290;&#19968;&#31181;&#21333;&#27169;&#24577;LLM&#30340;&#36755;&#20986;&#24517;&#39035;&#19982;&#39564;&#35777;&#30340;&#35777;&#25454;&#38598;&#20013;&#30340;&#23383;&#31526;&#20018;&#24847;&#20041;&#30456;&#21516;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#36807;&#31243;&#8212;&#8212;&#23398;&#20064;-&#32993;&#35328;&#20081;&#35821;-&#20462;&#21098;&#65288;Learn-Babble-Prune&#65289;&#65292;&#36890;&#36807;&#25298;&#32477;&#19981;&#21516;&#20041;&#30340;&#36755;&#20986;&#65292;&#20174;LLM&#20013;&#20135;&#29983;&#24544;&#23454;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure. Information about the truth or falsity of sentences is not statistically identified in the standard neural probabilistic language model setup, and so cannot be conditioned on to generate new strings. We then show how to constrain LLMs to produce output that does satisfy evidential closure. A multimodal LLM must learn about the external world (perceptual learning); it must learn a mapping from strings to states of the world (extensional learning); and, to achieve fluency when generalizing beyond a body of evidence, it must learn mappings from strings to their synonyms (intensional learning). The output of a unimodal LLM must be synonymous with strings in a validated evidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune, that yields faithful output from an LLM by rejecting output that is not syn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#36947;&#24503;&#22522;&#30784;&#29702;&#35770;&#65288;MFT&#65289;&#20316;&#20026;&#20998;&#26512;&#24037;&#20855;&#65292;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#23545;&#19968;&#31995;&#21015;&#29305;&#23450;&#30340;&#36947;&#24503;&#20215;&#20540;&#35266;&#20135;&#29983;&#20102;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#36947;&#24503;&#22522;&#30784;&#21644;&#25919;&#27835;&#20542;&#21521;&#30340;&#20851;&#32852;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;LLMs&#30340;&#20559;&#35265;&#22312;&#19981;&#21516;&#30340;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#25239;&#36873;&#25321;&#25552;&#31034;&#21487;&#20197;&#24341;&#23548;LLMs&#20135;&#29983;&#19981;&#21516;&#30340;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2310.15337</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Moral Foundations of Large Language Models. (arXiv:2310.15337v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#36947;&#24503;&#22522;&#30784;&#29702;&#35770;&#65288;MFT&#65289;&#20316;&#20026;&#20998;&#26512;&#24037;&#20855;&#65292;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#23545;&#19968;&#31995;&#21015;&#29305;&#23450;&#30340;&#36947;&#24503;&#20215;&#20540;&#35266;&#20135;&#29983;&#20102;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#36947;&#24503;&#22522;&#30784;&#21644;&#25919;&#27835;&#20542;&#21521;&#30340;&#20851;&#32852;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;LLMs&#30340;&#20559;&#35265;&#22312;&#19981;&#21516;&#30340;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#25239;&#36873;&#25321;&#25552;&#31034;&#21487;&#20197;&#24341;&#23548;LLMs&#20135;&#29983;&#19981;&#21516;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#24503;&#22522;&#30784;&#29702;&#35770;&#65288;MFT&#65289;&#26159;&#19968;&#31181;&#24515;&#29702;&#35780;&#20272;&#24037;&#20855;&#65292;&#23558;&#20154;&#31867;&#36947;&#24503;&#25512;&#29702;&#20998;&#35299;&#20026;&#21253;&#25324;&#20851;&#24515;/&#20260;&#23475;&#12289;&#33258;&#30001;/&#21387;&#36843;&#21644;&#23562;&#20005;/&#22549;&#33853;&#31561;&#20116;&#20010;&#22240;&#32032;&#65288;Graham&#31561;&#65292;2009&#65289;&#12290;&#20154;&#20204;&#22312;&#20316;&#20986;&#36947;&#24503;&#20915;&#31574;&#26102;&#22312;&#36825;&#20123;&#32500;&#24230;&#19978;&#30340;&#26435;&#37325;&#19981;&#21516;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#20182;&#20204;&#30340;&#25991;&#21270;&#32972;&#26223;&#21644;&#25919;&#27835;&#24847;&#35782;&#24418;&#24577;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#20114;&#32852;&#32593;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#20182;&#20204;&#21487;&#33021;&#21453;&#26144;&#20102;&#36825;&#20123;&#35821;&#26009;&#24211;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#20197;MFT&#20026;&#35270;&#35282;&#65292;&#20998;&#26512;&#27969;&#34892;&#30340;LLMs&#26159;&#21542;&#23545;&#19968;&#31995;&#21015;&#29305;&#23450;&#30340;&#36947;&#24503;&#20215;&#20540;&#35266;&#20135;&#29983;&#20102;&#20559;&#35265;&#12290;&#25105;&#20204;&#20998;&#26512;&#24050;&#30693;&#30340;LLMs&#65292;&#21457;&#29616;&#23427;&#20204;&#23637;&#29616;&#20102;&#29305;&#23450;&#30340;&#36947;&#24503;&#22522;&#30784;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#36947;&#24503;&#22522;&#30784;&#21644;&#25919;&#27835;&#20542;&#21521;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#36825;&#20123;&#20559;&#35265;&#30340;&#19968;&#33268;&#24615;&#65292;&#25110;&#32773;&#23427;&#20204;&#22312;&#27169;&#22411;&#34987;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#20013;&#26159;&#21542;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#22320;&#36873;&#25321;&#25552;&#31034;&#26469;&#40723;&#21169;LLMs&#20135;&#29983;&#19981;&#21516;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moral foundations theory (MFT) is a psychological assessment tool that decomposes human moral reasoning into five factors, including care/harm, liberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary in the weight they place on these dimensions when making moral decisions, in part due to their cultural upbringing and political ideology. As large language models (LLMs) are trained on datasets collected from the internet, they may reflect the biases that are present in such corpora. This paper uses MFT as a lens to analyze whether popular LLMs have acquired a bias towards a particular set of moral values. We analyze known LLMs and find they exhibit particular moral foundations, and show how these relate to human moral foundations and political affiliations. We also measure the consistency of these biases, or whether they vary strongly depending on the context of how the model is prompted. Finally, we show that we can adversarially select prompts that encourage the
&lt;/p&gt;</description></item><item><title>flwr-serverless&#26159;&#19968;&#31181;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#23433;&#20840;&#21644;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.15329</link><description>&lt;p&gt;
&#20351;&#29992;flwr-serverless&#30340;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Serverless Federated Learning with flwr-serverless. (arXiv:2310.15329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15329
&lt;/p&gt;
&lt;p&gt;
flwr-serverless&#26159;&#19968;&#31181;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#23433;&#20840;&#21644;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#25910;&#38598;&#21644;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#23384;&#20648;&#37327;&#30340;&#28608;&#22686;&#65292;&#32852;&#37030;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#21644;&#21463;&#27426;&#36814;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19990;&#30028;&#21508;&#22269;&#25552;&#20986;&#20102;&#35768;&#22810;&#20851;&#20110;&#20026;&#20010;&#20154;&#25968;&#25454;&#25552;&#20379;&#26356;&#22810;&#20445;&#25252;&#20197;&#21450;&#23545;&#25968;&#25454;&#38544;&#31169;&#25514;&#26045;&#30340;&#24378;&#21270;&#20852;&#36259;&#30340;&#24314;&#35758;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#26032;&#30340;&#21644;&#29616;&#26377;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#24320;&#21457;&#20687;&#32852;&#37030;&#23398;&#20064;&#36825;&#26679;&#33021;&#26377;&#25928;&#35757;&#32451;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#65288;&#22914;&#36793;&#32536;&#35774;&#22791;&#65289;&#30340;&#25968;&#25454;&#12289;&#21516;&#26102;&#19981;&#25439;&#23475;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#31574;&#30053;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#21517;&#20026;Flower&#65288;Flwr&#65289;&#30340;Python&#21253;&#65292;&#20026;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#12289;&#28789;&#27963;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;Flower&#21482;&#33021;&#36816;&#34892;&#21516;&#27493;&#32852;&#37030;&#23398;&#20064;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#36816;&#34892;&#25104;&#26412;&#39640;&#12289;&#32791;&#26102;&#38271;&#65292;&#22240;&#20026;&#36807;&#31243;&#21463;&#38480;&#20110;&#24930;&#36895;&#25110;&#19981;&#31283;&#23450;&#30340;&#23458;&#25143;&#31471;&#35757;&#32451;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is becoming increasingly relevant and popular as we witness a surge in data collection and storage of personally identifiable information. Alongside these developments there have been many proposals from governments around the world to provide more protections for individuals' data and a heightened interest in data privacy measures. As deep learning continues to become more relevant in new and existing domains, it is vital to develop strategies like federated learning that can effectively train data from different sources, such as edge devices, without compromising security and privacy. Recently, the Flower (\texttt{Flwr}) Python package was introduced to provide a scalable, flexible, and easy-to-use framework for implementing federated learning. However, to date, Flower is only able to run synchronous federated learning which can be costly and time-consuming to run because the process is bottlenecked by client-side training jobs that are slow or fragile. Here, we in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#25351;&#23548;&#20154;&#31867;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#35828;&#26126;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#24187;&#35273;&#21442;&#32771;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.15319</link><description>&lt;p&gt;
&#20026;&#22522;&#20110;&#25351;&#23548;&#24615;&#35828;&#26126;&#29983;&#25104;&#30340;&#24187;&#35273;&#26816;&#27979;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Hallucination Detection for Grounded Instruction Generation. (arXiv:2310.15319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15319
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#25351;&#23548;&#20154;&#31867;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#35828;&#26126;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#24187;&#35273;&#21442;&#32771;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27169;&#25311;&#30340;&#20303;&#23429;&#29615;&#22659;&#20013;&#29983;&#25104;&#25351;&#23548;&#20154;&#31867;&#23548;&#33322;&#30340;&#35828;&#26126;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#27169;&#22411;&#23384;&#22312;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#24187;&#35273;&#65306;&#23427;&#20204;&#29983;&#25104;&#19982;&#20154;&#31867;&#36319;&#38543;&#32773;&#22312;&#25551;&#36848;&#30340;&#36335;&#24452;&#19978;&#25191;&#34892;&#25110;&#36935;&#21040;&#30340;&#34892;&#20026;&#25110;&#29289;&#20307;&#19981;&#19968;&#33268;&#30340;&#21442;&#32771;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#22312;&#22823;&#22411;&#22270;&#20687;-&#25991;&#26412;&#23545;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#65292;&#26816;&#27979;&#36825;&#20123;&#24187;&#35273;&#21442;&#32771;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#32988;&#36807;&#20102;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;&#21253;&#25324;&#20351;&#29992;&#30001;&#35828;&#26126;&#29983;&#25104;&#27169;&#22411;&#20272;&#35745;&#30340;&#35789;&#27010;&#29575;&#20197;&#21450;&#22522;&#20110;LSTM&#21644;Transformer&#30340;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of generating instructions to guide humans to navigate in simulated residential environments. A major issue with current models is hallucination: they generate references to actions or objects that are inconsistent with what a human follower would perform or encounter along the described path. We develop a model that detects these hallucinated references by adopting a model pre-trained on a large corpus of image-text pairs, and fine-tuning it with a contrastive loss that separates correct instructions from instructions containing synthesized hallucinations. Our final model outperforms several baselines, including using word probability estimated by the instruction-generation model, and supervised models based on LSTM and Transformer.
&lt;/p&gt;</description></item><item><title>HetGPT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15318</link><description>&lt;p&gt;
HetGPT: &#21033;&#29992;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25552;&#31034;&#35843;&#25972;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks. (arXiv:2310.15318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15318
&lt;/p&gt;
&lt;p&gt;
HetGPT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#29616;&#20026;&#34920;&#31034;&#21644;&#20998;&#26512;Web&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20016;&#23500;&#20449;&#24687;&#30340;&#33258;&#28982;&#36873;&#25321;&#65292;&#20351;&#24471;&#22312;&#32447;&#39029;&#38754;&#20998;&#31867;&#21644;&#31038;&#20132;&#25512;&#33616;&#31561;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#8220;&#39044;&#35757;&#32451;&#65292;&#24494;&#35843;&#8221;&#33539;&#24335;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#24448;&#24448;&#23384;&#22312;&#39044;&#35757;&#32451;&#30446;&#26631;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36825;&#31181;&#24046;&#36317;&#21487;&#33021;&#23548;&#33268;&#8220;&#36127;&#36716;&#31227;&#8221;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;&#20852;&#36215;&#34920;&#26126;&#20102;&#23558;&#8220;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#8221;&#33539;&#24335;&#24212;&#29992;&#20110;&#22270;&#24418;&#30340;&#28508;&#21147;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#24418;&#25552;&#31034;&#25216;&#26415;&#38024;&#23545;&#30340;&#26159;&#21516;&#36136;&#22270;&#65292;&#24573;&#35270;&#20102;Web&#22270;&#30340;&#20869;&#22312;&#24322;&#26500;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HetGPT&#65292;
&lt;/p&gt;
&lt;p&gt;
Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "pre-train, fine-tune" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pretext tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "pre-train, prompt" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#32445;&#32422;&#24066;Airbnb&#25151;&#28304;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25209;&#35780;&#22320;&#21517;&#23398;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#19982;&#22320;&#28857;&#29305;&#24449;&#21270;&#30456;&#20851;&#30340;&#37325;&#35201;&#35805;&#35821;&#31867;&#21035;&#65292;&#20026;&#25209;&#35780;&#22320;&#21517;&#23398;&#30740;&#31350;&#25351;&#26126;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.15302</link><description>&lt;p&gt;
&#22522;&#20110;&#25209;&#35780;&#22320;&#21517;&#23398;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26694;&#26550;&#65306;&#20197;&#32445;&#32422;&#24066;Airbnb&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Toward a Critical Toponymy Framework for Named Entity Recognition: A Case Study of Airbnb in New York City. (arXiv:2310.15302v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15302
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#32445;&#32422;&#24066;Airbnb&#25151;&#28304;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25209;&#35780;&#22320;&#21517;&#23398;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#19982;&#22320;&#28857;&#29305;&#24449;&#21270;&#30456;&#20851;&#30340;&#37325;&#35201;&#35805;&#35821;&#31867;&#21035;&#65292;&#20026;&#25209;&#35780;&#22320;&#21517;&#23398;&#30740;&#31350;&#25351;&#26126;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#35780;&#22320;&#21517;&#23398;&#36890;&#36807;&#22320;&#21517;&#21644;&#30456;&#20851;&#22320;&#28857;&#30740;&#31350;&#26435;&#21147;&#12289;&#36164;&#26412;&#21644;&#25269;&#25239;&#30340;&#21160;&#24577;&#12290;&#20256;&#32479;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22320;&#21517;&#30340;&#35821;&#20041;&#20869;&#23481;&#21644;&#33258;&#19978;&#32780;&#19979;&#30340;&#21046;&#24230;&#36807;&#31243;&#65292;&#20294;&#36890;&#24120;&#24573;&#35270;&#20102;&#26222;&#36890;&#20154;&#22312;&#26085;&#24120;&#35805;&#35821;&#20013;&#20351;&#29992;&#22320;&#21517;&#30340;&#26041;&#24335;&#65292;&#20197;&#21450;&#19982;&#22320;&#21517;&#30456;&#20851;&#30340;&#22320;&#29702;&#31354;&#38388;&#25551;&#36848;&#31574;&#30053;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;2010&#24180;&#20195;&#30340;47,440&#20010;&#32445;&#32422;&#24066;Airbnb&#25151;&#28304;&#36827;&#34892;&#21019;&#26032;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#26500;&#24314;&#65292;&#24320;&#21457;&#35745;&#31639;&#26041;&#27861;&#20197;&#34913;&#37327;&#25991;&#21270;&#21644;&#32463;&#27982;&#36164;&#26412;&#22914;&#20309;&#24433;&#21709;&#20154;&#20204;&#23545;&#22320;&#28857;&#30340;&#31216;&#35859;&#26041;&#24335;&#12290;&#22522;&#20110;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#19982;&#22320;&#28857;&#29305;&#24449;&#21270;&#30456;&#20851;&#30340;&#37325;&#35201;&#35805;&#35821;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25351;&#21521;&#25209;&#35780;&#22320;&#21517;&#23398;&#30340;&#26032;&#26041;&#21521;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#20197;&#21069;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#35821;&#35328;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critical toponymy examines the dynamics of power, capital, and resistance through place names and the sites to which they refer. Studies here have traditionally focused on the semantic content of toponyms and the top-down institutional processes that produce them. However, they have generally ignored the ways in which toponyms are used by ordinary people in everyday discourse, as well as the other strategies of geospatial description that accompany and contextualize toponymic reference. Here, we develop computational methods to measure how cultural and economic capital shape the ways in which people refer to places, through a novel annotated dataset of 47,440 New York City Airbnb listings from the 2010s. Building on this dataset, we introduce a new named entity recognition (NER) model able to identify important discourse categories integral to the characterization of place. Our findings point toward new directions for critical toponymy and to a range of previously understudied linguist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#23616;&#37096;&#25910;&#25947;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NNLCI&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#39640;&#20445;&#30495;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#22312;&#26080;&#31896;&#36229;&#38899;&#36895;&#27969;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15299</link><description>&lt;p&gt;
&#20855;&#26377;&#23616;&#37096;&#25910;&#25947;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NNLCI&#65289;&#29992;&#20110;&#20855;&#26377;&#38750;&#32467;&#26500;&#21270;&#32593;&#26684;&#30340;&#36229;&#38899;&#36895;&#27969;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Neural Network with Local Converging Input (NNLCI) for Supersonic Flow Problems with Unstructured Grids. (arXiv:2310.15299v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#23616;&#37096;&#25910;&#25947;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NNLCI&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#39640;&#20445;&#30495;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#22312;&#26080;&#31896;&#36229;&#38899;&#36895;&#27969;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#20195;&#29702;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#19978;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#22788;&#29702;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20195;&#29702;&#27169;&#22411;&#30528;&#37325;&#20110;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20840;&#23616;&#25554;&#20540;&#65292;&#22240;&#27492;&#38656;&#35201;&#36739;&#22823;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#19988;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#22797;&#26434;&#29289;&#29702;&#38382;&#39064;&#30340;&#39640;&#20445;&#30495;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#23616;&#37096;&#25910;&#25947;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NNLCI&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#39640;&#20445;&#30495;&#39044;&#27979;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#23616;&#37096;&#20381;&#36182;&#22495;&#21644;&#25910;&#25947;&#30340;&#31895;&#30053;&#35299;&#20316;&#20026;&#36755;&#20837;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;&#20316;&#20026;&#39564;&#35777;&#26696;&#20363;&#65292;NNLCI&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#30740;&#31350;&#20855;&#26377;&#20984;&#22359;&#30340;&#23548;&#27969;&#36947;&#20013;&#30340;&#26080;&#31896;&#36229;&#38899;&#36895;&#27969;&#12290;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#20984;&#22359;&#20960;&#20309;&#24418;&#29366;&#21644;&#20301;&#32622;&#65292;&#29992;&#20110;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, surrogate models based on deep neural networks (DNN) have been widely used to solve partial differential equations, which were traditionally handled by means of numerical simulations. This kind of surrogate models, however, focuses on global interpolation of the training dataset, and thus requires a large network structure. The process is both time consuming and computationally costly, thereby restricting their use for high-fidelity prediction of complex physical problems. In the present study, we develop a neural network with local converging input (NNLCI) for high-fidelity prediction using unstructured data. The framework utilizes the local domain of dependence with converging coarse solutions as input, which greatly reduces computational resource and training time. As a validation case, the NNLCI method is applied to study inviscid supersonic flows in channels with bumps. Different bump geometries and locations are considered to benchmark the effectiveness and versa
&lt;/p&gt;</description></item><item><title>TaskDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#23545;&#35805;&#32452;&#25104;&#37096;&#20998;&#26469;&#35745;&#31639;&#30456;&#20284;&#24230;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15298</link><description>&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TaskDiff: A Similarity Metric for Task-Oriented Conversations. (arXiv:2310.15298v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15298
&lt;/p&gt;
&lt;p&gt;
TaskDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#23545;&#35805;&#32452;&#25104;&#37096;&#20998;&#26469;&#35745;&#31639;&#30456;&#20284;&#24230;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#25968;&#23383;&#21161;&#25163;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#22823;&#37327;&#20250;&#35805;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#21644;&#20010;&#24615;&#21270;&#21709;&#24212;&#29983;&#25104;&#12290;&#20351;&#29992;&#20687;ChatGPT&#36825;&#26679;&#30340;&#27969;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#36825;&#20123;&#21161;&#25163;&#36824;&#38656;&#35201;&#39069;&#22806;&#24378;&#35843;&#25552;&#31034;&#24037;&#31243;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#25991;&#26412;&#30456;&#20284;&#24230;&#24230;&#37327;&#26159;&#36825;&#31181;&#20998;&#26512;&#21644;&#35780;&#20272;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#26041;&#38754;&#24182;&#19981;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#29420;&#29305;&#30340;&#23545;&#35805;&#29305;&#24449;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;TaskDiff&#65292;&#23427;&#21033;&#29992;&#23545;&#35805;&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#65288;&#35805;&#35821;&#12289;&#24847;&#22270;&#21644;&#27133;&#65289;&#21450;&#20854;&#20998;&#24067;&#26469;&#35745;&#31639;&#30456;&#20284;&#24230;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;TaskDiff&#22312;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#30456;&#20851;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of conversational digital assistants has resulted in the availability of large amounts of conversational data which can be utilized for improved user experience and personalized response generation. Building these assistants using popular large language models like ChatGPT also require additional emphasis on prompt engineering and evaluation methods. Textual similarity metrics are a key ingredient for such analysis and evaluations. While many similarity metrics have been proposed in the literature, they have not proven effective for task-oriented conversations as they do not take advantage of unique conversational features. To address this gap, we present TaskDiff, a novel conversational similarity metric that utilizes different dialogue components (utterances, intents, and slots) and their distributions to compute similarity. Extensive experimental evaluation of TaskDiff on a benchmark dataset demonstrates its superior performance and improved robustness over other rela
&lt;/p&gt;</description></item><item><title>DeTiME&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#30340;LLMs&#22686;&#24378;&#25193;&#25955;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#32858;&#31867;&#30340;&#23884;&#20837;&#21644;&#20855;&#26377;&#22686;&#24378;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#33021;&#29983;&#25104;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2310.15296</link><description>&lt;p&gt;
DeTiME: &#20351;&#29992;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#30340;LLM&#22686;&#24378;&#25193;&#25955;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM. (arXiv:2310.15296v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15296
&lt;/p&gt;
&lt;p&gt;
DeTiME&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#30340;LLMs&#22686;&#24378;&#25193;&#25955;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#32858;&#31867;&#30340;&#23884;&#20837;&#21644;&#20855;&#26377;&#22686;&#24378;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#33021;&#29983;&#25104;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20805;&#28385;&#27963;&#21147;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTMs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;NTMs&#20027;&#35201;&#20351;&#29992;&#26469;&#33258;LLMs&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#36825;&#23545;&#20110;&#32858;&#31867;&#25110;&#20027;&#39064;&#29983;&#25104;&#26469;&#35828;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21517;&#20026;DeTiME&#30340;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;DeTiME&#21033;&#29992;&#32534;&#30721;-&#35299;&#30721;&#30340;LLMs&#20135;&#29983;&#39640;&#24230;&#21487;&#32858;&#31867;&#30340;&#23884;&#20837;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#33021;&#22815;&#29983;&#25104;&#26082;&#20855;&#26377;&#20248;&#36234;&#30340;&#32858;&#31867;&#24615;&#21448;&#20855;&#26377;&#22686;&#24378;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#19982;&#24050;&#35782;&#21035;&#20027;&#39064;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#21452;&#37325;&#21151;&#33021;&#20351;&#29992;&#25143;&#33021;&#22815;&#21516;&#26102;&#39640;&#25928;&#20135;&#29983;&#39640;&#24230;&#32858;&#31867;&#30340;&#20027;&#39064;&#21644;&#30456;&#20851;&#20869;&#23481;&#12290;DeTiME&#30340;&#28508;&#21147;&#36824;&#21253;&#25324;&#29983;&#25104;&#38598;&#32676;&#21270;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the burgeoning field of natural language processing, Neural Topic Models (NTMs) and Large Language Models (LLMs) have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic generation. Our study addresses this gap by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages ncoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion, our framework also provides the capability to generate content relevant to the identified topics. This dual functionality allows users to efficiently produce highly clustered topics and related content simultaneously. DeTiME's potential extends to generating clustered embeddi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;&#27169;&#22411;&#20197;&#35299;&#20915;&#22810;&#25945;&#24072;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#21644;COVID-19&#30123;&#33495;&#27979;&#35797;&#39046;&#22495;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#25581;&#31034;&#20102;&#21033;&#29992;&#25945;&#24072;&#38388;&#24046;&#24322;&#26469;&#23398;&#20064;&#20934;&#30830;&#22870;&#21169;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15288</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Active teacher selection for reinforcement learning from human feedback. (arXiv:2310.15288v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;&#27169;&#22411;&#20197;&#35299;&#20915;&#22810;&#25945;&#24072;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#21644;COVID-19&#30123;&#33495;&#27979;&#35797;&#39046;&#22495;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#25581;&#31034;&#20102;&#21033;&#29992;&#25945;&#24072;&#38388;&#24046;&#24322;&#26469;&#23398;&#20064;&#20934;&#30830;&#22870;&#21169;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30446;&#26631;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#19968;&#20010;&#26680;&#24515;&#38480;&#21046;&#26159;&#23427;&#20204;&#20551;&#35774;&#25152;&#26377;&#21453;&#39304;&#37117;&#26469;&#33258;&#19968;&#20010;&#21333;&#19968;&#30340;&#20154;&#31867;&#25945;&#24072;&#65292;&#23613;&#31649;&#38656;&#35201;&#35810;&#38382;&#19981;&#21516;&#25945;&#24072;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"Hidden Utility Bandit"&#65288;HUB&#65289;&#26694;&#26550;&#26469;&#24314;&#27169;&#25945;&#24072;&#22312;&#29702;&#24615;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24418;&#24335;&#21270;&#20102;&#20174;&#22810;&#20010;&#25945;&#24072;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#31181;&#35299;&#20915;&#31639;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#20004;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#39046;&#22495;&#65306;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#21644;COVID-19&#30123;&#33495;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;"Active Teacher Selection"&#65288;ATS&#65289;&#31639;&#27861;&#36890;&#36807;&#20027;&#21160;&#36873;&#25321;&#20309;&#26102;&#20197;&#21450;&#36873;&#25321;&#21738;&#20010;&#25945;&#24072;&#26469;&#26597;&#35810;&#65292;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;HUB&#26694;&#26550;&#21644;ATS&#31639;&#27861;&#23637;&#31034;&#20102;&#21033;&#29992;&#25945;&#24072;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#23398;&#20064;&#20934;&#30830;&#30340;&#22870;&#21169;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#40065;&#26834;&#22870;&#21169;&#24314;&#27169;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) enables machine learning systems to learn objectives from human feedback. A core limitation of these systems is their assumption that all feedback comes from a single human teacher, despite querying a range of distinct teachers. We propose the Hidden Utility Bandit (HUB) framework to model differences in teacher rationality, expertise, and costliness, formalizing the problem of learning from multiple teachers. We develop a variety of solution algorithms and apply them to two real-world domains: paper recommendation systems and COVID-19 vaccine testing. We find that the Active Teacher Selection (ATS) algorithm outperforms baseline algorithms by actively selecting when and which teacher to query. The HUB framework and ATS algorithm demonstrate the importance of leveraging differences between teachers to learn accurate reward models, facilitating future research on active teacher selection for robust reward modeling.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#38754;&#20020;&#33021;&#28304;&#12289;&#23545;&#40784;&#21644;&#20174;&#29421;&#20041;&#20154;&#24037;&#26234;&#33021;&#21040;AGI&#30340;&#19977;&#22823;&#25361;&#25112;&#30340;&#31995;&#32479;&#21270;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#33021;&#28304;&#28040;&#32791;&#12289;&#31995;&#32479;&#35774;&#35745;&#21644;&#23545;&#40784;&#38382;&#39064;&#19978;&#23384;&#22312;&#19981;&#36275;&#65292;&#32780;&#31995;&#32479;&#35774;&#35745;&#22312;&#35299;&#20915;&#23545;&#40784;&#12289;&#33021;&#28304;&#21644;AGI&#22823;&#25361;&#25112;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.15274</link><description>&lt;p&gt;
&#31995;&#32479;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#29992;&#20110;AGI&#65306;&#35299;&#20915;&#23545;&#40784;&#12289;&#33021;&#28304;&#21644;AGI&#22823;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Systematic AI Approach for AGI: Addressing Alignment, Energy, and AGI Grand Challenges. (arXiv:2310.15274v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#38754;&#20020;&#33021;&#28304;&#12289;&#23545;&#40784;&#21644;&#20174;&#29421;&#20041;&#20154;&#24037;&#26234;&#33021;&#21040;AGI&#30340;&#19977;&#22823;&#25361;&#25112;&#30340;&#31995;&#32479;&#21270;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#33021;&#28304;&#28040;&#32791;&#12289;&#31995;&#32479;&#35774;&#35745;&#21644;&#23545;&#40784;&#38382;&#39064;&#19978;&#23384;&#22312;&#19981;&#36275;&#65292;&#32780;&#31995;&#32479;&#35774;&#35745;&#22312;&#35299;&#20915;&#23545;&#40784;&#12289;&#33021;&#28304;&#21644;AGI&#22823;&#25361;&#25112;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30528;&#19977;&#22823;&#25361;&#25112;&#65306;&#33021;&#28304;&#22721;&#22418;&#12289;&#23545;&#40784;&#38382;&#39064;&#21644;&#20174;&#29421;&#20041;&#20154;&#24037;&#26234;&#33021;&#21040;AGI&#30340;&#39134;&#36291;&#12290;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#22312;&#27169;&#22411;&#35757;&#32451;&#21644;&#26085;&#24120;&#36816;&#34892;&#36807;&#31243;&#20013;&#28040;&#32791;&#30528;&#19981;&#21487;&#25345;&#32493;&#30340;&#33021;&#28304;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#33258;2020&#24180;&#20197;&#26469;&#65292;&#27599;&#20010;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#27599;&#20004;&#20010;&#26376;&#23601;&#32763;&#20493;&#65292;&#30452;&#25509;&#23548;&#33268;&#33021;&#28304;&#28040;&#32791;&#30340;&#22686;&#21152;&#12290;&#20174;&#20154;&#24037;&#26234;&#33021;&#21040;AGI&#30340;&#39134;&#36291;&#38656;&#35201;&#22810;&#20010;&#21151;&#33021;&#23376;&#31995;&#32479;&#20197;&#24179;&#34913;&#30340;&#26041;&#24335;&#36816;&#20316;&#65292;&#36825;&#38656;&#35201;&#19968;&#20010;&#31995;&#32479;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#32570;&#20047;&#31995;&#32479;&#35774;&#35745;&#65307;&#21363;&#20351;&#31995;&#32479;&#29305;&#24449;&#22312;&#20154;&#33041;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#20174;&#23427;&#22788;&#29702;&#20449;&#24687;&#30340;&#26041;&#24335;&#21040;&#23427;&#20570;&#20986;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;&#21516;&#26679;&#65292;&#24403;&#21069;&#30340;&#23545;&#40784;&#21644;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#31995;&#32479;&#35774;&#35745;&#65292;&#28982;&#32780;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#33041;&#30340;&#31995;&#32479;&#26550;&#26500;&#22312;&#20581;&#24247;&#30340;&#36947;&#24503;&#20915;&#31574;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#31995;&#32479;&#35774;&#35745;&#22312;&#35299;&#20915;&#23545;&#40784;&#12289;&#33021;&#28304;&#21644;AGI&#22823;&#25361;&#25112;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI faces a trifecta of grand challenges the Energy Wall, the Alignment Problem and the Leap from Narrow AI to AGI. Contemporary AI solutions consume unsustainable amounts of energy during model training and daily operations.Making things worse, the amount of computation required to train each new AI model has been doubling every 2 months since 2020, directly translating to increases in energy consumption.The leap from AI to AGI requires multiple functional subsystems operating in a balanced manner, which requires a system architecture. However, the current approach to artificial intelligence lacks system design; even though system characteristics play a key role in the human brain from the way it processes information to how it makes decisions. Similarly, current alignment and AI ethics approaches largely ignore system design, yet studies show that the brains system architecture plays a critical role in healthy moral decisions.In this paper, we argue that system design is critically im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#21644;&#19981;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35752;&#35770;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#25991;&#26412;&#21487;&#33021;&#23548;&#33268;&#30340;&#38382;&#39064;&#20197;&#21450;&#34920;&#26126;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#24847;&#20041;&#12290;&#21478;&#22806;&#65292;&#36824;&#25552;&#21040;&#20102;&#23545;&#25239;&#26816;&#27979;&#30340;&#31574;&#30053;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.15264</link><description>&lt;p&gt;
&#23545;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#21644;&#19981;&#21487;&#33021;&#24615;&#36827;&#34892;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards Possibilities &amp; Impossibilities of AI-generated Text Detection: A Survey. (arXiv:2310.15264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#21644;&#19981;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35752;&#35770;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#25991;&#26412;&#21487;&#33021;&#23548;&#33268;&#30340;&#38382;&#39064;&#20197;&#21450;&#34920;&#26126;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#24847;&#20041;&#12290;&#21478;&#22806;&#65292;&#36824;&#25552;&#21040;&#20102;&#23545;&#25239;&#26816;&#27979;&#30340;&#31574;&#30053;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#20854;&#29983;&#25104;&#20154;&#31867;&#21270;&#25991;&#26412;&#21709;&#24212;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#19968;&#20123;&#24037;&#20316;&#23545;LLMs&#30340;&#28508;&#22312;&#28389;&#29992;&#38382;&#39064;&#25552;&#20986;&#20102;&#20005;&#37325;&#20851;&#27880;&#65292;&#22914;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#12289;&#29983;&#25104;&#20551;&#26032;&#38395;&#12289;&#23398;&#26415;&#25220;&#34989;&#21644;&#27745;&#26579;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30740;&#31350;&#30028;&#36798;&#25104;&#20849;&#35782;&#65292;&#21363;&#24320;&#21457;&#29992;&#20110;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#65292;&#21482;&#35201;&#25105;&#20204;&#33021;&#21028;&#26029;&#32473;&#23450;&#25991;&#26412;&#26159;&#30001;&#20154;&#31867;&#36824;&#26159;AI&#32534;&#20889;&#30340;&#65292;&#25105;&#20204;&#23601;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#24212;&#23545;&#19978;&#36848;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#22823;&#37327;&#30340;&#26816;&#27979;&#26694;&#26550;&#65292;&#31361;&#20986;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#26816;&#27979;&#26694;&#26550;&#30340;&#21457;&#23637;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#36824;&#33268;&#21147;&#20110;&#35774;&#35745;&#35268;&#36991;&#26816;&#27979;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized the domain of natural language processing (NLP) with remarkable capabilities of generating human-like text responses. However, despite these advancements, several works in the existing literature have raised serious concerns about the potential misuse of LLMs such as spreading misinformation, generating fake news, plagiarism in academia, and contaminating the web. To address these concerns, a consensus among the research community is to develop algorithmic solutions to detect AI-generated text. The basic idea is that whenever we can tell if the given text is either written by a human or an AI, we can utilize this information to address the above-mentioned concerns. To that end, a plethora of detection frameworks have been proposed, highlighting the possibilities of AI-generated text detection. But in parallel to the development of detection frameworks, researchers have also concentrated on designing strategies to elude detection, i.e., f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22122;&#22768;&#29615;&#22659;&#20013;&#38382;&#39064;&#32763;&#35793;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21482;&#20351;&#29992;&#28304;&#35821;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#32763;&#35793;&#38382;&#39064;&#30340;&#20805;&#20998;&#24615;&#21644;&#27969;&#30021;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.15259</link><description>&lt;p&gt;
&#26080;&#21442;&#32771;&#39046;&#22495;&#33258;&#36866;&#24212;&#32763;&#35793;&#24102;&#26377;&#38382;&#39064;&#29305;&#23450;&#22870;&#21169;&#30340;&#22122;&#22768;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards. (arXiv:2310.15259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15259
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22122;&#22768;&#29615;&#22659;&#20013;&#38382;&#39064;&#32763;&#35793;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21482;&#20351;&#29992;&#28304;&#35821;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#32763;&#35793;&#38382;&#39064;&#30340;&#20805;&#20998;&#24615;&#21644;&#27969;&#30021;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;(CQA)&#24179;&#21488;&#26159;&#24110;&#21161;&#32452;&#32455;&#20869;&#29992;&#25143;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20351;&#23427;&#20204;&#23545;&#38750;&#33521;&#35821;&#29992;&#25143;&#21487;&#35775;&#38382;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#32763;&#35793;&#38382;&#39064;&#21487;&#20197;&#25299;&#23485;&#31038;&#21306;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#20351;&#26377;&#31867;&#20284;&#38382;&#39064;&#30340;&#20154;&#33021;&#22815;&#21463;&#30410;&#65292;&#20294;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#36827;&#34892;&#38382;&#39064;&#32763;&#35793;&#20250;&#38754;&#20020;&#26356;&#22810;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#35821;&#27861;&#27491;&#30830;&#24615;&#27809;&#26377;&#21463;&#21040;&#30417;&#25511;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#34987;&#38750;&#27597;&#35821;&#29992;&#25143;&#20197;&#38472;&#36848;&#21477;&#30340;&#24418;&#24335;&#34920;&#36798;&#65292;&#20855;&#26377;&#19981;&#27491;&#30830;&#30340;&#20027;&#35859;&#35821;&#24207;&#65292;&#29978;&#33267;&#26377;&#26102;&#32570;&#23569;&#38382;&#21495;&#12290;&#30001;&#20110;&#25968;&#25454;&#23384;&#22312;&#22122;&#22768;&#65292;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#21019;&#24314;&#19968;&#20010;&#21512;&#25104;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20063;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#28304;&#35821;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;BERTScore&#21644;Masked Language Model (MLM) S&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24179;&#34913;&#20102;&#20805;&#20998;&#24615;&#21644;&#27969;&#30021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question-Answering (CQA) portals serve as a valuable tool for helping users within an organization. However, making them accessible to non-English-speaking users continues to be a challenge. Translating questions can broaden the community's reach, benefiting individuals with similar inquiries in various languages. Translating questions using Neural Machine Translation (NMT) poses more challenges, especially in noisy environments, where the grammatical correctness of the questions is not monitored. These questions may be phrased as statements by non-native speakers, with incorrect subject-verb order and sometimes even missing question marks. Creating a synthetic parallel corpus from such data is also difficult due to its noisy nature. To address this issue, we propose a training methodology that fine-tunes the NMT system only using source-side data. Our approach balances adequacy and fluency by utilizing a loss function that combines BERTScore and Masked Language Model (MLM) S
&lt;/p&gt;</description></item><item><title>CRoW&#26159;&#19968;&#20010;&#25163;&#24037;&#31574;&#21010;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;NLP&#20219;&#21153;&#20013;&#24212;&#29992;&#24120;&#35782;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;NLP&#31995;&#32479;&#22312;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#34920;&#26126;&#24120;&#35782;&#25512;&#29702;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#20173;&#28982;&#36828;&#26410;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2310.15239</link><description>&lt;p&gt;
CRoW: &#22312;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#23545;&#24120;&#35782;&#25512;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks. (arXiv:2310.15239v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15239
&lt;/p&gt;
&lt;p&gt;
CRoW&#26159;&#19968;&#20010;&#25163;&#24037;&#31574;&#21010;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;NLP&#20219;&#21153;&#20013;&#24212;&#29992;&#24120;&#35782;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;NLP&#31995;&#32479;&#22312;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#34920;&#26126;&#24120;&#35782;&#25512;&#29702;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#20173;&#28982;&#36828;&#26410;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20851;&#20110;&#24120;&#35782;&#25512;&#29702;&#30340;&#30740;&#31350;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#35768;&#22810;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#20154;&#24037;&#22330;&#26223;&#19979;&#26500;&#24314;&#20102;&#24120;&#35782;&#25512;&#29702;&#25361;&#25112;&#65292;&#36825;&#20123;&#22330;&#26223;&#24182;&#19981;&#33021;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;NLP&#31995;&#32479;&#25152;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CRoW&#65292;&#19968;&#20010;&#25163;&#24037;&#31574;&#21010;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;NLP&#20219;&#21153;&#20013;&#24212;&#29992;&#24120;&#35782;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;CRoW&#20351;&#29992;&#22810;&#38454;&#27573;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#26500;&#24314;&#65292;&#36890;&#36807;&#36829;&#21453;&#24120;&#35782;&#30340;&#25200;&#21160;&#37325;&#20889;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#21033;&#29992;CRoW&#26469;&#30740;&#31350;NLP&#31995;&#32479;&#22312;&#29289;&#29702;&#12289;&#26102;&#38388;&#21644;&#31038;&#20132;&#25512;&#29702;&#31561;&#19981;&#21516;&#24120;&#35782;&#30693;&#35782;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;CRoW&#19978;&#35780;&#20272;NLP&#31995;&#32479;&#26102;&#19982;&#20154;&#31867;&#30456;&#27604;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#26174;&#31034;&#20986;&#24120;&#35782;&#25512;&#29702;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36824;&#36828;&#26410;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent efforts in natural language processing (NLP) commonsense reasoning research have yielded a considerable number of new datasets and benchmarks. However, most of these datasets formulate commonsense reasoning challenges in artificial scenarios that are not reflective of the tasks which real-world NLP systems are designed to solve. In this work, we present CRoW, a manually-curated, multi-task benchmark that evaluates the ability of models to apply commonsense reasoning in the context of six real-world NLP tasks. CRoW is constructed using a multi-stage data collection pipeline that rewrites examples from existing datasets using commonsense-violating perturbations. We use CRoW to study how NLP systems perform across different dimensions of commonsense knowledge, such as physical, temporal, and social reasoning. We find a significant performance gap when NLP systems are evaluated on CRoW compared to humans, showcasing that commonsense reasoning is far from being solved in real-world t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24341;&#21147;&#27874;&#27169;&#26495;&#24211;&#20013;&#21253;&#21547;&#39640;&#27425;&#35856;&#27874;&#27169;&#24335;&#65292;&#21033;&#29992;&#24341;&#21147;&#27874;&#27169;&#24335;&#20043;&#38388;&#30340;&#33258;&#28982;&#20851;&#32852;&#65292;&#21487;&#20197;&#22823;&#24133;&#24230;&#20943;&#23569;&#21305;&#37197;&#28388;&#27874;&#30340;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#25628;&#32034;&#24341;&#21147;&#27874;&#20107;&#20214;&#30340;&#28789;&#25935;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.15233</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#26041;&#27861;&#29992;&#20110;&#24102;&#26377;&#26356;&#39640;&#27425;&#35856;&#27874;&#30340;&#24341;&#21147;&#27874;&#27169;&#26495;&#24211;&#65306;&#36890;&#36807;&#21313;&#20493;&#20943;&#23569;&#21305;&#37197;&#28388;&#27874;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
A new approach to template banks of gravitational waves with higher harmonics: reducing matched-filtering cost by over an order of magnitude. (arXiv:2310.15233v1 [gr-qc])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24341;&#21147;&#27874;&#27169;&#26495;&#24211;&#20013;&#21253;&#21547;&#39640;&#27425;&#35856;&#27874;&#27169;&#24335;&#65292;&#21033;&#29992;&#24341;&#21147;&#27874;&#27169;&#24335;&#20043;&#38388;&#30340;&#33258;&#28982;&#20851;&#32852;&#65292;&#21487;&#20197;&#22823;&#24133;&#24230;&#20943;&#23569;&#21305;&#37197;&#28388;&#27874;&#30340;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#25628;&#32034;&#24341;&#21147;&#27874;&#20107;&#20214;&#30340;&#28789;&#25935;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#21147;&#27874;&#20107;&#20214;&#30340;&#25628;&#32034;&#20351;&#29992;&#20449;&#21495;&#27169;&#22411;&#25110;&#27169;&#26495;&#12290;&#30446;&#21069;&#22312;LIGO-Virgo-Kagra (LVK)&#25968;&#25454;&#20013;&#20351;&#29992;&#30340;&#27169;&#26495;&#20165;&#27169;&#25311;&#20102;&#20449;&#21495;&#30340;&#20027;&#23548;&#22235;&#26497;&#27169;&#24335;$(\ell,m)=(2,2)$&#65292;&#24573;&#30053;&#20102;&#27425;&#35201;&#30340;&#39640;&#38454;&#27169;&#24335;(HM)&#20363;&#22914;$(\ell,m)=(3,3)$&#65292;$(4,4)$&#65292;&#36825;&#20123;&#27169;&#24335;&#26159;&#30001;&#24191;&#20041;&#30456;&#23545;&#35770;&#39044;&#27979;&#30340;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#25628;&#32034;&#21487;&#33021;&#20250;&#22312;&#21442;&#25968;&#31354;&#38388;&#30340;&#19968;&#20123;&#26377;&#36259;&#21306;&#22495;&#65292;&#22914;&#39640;&#36136;&#37327;&#21644;&#38750;&#23545;&#31216;&#36136;&#37327;&#27604;&#30340;&#31995;&#32479;&#20013;&#22833;&#21435;&#23545;&#40657;&#27934;&#21512;&#24182;&#30340;&#28789;&#25935;&#24230;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#31574;&#30053;&#65292;&#23558;HM&#21253;&#21547;&#22312;&#27169;&#26495;&#24211;&#20013;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#24335;&#20043;&#38388;&#30340;&#33258;&#28982;&#20851;&#32852;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#29275;&#39039;&#38468;&#21152;&#20844;&#24335;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26469;&#27169;&#25311;&#19982;&#32473;&#23450;$(2,2)$&#27874;&#24418;&#30456;&#23545;&#24212;&#30340;&#33258;&#26059;&#23545;&#40784;&#30340;$(3,3)$&#65292;$(4,4)$&#27874;&#24418;&#12290;&#21487;&#20197;&#23545;&#27599;&#20010;&#27169;&#24335;&#30340;&#25968;&#25454;&#36827;&#34892;&#21333;&#29420;&#28388;&#27874;&#65292;&#24471;&#21040;&#20449;&#22122;&#27604;(SNR)&#30340;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#65292;&#28982;&#21518;&#21487;&#20197;&#20197;&#30456;&#23545;&#24265;&#20215;&#30340;&#26041;&#24335;&#23558;&#20854;&#32452;&#21512;&#36215;&#26469;&#36827;&#34892;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Searches for gravitational wave events use models, or templates, for the signals of interest. The templates used in current searches in the LIGO-Virgo-Kagra (LVK) data model the dominant quadrupole mode $(\ell,m)=(2,2)$ of the signals, and omit sub-dominant higher-order modes (HM) such as $(\ell,m)=(3,3)$, $(4,4)$, which are predicted by general relativity. Hence, these searches could lose sensitivity to black hole mergers in interesting parts of parameter space, such as systems with high-masses and asymmetric mass ratios. We develop a new strategy to include HM in template banks that exploits the natural connection between the modes. We use a combination of post-Newtonian formulae and machine learning tools to model aligned-spin $(3,3)$, $(4,4)$ waveforms corresponding to a given $(2,2)$ waveform. Each of these modes can be individually filtered against the data to yield separate timeseries of signal-to-noise ratios (SNR), which can be combined in a relatively inexpensive way to margi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65288;MPI&#65289;&#26469;&#26377;&#25928;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#26469;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#36335;&#24452;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21457;&#29616;&#20505;&#36873;&#33647;&#29289;&#12290;</title><link>http://arxiv.org/abs/2310.15211</link><description>&lt;p&gt;
&#27169;&#25311;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#36335;&#24452;&#37325;&#35201;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Path Importance for Effective Alzheimer's Disease Drug Repurposing. (arXiv:2310.15211v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65288;MPI&#65289;&#26469;&#26377;&#25928;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#26469;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#36335;&#24452;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21457;&#29616;&#20505;&#36873;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33647;&#29289;&#37325;&#29992;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#19988;&#36164;&#28304;&#39640;&#25928;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#21457;&#29616;&#33539;&#24335;&#24050;&#32463;&#23853;&#38706;&#22836;&#35282;&#12290;&#22312;&#21508;&#31181;&#33647;&#29289;&#37325;&#29992;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#32593;&#32476;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#21033;&#29992;&#22797;&#26434;&#32593;&#32476;&#65292;&#25972;&#21512;&#22810;&#31181;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65288;&#22914;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65289;&#65292;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#28508;&#22312;&#33647;&#29289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#32593;&#32476;&#20013;&#30456;&#21516;&#38271;&#24230;&#30340;&#36335;&#24452;&#23545;&#20110;&#35782;&#21035;&#33647;&#29289;&#30340;&#27835;&#30103;&#25928;&#26524;&#20855;&#26377;&#30456;&#31561;&#30340;&#37325;&#35201;&#24615;&#12290;&#20854;&#20182;&#39046;&#22495;&#21457;&#29616;&#65292;&#30456;&#21516;&#38271;&#24230;&#30340;&#36335;&#24452;&#24182;&#19981;&#19968;&#23450;&#20855;&#26377;&#30456;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#20381;&#36182;&#20110;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#23545;&#33647;&#29289;&#37325;&#29992;&#23581;&#35797;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MPI&#65288;&#27169;&#25311;&#36335;&#24452;&#37325;&#35201;&#24615;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#33647;&#29289;&#37325;&#29992;&#26041;&#27861;&#12290;MPI&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#65292;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#26469;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#36335;&#24452;&#65292;&#36825;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#32593;&#32476;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#23398;&#20064;&#30340;&#33410;&#28857;&#23884;&#20837;&#21487;&#20197;&#25552;&#39640;&#33647;&#29289;&#37325;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, drug repurposing has emerged as an effective and resource-efficient paradigm for AD drug discovery. Among various methods for drug repurposing, network-based methods have shown promising results as they are capable of leveraging complex networks that integrate multiple interaction types, such as protein-protein interactions, to more effectively identify candidate drugs. However, existing approaches typically assume paths of the same length in the network have equal importance in identifying the therapeutic effect of drugs. Other domains have found that same length paths do not necessarily have the same importance. Thus, relying on this assumption may be deleterious to drug repurposing attempts. In this work, we propose MPI (Modeling Path Importance), a novel network-based method for AD drug repurposing. MPI is unique in that it prioritizes important paths via learned node embeddings, which can effectively capture a network's rich structural information. Thus, leveraging learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;DNABERT-Cap&#65292;&#29992;&#20110;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20013;&#21033;&#29992;&#20102;&#21452;&#21521;&#32534;&#30721;&#22120;&#12289;&#33014;&#22218;&#23618;&#12289;&#21367;&#31215;&#21644;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#23618;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#32852;&#21512;&#20248;&#21270;&#26500;&#24314;&#20102;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#30340;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.15202</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;
&lt;/p&gt;
&lt;p&gt;
Predicting Transcription Factor Binding Sites using Transformer based Capsule Network. (arXiv:2310.15202v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;DNABERT-Cap&#65292;&#29992;&#20110;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20013;&#21033;&#29992;&#20102;&#21452;&#21521;&#32534;&#30721;&#22120;&#12289;&#33014;&#22218;&#23618;&#12289;&#21367;&#31215;&#21644;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#23618;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#32852;&#21512;&#20248;&#21270;&#26500;&#24314;&#20102;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#30340;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#30340;&#32467;&#21512;&#20301;&#28857;&#23545;&#20110;&#29702;&#35299;&#23427;&#20204;&#22914;&#20309;&#35843;&#25511;&#22522;&#22240;&#34920;&#36798;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#27835;&#30103;&#25163;&#27573;&#36827;&#34892;&#35843;&#33410;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#24050;&#32463;&#26377;&#30456;&#24403;&#22810;&#30340;&#24037;&#20316;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20294;&#20173;&#28982;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#33014;&#22218;&#32593;&#32476;DNABERT-Cap&#65292;&#29992;&#20110;&#21033;&#29992;ChIP-seq&#25968;&#25454;&#38598;&#25366;&#25496;&#39044;&#27979;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#12290;DNABERT-Cap&#26159;&#19968;&#20010;&#21452;&#21521;&#32534;&#30721;&#22120;&#65292;&#32463;&#36807;&#22823;&#37327;&#22522;&#22240;&#32452;DNA&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#33014;&#22218;&#23618;&#36827;&#34892;&#26368;&#32456;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#23545;&#21253;&#21547;&#21452;&#21521;&#32534;&#30721;&#22120;&#12289;&#33014;&#22218;&#23618;&#12289;&#21367;&#31215;&#21644;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#23618;&#29305;&#24449;&#30340;&#32852;&#21512;&#20248;&#21270;&#65292;&#26500;&#24314;&#20102;&#36716;&#24405;&#22240;&#23376;&#32467;&#21512;&#20301;&#28857;&#30340;&#39044;&#27979;&#22120;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20116;&#20010;&#32454;&#32990;&#31995;&#30340;&#22522;&#20934;ChIP-seq&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;A54&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction of binding sites for transcription factors is important to understand how they regulate gene expression and how this regulation can be modulated for therapeutic purposes. Although in the past few years there are significant works addressing this issue, there is still space for improvement. In this regard, a transformer based capsule network viz. DNABERT-Cap is proposed in this work to predict transcription factor binding sites mining ChIP-seq datasets. DNABERT-Cap is a bidirectional encoder pre-trained with large number of genomic DNA sequences, empowered with a capsule layer responsible for the final prediction. The proposed model builds a predictor for transcription factor binding sites using the joint optimisation of features encompassing both bidirectional encoder and capsule layer, along with convolutional and bidirectional long-short term memory layers. To evaluate the efficiency of the proposed approach, we use a benchmark ChIP-seq datasets of five cell lines viz. A54
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#29992;&#20110;&#30740;&#31350;&#30382;&#32932;&#24494;&#29983;&#29289;&#32452;&#20013;&#30340;&#20004;&#31181;&#32454;&#33740;&#32676;&#33853;&#65288;&#20849;&#29983;&#33740;&#21644;&#26426;&#20250;&#24615;&#33268;&#30149;&#33740;&#65289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#32771;&#34385;&#20102;&#25239;&#24494;&#29983;&#29289;&#32957;&#30340;&#20135;&#29983;&#12290;&#36890;&#36807;&#20351;&#29992;&#23454;&#39564;&#25968;&#25454;&#39564;&#35777;&#27169;&#22411;&#30340;&#31283;&#23450;&#29366;&#24577;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#30382;&#32932;&#24494;&#29983;&#29289;&#32452;&#20013;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#31454;&#20105;&#26426;&#21046;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15201</link><description>&lt;p&gt;
&#30382;&#32932;&#24494;&#29983;&#29289;&#32452;&#27169;&#22411;&#20013;&#30340;AMP&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#31181;&#32676;&#21160;&#21147;&#23398;&#20013;&#20934;&#31283;&#23450;&#19982;&#31283;&#23450;&#24615;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Skin Microbiome Model with AMP interactions and Analysis of Quasi-Stability vs Stability in Population Dynamics. (arXiv:2310.15201v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#29992;&#20110;&#30740;&#31350;&#30382;&#32932;&#24494;&#29983;&#29289;&#32452;&#20013;&#30340;&#20004;&#31181;&#32454;&#33740;&#32676;&#33853;&#65288;&#20849;&#29983;&#33740;&#21644;&#26426;&#20250;&#24615;&#33268;&#30149;&#33740;&#65289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#32771;&#34385;&#20102;&#25239;&#24494;&#29983;&#29289;&#32957;&#30340;&#20135;&#29983;&#12290;&#36890;&#36807;&#20351;&#29992;&#23454;&#39564;&#25968;&#25454;&#39564;&#35777;&#27169;&#22411;&#30340;&#31283;&#23450;&#29366;&#24577;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#30382;&#32932;&#24494;&#29983;&#29289;&#32452;&#20013;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#31454;&#20105;&#26426;&#21046;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30382;&#32932;&#24494;&#29983;&#29289;&#32452;&#22312;&#32500;&#25345;&#20581;&#24247;&#30382;&#32932;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23427;&#26159;&#19968;&#20010;&#30001;&#22810;&#20010;&#29289;&#31181;&#32452;&#25104;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#31454;&#20105;&#36164;&#28304;&#24182;&#19982;&#30382;&#32932;&#32454;&#32990;&#30456;&#20114;&#20316;&#29992;&#12290;&#30382;&#32932;&#24494;&#29983;&#29289;&#32452;&#30340;&#19981;&#24179;&#34913;&#65292;&#20063;&#31216;&#20026;&#33740;&#32676;&#32010;&#20081;&#65292;&#19982;&#22810;&#31181;&#30382;&#32932;&#29366;&#20917;&#65288;&#21253;&#25324;&#30180;&#30126;&#21644;&#29305;&#24212;&#24615;&#30382;&#28814;&#65289;&#30456;&#20851;&#12290;&#36890;&#24120;&#65292;&#33740;&#32676;&#32010;&#20081;&#19982;&#30382;&#32932;&#34987;&#26426;&#20250;&#24615;&#33268;&#30149;&#33740;&#32676;&#30340;&#23450;&#23621;&#26377;&#20851;&#12290;&#20197;&#38750;&#29305;&#24322;&#24615;&#28040;&#38500;&#30382;&#32932;&#24494;&#29983;&#29289;&#32452;&#30340;&#27835;&#30103;&#32467;&#26524;&#30456;&#20114;&#30683;&#30462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;2&#31181;&#31867;&#22411;&#30340;&#32454;&#33740;&#32676;&#33853;&#65288;&#30382;&#32932;&#20849;&#29983;&#33740;&#21644;&#26426;&#20250;&#24615;&#33268;&#30149;&#33740;&#65289;&#20197;&#21450;&#25239;&#24494;&#29983;&#29289;&#32957;&#30340;&#20135;&#29983;&#65292;&#20197;&#30740;&#31350;&#39537;&#21160;&#19968;&#31181;&#32676;&#33853;&#20248;&#21183;&#20110;&#21478;&#19968;&#31181;&#32676;&#33853;&#30340;&#26426;&#21046;&#12290;&#36890;&#36807;&#20351;&#29992;&#20986;&#29256;&#30340;&#23454;&#39564;&#25968;&#25454;&#65292;&#20551;&#35774;&#36825;&#20123;&#25968;&#25454;&#23545;&#24212;&#20110;&#25105;&#20204;&#27169;&#22411;&#20013;&#30340;&#31283;&#23450;&#29366;&#24577;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The skin microbiome plays an important role in the maintenance of a healthy skin. It is an ecosystem, composed of several species, competing for resources and interacting with the skin cells. Imbalance in the cutaneous microbiome, also called dysbiosis, has been correlated with several skin conditions, including acne and atopic dermatitis. Generally, dysbiosis is linked to colonization of the skin by a population of opportunistic pathogenic bacteria. Treatments consisting in non-specific elimination of cutaneous microflora have shown conflicting results. In this article, we introduce a mathematical model based on ordinary differential equations, with 2 types of bacteria populations (skin commensals and opportunistic pathogens) and including the production of antimicrobial peptides to study the mechanisms driving the dominance of one population over the other. By using published experimental data, assumed to correspond to the observation of stable states in our model, we reduce the numb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20803;&#31070;&#32463;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;EMNH&#65289;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20803;&#27169;&#22411;&#24182;&#36827;&#34892;&#24494;&#35843;&#65292;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EMNH&#22312;&#23398;&#20064;&#25928;&#29575;&#21644;&#35299;&#20915;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.15196</link><description>&lt;p&gt;
&#39640;&#25928;&#20803;&#31070;&#32463;&#21551;&#21457;&#24335;&#31639;&#27861;&#29992;&#20110;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Efficient Meta Neural Heuristic for Multi-Objective Combinatorial Optimization. (arXiv:2310.15196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20803;&#31070;&#32463;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;EMNH&#65289;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20803;&#27169;&#22411;&#24182;&#36827;&#34892;&#24494;&#35843;&#65292;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EMNH&#22312;&#23398;&#20064;&#25928;&#29575;&#21644;&#35299;&#20915;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#35299;&#20915;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#22312;&#23398;&#20064;&#25928;&#29575;&#21644;&#35299;&#20915;&#36136;&#37327;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20803;&#31070;&#32463;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;EMNH&#65289;&#65292;&#20854;&#20013;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#20803;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#20960;&#20010;&#27493;&#39588;&#23545;&#24212;&#30340;&#21333;&#30446;&#26631;&#23376;&#38382;&#39064;&#26469;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21033;&#29992;&#65288;&#37096;&#20998;&#65289;&#26550;&#26500;&#20849;&#20139;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#23454;&#29616;&#20803;&#27169;&#22411;&#30340;&#24182;&#34892;&#23398;&#20064;&#65292;&#20197;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65307;&#21516;&#26102;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#19982;&#26435;&#37325;&#21521;&#37327;&#30456;&#20851;&#30340;&#27604;&#20363;&#23545;&#31216;&#37319;&#26679;&#26041;&#27861;&#26469;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23618;&#27425;&#21270;&#26041;&#27861;&#26469;&#31995;&#32479;&#22320;&#22788;&#29702;&#25152;&#26377;&#30340;&#23376;&#38382;&#39064;&#12290;&#22312;&#22810;&#30446;&#26631;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;MOTSP&#65289;&#12289;&#22810;&#30446;&#26631;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;MOVRPTW&#65289;&#21644;&#22810;&#30446;&#26631;&#32972;&#21253;&#38382;&#39064;&#65288;MOKP&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;EMNH&#22312;&#23398;&#20064;&#25928;&#29575;&#21644;&#35299;&#20915;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural heuristics based on deep reinforcement learning have exhibited promise in solving multi-objective combinatorial optimization problems (MOCOPs). However, they are still struggling to achieve high learning efficiency and solution quality. To tackle this issue, we propose an efficient meta neural heuristic (EMNH), in which a meta-model is first trained and then fine-tuned with a few steps to solve corresponding single-objective subproblems. Specifically, for the training process, a (partial) architecture-shared multi-task model is leveraged to achieve parallel learning for the meta-model, so as to speed up the training; meanwhile, a scaled symmetric sampling method with respect to the weight vectors is designed to stabilize the training. For the fine-tuning process, an efficient hierarchical method is proposed to systematically tackle all the subproblems. Experimental results on the multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle ro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22810;&#26679;&#24615;&#22686;&#24378;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;NHDE&#65289;&#26469;&#35299;&#20915;&#31070;&#32463;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#65288;MOCO&#65289;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25351;&#31034;&#22120;&#22686;&#24378;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#37325;&#24085;&#32047;&#25176;&#26368;&#26377;&#31574;&#30053;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22810;&#19988;&#20855;&#26377;&#26356;&#39640;&#22810;&#26679;&#24615;&#30340;&#24085;&#32047;&#25176;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.15195</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#26679;&#24615;&#22686;&#24378;&#30340;&#31070;&#32463;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural Multi-Objective Combinatorial Optimization with Diversity Enhancement. (arXiv:2310.15195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15195
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22810;&#26679;&#24615;&#22686;&#24378;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;NHDE&#65289;&#26469;&#35299;&#20915;&#31070;&#32463;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#65288;MOCO&#65289;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25351;&#31034;&#22120;&#22686;&#24378;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#37325;&#24085;&#32047;&#25176;&#26368;&#26377;&#31574;&#30053;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#22810;&#19988;&#20855;&#26377;&#26356;&#39640;&#22810;&#26679;&#24615;&#30340;&#24085;&#32047;&#25176;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31070;&#32463;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#65288;MOCO&#65289;&#38382;&#39064;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#20998;&#35299;&#65292;&#36825;&#24448;&#24448;&#23548;&#33268;&#23376;&#38382;&#39064;&#30340;&#37325;&#22797;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#24471;&#21040;&#26377;&#38480;&#30340;&#24085;&#32047;&#25176;&#38598;&#12290;&#38500;&#20102;&#20998;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22810;&#26679;&#24615;&#22686;&#24378;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;NHDE&#65289;&#65292;&#20174;&#20004;&#20010;&#35282;&#24230;&#20135;&#29983;&#26356;&#22810;&#24085;&#32047;&#25176;&#35299;&#12290;&#19968;&#26041;&#38754;&#65292;&#20026;&#20102;&#38459;&#27490;&#19981;&#21516;&#23376;&#38382;&#39064;&#30340;&#37325;&#22797;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#31034;&#22120;&#22686;&#24378;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#25351;&#23548;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#24322;&#26500;&#22270;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#23454;&#20363;&#22270;&#21644;&#24085;&#32047;&#25176;&#21069;&#27839;&#22270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20026;&#20102;&#22312;&#27599;&#20010;&#23376;&#38382;&#39064;&#30340;&#37051;&#22495;&#20013;&#25366;&#25496;&#26356;&#22810;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#37325;&#24085;&#32047;&#25176;&#26368;&#26377;&#31574;&#30053;&#26469;&#37319;&#26679;&#21644;&#20445;&#30041;&#29702;&#24819;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#32463;&#20856;&#30340; MOCO &#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340; NHDE &#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#26356;&#39640;&#22810;&#26679;&#24615;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20248;&#36234;&#30340;&#32508;&#21512;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Most of existing neural methods for multi-objective combinatorial optimization (MOCO) problems solely rely on decomposition, which often leads to repetitive solutions for the respective subproblems, thus a limited Pareto set. Beyond decomposition, we propose a novel neural heuristic with diversity enhancement (NHDE) to produce more Pareto solutions from two perspectives. On the one hand, to hinder duplicated solutions for different subproblems, we propose an indicator-enhanced deep reinforcement learning method to guide the model, and design a heterogeneous graph attention mechanism to capture the relations between the instance graph and the Pareto front graph. On the other hand, to excavate more solutions in the neighborhood of each subproblem, we present a multiple Pareto optima strategy to sample and preserve desirable solutions. Experimental results on classic MOCO problems show that our NHDE is able to generate a Pareto front with higher diversity, thereby achieving superior overa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#35299;&#20915;&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#21021;&#22987;&#29468;&#27979;&#20197;&#21450;&#21033;&#29992;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#23398;&#20064;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#30340;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.15191</link><description>&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Application of deep and reinforcement learning to boundary control problems. (arXiv:2310.15191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#35299;&#20915;&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#21021;&#22987;&#29468;&#27979;&#20197;&#21450;&#21033;&#29992;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#23398;&#20064;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#30340;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#26159;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#38750;&#20984;&#20248;&#21270;&#21644;&#25511;&#21046;&#38382;&#39064;&#65292;&#21253;&#25324;&#27969;&#20307;&#21147;&#23398;&#12289;&#32467;&#26500;&#24037;&#31243;&#21644;&#28909;&#20256;&#36882;&#20248;&#21270;&#12290;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#20351;&#31526;&#21512;&#25511;&#21046;&#26041;&#31243;&#35201;&#27714;&#30340;&#23553;&#38381;&#22495;&#20869;&#30340;&#29366;&#24577;&#20540;&#36798;&#21040;&#26399;&#26395;&#20540;&#30340;&#26368;&#20248;&#36793;&#30028;&#20540;&#12290;&#20256;&#32479;&#19978;&#65292;&#20351;&#29992;&#38750;&#32447;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#20869;&#28857;&#27861;&#65288;IPM&#65289;&#26469;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#36981;&#24490;&#36845;&#20195;&#20248;&#21270;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31354;&#38388;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#26377;&#21551;&#21457;&#24615;&#30340;&#21021;&#22987;&#29468;&#27979;&#65292;&#24182;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#30340;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#12290;&#20351;&#29992;&#20174;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#38382;&#39064;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#39564;&#35777;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#36793;&#30028;&#25511;&#21046;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The boundary control problem is a non-convex optimization and control problem in many scientific domains, including fluid mechanics, structural engineering, and heat transfer optimization. The aim is to find the optimal values for the domain boundaries such that the enclosed domain adhering to the governing equations attains the desired state values. Traditionally, non-linear optimization methods, such as the Interior-Point method (IPM), are used to solve such problems.  This project explores the possibilities of using deep learning and reinforcement learning to solve boundary control problems. We adhere to the framework of iterative optimization strategies, employing a spatial neural network to construct well-informed initial guesses, and a spatio-temporal neural network learns the iterative optimization algorithm using policy gradients. Synthetic data, generated from the problems formulated in the literature, is used for training, testing and validation. The numerical experiments ind
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#24494;&#32467;&#26500;&#19982;&#21147;&#23398;&#24615;&#33021;&#24314;&#31435;&#26144;&#23556;&#20851;&#31995;&#65292;&#21152;&#24555;&#20102;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;&#26681;&#25454;&#26399;&#26395;&#24615;&#33021;&#29983;&#25104;&#24494;&#32467;&#26500;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.15188</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#31896;&#24377;&#32420;&#32500;&#22797;&#21512;&#26448;&#26009;&#30340;&#21160;&#24577;&#21147;&#23398;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Approaches for Dynamic Mechanical Analysis of Viscoelastic Fiber Composites. (arXiv:2310.15188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#24494;&#32467;&#26500;&#19982;&#21147;&#23398;&#24615;&#33021;&#24314;&#31435;&#26144;&#23556;&#20851;&#31995;&#65292;&#21152;&#24555;&#20102;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;&#26681;&#25454;&#26399;&#26395;&#24615;&#33021;&#29983;&#25104;&#24494;&#32467;&#26500;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#29983;&#24577;&#35774;&#35745;&#26631;&#20934;&#30340;&#25512;&#21160;&#65292;&#22686;&#24378;&#22411;&#32858;&#21512;&#29289;&#65288;RP&#65289;&#22797;&#21512;&#26448;&#26009;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#36825;&#35201;&#27714;&#22312;&#36731;&#37327;&#21270;&#12289;&#21018;&#24615;&#21644;&#26377;&#25928;&#30340;&#25391;&#21160;&#25511;&#21046;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#36825;&#20123;&#26448;&#26009;&#23545;&#20110;&#25552;&#39640;&#33298;&#36866;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#33021;&#28304;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#21160;&#24577;&#21147;&#23398;&#20998;&#26512;&#65288;DMA&#65289;&#21487;&#34920;&#24449;&#31896;&#24377;&#24615;&#34892;&#20026;&#65292;&#28982;&#32780;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21152;&#36895;&#24494;&#32467;&#26500;&#35774;&#35745;&#21644;&#29702;&#35299;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#24494;&#32467;&#26500;&#26144;&#23556;&#21040;&#20854;&#21147;&#23398;&#24615;&#33021;&#19978;&#65292;&#21152;&#24555;&#36807;&#31243;&#24182;&#23454;&#29616;&#20174;&#26399;&#26395;&#24615;&#33021;&#29983;&#25104;&#24494;&#32467;&#26500;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increased adoption of reinforced polymer (RP) composite materials, driven by eco-design standards, calls for a fine balance between lightness, stiffness, and effective vibration control. These materials are integral to enhancing comfort, safety, and energy efficiency. Dynamic Mechanical Analysis (DMA) characterizes viscoelastic behavior, yet there's a growing interest in using Machine Learning (ML) to expedite the design and understanding of microstructures. In this paper we aim to map microstructures to their mechanical properties using deep neural networks, speeding up the process and allowing for the generation of microstructures from desired properties.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#31354;&#38388;&#21487;&#21464;&#24615;&#21644;&#27169;&#22411;&#30456;&#20114;&#20381;&#36182;&#30340;&#38382;&#39064;&#65292;&#20934;&#30830;&#21487;&#38752;&#22320;&#39044;&#27979;&#26410;&#26469;&#28023;&#24179;&#38754;&#19978;&#21319;&#65292;&#24182;&#38477;&#20302;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15179</link><description>&lt;p&gt;
&#20943;&#23569;&#28023;&#24179;&#38754;&#19978;&#21319;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#19968;&#31181;&#31354;&#38388;&#21487;&#21464;&#24615;&#24863;&#30693;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reducing Uncertainty in Sea-level Rise Prediction: A Spatial-variability-aware Approach. (arXiv:2310.15179v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15179
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#31354;&#38388;&#21487;&#21464;&#24615;&#21644;&#27169;&#22411;&#30456;&#20114;&#20381;&#36182;&#30340;&#38382;&#39064;&#65292;&#20934;&#30830;&#21487;&#38752;&#22320;&#39044;&#27979;&#26410;&#26469;&#28023;&#24179;&#38754;&#19978;&#21319;&#65292;&#24182;&#38477;&#20302;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22810;&#27169;&#22411;&#38598;&#21512;&#27668;&#20505;&#39044;&#27979;&#65292;&#30446;&#26631;&#26159;&#20934;&#30830;&#21487;&#38752;&#22320;&#39044;&#27979;&#26410;&#26469;&#28023;&#24179;&#38754;&#19978;&#21319;&#65292;&#21516;&#26102;&#38477;&#20302;&#19981;&#30830;&#23450;&#24615;&#12290;&#30001;&#20110;&#27668;&#20505;&#21464;&#21270;&#23545;&#26497;&#22320;&#20912;&#30422;&#21644;&#28023;&#27915;&#30340;&#24433;&#21709;&#65292;&#28023;&#24179;&#38754;&#19978;&#21319;&#24433;&#21709;&#30528;&#27839;&#28023;&#31038;&#21306;&#21644;&#20854;&#20182;&#22320;&#21306;&#30340;&#25968;&#30334;&#19975;&#20154;&#21475;&#65292;&#22240;&#27492;&#36825;&#20010;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;&#30001;&#20110;&#31354;&#38388;&#21464;&#24322;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#22914;&#21487;&#33021;&#30340;&#20020;&#30028;&#28857;&#65288;&#20363;&#22914;&#65292;&#26684;&#38517;&#20848;&#25110;&#35199;&#37096;&#21335;&#26497;&#27954;&#20912;&#26550;&#30340;&#23849;&#22604;&#65289;&#12289;&#27668;&#20505;&#21453;&#39304;&#24490;&#29615;&#65288;&#20363;&#22914;&#65292;&#20113;&#12289;&#27704;&#20037;&#20923;&#22303;&#34701;&#21270;&#65289;&#12289;&#26410;&#26469;&#25919;&#31574;&#20915;&#31574;&#21644;&#20154;&#31867;&#34892;&#20026;&#65292;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#27668;&#20505;&#24314;&#27169;&#26041;&#27861;&#22312;&#22238;&#24402;&#25110;&#28145;&#24230;&#23398;&#20064;&#20013;&#20840;&#23616;&#20351;&#29992;&#30456;&#21516;&#30340;&#26435;&#37325;&#26469;&#32452;&#21512;&#19981;&#21516;&#30340;&#27668;&#20505;&#39044;&#27979;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#20934;&#30830;&#21487;&#38752;&#30340;&#28023;&#24179;&#38754;&#19978;&#21319;&#39044;&#27979;&#38656;&#35201;&#19981;&#21516;&#30340;&#22320;&#21306;&#37319;&#29992;&#19981;&#21516;&#30340;&#21152;&#26435;&#26041;&#26696;&#26159;&#19981;&#22815;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#22495;&#22238;&#24402;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#31354;&#38388;&#21487;&#21464;&#24615;&#21644;&#27169;&#22411;&#30456;&#20114;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given multi-model ensemble climate projections, the goal is to accurately and reliably predict future sea-level rise while lowering the uncertainty. This problem is important because sea-level rise affects millions of people in coastal communities and beyond due to climate change's impacts on polar ice sheets and the ocean. This problem is challenging due to spatial variability and unknowns such as possible tipping points (e.g., collapse of Greenland or West Antarctic ice-shelf), climate feedback loops (e.g., clouds, permafrost thawing), future policy decisions, and human actions. Most existing climate modeling approaches use the same set of weights globally, during either regression or deep learning to combine different climate projections. Such approaches are inadequate when different regions require different weighting schemes for accurate and reliable sea-level rise predictions. This paper proposes a zonal regression model which addresses spatial variability and model inter-depende
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#36203;&#27604;&#23433;&#23398;&#20064;&#21644;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#23454;&#29616;&#20102;&#31070;&#32463;&#27169;&#20223;&#30340;&#35748;&#30693;&#20849;&#21516;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21512;&#25104;&#22797;&#26434;&#30340;&#23383;&#35789;&#24207;&#21015;&#25110;&#20135;&#29983;&#22797;&#26434;&#30340;&#22270;&#20687;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.15177</link><description>&lt;p&gt;
&#36890;&#36807;&#36203;&#27604;&#23433;&#23398;&#20064;&#21644;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#23454;&#29616;&#20102;&#31070;&#32463;&#27169;&#20223;&#30340;&#35748;&#30693;&#20849;&#21516;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Neuro-Mimetic Realization of the Common Model of Cognition via Hebbian Learning and Free Energy Minimization. (arXiv:2310.15177v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15177
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#36203;&#27604;&#23433;&#23398;&#20064;&#21644;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#23454;&#29616;&#20102;&#31070;&#32463;&#27169;&#20223;&#30340;&#35748;&#30693;&#20849;&#21516;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21512;&#25104;&#22797;&#26434;&#30340;&#23383;&#35789;&#24207;&#21015;&#25110;&#20135;&#29983;&#22797;&#26434;&#30340;&#22270;&#20687;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#30340;&#31070;&#32463;&#29983;&#25104;&#27169;&#22411;&#26085;&#30410;&#27969;&#34892;&#65292;&#33021;&#22815;&#21512;&#25104;&#22797;&#26434;&#30340;&#23383;&#35789;&#24207;&#21015;&#25110;&#20135;&#29983;&#22797;&#26434;&#30340;&#22270;&#20687;&#27169;&#24335;&#65292;&#25104;&#20026;&#20102;&#25152;&#35859;"&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;"&#30340;&#28909;&#38376;&#20195;&#34920;&#12290;&#38500;&#20102;&#32473;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24102;&#26469;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#20043;&#22806;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#36824;&#32473;&#35748;&#30693;&#31185;&#23398;&#25552;&#20986;&#20102;&#26377;&#36259;&#30340;&#38382;&#39064;&#65292;&#35813;&#39046;&#22495;&#26088;&#22312;&#25581;&#31034;&#26500;&#25104;&#24605;&#32500;&#21644;&#22823;&#33041;&#36807;&#31243;&#30340;&#26412;&#36136;&#65292;&#20197;&#21450;&#29702;&#35299;&#36825;&#31181;&#21151;&#33021;&#22914;&#20309;&#22312;&#29983;&#29289;&#65288;&#25110;&#20154;&#24037;&#65289;&#22522;&#36136;&#20013;&#33719;&#21462;&#21644;&#23454;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35748;&#20026;&#26377;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#38271;&#26399;&#36884;&#24452;&#22312;&#20110;&#35774;&#35745;&#35748;&#30693;&#26550;&#26500;&#65292;&#36825;&#26159;&#35813;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#30340;&#20256;&#32479;&#65292;&#22522;&#26412;&#19978;&#26159;&#26681;&#25454;&#31070;&#32463;&#27169;&#20223;&#30340;&#29983;&#25104;&#24615;&#26500;&#24314;&#27169;&#22359;&#26500;&#24314;&#30340;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;COGitive NEural GENerative&#31995;&#32479;&#65292;&#23427;&#20197;&#36203;&#27604;&#23433;&#23398;&#20064;&#21644;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#27169;&#20223;&#30340;&#35748;&#30693;&#20849;&#21516;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last few years, large neural generative models, capable of synthesizing intricate sequences of words or producing complex image patterns, have recently emerged as a popular representation of what has come to be known as "generative artificial intelligence" (generative AI). Beyond opening the door to new opportunities as well as challenges for the domain of statistical machine learning, the rising popularity of generative AI brings with it interesting questions for Cognitive Science, which seeks to discover the nature of the processes that underpin minds and brains as well as to understand how such functionality might be acquired and instantiated in biological (or artificial) substrate. With this goal in mind, we argue that a promising long-term pathway lies in the crafting of cognitive architectures, a long-standing tradition of the field, cast fundamentally in terms of neuro-mimetic generative building blocks. Concretely, we discuss the COGnitive Neural GENerative system, whi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#23637;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;&#36825;&#31181;&#23398;&#20064;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#21560;&#25910;&#24191;&#27867;&#36866;&#29992;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20851;&#20110;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20135;&#29983;&#30340;&#20004;&#31181;&#20551;&#35774;&#65292;&#24182;&#23601;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#28508;&#22312;&#39118;&#38505;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.15047</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20803;-&#65288;&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#65289;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta- (out-of-context) learning in neural networks. (arXiv:2310.15047v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15047
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#23637;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;&#36825;&#31181;&#23398;&#20064;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#21560;&#25910;&#24191;&#27867;&#36866;&#29992;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20851;&#20110;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20135;&#29983;&#30340;&#20004;&#31181;&#20551;&#35774;&#65292;&#24182;&#23601;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#28508;&#22312;&#39118;&#38505;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Brown&#31561;&#20154;&#65288;2020&#65289;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#30340;&#21512;&#25104;&#23454;&#39564;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20351;LLMs&#26356;&#23481;&#26131;&#8220;&#20869;&#21270;&#8221;&#25991;&#26412;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#35813;&#25991;&#26412;&#24191;&#27867;&#36866;&#29992;&#65288;&#20363;&#22914;&#30495;&#23454;&#38472;&#36848;&#25110;&#26435;&#23041;&#26469;&#28304;&#30340;&#25991;&#26412;&#65289;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#23427;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#21512;&#25104;&#35745;&#31639;&#26426;&#35270;&#35273;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20551;&#35774;&#65292;&#35299;&#37322;&#20102;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#30340;&#20986;&#29616;&#65306;&#19968;&#31181;&#26159;&#20381;&#36182;&#20110;&#27169;&#22411;&#22312;&#20854;&#21442;&#25968;&#20013;&#23384;&#20648;&#30693;&#35782;&#30340;&#26041;&#24335;&#65292;&#21478;&#19968;&#31181;&#26159;&#26263;&#31034;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#30340;&#38544;&#21547;&#26799;&#24230;&#23545;&#40784;&#20559;&#24046;&#21487;&#33021;&#36127;&#36131;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#33021;&#24847;&#21619;&#30528;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#24605;&#32771;&#65292;&#24182;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/krasheni&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brown et al. (2020) famously introduced the phenomenon of in-context learning in large language models (LLMs). We establish the existence of a phenomenon we call meta-out-of-context learning (meta-OCL) via carefully designed synthetic experiments with LLMs. Our results suggest that meta-OCL leads LLMs to more readily "internalize" the semantic content of text that is, or appears to be, broadly useful (such as true statements, or text from authoritative sources) and use it in appropriate circumstances. We further demonstrate meta-OCL in a synthetic computer vision setting, and propose two hypotheses for the emergence of meta-OCL: one relying on the way models store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descent-based optimizers may be responsible. Finally, we reflect on what our results might imply about capabilities of future AI systems, and discuss potential risks. Our code can be found at https://github.com/krasheni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#25216;&#26415;(MLT)&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#25991;&#26412;&#34920;&#31034;&#26500;&#24314;&#30340;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#32452;&#21512;&#65292;&#22312;&#19981;&#24179;&#34913;&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#38408;&#20540;&#31227;&#21160;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15019</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#20803;&#23398;&#20064;: &#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Meta learning with language models: Challenges and opportunities in the classification of imbalanced text. (arXiv:2310.15019v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#25216;&#26415;(MLT)&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#25991;&#26412;&#34920;&#31034;&#26500;&#24314;&#30340;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#32452;&#21512;&#65292;&#22312;&#19981;&#24179;&#34913;&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#38408;&#20540;&#31227;&#21160;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#36829;&#35268;&#35328;&#35770;&#20869;&#23481;&#26159;&#37325;&#35201;&#20294;&#22256;&#38590;&#30340;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26159;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#38480;&#21046;&#20197;&#21450;&#36829;&#35268;&#23450;&#20041;&#21644;&#25968;&#25454;&#26631;&#27880;&#30340;&#19981;&#19968;&#33268;&#24615;&#31561;&#22240;&#32032;&#65292;&#38590;&#20197;&#31361;&#30772;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#26377;&#38480;&#36164;&#28304;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#25216;&#26415;(MLT)&#65292;&#23427;&#23558;&#20351;&#29992;&#19981;&#21516;&#25991;&#26412;&#34920;&#31034;&#26500;&#24314;&#30340;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#32452;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#35777;&#26126;&#65292;&#25152;&#24471;&#21040;&#30340;&#25216;&#26415;&#22312;&#25968;&#20540;&#19978;&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#20135;&#29983;&#21512;&#29702;&#30340;&#32452;&#21512;&#26435;&#37325;&#12290;&#25105;&#20204;&#23558;MLT&#19982;&#38408;&#20540;&#31227;&#21160;(TM)&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#32452;&#21512;&#39044;&#27979;&#22120;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#20998;&#24067;&#21644;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;MLT&#26041;&#27861;&#30340;&#32479;&#35745;&#20248;&#21183;&#12290;&#25152;&#26377;&#20316;&#32773;&#23545;&#36825;&#39033;&#24037;&#20316;&#36129;&#29486;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting out of policy speech (OOPS) content is important but difficult. While machine learning is a powerful tool to tackle this challenging task, it is hard to break the performance ceiling due to factors like quantity and quality limitations on training data and inconsistencies in OOPS definition and data labeling. To realize the full potential of available limited resources, we propose a meta learning technique (MLT) that combines individual models built with different text representations. We analytically show that the resulting technique is numerically stable and produces reasonable combining weights. We combine the MLT with a threshold-moving (TM) technique to further improve the performance of the combined predictor on highly-imbalanced in-distribution and out-of-distribution datasets. We also provide computational results to show the statistically significant advantages of the proposed MLT approach.  All authors contributed equally to this work.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24555;&#36895;&#20934;&#30830;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26041;&#27861;F-PCMCI&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#37325;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.14925</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#39640;&#25928;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Efficient Causal Discovery for Robotics Applications. (arXiv:2310.14925v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14925
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24555;&#36895;&#20934;&#30830;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26041;&#27861;F-PCMCI&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#37325;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19982;&#20154;&#31867;&#20849;&#20139;&#29615;&#22659;&#30340;&#33258;&#21160;&#21270;&#20219;&#21153;&#20013;&#20351;&#29992;&#26426;&#22120;&#20154;&#65292;&#20363;&#22914;&#20179;&#24211;&#12289;&#36141;&#29289;&#20013;&#24515;&#25110;&#21307;&#38498;&#65292;&#38656;&#35201;&#36825;&#20123;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#38468;&#36817;&#29289;&#20307;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#22522;&#26412;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21019;&#24314;&#34920;&#31034;&#36825;&#20123;&#20803;&#32032;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#26410;&#39044;&#35265;&#21040;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#24182;&#39044;&#27979;&#29305;&#23450;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#36866;&#24212;&#26426;&#22120;&#20154;&#65292;&#22240;&#26524;&#20998;&#26512;&#24517;&#39035;&#21516;&#26102;&#24555;&#36895;&#21644;&#20934;&#30830;&#65292;&#28385;&#36275;&#23454;&#26102;&#38656;&#27714;&#21644;&#22823;&#22810;&#25968;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;F-PCMCI&#30340;&#24555;&#36895;&#20934;&#30830;&#22240;&#26524;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#36827;&#34892;&#20102;&#23454;&#35777;&#28436;&#31034;&#12290;&#35813;&#24212;&#29992;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;F-PCMCI&#22914;&#20309;&#20934;&#30830;&#36805;&#36895;&#22320;&#37325;&#26500;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using robots for automating tasks in environments shared with humans, such as warehouses, shopping centres, or hospitals, requires these robots to comprehend the fundamental physical interactions among nearby agents and objects. Specifically, creating models to represent cause-and-effect relationships among these elements can aid in predicting unforeseen human behaviours and anticipate the outcome of particular robot actions. To be suitable for robots, causal analysis must be both fast and accurate, meeting real-time demands and the limited computational resources typical in most robotics applications. In this paper, we present a practical demonstration of our approach for fast and accurate causal analysis, known as Filtered PCMCI (F-PCMCI), along with a real-world robotics application. The provided application illustrates how our F-PCMCI can accurately and promptly reconstruct the causal model of a human-robot interaction scenario, which can then be leveraged to enhance the quality of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24378;&#35843;&#20102;&#24320;&#21457;&#36825;&#26679;&#30340;&#26816;&#27979;&#22120;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#30340;&#30740;&#31350;&#21019;&#26032;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.14724</link><description>&lt;p&gt;
&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#30340;&#35843;&#26597;&#65306;&#24517;&#35201;&#24615;&#12289;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions. (arXiv:2310.14724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24378;&#35843;&#20102;&#24320;&#21457;&#36825;&#26679;&#30340;&#26816;&#27979;&#22120;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#30340;&#30740;&#31350;&#21019;&#26032;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#22797;&#26434;&#35821;&#35328;&#30340;&#24378;&#22823;&#33021;&#21147;&#20351;&#24471;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#28044;&#20837;&#21040;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#24182;&#24471;&#21040;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#25509;&#21463;&#12290;&#38543;&#30528;LLMs&#30340;&#19981;&#26029;&#25193;&#23637;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#26816;&#27979;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#26816;&#27979;&#22120;&#12290;&#36825;&#23545;&#20110;&#20943;&#23569;LLMs&#28508;&#22312;&#30340;&#35823;&#29992;&#65292;&#24182;&#20445;&#25252;&#33402;&#26415;&#34920;&#36798;&#21644;&#31038;&#20132;&#32593;&#32476;&#31561;&#39046;&#22495;&#20813;&#21463;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#26377;&#23475;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#26088;&#22312;&#30830;&#23450;&#19968;&#27573;&#25991;&#26412;&#26159;&#21542;&#30001;LLM&#29983;&#25104;&#65292;&#23454;&#36136;&#19978;&#26159;&#19968;&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#12290;&#26816;&#27979;&#22120;&#25216;&#26415;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#25512;&#21160;&#22240;&#32032;&#21253;&#25324;&#27700;&#21360;&#25216;&#26415;&#12289;&#38646;&#26679;&#26412;&#26041;&#27861;&#12289;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#12289;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#12289;&#23558;LLMs&#20316;&#20026;&#26816;&#27979;&#22120;&#20197;&#21450;&#20154;&#31867;&#36741;&#21161;&#26041;&#27861;&#30340;&#21019;&#26032;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#27719;&#38598;&#20102;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#30340;&#30740;&#31350;&#31361;&#30772;&#65292;&#24182;&#24378;&#35843;&#20102;&#36843;&#20999;&#30340;&#38656;&#27714;&#21644;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The powerful ability to understand, follow, and generate complex language emerging from large language models (LLMs) makes LLM-generated text flood many areas of our daily lives at an incredible speed and is widely accepted by humans. As LLMs continue to expand, there is an imperative need to develop detectors that can detect LLM-generated text. This is crucial to mitigate potential misuse of LLMs and safeguard realms like artistic expression and social networks from harmful influence of LLM-generated content. The LLM-generated text detection aims to discern if a piece of text was produced by an LLM, which is essentially a binary classification task. The detector techniques have witnessed notable advancements recently, propelled by innovations in watermarking techniques, zero-shot methods, fine-turning LMs methods, adversarial learning methods, LLMs as detectors, and human-assisted methods. In this survey, we collate recent research breakthroughs in this area and underscore the pressin
&lt;/p&gt;</description></item><item><title>CorefPrompt&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#30340;&#20860;&#23481;&#24615;&#26469;&#36827;&#34892;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#12290;&#35813;&#26041;&#27861;&#23558;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#36716;&#21270;&#20026;&#19968;&#20010;&#22635;&#31354;&#24335;MLM&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#30340;&#25552;&#31034;&#20219;&#21153;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#65292;&#26368;&#32456;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.14512</link><description>&lt;p&gt;
CorefPrompt: &#22522;&#20110;&#25552;&#31034;&#30340;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#36890;&#36807;&#27979;&#37327;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#30340;&#20860;&#23481;&#24615;
&lt;/p&gt;
&lt;p&gt;
CorefPrompt: Prompt-based Event Coreference Resolution by Measuring Event Type and Argument Compatibilities. (arXiv:2310.14512v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14512
&lt;/p&gt;
&lt;p&gt;
CorefPrompt&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#30340;&#20860;&#23481;&#24615;&#26469;&#36827;&#34892;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#12290;&#35813;&#26041;&#27861;&#23558;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#36716;&#21270;&#20026;&#19968;&#20010;&#22635;&#31354;&#24335;MLM&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#30340;&#25552;&#31034;&#20219;&#21153;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#65292;&#26368;&#32456;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#26088;&#22312;&#23558;&#25351;&#20195;&#21516;&#19968;&#23454;&#38469;&#20107;&#20214;&#30340;&#20107;&#20214;&#25552;&#21450;&#32858;&#31867;&#22312;&#19968;&#36215;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#8220;&#20808;&#32534;&#30721;&#65292;&#28982;&#21518;&#35780;&#20998;&#8221;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#25351;&#20195;&#28040;&#35299;&#20381;&#36182;&#20110;&#20107;&#20214;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#24456;&#38590;&#21033;&#29992;&#20154;&#24037;&#24635;&#32467;&#30340;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#35268;&#21017;&#65292;&#20363;&#22914;&#65292;&#25351;&#20195;&#21516;&#19968;&#20107;&#20214;&#30340;&#20107;&#20214;&#24212;&#20855;&#26377;&#30456;&#21516;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#20197;&#25351;&#23548;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;CorefPrompt&#65292;&#23558;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#36716;&#21270;&#20026;&#19968;&#20010;&#22635;&#31354;&#24335;MLM&#65288;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65289;&#20219;&#21153;&#12290;&#36825;&#26679;&#21487;&#20197;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#27169;&#26495;&#20013;&#21516;&#26102;&#36827;&#34892;&#20107;&#20214;&#24314;&#27169;&#21644;&#25351;&#20195;&#28040;&#35299;&#21028;&#21035;&#65292;&#24182;&#19988;&#20855;&#26377;&#23436;&#20840;&#20849;&#20139;&#30340;&#19978;&#19979;&#25991;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#36741;&#21161;&#30340;&#25552;&#31034;&#20219;&#21153;&#65292;&#20107;&#20214;&#31867;&#22411;&#20860;&#23481;&#24615;&#21644;&#21442;&#25968;&#20860;&#23481;&#24615;&#65292;&#20197;&#26126;&#30830;&#23637;&#31034;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#24110;&#21161;&#27169;&#22411;&#20570;&#20986;&#26368;&#32456;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;CorefPrompt&#22312;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event coreference resolution (ECR) aims to group event mentions referring to the same real-world event into clusters. Most previous studies adopt the "encoding first, then scoring" framework, making the coreference judgment rely on event encoding. Furthermore, current methods struggle to leverage human-summarized ECR rules, e.g., coreferential events should have the same event type, to guide the model. To address these two issues, we propose a prompt-based approach, CorefPrompt, to transform ECR into a cloze-style MLM (masked language model) task. This allows for simultaneous event modeling and coreference discrimination within a single template, with a fully shared context. In addition, we introduce two auxiliary prompt tasks, event-type compatibility and argument compatibility, to explicitly demonstrate the reasoning process of ECR, which helps the model make final predictions. Experimental results show that our method CorefPrompt performs well in a state-of-the-art (SOTA) benchmark.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38543;&#26426;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#21521;&#23548;&#25968;&#26469;&#26356;&#26032;&#21442;&#25968;&#12290;&#31639;&#27861;&#36890;&#36807;&#37319;&#26679;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#38543;&#26426;&#26041;&#21521;&#65292;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.14168</link><description>&lt;p&gt;
&#38543;&#26426;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Randomized Forward Mode of Automatic Differentiation for Optimization Algorithms. (arXiv:2310.14168v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38543;&#26426;&#21069;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#21521;&#23548;&#25968;&#26469;&#26356;&#26032;&#21442;&#25968;&#12290;&#31639;&#27861;&#36890;&#36807;&#37319;&#26679;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#38543;&#26426;&#26041;&#21521;&#65292;&#20351;&#29992;&#27491;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21453;&#21521;&#20256;&#25773;&#21033;&#29992;&#20102;&#33258;&#21160;&#24494;&#20998;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#21363;&#21453;&#21521;&#27169;&#24335;&#24494;&#20998;&#65292;&#25110;&#31216;&#20026;&#21521;&#37327;&#38597;&#21487;&#27604;&#20056;&#31215;(VJP)&#65292;&#25110;&#22312;&#24494;&#20998;&#20960;&#20309;&#30340;&#32972;&#26223;&#19979;&#34987;&#31216;&#20026;&#25289;&#22238;&#36807;&#31243;&#12290;&#26799;&#24230;&#30340;&#35745;&#31639;&#23545;&#20110;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26356;&#26032;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#36807;&#27491;&#21521;&#27169;&#24335;AD&#25110;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;(JVP)&#39640;&#25928;&#35745;&#31639;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#21521;&#23548;&#25968;&#26469;&#26356;&#26032;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;&#36825;&#20123;JVP&#27839;&#30528;&#20174;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#65288;&#20363;&#22914;&#20271;&#21162;&#21033;&#12289;&#27491;&#24577;&#12289;&#32500;&#26684;&#32435;&#12289;&#25289;&#26222;&#25289;&#26031;&#21644;&#22343;&#21248;&#20998;&#24067;&#65289;&#37319;&#26679;&#30340;&#38543;&#26426;&#26041;&#21521;&#35745;&#31639;&#12290;&#26799;&#24230;&#30340;&#35745;&#31639;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#36827;&#34892;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#21253;&#25324;&#25910;&#25947;&#36895;&#24230;&#20197;&#21450;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation within neural networks leverages a fundamental element of automatic differentiation, which is referred to as the reverse mode differentiation, or vector Jacobian Product (VJP) or, in the context of differential geometry, known as the pull-back process. The computation of gradient is important as update of neural network parameters is performed using gradient descent method. In this study, we present a genric randomized method, which updates the parameters of neural networks by using directional derivatives of loss functions computed efficiently by using forward mode AD or Jacobian vector Product (JVP). These JVP are computed along the random directions sampled from different probability distributions e.g., Bernoulli, Normal, Wigner, Laplace and Uniform distributions. The computation of gradient is performed during the forward pass of the neural network. We also present a rigorous analysis of the presented methods providing the rate of convergence along with the computat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;COMET&#30340;&#21019;&#26032;&#23618;&#27425;&#23545;&#27604;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#24320;&#21457;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;COMET&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14017</link><description>&lt;p&gt;
&#20840;&#38754;&#23545;&#27604;&#65306;&#38754;&#21521;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#23618;&#27425;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series. (arXiv:2310.14017v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;COMET&#30340;&#21019;&#26032;&#23618;&#27425;&#23545;&#27604;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#24320;&#21457;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#29305;&#24615;&#65292;&#24182;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;COMET&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20943;&#23569;&#20102;&#23545;&#21171;&#21160;&#23494;&#38598;&#12289;&#39046;&#22495;&#29305;&#23450;&#21644;&#31232;&#32570;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#25968;&#25454;&#23618;&#38754;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#30340;&#22797;&#26434;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COMET&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#23618;&#27425;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#20013;&#25152;&#26377;&#20869;&#22312;&#23618;&#32423;&#30340;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#22411;&#31995;&#32479;&#22320;&#25429;&#25417;&#20102;&#26469;&#33258;&#22235;&#20010;&#28508;&#22312;&#23618;&#32423;&#30340;&#25968;&#25454;&#19968;&#33268;&#24615;&#65306;&#35266;&#27979;&#12289;&#26679;&#26412;&#12289;&#35797;&#39564;&#21644;&#24739;&#32773;&#23618;&#32423;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#24320;&#21457;&#23545;&#27604;&#25439;&#22833;&#65292;&#25105;&#20204;&#21487;&#20197;&#23398;&#20064;&#21040;&#20445;&#25345;&#20840;&#38754;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#23454;&#29616;&#33258;&#30417;&#30563;&#26041;&#27861;&#20013;&#30340;&#20449;&#24687;&#26368;&#22823;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29420;&#31435;&#24739;&#32773;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#23558;COMET&#19982;&#20845;&#20010;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive representation learning is crucial in medical time series analysis as it alleviates dependency on labor-intensive, domain-specific, and scarce expert annotations. However, existing contrastive learning methods primarily focus on one single data level, which fails to fully exploit the intricate nature of medical time series. To address this issue, we present COMET, an innovative hierarchical framework that leverages data consistencies at all inherent levels in medical time series. Our meticulously designed model systematically captures data consistency from four potential levels: observation, sample, trial, and patient levels. By developing contrastive loss at multiple levels, we can learn effective representations that preserve comprehensive data consistency, maximizing information utilization in a self-supervised manner. We conduct experiments in the challenging patient-independent setting. We compare COMET against six baselines using three diverse datasets, which include 
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#20551;&#35774;&#38169;&#35823;&#20197;&#21450;&#20248;&#21270;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13639</link><description>&lt;p&gt;
&#23545;&#27604;&#20559;&#22909;&#23398;&#20064;&#65306;&#23398;&#20064;&#29992;&#25143;&#21453;&#39304;&#32780;&#26080;&#38656;RL&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Contrastive Preference Learning: Learning from Human Feedback without RL. (arXiv:2310.13639v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13639
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#20551;&#35774;&#38169;&#35823;&#20197;&#21450;&#20248;&#21270;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#36890;&#24120;&#30340;RLHF&#31639;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#65292;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20248;&#21270;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#20197;&#23545;&#40784;&#27169;&#22411;&#12290;&#36825;&#31181;&#33539;&#24335;&#20551;&#35774;&#20154;&#31867;&#20559;&#22909;&#26159;&#26681;&#25454;&#22870;&#21169;&#20998;&#24067;&#30340;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23454;&#38469;&#19978;&#23427;&#20204;&#36981;&#24490;&#29992;&#25143;&#26368;&#20339;&#31574;&#30053;&#19979;&#30340;&#36951;&#25022;&#12290;&#22240;&#27492;&#65292;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#19981;&#20165;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#38169;&#35823;&#20551;&#35774;&#65292;&#36824;&#23548;&#33268;&#20102;&#30001;&#20110;&#31574;&#30053;&#26799;&#24230;&#25110;RL&#38454;&#27573;&#30340;&#33258;&#21161;&#27861;&#24341;&#36215;&#30340;&#26840;&#25163;&#30340;&#20248;&#21270;&#25361;&#25112;&#12290;&#30001;&#20110;&#36825;&#20123;&#20248;&#21270;&#25361;&#25112;&#65292;&#24403;&#20195;&#30340;RLHF&#26041;&#27861;&#38480;&#21046;&#33258;&#24049;&#21482;&#33021;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#25110;&#38480;&#21046;&#20102;&#35266;&#27979;&#32500;&#24230;&#65288;&#22914;&#22522;&#20110;&#29366;&#24577;&#30340;&#26426;&#22120;&#20154;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;"
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new famil
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#39118;&#26684;&#25216;&#26415;&#25552;&#21319;&#25351;&#32441;&#27963;&#20307;&#26816;&#27979;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;LivDet 2023&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13573</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#39118;&#26684;&#25216;&#26415;&#25552;&#21319;&#25351;&#32441;&#27963;&#20307;&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Generalization with Adaptive Style Techniques for Fingerprint Liveness Detection. (arXiv:2310.13573v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#39118;&#26684;&#25216;&#26415;&#25552;&#21319;&#25351;&#32441;&#27963;&#20307;&#26816;&#27979;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;LivDet 2023&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24615;&#33021;&#20248;&#24322;&#30340;&#25351;&#32441;&#27963;&#20307;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#22312;LivDet 2023&#25351;&#32441;&#34920;&#31034;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;94.68%&#65292;&#22312;LivDet 2023&#21160;&#20316;&#20013;&#30340;&#27963;&#20307;&#26816;&#27979;&#20013;&#33719;&#24471;&#20102;&#31532;&#20108;&#21517;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#39118;&#26684;&#36716;&#25442;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#38754;&#23545;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;LivDet 2023&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a high-performance fingerprint liveness feature extraction technique that secured first place in LivDet 2023 Fingerprint Representation Challenge. Additionally, we developed a practical fingerprint recognition system with 94.68% accuracy, earning second place in LivDet 2023 Liveness Detection in Action. By investigating various methods, particularly style transfer, we demonstrate improvements in accuracy and generalization when faced with limited training data. As a result, our approach achieved state-of-the-art performance in LivDet 2023 Challenges.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.13548</link><description>&lt;p&gt;
&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12300;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12301;&#26159;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#19968;&#31181;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;RLHF&#21487;&#33021;&#20250;&#40723;&#21169;&#27169;&#22411;&#36890;&#36807;&#19982;&#29992;&#25143;&#20449;&#24565;&#30456;&#31526;&#30340;&#22238;&#31572;&#26469;&#20195;&#26367;&#30495;&#23454;&#22238;&#31572;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#35844;&#23194;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;RLHF&#35757;&#32451;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#20197;&#21450;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#26159;&#21542;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20116;&#20010;&#26368;&#20808;&#36827;&#30340;AI&#21161;&#25163;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#33258;&#30001;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#19968;&#36143;&#34920;&#29616;&#20986;&#35844;&#23194;&#34892;&#20026;&#12290;&#20026;&#20102;&#29702;&#35299;&#20154;&#31867;&#20559;&#22909;&#26159;&#21542;&#39537;&#21160;&#20102;RLHF&#27169;&#22411;&#30340;&#36825;&#31181;&#24191;&#27867;&#34892;&#20026;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#22238;&#31572;&#19982;&#29992;&#25143;&#30340;&#35266;&#28857;&#30456;&#31526;&#26102;&#65292;&#23427;&#26356;&#26377;&#21487;&#33021;&#34987;&#36873;&#20013;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#20559;&#22909;&#27169;&#22411;&#65288;PMs&#65289;&#23558;&#26377;&#35828;&#26381;&#21147;&#30340;&#35844;&#23194;&#22238;&#31572;&#19982;&#27491;&#30830;&#22238;&#31572;&#30456;&#27604;&#65292;&#26377;&#26102;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#22320;&#36873;&#25321;&#20102;&#35844;&#23194;&#22238;&#31572;&#12290;&#20248;&#21270;&#27169;&#22411;&#36755;&#20986;&#20197;&#28385;&#36275;PMs&#26377;&#26102;&#20063;&#20250;&#22312;&#30495;&#23454;&#24615;&#21644;&#35844;&#23194;&#34892;&#20026;&#20043;&#38388;&#20570;&#20986;&#21462;&#33293;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28165;&#29702;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#22122;&#22768;&#36755;&#20837;&#65292;&#36890;&#36807;&#20174;MTNT&#25968;&#25454;&#38598;&#20013;&#28165;&#29702;&#30446;&#26631;&#35821;&#21477;&#30340;&#22122;&#22768;&#65292;&#29983;&#25104;&#20102;C-MTNT&#25968;&#25454;&#38598;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2310.13469</link><description>&lt;p&gt;
&#35753;&#35821;&#35328;&#27169;&#22411;&#28165;&#29702;&#24744;&#30340;&#26377;&#22122;&#38899;&#30340;&#32763;&#35793;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Ask Language Model to Clean Your Noisy Translation Data. (arXiv:2310.13469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13469
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28165;&#29702;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#22122;&#22768;&#36755;&#20837;&#65292;&#36890;&#36807;&#20174;MTNT&#25968;&#25454;&#38598;&#20013;&#28165;&#29702;&#30446;&#26631;&#35821;&#21477;&#30340;&#22122;&#22768;&#65292;&#29983;&#25104;&#20102;C-MTNT&#25968;&#25454;&#38598;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#22122;&#22768;&#36755;&#20837;&#30340;&#33030;&#24369;&#24615;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20174;&#22122;&#22768;&#36755;&#20837;&#20013;&#29983;&#25104;&#24178;&#20928;&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;MTNT&#25968;&#25454;&#38598;&#34987;&#24191;&#27867;&#29992;&#20316;&#35780;&#20272;NMT&#27169;&#22411;&#23545;&#22122;&#22768;&#36755;&#20837;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28304;&#35821;&#21477;&#21644;&#30446;&#26631;&#35821;&#21477;&#20013;&#37117;&#23384;&#22312;&#22122;&#22768;&#65292;&#20854;&#23454;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28165;&#29702;MTNT&#20013;&#30446;&#26631;&#35821;&#21477;&#30340;&#22122;&#22768;&#65292;&#20351;&#20854;&#26356;&#36866;&#29992;&#20110;&#22122;&#22768;&#35780;&#20272;&#30340;&#22522;&#20934;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#21435;&#22122;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#32771;&#34385;&#35821;&#20041;&#21547;&#20041;&#30340;&#21516;&#26102;&#21024;&#38500;&#34920;&#24773;&#31526;&#21495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#26356;&#25913;&#20442;&#35821;&#12289;&#26415;&#35821;&#21644;&#31895;&#21475;&#12290;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#34987;&#31216;&#20026;C-MTNT&#65292;&#22122;&#22768;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have demonstrated remarkable performance in neural machine translation (NMT). However, their vulnerability to noisy input poses a significant challenge in practical implementation, where generating clean output from noisy input is crucial. The MTNT dataset \cite{MTNT} is widely used as a benchmark for evaluating the robustness of NMT models against noisy input. Nevertheless, its utility is limited due to the presence of noise in both the source and target sentences. To address this limitation, we focus on cleaning the noise from the target sentences in MTNT, making it more suitable as a benchmark for noise evaluation. Leveraging the capabilities of large language models (LLMs), we observe their impressive abilities in noise removal. For example, they can remove emojis while considering their semantic meaning. Additionally, we show that LLM can effectively rephrase slang, jargon, and profanities. The resulting datasets, called C-MTNT, exhibit significantly less noise 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;CO2&#25490;&#25918;&#21644;&#34892;&#39542;&#26102;&#38388;&#31561;&#25351;&#26631;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13129</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;CO2&#25490;&#25918;&#30340;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-based Intelligent Traffic Signal Controls with Optimized CO2 emissions. (arXiv:2310.13129v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;CO2&#25490;&#25918;&#21644;&#34892;&#39542;&#26102;&#38388;&#31561;&#25351;&#26631;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20132;&#36890;&#32593;&#32476;&#38754;&#20020;&#30528;&#27425;&#20248;&#25511;&#21046;&#31574;&#30053;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#23545;&#20154;&#31867;&#20581;&#24247;&#12289;&#29615;&#22659;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182; contribute to traffic congestion. &#30001;&#20110;&#20132;&#36890;&#25317;&#22581;&#23548;&#33268;&#30340;&#31354;&#27668;&#27745;&#26579;&#27700;&#24179;&#19978;&#21319;&#21644;&#36890;&#21220;&#26102;&#38388;&#24310;&#38271;&#65292;&#20132;&#21449;&#36335;&#21475;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#22120;&#25104;&#20026;&#29616;&#20195;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#26377;&#20960;&#20010;&#33258;&#36866;&#24212;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#22120;&#65292;&#20294;&#23545;&#20854;&#27604;&#36739;&#24615;&#33021;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#20108;&#27687;&#21270;&#30899;&#65288;CO2&#65289;&#25490;&#25918;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#25991;&#29486;&#23545;&#35813;&#39046;&#22495;&#20851;&#27880;&#19981;&#22815;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EcoLight&#65292;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22870;&#21169;&#22609;&#36896;&#26041;&#26696;&#65292;&#19981;&#20165;&#33021;&#20943;&#23569;CO2&#25490;&#25918;&#65292;&#36824;&#33021;&#22312;&#35832;&#22914;&#34892;&#39542;&#26102;&#38388;&#31561;&#25351;&#26631;&#19978;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#39542;&#26102;&#38388;&#12289;CO2&#25490;&#25918;&#12289;&#31561;&#25351;&#26631;&#27604;&#36739;&#20102;&#34920;&#26684;&#24335;Q-Learning&#12289;DQN&#12289;SARSA&#21644;A2C&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, transportation networks face the challenge of sub-optimal control policies that can have adverse effects on human health, the environment, and contribute to traffic congestion. Increased levels of air pollution and extended commute times caused by traffic bottlenecks make intersection traffic signal controllers a crucial component of modern transportation infrastructure. Despite several adaptive traffic signal controllers in literature, limited research has been conducted on their comparative performance. Furthermore, despite carbon dioxide (CO2) emissions' significance as a global issue, the literature has paid limited attention to this area. In this report, we propose EcoLight, a reward shaping scheme for reinforcement learning algorithms that not only reduces CO2 emissions but also achieves competitive results in metrics such as travel time. We compare the performance of tabular Q-Learning, DQN, SARSA, and A2C algorithms using metrics such as travel time, CO2 emissions, wa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.13121</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#20013;&#30340;&#21152;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20687;Transformer&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#23545;&#20110;&#20854;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#35813;&#27169;&#22411;&#24320;&#22987;&#35745;&#31639;&#36739;&#26202;&#65292;&#20294;&#25191;&#34892;&#36895;&#24230;&#38750;&#24120;&#24555;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20104;&#20197;&#35299;&#37322;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35814;&#32454;&#35299;&#37322;&#20102;&#35813;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#21644;&#25968;&#23398;&#24314;&#27169;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#24191;&#27867;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20998;&#26512;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#22810;&#23618;Transformer&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#33258;&#27880;&#24847;&#26426;&#21046;&#20013;QKV&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.11398</link><description>&lt;p&gt;
&#31070;&#32463;&#27880;&#24847;&#21147;&#65306;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#33258;&#27880;&#24847;&#26426;&#21046;&#20013;&#30340;QKV&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks. (arXiv:2310.11398v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#33258;&#27880;&#24847;&#26426;&#21046;&#20013;QKV&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#20027;&#35201;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#26469;&#35745;&#31639;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;(QKV)&#65292;&#20294;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20248;&#36873;&#25321;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;QKV&#35745;&#31639;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#29305;&#27530;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#35745;&#31639;&#12290;&#36890;&#36807;&#22312;IWSLT 2017&#24503;&#33521;&#32763;&#35793;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;Marian&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;BLEU&#24471;&#20998;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;Wikitext-103&#25968;&#25454;&#38598;&#35757;&#32451;Roberta&#27169;&#22411;&#26102;&#20063;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of deep learning, the self-attention mechanism has substantiated its pivotal role across a myriad of tasks, encompassing natural language processing and computer vision. Despite achieving success across diverse applications, the traditional self-attention mechanism primarily leverages linear transformations for the computation of query, key, and value (QKV), which may not invariably be the optimal choice under specific circumstances. This paper probes into a novel methodology for QKV computation-implementing a specially-designed neural network structure for the calculation. Utilizing a modified Marian model, we conducted experiments on the IWSLT 2017 German-English translation task dataset and juxtaposed our method with the conventional approach. The experimental results unveil a significant enhancement in BLEU scores with our method. Furthermore, our approach also manifested superiority when training the Roberta model with the Wikitext-103 dataset, reflecting a notable re
&lt;/p&gt;</description></item><item><title>&#22312;&#29992;&#25143;&#24314;&#27169;&#20013;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22686;&#24378;&#33258;&#30417;&#30563;&#25490;&#24207;&#26041;&#27861;&#39044;&#35757;&#32451;&#29992;&#25143;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#21644;&#29616;&#26377;&#22686;&#24378;&#26041;&#27861;&#24341;&#20837;&#30340;&#22122;&#38899;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09706</link><description>&lt;p&gt;
AdaptSSR: &#20351;&#29992;&#33258;&#36866;&#24212;&#22686;&#24378;&#33258;&#30417;&#30563;&#25490;&#24207;&#26041;&#27861;&#39044;&#35757;&#32451;&#29992;&#25143;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AdaptSSR: Pre-training User Model with Augmentation-Adaptive Self-Supervised Ranking. (arXiv:2310.09706v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09706
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29992;&#25143;&#24314;&#27169;&#20013;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22686;&#24378;&#33258;&#30417;&#30563;&#25490;&#24207;&#26041;&#27861;&#39044;&#35757;&#32451;&#29992;&#25143;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#21644;&#29616;&#26377;&#22686;&#24378;&#26041;&#27861;&#24341;&#20837;&#30340;&#22122;&#38899;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#26088;&#22312;&#25429;&#25417;&#29992;&#25143;&#30340;&#29305;&#24449;&#25110;&#20852;&#36259;&#65292;&#20294;&#21463;&#21040;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24448;&#24448;&#38656;&#35201;&#20381;&#36182;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#20960;&#39033;&#30740;&#31350;&#36890;&#36807;&#22312;&#22823;&#37327;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19968;&#33324;&#32780;&#35328;&#65292;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26500;&#24314;&#30340;&#21516;&#19968;&#34892;&#20026;&#24207;&#21015;&#30340;&#19981;&#21516;&#35270;&#22270;&#22312;&#35821;&#20041;&#19978;&#26159;&#19968;&#33268;&#30340;&#65292;&#21363;&#21453;&#26144;&#29992;&#25143;&#30340;&#30456;&#20284;&#29305;&#24449;&#25110;&#20852;&#36259;&#65292;&#24182;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26368;&#22823;&#21270;&#23427;&#20204;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29992;&#25143;&#34892;&#20026;&#30340;&#22810;&#26679;&#20852;&#36259;&#21644;&#22823;&#37327;&#22122;&#38899;&#65292;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#24448;&#24448;&#20250;&#20002;&#22833;&#26576;&#20123;&#29992;&#25143;&#29305;&#24449;&#25110;&#24341;&#20837;&#22122;&#22768;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#30452;&#25509;&#26368;&#22823;&#21270;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21487;&#33021;&#23548;&#33268;&#36127;&#38754;&#36801;&#31227;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#29992;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26367;&#20195;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65306;&#33258;&#36866;&#24212;&#22686;&#24378;&#33258;&#30417;&#30563;&#25490;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
User modeling, which aims to capture users' characteristics or interests, heavily relies on task-specific labeled data and suffers from the data sparsity issue. Several recent studies tackled this problem by pre-training the user model on massive user behavior sequences with a contrastive learning task. Generally, these methods assume different views of the same behavior sequence constructed via data augmentation are semantically consistent, i.e., reflecting similar characteristics or interests of the user, and thus maximizing their agreement in the feature space. However, due to the diverse interests and heavy noise in user behaviors, existing augmentation methods tend to lose certain characteristics of the user or introduce noisy behaviors. Thus, forcing the user model to directly maximize the similarity between the augmented views may result in a negative transfer. To this end, we propose to replace the contrastive learning task with a new pretext task: Augmentation-Adaptive SelfSup
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08644</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#24314;&#27169;&#20013;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems. (arXiv:2310.08644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25968;&#21313;&#24180;&#26469;&#33268;&#21147;&#20110;&#26500;&#24314;&#29992;&#20110;&#39044;&#27979;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#26102;&#38388;&#24207;&#21015;&#28436;&#21270;&#30340;&#29289;&#29702;-&#27010;&#24565; (PC) &#27169;&#22411;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064; (ML) &#30340;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20174;ML&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#29289;&#29702;&#29702;&#35299;&#30340;&#22256;&#38590;&#20351;&#24471;&#20854;&#22312;&#22686;&#24378;&#23545;&#31995;&#32479;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#31185;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#24212;&#29992;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35299;&#29289;&#29702;&#24615;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120; (MCP) &#20316;&#20026;&#24357;&#21512;PC&#27169;&#22411;&#21644;ML&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;MCP&#21033;&#29992;PC&#27169;&#22411;&#21644;GRNNs&#32972;&#21518;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#20869;&#22312;&#21516;&#26500;&#24615;&#65292;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26126;&#30830;&#34920;&#31034;&#29289;&#29702;&#36807;&#31243;&#30340;&#36136;&#37327;&#20445;&#25345;&#24615;&#36136;&#65292;&#21516;&#26102;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#21644;&#29616;&#25104;&#30340;ML&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36825;&#31181;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#65288;&#21487;&#35299;&#37322;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although decades of effort have been devoted to building Physical-Conceptual (PC) models for predicting the time-series evolution of geoscientific systems, recent work shows that Machine Learning (ML) based Gated Recurrent Neural Network technology can be used to develop models that are much more accurate. However, the difficulty of extracting physical understanding from ML-based models complicates their utility for enhancing scientific knowledge regarding system structure and function. Here, we propose a physically-interpretable Mass Conserving Perceptron (MCP) as a way to bridge the gap between PC-based and ML-based modeling approaches. The MCP exploits the inherent isomorphism between the directed graph structures underlying both PC models and GRNNs to explicitly represent the mass-conserving nature of physical processes while enabling the functional nature of such processes to be directly learned (in an interpretable manner) from available data using off-the-shelf ML technology. As
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#37051;&#24863;&#30693;&#34920;&#31034;&#30340;&#20869;&#23384;&#39640;&#25928;&#30340;&#20301;&#32622;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#26550;&#26500;&#26469;&#35299;&#20915;&#31232;&#30095;&#24615;&#21644;&#22320;&#29702;&#20449;&#24687;&#25972;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.06484</link><description>&lt;p&gt;
&#20869;&#23384;&#39640;&#25928;&#30340;&#20301;&#32622;&#25512;&#33616;&#36890;&#36807;&#36817;&#37051;&#24863;&#30693;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Memory efficient location recommendation through proximity-aware representation. (arXiv:2310.06484v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06484
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#37051;&#24863;&#30693;&#34920;&#31034;&#30340;&#20869;&#23384;&#39640;&#25928;&#30340;&#20301;&#32622;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#26550;&#26500;&#26469;&#35299;&#20915;&#31232;&#30095;&#24615;&#21644;&#22320;&#29702;&#20449;&#24687;&#25972;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#20301;&#32622;&#25512;&#33616;&#22312;&#29616;&#20195;&#29983;&#27963;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21487;&#20197;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#65292;&#32473;&#20225;&#19994;&#24102;&#26469;&#26356;&#22810;&#30340;&#21033;&#28070;&#24182;&#21327;&#21161;&#25919;&#24220;&#34892;&#25919;&#24037;&#20316;&#12290;&#23613;&#31649;&#38543;&#30528;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#20301;&#32622;&#25512;&#33616;&#30340;&#26041;&#27861;&#24050;&#32463;&#26377;&#20102;&#26174;&#33879;&#36827;&#27493;&#65292;&#20294;&#22320;&#29702;&#20449;&#24687;&#30340;&#21033;&#29992;&#20173;&#28982;&#26377;&#38480;&#65292;&#21516;&#26102;&#36824;&#38754;&#20020;&#30528;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#37051;&#24863;&#30693;&#30340;&#39034;&#24207;&#25512;&#33616;&#21306;&#22495;&#34920;&#31034;&#26041;&#27861;&#65288;&#31616;&#31216;PASR&#65289;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#26550;&#26500;&#26500;&#24314;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#21019;&#26032;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#24378;&#35843;&#20449;&#24687;&#20016;&#23500;&#30340;&#36127;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;PASR&#36890;&#36807;&#22312;&#27599;&#20010;GPS&#28857;&#30340;&#20998;&#23618;&#32593;&#26684;&#21644;&#36817;&#37051;&#32593;&#26684;&#20013;&#20351;&#29992;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#22320;&#29702;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#22320;&#29702;&#20449;&#24687;&#30340;&#25972;&#21512;&#12290;&#20026;&#36827;&#19968;&#27493;&#21033;&#29992;&#22320;&#29702;&#20449;&#24687;&#65292;&#25105;&#20204;&#20351;&#29992;&#36817;&#37051;&#32593;&#26684;&#21644;POI&#31867;&#21035;&#30340;&#37096;&#20998;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#36827;&#34892;&#20301;&#32622;&#21521;&#37327;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential location recommendation plays a huge role in modern life, which can enhance user experience, bring more profit to businesses and assist in government administration. Although methods for location recommendation have evolved significantly thanks to the development of recommendation systems, there is still limited utilization of geographic information, along with the ongoing challenge of addressing data sparsity. In response, we introduce a Proximity-aware based region representation for Sequential Recommendation (PASR for short), built upon the Self-Attention Network architecture. We tackle the sparsity issue through a novel loss function employing importance sampling, which emphasizes informative negative samples during optimization. Moreover, PASR enhances the integration of geographic information by employing a self-attention-based geography encoder to the hierarchical grid and proximity grid at each GPS point. To further leverage geographic information, we utilize the pro
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#21453;&#22270;&#28789;&#27979;&#35797;&#65288;CT^2&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#29616;&#26377;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#38754;&#23545;&#29983;&#25104;AI&#30340;&#39118;&#38505;&#21644;&#21518;&#26524;&#24341;&#36215;&#20851;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;AI&#29983;&#25104;&#20316;&#21697;&#24402;&#23646;&#38382;&#39064;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.05030</link><description>&lt;p&gt;
&#21453;&#22270;&#28789;&#27979;&#35797;CT^2&#65306;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#27809;&#26377;&#20320;&#24819;&#35937;&#30340;&#37027;&#20040;&#23481;&#26131;&#8212;&#8212;&#24341;&#20837;AI&#21487;&#26816;&#27979;&#24615;&#25351;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as You May Think -- Introducing AI Detectability Index. (arXiv:2310.05030v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05030
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#21453;&#22270;&#28789;&#27979;&#35797;&#65288;CT^2&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#29616;&#26377;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#38754;&#23545;&#29983;&#25104;AI&#30340;&#39118;&#38505;&#21644;&#21518;&#26524;&#24341;&#36215;&#20851;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;AI&#29983;&#25104;&#20316;&#21697;&#24402;&#23646;&#38382;&#39064;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#31561;&#29983;&#25104;AI&#30340;&#20852;&#36215;&#65292;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#39118;&#38505;&#21644;&#21518;&#26524;&#36234;&#26469;&#36234;&#24341;&#36215;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;AI&#29983;&#25104;&#20316;&#21697;&#30340;&#24402;&#23646;&#38382;&#39064;&#65292;&#32654;&#22269;&#29256;&#26435;&#23616;&#21457;&#24067;&#20102;&#19968;&#20221;&#22768;&#26126;&#65292;&#25351;&#20986;&#8220;&#22914;&#26524;&#19968;&#20214;&#20316;&#21697;&#30340;&#20256;&#32479;&#21019;&#20316;&#20803;&#32032;&#30001;&#26426;&#22120;&#29983;&#25104;&#65292;&#37027;&#20040;&#36825;&#20214;&#20316;&#21697;&#23601;&#32570;&#20047;&#20154;&#31867;&#21019;&#20316;&#65292;&#29256;&#26435;&#23616;&#23558;&#19981;&#20250;&#30331;&#35760;&#23427;&#8221;&#12290;&#27492;&#22806;&#65292;&#32654;&#22269;&#21644;&#27431;&#30431;&#25919;&#24220;&#26368;&#36817;&#20063;&#33609;&#25311;&#20102;&#20851;&#20110;AI&#30417;&#31649;&#26694;&#26550;&#30340;&#21021;&#27493;&#25552;&#26696;&#12290;&#22312;&#36825;&#31181;&#23545;&#29983;&#25104;AI&#30340;&#20851;&#27880;&#20013;&#65292;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#21463;&#21040;&#30740;&#31350;&#31435;&#21363;&#20851;&#27880;&#30340;&#35805;&#39064;&#65292;&#19968;&#20123;&#21021;&#27493;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#38543;&#21518;&#20986;&#29616;&#20102;&#32469;&#36807;&#26816;&#27979;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21453;&#22270;&#28789;&#27979;&#35797;&#65288;CT^2&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#29616;&#26377;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office released a statement stating that 'If a work's traditional elements of authorship were produced by a machine, the work lacks human authorship and the Office will not register it'. Furthermore, both the US and the EU governments have recently drafted their initial proposals regarding the regulatory framework for AI. Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by emergence of techniques to bypass detection. This paper introduces the Counter Turing Test (CT^2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques. Our em
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01320</link><description>&lt;p&gt;
Avalon&#30340;&#24605;&#32771;&#28216;&#25103;&#65306;&#36890;&#36807;&#36882;&#24402;&#24605;&#32771;&#23545;&#25239;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation. (arXiv:2310.01320v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#22312;LLM&#20316;&#20026;&#26234;&#33021;&#20307;&#39046;&#22495;&#30340;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26222;&#36941;&#30340;&#20551;&#35774;&#26159;LLM&#22788;&#29702;&#30340;&#20449;&#24687;&#22987;&#32456;&#26159;&#35802;&#23454;&#30340;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#31038;&#20250;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#27450;&#39575;&#25110;&#35823;&#23548;&#24615;&#20449;&#24687;&#12290;&#36825;&#20010;&#30095;&#24573;&#20351;&#24471;LLM&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25805;&#32437;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21033;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#25506;&#32034;LLM&#22312;&#27450;&#39575;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#12290;Avalon&#20805;&#28385;&#20102;&#38169;&#35823;&#20449;&#24687;&#65292;&#24182;&#38656;&#35201;&#22797;&#26434;&#30340;&#36923;&#36753;&#65292;&#34920;&#29616;&#20026;&#8220;&#24605;&#32771;&#30340;&#28216;&#25103;&#8221;&#12290;&#21463;&#21040;&#20154;&#31867;&#22312;Avalon&#28216;&#25103;&#20013;&#36882;&#24402;&#24605;&#32771;&#21644;&#36879;&#35270;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#65292;&#20197;&#22686;&#24378;LLM&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;ReCon&#32467;&#21512;&#20102;&#20844;&#24335;&#21270;&#24605;&#32771;&#21644;&#23436;&#21892;&#24605;&#32771;&#30340;&#36807;&#31243;&#65307;&#20844;&#24335;&#21270;&#24605;&#32771;&#20135;&#29983;&#21021;&#22987;&#24605;&#32771;&#65292;&#23436;&#21892;&#24605;&#32771;&#23545;&#21021;&#22987;&#24605;&#32771;&#36827;&#34892;&#35843;&#25972;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial tho
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#20449;&#30340;&#21327;&#21516;&#24863;&#30693;&#26694;&#26550;ACC-DA&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#36890;&#20449;&#22270;&#21644;&#33258;&#36866;&#24212;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.00013</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#39550;&#39542;&#20013;&#22522;&#20110;&#39046;&#22495;&#21305;&#37197;&#30340;&#21327;&#21516;&#24863;&#30693;&#30340;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Adaptive Communications in Collaborative Perception with Domain Alignment for Autonomous Driving. (arXiv:2310.00013v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00013
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#20449;&#30340;&#21327;&#21516;&#24863;&#30693;&#26694;&#26550;ACC-DA&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#36890;&#20449;&#22270;&#21644;&#33258;&#36866;&#24212;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36890;&#20449;&#20801;&#35768;&#36710;&#36742;&#20132;&#25442;&#34917;&#20805;&#20449;&#24687;&#65292;&#22810;&#20010;&#36830;&#25509;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#30340;&#21327;&#21516;&#24863;&#30693;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#24863;&#30693;&#33021;&#21147;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36890;&#36947;&#21464;&#21270;&#21644;&#21327;&#21516;&#36710;&#36742;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACC-DA&#65292;&#19968;&#20010;&#36890;&#36947;&#24863;&#30693;&#30340;&#21327;&#21516;&#24863;&#30693;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#36890;&#20449;&#22270;&#24182;&#26368;&#23567;&#21270;&#24179;&#22343;&#20256;&#36755;&#24310;&#36831;&#65292;&#21516;&#26102;&#20943;&#36731;&#25968;&#25454;&#24322;&#26500;&#24615;&#24102;&#26469;&#30340;&#21103;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#28857;&#21253;&#25324;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#20256;&#36755;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#36890;&#36947;&#20449;&#24687;&#29366;&#24577;&#26500;&#24314;&#36890;&#20449;&#22270;&#24182;&#26368;&#23567;&#21270;&#20256;&#36755;&#24310;&#36831;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#65292;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#30721;&#29575;-&#30072;&#21464;&#25240;&#34935;&#20197;&#22686;&#24378;&#24863;&#30693;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#23427;&#26368;&#23567;&#21270;&#20102;&#26102;&#22495;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative perception among multiple connected and autonomous vehicles can greatly enhance perceptive capabilities by allowing vehicles to exchange supplementary information via communications. Despite advances in previous approaches, challenges still remain due to channel variations and data heterogeneity among collaborative vehicles. To address these issues, we propose ACC-DA, a channel-aware collaborative perception framework to dynamically adjust the communication graph and minimize the average transmission delay while mitigating the side effects from the data heterogeneity. Our novelties lie in three aspects. We first design a transmission delay minimization method, which can construct the communication graph and minimize the transmission delay according to different channel information state. We then propose an adaptive data reconstruction mechanism, which can dynamically adjust the rate-distortion trade-off to enhance perception efficiency. Moreover, it minimizes the temporal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#25552;&#20379;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#21644;&#36991;&#20813;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2309.12671</link><description>&lt;p&gt;
&#22914;&#20309;&#24494;&#35843;&#27169;&#22411;&#65306;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization. (arXiv:2309.12671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#25552;&#20379;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#21644;&#36991;&#20813;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#21644;&#25512;&#23548;&#20986;&#20855;&#26377;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#30340;&#26377;&#25928;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#31639;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#27169;&#22411;&#23398;&#20064;&#21644;&#31574;&#30053;&#20248;&#21270;&#20043;&#38388;&#30340;&#39640;&#32806;&#21512;&#24615;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#38752;&#22238;&#25253;&#24046;&#24322;&#26469;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#65292;&#24573;&#30053;&#20102;&#27169;&#22411;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#36807;&#22810;&#30340;&#27169;&#22411;&#26356;&#26032;&#32780;&#24615;&#33021;&#19979;&#38477;&#12290;&#20854;&#20182;&#26041;&#27861;&#20351;&#29992;&#24615;&#33021;&#24046;&#24322;&#36793;&#30028;&#26469;&#26126;&#30830;&#32771;&#34385;&#27169;&#22411;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#22266;&#23450;&#30340;&#38408;&#20540;&#26469;&#38480;&#21046;&#27169;&#22411;&#20559;&#31227;&#65292;&#23548;&#33268;&#23545;&#38408;&#20540;&#30340;&#20005;&#37325;&#20381;&#36182;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#20047;&#36866;&#24212;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#19968;&#20010;&#21487;&#20197;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#28982;&#21518;&#21046;&#23450;&#19968;&#20010;&#24494;&#35843;&#36807;&#31243;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#33719;&#24471;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#65292;&#21516;&#26102;&#36991;&#20813;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;&#22522;&#20110;&#36825;&#20123;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30452;&#35266;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Designing and deriving effective model-based reinforcement learning (MBRL) algorithms with a performance improvement guarantee is challenging, mainly attributed to the high coupling between model learning and policy optimization. Many prior methods that rely on return discrepancy to guide model learning ignore the impacts of model shift, which can lead to performance deterioration due to excessive model updates. Other methods use performance difference bound to explicitly consider model shift. However, these methods rely on a fixed threshold to constrain model shift, resulting in a heavy dependence on the threshold and a lack of adaptability during the training process. In this paper, we theoretically derive an optimization objective that can unify model shift and model bias and then formulate a fine-tuning process. This process adaptively adjusts the model updates to get a performance improvement guarantee while avoiding model overfitting. Based on these, we develop a straightforward 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#37327;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#27169;&#22411;&#30340;&#39640;&#25928;&#27010;&#24565;&#28418;&#31227;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#25216;&#26415;&#26469;&#32500;&#25345;&#26816;&#27979;&#22120;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#37325;&#26032;&#35757;&#32451;&#39057;&#29575;&#21644;&#25968;&#25454;&#20351;&#29992;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.09807</link><description>&lt;p&gt;
&#25209;&#37327;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#27169;&#22411;&#30340;&#39640;&#25928;&#27010;&#24565;&#28418;&#31227;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Efficient Concept Drift Handling for Batch Android Malware Detection Models. (arXiv:2309.09807v1 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#37327;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#27169;&#22411;&#30340;&#39640;&#25928;&#27010;&#24565;&#28418;&#31227;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#25216;&#26415;&#26469;&#32500;&#25345;&#26816;&#27979;&#22120;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#37325;&#26032;&#35757;&#32451;&#39057;&#29575;&#21644;&#25968;&#25454;&#20351;&#29992;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Android&#24212;&#29992;&#31243;&#24207;&#30340;&#24555;&#36895;&#21457;&#23637;&#24615;&#23545;&#20110;&#38745;&#24577;&#25209;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#24555;&#23601;&#20250;&#36807;&#26102;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20010;&#25361;&#25112;&#65292;&#20294;&#29616;&#26377;&#25991;&#29486;&#23545;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#20851;&#27880;&#26377;&#38480;&#65292;&#35768;&#22810;&#20808;&#36827;&#30340;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#65288;&#22914;Drebin&#12289;DroidDet&#21644;MaMaDroid&#65289;&#20381;&#36182;&#20110;&#38745;&#24577;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37325;&#26032;&#35757;&#32451;&#25216;&#26415;&#22914;&#20309;&#33021;&#22815;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#20445;&#25345;&#26816;&#27979;&#22120;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#20010;&#26041;&#38754;&#23545;&#26816;&#27979;&#22120;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#65306;1&#65289;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#30340;&#39057;&#29575;&#65292;2&#65289;&#29992;&#20110;&#37325;&#26032;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22312;&#31532;&#19968;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#21608;&#26399;&#24615;&#37325;&#26032;&#35757;&#32451;&#19982;&#20165;&#22312;&#24517;&#35201;&#26102;&#35302;&#21457;&#37325;&#26032;&#35757;&#32451;&#30340;&#26356;&#20808;&#36827;&#30340;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#31532;&#20108;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29992;&#20110;&#20943;&#23569;&#29992;&#20110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#25454;&#37327;&#30340;&#37319;&#26679;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22266;&#23450;si
&lt;/p&gt;
&lt;p&gt;
The rapidly evolving nature of Android apps poses a significant challenge to static batch machine learning algorithms employed in malware detection systems, as they quickly become obsolete. Despite this challenge, the existing literature pays limited attention to addressing this issue, with many advanced Android malware detection approaches, such as Drebin, DroidDet and MaMaDroid, relying on static models. In this work, we show how retraining techniques are able to maintain detector capabilities over time. Particularly, we analyze the effect of two aspects in the efficiency and performance of the detectors: 1) the frequency with which the models are retrained, and 2) the data used for retraining. In the first experiment, we compare periodic retraining with a more advanced concept drift detection method that triggers retraining only when necessary. In the second experiment, we analyze sampling methods to reduce the amount of data used to retrain models. Specifically, we compare fixed si
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;D-Waves&#30340;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#26694;&#26550;&#12289;&#23500;&#22763;&#36890;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#25968;&#23383;&#36864;&#28779;&#22120;&#21644;Gurobi&#30340;&#32463;&#20856;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#36816;&#36755;&#26426;&#22120;&#20154;&#35843;&#24230;&#38382;&#39064;&#30340;&#25351;&#23548;&#65292;&#21457;&#29616;&#25968;&#23383;&#36864;&#28779;&#22120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#28151;&#21512;&#37327;&#23376;&#36864;&#28779;&#22120;&#25552;&#20379;&#20102;&#19968;&#20123;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2309.09736</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#36816;&#36755;&#26426;&#22120;&#20154;&#35843;&#24230;&#38382;&#39064;&#30340;&#37327;&#23376;&#20248;&#21270;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Quantum Optimization Case Study for a Transport Robot Scheduling Problem. (arXiv:2309.09736v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;D-Waves&#30340;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#26694;&#26550;&#12289;&#23500;&#22763;&#36890;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#25968;&#23383;&#36864;&#28779;&#22120;&#21644;Gurobi&#30340;&#32463;&#20856;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#36816;&#36755;&#26426;&#22120;&#20154;&#35843;&#24230;&#38382;&#39064;&#30340;&#25351;&#23548;&#65292;&#21457;&#29616;&#25968;&#23383;&#36864;&#28779;&#22120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#28151;&#21512;&#37327;&#23376;&#36864;&#28779;&#22120;&#25552;&#20379;&#20102;&#19968;&#20123;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;D-Waves&#30340;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#26694;&#26550;&#12289;&#23500;&#22763;&#36890;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#25968;&#23383;&#36864;&#28779;&#22120;&#21644;Gurobi&#30340;&#26368;&#20808;&#36827;&#32463;&#20856;&#27714;&#35299;&#22120;&#22312;&#35299;&#20915;&#36816;&#36755;&#26426;&#22120;&#20154;&#35843;&#24230;&#38382;&#39064;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#38382;&#39064;&#28304;&#20110;&#19968;&#20010;&#20855;&#26377;&#24037;&#19994;&#30456;&#20851;&#24615;&#30340;&#29616;&#23454;&#22330;&#26223;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#35774;&#35745;&#29702;&#24565;&#12290;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#19981;&#21516;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#32452;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#31471;&#21040;&#31471;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;Gurobi&#30452;&#25509;&#27604;&#36739;&#65292;&#25968;&#23383;&#36864;&#28779;&#22120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#28151;&#21512;&#37327;&#23376;&#36864;&#28779;&#22120;&#25552;&#20379;&#20102;&#19968;&#20123;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;&#20351;&#29992;&#19981;&#21516;&#31574;&#30053;&#35299;&#20915;&#24212;&#29992;&#23548;&#21521;&#20248;&#21270;&#38382;&#39064;&#30340;&#24037;&#20316;&#27969;&#31243;&#30340;&#35265;&#35299;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive case study comparing the performance of D-Waves' quantum-classical hybrid framework, Fujitsu's quantum-inspired digital annealer, and Gurobi's state-of-the-art classical solver in solving a transport robot scheduling problem. This problem originates from an industrially relevant real-world scenario. We provide three different models for our problem following different design philosophies. In our benchmark, we focus on the solution quality and end-to-end runtime of different model and solver combinations. We find promising results for the digital annealer and some opportunities for the hybrid quantum annealer in direct comparison with Gurobi. Our study provides insights into the workflow for solving an application-oriented optimization problem with different strategies, and can be useful for evaluating the strengths and weaknesses of different approaches.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#25512;&#33616;&#22242;&#38431;&#65292;&#20351;&#24471;&#27599;&#20010;&#22242;&#38431;&#33021;&#22815;&#28385;&#36275;&#39033;&#30446;&#35201;&#27714;&#30340;&#25216;&#33021;&#35206;&#30422;&#65292;&#24182;&#19988;&#24179;&#34913;&#20505;&#36873;&#25104;&#21592;&#20043;&#38388;&#30340;&#24037;&#20316;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2309.09404</link><description>&lt;p&gt;
&#29992;&#24320;&#25918;&#25968;&#25454;&#39537;&#21160;&#30340;&#22242;&#38431;&#25512;&#33616;&#20419;&#36827;&#30740;&#31350;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals. (arXiv:2309.09404v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09404
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#25512;&#33616;&#22242;&#38431;&#65292;&#20351;&#24471;&#27599;&#20010;&#22242;&#38431;&#33021;&#22815;&#28385;&#36275;&#39033;&#30446;&#35201;&#27714;&#30340;&#25216;&#33021;&#35206;&#30422;&#65292;&#24182;&#19988;&#24179;&#34913;&#20505;&#36873;&#25104;&#21592;&#20043;&#38388;&#30340;&#24037;&#20316;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22242;&#38431;&#24314;&#35774;&#21644;&#20419;&#36827;&#21512;&#20316;&#26159;&#38750;&#24120;&#24120;&#35265;&#30340;&#21830;&#19994;&#27963;&#21160;&#12290;&#19968;&#20010;&#20363;&#23376;&#23601;&#26159;TeamingForFunding&#38382;&#39064;&#65292;&#30740;&#31350;&#26426;&#26500;&#21644;&#30740;&#31350;&#20154;&#21592;&#22312;&#21521;&#36164;&#21161;&#26426;&#26500;&#30003;&#35831;&#26102;&#65292;&#24076;&#26395;&#33021;&#22815;&#25214;&#21040;&#21512;&#20316;&#26426;&#20250;&#20197;&#22238;&#24212;&#21518;&#32773;&#30340;&#39033;&#30446;&#30003;&#35831;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#25512;&#33616;&#22242;&#38431;&#65292;&#20351;&#24471;&#27599;&#20010;&#22242;&#38431;&#37117;&#33021;&#22815;&#36798;&#21040;&#26426;&#20250;&#35201;&#27714;&#30340;&#26368;&#39640;&#25216;&#33021;&#35206;&#30422;&#65292;&#24182;&#19988;&#20505;&#36873;&#25104;&#21592;&#20043;&#38388;&#30340;&#24037;&#20316;&#20998;&#37197;&#26159;&#24179;&#34913;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#21462;&#24320;&#25918;&#25968;&#25454;&#20013;&#30340;&#39033;&#30446;&#30003;&#35831;&#65288;&#38656;&#27714;&#65289;&#21644;&#30740;&#31350;&#20154;&#21592;&#31616;&#20171;&#65288;&#20379;&#32473;&#65289;&#20013;&#30340;&#25216;&#33021;&#28508;&#21147;&#65292;&#20351;&#29992;&#20998;&#31867;&#27861;&#23545;&#20854;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#21305;&#37197;&#38656;&#27714;&#21644;&#20379;&#32473;&#12290;&#25105;&#20204;&#21019;&#24314;&#22242;&#38431;&#20197;&#26368;&#22823;&#21270;&#19968;&#20010;&#26032;&#30340;&#34913;&#37327;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#30340;&#24230;&#37327;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23450;&#37327;&#39564;&#35777;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#25104;&#21151;&#65292;&#36890;&#36807;&#8230;
&lt;/p&gt;
&lt;p&gt;
Building teams and promoting collaboration are two very common business activities. An example of these are seen in the TeamingForFunding problem, where research institutions and researchers are interested to identify collaborative opportunities when applying to funding agencies in response to latter's calls for proposals. We describe a novel system to recommend teams using a variety of AI methods, such that (1) each team achieves the highest possible skill coverage that is demanded by the opportunity, and (2) the workload of distributing the opportunities is balanced amongst the candidate members. We address these questions by extracting skills latent in open data of proposal calls (demand) and researcher profiles (supply), normalizing them using taxonomies, and creating efficient algorithms that match demand to supply. We create teams to maximize goodness along a novel metric balancing short- and long-term objectives. We validate the success of our algorithms (1) quantitatively, by e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#27979;&#37327;&#25991;&#26412;&#20013;&#30340;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#31995;&#32479;VAGO&#65292;&#20197;&#21450;&#22522;&#20110;BERT-like&#26550;&#26500;&#30340;&#31070;&#32463;&#20811;&#38534;&#65292;&#35813;&#26041;&#27861;&#22312;&#22266;&#23450;&#35821;&#26009;&#24211;&#21644;&#22810;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06132</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#20013;&#27979;&#37327;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#65306;&#20174;&#31526;&#21495;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;VAGO
&lt;/p&gt;
&lt;p&gt;
Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO. (arXiv:2309.06132v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#27979;&#37327;&#25991;&#26412;&#20013;&#30340;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#31995;&#32479;VAGO&#65292;&#20197;&#21450;&#22522;&#20110;BERT-like&#26550;&#26500;&#30340;&#31070;&#32463;&#20811;&#38534;&#65292;&#35813;&#26041;&#27861;&#22312;&#22266;&#23450;&#35821;&#26009;&#24211;&#21644;&#22810;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#27979;&#37327;&#25991;&#26412;&#20013;&#30340;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19987;&#23478;&#31995;&#32479;VAGO&#65292;&#24182;&#22312;&#19968;&#23567;&#32452;&#20107;&#23454;&#19982;&#35266;&#28857;&#21477;&#23376;&#30340;&#22522;&#20934;&#19978;&#23545;&#20854;&#36827;&#34892;&#20102;&#35828;&#26126;&#65292;&#24182;&#22312;&#26356;&#22823;&#30340;&#27861;&#35821;&#26032;&#38395;&#35821;&#26009;&#24211;FreSaDa&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#30830;&#35748;&#35773;&#21050;&#24615;&#25991;&#26412;&#20013;&#20027;&#35266;&#26631;&#35760;&#30340;&#26356;&#39640;&#27969;&#34892;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;BERT-like&#26550;&#26500;&#30340;VAGO&#31070;&#32463;&#20811;&#38534;&#65292;&#35813;&#26550;&#26500;&#22522;&#20110;&#22312;FreSaDa&#19978;&#33719;&#24471;&#30340;&#31526;&#21495;VAGO&#20998;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65288;LIME&#65289;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31070;&#32463;&#29256;&#26412;&#22312;&#20016;&#23500;&#31526;&#21495;&#29256;&#26412;&#30340;&#35789;&#20856;&#21644;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#29256;&#26412;&#26041;&#38754;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a hybrid approach to the automated measurement of vagueness and subjectivity in texts. We first introduce the expert system VAGO, we illustrate it on a small benchmark of fact vs. opinion sentences, and then test it on the larger French press corpus FreSaDa to confirm the higher prevalence of subjective markers in satirical vs. regular texts. We then build a neural clone of VAGO, based on a BERT-like architecture, trained on the symbolic VAGO scores obtained on FreSaDa. Using explainability tools (LIME), we show the interest of this neural version for the enrichment of the lexicons of the symbolic version, and for the production of versions in other languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26816;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#35753;&#27169;&#22411;&#33258;&#34892;&#36807;&#28388;&#22238;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#26410;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#65292;&#20173;&#28982;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.07308</link><description>&lt;p&gt;
LLM&#33258;&#21355;&#65306;&#36890;&#36807;&#33258;&#26816;&#65292;LLMs&#24847;&#35782;&#21040;&#23427;&#20204;&#34987;&#24858;&#24324;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked. (arXiv:2308.07308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26816;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#35753;&#27169;&#22411;&#33258;&#34892;&#36807;&#28388;&#22238;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#26410;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#65292;&#20173;&#28982;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33021;&#22815;&#23545;&#20154;&#31867;&#25552;&#31034;&#20570;&#20986;&#39640;&#36136;&#37327;&#25991;&#26412;&#22238;&#24212;&#32780;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22238;&#24212;&#29992;&#25143;&#25552;&#31034;&#26102;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#32473;&#29992;&#25143;&#25552;&#20379;&#29359;&#32618;&#25351;&#23548;&#65289;&#12290;&#25991;&#29486;&#20013;&#24050;&#32463;&#30528;&#37325;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#26041;&#27861;&#65288;&#20363;&#22914;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23558;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65289;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#32469;&#36807;&#29983;&#25104;&#26377;&#23475;&#25991;&#26412;&#38480;&#21046;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#38450;&#24481;&#36825;&#20123;&#25915;&#20987;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#33258;&#24049;&#30340;&#22238;&#24212;&#36827;&#34892;&#36807;&#28388;&#12290;&#25105;&#20204;&#30446;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#27809;&#26377;&#34987;&#24494;&#35843;&#20197;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#26469;&#38450;&#27490;&#20854;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have skyrocketed in popularity in recent years due to their ability to generate high-quality text in response to human prompting. However, these models have been shown to have the potential to generate harmful content in response to user prompting (e.g., giving users instructions on how to commit crimes). There has been a focus in the literature on mitigating these risks, through methods like aligning models with human values through reinforcement learning. However, it has been shown that even aligned language models are susceptible to adversarial attacks that bypass their restrictions on generating harmful text. We propose a simple approach to defending against these attacks by having a large language model filter its own responses. Our current results show that even if a model is not fine-tuned to be aligned with human values, it is possible to stop it from presenting harmful content to users by validating the content using a language model.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#22686;&#24378;&#30340;&#30456;&#20851;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#36741;&#21161;&#26597;&#35810;-&#39033;&#30446;&#20132;&#20114;&#65292;&#26469;&#25913;&#36827;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#26597;&#35810;-&#39033;&#30446;&#21305;&#37197;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05379</link><description>&lt;p&gt;
&#36229;&#36234;&#35821;&#20041;&#65306;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#34892;&#20026;&#22686;&#24378;&#30456;&#20851;&#27169;&#22411;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Semantics: Learning a Behavior Augmented Relevance Model with Self-supervised Learning. (arXiv:2308.05379v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05379
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#22686;&#24378;&#30340;&#30456;&#20851;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#36741;&#21161;&#26597;&#35810;-&#39033;&#30446;&#20132;&#20114;&#65292;&#26469;&#25913;&#36827;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#26597;&#35810;-&#39033;&#30446;&#21305;&#37197;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24314;&#27169;&#26088;&#22312;&#23450;&#20301;&#19982;&#23545;&#24212;&#26597;&#35810;&#30456;&#20851;&#30340;&#29702;&#24819;&#39033;&#30446;&#65292;&#36825;&#23545;&#20110;&#25628;&#32034;&#24341;&#25806;&#30830;&#20445;&#29992;&#25143;&#20307;&#39564;&#38750;&#24120;&#37325;&#35201;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#20256;&#32479;&#26041;&#27861;&#36890;&#36807;&#35780;&#20272;&#26597;&#35810;&#19982;&#39033;&#30446;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#32431;&#35821;&#20041;&#21305;&#37197;&#24182;&#19981;&#26159;&#21807;&#19968;&#30340;&#26041;&#27861;&#12290;&#23454;&#38469;&#19978;&#65292;&#20174;&#29992;&#25143;&#25628;&#32034;&#35760;&#24405;&#30340;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#36741;&#21161;&#26597;&#35810;-&#39033;&#30446;&#20132;&#20114;&#21487;&#20197;&#25552;&#20379;&#36827;&#19968;&#27493;&#25581;&#31034;&#29992;&#25143;&#25628;&#32034;&#24847;&#22270;&#30340;&#32447;&#32034;&#12290;&#24471;&#30410;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#34892;&#20026;&#22686;&#24378;&#30456;&#20851;&#23398;&#20064;&#27169;&#22411;&#30340;&#25903;&#20184;&#23453;&#25628;&#32034;&#27169;&#22411;&#65288;BARL-ASe&#65289;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#30446;&#26631;&#39033;&#30446;&#30340;&#30456;&#37051;&#26597;&#35810;&#21644;&#30446;&#26631;&#26597;&#35810;&#30340;&#30456;&#37051;&#39033;&#30446;&#26469;&#34917;&#20805;&#30446;&#26631;&#26597;&#35810;-&#39033;&#30446;&#30340;&#35821;&#20041;&#21305;&#37197;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#24314;&#31435;&#20102;&#22810;&#23618;&#20849;&#21516;&#27880;&#24847;&#21147;&#65292;&#20174;&#30456;&#37051;&#21644;&#30446;&#26631;&#35270;&#22270;&#20013;&#25552;&#21462;&#20102;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#27169;&#22411;&#38543;&#21518;&#37319;&#29992;&#37051;&#23621;-&#30446;&#26631;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26469;&#25552;&#39640;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance modeling aims to locate desirable items for corresponding queries, which is crucial for search engines to ensure user experience. Although most conventional approaches address this problem by assessing the semantic similarity between the query and item, pure semantic matching is not everything. In reality, auxiliary query-item interactions extracted from user historical behavior data of the search log could provide hints to reveal users' search intents further. Drawing inspiration from this, we devise a novel Behavior Augmented Relevance Learning model for Alipay Search (BARL-ASe) that leverages neighbor queries of target item and neighbor items of target query to complement target query-item semantic matching. Specifically, our model builds multi-level co-attention for distilling coarse-grained and fine-grained semantic representations from both neighbor and target views. The model subsequently employs neighbor-target self-supervised learning to improve the accuracy and robu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35748;&#30693;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25104;&#29087;&#26234;&#33021;&#29366;&#24577;&#19979;&#25805;&#20316;&#32047;&#31215;&#30340;&#30693;&#35782;&#65292;&#24182;&#20381;&#36182;&#36866;&#24403;&#30340;&#24847;&#24895;&#36827;&#34892;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.04600</link><description>&lt;p&gt;
&#27169;&#22411;&#30340;&#27169;&#22411;--&#31532;&#19968;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Model of models -- Part 1. (arXiv:2308.04600v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35748;&#30693;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25104;&#29087;&#26234;&#33021;&#29366;&#24577;&#19979;&#25805;&#20316;&#32047;&#31215;&#30340;&#30693;&#35782;&#65292;&#24182;&#20381;&#36182;&#36866;&#24403;&#30340;&#24847;&#24895;&#36827;&#34892;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35748;&#30693;&#27169;&#22411;&#65292;&#20316;&#20026;AGI&#20195;&#29702;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#35813;&#27169;&#22411;&#26159;&#22312;&#20854;&#25104;&#29087;&#30340;&#26234;&#33021;&#29366;&#24577;&#19979;&#24341;&#20837;&#30340;&#65292;&#26159;DENN&#21644;&#29305;&#21035;&#26159;AKREM&#20043;&#21069;&#27169;&#22411;&#30340;&#24310;&#20280;&#65292;&#21253;&#25324;&#25805;&#20316;&#27169;&#22411;&#65288;&#26694;&#26550;/&#31867;&#21035;&#65289;&#21644;&#24847;&#24895;&#12290;&#35813;&#27169;&#22411;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#35748;&#30693;&#26159;&#22312;&#36866;&#24403;&#30340;&#24847;&#24895;&#24341;&#23548;&#19979;&#23545;&#32047;&#31215;&#30340;&#30693;&#35782;&#36827;&#34892;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20551;&#35774;&#22312;&#25104;&#29087;&#30340;&#26234;&#33021;&#29366;&#24577;&#20043;&#21069;&#30340;&#28436;&#21270;&#38454;&#27573;&#65292;&#34892;&#20026;&#65288;&#30693;&#35782;&#30340;&#19968;&#37096;&#20998;&#65289;&#26159;&#23398;&#20064;&#19982;&#24847;&#24895;&#23545;&#40784;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#27599;&#20010;&#24050;&#30693;&#26234;&#33021;&#26041;&#38754;&#30340;&#20108;&#20803;&#24615;&#21407;&#21017;&#65292;&#20363;&#22914;&#19978;&#19979;&#23398;&#20064;&#27169;&#22411;&#65292;&#27867;&#21270;&#19982;&#29305;&#21270;&#65292;&#31561;&#31561;&#12290;&#27492;&#22806;&#65292;&#20513;&#23548;&#19968;&#31181;&#25972;&#20307;&#30340;AGI&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#35748;&#30693;&#25110;&#25928;&#29575;&#38382;&#39064;&#65292;&#20197;&#21487;&#37325;&#29992;&#24615;&#21644;&#31616;&#27905;&#24615;&#30340;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23454;&#29616;&#30693;&#35782;&#21644;&#35268;&#21017;&#20043;&#38388;&#30340;&#21452;&#37325;&#21472;&#21152;&#26041;&#24335;&#26469;&#25551;&#36848;&#36798;&#21040;&#36825;&#31181;&#25104;&#29087;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new cognitive model, acting as the main component of an AGI agent. The model is introduced in its mature intelligence state, and as an extension of previous models, DENN, and especially AKREM, by including operational models (frames/classes) and will. This model's core assumption is that cognition is about operating on accumulated knowledge, with the guidance of an appropriate will. Also, we assume that the actions, part of knowledge, are learning to be aligned with will, during the evolution phase that precedes the mature intelligence state. In addition, this model is mainly based on the duality principle in every known intelligent aspect, such as exhibiting both top-down and bottom-up model learning, generalization verse specialization, and more. Furthermore, a holistic approach is advocated for AGI designing, and cognition under constraints or efficiency is proposed, in the form of reusability and simplicity. Finally, reaching this mature state is described via
&lt;/p&gt;</description></item><item><title>MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02490</link><description>&lt;p&gt;
MM-Vet: &#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32508;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02490
&lt;/p&gt;
&lt;p&gt;
MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#30340;LMM&#23637;&#31034;&#20102;&#21508;&#31181;&#26377;&#36259;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35299;&#20915;&#20070;&#20889;&#22312;&#40657;&#26495;&#19978;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25512;&#29702;&#26032;&#38395;&#22270;&#29255;&#20013;&#30340;&#20107;&#20214;&#21644;&#21517;&#20154;&#65292;&#20197;&#21450;&#35299;&#37322;&#35270;&#35273;&#31505;&#35805;&#12290;&#24555;&#36895;&#30340;&#27169;&#22411;&#36827;&#27493;&#32473;&#35780;&#20272;&#26631;&#20934;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38382;&#39064;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22914;&#20309;&#31995;&#32479;&#22320;&#26500;&#24314;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65307;&#65288;2&#65289;&#22914;&#20309;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#21644;&#22238;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;3&#65289;&#22914;&#20309;&#32473;&#20986;&#36229;&#20986;&#31616;&#21333;&#24615;&#33021;&#25490;&#21517;&#30340;&#27169;&#22411;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#27934;&#23519;&#65306;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26377;&#36259;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#19968;&#31181;&#36890;&#25165;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;MM-Vet&#23450;&#20041;&#20102;6&#20010;&#26680;&#24515;VL&#33021;&#21147;&#65292;&#24182;&#26816;&#26597;&#20102;&#20174;&#36825;&#20123;&#33021;&#21147;&#32452;&#21512;&#20013;&#24471;&#20986;&#30340;16&#31181;&#26377;&#36259;&#30340;&#25972;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16387</link><description>&lt;p&gt;
Relation-Oriented: &#36808;&#21521;&#19982;&#30693;&#35782;&#23545;&#20934;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24212;&#29992;&#19968;&#20010;&#35266;&#23519;&#23548;&#21521;&#30340;&#21407;&#21017;&#65292;&#20854;&#20013;&#35266;&#23519;&#21464;&#37327;&#20808;&#23384;&#22312;&#24182;&#20026;&#26500;&#24314;&#20851;&#31995;&#22880;&#23450;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#26469;&#35828;&#36275;&#22815;&#20102;&#65292;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#25972;&#21512;&#26292;&#38706;&#20102;&#35266;&#23519;&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#23454;&#38469;&#29702;&#35299;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22609;&#36896;&#20102;&#30001;&#20851;&#31995;&#23450;&#20041;&#30340;&#35748;&#30693;&#23454;&#20307;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36328;&#36234;&#26102;&#38388;&#21644;&#36229;&#32500;&#24230;&#31354;&#38388;&#21046;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#34987;&#38480;&#21046;&#22312;&#35266;&#23519;&#26500;&#24314;&#20013;&#12290;&#20174;&#19968;&#31181;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20581;&#24247;&#20449;&#24687;&#23398;&#30340;&#30452;&#35266;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22312;&#25105;&#20204;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#36825;&#31181;&#19981;&#23545;&#40784;&#30340;&#26681;&#28304;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#19968;&#31181;&#23454;&#38469;&#23454;&#26045;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, we naturally apply an Observation-Oriented principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the misalignment between the observational models and our actual comprehension. Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs. From an innovative Relation-Oriented perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the relation-defined representation learning methodology as a practical implementation of Relation-Oriented modeling, supported by extensive experimental validation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29305;&#24449;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#27604;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#26356;&#21487;&#38752;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#29992;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#26367;&#25442;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#33021;&#22815;&#25552;&#39640;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15317</link><description>&lt;p&gt;
DiffKendall:&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall's Rank Correlation. (arXiv:2307.15317v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29305;&#24449;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#27604;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#26356;&#21487;&#38752;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#29992;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#26367;&#25442;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#33021;&#22815;&#25552;&#39640;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26088;&#22312;&#23558;&#22312;&#22522;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#27169;&#22411;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#26032;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#36825;&#32463;&#24120;&#23548;&#33268;&#26032;&#31867;&#21035;&#19978;&#36890;&#36947;&#19978;&#29305;&#24449;&#20540;&#30340;&#20998;&#24067;&#30456;&#23545;&#22343;&#21248;&#65292;&#38590;&#20197;&#30830;&#23450;&#26032;&#20219;&#21153;&#20013;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#12290;&#26631;&#20934;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#22914;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#36127;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#65292;&#26469;&#34913;&#37327;&#20004;&#20010;&#29305;&#24449;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#39640;&#20960;&#20309;&#30456;&#20284;&#24230;&#30340;&#29305;&#24449;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#35821;&#20041;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#29305;&#24449;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#27604;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#29992;Kendall&#25490;&#21517;&#30456;&#20851;&#24615;&#26367;&#25442;&#20960;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#33021;&#22815;&#25552;&#39640;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning aims to adapt models trained on the base dataset to novel tasks where the categories are not seen by the model before. This often leads to a relatively uniform distribution of feature values across channels on novel classes, posing challenges in determining channel importance for novel tasks. Standard few-shot learning methods employ geometric similarity metrics such as cosine similarity and negative Euclidean distance to gauge the semantic relatedness between two features. However, features with high geometric similarities may carry distinct semantics, especially in the context of few-shot learning. In this paper, we demonstrate that the importance ranking of feature channels is a more reliable indicator for few-shot learning than geometric similarity metrics. We observe that replacing the geometric similarity metric with Kendall's rank correlation only during inference is able to improve the performance of few-shot learning across a wide range of datasets with diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#23519;&#20102;&#38543;&#26426;&#28216;&#36208;&#24322;&#24120;&#26816;&#27979;(RWAD)&#38754;&#20020;&#30340;&#22270;&#31354;&#38388;&#25915;&#20987;&#21644;&#29305;&#24449;&#31354;&#38388;&#25915;&#20987;&#65292;&#35777;&#26126;&#20102;&#25915;&#20987;RWAD&#30340;&#22797;&#26434;&#24230;&#26159;NP&#38590;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25915;&#20987;&#31574;&#30053;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#22270;&#24341;&#23548;&#25915;&#20987;&#35774;&#35745;&#20102;&#26356;&#24378;&#22823;&#30340;&#29305;&#24449;&#31354;&#38388;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.14387</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#21452;&#37325;&#31354;&#38388;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Dual-Space Attacks against Random-Walk-based Anomaly Detection. (arXiv:2307.14387v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#23519;&#20102;&#38543;&#26426;&#28216;&#36208;&#24322;&#24120;&#26816;&#27979;(RWAD)&#38754;&#20020;&#30340;&#22270;&#31354;&#38388;&#25915;&#20987;&#21644;&#29305;&#24449;&#31354;&#38388;&#25915;&#20987;&#65292;&#35777;&#26126;&#20102;&#25915;&#20987;RWAD&#30340;&#22797;&#26434;&#24230;&#26159;NP&#38590;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25915;&#20987;&#31574;&#30053;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#22270;&#24341;&#23548;&#25915;&#20987;&#35774;&#35745;&#20102;&#26356;&#24378;&#22823;&#30340;&#29305;&#24449;&#31354;&#38388;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#28216;&#36208;&#24322;&#24120;&#26816;&#27979;(RWAD)&#36890;&#24120;&#29992;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#35782;&#21035;&#24322;&#24120;&#27169;&#24335;&#12290;RWAD&#30340;&#19968;&#20010;&#26377;&#36259;&#29305;&#28857;&#26159;&#36755;&#20837;&#22270;&#21487;&#20197;&#26159;&#39044;&#20808;&#23384;&#22312;&#30340;&#65292;&#20063;&#21487;&#20197;&#30001;&#21407;&#22987;&#29305;&#24449;&#26500;&#24314;&#32780;&#26469;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;RWAD&#26377;&#20004;&#31181;&#28508;&#22312;&#30340;&#25915;&#20987;&#26041;&#24335;&#65306;&#22270;&#31354;&#38388;&#25915;&#20987;&#21644;&#29305;&#24449;&#31354;&#38388;&#25915;&#20987;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#23454;&#38469;&#30340;&#21452;&#37325;&#31354;&#38388;&#25915;&#20987;&#65292;&#25506;&#31350;&#20102;&#22270;&#31354;&#38388;&#25915;&#20987;&#21644;&#29305;&#24449;&#31354;&#38388;&#25915;&#20987;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25915;&#20987;RWAD&#26159;NP&#38590;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#22270;&#31354;&#38388;&#25915;&#20987;&#24418;&#24335;&#21270;&#20026;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#31574;&#30053;&#65306;&#20132;&#26367;&#36845;&#20195;&#25915;&#20987;(alterI-attack)&#25110;&#21033;&#29992;&#38543;&#26426;&#28216;&#36208;&#27169;&#22411;&#30340;&#38381;&#21512;&#35299;(cf-attack)&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#31354;&#38388;&#25915;&#20987;&#30340;&#32467;&#26524;&#26469;&#25351;&#23548;&#35774;&#35745;&#26356;&#24378;&#22823;&#30340;&#29305;&#24449;&#31354;&#38388;&#25915;&#20987;(&#21363;&#65292;&#22270;&#24341;&#23548;&#25915;&#20987;)&#12290;
&lt;/p&gt;
&lt;p&gt;
Random Walks-based Anomaly Detection (RWAD) is commonly used to identify anomalous patterns in various applications. An intriguing characteristic of RWAD is that the input graph can either be pre-existing or constructed from raw features. Consequently, there are two potential attack surfaces against RWAD: graph-space attacks and feature-space attacks. In this paper, we explore this vulnerability by designing practical dual-space attacks, investigating the interplay between graph-space and feature-space attacks. To this end, we conduct a thorough complexity analysis, proving that attacking RWAD is NP-hard. Then, we proceed to formulate the graph-space attack as a bi-level optimization problem and propose two strategies to solve it: alternative iteration (alterI-attack) or utilizing the closed-form solution of the random walk model (cf-attack). Finally, we utilize the results from the graph-space attacks as guidance to design more powerful feature-space attacks (i.e., graph-guided attack
&lt;/p&gt;</description></item><item><title>WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13854</link><description>&lt;p&gt;
WebArena: &#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13854
&lt;/p&gt;
&lt;p&gt;
WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#28508;&#21147;&#36880;&#28176;&#26174;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#26159;&#22312;&#31616;&#21270;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#21644;&#27979;&#35797;&#30340;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#22797;&#29616;&#30340;&#26234;&#33021;&#20307;&#25351;&#20196;&#21644;&#25511;&#21046;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#24120;&#35265;&#39046;&#22495;&#30340;&#23436;&#20840;&#21151;&#33021;&#32593;&#31449;&#30340;&#29615;&#22659;&#65292;&#20998;&#21035;&#26159;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#35770;&#22363;&#35752;&#35770;&#12289;&#21327;&#21516;&#36719;&#20214;&#24320;&#21457;&#21644;&#20869;&#23481;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#29615;&#22659;&#20351;&#29992;&#24037;&#20855;&#65288;&#22914;&#22320;&#22270;&#65289;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;&#29992;&#25143;&#25163;&#20876;&#65289;&#26469;&#40723;&#21169;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#32452;&#37325;&#28857;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;&#25105;&#20204;&#22522;&#20934;&#20219;&#21153;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#38271;&#36828;&#30340;&#35270;&#37326;&#65292;&#24182;&#19988;&#34987;&#35774;&#35745;&#20026;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#28145;&#23618;&#27425;&#30340;&#20219;&#21153;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UGAT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#21160;&#24577;&#36716;&#25442;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#65292;&#23454;&#29616;&#20102;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#30495;&#23454;&#29615;&#22659;&#30340;&#31574;&#30053;&#36716;&#31227;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12388</link><description>&lt;p&gt;
&#38754;&#21521;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22522;&#20110;&#23454;&#20363;&#30340;&#34892;&#21160;&#36716;&#25442;&#30340;&#27169;&#25311;&#21040;&#23454;&#38469;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control. (arXiv:2307.12388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UGAT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#21160;&#24577;&#36716;&#25442;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#65292;&#23454;&#29616;&#20102;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#30495;&#23454;&#29615;&#22659;&#30340;&#31574;&#30053;&#36716;&#31227;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;TSC&#65289;&#26159;&#19968;&#20010;&#24433;&#21709;&#25968;&#30334;&#19975;&#20154;&#26085;&#24120;&#29983;&#27963;&#30340;&#22797;&#26434;&#32780;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#20248;&#21270;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#21069;&#22522;&#20110;RL&#30340;TSC&#26041;&#27861;&#20027;&#35201;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#65292;&#23384;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#20043;&#38388;&#24615;&#33021;&#24046;&#36317;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#21040;&#23454;&#38469;&#29615;&#22659;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;UGAT&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#20013;&#21160;&#24577;&#36716;&#25442;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#65292;&#20197;&#20943;&#36731;&#36716;&#31227;&#21160;&#24577;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#23558;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#23398;&#20064;&#31574;&#30053;&#36716;&#31227;&#21040;&#30495;&#23454;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#20132;&#36890;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#36716;&#31227;&#21518;&#30340;RL&#31574;&#30053;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic signal control (TSC) is a complex and important task that affects the daily lives of millions of people. Reinforcement Learning (RL) has shown promising results in optimizing traffic signal control, but current RL-based TSC methods are mainly trained in simulation and suffer from the performance gap between simulation and the real world. In this paper, we propose a simulation-to-real-world (sim-to-real) transfer approach called UGAT, which transfers a learned policy trained from a simulated environment to a real-world environment by dynamically transforming actions in the simulation with uncertainty to mitigate the domain gap of transition dynamics. We evaluate our method on a simulated traffic environment and show that it significantly improves the performance of the transferred RL policy in the real world.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#20889;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03838</link><description>&lt;p&gt;
RADAR: &#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
RADAR: Robust AI-Text Detection via Adversarial Learning. (arXiv:2307.03838v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#20889;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20197;&#21450;ChatGPT&#31867;&#24212;&#29992;&#30340;&#26222;&#21450;&#24050;&#32463;&#27169;&#31946;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#39640;&#36136;&#37327;&#25991;&#26412;&#29983;&#25104;&#30340;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#23545;&#25105;&#20204;&#30340;&#25216;&#26415;&#21644;&#31038;&#20250;&#39044;&#26399;&#30340;&#38761;&#21629;&#24615;&#21464;&#21270;&#22806;&#65292;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#65288;AI&#25991;&#26412;&#65289;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#22256;&#38590;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#28389;&#29992;&#21644;&#20844;&#24179;&#24615;&#25361;&#25112;&#65292;&#20363;&#22914;&#34394;&#20551;&#20869;&#23481;&#29983;&#25104;&#65292;&#25220;&#34989;&#20197;&#21450;&#23545;&#26080;&#36764;&#20316;&#32773;&#30340;&#38169;&#35823;&#25351;&#25511;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#24403;&#21069;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#22522;&#20110;LLM&#30340;&#25913;&#20889;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#20849;&#21516;&#35757;&#32451;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#12290;RADAR&#22522;&#20110;&#19968;&#20010;&#25913;&#20889;&#22120;&#21644;&#19968;&#20010;&#26816;&#27979;&#22120;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#25913;&#20889;&#22120;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#36924;&#30495;&#30340;&#20869;&#23481;&#20197;&#35268;&#36991;AI&#25991;&#26412;&#26816;&#27979;&#12290;RADAR&#20351;&#29992;&#26469;&#33258;&#26816;&#27979;&#22120;&#30340;&#21453;&#39304;&#26469;&#26356;&#26032;&#25913;&#20889;&#22120;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusation of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a Robust AI-text Detector via Adversarial leaRning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice vers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.03552</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#21160;&#24577;&#20559;&#31227;&#30340;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
State Regularized Policy Optimization on Data with Dynamics Shift. (arXiv:2306.03552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#21463;&#21040;&#21160;&#24577;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#21363;&#20855;&#26377;&#19981;&#21516;&#30340;&#29615;&#22659;&#21160;&#24577;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#29615;&#22659;&#21442;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26681;&#25454;&#20854;&#29615;&#22659;&#21442;&#25968;&#23558;&#24102;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20998;&#24320;&#20197;&#35757;&#32451;&#30456;&#24212;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20986;&#29616;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#25968;&#25454;&#26159;&#8220;&#29305;&#23450;&#22330;&#26223;&#8221;&#20351;&#29992;&#30340;&#65292;&#38024;&#23545;&#26576;&#20010;&#29615;&#22659;&#35757;&#32451;&#30340;&#31574;&#30053;&#19981;&#33021;&#20174;&#25910;&#38598;&#22312;&#20854;&#20182;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#25152;&#26377;&#20854;&#20182;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#21644;&#19981;&#21516;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#65292;&#26368;&#20248;&#31574;&#30053;&#20855;&#26377;&#31867;&#20284;&#30340;&#31283;&#24577;&#20998;&#24067;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#29305;&#24615;&#65292;&#24182;&#20174;&#20855;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31283;&#24577;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#37325;&#29992;&#12290;&#36825;&#31181;&#20998;&#24067;&#29992;&#20110;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#23548;&#33268;&#20102; SRPO&#65288;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;&#65289;&#31639;&#27861;&#30340;&#20986;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SRPO &#22312;&#20855;&#26377;&#21160;&#24577;&#20559;&#31227;&#30340;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\textbf{S}tat
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;GPT-3&#34920;&#29616;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#24403;&#38382;&#39064;&#19981;&#26159;&#29992;&#33521;&#35821;&#25552;&#20986;&#26102;&#12290;&#36825;&#19982;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#38382;&#39064;&#30340;&#35821;&#35328;&#24046;&#24322;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.16339</link><description>&lt;p&gt;
&#24403;&#38382;&#39064;&#19981;&#26159;&#29992;&#33521;&#35821;&#25552;&#20986;&#26102;&#65292;&#19981;&#35201;&#23436;&#20840;&#20449;&#20219;GPT
&lt;/p&gt;
&lt;p&gt;
Don't Trust GPT When Your Question Is Not In English. (arXiv:2305.16339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16339
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;GPT-3&#34920;&#29616;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#24403;&#38382;&#39064;&#19981;&#26159;&#29992;&#33521;&#35821;&#25552;&#20986;&#26102;&#12290;&#36825;&#19982;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#38382;&#39064;&#30340;&#35821;&#35328;&#24046;&#24322;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;LLMs&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22810;&#39033;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#30456;&#23545;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#22914;&#20309;&#33719;&#24471;&#23427;&#20204;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#20197;&#21450;&#34920;&#29616;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#24046;&#24322;&#20173;&#28982;&#23384;&#22312;&#22522;&#26412;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#23545;LLMs&#30340;&#30740;&#31350;&#38750;&#24120;&#20851;&#38190;&#65292;&#22240;&#20026;&#29992;&#25143;&#21644;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#26469;&#33258;&#22810;&#31181;&#35821;&#35328;&#32972;&#26223;&#65292;&#21487;&#33021;&#24433;&#21709;&#20182;&#20204;&#23545;LLMs&#32467;&#26524;&#30340;&#21033;&#29992;&#21644;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#20197;&#23450;&#24615;&#35780;&#20272;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;LLMs&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22312;&#36328;&#35821;&#35328;&#27867;&#21270;&#29616;&#35937;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21363;&#19981;&#20805;&#36275;&#30340;&#22810;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#23548;&#33268;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#35821;&#35328;&#36827;&#34892;&#20102;GPT-3&#30340;&#23454;&#39564;&#65292;&#36825;&#20123;&#35821;&#35328;&#28085;&#30422;&#20102;&#20174;&#21360;&#27431;&#35821;&#31995;&#21040;&#38750;&#21360;&#27431;&#35821;&#31995;&#30340;&#21508;&#31181;&#35821;&#35328;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21644;&#39564;&#35777;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#22312;&#35813;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20294;&#22914;&#26524;&#36755;&#20837;&#38382;&#39064;&#19981;&#26159;&#33521;&#35821;&#65292;GPT-3&#22312;&#20854;&#20182;&#35821;&#35328;&#19979;&#30340;&#34920;&#29616;&#26174;&#33879;&#36739;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#19982;&#35757;&#32451;&#35821;&#35328;&#21644;&#36755;&#20837;&#38382;&#39064;&#30340;&#35821;&#35328;&#24046;&#24322;&#26377;&#20851;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36827;&#34892;&#38750;&#33521;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26102;&#65292;&#38656;&#35201;&#35880;&#24910;&#20351;&#29992;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional natural language understanding abilities and have excelled in a variety of natural language processing (NLP)tasks in recent years. Despite the fact that most LLMs are trained predominantly in English, multiple studies have demonstrated their comparative performance in many other languages. However, fundamental questions persist regarding how LLMs acquire their multi-lingual abilities and how performance varies across different languages. These inquiries are crucial for the study of LLMs since users and researchers often come from diverse language backgrounds, potentially influencing their utilization and interpretation of LLMs' results. In this work, we propose a systematic way of qualifying the performance disparities of LLMs under multilingual settings. We investigate the phenomenon of across-language generalizations in LLMs, wherein insufficient multi-lingual training data leads to advanced multi-lingual capabilities. To acc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#35780;&#20272;&#20219;&#21153;&#65292;&#21517;&#20026;ToMChallenges&#65292;&#20197;&#25506;&#32034;&#24515;&#26234;&#29702;&#35770;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#26234;&#29702;&#35770;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#19968;&#33268;&#65292;&#31283;&#23450;&#22320;&#25191;&#34892;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15068</link><description>&lt;p&gt;
ToMChallenges: &#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#35780;&#20272;&#20219;&#21153;&#65292;&#29992;&#20110;&#25506;&#32034;&#24515;&#26234;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind. (arXiv:2305.15068v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#35780;&#20272;&#20219;&#21153;&#65292;&#21517;&#20026;ToMChallenges&#65292;&#20197;&#25506;&#32034;&#24515;&#26234;&#29702;&#35770;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#26234;&#29702;&#35770;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#19968;&#33268;&#65292;&#31283;&#23450;&#22320;&#25191;&#34892;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#26159;&#29702;&#35299;&#19981;&#21516;&#20010;&#20307;&#24515;&#26234;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#25191;&#34892;ToM&#20219;&#21153;&#23384;&#22312;&#28608;&#28872;&#30340;&#20105;&#35758;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25552;&#31034;&#26469;&#27979;&#35797;LLMs&#19978;&#30340;ToM&#65292;&#32467;&#26524;&#19981;&#19968;&#33268;&#65306;&#19968;&#20123;&#30740;&#31350;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23637;&#31034;ToM&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#25345;&#30456;&#21453;&#35266;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToMChallenges&#65292;&#19968;&#20010;&#22522;&#20110;Sally-Anne&#21644;Smarties&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#24515;&#26234;&#29702;&#35770;&#24182;&#21253;&#21547;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#35780;&#20998;&#22120;&#26469;&#31616;&#21270;&#31572;&#26696;&#35780;&#20272;&#36807;&#31243;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19977;&#20010;&#27169;&#22411;&#65306;davinci&#12289;turbo&#21644;gpt-4&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#21644;&#38169;&#35823;&#20998;&#26512;&#26174;&#31034;&#65292;LLMs&#22312;&#25552;&#31034;&#21644;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#19981;&#19968;&#33268;&#12290;&#23545;LLMs&#26469;&#35828;&#65292;&#31283;&#23450;&#22320;&#25191;&#34892;ToM&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM), the capacity to comprehend the mental states of distinct individuals, is essential for numerous practical applications. With the development of large language models (LLMs), there is a heated debate about whether they are able to perform ToM tasks. Previous studies have used different tasks and prompts to test the ToM on LLMs and the results are inconsistent: some studies asserted these models are capable of exhibiting ToM, while others suggest the opposite. In this study, We present ToMChallenges, a dataset for comprehensively evaluating the Theory of Mind based on the Sally-Anne and Smarties tests with a diverse set of tasks. In addition, we also propose an auto-grader to streamline the answer evaluation process. We tested three models: davinci, turbo, and gpt-4. Our evaluation results and error analyses show that LLMs have inconsistent behaviors across prompts and tasks. Performing the ToM tasks robustly remains a challenge for the LLMs. In addition, our paper 
&lt;/p&gt;</description></item><item><title>GRACE&#26159;&#19968;&#31181;&#21028;&#21035;&#22120;&#24341;&#23548;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#36880;&#27493;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#24615;&#21028;&#21035;&#22120;&#26469;&#35780;&#20998;&#19979;&#19968;&#27493;&#20505;&#36873;&#65292;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#23481;&#26131;&#24471;&#21040;&#38169;&#35823;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#25968;&#23398;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;GRACE&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26377;&#26126;&#26174;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14934</link><description>&lt;p&gt;
GRACE: &#21028;&#21035;&#22120;&#24341;&#23548;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
GRACE: Discriminator-Guided Chain-of-Thought Reasoning. (arXiv:2305.14934v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14934
&lt;/p&gt;
&lt;p&gt;
GRACE&#26159;&#19968;&#31181;&#21028;&#21035;&#22120;&#24341;&#23548;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#36880;&#27493;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#24615;&#21028;&#21035;&#22120;&#26469;&#35780;&#20998;&#19979;&#19968;&#27493;&#20505;&#36873;&#65292;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#23481;&#26131;&#24471;&#21040;&#38169;&#35823;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#25968;&#23398;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;GRACE&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26377;&#26126;&#26174;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27493;&#25512;&#29702;&#30340;&#32972;&#26223;&#19979;&#65292;&#20363;&#22914;&#20351;&#29992;&#24605;&#32500;&#38142;&#65292;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#20250;&#23545;&#38169;&#35823;&#30340;&#27493;&#39588;&#20998;&#37197;&#36739;&#39640;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#24615;&#30340;&#35299;&#30721;&#31574;&#30053;&#24448;&#24448;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GRACE&#30340;&#24341;&#23548;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#36880;&#27493;&#35299;&#30721;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#27491;&#30830;&#24615;&#21028;&#21035;&#22120;&#35757;&#32451;&#26469;&#24341;&#23548;&#35299;&#30721;&#36807;&#31243;&#20135;&#29983;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;GRACE&#20351;&#29992;&#19968;&#20010;&#22312;&#27491;&#30830;&#21644;&#38169;&#35823;&#27493;&#39588;&#19978;&#36827;&#34892;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#30340;&#21028;&#21035;&#22120;&#65292;&#35813;&#21028;&#21035;&#22120;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#22522;&#20110;&#27491;&#30830;&#24615;&#23545;&#19979;&#19968;&#27493;&#20505;&#36873;&#36827;&#34892;&#35780;&#20998;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;GRACE&#21482;&#38656;&#35201;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992;FLAN-T5&#21644;LLaMA&#31995;&#21015;&#30340;&#27169;&#22411;&#65292;&#23545;&#22235;&#20010;&#25968;&#23398;&#21644;&#20004;&#20010;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#20102;GRACE&#30340;&#35780;&#20272;&#65292;&#22312;&#22823;&#22810;&#25968;&#35774;&#32622;&#20013;&#65292;&#19982;&#36138;&#23146;&#35299;&#30721;&#12289;&#39564;&#35777;&#22120;&#21644;&#33258;&#19968;&#33268;&#24615;&#30456;&#27604;&#65292;GRACE&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of multi-step reasoning, e.g., with chain-of-thought, language models (LMs) can easily assign a high likelihood to incorrect steps. As a result, decoding strategies that optimize for solution likelihood often yield incorrect solutions. To address this issue, we propose Guiding chain-of-thought ReAsoning with a CorrectnEss Discriminator (GRACE), a stepwise decoding approach that steers the decoding process towards producing correct reasoning steps. GRACE employs a discriminator trained with a contrastive loss over correct and incorrect steps, which is used during decoding to score next-step candidates based on their correctness. Importantly, GRACE only requires sampling from the LM, without the need for LM training or fine-tuning. Using models from FLAN-T5 and LLaMA families, we evaluate GRACE over four math and two symbolic reasoning tasks, where it exhibits substantial performance gains compared to greedy decoding, verifiers, and self-consistency in most settings. When 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#29983;&#25104;&#20197;Python&#20195;&#30721;&#24418;&#24335;&#34920;&#36798;&#30340;&#25991;&#23383;&#28216;&#25103;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;GPT-4&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#28216;&#25103;&#20316;&#20026;&#27169;&#26495;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#22871;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#27169;&#25311;&#36924;&#30495;&#24230;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.14879</link><description>&lt;p&gt;
ByteSized32: &#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20197;&#25991;&#23383;&#28216;&#25103;&#24418;&#24335;&#34920;&#36798;&#30340;&#20219;&#21153;&#29305;&#23450;&#19990;&#30028;&#27169;&#22411;&#30340;&#35821;&#26009;&#24211;&#21644;&#25361;&#25112;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games. (arXiv:2305.14879v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14879
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#29983;&#25104;&#20197;Python&#20195;&#30721;&#24418;&#24335;&#34920;&#36798;&#30340;&#25991;&#23383;&#28216;&#25103;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;GPT-4&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#28216;&#25103;&#20316;&#20026;&#27169;&#26495;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#22871;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#27169;&#25311;&#36924;&#30495;&#24230;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#30340;&#26126;&#30830;&#12289;&#21487;&#35299;&#37322;&#21644;&#20114;&#21160;&#19990;&#30028;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#25805;&#20316;&#21270;&#20026;&#29983;&#25104;&#20197;Python&#20195;&#30721;&#24418;&#24335;&#34920;&#36798;&#30340;&#25991;&#23383;&#28216;&#25103;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20415;&#20110;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ByteSized32&#65292;&#19968;&#20010;&#21253;&#21547;32&#20010;&#20197;&#25512;&#29702;&#20026;&#37325;&#28857;&#30340;&#25991;&#23383;&#28216;&#25103;&#30340;&#35821;&#26009;&#24211;&#65292;&#24635;&#20849;&#26377;2&#19975;&#34892;Python&#20195;&#30721;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;GPT-4&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#28216;&#25103;&#20316;&#20026;&#21333;&#27425;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27169;&#26495;&#65292;&#22312;28%&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#29983;&#25104;&#26410;&#35265;&#36807;&#20027;&#39064;&#30340;&#21487;&#36816;&#34892;&#28216;&#25103;&#12290;&#24403;&#20801;&#35768;&#33258;&#25105;&#21453;&#24605;&#31243;&#24207;&#38169;&#35823;&#26102;&#65292;&#28216;&#25103;&#30340;&#21487;&#36816;&#34892;&#24615;&#22823;&#22823;&#25552;&#39640;&#33267;57%&#12290;&#34429;&#28982;&#35780;&#20272;&#27169;&#25311;&#36924;&#30495;&#24230;&#27604;&#36739;&#36153;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#28216;&#25103;&#30340;&#36924;&#30495;&#24230;&#12289;&#25216;&#26415;&#26377;&#25928;&#24615;&#12289;&#19982;&#20219;&#21153;&#35268;&#26684;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#21487;&#33719;&#32988;&#24615;&#65292;&#26174;&#31034;&#20986;&#19982;&#19987;&#23478;&#20154;&#24037;&#35780;&#32423;&#30456;&#24403;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the capacity of language models to generate explicit, interpretable, and interactive world models of scientific and common-sense reasoning tasks. We operationalize this as a task of generating text games, expressed as hundreds of lines of Python code. To facilitate this task, we introduce ByteSized32 (Code: github.com/cognitiveailab/BYTESIZED32), a corpus of 32 reasoning-focused text games totaling 20k lines of Python code. We empirically demonstrate that GPT-4 can use these games as templates for single-shot in-context learning, successfully producing runnable games on unseen topics in 28% of cases. When allowed to self-reflect on program errors, game runnability substantially increases to 57%. While evaluating simulation fidelity is labor-intensive, we introduce a suite of automated metrics to assess game fidelity, technical validity, adherence to task specifications, and winnability, showing a high degree of agreement with expert human ratings. We pose t
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#32763;&#35793;&#21518;&#32534;&#36753;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#20135;&#29983;&#26377;&#24847;&#20041;&#19988;&#21487;&#38752;&#30340;&#32534;&#36753;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#24182;&#28040;&#38500;&#20102;&#21508;&#31867;&#37325;&#35201;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2305.14878</link><description>&lt;p&gt;
&#21033;&#29992;GPT-4&#36827;&#34892;&#33258;&#21160;&#32763;&#35793;&#21518;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Leveraging GPT-4 for Automatic Translation Post-Editing. (arXiv:2305.14878v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14878
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#32763;&#35793;&#21518;&#32534;&#36753;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#20135;&#29983;&#26377;&#24847;&#20041;&#19988;&#21487;&#38752;&#30340;&#32534;&#36753;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#24182;&#28040;&#38500;&#20102;&#21508;&#31867;&#37325;&#35201;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#20195;&#34920;&#20102;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#30340;&#39046;&#20808;&#26041;&#27861;&#65292;&#20294;NMT&#27169;&#22411;&#30340;&#36755;&#20986;&#20173;&#38656;&#35201;&#36827;&#34892;&#32763;&#35793;&#21518;&#32534;&#36753;&#20197;&#32416;&#27491;&#38169;&#35823;&#24182;&#22312;&#20851;&#38190;&#29615;&#22659;&#19979;&#25552;&#39640;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#30452;&#25509;&#32763;&#35793;&#21518;&#32534;&#36753;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24182;&#25506;&#32034;&#20351;&#29992;GPT-4&#33258;&#21160;&#32763;&#35793;&#21518;&#32534;&#36753;NMT&#36755;&#20986;&#30340;&#22810;&#31181;&#35821;&#35328;&#23545;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292; GPT-4&#22312;&#32763;&#35793;&#21518;&#32534;&#36753;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#20135;&#29983;&#26377;&#24847;&#20041;&#19988;&#21487;&#38752;&#30340;&#32534;&#36753;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#20854;&#24635;&#20307;&#36136;&#37327;&#24182;&#28040;&#38500;&#21508;&#31867;&#37325;&#35201;&#38169;&#35823;&#12290;&#29305;&#21035;&#26159;&#65292;&#20154;&#31867;&#23545;&#32534;&#36753;&#21487;&#38752;&#24615;&#36827;&#34892;&#35780;&#20272;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;LLM&#21462;&#24471;&#20102;&#22823;&#24133;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;GPT-4&#30340;&#32763;&#35793;&#21518;&#32534;&#36753;&#22312;WMT-22&#30340;&#33521;&#20013;&#12289;&#33521;&#24503;&#12289;&#20013;&#33521;&#21644;&#24503;&#33521;&#35821;&#31181;&#23545;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Neural Machine Translation (NMT) represents the leading approach to Machine Translation (MT), the outputs of NMT models still require translation post-editing to rectify errors and enhance quality under critical settings. In this work, we formalize the task of direct translation post-editing with Large Language Models (LLMs) and explore the use of GPT-4 to automatically post-edit NMT outputs across several language pairs. Our results demonstrate that GPT-4 is adept at translation post-editing, producing meaningful and trustworthy edits to translations that help improve its general quality as well as remove different classes of major errors in translations. In particular, human evaluations on assessing edit trustworthiness show that GPT-4 exhibits a large improvement over the prior state-of-the-art LLM. Notably, we improve upon state-of-the-art performance on WMT-22 English-Chinese, English-German, Chinese-English and German-English language pairs using GPT-4 based post-editing, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#21028;&#26029;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#26159;&#21542;&#22987;&#32456;&#20934;&#30830;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13981</link><description>&lt;p&gt;
&#20445;&#25345;&#30693;&#35782;&#19981;&#21464;&#24615;&#65306;&#37325;&#26032;&#24605;&#32771;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction. (arXiv:2305.13981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#21028;&#26029;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#26159;&#21542;&#22987;&#32456;&#20934;&#30830;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#26159;&#30830;&#20445;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#32780;&#35328;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#35780;&#20272;&#22522;&#20934;&#37117;&#19987;&#27880;&#20110;&#39564;&#35777;&#37197;&#23545;&#21305;&#37197;&#30340;&#27491;&#30830;&#24615;&#65292;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#40065;&#26834;&#24615;&#27979;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#27169;&#25311;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#21516;&#19968;&#30693;&#35782;&#21547;&#20041;&#30340;&#21477;&#27861;&#21644;&#34920;&#36798;&#20998;&#24067;&#20250;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#35774;&#35745;&#21644;&#27880;&#37322;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#20854;&#20013;&#27599;&#20010;&#31034;&#20363;&#37117;&#26159;&#19968;&#20010;&#30693;&#35782;&#19981;&#21464;&#30340;&#22242;&#20307;&#65292;&#30001;&#20855;&#26377;&#30456;&#21516;&#21547;&#20041;&#20294;&#32467;&#26500;&#19981;&#21516;&#30340;&#21477;&#23376;&#32452;&#25104;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#38416;&#36848;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#24403;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#22987;&#32456;&#20934;&#30830;&#26102;&#65292;&#34987;&#21028;&#23450;&#20026;&#40065;&#26834;&#24615;&#24378;&#12290;&#25105;&#20204;&#23545;&#36807;&#21435;&#21313;&#24180;&#20013;&#21457;&#34920;&#30340;&#20960;&#31181;&#20856;&#22411;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial measurement of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a 
&lt;/p&gt;</description></item><item><title>Instruct-Align&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#26694;&#26550;&#65292;&#20351;&#24471;&#25945;&#23398;&#35843;&#25972;&#30340;LLMs&#33021;&#22815;&#23398;&#20064;&#26032;&#35821;&#35328;&#65292;&#19988;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2305.13627</link><description>&lt;p&gt;
Instruct-Align&#65306;&#36890;&#36807;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#23558;&#26032;&#35821;&#35328;&#25945;&#32473;LLM
&lt;/p&gt;
&lt;p&gt;
Instruct-Align: Teaching Novel Languages with to LLMs through Alignment-based Cross-Lingual Instruction. (arXiv:2305.13627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13627
&lt;/p&gt;
&lt;p&gt;
Instruct-Align&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#26694;&#26550;&#65292;&#20351;&#24471;&#25945;&#23398;&#35843;&#25972;&#30340;LLMs&#33021;&#22815;&#23398;&#20064;&#26032;&#35821;&#35328;&#65292;&#19988;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#23398;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#21644;&#22810;&#31181;&#20219;&#21153;&#19978;&#30340;&#21331;&#36234;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#19981;&#21516;&#35821;&#35328;&#30340;&#27867;&#21270;&#33021;&#21147;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#23569;&#25968;&#35821;&#35328;&#25110;&#32773;&#26159;&#26410;&#30693;&#35821;&#35328;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#21457;&#29616;&#65292;&#31616;&#21333;&#22320;&#23558;&#26032;&#35821;&#35328;&#36866;&#24212;&#21040;&#32463;&#36807;&#25945;&#23398;&#35843;&#25972;&#30340;LLM&#20013;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20174;&#32780;&#23548;&#33268;&#36825;&#20123;LLM&#22833;&#21435;&#22810;&#20219;&#21153;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31216;&#20026;Instruct-Align&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#65292;&#20351;&#24471;&#32463;&#36807;&#25945;&#23398;&#35843;&#25972;&#30340;LLM&#33021;&#22815;&#23398;&#20064;&#21040;&#30475;&#19981;&#35265;&#30340;&#21644;&#20043;&#21069;&#23398;&#20064;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#12290;&#25105;&#20204;&#22312;BLOOMZ-560M&#25968;&#25454;&#38598;&#19978;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;Instruct-Align&#33021;&#22815;&#22312;&#20165;&#20351;&#29992;&#26377;&#38480;&#37327;&#30340;&#24179;&#34892;&#35821;&#26009;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#35821;&#35328;&#65292;&#24182;&#19988;&#36890;&#36807;&#25345;&#32493;&#30340;&#25945;&#23398;&#35843;&#25972;&#65292;&#38450;&#27490;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) have shown remarkable generalization capability over multiple tasks in multiple languages. Nevertheless, their generalization towards different languages varies especially to underrepresented languages or even to unseen languages. Prior works on adapting new languages to LLMs find that naively adapting new languages to instruction-tuned LLMs will result in catastrophic forgetting, which in turn causes the loss of multitasking ability in these LLMs. To tackle this, we propose the Instruct-Align a.k.a (IA)$^1$ framework, which enables instruction-tuned LLMs to learn cross-lingual alignment between unseen and previously learned languages via alignment-based cross-lingual instruction-tuning. Our preliminary result on BLOOMZ-560M shows that (IA)$^1$ is able to learn a new language effectively with only a limited amount of parallel data and at the same time prevent catastrophic forgetting by applying continual instruction-tuning through experien
&lt;/p&gt;</description></item><item><title>CoEdIT&#26159;&#19968;&#31181;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#32534;&#36753;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#39640;&#29992;&#25143;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#25552;&#39640;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.09857</link><description>&lt;p&gt;
CoEdIT&#65306;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
CoEdIT: Text Editing by Task-Specific Instruction Tuning. (arXiv:2305.09857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09857
&lt;/p&gt;
&lt;p&gt;
CoEdIT&#26159;&#19968;&#31181;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#32534;&#36753;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#39640;&#29992;&#25143;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#25552;&#39640;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32534;&#36753;&#25110;&#20462;&#35746;&#26159;&#20154;&#31867;&#20889;&#20316;&#36807;&#31243;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#21151;&#33021;&#12290;&#29702;&#35299;LLMs&#22312;&#36827;&#34892;&#39640;&#36136;&#37327;&#20462;&#35746;&#21644;&#19982;&#20154;&#31867;&#20889;&#20316;&#32773;&#21327;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#26159;&#26500;&#24314;&#26377;&#25928;&#20889;&#20316;&#21161;&#25163;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;LLMs&#21644;&#25351;&#20196;&#35843;&#25972;&#30340;&#20808;&#21069;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#36827;&#34892;&#25991;&#26412;&#20462;&#35746;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#25552;&#39640;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CoEdIT&#65292;&#36825;&#26159;&#19968;&#27454;&#29992;&#20110;&#20889;&#20316;&#36741;&#21161;&#30340;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#12290;CoEdIT&#20174;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#25351;&#20196;&#65292;&#25351;&#23450;&#25152;&#38656;&#25991;&#26412;&#30340;&#23646;&#24615;&#65292;&#20363;&#22914;&#8220;&#20351;&#21477;&#23376;&#26356;&#31616;&#21333;&#8221;&#25110;&#8220;&#20197;&#26356;&#20013;&#31435;&#30340;&#39118;&#26684;&#20889;&#20316;&#8221;&#65292;&#24182;&#36755;&#20986;&#32534;&#36753;&#21518;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;1&#65289;&#22312;&#21508;&#31181;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#65288;2&#65289;&#19982;&#20844;&#24320;&#21487;&#29992;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text editing or revision is an essential function of the human writing process. Understanding the capabilities of LLMs for making high-quality revisions and collaborating with human writers is a critical step toward building effective writing assistants. With the prior success of LLMs and instruction tuning, we leverage instruction-tuned LLMs for text revision to improve the quality of user-generated text and improve the efficiency of the process. We introduce CoEdIT, a state-of-the-art text editing model for writing assistance. CoEdIT takes instructions from the user specifying the attributes of the desired text, such as "Make the sentence simpler" or "Write it in a more neutral style," and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly availa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReBRAC&#30340;&#26497;&#31616;&#31639;&#27861;&#65292;&#23427;&#22312;TD3+BC&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#25972;&#21512;&#20102;&#35774;&#35745;&#20803;&#32032;&#65292;&#36890;&#36807;&#23545;&#36817;&#26399;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20854;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19978;&#30340;&#39046;&#20808;&#22320;&#20301;&#12290;</title><link>http://arxiv.org/abs/2305.09836</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26497;&#31616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Minimalist Approach to Offline Reinforcement Learning. (arXiv:2305.09836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReBRAC&#30340;&#26497;&#31616;&#31639;&#27861;&#65292;&#23427;&#22312;TD3+BC&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#25972;&#21512;&#20102;&#35774;&#35745;&#20803;&#32032;&#65292;&#36890;&#36807;&#23545;&#36817;&#26399;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20854;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19978;&#30340;&#39046;&#20808;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;&#34429;&#28982;&#36825;&#20123;&#31639;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#24456;&#22810;&#31639;&#27861;&#21253;&#21547;&#20102;&#30475;&#20284;&#24494;&#19981;&#36275;&#36947;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#36825;&#20123;&#36873;&#25321;&#23545;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#36229;&#20986;&#20102;&#26680;&#24515;&#31639;&#27861;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#23545;&#20110;&#24050;&#26377;&#22522;&#32447;&#31639;&#27861;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23545;&#36817;&#26399;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ReBRAC&#30340;&#26497;&#31616;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;TD3+BC&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#25972;&#21512;&#20102;&#36825;&#20123;&#35774;&#35745;&#20803;&#32032;&#12290;&#25105;&#20204;&#20351;&#29992;D4RL&#21644;V-D4RL&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#20102;ReBRAC&#22312;51&#20010;&#20855;&#26377;&#33258;&#25105;&#24863;&#30693;&#21644;&#35270;&#35273;&#29366;&#24577;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#19981;&#38656;&#35201;&#38598;&#25104;&#30340;&#26041;&#27861;&#20013;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35828;&#26126;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#28040;&#34701;&#30740;&#31350;&#21644;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;ReBRAC&#30340;&#25104;&#21151;&#28304;&#20110;&#20854;&#22522;&#20110;&#31574;&#30053;&#25913;&#36827;&#21644;&#35780;&#35770;&#23478;&#27491;&#21017;&#21270;&#30340;&#21407;&#21017;&#24615;&#35774;&#35745;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed significant advancements in offline reinforcement learning (RL), resulting in the development of numerous algorithms with varying degrees of complexity. While these algorithms have led to noteworthy improvements, many incorporate seemingly minor design choices that impact their effectiveness beyond core algorithmic advances. However, the effect of these design choices on established baselines remains understudied. In this work, we aim to bridge this gap by conducting a retrospective analysis of recent works in offline RL and propose ReBRAC, a minimalistic algorithm that integrates such design elements built on top of the TD3+BC method. We evaluate ReBRAC on 51 datasets with both proprioceptive and visual state spaces using D4RL and V-D4RL benchmarks, demonstrating its state-of-the-art performance among ensemble-free methods. To further illustrate the efficacy of these design choices, we perform a large-scale ablation study and hyperparameter sensitivity anal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#24050;&#26377;&#30340;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#36830;&#25509;&#21040;&#35270;&#35273;-&#35821;&#35328;LLM&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#36328;&#19981;&#21516;&#22823;&#23567;&#21644;&#31867;&#22411;&#30340;LLMs&#30340;VPG&#36716;&#31227;&#26041;&#26696;VPGTrans&#65292;&#35813;&#26041;&#26696;&#22312;VQA&#21644;NLVR2&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.01278</link><description>&lt;p&gt;
&#27178;&#21521;&#36801;&#31227;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#22312;VL-LLMs&#20043;&#38388;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transfer Visual Prompt Generator across LLMs. (arXiv:2305.01278v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#24050;&#26377;&#30340;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#36830;&#25509;&#21040;&#35270;&#35273;-&#35821;&#35328;LLM&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#36328;&#19981;&#21516;&#22823;&#23567;&#21644;&#31867;&#22411;&#30340;LLMs&#30340;VPG&#36716;&#31227;&#26041;&#26696;VPGTrans&#65292;&#35813;&#26041;&#26696;&#22312;VQA&#21644;NLVR2&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#65288;VPG&#65289;&#36830;&#25509;&#24050;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;LLM&#65288;VL-LLM&#65289;&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#36328;&#19981;&#21516;&#22823;&#23567;&#21644;&#31867;&#22411;&#30340;LLMs&#30340;VPG&#36716;&#31227;&#26041;&#26696;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;VPGTrans&#30340;&#20004;&#38454;&#27573;&#36716;&#31227;&#26694;&#26550;&#65292;&#23427;&#22312;VQA&#21644;NLVR2&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#36716;&#31227;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
While developing a new vision-language LLM (VL-LLM) by pre-training on tremendous image-text pairs from scratch can be exceedingly resource-consuming, connecting an existing LLM with a comparatively lightweight visual prompt generator (VPG) becomes a feasible paradigm. However, further tuning the VPG part of the VL-LLM still suffers from indispensable computational costs, i.e., requiring thousands of GPU hours and millions of training data. One alternative solution is to transfer an existing VPG from any existing VL-LLMs for the target VL-LLM.  In this work, we for the first time investigate the VPG transferability across LLMs, and explore a solution to reduce the cost of VPG transfer. We first study the VPG transfer across different LLM sizes (e.g., small-to-large), and across different LLM types, through which we diagnose the key factors to maximize the transfer efficiency. Based on our observation, we design a two-stage transfer framework named VPGTrans, which is simple yet highly e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20026;&#35797;&#39564;&#22330;&#65292;&#28145;&#20837;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#31687;&#24314;&#27169;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#21033;&#29992;LLMs&#24378;&#22823;&#30340;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#25552;&#31034;&#26041;&#38754;&#36827;&#34892;&#25913;&#36827;&#20063;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#19988;LLMs&#26377;&#28508;&#21147;&#32534;&#30721;&#20016;&#23500;&#30340;&#35821;&#31687;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.02210</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Document-Level Machine Translation with Large Language Models. (arXiv:2304.02210v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20026;&#35797;&#39564;&#22330;&#65292;&#28145;&#20837;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#31687;&#24314;&#27169;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#21033;&#29992;LLMs&#24378;&#22823;&#30340;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#25552;&#31034;&#26041;&#38754;&#36827;&#34892;&#25913;&#36827;&#20063;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#19988;LLMs&#26377;&#28508;&#21147;&#32534;&#30721;&#20016;&#23500;&#30340;&#35821;&#31687;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Chat-GPT&#21487;&#20197;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#29983;&#25104;&#36830;&#36143;&#65292;&#36830;&#36143;&#65292;&#30456;&#20851;&#21644;&#27969;&#30021;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#20197;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20026;&#35797;&#39564;&#22330;&#65292;&#25552;&#20379;&#20102;LLMs&#22312;&#35821;&#31687;&#24314;&#27169;&#26041;&#38754;&#30340;&#28145;&#20837;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#20851;&#27880;&#19977;&#20010;&#26041;&#38754;&#65306;1&#65289;&#35821;&#31687;&#24863;&#30693;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35843;&#26597;&#19981;&#21516;&#25552;&#31034;&#23545;&#25991;&#26723;&#32423;&#32763;&#35793;&#36136;&#37327;&#21644;&#35821;&#31687;&#29616;&#35937;&#30340;&#24433;&#21709;&#65307;2&#65289;&#32763;&#35793;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#27604;&#36739;Chat-GPT&#19982;&#21830;&#19994;MT&#31995;&#32479;&#21644;&#39640;&#32423;&#25991;&#26723;&#32423;MT&#26041;&#27861;&#30340;&#32763;&#35793;&#24615;&#33021;&#65307;3&#65289;&#35821;&#31687;&#24314;&#27169;&#33021;&#21147;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#20013;&#32534;&#30721;&#30340;&#35821;&#31687;&#30693;&#35782;&#65292;&#24182;&#30740;&#31350;&#22521;&#35757;&#25216;&#26415;&#23545;&#35821;&#31687;&#24314;&#27169;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#35780;&#20272;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#24778;&#35766;&#22320;&#21457;&#29616;&#65292;1&#65289;&#21033;&#29992;&#24378;&#22823;&#30340;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#65292;ChatGPT&#22312;&#25991;&#26723;&#32423;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#21830;&#19994;MT&#31995;&#32479;&#21644;&#39640;&#32423;&#25991;&#26723;&#32423;MT&#26041;&#27861;&#65307;2&#65289;&#20462;&#25913;&#26126;&#30830;&#38024;&#23545;&#35821;&#31687;&#29616;&#35937;&#30340;&#25552;&#31034;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65307;3&#65289;LLMs&#26377;&#28508;&#21147;&#32534;&#30721;&#20016;&#23500;&#30340;&#35821;&#31687;&#30693;&#35782;&#65292;&#22521;&#35757;&#25216;&#26415;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#31181;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as Chat-GPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs' ability on discourse modeling. The study fo-cuses on three aspects: 1) Effects of Discourse-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of Chat-GPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and examine the impact of training techniques on discourse modeling. By evaluating a number of benchmarks, we surprisingly find that 1) leveraging their powerful long-text mod-eling capabilities, ChatGPT outperforms commercial MT systems 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2303.13988</link><description>&lt;p&gt;
&#26426;&#22120;&#24515;&#29702;&#23398;&#65306;&#21033;&#29992;&#24515;&#29702;&#23398;&#26041;&#27861;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32039;&#23494;&#32467;&#21512;&#30340;&#20808;&#38155;&#12290;&#30001;&#20110;&#24555;&#36895;&#25216;&#26415;&#36827;&#27493;&#21644;&#20854;&#26497;&#39640;&#30340;&#36890;&#29992;&#24615;&#65292;&#29616;&#20170;LLM&#24050;&#32463;&#25317;&#26377;&#25968;&#30334;&#19975;&#29992;&#25143;&#65292;&#24182;&#27491;&#22788;&#20110;&#25104;&#20026;&#20027;&#35201;&#20449;&#24687;&#26816;&#32034;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#38382;&#39064;&#35299;&#20915;&#31561;&#25216;&#26415;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#23545;&#20854;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#21644;&#23457;&#26597;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#24403;&#21069;LLM&#20013;&#20986;&#29616;&#24840;&#21152;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#23558;&#20854;&#35270;&#20026;&#21442;&#19982;&#20154;&#31867;&#24515;&#29702;&#23454;&#39564;&#30340;&#23545;&#35937;&#65292;&#20197;&#20415;&#26356;&#20026;&#20840;&#38754;&#22320;&#35780;&#20272;&#20854;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;"&#26426;&#22120;&#24515;&#29702;&#23398;"&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#21508;&#31867;&#24515;&#29702;&#23398;&#20998;&#25903;&#22914;&#20309;&#20026;LLM&#30340;&#34892;&#20026;&#27979;&#35797;&#25552;&#20379;&#26377;&#29992;&#21442;&#32771;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25551;&#36848;&#20102;&#34892;&#20026;&#27979;&#35797;&#32467;&#26524;&#22914;&#20309;&#20026;&#26410;&#26469;&#30340;LLM&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behaviora
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#30340;&#26631;&#35760;&#20195;&#20215;&#22823;&#65292;&#26631;&#35760;&#19981;&#36275;&#12290;&#22240;&#27492;&#21457;&#23637;&#20102;&#39640;&#25928;&#26631;&#35760;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#21644;&#24369;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#36825;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.12484</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#39640;&#25928;&#26631;&#35760;&#28145;&#24230;&#23398;&#20064;&#30340;&#25361;&#25112;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions. (arXiv:2303.12484v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12484
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#30340;&#26631;&#35760;&#20195;&#20215;&#22823;&#65292;&#26631;&#35760;&#19981;&#36275;&#12290;&#22240;&#27492;&#21457;&#23637;&#20102;&#39640;&#25928;&#26631;&#35760;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#21644;&#24369;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#36825;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#36805;&#36895;&#21457;&#23637;&#65292;&#24182;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#35757;&#32451;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#25910;&#38598;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#38656;&#35201;&#26114;&#36149;&#32791;&#26102;&#12290;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65288;MIA&#65289;&#39046;&#22495;&#65292;&#25968;&#25454;&#26377;&#38480;&#65292;&#26631;&#31614;&#24456;&#38590;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#39640;&#25928;&#26631;&#35760;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20805;&#20998;&#21033;&#29992;&#26631;&#35760;&#25968;&#25454;&#20197;&#21450;&#38750;&#26631;&#35760;&#21644;&#24369;&#26631;&#35760;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#36817;300&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#24191;&#27867;&#35843;&#26597;&#65292;&#20197;&#20840;&#38754;&#27010;&#36848;&#26368;&#26032;&#36827;&#23637;&#30340;&#39640;&#25928;&#26631;&#35760;&#23398;&#20064;&#31574;&#30053;&#22312;MIA&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#39640;&#25928;&#26631;&#35760;&#23398;&#20064;&#30340;&#32972;&#26223;&#65292;&#24182;&#23558;&#19981;&#21516;&#26041;&#26696;&#30340;&#26041;&#27861;&#24402;&#31867;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#27599;&#31181;&#26041;&#26696;&#35814;&#32454;&#30740;&#31350;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#35843;&#26597;&#65292;&#35206;&#30422;&#20102;&#19981;&#20165;&#26159;&#26631;&#20934;&#31574;&#30053;&#65292;&#36824;&#21253;&#25324;&#20351;&#29992;&#21518;&#22788;&#29702;&#21644;&#38598;&#21512;&#26041;&#27861;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has seen rapid growth in recent years and achieved state-of-the-art performance in a wide range of applications. However, training models typically requires expensive and time-consuming collection of large quantities of labeled data. This is particularly true within the scope of medical imaging analysis (MIA), where data are limited and labels are expensive to be acquired. Thus, label-efficient deep learning methods are developed to make comprehensive use of the labeled data as well as the abundance of unlabeled and weak-labeled data. In this survey, we extensively investigated over 300 recent papers to provide a comprehensive overview of recent progress on label-efficient learning strategies in MIA. We first present the background of label-efficient learning and categorize the approaches into different schemes. Next, we examine the current state-of-the-art methods in detail through each scheme. Specifically, we provide an in-depth investigation, covering not only canonic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.06273</link><description>&lt;p&gt;
ChatGPT&#30340;&#19968;&#33268;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Consistency Analysis of ChatGPT. (arXiv:2303.06273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the consistency issue of ChatGPT and finds that although it has improved language understanding ability, it frequently fails to generate logically correct predictions. Therefore, further consideration is needed for its real-world applications, especially in terms of risk.
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#31572;&#23545;&#35805;&#31995;&#32479;&#65292;&#33258;&#25512;&#20986;&#20197;&#26469;&#24191;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#23427;&#22312;&#27861;&#24459;&#12289;&#21307;&#23398;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#30340;&#19987;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#32489;&#65292;&#20294;&#20063;&#26377;&#20154;&#23545;&#20854;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24230;&#34920;&#31034;&#24576;&#30097;&#12290;&#26412;&#25991;&#38024;&#23545;ChatGPT&#22312;&#36923;&#36753;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#21487;&#20449;&#24230;&#36827;&#34892;&#20102;&#35843;&#26597;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;ChatGPT&#20284;&#20046;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#23427;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;ChatGPT&#26159;&#19968;&#31181;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#21644;&#26377;&#21069;&#36884;&#30340;&#26032;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22914;&#26524;&#27809;&#26377;&#32463;&#36807;&#24443;&#24213;&#30340;&#20154;&#24037;&#26816;&#26597;&#65292;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, a question-and-answer dialogue system based on a large language model, has gained huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, including the law, medical, and finance domains, adding extra support to the claim that AI now can assist and, even, replace humans in industrial fields. Others, however, doubt its reliability and trustworthiness. In this paper, we investigate ChatGPT's trustworthiness regarding logically consistent behaviours. Our findings suggest that, although ChatGPT seems to achieve an improved language understanding ability, it still fails to generate logically correct predictions frequently. Hence, while it is true that ChatGPT is an impressive and promising new technique, we conclude that its usage in real-world applications without thorough human inspection requires further consideration, especially for risk
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#21644;&#26041;&#38754;&#29305;&#23450;&#65292;&#25105;&#20204;&#22312;&#20116;&#20010;NLG&#20803;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#34920;&#26126;ChatGPT&#20316;&#20026;NLG&#35780;&#20272;&#25351;&#26631;&#24182;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#19968;&#33268;&#65292;&#23588;&#20854;&#26159;&#22312;&#27969;&#30021;&#24230;&#26041;&#38754;&#12290;&#36825;&#25552;&#37266;&#20154;&#20204;&#22312;&#20351;&#29992;ChatGPT&#20316;&#20026;&#21807;&#19968;&#30340;&#33258;&#21160;NLG&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2303.04048</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#35780;&#20215;&#25351;&#26631;&#21487;&#38752;&#21527;&#65311;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good NLG Evaluator? A Preliminary Study. (arXiv:2303.04048v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04048
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#21644;&#26041;&#38754;&#29305;&#23450;&#65292;&#25105;&#20204;&#22312;&#20116;&#20010;NLG&#20803;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#34920;&#26126;ChatGPT&#20316;&#20026;NLG&#35780;&#20272;&#25351;&#26631;&#24182;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#19968;&#33268;&#65292;&#23588;&#20854;&#26159;&#22312;&#27969;&#30021;&#24230;&#26041;&#38754;&#12290;&#36825;&#25552;&#37266;&#20154;&#20204;&#22312;&#20351;&#29992;ChatGPT&#20316;&#20026;&#21807;&#19968;&#30340;&#33258;&#21160;NLG&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#20197;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#20026;&#22522;&#30784;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;ChatGPT&#20316;&#20026;&#19968;&#31181;&#35780;&#20272;&#25351;&#26631;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#32771;&#34385;&#21040;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#27169;&#22411;&#30340;&#36136;&#37327;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;NLG&#25351;&#26631;&#20197;&#20854;&#31967;&#31957;&#30340;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#32780;&#38395;&#21517;&#65292;&#22240;&#27492;&#25105;&#20204;&#26159;&#21542;&#20250;&#35748;&#20026;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;NLG&#35780;&#20272;&#25351;&#26631;&#12290;&#22312;&#36825;&#31687;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#20803;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;ChatGPT&#20316;&#20026;NLG&#25351;&#26631;&#30340;&#21487;&#38752;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;ChatGPT&#35270;&#20026;&#20154;&#31867;&#35780;&#20272;&#22120;&#65292;&#24182;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#65288;&#20363;&#22914;&#25688;&#35201;&#65289;&#21644;&#26041;&#38754;&#29305;&#23450;&#65288;&#20363;&#22914;&#30456;&#20851;&#24615;&#65289;&#36827;&#34892;&#35828;&#26126;&#65292;&#20197;&#20419;&#20351;ChatGPT&#35780;&#20272;NLG&#27169;&#22411;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#25688;&#35201;&#12289;&#25925;&#20107;&#29983;&#25104;&#21644;&#32763;&#35793;&#22312;&#20869;&#30340;&#20116;&#20010;NLG&#20803;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#26041;&#38754;&#65288;&#20363;&#22914;&#27969;&#30021;&#24230;&#65289;&#65292;ChatGPT&#24182;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#19968;&#33268;&#12290;&#36825;&#25552;&#37266;&#20154;&#20204;&#22312;&#20351;&#29992;ChatGPT&#20316;&#20026;&#21807;&#19968;&#30340;&#33258;&#21160;NLG&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and 
&lt;/p&gt;</description></item><item><title>CoSyn&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#21327;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#20132;&#20114;&#26426;&#21046;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#36827;&#34892;&#25805;&#20316;&#65292;&#20197;&#36866;&#24212;&#31038;&#20132;&#23186;&#20307;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.03387</link><description>&lt;p&gt;
CoSyn&#65306;&#20351;&#29992;&#19978;&#19979;&#25991;&#21327;&#21516;&#30340;&#21452;&#26354;&#32447;&#32593;&#32476;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;
&lt;/p&gt;
&lt;p&gt;
CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network. (arXiv:2303.03387v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03387
&lt;/p&gt;
&lt;p&gt;
CoSyn&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#21327;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#20132;&#20114;&#26426;&#21046;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#36827;&#34892;&#25805;&#20316;&#65292;&#20197;&#36866;&#24212;&#31038;&#20132;&#23186;&#20307;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23545;&#35805;&#20013;&#38544;&#21547;&#30340;&#20167;&#24680;&#35328;&#35770;&#23545;&#26469;&#33258;&#21508;&#20010;&#32676;&#20307;&#30340;&#20154;&#20204;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#27492;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#36234;&#26469;&#36234;&#22810;&#12290;&#22823;&#37096;&#20998;&#20043;&#21069;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#20110;&#26816;&#27979;&#26126;&#30830;&#30340;&#20167;&#24680;&#35328;&#35770;&#65292;&#36825;&#20123;&#35328;&#35770;&#26126;&#26174;&#19988;&#21033;&#29992;&#20102;&#20167;&#24680;&#30701;&#35821;&#65292;&#23545;&#20110;&#26816;&#27979;&#38544;&#21547;&#25110;&#36890;&#36807;&#38388;&#25509;&#25110;&#32534;&#30721;&#35821;&#35328;&#34920;&#36798;&#20986;&#30340;&#20167;&#24680;&#35328;&#35770;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoSyn&#65292;&#19968;&#20010;&#19978;&#19979;&#25991;&#21327;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26126;&#30830;&#22320;&#32467;&#21512;&#20102;&#29992;&#25143;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#26469;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#12290;CoSyn&#24341;&#20837;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#32534;&#30721;&#36825;&#20123;&#22806;&#37096;&#19978;&#19979;&#25991;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#20132;&#20114;&#26426;&#21046;&#65292;&#28165;&#26224;&#22320;&#25429;&#25417;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#29420;&#31435;&#35780;&#20272;&#20102;&#20174;&#36825;&#20123;&#22024;&#26434;&#30340;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#30340;&#20449;&#24687;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#36827;&#34892;&#25152;&#26377;&#36825;&#20123;&#25805;&#20316;&#65292;&#20197;&#36866;&#24212;&#31038;&#20132;&#23186;&#20307;&#30340;&#26080;&#26631;&#24230;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tremendous growth of social media users interacting in online conversations has led to significant growth in hate speech, affecting people from various demographics. Most of the prior works focus on detecting explicit hate speech, which is overt and leverages hateful phrases, with very little work focusing on detecting hate speech that is implicit or denotes hatred through indirect or coded language. In this paper, we present CoSyn, a context-synergized neural network that explicitly incorporates user- and conversational context for detecting implicit hate speech in online conversations. CoSyn introduces novel ways to encode these external contexts and employs a novel context interaction mechanism that clearly captures the interplay between them, making independent assessments of the amounts of information to be retrieved from these noisy contexts. Additionally, it carries out all these operations in the hyperbolic space to account for the scale-free dynamics of social media. We de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#39564;&#24615;&#22320;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#19981;&#21516;&#30340;&#33539;&#24335;&#25191;&#34892;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;&#65292;&#24182;&#25104;&#21151;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;CLS&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;CLS&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#26368;&#20339;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2302.14229</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Cross-Lingual Summarization via Large Language Models. (arXiv:2302.14229v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#39564;&#24615;&#22320;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#19981;&#21516;&#30340;&#33539;&#24335;&#25191;&#34892;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;&#65292;&#24182;&#25104;&#21151;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;CLS&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;CLS&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#26368;&#20339;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#28304;&#35821;&#35328;&#25991;&#26412;&#65292;&#36328;&#35821;&#35328;&#25688;&#35201;&#65288;CLS&#65289;&#26088;&#22312;&#29983;&#25104;&#21478;&#19968;&#31181;&#30446;&#26631;&#35821;&#35328;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#27604;&#22914;GPT-3.5&#12289;ChatGPT&#21644;GPT-4&#65292;&#24341;&#36215;&#20102;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;LLM&#22312;CLS&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#26412;&#25991;&#23454;&#39564;&#24615;&#22320;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26469;&#25351;&#23548;LLM&#20174;&#19981;&#21516;&#30340;&#33539;&#24335;&#65288;&#21363;&#31471;&#21040;&#31471;&#21644;&#27969;&#27700;&#32447;&#65289;&#25191;&#34892;&#38646;&#26679;&#26412;CLS&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#25688;&#35201;&#36827;&#34892;&#21021;&#27493;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#21644;GPT-4&#21407;&#26412;&#26356;&#21916;&#27426;&#29983;&#25104;&#35814;&#32454;&#20449;&#24687;&#30340;&#38271;&#25688;&#35201;&#12290;&#20294;&#36825;&#20004;&#20010;LLM&#22312;&#20132;&#20114;&#24335;&#25552;&#31034;&#30340;&#24110;&#21161;&#19979;&#21487;&#20197;&#36827;&#19968;&#27493;&#24179;&#34913;&#20449;&#24687;&#37327;&#21644;&#31616;&#27905;&#24615;&#65292;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;CLS&#24615;&#33021;&#12290;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;CLS&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;CLS&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#26368;&#20339;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a document in a source language, cross-lingual summarization (CLS) aims to generate a summary in a different target language. Recently, the emergence of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has attracted wide attention from the computational linguistics community. However, it is not yet known the performance of LLMs on CLS. In this report, we empirically use various prompts to guide LLMs to perform zero-shot CLS from different paradigms (i.e., end-to-end and pipeline), and provide a preliminary evaluation on the generated summaries. We find that ChatGPT and GPT-4 originally prefer to produce lengthy summaries with detailed information. These two LLMs can further balance informativeness and conciseness with the help of an interactive prompt, significantly improving their CLS performance. Experimental results on three widely-used CLS datasets show that GPT-4 achieves state-of-the-art zero-shot CLS performance, and performs competitively compared with th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.01735</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65306;&#26041;&#24046;&#32553;&#20943;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective. (arXiv:2302.01735v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. The concept of variance-reduced estimation is used to build ARCO, and certain variance-reduction techniques are shown to be particularly beneficial in medical image segmentation.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23545;&#27604;&#23398;&#20064;&#26159;&#25552;&#39640;&#35270;&#35273;&#34920;&#31034;&#36136;&#37327;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#26679;&#26412;&#23545;&#26469;&#23454;&#29616;&#12290;&#36825;&#26159;&#36890;&#36807;&#35266;&#23519;&#21040;&#65292;&#22312;&#27809;&#26377;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#37319;&#26679;&#20855;&#26377;&#30495;&#27491;&#19981;&#21516;&#35299;&#21078;&#29305;&#24449;&#30340;&#36127;&#26679;&#26412;&#65292;&#21017;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#26469;&#33258;&#30456;&#20284;&#30340;&#35299;&#21078;&#29305;&#24449;&#65292;&#27169;&#22411;&#21487;&#33021;&#38590;&#20197;&#21306;&#20998;&#23569;&#25968;&#23614;&#31867;&#26679;&#26412;&#65292;&#20351;&#24471;&#23614;&#31867;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
For medical image segmentation, contrastive learning is the dominant practice to improve the quality of visual representations by contrasting semantically similar and dissimilar pairs of samples. This is enabled by the observation that without accessing ground truth label, negative examples with truly dissimilar anatomical features, if sampled, can significantly improve the performance. In reality, however, these samples may come from similar anatomical features and the models may struggle to distinguish the minority tail-class samples, making the tail classes more prone to misclassification, both of which typically lead to model collapse. In this paper, we propose ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. In particular, we first propose building ARCO through the concept of variance-reduced estimation, and show that certain variance-reduction techniques are particularly beneficial in medical image se
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19978;&#19979;&#25991;&#20462;&#21098;&#20803;&#23398;&#20064;&#23454;&#29616;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;&#35757;&#32451;&#30340;&#20248;&#21270;&#65292; &#26174;&#33879;&#33410;&#30465;&#20869;&#23384;&#65292;&#24182;&#33021;&#22312;&#30701;&#26102;&#38388;&#20869;&#23398;&#20064;&#39640;&#36136;&#37327;&#31070;&#32463;&#22330;&#12290;</title><link>http://arxiv.org/abs/2302.00617</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#20462;&#21098;&#20803;&#23398;&#20064;&#23398;&#20064;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;
&lt;/p&gt;
&lt;p&gt;
Learning Large-scale Neural Fields via Context Pruned Meta-Learning. (arXiv:2302.00617v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00617
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#20462;&#21098;&#20803;&#23398;&#20064;&#23454;&#29616;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;&#35757;&#32451;&#30340;&#20248;&#21270;&#65292; &#26174;&#33879;&#33410;&#30465;&#20869;&#23384;&#65292;&#24182;&#33021;&#22312;&#30701;&#26102;&#38388;&#20869;&#23398;&#20064;&#39640;&#36136;&#37327;&#31070;&#32463;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#22312;&#32447;&#19978;&#19979;&#25991;&#28857;&#36873;&#25321;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#31070;&#32463;&#22330;&#35757;&#32451;&#30340;&#39640;&#25928;&#20248;&#21270;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#23398;&#20064;&#27493;&#39588;&#38598;&#20013;&#22312;&#20855;&#26377;&#26368;&#39640;&#26399;&#26395;&#31435;&#21363;&#27169;&#22411;&#36136;&#37327;&#25913;&#36827;&#30340;&#25968;&#25454;&#23376;&#38598;&#19978;&#65292;&#23454;&#29616;&#20840;&#23616;&#32467;&#26500;&#30340;&#20960;&#20046;&#21363;&#26102;&#24314;&#27169;&#21644;&#39640;&#39057;&#32454;&#33410;&#30340;&#21518;&#32493;&#32454;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#26657;&#27491;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20803;&#23398;&#20064;&#21021;&#22987;&#21270;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20943;&#23569;&#19978;&#19979;&#25991;&#38598;&#26102;&#24341;&#20837;&#30340;&#20219;&#20309;&#35823;&#24046;&#30340;&#26368;&#23567;&#21270;&#65292;&#24182;&#21516;&#26102;&#32531;&#35299;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#25152;&#24102;&#26469;&#30340;&#30701;&#35270;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#20803;&#27979;&#35797;&#26102;&#36827;&#34892;&#26799;&#24230;&#37325;&#26032;&#32553;&#25918;&#65292;&#20174;&#32780;&#22312;&#26174;&#33879;&#32553;&#30701;&#20248;&#21270;&#36807;&#31243;&#30340;&#21516;&#26102;&#23398;&#20064;&#26497;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#22330;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#30452;&#35266;&#26131;&#25026;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#37325;&#26500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an efficient optimization-based meta-learning technique for large-scale neural field training by realizing significant memory savings through automated online context point selection. This is achieved by focusing each learning step on the subset of data with the highest expected immediate improvement in model quality, resulting in the almost instantaneous modeling of global structure and subsequent refinement of high-frequency details. We further improve the quality of our meta-learned initialization by introducing a bootstrap correction resulting in the minimization of any error introduced by reduced context sets while simultaneously mitigating the well-known myopia of optimization-based meta-learning. Finally, we show how gradient re-scaling at meta-test time allows the learning of extremely high-quality neural fields in significantly shortened optimization procedures. Our framework is model-agnostic, intuitive, straightforward to implement, and shows significant reconst
&lt;/p&gt;</description></item><item><title>&#25209;&#37327;&#25552;&#31034;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#30340;&#35745;&#31639;&#21644;&#36130;&#21153;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#19979;&#28216;&#24615;&#33021;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#25209;&#37327;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#20960;&#20046;&#20197;&#20498;&#25968;&#32447;&#24615;&#20851;&#31995;&#38477;&#20302;&#20102;&#25512;&#26029;&#25104;&#26412;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#39564;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#25209;&#37327;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;Chat-based LLMs&#65292;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#25209;&#37327;&#25552;&#31034;&#20063;&#20855;&#26377;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2301.08721</link><description>&lt;p&gt;
&#25209;&#37327;&#25552;&#31034;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;API&#36827;&#34892;&#39640;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Batch Prompting: Efficient Inference with Large Language Model APIs. (arXiv:2301.08721v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08721
&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#25552;&#31034;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#30340;&#35745;&#31639;&#21644;&#36130;&#21153;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#19979;&#28216;&#24615;&#33021;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#25209;&#37327;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#20960;&#20046;&#20197;&#20498;&#25968;&#32447;&#24615;&#20851;&#31995;&#38477;&#20302;&#20102;&#25512;&#26029;&#25104;&#26412;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#39564;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#25209;&#37327;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;Chat-based LLMs&#65292;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#25209;&#37327;&#25552;&#31034;&#20063;&#20855;&#26377;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22823;&#37327;&#26679;&#26412;&#30340;&#25512;&#26029;&#21487;&#33021;&#20250;&#22312;&#35745;&#31639;&#21644;&#36130;&#21153;&#19978;&#20195;&#20215;&#39640;&#26114;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#37327;&#25552;&#31034;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20351;LLM&#33021;&#22815;&#25209;&#37327;&#36827;&#34892;&#25512;&#26029;&#65292;&#32780;&#19981;&#26159;&#36880;&#20010;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;&#20196;&#29260;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19979;&#28216;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#30528;&#27599;&#25209;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#25512;&#26029;&#25104;&#26412;&#20960;&#20046;&#20197;&#20498;&#25968;&#32447;&#24615;&#20851;&#31995;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;&#24120;&#35782;&#38382;&#31572;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;NLI/NLU&#31561;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#24191;&#27867;&#39564;&#35777;&#20102;&#25209;&#37327;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65306;&#25209;&#37327;&#25552;&#31034;&#26174;&#33879;&#65288;&#27599;&#25209;&#20845;&#20010;&#26679;&#26412;&#26102;&#26368;&#39640;&#21487;&#20943;&#23569;5&#20493;&#65289;&#38477;&#20302;&#20102;LLM&#65288;Codex&#65289;&#30340;&#25512;&#26029;&#20196;&#29260;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#25110;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;LLM&#65292;&#20363;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25209;&#37327;&#25552;&#31034;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly~(up to 5x with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5 and GPT-4, we show the benefits of batch prompting also hold. Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#22823;&#27744;&#21270;&#29305;&#24449;&#22270;&#30340;&#20301;&#31227;&#19981;&#21464;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#22797;&#25968;&#27169;&#30340;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#20301;&#31227;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.11740</link><description>&lt;p&gt;
&#20851;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#22823;&#27744;&#21270;&#29305;&#24449;&#22270;&#30340;&#20301;&#31227;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Shift Invariance of Max Pooling Feature Maps in Convolutional Neural Networks. (arXiv:2209.11740v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#22823;&#27744;&#21270;&#29305;&#24449;&#22270;&#30340;&#20301;&#31227;&#19981;&#21464;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#22797;&#25968;&#27169;&#30340;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#20301;&#31227;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#25913;&#21892;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#30340;&#25968;&#23398;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#20854;&#31532;&#19968;&#23618;&#20013;&#20986;&#29616;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#24403;&#22312;&#20687;ImageNet&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20854;&#31532;&#19968;&#23618;&#24448;&#24448;&#23398;&#20064;&#21040;&#19982;&#26041;&#21521;&#36793;&#36890;&#28388;&#27874;&#22120;&#38750;&#24120;&#30456;&#20284;&#30340;&#21442;&#25968;&#12290;&#20351;&#29992;&#36825;&#26679;&#30340;Gabor&#28388;&#27874;&#22120;&#36827;&#34892;&#23376;&#37319;&#26679;&#21367;&#31215;&#23481;&#26131;&#20986;&#29616;&#28151;&#21472;&#38382;&#39064;&#65292;&#23548;&#33268;&#23545;&#36755;&#20837;&#30340;&#23567;&#20559;&#31227;&#25935;&#24863;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26368;&#22823;&#27744;&#21270;&#31639;&#23376;&#36817;&#20284;&#22797;&#25968;&#27169;&#30340;&#26465;&#20214;&#65292;&#20351;&#20854;&#20960;&#20046;&#20855;&#26377;&#20301;&#31227;&#19981;&#21464;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#23376;&#37319;&#26679;&#21367;&#31215;&#21518;&#26368;&#22823;&#27744;&#21270;&#30340;&#20301;&#31227;&#31283;&#23450;&#24615;&#24230;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#28388;&#27874;&#22120;&#30340;&#39057;&#29575;&#21644;&#26041;&#21521;&#22312;&#23454;&#29616;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#32771;&#34385;&#22522;&#20110;&#21452;&#26641;&#22797;&#23567;&#27874;&#21253;&#21464;&#25442;&#30340;&#30830;&#23450;&#24615;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21363;&#31163;&#25955;Gabor&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on improving the mathematical interpretability of convolutional neural networks (CNNs) in the context of image classification. Specifically, we tackle the instability issue arising in their first layer, which tends to learn parameters that closely resemble oriented band-pass filters when trained on datasets like ImageNet. Subsampled convolutions with such Gabor-like filters are prone to aliasing, causing sensitivity to small input shifts. In this context, we establish conditions under which the max pooling operator approximates a complex modulus, which is nearly shift invariant. We then derive a measure of shift invariance for subsampled convolutions followed by max pooling. In particular, we highlight the crucial role played by the filter's frequency and orientation in achieving stability. We experimentally validate our theory by considering a deterministic feature extractor based on the dual-tree complex wavelet packet transform, a particular case of discrete Gabor
&lt;/p&gt;</description></item><item><title>DenseShift&#32593;&#32476;&#26159;&#19968;&#31181;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20302;&#20301;&#24130;&#20056;&#27861;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;Shift&#32593;&#32476;&#30340;&#31934;&#24230;&#21644;&#24341;&#20837;&#38750;&#37327;&#21270;&#28014;&#28857;&#28608;&#27963;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09708</link><description>&lt;p&gt;
DenseShift: &#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20302;&#20301;&#24130;&#20056;&#27861;&#30340;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization. (arXiv:2208.09708v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09708
&lt;/p&gt;
&lt;p&gt;
DenseShift&#32593;&#32476;&#26159;&#19968;&#31181;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20302;&#20301;&#24130;&#20056;&#27861;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;Shift&#32593;&#32476;&#30340;&#31934;&#24230;&#21644;&#24341;&#20837;&#38750;&#37327;&#21270;&#28014;&#28857;&#28608;&#27963;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#36793;&#32536;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20854;&#19981;&#26029;&#22686;&#21152;&#30340;&#36164;&#28304;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#26080;&#20056;&#27861;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#24130;&#20056;&#27861;&#30340;&#37327;&#21270;&#65292;&#20063;&#34987;&#31216;&#20026;Shift&#32593;&#32476;&#65292;&#26088;&#22312;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#21644;&#31616;&#21270;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20302;&#20301;Shift&#32593;&#32476;&#19981;&#22914;&#20840;&#31934;&#24230;&#32593;&#32476;&#20934;&#30830;&#65292;&#36890;&#24120;&#21463;&#21040;&#26377;&#38480;&#26435;&#37325;&#33539;&#22260;&#32534;&#30721;&#26041;&#26696;&#21644;&#37327;&#21270;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DenseShift&#32593;&#32476;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;Shift&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#35270;&#35273;&#21644;&#35821;&#38899;&#24212;&#29992;&#23454;&#29616;&#20102;&#19982;&#20840;&#31934;&#24230;&#32593;&#32476;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#37327;&#21270;&#28014;&#28857;&#28608;&#27963;&#30340;&#39640;&#25928;DenseShift&#32593;&#32476;&#37096;&#32626;&#26041;&#27861;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;1.6&#20493;&#21152;&#36895;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20302;&#20301;Shift&#32593;&#32476;&#20013;&#38646;&#26435;&#37325;&#20540;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently deploying deep neural networks on low-resource edge devices is challenging due to their ever-increasing resource requirements. To address this issue, researchers have proposed multiplication-free neural networks, such as Power-of-Two quantization, or also known as Shift networks, which aim to reduce memory usage and simplify computation. However, existing low-bit Shift networks are not as accurate as their full-precision counterparts, typically suffering from limited weight range encoding schemes and quantization loss. In this paper, we propose the DenseShift network, which significantly improves the accuracy of Shift networks, achieving competitive performance to full-precision networks for vision and speech applications. In addition, we introduce a method to deploy an efficient DenseShift network using non-quantized floating-point activations, while obtaining 1.6X speed-up over existing methods. To achieve this, we demonstrate that zero-weight values in low-bit Shift netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#35299;&#32544;&#35270;&#35282;&#22788;&#29702;&#35270;&#39057;&#39046;&#22495;&#26080;&#30417;&#30563;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#32544;&#38745;&#24577;&#21644;&#21160;&#24577;&#20449;&#24687;&#24182;&#20351;&#29992;&#22810;&#31181;&#32422;&#26463;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#31227;&#38500;&#31354;&#38388;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#21644;&#20943;&#23569;&#26102;&#38388;&#39046;&#22495;&#24046;&#24322;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.07365</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#21160;&#20316;&#35782;&#21035;&#65306;&#19968;&#20010;&#35299;&#32544;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective. (arXiv:2208.07365v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#35299;&#32544;&#35270;&#35282;&#22788;&#29702;&#35270;&#39057;&#39046;&#22495;&#26080;&#30417;&#30563;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#32544;&#38745;&#24577;&#21644;&#21160;&#24577;&#20449;&#24687;&#24182;&#20351;&#29992;&#22810;&#31181;&#32422;&#26463;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#31227;&#38500;&#31354;&#38388;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#21644;&#20943;&#23569;&#26102;&#38388;&#39046;&#22495;&#24046;&#24322;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#19968;&#39033;&#23454;&#36341;&#24615;&#32780;&#21448;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#39318;&#27425;&#20174;&#35299;&#32544;&#35270;&#35282;&#20837;&#25163;&#22788;&#29702;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#36890;&#36807;&#35299;&#32544;&#26469;&#20998;&#21035;&#22788;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#39046;&#22495;&#30340;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#21253;&#21547;&#38745;&#24577;&#20449;&#24687;&#30340;&#19968;&#32452;&#28508;&#22312;&#22240;&#32032;&#21644;&#21253;&#21547;&#21160;&#24577;&#20449;&#24687;&#30340;&#21478;&#19968;&#32452;&#28508;&#22312;&#22240;&#32032;&#20013;&#29983;&#25104;&#36328;&#39046;&#22495;&#35270;&#39057;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36716;&#31227;&#26102;&#24207;VAE&#65288;TranSVAE&#65289;&#26694;&#26550;&#26469;&#24314;&#27169;&#36825;&#31181;&#29983;&#25104;&#36807;&#31243;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#32422;&#26463;&#28508;&#22312;&#22240;&#32032;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#20123;&#32422;&#26463;&#65292;&#38745;&#24577;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#35299;&#32544;&#21487;&#20197;&#36731;&#26494;&#31227;&#38500;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#20174;&#24103;&#21644;&#35270;&#39057;&#23618;&#38754;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#26102;&#38388;&#24046;&#24322;&#12290;&#22312;UCF-HMDB&#12289;Jester&#21644;Epic-Kitchens&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;TranSVAE&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised video domain adaptation is a practical yet challenging task. In this work, for the first time, we tackle it from a disentanglement view. Our key idea is to handle the spatial and temporal domain divergence separately through disentanglement. Specifically, we consider the generation of cross-domain videos from two sets of latent factors, one encoding the static information and another encoding the dynamic information. A Transfer Sequential VAE (TranSVAE) framework is then developed to model such generation. To better serve for adaptation, we propose several objectives to constrain the latent factors. With these constraints, the spatial divergence can be readily removed by disentangling the static domain-specific information out, and the temporal divergence is further reduced from both frame- and video-levels through adversarial learning. Extensive experiments on the UCF-HMDB, Jester, and Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE compared wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#30340;&#26032;&#39062;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#24515;&#33039;&#21608;&#26399;&#24615;&#29305;&#24615;&#65292;&#22312;&#23156;&#20799;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19977;&#31181;&#21464;&#20998;&#28508;&#22312;&#36712;&#36857;&#27169;&#22411;&#65292;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#20005;&#37325;&#30340;&#20808;&#22825;&#24615;&#24515;&#33039;&#32570;&#38519;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.15316</link><description>&lt;p&gt;
&#21160;&#24577;&#21464;&#20998;&#36712;&#36857;&#27169;&#22411;&#20013;&#30340;&#24515;&#33039;&#36229;&#22768;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection in Echocardiograms with Dynamic Variational Trajectory Models. (arXiv:2206.15316v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#30340;&#26032;&#39062;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#24515;&#33039;&#21608;&#26399;&#24615;&#29305;&#24615;&#65292;&#22312;&#23156;&#20799;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19977;&#31181;&#21464;&#20998;&#28508;&#22312;&#36712;&#36857;&#27169;&#22411;&#65292;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#20005;&#37325;&#30340;&#20808;&#22825;&#24615;&#24515;&#33039;&#32570;&#38519;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#30340;&#26032;&#39062;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24515;&#33039;&#21608;&#26399;&#24615;&#30340;&#29305;&#24615;&#65292;&#23398;&#20064;&#19977;&#31181;&#21464;&#20998;&#28508;&#22312;&#36712;&#36857;&#27169;&#22411;&#65288;TVAE&#65289;&#30340;&#21464;&#20307;&#12290;&#20854;&#20013;&#21069;&#20004;&#31181;&#21464;&#20307;&#65288;TVAE-C&#21644;TVAE-R&#65289;&#27169;&#25311;&#24515;&#33039;&#30340;&#20005;&#26684;&#21608;&#26399;&#24615;&#36816;&#21160;&#65292;&#32780;&#31532;&#19977;&#31181;&#21464;&#20307;&#65288;TVAE-S&#65289;&#26356;&#20026;&#36890;&#29992;&#65292;&#20801;&#35768;&#35270;&#39057;&#20013;&#31354;&#38388;&#34920;&#31034;&#30340;&#31227;&#20301;&#12290;&#25152;&#26377;&#27169;&#22411;&#37117;&#22312;&#19968;&#20010;&#26032;&#30340;&#20869;&#37096;&#23156;&#20799;&#24515;&#33039;&#36229;&#22768;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#20581;&#24247;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23398;&#20064;&#20581;&#24247;&#20154;&#32676;&#30340;&#35268;&#33539;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#65288;MAP&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#26469;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#31163;&#32676;&#26679;&#26412;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#20005;&#37325;&#30340;&#20808;&#22825;&#24615;&#24515;&#33039;&#32570;&#38519;&#65292;&#22914;&#22467;&#26222;&#26031;&#22374;&#24322;&#24120;&#25110;Shone&#32508;&#21512;&#24449;&#12290;&#27492;&#22806;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#26631;&#20934;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;MAP-based&#24322;&#24120;&#26816;&#27979;&#65292;&#23427;&#23454;&#29616;&#20102;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel anomaly detection method for echocardiogram videos. The introduced method takes advantage of the periodic nature of the heart cycle to learn three variants of a variational latent trajectory model (TVAE). While the first two variants (TVAE-C and TVAE-R) model strict periodic movements of the heart, the third (TVAE-S) is more general and allows shifts in the spatial representation throughout the video. All models are trained on the healthy samples of a novel in-house dataset of infant echocardiogram videos consisting of multiple chamber views to learn a normative prior of the healthy population. During inference, maximum a posteriori (MAP) based anomaly detection is performed to detect out-of-distribution samples in our dataset. The proposed method reliably identifies severe congenital heart defects, such as Ebstein's Anomaly or Shone-complex. Moreover, it achieves superior performance over MAP-based anomaly detection with standard variational autoencoders when detect
&lt;/p&gt;</description></item><item><title>JAMES&#26159;&#19968;&#20010;&#29992;&#20110;&#23703;&#20301;&#32844;&#31216;&#35268;&#33539;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#19977;&#31181;&#29420;&#29305;&#23884;&#20837;&#21644;&#20351;&#29992;&#21327;&#21516;&#27880;&#24847;&#26426;&#21046;&#21644;&#31070;&#32463;&#36923;&#36753;&#25512;&#29702;&#34920;&#31034;&#26469;&#26377;&#25928;&#22320;&#25429;&#25417;&#23703;&#20301;&#32844;&#31216;&#30340;&#21508;&#31181;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#35821;&#20041;&#30456;&#20284;&#24615;&#12289;&#38750;&#35268;&#33539;&#21270;&#29992;&#25143;&#21019;&#24314;&#30340;&#32844;&#31216;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#20013;&#22823;&#35268;&#27169;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#23703;&#20301;&#32844;&#31216;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2202.10739</link><description>&lt;p&gt;
JAMES: &#22522;&#20110;&#22810;&#26041;&#38754;&#22270;&#23884;&#20837;&#21644;&#25512;&#29702;&#30340;&#23703;&#20301;&#32844;&#31216;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
JAMES: Normalizing Job Titles with Multi-Aspect Graph Embeddings and Reasoning. (arXiv:2202.10739v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10739
&lt;/p&gt;
&lt;p&gt;
JAMES&#26159;&#19968;&#20010;&#29992;&#20110;&#23703;&#20301;&#32844;&#31216;&#35268;&#33539;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#19977;&#31181;&#29420;&#29305;&#23884;&#20837;&#21644;&#20351;&#29992;&#21327;&#21516;&#27880;&#24847;&#26426;&#21046;&#21644;&#31070;&#32463;&#36923;&#36753;&#25512;&#29702;&#34920;&#31034;&#26469;&#26377;&#25928;&#22320;&#25429;&#25417;&#23703;&#20301;&#32844;&#31216;&#30340;&#21508;&#31181;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#35821;&#20041;&#30456;&#20284;&#24615;&#12289;&#38750;&#35268;&#33539;&#21270;&#29992;&#25143;&#21019;&#24314;&#30340;&#32844;&#31216;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#20013;&#22823;&#35268;&#27169;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#23703;&#20301;&#32844;&#31216;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#32844;&#20301;&#24066;&#22330;&#20013;&#65292;&#24314;&#31435;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#23703;&#20301;&#32844;&#31216;&#20998;&#31867;&#20307;&#31995;&#23545;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#24037;&#20316;&#25512;&#33616;&#12289;&#29992;&#25143;&#32844;&#19994;&#20998;&#26512;&#21644;&#31163;&#32844;&#39044;&#27979;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#23703;&#20301;&#32844;&#31216;&#35268;&#33539;&#21270;&#26159;&#23558;&#29992;&#25143;&#21019;&#24314;&#30340;&#38750;&#26631;&#20934;&#23703;&#20301;&#32844;&#31216;&#20998;&#31867;&#20026;&#35268;&#33539;&#21270;&#32844;&#31216;&#30340;&#19968;&#20010;&#28165;&#27905;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#23703;&#20301;&#32844;&#31216;&#35268;&#33539;&#21270;&#38382;&#39064;&#24182;&#19981;&#23481;&#26131;&#65292;&#38754;&#20020;&#30528;&#20197;&#19979;&#25361;&#25112;&#65306;(1)&#19981;&#21516;&#23703;&#20301;&#32844;&#31216;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;(2)&#38750;&#35268;&#33539;&#21270;&#30340;&#29992;&#25143;&#21019;&#24314;&#30340;&#23703;&#20301;&#32844;&#31216;&#65292;&#20197;&#21450;(3)&#23454;&#38469;&#24212;&#29992;&#20013;&#22823;&#35268;&#27169;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#23703;&#20301;&#32844;&#31216;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JAMES&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#26500;&#24314;&#30446;&#26631;&#23703;&#20301;&#32844;&#31216;&#30340;&#19977;&#31181;&#29420;&#29305;&#23884;&#20837;&#65288;&#21363;&#22270;&#12289;&#19978;&#19979;&#25991;&#21644;&#21477;&#27861;&#65289;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#20854;&#21508;&#31181;&#29305;&#24449;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#21327;&#21516;&#27880;&#24847;&#26426;&#21046;&#26469;&#27880;&#24847;&#22320;&#32467;&#21512;&#36825;&#20123;&#23884;&#20837;&#65292;&#36824;&#20351;&#29992;&#31070;&#32463;&#36923;&#36753;&#25512;&#29702;&#34920;&#31034;&#20849;&#21516;&#20272;&#35745;&#28151;&#20081;&#30340;&#23703;&#20301;&#32844;&#31216;&#19982;&#35268;&#33539;&#21270;&#23703;&#20301;&#32844;&#31216;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online job marketplaces, it is important to establish a well-defined job title taxonomy for various downstream tasks (e.g., job recommendation, users' career analysis, and turnover prediction). Job Title Normalization (JTN) is such a cleaning step to classify user-created non-standard job titles into normalized ones. However, solving the JTN problem is non-trivial with challenges: (1) semantic similarity of different job titles, (2) non-normalized user-created job titles, and (3) large-scale and long-tailed job titles in real-world applications. To this end, we propose a novel solution, named JAMES, that constructs three unique embeddings (i.e., graph, contextual, and syntactic) of a target job title to effectively capture its various traits. We further propose a multi-aspect co-attention mechanism to attentively combine these embeddings, and employ neural logical reasoning representations to collaboratively estimate similarities between messy job titles and normalized job titles in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#36817;&#20284;&#20056;&#27861;&#22120;&#65292;&#24182;&#26681;&#25454;&#25805;&#20316;&#25968;&#20998;&#24067;&#26469;&#26368;&#23567;&#21270;&#24179;&#22343;&#35823;&#24046;&#12290;&#25152;&#25552;&#20056;&#27861;&#22120;&#22312;DNN&#20013;&#36798;&#21040;&#20102;&#27604;&#26368;&#20339;&#22797;&#21046;&#30340;&#36817;&#20284;&#20056;&#27861;&#22120;&#39640;&#36798;50.24%&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#36739;&#23567;&#30340;&#38754;&#31215;&#12289;&#21151;&#32791;&#21644;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2201.08022</link><description>&lt;p&gt;
HEAM: &#39640;&#25928;&#36817;&#20284;&#20056;&#27861;&#22120;&#20248;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HEAM: High-Efficiency Approximate Multiplier Optimization for Deep Neural Networks. (arXiv:2201.08022v4 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#36817;&#20284;&#20056;&#27861;&#22120;&#65292;&#24182;&#26681;&#25454;&#25805;&#20316;&#25968;&#20998;&#24067;&#26469;&#26368;&#23567;&#21270;&#24179;&#22343;&#35823;&#24046;&#12290;&#25152;&#25552;&#20056;&#27861;&#22120;&#22312;DNN&#20013;&#36798;&#21040;&#20102;&#27604;&#26368;&#20339;&#22797;&#21046;&#30340;&#36817;&#20284;&#20056;&#27861;&#22120;&#39640;&#36798;50.24%&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#36739;&#23567;&#30340;&#38754;&#31215;&#12289;&#21151;&#32791;&#21644;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#36817;&#20284;&#20056;&#27861;&#22120;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#26681;&#25454;&#25805;&#20316;&#25968;&#20998;&#24067;&#26368;&#23567;&#21270;&#24179;&#22343;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#20056;&#27861;&#22120;&#22312;DNN&#20013;&#27604;&#26368;&#20339;&#22797;&#21046;&#30340;&#36817;&#20284;&#20056;&#27861;&#22120;&#39640;&#36798;50.24%&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#38754;&#31215;&#20943;&#23567;15.76%&#65292;&#21151;&#32791;&#20943;&#23569;25.05%&#65292;&#24310;&#36831;&#32553;&#30701;3.50%&#12290;&#19982;&#31934;&#30830;&#20056;&#27861;&#22120;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#20056;&#27861;&#22120;&#20998;&#21035;&#20943;&#23569;&#20102;44.94%&#30340;&#38754;&#31215;&#12289;47.63%&#30340;&#21151;&#32791;&#21644;16.78%&#30340;&#24310;&#36831;&#65292;&#20960;&#20046;&#27809;&#26377;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#20056;&#27861;&#22120;&#36827;&#34892;&#27979;&#35797;&#30340;DNN&#21152;&#36895;&#22120;&#27169;&#22359;&#27604;&#21407;&#22987;&#27169;&#22359;&#38754;&#31215;&#20943;&#23567;&#20102;18.70%&#65292;&#21151;&#32791;&#20943;&#23569;&#20102;9.99%&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an optimization method for the automatic design of approximate multipliers, which minimizes the average error according to the operand distributions. Our multiplier achieves up to 50.24% higher accuracy than the best reproduced approximate multiplier in DNNs, with 15.76% smaller area, 25.05% less power consumption, and 3.50% shorter delay. Compared with an exact multiplier, our multiplier reduces the area, power consumption, and delay by 44.94%, 47.63%, and 16.78%, respectively, with negligible accuracy losses. The tested DNN accelerator modules with our multiplier obtain up to 18.70% smaller area and 9.99% less power consumption than the original modules.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#31616;&#21333;&#30340;&#20915;&#31574;&#26641;&#25193;&#23637;&#65292;&#36890;&#36807;&#32487;&#32493;&#29983;&#38271;&#29616;&#26377;&#26641;&#26469;&#26356;&#26032;&#23427;&#20204;&#65292;&#24182;&#29992;&#26032;&#26641;&#26367;&#25442;&#19968;&#20123;&#26087;&#26641;&#26469;&#25511;&#21046;&#24635;&#26641;&#30340;&#25968;&#37327;&#12290;&#22312;72&#20010;&#20998;&#31867;&#38382;&#39064;&#30340;&#22522;&#20934;&#22871;&#20214;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2110.08483</link><description>&lt;p&gt;
&#26368;&#31616;&#26131;&#30340;&#27969;&#24335;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Simplest Streaming Trees. (arXiv:2110.08483v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08483
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#31616;&#21333;&#30340;&#20915;&#31574;&#26641;&#25193;&#23637;&#65292;&#36890;&#36807;&#32487;&#32493;&#29983;&#38271;&#29616;&#26377;&#26641;&#26469;&#26356;&#26032;&#23427;&#20204;&#65292;&#24182;&#29992;&#26032;&#26641;&#26367;&#25442;&#19968;&#20123;&#26087;&#26641;&#26469;&#25511;&#21046;&#24635;&#26641;&#30340;&#25968;&#37327;&#12290;&#22312;72&#20010;&#20998;&#31867;&#38382;&#39064;&#30340;&#22522;&#20934;&#22871;&#20214;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26862;&#26519;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#26641;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#25968;&#25454;&#38382;&#39064;&#19978;&#20173;&#28982;&#26159;&#20027;&#27969;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#24403;&#21069;&#30340;&#23454;&#29616;&#21482;&#33021;&#20197;&#25209;&#22788;&#29702;&#27169;&#24335;&#36816;&#34892;&#65292;&#22240;&#27492;&#19981;&#33021;&#22312;&#26377;&#26356;&#22810;&#25968;&#25454;&#21040;&#36798;&#26102;&#36827;&#34892;&#22686;&#37327;&#26356;&#26032;&#12290;&#20043;&#21069;&#26377;&#20960;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#27969;&#24335;&#20915;&#31574;&#26641;&#21644;&#38598;&#25104;&#26469;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26368;&#26032;&#31639;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#22312;&#26576;&#20123;&#38382;&#39064;&#19978;&#31934;&#24230;&#20302;&#21644;&#22312;&#20854;&#20182;&#38382;&#39064;&#19978;&#20869;&#23384;&#20351;&#29992;&#37327;&#22823;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26368;&#31616;&#21333;&#30340;&#20915;&#31574;&#26641;&#25193;&#23637;&#65306;&#32473;&#23450;&#26032;&#25968;&#25454;&#26102;&#65292;&#36890;&#36807;&#32487;&#32493;&#29983;&#38271;&#29616;&#26377;&#26641;&#26469;&#26356;&#26032;&#23427;&#20204;&#65292;&#24182;&#29992;&#26032;&#26641;&#26367;&#25442;&#19968;&#20123;&#26087;&#26641;&#26469;&#25511;&#21046;&#24635;&#26641;&#30340;&#25968;&#37327;&#12290;&#22312;&#21253;&#21547;72&#20010;&#20998;&#31867;&#38382;&#39064;&#30340;&#22522;&#20934;&#22871;&#20214;&#65288;OpenML-CC18&#25968;&#25454;&#22871;&#20214;&#65289;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;Stream Decision Forest&#65288;SDF&#65289;&#26082;&#19981;&#36973;&#21463;&#19978;&#36848;&#38382;&#39064;&#30340;&#22256;&#25200;
&lt;/p&gt;
&lt;p&gt;
Decision forests, including random forests and gradient boosting trees, remain the leading machine learning methods for many real-world data problems, especially on tabular data. However, most of the current implementations only operate in batch mode, and therefore cannot incrementally update when more data arrive. Several previous works developed streaming trees and ensembles to overcome this limitation. Nonetheless, we found that those state-of-the-art algorithms suffer from a number of drawbacks, including low accuracy on some problems and high memory usage on others. We therefore developed the simplest possible extension of decision trees: given new data, simply update existing trees by continuing to grow them, and replace some old trees with new ones to control the total number of trees. In a benchmark suite containing 72 classification problems (the OpenML-CC18 data suite), we illustrate that our approach, Stream Decision Forest (SDF), does not suffer from either of the aforement
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Bellman&#19968;&#33268;&#30340;&#24754;&#35266;&#35770;&#36848;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#65292;&#36890;&#36807;&#22312;&#19982;Bellman&#26041;&#31243;&#19968;&#33268;&#30340;&#20989;&#25968;&#38598;&#21512;&#19978;&#23454;&#26045;&#21021;&#22987;&#29366;&#24577;&#30340;&#24754;&#35266;&#20027;&#20041;&#65292;&#25913;&#21892;&#20102;&#22522;&#20110;&#22870;&#21169;&#30340;&#24754;&#35266;&#20027;&#20041;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.06926</link><description>&lt;p&gt;
Bellman&#19968;&#33268;&#30340;&#24754;&#35266;&#35770;&#36848;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bellman-consistent Pessimism for Offline Reinforcement Learning. (arXiv:2106.06926v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Bellman&#19968;&#33268;&#30340;&#24754;&#35266;&#35770;&#36848;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#65292;&#36890;&#36807;&#22312;&#19982;Bellman&#26041;&#31243;&#19968;&#33268;&#30340;&#20989;&#25968;&#38598;&#21512;&#19978;&#23454;&#26045;&#21021;&#22987;&#29366;&#24577;&#30340;&#24754;&#35266;&#20027;&#20041;&#65292;&#25913;&#21892;&#20102;&#22522;&#20110;&#22870;&#21169;&#30340;&#24754;&#35266;&#20027;&#20041;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#25512;&#29702;&#25968;&#25454;&#38598;&#32570;&#20047;&#35814;&#23613;&#25506;&#32034;&#26102;&#65292;&#20351;&#29992;&#24754;&#35266;&#20027;&#20041;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#24754;&#35266;&#20027;&#20041;&#22686;&#21152;&#20102;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#36807;&#24230;&#24754;&#35266;&#30340;&#25512;&#29702;&#21516;&#26679;&#20250;&#38459;&#30861;&#21457;&#29616;&#33391;&#22909;&#31574;&#30053;&#65292;&#36825;&#23545;&#20110;&#27969;&#34892;&#30340;&#22522;&#20110;&#22870;&#21169;&#30340;&#24754;&#35266;&#20027;&#20041;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Bellman&#19968;&#33268;&#30340;&#24754;&#35266;&#20027;&#20041;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#65306;&#25105;&#20204;&#19981;&#26159;&#35745;&#31639;&#20540;&#20989;&#25968;&#30340;&#36880;&#28857;&#19979;&#30028;&#65292;&#32780;&#26159;&#22312;&#19982;Bellman&#26041;&#31243;&#19968;&#33268;&#30340;&#20989;&#25968;&#38598;&#21512;&#19978;&#23454;&#26045;&#21021;&#22987;&#29366;&#24577;&#19978;&#30340;&#24754;&#35266;&#20027;&#20041;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20445;&#35777;&#20165;&#38656;&#35201;&#26631;&#20934;&#30340;Bellman&#23553;&#38381;&#24615;&#20316;&#20026;&#25506;&#32034;&#24615;&#35774;&#32622;&#20013;&#30340;&#35201;&#27714;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#22870;&#21169;&#30340;&#24754;&#35266;&#20027;&#20041;&#26080;&#27861;&#25552;&#20379;&#20445;&#35777;&#12290;&#21363;&#20351;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#26356;&#24378;&#30340;&#34920;&#29616;&#21147;&#20551;&#35774;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#26679;&#26412;&#22797;&#26434;&#24615;&#19978;&#20248;&#20110;&#26368;&#36817;&#30340;&#22522;&#20110;&#22870;&#21169;&#30340;&#26041;&#27861;&#65292;&#22797;&#26434;&#24615;&#25913;&#21892;&#20102;&#927;(d)&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of pessimism, when reasoning about datasets lacking exhaustive exploration has recently gained prominence in offline reinforcement learning. Despite the robustness it adds to the algorithm, overly pessimistic reasoning can be equally damaging in precluding the discovery of good policies, which is an issue for the popular bonus-based pessimism. In this paper, we introduce the notion of Bellman-consistent pessimism for general function approximation: instead of calculating a point-wise lower bound for the value function, we implement pessimism at the initial state over the set of functions consistent with the Bellman equations. Our theoretical guarantees only require Bellman closedness as standard in the exploratory setting, in which case bonus-based pessimism fails to provide guarantees. Even in the special case of linear function approximation where stronger expressivity assumptions hold, our result improves upon a recent bonus-based approach by $\mathcal{O}(d)$ in its sample c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ELHr&#25551;&#36848;&#36923;&#36753;&#20013;&#22788;&#29702;&#28335;&#28304;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#28335;&#28304;&#26631;&#35760;&#12289;&#28335;&#28304;&#22810;&#39033;&#24335;&#20197;&#21450;&#20551;&#35774;&#21322;&#29615;&#20855;&#26377;&#20056;&#27861;&#24130;&#31561;&#24615;&#26469;&#35299;&#20915;&#21512;&#21462;&#25805;&#20316;&#24102;&#26469;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;ELHr&#30340;&#35821;&#20041;&#65292;&#24182;&#30740;&#31350;&#20102;&#24102;&#28335;&#28304;&#30340;&#26412;&#20307;&#34917;&#20840;&#12289;&#35745;&#31639;&#19982;&#21518;&#26524;&#30456;&#20851;&#30340;&#20844;&#29702;&#38598;&#21512;&#21644;&#26597;&#35810;&#22238;&#31572;&#19977;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2001.07541</link><description>&lt;p&gt;
ELHr&#25551;&#36848;&#36923;&#36753;&#20013;&#30340;&#28335;&#28304;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Provenance for the Description Logic ELHr. (arXiv:2001.07541v3 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.07541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ELHr&#25551;&#36848;&#36923;&#36753;&#20013;&#22788;&#29702;&#28335;&#28304;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#28335;&#28304;&#26631;&#35760;&#12289;&#28335;&#28304;&#22810;&#39033;&#24335;&#20197;&#21450;&#20551;&#35774;&#21322;&#29615;&#20855;&#26377;&#20056;&#27861;&#24130;&#31561;&#24615;&#26469;&#35299;&#20915;&#21512;&#21462;&#25805;&#20316;&#24102;&#26469;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;ELHr&#30340;&#35821;&#20041;&#65292;&#24182;&#30740;&#31350;&#20102;&#24102;&#28335;&#28304;&#30340;&#26412;&#20307;&#34917;&#20840;&#12289;&#35745;&#31639;&#19982;&#21518;&#26524;&#30456;&#20851;&#30340;&#20844;&#29702;&#38598;&#21512;&#21644;&#26597;&#35810;&#22238;&#31572;&#19977;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22788;&#29702;ELHr&#26412;&#20307;&#20013;&#30340;&#28335;&#28304;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26368;&#36817;&#22312;&#22522;&#20110;&#26412;&#20307;&#30340;&#25968;&#25454;&#35775;&#38382;&#20013;&#24341;&#20837;&#30340;&#19968;&#20010;&#35774;&#32622;&#65292;&#35813;&#35774;&#32622;&#22522;&#20110;&#21322;&#29615;&#24182;&#25193;&#23637;&#20102;&#32463;&#20856;&#25968;&#25454;&#28335;&#28304;&#65292;&#22312;&#35813;&#35774;&#32622;&#20013;&#65292;&#26412;&#20307;&#20844;&#29702;&#38468;&#26377;&#28335;&#28304;&#26631;&#35760;&#12290;&#21518;&#26524;&#20250;&#32487;&#25215;&#34893;&#29983;&#23427;&#30340;&#20844;&#29702;&#30340;&#28335;&#28304;&#65292;&#20135;&#29983;&#19968;&#20010;&#28335;&#28304;&#22810;&#39033;&#24335;&#20316;&#20026;&#27880;&#37322;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;ELHr&#24773;&#20917;&#19979;&#30340;&#35821;&#20041;&#65292;&#24182;&#23637;&#31034;&#20102;&#21512;&#21462;&#25805;&#20316;&#23545;&#22788;&#29702;&#28335;&#28304;&#25552;&#20986;&#20102;&#21508;&#31181;&#22256;&#38590;&#65292;&#20854;&#20013;&#19968;&#20123;&#22256;&#38590;&#21487;&#20197;&#36890;&#36807;&#20551;&#35774;&#21322;&#29615;&#20855;&#26377;&#20056;&#27861;&#24130;&#31561;&#24615;&#26469;&#20943;&#36731;&#12290;&#22312;&#36825;&#20010;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#38382;&#39064;&#65306;&#24102;&#26377;&#28335;&#28304;&#30340;&#26412;&#20307;&#34917;&#20840;&#12289;&#35745;&#31639;&#19982;&#21518;&#26524;&#30456;&#20851;&#30340;&#20844;&#29702;&#38598;&#21512;&#20197;&#21450;&#26597;&#35810;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of handling provenance information in ELHr ontologies. We consider a setting recently introduced for ontology-based data access, based on semirings and extending classical data provenance, in which ontology axioms are annotated with provenance tokens. A consequence inherits the provenance of the axioms involved in deriving it, yielding a provenance polynomial as an annotation. We analyse the semantics for the ELHr case and show that the presence of conjunctions poses various difficulties for handling provenance, some of which are mitigated by assuming multiplicative idempotency of the semiring. Under this assumption, we study three problems: ontology completion with provenance, computing the set of relevant axioms for a consequence, and query answering.
&lt;/p&gt;</description></item></channel></rss>