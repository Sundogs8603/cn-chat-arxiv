<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;SECToR&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#65292;</title><link>http://arxiv.org/abs/2309.08589</link><description>&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#26159;&#19968;&#31181;&#31574;&#30053;&#25913;&#36827;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Reasoning is a Policy Improvement Operator. (arXiv:2309.08589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08589
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;SECToR&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#20854;&#20196;&#20154;&#36190;&#21497;&#30340;&#26032;&#33021;&#21147;&#20196;&#19990;&#30028;&#20026;&#20043;&#24778;&#21497;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30446;&#21069;&#32570;&#20047;&#33258;&#25105;&#23398;&#20064;&#26032;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#25509;&#21463;&#22823;&#37327;&#30001;&#20154;&#31867;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SECToR&#65288;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#23454;&#29616;&#33258;&#25105;&#25945;&#32946;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#12290;&#21463;&#21040;&#20197;&#21069;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;Silver&#31561;&#20154;&#65292;2017&#65289;&#21644;&#20154;&#31867;&#35748;&#30693;&#65288;Kahneman&#65292;2011&#65289;&#20013;&#30340;&#30456;&#20851;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;SECToR&#39318;&#20808;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#36880;&#28176;&#24605;&#32771;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;SECToR&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#29983;&#25104;&#30456;&#21516;&#30340;&#31572;&#26696;&#65292;&#36825;&#27425;&#19981;&#20877;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#12290;&#36890;&#36807;SECToR&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#23398;&#20250;&#20102;&#36827;&#34892;&#22810;&#36798;29&#20301;&#25968;&#23383;&#30340;&#21152;&#27861;&#36816;&#31639;&#65292;&#32780;&#27809;&#26377;&#20219;&#20309;&#36229;&#36807;6&#20301;&#25968;&#23383;&#30340;&#22522;&#20934;&#30495;&#23454;&#31034;&#20363;&#65292;&#20165;&#36890;&#36807;&#21021;&#22987;&#30340;&#30417;&#30563;&#24494;&#35843;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Large language models have astounded the world with fascinating new capabilities. However, they currently lack the ability to teach themselves new skills, relying instead on being trained on large amounts of human-generated data. We introduce SECToR (Self-Education via Chain-of-Thought Reasoning), a proof-of-concept demonstration that language models can successfully teach themselves new skills using chain-of-thought reasoning. Inspired by previous work in both reinforcement learning (Silver et al., 2017) and human cognition (Kahneman, 2011), SECToR first uses chain-of-thought reasoning to slowly think its way through problems. SECToR then fine-tunes the model to generate those same answers, this time without using chain-of-thought reasoning. Language models trained via SECToR autonomously learn to add up to 29-digit numbers without any access to any ground truth examples beyond an initial supervised fine-tuning phase consisting only of numbers with 6 or fewer digits. Our central hypot
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#21160;&#20316;&#25968;&#25454;&#30340;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#12290;&#36890;&#36807;&#31526;&#21495;&#35745;&#21010;&#12289;&#35270;&#39057;&#25193;&#25955;&#21644;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#26032;&#29615;&#22659;&#20013;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08587</link><description>&lt;p&gt;
&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Compositional Foundation Models for Hierarchical Planning. (arXiv:2309.08587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#21160;&#20316;&#25968;&#25454;&#30340;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#12290;&#36890;&#36807;&#31526;&#21495;&#35745;&#21010;&#12289;&#35270;&#39057;&#25193;&#25955;&#21644;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#26032;&#29615;&#22659;&#20013;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26032;&#29615;&#22659;&#20013;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#38656;&#35201;&#36827;&#34892;&#36328;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#30340;&#23618;&#27425;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#20998;&#21035;&#23545;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#21160;&#20316;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20849;&#21516;&#35299;&#20915;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#22312;&#29615;&#22659;&#20013;&#25166;&#26681;&#30340;&#31526;&#21495;&#35745;&#21010;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#29983;&#25104;&#30340;&#35270;&#39057;&#35745;&#21010;&#36890;&#36807;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#19982;&#35270;&#35273;-&#21160;&#20316;&#25511;&#21046;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#22312;&#27492;&#23618;&#27425;&#32467;&#26500;&#20013;&#36827;&#34892;&#26377;&#25928;&#25512;&#29702;&#65292;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#24378;&#21046;&#20445;&#25345;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make effective decisions in novel environments with long-horizon goals, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, visually reasoning about the underlying plans, and executing actions in accordance with the devised plan through visual-motor control. We propose Compositional Foundation Models for Hierarchical Planning (HiP), a foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks. We use a large language model to construct symbolic plans that are grounded in the environment through a large video diffusion model. Generated video plans are then grounded to visual-motor control, through an inverse dynamics model that infers actions from generated videos. To enable effective reasoning within this hierarchy, we enforce consistency between the models via iterative refinement. We illustr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#36801;&#31227;&#21040;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#19981;&#21516;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08565</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#19978;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#33021;&#21542;&#36801;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?. (arXiv:2309.08565v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#36801;&#31227;&#21040;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#19981;&#21516;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#23450;&#21046;&#20026;&#31526;&#21512;&#32454;&#31890;&#24230;&#23646;&#24615;&#65288;&#22914;&#24418;&#24335;&#65289;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#33267;&#23569;&#19968;&#20123;&#24102;&#26377;&#23646;&#24615;&#27880;&#37322;&#30340;&#30417;&#30563;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#31232;&#32570;&#20173;&#28982;&#26159;&#23558;&#27492;&#23450;&#21046;&#33021;&#21147;&#26222;&#21450;&#21040;&#26356;&#24191;&#27867;&#35821;&#35328;&#33539;&#22260;&#65292;&#23588;&#20854;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#19968;&#20010;&#29942;&#39048;&#12290;&#37492;&#20110;&#26368;&#36817;&#22312;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#20316;&#20026;&#23545;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#36827;&#34892;&#23646;&#24615;&#25511;&#21046;&#33021;&#21147;&#36801;&#31227;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;NLLB-200&#27169;&#22411;&#23545;&#23646;&#24615;&#25511;&#21046;&#22120;&#30340;&#36801;&#31227;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#20004;&#31181;&#33539;&#24335;&#26159;&#20114;&#34917;&#30340;&#65292;&#36890;&#36807;&#19968;&#33268;&#30340;&#25913;&#36827;&#26469;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customizing machine translation models to comply with fine-grained attributes such as formality has seen tremendous progress recently. However, current approaches mostly rely on at least some supervised data with attribute annotation. Data scarcity therefore remains a bottleneck to democratizing such customization possibilities to a wider range of languages, lower-resource ones in particular. Given recent progress in pretrained massively multilingual translation models, we use them as a foundation to transfer the attribute controlling capabilities to languages without supervised data. In this work, we present a comprehensive analysis of transferring attribute controllers based on a pretrained NLLB-200 model. We investigate both training- and inference-time control techniques under various data scenarios, and uncover their relative strengths and weaknesses in zero-shot performance and domain robustness. We show that both paradigms are complementary, as shown by consistent improvements o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20010;&#20307;&#24739;&#32773;&#30340;&#30142;&#30149;&#36827;&#23637;&#21644;&#24739;&#32773;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#65292;&#26469;&#20248;&#21270;&#21307;&#30103;&#36164;&#28304;&#30340;&#20998;&#37197;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#20998;&#37197;&#30340;&#20844;&#24179;&#24615;&#21644;&#25972;&#20307;&#24739;&#32773;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.08560</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#19988;&#20844;&#24179;&#20998;&#37197;&#21307;&#30103;&#36164;&#28304;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Efficient and Fair Allocation of Health Care Resources. (arXiv:2309.08560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20010;&#20307;&#24739;&#32773;&#30340;&#30142;&#30149;&#36827;&#23637;&#21644;&#24739;&#32773;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#65292;&#26469;&#20248;&#21270;&#21307;&#30103;&#36164;&#28304;&#30340;&#20998;&#37197;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#20998;&#37197;&#30340;&#20844;&#24179;&#24615;&#21644;&#25972;&#20307;&#24739;&#32773;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#36164;&#28304;&#30340;&#31232;&#32570;&#24615;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#36991;&#20813;&#30340;&#37197;&#32473;&#38382;&#39064;&#12290;&#20363;&#22914;&#65292;&#36890;&#27668;&#26426;&#30340;&#20379;&#24212;&#36890;&#24120;&#26377;&#38480;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#21355;&#29983;&#32039;&#24613;&#24773;&#20917;&#25110;&#36164;&#28304;&#26377;&#38480;&#30340;&#21307;&#30103;&#29615;&#22659;&#20013;&#65292;&#22914;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#12290;&#30446;&#21069;&#65292;&#38024;&#23545;&#21307;&#30103;&#36164;&#28304;&#20998;&#37197;&#30340;&#21327;&#35758;&#24182;&#27809;&#26377;&#26222;&#36941;&#25509;&#21463;&#30340;&#26631;&#20934;&#65292;&#23548;&#33268;&#21508;&#22269;&#25919;&#24220;&#26681;&#25454;&#19981;&#21516;&#30340;&#26631;&#20934;&#21644;&#22522;&#20110;&#21551;&#21457;&#24335;&#21327;&#35758;&#26469;&#20248;&#20808;&#32771;&#34385;&#24739;&#32773;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#37325;&#30151;&#25252;&#29702;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#65292;&#20197;&#20844;&#24179;&#26377;&#25928;&#22320;&#37197;&#32473;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#65292;&#29992;&#20110;&#23558;&#20010;&#20307;&#24739;&#32773;&#30340;&#30149;&#24773;&#36827;&#23637;&#21644;&#24739;&#32773;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#25972;&#21512;&#21040;&#37325;&#30151;&#25252;&#29702;&#36164;&#28304;&#20998;&#37197;&#20013;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#20998;&#37197;&#30340;&#20844;&#24179;&#24615;&#21644;&#25972;&#20307;&#24739;&#32773;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#36807;&#24230;&#37197;&#32473;&#36164;&#28304;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scarcity of health care resources could result in the unavoidable consequence of rationing. For example, ventilators are often limited in supply, especially during public health emergencies or in resource-constrained health care settings, such as amid the pandemic of COVID-19. Currently, there is no universally accepted standard for health care resource allocation protocols, resulting in different governments prioritizing patients based on various criteria and heuristic-based protocols. In this study, we investigate the use of reinforcement learning for critical care resource allocation policy optimization to fairly and effectively ration resources. We propose a transformer-based deep Q-network to integrate the disease progression of individual patients and the interaction effects among patients during the critical care resource allocation. We aim to improve both fairness of allocation and overall patient outcomes. Our experiments demonstrate that our method significantly reduces exces
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#24182;&#19988;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.08549</link><description>&lt;p&gt;
&#22522;&#20110;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#30340;&#35757;&#32451;&#26469;&#25269;&#24481;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks. (arXiv:2309.08549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#24182;&#19988;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#20063;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#38450;&#24481;&#26041;&#27861;&#26469;&#38450;&#27490;&#26469;&#33258;&#19981;&#21487;&#20449;&#25968;&#25454;&#28304;&#30340;&#28508;&#22312;&#27745;&#26579;&#25915;&#20987;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#38024;&#23545;&#29305;&#23450;&#25915;&#20987;&#36827;&#34892;&#38450;&#24481;&#65292;&#36825;&#32473;&#20102;&#25915;&#20987;&#32773;&#35768;&#22810;&#21487;&#21033;&#29992;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#39640;&#25928;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;&#20581;&#24247;&#24433;&#21709;&#21147;&#22122;&#22768;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#65292;&#25105;&#20204;&#21046;&#36896;&#20102;&#26377;&#21161;&#20110;&#21152;&#24378;&#20998;&#31867;&#27169;&#22411;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#30340;&#20581;&#24247;&#22122;&#22768;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26102;&#26377;&#25928;&#36816;&#34892;&#65292;&#32780;&#19981;&#26159;&#22914;&#20960;&#31181;&#20043;&#21069;&#30340;&#26041;&#27861;&#20013;&#37027;&#26679;&#21521;&#25152;&#26377;&#31034;&#20363;&#28155;&#21152;&#22122;&#22768;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#32771;&#34385;&#19981;&#21516;&#30340;&#23454;&#38469;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#26368;&#26032;&#25915;&#20987;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;H
&lt;/p&gt;
&lt;p&gt;
While numerous defense methods have been proposed to prohibit potential poisoning attacks from untrusted data sources, most research works only defend against specific attacks, which leaves many avenues for an adversary to exploit. In this work, we propose an efficient and robust training approach to defend against data poisoning attacks based on influence functions, named Healthy Influential-Noise based Training. Using influence functions, we craft healthy noise that helps to harden the classification model against poisoning attacks without significantly affecting the generalization ability on test data. In addition, our method can perform effectively when only a subset of the training data is modified, instead of the current method of adding noise to all examples that has been used in several previous works. We conduct comprehensive evaluations over two image datasets with state-of-the-art poisoning attacks under different realistic attack scenarios. Our empirical results show that H
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#30340;&#25928;&#26524;&#19982;&#26816;&#32034;&#22120;&#24615;&#33021;&#30456;&#20851;&#65292;&#23545;&#20110;&#24369;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#25552;&#39640;&#20102;&#20998;&#25968;&#65292;&#20294;&#23545;&#20110;&#24378;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#36890;&#24120;&#20250;&#25439;&#23475;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.08541</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26597;&#35810;&#21644;&#25991;&#26723;&#25193;&#23637;&#20309;&#26102;&#22833;&#36133;&#65311;&#26041;&#27861;&#12289;&#26816;&#32034;&#22120;&#21644;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets. (arXiv:2309.08541v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08541
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#30340;&#25928;&#26524;&#19982;&#26816;&#32034;&#22120;&#24615;&#33021;&#30456;&#20851;&#65292;&#23545;&#20110;&#24369;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#25552;&#39640;&#20102;&#20998;&#25968;&#65292;&#20294;&#23545;&#20110;&#24378;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#36890;&#24120;&#20250;&#25439;&#23475;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#21487;&#20197;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#25216;&#26415;&#26159;&#21542;&#26222;&#36941;&#26377;&#30410;&#65292;&#36824;&#26159;&#20165;&#22312;&#29305;&#23450;&#35774;&#32622;&#19979;&#26377;&#25928;&#65292;&#20363;&#22914;&#23545;&#20110;&#29305;&#23450;&#30340;&#26816;&#32034;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#39046;&#22495;&#25110;&#26597;&#35810;&#31867;&#22411;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;&#22522;&#20110;LM&#30340;&#25193;&#23637;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26816;&#32034;&#22120;&#24615;&#33021;&#19982;&#25193;&#23637;&#30340;&#22686;&#30410;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;&#65306;&#25193;&#23637;&#25913;&#21892;&#20102;&#36739;&#24369;&#27169;&#22411;&#30340;&#20998;&#25968;&#65292;&#20294;&#36890;&#24120;&#20250;&#25439;&#23475;&#36739;&#24378;&#27169;&#22411;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#36235;&#21183;&#22312;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#19968;&#32452;&#23454;&#39564;&#20013;&#25104;&#31435;&#12290;&#36890;&#36807;&#23450;&#24615;&#38169;&#35823;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#23613;&#31649;&#25193;&#23637;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#65288;&#21487;&#33021;&#25913;&#21892;&#20102;&#21484;&#22238;&#29575;&#65289;&#65292;&#20294;&#23427;&#20204;&#20063;&#22686;&#21152;&#20102;&#22122;&#22768;&#65292;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20986;&#39030;&#32423;&#30456;&#20851;&#25991;&#26723;&#65288;&#20174;&#32780;&#24341;&#20837;&#20102;&#38169;&#35823;&#30340;&#27491;&#20363;&#65289;
&lt;/p&gt;
&lt;p&gt;
Using large language models (LMs) for query or document expansion can improve generalization in information retrieval. However, it is unknown whether these techniques are universally beneficial or only effective in specific settings, such as for particular retrieval models, dataset domains, or query types. To answer this, we conduct the first comprehensive analysis of LM-based expansion. We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models. We show this trend holds across a set of eleven expansion techniques, twelve datasets with diverse distribution shifts, and twenty-four retrieval models. Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Whisper&#27169;&#22411;&#20174;&#26410;&#26631;&#27880;&#30340;&#22810;&#35821;&#35328;&#35270;&#21548;&#25968;&#25454;&#33258;&#21160;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#19982;&#20154;&#24037;&#26631;&#27880;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08535</link><description>&lt;p&gt;
&#29992;Whisper&#27169;&#22411;&#20174;&#33258;&#21160;&#26631;&#27880;&#20013;&#33719;&#24471;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Visual Speech Recognition for Low-resource Languages with Automatic Labels From Whisper Model. (arXiv:2309.08535v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Whisper&#27169;&#22411;&#20174;&#26410;&#26631;&#27880;&#30340;&#22810;&#35821;&#35328;&#35270;&#21548;&#25968;&#25454;&#33258;&#21160;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#19982;&#20154;&#24037;&#26631;&#27880;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;(VSR)&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#26631;&#27880;&#25968;&#25454;&#26377;&#38480;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#19982;&#20043;&#21069;&#35797;&#22270;&#36890;&#36807;&#20174;&#20854;&#20182;&#35821;&#35328;&#23398;&#20064;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#30446;&#26631;&#35821;&#35328;&#30340;VSR&#24615;&#33021;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#20154;&#24037;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#19981;&#21516;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;Whisper&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36827;&#34892;&#35821;&#35328;&#35782;&#21035;&#21644;&#22522;&#20110;&#38899;&#39057;&#30340;&#35821;&#38899;&#35782;&#21035;&#12290;&#23427;&#29992;&#20110;&#36807;&#28388;&#25152;&#38656;&#35821;&#35328;&#30340;&#25968;&#25454;&#65292;&#24182;&#20174;&#26410;&#27880;&#37322;&#30340;&#22810;&#35821;&#35328;&#35270;&#21548;&#25968;&#25454;&#27744;&#20013;&#36716;&#24405;&#26631;&#31614;&#12290;&#36890;&#36807;&#27604;&#36739;&#20351;&#29992;&#33258;&#21160;&#26631;&#31614;&#21644;&#20154;&#24037;&#26631;&#27880;&#26631;&#31614;&#35757;&#32451;&#30340;VSR&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#19981;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#36798;&#21040;&#19982;&#20154;&#24037;&#27880;&#37322;&#26631;&#31614;&#30456;&#20284;&#30340;VSR&#24615;&#33021;&#12290;&#36890;&#36807;&#33258;&#21160;&#26631;&#27880;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#26631;&#27880;&#20102;&#22823;&#35268;&#27169;&#26410;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a powerful Visual Speech Recognition (VSR) method for multiple languages, especially for low-resource languages that have a limited number of labeled data. Different from previous methods that tried to improve the VSR performance for the target language by using knowledge learned from other languages, we explore whether we can increase the amount of training data itself for the different languages without human intervention. To this end, we employ a Whisper model which can conduct both language identification and audio-based speech recognition. It serves to filter data of the desired languages and transcribe labels from the unannotated, multilingual audio-visual data pool. By comparing the performances of VSR models trained on automatic labels and the human-annotated labels, we show that we can achieve similar VSR performance to that of human-annotated labels even without utilizing human annotations. Through the automated labeling process, we label large-scale unlab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;EvoPrompt&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;EvoPrompt&#21487;&#20197;&#33258;&#21160;&#21270;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#21644;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08532</link><description>&lt;p&gt;
&#36890;&#36807;&#36827;&#21270;&#31639;&#27861;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#24378;&#22823;&#30340;&#25552;&#31034;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers. (arXiv:2309.08532v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;EvoPrompt&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;EvoPrompt&#21487;&#20197;&#33258;&#21160;&#21270;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#21644;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#21162;&#21147;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;EvoPrompt&#65292;&#23427;&#20511;&#37492;&#20102;&#36827;&#21270;&#31639;&#27861;&#30340;&#24605;&#24819;&#65292;&#22240;&#20026;&#23427;&#20204;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#24555;&#36895;&#30340;&#25910;&#25947;&#24615;&#12290;&#20026;&#20102;&#20351;&#36827;&#21270;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#24182;&#19988;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#31163;&#25955;&#25552;&#31034;&#65292;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#36830;&#25509;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#39640;&#25928;&#20248;&#21270;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EvoPrompt&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#26799;&#24230;&#25110;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#19968;&#32452;&#25552;&#31034;&#20013;&#24320;&#22987;&#65292;&#24182;&#22522;&#20110;&#36827;&#21270;&#31639;&#23376;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26032;&#30340;&#25552;&#31034;&#65292;&#26681;&#25454;&#24320;&#21457;&#38598;&#25913;&#36827;&#25552;&#31034;&#30340;&#31181;&#32676;&#12290;&#25105;&#20204;&#23545;&#38381;&#28304;&#21644;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;GPT-3&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26174;&#33879;&#36890;&#36947;&#24494;&#35843;&#8221;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#24449;&#22270;&#20013;&#30340;&#37096;&#20998;&#36890;&#36947;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#22312;&#20302;&#25968;&#25454;&#36164;&#28304;&#22330;&#26223;&#19979;&#20302;&#21442;&#25968;&#25104;&#26412;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#20840;&#38754;&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08513</link><description>&lt;p&gt;
&#36890;&#36807;&#26174;&#33879;&#36890;&#36947;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#31616;&#21333;&#22522;&#20934;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels. (arXiv:2309.08513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26174;&#33879;&#36890;&#36947;&#24494;&#35843;&#8221;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#24449;&#22270;&#20013;&#30340;&#37096;&#20998;&#36890;&#36947;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#22312;&#20302;&#25968;&#25454;&#36164;&#28304;&#22330;&#26223;&#19979;&#20302;&#21442;&#25968;&#25104;&#26412;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#20840;&#38754;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#31034;&#20248;&#21183;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#24494;&#35843;&#39069;&#22806;&#30340;1%&#21442;&#25968;&#23601;&#33021;&#22312;&#20302;&#25968;&#25454;&#36164;&#28304;&#22330;&#26223;&#19979;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#24494;&#35843;&#22810;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#26102;&#24573;&#35270;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#31216;&#20026;&#8220;&#26174;&#33879;&#36890;&#36947;&#24494;&#35843;&#8221;&#65288;SCT&#65289;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#19982;&#20219;&#21153;&#22270;&#20687;&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#65292;&#36873;&#25321;&#29305;&#24449;&#22270;&#20013;&#30340;&#37096;&#20998;&#36890;&#36947;&#65292;&#20351;&#24471;&#25105;&#20204;&#21482;&#38656;&#35201;&#24494;&#35843;&#20854;&#20013;&#30340;1/8&#36890;&#36947;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#21442;&#25968;&#25104;&#26412;&#24182;&#22312;VTAB-1K&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;18&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#20840;&#38754;&#24494;&#35843;&#12290;&#36825;&#20165;&#22686;&#21152;&#20102;0.11M ViT-B&#21442;&#25968;&#65292;&#30456;&#27604;&#20840;&#38754;&#24494;&#35843;&#65292;&#20943;&#23569;&#20102;780&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained vision transformers have strong representation benefits to various downstream tasks. Recently, many parameter-efficient fine-tuning (PEFT) methods have been proposed, and their experiments demonstrate that tuning only 1% of extra parameters could surpass full fine-tuning in low-data resource scenarios. However, these methods overlook the task-specific information when fine-tuning diverse downstream tasks. In this paper, we propose a simple yet effective method called "Salient Channel Tuning" (SCT) to leverage the task-specific information by forwarding the model with the task images to select partial channels in a feature map that enables us to tune only 1/8 channels leading to significantly lower parameter costs. Experiments outperform full fine-tuning on 18 out of 19 tasks in the VTAB-1K benchmark by adding only 0.11M parameters of the ViT-B, which is 780$\times$ fewer than its full fine-tuning counterpart. Furthermore, experiments on domain generalization and few-shot le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#30340;&#20581;&#24247;&#22768;&#26126;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;750&#20010;&#30001;&#21307;&#23398;&#19987;&#23478;&#26631;&#27880;&#30340;&#20581;&#24247;&#30456;&#20851;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#20102;&#26469;&#33258;&#20020;&#24202;&#30740;&#31350;&#30340;&#35777;&#25454;&#25903;&#25345;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#35777;&#25454;&#26816;&#32034;&#12289;&#30495;&#23454;&#24615;&#39044;&#27979;&#21644;&#35299;&#37322;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.08503</link><description>&lt;p&gt;
HealthFC&#65306;&#19968;&#20221;&#29992;&#20110;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#20107;&#23454;&#26816;&#39564;&#30340;&#20581;&#24247;&#22768;&#26126;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HealthFC: A Dataset of Health Claims for Evidence-Based Medical Fact-Checking. (arXiv:2309.08503v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#30340;&#20581;&#24247;&#22768;&#26126;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;750&#20010;&#30001;&#21307;&#23398;&#19987;&#23478;&#26631;&#27880;&#30340;&#20581;&#24247;&#30456;&#20851;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#20102;&#26469;&#33258;&#20020;&#24202;&#30740;&#31350;&#30340;&#35777;&#25454;&#25903;&#25345;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#35777;&#25454;&#26816;&#32034;&#12289;&#30495;&#23454;&#24615;&#39044;&#27979;&#21644;&#35299;&#37322;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#36890;&#36807;&#20114;&#32852;&#32593;&#26597;&#35810;&#20581;&#24247;&#30456;&#20851;&#24314;&#35758;&#24050;&#25104;&#20026;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#21028;&#26029;&#22312;&#32447;&#25214;&#21040;&#30340;&#21307;&#23398;&#22768;&#26126;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25214;&#21040;&#30456;&#24212;&#30340;&#35777;&#25454;&#65292;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20107;&#23454;&#26816;&#39564;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36890;&#36807;&#21487;&#38752;&#30693;&#35782;&#26469;&#28304;&#30340;&#35777;&#25454;&#35780;&#20272;&#20107;&#23454;&#22768;&#26126;&#30495;&#23454;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#25512;&#21160;&#27492;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;750&#20010;&#20581;&#24247;&#30456;&#20851;&#22768;&#26126;&#65292;&#22312;&#21487;&#20449;&#24230;&#26041;&#38754;&#30001;&#21307;&#23398;&#19987;&#23478;&#36827;&#34892;&#20102;&#26631;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#26469;&#33258;&#36866;&#24403;&#30340;&#20020;&#24202;&#30740;&#31350;&#30340;&#35777;&#25454;&#25903;&#25345;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#31361;&#20986;&#20854;&#29305;&#28857;&#21644;&#25361;&#25112;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#19982;&#33258;&#21160;&#20107;&#23454;&#26816;&#39564;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#35777;&#25454;&#26816;&#32034;&#12289;&#30495;&#23454;&#24615;&#39044;&#27979;&#21644;&#35299;&#37322;&#29983;&#25104;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#19981;&#21516;&#26041;&#27861;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seeking health-related advice on the internet has become a common practice in the digital era. Determining the trustworthiness of medical claims found online and finding appropriate evidence for this information is increasingly challenging. Fact-checking has emerged as an approach to assess the veracity of factual claims using evidence from credible knowledge sources. To help advance the automation of this task, in this paper, we introduce a novel dataset of 750 health-related claims, labeled for veracity by medical experts and backed with evidence from appropriate clinical studies. We provide an analysis of the dataset, highlighting its characteristics and challenges. The dataset can be used for Machine Learning tasks related to automated fact-checking such as evidence retrieval, veracity prediction, and explanation generation. For this purpose, we provide baseline models based on different approaches, examine their performance, and discuss the findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;</title><link>http://arxiv.org/abs/2309.08499</link><description>&lt;p&gt;
P-ROCKET: &#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
P-ROCKET: Pruning Random Convolution Kernels for Time Series Classification. (arXiv:2309.08499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;ROCKET&#21644;MINIROCKET&#22240;&#20854;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;ROCKET&#21644;MINIROCKET&#21033;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#38543;&#26426;&#19968;&#32500;&#21367;&#31215;&#26680;&#65292;&#21487;&#20197;&#24555;&#36895;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#39640;&#25928;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20840;&#38754;&#25429;&#25417;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#26469;&#35828;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36827;&#21270;&#31639;&#27861;S-ROCKET&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#21098;&#26525;&#20887;&#20313;&#30340;&#21367;&#31215;&#26680;&#12290;&#28982;&#32780;&#65292;&#36827;&#21270;&#31639;&#27861;&#26412;&#36523;&#30340;&#29305;&#24615;&#23548;&#33268;&#22312;S-ROCKET&#20013;&#35780;&#20272;&#21367;&#31215;&#26680;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20013;&#65292;&#19982;&#30452;&#25509;&#35780;&#20272;&#20855;&#26377;&#38750;&#26174;&#33879;&#24046;&#24322;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;S-ROCKET&#19981;&#21516;&#65292;&#25105;&#20204;&#20174;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#36890;&#36807;&#28040;&#38500;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#36830;&#25509;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, two time series classification models, ROCKET and MINIROCKET, have attracted much attention for their low training cost and state-of-the-art accuracy. Utilizing random 1-D convolutional kernels without training, ROCKET and MINIROCKET can rapidly extract features from time series data, allowing for the efficient fitting of linear classifiers. However, to comprehensively capture useful features, a large number of random kernels are required, which is incompatible for resource-constrained devices. Therefore, a heuristic evolutionary algorithm named S-ROCKET is devised to recognize and prune redundant kernels. Nevertheless, the inherent nature of evolutionary algorithms renders the evaluation of kernels within S-ROCKET an unacceptable time-consuming process. In this paper, diverging from S-ROCKET, which directly evaluates random kernels with nonsignificant differences, we remove kernels from a feature selection perspective by eliminating associating connections in the sequ
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30693;&#35782;&#24037;&#31243;&#20219;&#21153;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23558;&#20027;&#39064;&#21644;&#20851;&#31995;&#23545;&#36716;&#21270;&#20026;&#23383;&#31526;&#20018;&#26684;&#24335;&#65292;&#24182;&#23558;&#23427;&#20204;&#38142;&#25509;&#21040;&#30456;&#24212;&#30340;Wikidata QID&#19978;&#65292;&#24320;&#21457;&#20102;LLMKE&#27969;&#27700;&#32447;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#30340;&#30693;&#35782;&#22240;&#39046;&#22495;&#32780;&#24322;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#23454;&#39564;&#20197;&#30830;&#23450;&#20854;&#22312;&#33258;&#21160;&#30693;&#35782;&#24211;&#34917;&#20840;&#21644;&#20462;&#27491;&#26041;&#38754;&#30340;&#24212;&#29992;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#36824;&#26174;&#31034;&#20102;LLMs&#22312;&#21327;&#20316;&#30693;&#35782;&#24037;&#31243;&#26041;&#38754;&#30340;&#26377;&#24076;&#26395;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.08491</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#24037;&#31243;&#65288;LLMKE&#65289;&#65306;&#20197;Wikidata&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata. (arXiv:2309.08491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30693;&#35782;&#24037;&#31243;&#20219;&#21153;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23558;&#20027;&#39064;&#21644;&#20851;&#31995;&#23545;&#36716;&#21270;&#20026;&#23383;&#31526;&#20018;&#26684;&#24335;&#65292;&#24182;&#23558;&#23427;&#20204;&#38142;&#25509;&#21040;&#30456;&#24212;&#30340;Wikidata QID&#19978;&#65292;&#24320;&#21457;&#20102;LLMKE&#27969;&#27700;&#32447;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#30340;&#30693;&#35782;&#22240;&#39046;&#22495;&#32780;&#24322;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#23454;&#39564;&#20197;&#30830;&#23450;&#20854;&#22312;&#33258;&#21160;&#30693;&#35782;&#24211;&#34917;&#20840;&#21644;&#20462;&#27491;&#26041;&#38754;&#30340;&#24212;&#29992;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#36824;&#26174;&#31034;&#20102;LLMs&#22312;&#21327;&#20316;&#30693;&#35782;&#24037;&#31243;&#26041;&#38754;&#30340;&#26377;&#24076;&#26395;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;ISWC 2023 LM-KBC&#25361;&#25112;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30693;&#35782;&#24037;&#31243;&#20219;&#21153;&#30340;&#24212;&#29992;&#12290;&#38024;&#23545;&#35813;&#20219;&#21153;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#23558;&#26469;&#33258;Wikidata&#30340;&#20027;&#39064;&#21644;&#20851;&#31995;&#23545;&#36716;&#21270;&#20026;&#30456;&#24212;&#30340;&#23383;&#31526;&#20018;&#26684;&#24335;&#65292;&#24182;&#23558;&#23427;&#20204;&#38142;&#25509;&#21040;&#30456;&#24212;&#30340;Wikidata QID&#19978;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#36827;&#34892;&#30693;&#35782;&#24037;&#31243;&#30340;&#27969;&#27700;&#32447;&#65288;LLMKE&#65289;&#65292;&#32467;&#21512;&#20102;&#30693;&#35782;&#25506;&#27979;&#21644;Wikidata&#23454;&#20307;&#26144;&#23556;&#12290;&#35813;&#26041;&#27861;&#22312;&#23646;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;0.701&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#65292;&#24471;&#20998;&#22312;1.00&#21040;0.328&#20043;&#38388;&#21464;&#21270;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30340;&#30693;&#35782;&#22240;&#39046;&#22495;&#32780;&#24322;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#23454;&#39564;&#26469;&#30830;&#23450;LLMs&#22312;&#33258;&#21160;&#30693;&#35782;&#24211;&#65288;&#22914;Wikidata&#65289;&#34917;&#20840;&#21644;&#20462;&#27491;&#26041;&#38754;&#30340;&#24212;&#29992;&#26465;&#20214;&#12290;&#32467;&#26524;&#30340;&#35843;&#26597;&#36824;&#26174;&#31034;&#20102;LLMs&#22312;&#21327;&#20316;&#30693;&#35782;&#24037;&#31243;&#26041;&#38754;&#30340;&#26377;&#24076;&#26395;&#30340;&#36129;&#29486;&#12290;LLMKE&#33719;&#32988;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore the use of Large Language Models (LLMs) for knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge. For this task, given subject and relation pairs sourced from Wikidata, we utilize pre-trained LLMs to produce the relevant objects in string format and link them to their respective Wikidata QIDs. We developed a pipeline using LLMs for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata entity mapping. The method achieved a macro-averaged F1-score of 0.701 across the properties, with the scores varying from 1.00 to 0.328. These results demonstrate that the knowledge of LLMs varies significantly depending on the domain and that further experimentation is required to determine the circumstances under which LLMs can be used for automatic Knowledge Base (e.g., Wikidata) completion and correction. The investigation of the results also suggests the promising contribution of LLMs in collaborative knowledge engineering. LLMKE won
&lt;/p&gt;</description></item><item><title>XFedHunter&#26159;&#19968;&#31181;&#29992;&#20110;SDN&#20013;&#26816;&#27979;&#25345;&#20037;&#24615;&#39640;&#32423;&#23041;&#32961;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;ML&#39044;&#27979;&#30340;&#35299;&#37322;&#26469;&#25581;&#31034;&#25915;&#20987;&#32773;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.08485</link><description>&lt;p&gt;
XFedHunter: &#19968;&#31181;&#29992;&#20110;SDN&#20013;&#25345;&#20037;&#24615;&#39640;&#32423;&#23041;&#32961;&#26816;&#27979;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
XFedHunter: An Explainable Federated Learning Framework for Advanced Persistent Threat Detection in SDN. (arXiv:2309.08485v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08485
&lt;/p&gt;
&lt;p&gt;
XFedHunter&#26159;&#19968;&#31181;&#29992;&#20110;SDN&#20013;&#26816;&#27979;&#25345;&#20037;&#24615;&#39640;&#32423;&#23041;&#32961;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;ML&#39044;&#27979;&#30340;&#35299;&#37322;&#26469;&#25581;&#31034;&#25915;&#20987;&#32773;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#65288;APT&#65289;&#25915;&#20987;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#65292;&#37319;&#29992;&#20102;&#22810;&#31181;&#20808;&#36827;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#20197;&#38024;&#23545;&#32452;&#32455;&#24182;&#31363;&#21462;&#25935;&#24863;&#21644;&#26426;&#23494;&#20449;&#24687;&#12290;APT&#25915;&#20987;&#21253;&#25324;&#22810;&#20010;&#38454;&#27573;&#21644;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#40657;&#23458;&#24320;&#21457;&#30340;&#26032;&#30340;&#21019;&#26032;&#25216;&#26415;&#21644;&#25216;&#26415;&#26469;&#35268;&#36991;&#23433;&#20840;&#36719;&#20214;&#30417;&#25511;&#12290;&#20026;&#20102;&#26377;&#25928;&#38450;&#25252;APT&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39044;&#27979;&#30340;&#35299;&#37322;&#26469;&#26816;&#27979;&#21644;&#39044;&#27979;APT&#25351;&#26631;&#23545;&#20110;&#25581;&#31034;&#28508;&#20239;&#22312;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#25915;&#20987;&#32773;&#30340;&#29305;&#24449;&#33267;&#20851;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#22312;&#19981;&#25439;&#23475;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#26234;&#33021;&#24212;&#29992;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#25935;&#24863;&#25968;&#25454;&#21644;&#39640;&#36136;&#37327;&#26631;&#31614;&#23545;&#20110;&#26500;&#24314;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#32593;&#32476;&#23041;&#32961;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;XFedHunter&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advanced Persistent Threat (APT) attacks are highly sophisticated and employ a multitude of advanced methods and techniques to target organizations and steal sensitive and confidential information. APT attacks consist of multiple stages and have a defined strategy, utilizing new and innovative techniques and technologies developed by hackers to evade security software monitoring. To effectively protect against APTs, detecting and predicting APT indicators with an explanation from Machine Learning (ML) prediction is crucial to reveal the characteristics of attackers lurking in the network system. Meanwhile, Federated Learning (FL) has emerged as a promising approach for building intelligent applications without compromising privacy. This is particularly important in cybersecurity, where sensitive data and high-quality labeling play a critical role in constructing effective machine learning models for detecting cyber threats. Therefore, this work proposes XFedHunter, an explainable feder
&lt;/p&gt;</description></item><item><title>VulnSense&#26694;&#26550;&#20351;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#22270;&#24418;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#65292;&#39640;&#25928;&#26816;&#27979;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#20013;&#30340;&#28431;&#27934;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08474</link><description>&lt;p&gt;
VulnSense: &#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#22312;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#20013;&#30340;&#39640;&#25928;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
VulnSense: Efficient Vulnerability Detection in Ethereum Smart Contracts by Multimodal Learning with Graph Neural Network and Language Model. (arXiv:2309.08474v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08474
&lt;/p&gt;
&lt;p&gt;
VulnSense&#26694;&#26550;&#20351;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#22270;&#24418;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#65292;&#39640;&#25928;&#26816;&#27979;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#20013;&#30340;&#28431;&#27934;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;VulnSense&#26694;&#26550;&#65292;&#19968;&#20010;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#22270;&#24418;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#39640;&#25928;&#26816;&#27979;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#20013;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#26234;&#33021;&#21512;&#32422;&#20013;&#30340;&#28304;&#20195;&#30721;&#12289;&#25805;&#20316;&#30721;&#24207;&#21015;&#21644;&#20174;&#23383;&#33410;&#30721;&#20013;&#25552;&#21462;&#30340;&#25511;&#21046;&#27969;&#22270;&#65288;CFG&#65289;&#31561;&#19977;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#37319;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#22120;&#65288;BERT&#65289;&#12289;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;BiLSTM&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#25552;&#21462;&#21644;&#20998;&#26512;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#22810;&#27169;&#24577;&#26041;&#27861;&#30340;&#26368;&#21518;&#19968;&#23618;&#26159;&#19968;&#20010;&#20840;&#36830;&#25509;&#23618;&#65292;&#29992;&#20110;&#39044;&#27979;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#20013;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#28431;&#27934;&#26816;&#27979;&#26041;&#27861;&#20165;&#20381;&#36182;&#21333;&#19968;&#29305;&#24449;&#25110;&#21333;&#19968;&#27169;&#22411;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;1,769&#20010;&#26234;&#33021;&#21512;&#32422;&#38598;&#21512;&#23545;VulnSense&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents VulnSense framework, a comprehensive approach to efficiently detect vulnerabilities in Ethereum smart contracts using a multimodal learning approach on graph-based and natural language processing (NLP) models. Our proposed framework combines three types of features from smart contracts comprising source code, opcode sequences, and control flow graph (CFG) extracted from bytecode. We employ Bidirectional Encoder Representations from Transformers (BERT), Bidirectional Long Short-Term Memory (BiLSTM) and Graph Neural Network (GNN) models to extract and analyze these features. The final layer of our multimodal approach consists of a fully connected layer used to predict vulnerabilities in Ethereum smart contracts. Addressing limitations of existing vulnerability detection methods relying on single-feature or single-model deep learning techniques, our method surpasses accuracy and effectiveness constraints. We assess VulnSense using a collection of 1.769 smart contracts 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21521;&#26377;&#35266;&#28857;&#30340;&#20154;&#35299;&#37322;&#25628;&#32034;&#32467;&#26524;&#31435;&#22330;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#31435;&#22330;&#26631;&#31614;&#21644;&#35299;&#37322;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#28040;&#36153;&#26356;&#22810;&#19981;&#21516;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#20294;&#27809;&#26377;&#21457;&#29616;&#31995;&#32479;&#24615;&#35266;&#28857;&#25913;&#21464;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.08460</link><description>&lt;p&gt;
&#21521;&#26377;&#35266;&#28857;&#30340;&#20154;&#35299;&#37322;&#25628;&#32034;&#32467;&#26524;&#31435;&#22330;
&lt;/p&gt;
&lt;p&gt;
Explaining Search Result Stances to Opinionated People. (arXiv:2309.08460v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08460
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21521;&#26377;&#35266;&#28857;&#30340;&#20154;&#35299;&#37322;&#25628;&#32034;&#32467;&#26524;&#31435;&#22330;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#31435;&#22330;&#26631;&#31614;&#21644;&#35299;&#37322;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#28040;&#36153;&#26356;&#22810;&#19981;&#21516;&#30340;&#25628;&#32034;&#32467;&#26524;&#65292;&#20294;&#27809;&#26377;&#21457;&#29616;&#31995;&#32479;&#24615;&#35266;&#28857;&#25913;&#21464;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#22312;&#24418;&#25104;&#35266;&#28857;&#20043;&#21069;&#20351;&#29992;&#32593;&#32476;&#25628;&#32034;&#24341;&#25806;&#25214;&#21040;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20855;&#26377;&#19981;&#21516;&#24433;&#21709;&#27700;&#24179;&#30340;&#23454;&#38469;&#20915;&#31574;&#12290;&#25628;&#32034;&#30340;&#35748;&#30693;&#21162;&#21147;&#21487;&#33021;&#20351;&#26377;&#35266;&#28857;&#30340;&#29992;&#25143;&#23481;&#26131;&#21463;&#21040;&#35748;&#30693;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#30830;&#35748;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#31435;&#22330;&#26631;&#31614;&#21450;&#20854;&#35299;&#37322;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#28040;&#36153;&#26356;&#22810;&#19981;&#21516;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;&#25105;&#20204;&#33258;&#21160;&#23545;&#19977;&#20010;&#20027;&#39064;&#65288;&#30693;&#35782;&#20135;&#26435;&#12289;&#26657;&#26381;&#21644;&#26080;&#31070;&#35770;&#65289;&#30340;&#25628;&#32034;&#32467;&#26524;&#36827;&#34892;&#20998;&#31867;&#21644;&#26631;&#35760;&#65292;&#20998;&#20026;&#21453;&#23545;&#12289;&#20013;&#31435;&#21644;&#25903;&#25345;&#65292;&#24182;&#20026;&#36825;&#20123;&#26631;&#31614;&#29983;&#25104;&#35299;&#37322;&#12290;&#22312;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#20013;&#65288;N =203&#65289;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25628;&#32034;&#32467;&#26524;&#31435;&#22330;&#20559;&#35265;&#65288;&#24179;&#34913; vs &#20559;&#35265;&#65289;&#21644;&#35299;&#37322;&#27700;&#24179;&#65288;&#32431;&#25991;&#26412;&#12289;&#20165;&#26631;&#31614;&#12289;&#26631;&#31614;&#21644;&#35299;&#37322;&#65289;&#26159;&#21542;&#20250;&#24433;&#21709;&#34987;&#28857;&#20987;&#30340;&#25628;&#32034;&#32467;&#26524;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#31435;&#22330;&#26631;&#31614;&#21644;&#35299;&#37322;&#21487;&#20197;&#23548;&#33268;&#26356;&#22810;&#26679;&#21270;&#30340;&#25628;&#32034;&#32467;&#26524;&#28040;&#36153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24182;&#27809;&#26377;&#21457;&#29616;&#31995;&#32479;&#24615;&#35266;&#28857;&#25913;&#21464;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
People use web search engines to find information before forming opinions, which can lead to practical decisions with different levels of impact. The cognitive effort of search can leave opinionated users vulnerable to cognitive biases, e.g., the confirmation bias. In this paper, we investigate whether stance labels and their explanations can help users consume more diverse search results. We automatically classify and label search results on three topics (i.e., intellectual property rights, school uniforms, and atheism) as against, neutral, and in favor, and generate explanations for these labels. In a user study (N =203), we then investigate whether search result stance bias (balanced vs biased) and the level of explanation (plain text, label only, label and explanation) influence the diversity of search results clicked. We find that stance labels and explanations lead to a more diverse search result consumption. However, we do not find evidence for systematic opinion change among us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#21644;&#37319;&#26679;&#20998;&#35299;&#28508;&#31354;&#38388;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#20154;&#21475;&#32676;&#20307;&#65292;&#20197;&#35299;&#20915;&#29616;&#20195;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#20013;&#25968;&#25454;&#38598;&#20559;&#35265;&#23548;&#33268;&#30340;&#19981;&#20844;&#24179;&#20851;&#27880;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08442</link><description>&lt;p&gt;
&#26397;&#30528;&#36127;&#36131;&#20219;&#30340;&#20154;&#33080;&#25968;&#25454;&#38598;&#65306;&#23545;&#20174;&#20154;&#21475;&#32676;&#20307;&#20013;&#37319;&#26679;&#20154;&#33080;&#22270;&#20687;&#30340;&#20998;&#35299;&#28508;&#31354;&#38388;&#20998;&#24067;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Toward responsible face datasets: modeling the distribution of a disentangled latent space for sampling face images from demographic groups. (arXiv:2309.08442v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#27169;&#21644;&#37319;&#26679;&#20998;&#35299;&#28508;&#31354;&#38388;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#20154;&#21475;&#32676;&#20307;&#65292;&#20197;&#35299;&#20915;&#29616;&#20195;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#20013;&#25968;&#25454;&#38598;&#20559;&#35265;&#23548;&#33268;&#30340;&#19981;&#20844;&#24179;&#20851;&#27880;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#20123;&#29616;&#20195;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#34987;&#26333;&#20809;&#20986;&#21487;&#33021;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#36827;&#34892;&#27495;&#35270;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#23545;&#24615;&#21035;&#21644;&#20986;&#36523;&#31561;&#21508;&#31181;&#38754;&#37096;&#23646;&#24615;&#30340;&#19981;&#20844;&#24179;&#20851;&#27880;&#12290;&#21407;&#22240;&#22312;&#20110;&#34987;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#20559;&#35265;&#21644;&#19981;&#24179;&#34913;&#30340;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#37319;&#38598;&#19968;&#20010;&#21508;&#20010;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#37117;&#24179;&#34913;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#26367;&#20195;&#26041;&#26696;&#65292;&#21363;&#29983;&#25104;&#19968;&#20010;&#20855;&#26377;&#24179;&#34913;&#24615;&#21644;&#21487;&#33021;&#26080;&#20559;&#35265;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;&#35757;&#32451;&#12289;&#27491;&#21017;&#21270;&#25110;&#35780;&#20272;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#21644;&#37319;&#26679;&#19968;&#20010;StyleGAN&#28508;&#31354;&#38388;&#30340;&#20998;&#35299;&#25237;&#24433;&#65292;&#20197;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#20154;&#21475;&#32676;&#20307;&#65288;&#20363;&#22914; $hispanic-female$&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#21512;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#20154;&#21475;&#32676;&#20307;&#65292;&#19988;&#36825;&#20123;&#36523;&#20221;&#19982;&#21407;&#22987;&#35757;&#32451;&#38598;&#20013;&#30340;&#36523;&#20221;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been exposed that some modern facial recognition systems could discriminate specific demographic groups and may lead to unfair attention with respect to various facial attributes such as gender and origin. The main reason are the biases inside datasets, unbalanced demographics, used to train theses models. Unfortunately, collecting a large-scale balanced dataset with respect to various demographics is impracticable.  In this paper, we investigate as an alternative the generation of a balanced and possibly bias-free synthetic dataset that could be used to train, to regularize or to evaluate deep learning-based facial recognition models. We propose to use a simple method for modeling and sampling a disentangled projection of a StyleGAN latent space to generate any combination of demographic groups (e.g. $hispanic-female$). Our experiments show that we can synthesis any combination of demographic groups effectively and the identities are different from the original traini
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;&#65288;LSX&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#32473;&#20104;&#35299;&#37322;&#21644;&#25209;&#35780;&#32773;&#30340;&#21453;&#39304;&#26469;&#25913;&#36827;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#31561;&#22522;&#26412;&#20219;&#21153;&#65292;&#24182;&#26377;&#28508;&#21147;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.08395</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Learning by Self-Explaining. (arXiv:2309.08395v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08395
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;&#65288;LSX&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#32473;&#20104;&#35299;&#37322;&#21644;&#25209;&#35780;&#32773;&#30340;&#21453;&#39304;&#26469;&#25913;&#36827;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#31561;&#22522;&#26412;&#20219;&#21153;&#65292;&#24182;&#26377;&#28508;&#21147;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#20174;&#29983;&#29289;&#23398;&#20013;&#23547;&#25214;&#28789;&#24863;&#65292;&#29305;&#21035;&#26159;&#20154;&#31867;&#26234;&#33021;&#12290;&#19982;&#30446;&#21069;&#20027;&#35201;&#23558;&#35299;&#37322;&#35270;&#20026;&#27169;&#22411;&#26816;&#26597;&#25163;&#27573;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30456;&#27604;&#65292;&#20174;&#24515;&#29702;&#23398;&#20013;&#21457;&#29616;&#33258;&#25105;&#35299;&#37322;&#22312;&#20195;&#29702;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#22909;&#22788;&#26377;&#20123;&#34987;&#24573;&#35270;&#20102;&#12290;&#21463;&#21040;&#36825;&#20010;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322; (LSX)&#12290;&#20854;&#20013;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#65292;&#19968;&#20010;&#23398;&#20064;&#27169;&#22359; (&#23398;&#20064;&#32773;) &#25191;&#34892;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#27604;&#22914;&#22270;&#20687;&#20998;&#31867;&#65292;&#24182;&#23545;&#20854;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#20869;&#37096;&#25209;&#35780;&#32773;&#27169;&#22359;&#22522;&#20110;&#21407;&#22987;&#20219;&#21153;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#23398;&#20064;&#32773;&#36890;&#36807;&#25209;&#35780;&#32773;&#30340;&#21453;&#39304;&#24471;&#21040;&#25913;&#36827;&#65292;&#24182;&#26681;&#25454;&#38656;&#35201;&#37325;&#22797;&#36825;&#20010;&#24490;&#29615;&#12290;&#32972;&#21518;&#30340;&#30452;&#35273;&#26159;&#65292;&#22914;&#26524;&#25209;&#35780;&#32773;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#30340;&#35299;&#37322;&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#21017;&#35813;&#35299;&#37322;&#34987;&#35748;&#20026;&#26159;&#8220;&#22909;&#8221;&#30340;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#23454;&#29616;&#21487;&#33021;&#24615;&#65292;&#20294;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#23454;&#26045;&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;&#30340;&#19968;&#33324;&#25351;&#23548;&#21407;&#21017;&#12290;&#26377;&#24453;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#26469;&#25506;&#32034;&#36825;&#19968;&#23398;&#20064;&#33539;&#24335;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) research has a long track record of drawing inspirations from findings from biology, in particular human intelligence. In contrast to current AI research that mainly treats explanations as a means for model inspection, a somewhat neglected finding from human psychology is the benefit of self-explaining in an agents' learning process. Motivated by this, we introduce a novel learning paradigm, termed Learning by Self-Explaining (LSX). The underlying idea is that a learning module (learner) performs a base task, e.g. image classification, and provides explanations to its decisions. An internal critic module next evaluates the quality of these explanations given the original task. Finally, the learner is refined with the critic's feedback and the loop is repeated as required. The intuition behind this is that an explanation is considered "good" if the critic can perform the same task given the respective explanation. Despite many implementation possibilities th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;M$^3$Net&#65292;&#19968;&#31181;&#29992;&#20110;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#30340;&#22810;&#32423;&#12289;&#28151;&#21512;&#21644;&#22810;&#38454;&#27573;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#20132;&#20114;&#22359;&#21644;&#28151;&#21512;&#27880;&#24847;&#21147;&#22359;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26174;&#33879;&#21306;&#22495;&#30340;&#26816;&#27979;&#24615;&#33021;&#24182;&#20934;&#30830;&#23450;&#20301;&#22797;&#26434;&#23545;&#35937;&#30340;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2309.08365</link><description>&lt;p&gt;
M$^3$Net&#65306;&#29992;&#20110;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#30340;&#22810;&#32423;&#12289;&#28151;&#21512;&#21644;&#22810;&#38454;&#27573;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
M$^3$Net: Multilevel, Mixed and Multistage Attention Network for Salient Object Detection. (arXiv:2309.08365v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;M$^3$Net&#65292;&#19968;&#31181;&#29992;&#20110;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#30340;&#22810;&#32423;&#12289;&#28151;&#21512;&#21644;&#22810;&#38454;&#27573;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#20132;&#20114;&#22359;&#21644;&#28151;&#21512;&#27880;&#24847;&#21147;&#22359;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26174;&#33879;&#21306;&#22495;&#30340;&#26816;&#27979;&#24615;&#33021;&#24182;&#20934;&#30830;&#23450;&#20301;&#22797;&#26434;&#23545;&#35937;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;U-Net&#25110;&#29305;&#24449;&#37329;&#23383;&#22612;&#32467;&#26500;&#65292;&#31616;&#21333;&#22320;&#32858;&#21512;&#19981;&#21516;&#23610;&#24230;&#30340;&#29305;&#24449;&#22270;&#65292;&#24573;&#35270;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#24615;&#21644;&#30456;&#20114;&#20381;&#36182;&#24615;&#20197;&#21450;&#23427;&#20204;&#23545;&#26368;&#32456;&#39044;&#27979;&#30340;&#36129;&#29486;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M$^3$Net&#65292;&#21363;&#22810;&#32423;&#12289;&#28151;&#21512;&#21644;&#22810;&#38454;&#27573;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#26174;&#33879;&#30446;&#26631;&#26816;&#27979;&#65288;SOD&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#20132;&#20114;&#22359;&#65292;&#21019;&#26032;&#24615;&#22320;&#24341;&#20837;&#20102;&#20132;&#21449;&#27880;&#24847;&#21147;&#26041;&#27861;&#26469;&#23454;&#29616;&#22810;&#32423;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20801;&#35768;&#39640;&#23618;&#29305;&#24449;&#25351;&#23548;&#20302;&#23618;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#22686;&#24378;&#26174;&#33879;&#21306;&#22495;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;&#20197;&#21069;&#22522;&#20110;Transformer&#30340;SOD&#26041;&#27861;&#21482;&#20351;&#29992;&#20840;&#23616;&#33258;&#27880;&#24847;&#21147;&#26469;&#23450;&#20301;&#26174;&#33879;&#21306;&#22495;&#65292;&#32780;&#26080;&#27861;&#36991;&#20813;&#24573;&#35270;&#22797;&#26434;&#23545;&#35937;&#30340;&#32454;&#33410;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#27880;&#24847;&#21147;&#22359;&#12290;&#27492;&#22359;&#32467;&#21512;&#20102;&#20840;&#23616;&#33258;&#27880;&#24847;&#21147;&#21644;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#26088;&#22312;&#24314;&#27169;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing salient object detection methods mostly use U-Net or feature pyramid structure, which simply aggregates feature maps of different scales, ignoring the uniqueness and interdependence of them and their respective contributions to the final prediction. To overcome these, we propose the M$^3$Net, i.e., the Multilevel, Mixed and Multistage attention network for Salient Object Detection (SOD). Firstly, we propose Multiscale Interaction Block which innovatively introduces the cross-attention approach to achieve the interaction between multilevel features, allowing high-level features to guide low-level feature learning and thus enhancing salient regions. Secondly, considering the fact that previous Transformer based SOD methods locate salient regions only using global self-attention while inevitably overlooking the details of complex objects, we propose the Mixed Attention Block. This block combines global self-attention and window self-attention, aiming at modeling context at b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;&#65292;&#21253;&#25324;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#31561;&#26041;&#38754;&#12290;&#21363;&#20351;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08345</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases. (arXiv:2309.08345v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;&#65292;&#21253;&#25324;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#31561;&#26041;&#38754;&#12290;&#21363;&#20351;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#19982;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#31561;&#29616;&#23454;&#29615;&#22659;&#30340;&#25972;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#27424;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#24433;&#21709;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#24212;&#29992;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#8220;&#20135;&#29983;&#34394;&#20551;&#20449;&#24687;&#8221;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;LM&#22312;&#22788;&#29702;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#20219;&#21153;&#26102;&#25152;&#36935;&#21040;&#30340;&#20581;&#22766;&#24615;&#25361;&#25112;&#12290;&#30740;&#31350;&#35206;&#30422;&#20102;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#30340;&#22330;&#26223;&#65292;&#20363;&#22914;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#21508;&#31181;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#25581;&#31034;&#20102;&#21363;&#20351;&#22312;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65292;&#20808;&#36827;&#30340;&#23567;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have already demonstrated remarkable abilities in understanding and generating both natural and formal language. Despite these advances, their integration with real-world environments such as large-scale knowledge bases (KBs) remains an underdeveloped area, affecting applications such as semantic parsing and indulging in "hallucinated" information. This paper is an experimental investigation aimed at uncovering the robustness challenges that LMs encounter when tasked with knowledge base question answering (KBQA). The investigation covers scenarios with inconsistent data distribution between training and inference, such as generalization to unseen domains, adaptation to various language variations, and transferability across different datasets. Our comprehensive experiments reveal that even when employed with our proposed data augmentation techniques, advanced small and large language models exhibit poor performance in various dimensions. While the LM is a promisin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#35841;&#20250;&#25442;&#24037;&#20316;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#20351;&#29992;&#22810;&#31181;ML&#31639;&#27861;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#20351;&#29992;&#20102;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#12290;&#35780;&#20272;&#27169;&#22411;&#26102;&#20351;&#29992;&#20102;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1-Score&#21644;&#20934;&#30830;&#29575;&#31561;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.08333</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#39044;&#27979;&#35841;&#20250;&#25442;&#24037;&#20316;
&lt;/p&gt;
&lt;p&gt;
Let's Predict Who Will Move to a New Job. (arXiv:2309.08333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#35841;&#20250;&#25442;&#24037;&#20316;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#20351;&#29992;&#22810;&#31181;ML&#31639;&#27861;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#20351;&#29992;&#20102;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#12290;&#35780;&#20272;&#27169;&#22411;&#26102;&#20351;&#29992;&#20102;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1-Score&#21644;&#20934;&#30830;&#29575;&#31561;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#19968;&#23478;&#20844;&#21496;&#30340;&#20154;&#21147;&#36164;&#28304;&#37096;&#38376;&#37117;&#38754;&#20020;&#30528;&#39044;&#27979;&#30003;&#35831;&#20154;&#26159;&#21542;&#20250;&#23547;&#25214;&#26032;&#24037;&#20316;&#25110;&#32773;&#30041;&#22312;&#20844;&#21496;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26469;&#39044;&#27979;&#35841;&#20250;&#25442;&#24037;&#20316;&#12290;&#39318;&#20808;&#65292;&#23558;&#25968;&#25454;&#39044;&#22788;&#29702;&#25104;&#36866;&#21512;ML&#27169;&#22411;&#30340;&#26684;&#24335;&#12290;&#20026;&#20102;&#22788;&#29702;&#20998;&#31867;&#29305;&#24449;&#65292;&#24212;&#29992;&#25968;&#25454;&#32534;&#30721;&#24182;&#25191;&#34892;&#20960;&#31181;ML&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#12289;&#36923;&#36753;&#22238;&#24402;&#65288;LR&#65289;&#12289;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#12290;&#20026;&#20102;&#25552;&#39640;ML&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#36827;&#34892;&#20445;&#30041;&#12290;&#20351;&#29992;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1-Score&#21644;&#20934;&#30830;&#29575;&#31561;&#20915;&#31574;&#25903;&#25345;&#24230;&#37327;&#26469;&#35780;&#20272;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Any company's human resources department faces the challenge of predicting whether an applicant will search for a new job or stay with the company. In this paper, we discuss how machine learning (ML) is used to predict who will move to a new job. First, the data is pre-processed into a suitable format for ML models. To deal with categorical features, data encoding is applied and several MLA (ML Algorithms) are performed including Random Forest (RF), Logistic Regression (LR), Decision Tree (DT), and eXtreme Gradient Boosting (XGBoost). To improve the performance of ML models, the synthetic minority oversampling technique (SMOTE) is used to retain them. Models are assessed using decision support metrics such as precision, recall, F1-Score, and accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20248;&#21270;&#22823;&#32928;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#24182;&#32467;&#21512;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22823;&#32928;3D&#24418;&#29366;&#30340;&#31934;&#21270;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.08289</link><description>&lt;p&gt;
&#21033;&#29992;&#28857;&#25193;&#25955;&#27169;&#22411;&#23545;&#22823;&#32928;&#30340;3D&#24418;&#29366;&#36827;&#34892;&#31934;&#21270;&#20197;&#29983;&#25104;&#25968;&#23383;&#24187;&#24433;
&lt;/p&gt;
&lt;p&gt;
Large Intestine 3D Shape Refinement Using Point Diffusion Models for Digital Phantom Generation. (arXiv:2309.08289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20248;&#21270;&#22823;&#32928;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#24182;&#32467;&#21512;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22823;&#32928;3D&#24418;&#29366;&#30340;&#31934;&#21270;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#24314;&#27169;&#20154;&#20307;&#22120;&#23448;&#22312;&#26500;&#24314;&#34394;&#25311;&#25104;&#20687;&#35797;&#39564;&#30340;&#35745;&#31639;&#20223;&#30495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20174;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20013;&#29983;&#25104;&#35299;&#21078;&#23398;&#19978;&#21487;&#20449;&#30340;&#22120;&#23448;&#34920;&#38754;&#37325;&#24314;&#20173;&#28982;&#23545;&#20154;&#20307;&#32467;&#26500;&#20013;&#30340;&#35768;&#22810;&#22120;&#23448;&#26469;&#35828;&#26159;&#20010;&#25361;&#25112;&#12290;&#22312;&#22788;&#29702;&#22823;&#32928;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#23588;&#20026;&#26126;&#26174;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#20248;&#21270;&#22823;&#32928;&#20998;&#21106;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#22120;&#23448;&#34920;&#31034;&#20026;&#20174;3D&#20998;&#21106;&#25513;&#27169;&#34920;&#38754;&#37319;&#26679;&#24471;&#21040;&#30340;&#28857;&#20113;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#33719;&#24471;&#22120;&#23448;&#24418;&#29366;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#20998;&#23618;&#28508;&#22312;&#31354;&#38388;&#20013;&#35757;&#32451;&#20004;&#20010;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#34892;&#24418;&#29366;&#31934;&#21270;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#24418;&#29366;&#30340;&#26356;&#22909;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate 3D modeling of human organs plays a crucial role in building computational phantoms for virtual imaging trials. However, generating anatomically plausible reconstructions of organ surfaces from computed tomography scans remains challenging for many structures in the human body. This challenge is particularly evident when dealing with the large intestine. In this study, we leverage recent advancements in geometric deep learning and denoising diffusion probabilistic models to refine the segmentation results of the large intestine. We begin by representing the organ as point clouds sampled from the surface of the 3D segmentation mask. Subsequently, we employ a hierarchical variational autoencoder to obtain global and local latent representations of the organ's shape. We train two conditional denoising diffusion models in the hierarchical latent space to perform shape refinement. To further enhance our method, we incorporate a state-of-the-art surface reconstruction model, allowin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PPO&#65289;&#38024;&#23545;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#36873;&#25321;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26102;&#38388;&#21644;&#27745;&#26579;&#26469;&#32531;&#35299;&#20132;&#36890;&#38459;&#22622;&#38382;&#39064;&#65292;&#32463;&#23454;&#35777;&#20998;&#26512;&#21644;&#23450;&#24615;&#35780;&#20272;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08254</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Quantitative and Qualitative Evaluation of Reinforcement Learning Policies for Autonomous Vehicles. (arXiv:2309.08254v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PPO&#65289;&#38024;&#23545;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#36873;&#25321;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26102;&#38388;&#21644;&#27745;&#26579;&#26469;&#32531;&#35299;&#20132;&#36890;&#38459;&#22622;&#38382;&#39064;&#65292;&#32463;&#23454;&#35777;&#20998;&#26512;&#21644;&#23450;&#24615;&#35780;&#20272;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#20132;&#36890;&#29615;&#22659;&#20013;&#20248;&#21270;&#20132;&#36890;&#21160;&#21147;&#23398;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#19982;&#20154;&#39550;&#39542;&#36710;&#36742;&#24182;&#23384;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;AVs&#36873;&#25321;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#31574;&#30053;&#26469;&#26368;&#23567;&#21270;&#20132;&#36890;&#38459;&#22622;&#65288;&#21363;&#26368;&#23567;&#21270;&#27178;&#36807;&#31859;&#20848;&#30340;&#29615;&#24418;&#36947;&#30340;&#26102;&#38388;&#65289;&#24182;&#20943;&#23569;&#27745;&#26579;&#12290;&#36890;&#36807;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#27745;&#26579;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20808;&#36827;&#30340;&#39550;&#39542;&#33329;&#23450;&#24615;&#35780;&#20272;&#20102;&#23398;&#21040;&#30340;&#31574;&#30053;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#25509;&#36817;&#30495;&#23454;&#19990;&#30028;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35780;&#20272;&#31574;&#30053;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25509;&#21463;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#22120;&#36827;&#34892;&#20102;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#20132;&#36890;&#24179;&#31283;&#24615;&#21644;&#23433;&#20840;&#24863;&#31561;&#19968;&#31995;&#21015;&#25351;&#26631;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#39550;&#39542;&#36710;&#36742;&#30340;&#24863;&#30693;&#21644;&#34892;&#36710;&#24179;&#28369;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing traffic dynamics in an evolving transportation landscape is crucial, particularly in scenarios where autonomous vehicles (AVs) with varying levels of autonomy coexist with human-driven cars. This paper presents a novel approach to optimizing choices of AVs using Proximal Policy Optimization (PPO), a reinforcement learning algorithm. We learned a policy to minimize traffic jams (i.e., minimize the time to cross the scenario) and to minimize pollution in a roundabout in Milan, Italy. Through empirical analysis, we demonstrate that our approach can reduce time and pollution levels. Furthermore, we qualitatively evaluate the learned policy using a cutting-edge cockpit to assess its performance in near-real-world conditions. To gauge the practicality and acceptability of the policy, we conducted evaluations with human participants using the simulator, focusing on a range of metrics like traffic smoothness and safety perception. In general, our findings show that human-driven vehi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20960;&#20309;&#35282;&#24230;&#30740;&#31350;&#20102;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#22810;&#35299;&#21644;&#30072;&#21464;&#34920;&#31034;&#38382;&#39064;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08247</link><description>&lt;p&gt;
&#23545;&#33258;&#32534;&#30721;&#22120;&#30340;&#20960;&#20309;&#35282;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Geometric Perspective on Autoencoders. (arXiv:2309.08247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20960;&#20309;&#35282;&#24230;&#30740;&#31350;&#20102;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#22810;&#35299;&#21644;&#30072;&#21464;&#34920;&#31034;&#38382;&#39064;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#30340;&#20960;&#20309;&#26041;&#38754;&#65292;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;&#34987;&#30456;&#23545;&#36739;&#23569;&#22320;&#35748;&#35782;&#21040;&#12290;&#32473;&#23450;&#19968;&#32452;&#20960;&#20046;&#20301;&#20110;&#26576;&#20010;&#36739;&#20302;&#32500;&#24230;&#27969;&#24418;&#19978;&#30340;&#39640;&#32500;&#25968;&#25454;&#28857;&#65292;&#33258;&#32534;&#30721;&#22120;&#21516;&#26102;&#23398;&#20064;&#27969;&#24418;&#21644;&#20854;&#22352;&#26631;&#22270;&#12290;&#36825;&#31181;&#20960;&#20309;&#35282;&#24230;&#33258;&#28982;&#24341;&#21457;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#27604;&#22914;&#8220;&#26377;&#38480;&#30340;&#25968;&#25454;&#28857;&#23545;&#24212;&#20110;&#21333;&#19968;&#30340;&#27969;&#24418;&#21527;&#65311;&#8221;&#25110;&#32773;&#8220;&#21482;&#26377;&#19968;&#20010;&#22352;&#26631;&#22270;&#21487;&#20197;&#34920;&#31034;&#27969;&#24418;&#21527;&#65311;&#8221;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#22238;&#31572;&#26159;&#21542;&#23450;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#32473;&#23450;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#26377;&#22810;&#20010;&#35299;&#30340;&#33258;&#32534;&#30721;&#22120;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26377;&#26102;&#20250;&#20135;&#29983;&#20855;&#26377;&#20005;&#37325;&#30072;&#21464;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#30340;&#38169;&#35823;&#27969;&#24418;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26368;&#36817;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the geometric aspect of the autoencoder framework, which, despite its importance, has been relatively less recognized. Given a set of high-dimensional data points that approximately lie on some lower-dimensional manifold, an autoencoder learns the \textit{manifold} and its \textit{coordinate chart}, simultaneously. This geometric perspective naturally raises inquiries like "Does a finite set of data points correspond to a single manifold?" or "Is there only one coordinate chart that can represent the manifold?". The responses to these questions are negative, implying that there are multiple solution autoencoders given a dataset. Consequently, they sometimes produce incorrect manifolds with severely distorted latent space representations. In this paper, we introduce recent geometric approaches that address these issues.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#23454;&#26102;&#25512;&#29702;&#33021;&#21147;&#30340;&#27969;&#24335;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#34394;&#25311;&#26799;&#24230;&#36827;&#34892;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#65292;&#20511;&#21161;&#35821;&#20041;&#35760;&#24518;&#26469;&#25233;&#21046;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.08227</link><description>&lt;p&gt;
VERSE&#65306;&#20855;&#26377;&#23454;&#26102;&#25512;&#29702;&#33021;&#21147;&#30340;&#34394;&#25311;&#26799;&#24230;&#24863;&#30693;&#27969;&#36716;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VERSE: Virtual-Gradient Aware Streaming Lifelong Learning with Anytime Inference. (arXiv:2309.08227v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08227
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#23454;&#26102;&#25512;&#29702;&#33021;&#21147;&#30340;&#27969;&#24335;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#34394;&#25311;&#26799;&#24230;&#36827;&#34892;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#65292;&#20511;&#21161;&#35821;&#20041;&#35760;&#24518;&#26469;&#25233;&#21046;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#26159;&#25351;&#22312;&#35757;&#32451;AI&#20195;&#29702;&#30340;&#21516;&#26102;&#65292;&#38450;&#27490;&#20854;&#36951;&#24536;&#20197;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#20851;&#27880;&#22312;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#32456;&#36523;&#23398;&#20064;&#65292;&#24182;&#19988;&#32570;&#20047;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#20943;&#36731;&#36951;&#24536;&#30340;&#33021;&#21147;&#12290;&#27969;&#24335;&#32456;&#36523;&#23398;&#20064;&#26159;&#32456;&#36523;&#23398;&#20064;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#21160;&#24577;&#30340;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#32780;&#19981;&#36951;&#24536;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#27969;&#24335;&#30340;&#65292;&#20165;&#38656;&#35201;&#23545;&#25968;&#25454;&#36827;&#34892;&#19968;&#27425;&#36941;&#21382;&#65292;&#21487;&#20197;&#20197;&#31867;&#22686;&#37327;&#30340;&#26041;&#24335;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#34892;&#21363;&#26102;&#35780;&#20272;&#65288;&#23454;&#26102;&#25512;&#29702;&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#34394;&#25311;&#26799;&#24230;&#65292;&#20197;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#20511;&#21161;&#22522;&#20110;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#30340;&#35821;&#20041;&#35760;&#24518;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning, also referred to as continual learning, is the problem of training an AI agent continuously while also preventing it from forgetting its previously acquired knowledge. Most of the existing methods primarily focus on lifelong learning within a static environment and lack the ability to mitigate forgetting in a quickly-changing dynamic environment. Streaming lifelong learning is a challenging setting of lifelong learning with the goal of continuous learning in a dynamic non-stationary environment without forgetting. We introduce a novel approach to lifelong learning, which is streaming, requires a single pass over the data, can learn in a class-incremental manner, and can be evaluated on-the-fly (anytime inference). To accomplish these, we propose virtual gradients for continual representation learning to prevent catastrophic forgetting and leverage an exponential-moving-average-based semantic memory to further enhance performance. Extensive experiments on diverse data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(&#22914;GPT3.5)&#21487;&#20197;&#35299;&#20915;&#21644;&#35299;&#37322;&#29289;&#29702;&#35789;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29289;&#29702;&#30693;&#35782;&#36827;&#34892;&#35745;&#31639;&#21644;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#35299;&#20915;&#29575;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#33021;&#22815;&#24635;&#32467;&#28041;&#21450;&#30340;&#30693;&#35782;&#12289;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#21019;&#36896;&#26032;&#30340;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08182</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#21644;&#35299;&#37322;&#29289;&#29702;&#35789;&#38382;&#39064;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level. (arXiv:2309.08182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(&#22914;GPT3.5)&#21487;&#20197;&#35299;&#20915;&#21644;&#35299;&#37322;&#29289;&#29702;&#35789;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29289;&#29702;&#30693;&#35782;&#36827;&#34892;&#35745;&#31639;&#21644;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#35299;&#20915;&#29575;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#33021;&#22815;&#24635;&#32467;&#28041;&#21450;&#30340;&#30693;&#35782;&#12289;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#21019;&#36896;&#26032;&#30340;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22522;&#20110;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#19981;&#20165;&#21487;&#20197;&#35299;&#20915;&#32431;&#25968;&#23398;&#39064;&#65292;&#36824;&#21487;&#20197;&#35299;&#20915;&#29289;&#29702;&#35789;&#38382;&#39064;-&#21363;&#22522;&#20110;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#36827;&#34892;&#35745;&#31639;&#21644;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#27880;&#37322;&#20102;&#31532;&#19968;&#20010;&#29289;&#29702;&#35789;&#38382;&#39064;&#25968;&#25454;&#38598;-PhysQA&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;1000&#20010;&#21021;&#20013;&#29289;&#29702;&#35789;&#38382;&#39064;&#65288;&#21253;&#25324;&#36816;&#21160;&#23398;&#12289;&#36136;&#37327;&#21644;&#23494;&#24230;&#12289;&#21147;&#23398;&#12289;&#28909;&#23398;&#21644;&#30005;&#23398;&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;GPT3.5&#26469;&#29983;&#25104;&#36825;&#20123;&#38382;&#39064;&#30340;&#31572;&#26696;&#65292;&#21457;&#29616;GPT3.5&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#19978;&#33258;&#21160;&#35299;&#20915;49.3%&#30340;&#38382;&#39064;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#19978;&#21017;&#20026;73.2%&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#31867;&#20284;&#38382;&#39064;&#21450;&#20854;&#31572;&#26696;&#20316;&#20026;&#25552;&#31034;&#65292;LLM&#21487;&#20197;&#35299;&#20915;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#22522;&#30784;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;&#38500;&#20102;&#33258;&#21160;&#35299;&#20915;&#38382;&#39064;&#65292;GPT3.5&#36824;&#21487;&#20197;&#24635;&#32467;&#38382;&#39064;&#28041;&#21450;&#30340;&#30693;&#35782;&#25110;&#20027;&#39064;&#65292;&#29983;&#25104;&#30456;&#20851;&#35299;&#37322;&#65292;&#24182;&#26681;&#25454;&#36755;&#20837;&#38382;&#39064;&#32508;&#21512;&#20986;&#26032;&#30340;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work demonstrates that large language model (LLM) pre-trained on texts can not only solve pure math word problems, but also physics word problems-problems to be solved by calculation and inference based on some prior physical knowledge. We collect and annotate the first physics word problem dataset-PhysQA, which contains over 1000 junior high school physics word problems (on Kinematics, Mass&amp;Density, Mechanics, Heat, Electricity). Then we use OpenAI' s GPT3.5 to generate the answer of these problems and found that GPT3.5 could automatically solve 49.3% of the problems on zero-shot learning and 73.2% on few-shot learning. This result show that by using similar problem and its answer as prompt, LLM could solve elementary physics word problems approaching human level. Besides automatically solving problems, GPT3.5 could also summarize the knowledge or topic examined by the problem, generate the relevant explanation, and synthesis new physics word problems according tothe input problem
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26469;&#23398;&#20064;&#25429;&#25417;&#25968;&#25454;&#30456;&#20851;&#30340;&#19981;&#21464;&#24615;&#30340;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500;&#30340;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#23398;&#20064;&#30340;&#32593;&#32476;&#26550;&#26500;&#22312;&#35270;&#35273;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#37117;&#27604;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#33394;&#65292;&#19981;&#20165;&#25928;&#29575;&#39640;&#65292;&#32780;&#19988;&#25928;&#26524;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.08171</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#25581;&#31034;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unveiling Invariances via Neural Network Pruning. (arXiv:2309.08171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08171
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26469;&#23398;&#20064;&#25429;&#25417;&#25968;&#25454;&#30456;&#20851;&#30340;&#19981;&#21464;&#24615;&#30340;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500;&#30340;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#23398;&#20064;&#30340;&#32593;&#32476;&#26550;&#26500;&#22312;&#35270;&#35273;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#37117;&#27604;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#33394;&#65292;&#19981;&#20165;&#25928;&#29575;&#39640;&#65292;&#32780;&#19988;&#25928;&#26524;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21464;&#24615;&#25551;&#36848;&#20102;&#23545;&#25968;&#25454;&#24213;&#23618;&#35821;&#20041;&#27809;&#26377;&#24433;&#21709;&#30340;&#36716;&#25442;&#12290;&#20445;&#25345;&#33258;&#28982;&#19981;&#21464;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#33391;&#22909;&#30340;&#24402;&#32435;&#20559;&#24046;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#29616;&#20195;&#32593;&#32476;&#34987;&#25163;&#24037;&#35774;&#35745;&#29992;&#26469;&#22788;&#29702;&#20247;&#25152;&#21608;&#30693;&#30340;&#19981;&#21464;&#24615;&#65288;&#20363;&#22914;&#24179;&#31227;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21098;&#26525;&#26469;&#23398;&#20064;&#25429;&#25417;&#25968;&#25454;&#30456;&#20851;&#30340;&#19981;&#21464;&#24615;&#30340;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#23398;&#21040;&#30340;&#32593;&#32476;&#26550;&#26500;&#22312;&#35270;&#35273;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#37117;&#27604;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#22312;&#25928;&#29575;&#21644;&#25928;&#26524;&#19978;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#22312;3&#20010;&#35270;&#35273;&#21644;40&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariance describes transformations that do not alter data's underlying semantics. Neural networks that preserve natural invariance capture good inductive biases and achieve superior performance. Hence, modern networks are handcrafted to handle well-known invariances (ex. translations). We propose a framework to learn novel network architectures that capture data-dependent invariances via pruning. Our learned architectures consistently outperform dense neural networks on both vision and tabular datasets in both efficiency and effectiveness. We demonstrate our framework on multiple deep learning models across 3 vision and 40 tabular datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22270;&#28145;&#24230;&#26680;&#23398;&#20064;&#26694;&#26550;&#26469;&#22788;&#29702;&#32593;&#32476;&#25968;&#25454;&#19978;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#27491;&#24615;&#20551;&#35774;&#36829;&#21453;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08165</link><description>&lt;p&gt;
&#39044;&#27979;&#36824;&#26159;&#25298;&#32477;&#65306;&#32593;&#32476;&#25968;&#25454;&#19978;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#19982;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
To Predict or to Reject: Causal Effect Estimation with Uncertainty on Networked Data. (arXiv:2309.08165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22270;&#28145;&#24230;&#26680;&#23398;&#20064;&#26694;&#26550;&#26469;&#22788;&#29702;&#32593;&#32476;&#25968;&#25454;&#19978;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#27491;&#24615;&#20551;&#35774;&#36829;&#21453;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32593;&#32476;&#35266;&#23519;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#24615;&#65292;&#23545;&#20110;&#26576;&#20123;&#20010;&#20307;&#30340;&#22240;&#26524;&#25928;&#24212;&#39044;&#27979;&#21487;&#33021;&#20005;&#37325;&#36829;&#21453;&#27491;&#24615;/&#37325;&#21472;&#20551;&#35774;&#65292;&#23548;&#33268;&#20272;&#35745;&#19981;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#32593;&#32476;&#25968;&#25454;&#20010;&#20307;&#32423;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#30340;&#36825;&#31181;&#28508;&#22312;&#39118;&#38505;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#21019;&#24314;&#19968;&#20010;&#26356;&#21487;&#20449;&#36182;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#22270;&#28145;&#24230;&#26680;&#23398;&#20064; (GraphDKL) &#26694;&#26550;&#65292;&#24182;&#36890;&#36807;Lipschitz&#32422;&#26463;&#26469;&#24314;&#27169;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20197;&#35782;&#21035;&#19981;&#21487;&#38752;&#30340;&#20272;&#35745;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;GraphDKL&#26159;&#31532;&#19968;&#20010;&#22312;&#25191;&#34892;&#22270;&#19978;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26102;&#22788;&#29702;&#27491;&#24615;&#20551;&#35774;&#36829;&#21453;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32593;&#32476;&#25968;&#25454;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the imbalanced nature of networked observational data, the causal effect predictions for some individuals can severely violate the positivity/overlap assumption, rendering unreliable estimations. Nevertheless, this potential risk of individual-level treatment effect estimation on networked data has been largely under-explored. To create a more trustworthy causal effect estimator, we propose the uncertainty-aware graph deep kernel learning (GraphDKL) framework with Lipschitz constraint to model the prediction uncertainty with Gaussian process and identify unreliable estimations. To the best of our knowledge, GraphDKL is the first framework to tackle the violation of positivity assumption when performing causal effect estimation with graphs. With extensive experiments, we demonstrate the superiority of our proposed method in uncertainty-aware causal effect estimation on networked data.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#36827;&#34892;&#27979;&#37327;&#26102;&#65292;&#19981;&#21516;&#30340;&#25552;&#31034;&#20250;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#20154;&#26684;&#24471;&#20998;&#65292;&#22240;&#27492;&#32570;&#20047;&#23458;&#35266;&#26631;&#20934;&#26469;&#21028;&#26029;&#21738;&#20010;&#25552;&#31034;&#26356;&#27491;&#30830;&#12290;</title><link>http://arxiv.org/abs/2309.08163</link><description>&lt;p&gt;
&#30740;&#31350;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#27979;&#37327;&#20013;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the Applicability of Self-Assessment Tests for Personality Measurement of Large Language Models. (arXiv:2309.08163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08163
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#36827;&#34892;&#27979;&#37327;&#26102;&#65292;&#19981;&#21516;&#30340;&#25552;&#31034;&#20250;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#20154;&#26684;&#24471;&#20998;&#65292;&#22240;&#27492;&#32570;&#20047;&#23458;&#35266;&#26631;&#20934;&#26469;&#21028;&#26029;&#21738;&#20010;&#25552;&#31034;&#26356;&#27491;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#21457;&#23637;&#65292;&#21508;&#31181;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#20351;&#29992;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#30340;&#24515;&#29702;&#24037;&#20855;&#26469;&#37327;&#21270;&#23427;&#20204;&#30340;&#34892;&#20026;&#12290;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;&#20351;&#29992;&#20154;&#26684;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#8220;&#20154;&#26684;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#20851;&#20110;&#20351;&#29992;&#20154;&#26684;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#19977;&#20010;&#19981;&#21516;&#35770;&#25991;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#26469;&#35780;&#20272;&#21516;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#19977;&#20010;&#25552;&#31034;&#23548;&#33268;&#20102;&#38750;&#24120;&#19981;&#21516;&#30340;&#20154;&#26684;&#24471;&#20998;&#12290;&#36825;&#19968;&#31616;&#21333;&#27979;&#35797;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#26684;&#33258;&#25105;&#35780;&#20272;&#24471;&#20998;&#21462;&#20915;&#20110;&#25552;&#31034;&#32773;&#30340;&#20027;&#35266;&#36873;&#25321;&#12290;&#30001;&#20110;&#25105;&#20204;&#19981;&#30693;&#36947;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#24471;&#20998;&#30340;&#30495;&#23454;&#20540;&#65292;&#22240;&#20026;&#27492;&#31867;&#38382;&#39064;&#27809;&#26377;&#27491;&#30830;&#31572;&#26696;&#65292;&#25152;&#20197;&#26080;&#27861;&#22768;&#26126;&#26576;&#20010;&#25552;&#31034;&#27604;&#20854;&#20182;&#25552;&#31034;&#26356;&#27491;&#30830;&#25110;&#26356;&#19981;&#27491;&#30830;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20154;&#26684;&#36873;&#39033;&#39034;&#24207;&#23545;&#31216;&#24615;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLM) evolve in their capabilities, various recent studies have tried to quantify their behavior using psychological tools created to study human behavior. One such example is the measurement of "personality" of LLMs using personality self-assessment tests. In this paper, we take three such studies on personality measurement of LLMs that use personality self-assessment tests created to study human behavior. We use the prompts used in these three different papers to measure the personality of the same LLM. We find that all three prompts lead very different personality scores. This simple test reveals that personality self-assessment scores in LLMs depend on the subjective choice of the prompter. Since we don't know the ground truth value of personality scores for LLMs as there is no correct answer to such questions, there's no way of claiming if one prompt is more or less correct than the other. We then introduce the property of option order symmetry for persona
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#38656;&#27714;&#39537;&#21160;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#30340;&#38656;&#27714;&#19982;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#23646;&#24615;&#31354;&#38388;&#36827;&#34892;&#23548;&#33322;&#20915;&#31574;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#29992;&#25143;&#26080;&#27861;&#30693;&#36947;&#23545;&#35937;&#21517;&#31216;&#25110;&#25351;&#23450;&#23545;&#35937;&#19981;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08138</link><description>&lt;p&gt;
&#23547;&#25214;&#20320;&#24819;&#35201;&#30340;&#65306;&#23398;&#20064;&#23454;&#29616;&#38656;&#27714;&#39537;&#21160;&#23548;&#33322;&#30340;&#23545;&#35937;&#23646;&#24615;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation. (arXiv:2309.08138v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#38656;&#27714;&#39537;&#21160;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#30340;&#38656;&#27714;&#19982;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#23646;&#24615;&#31354;&#38388;&#36827;&#34892;&#23548;&#33322;&#20915;&#31574;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#29992;&#25143;&#26080;&#27861;&#30693;&#36947;&#23545;&#35937;&#21517;&#31216;&#25110;&#25351;&#23450;&#23545;&#35937;&#19981;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23545;&#35937;&#23548;&#33322;&#65288;VON&#65289;&#30340;&#20219;&#21153;&#26159;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#32473;&#23450;&#30340;&#22330;&#26223;&#20013;&#23450;&#20301;&#29305;&#23450;&#30340;&#23545;&#35937;&#12290;&#20026;&#20102;&#25104;&#21151;&#23436;&#25104;VON&#20219;&#21153;&#65292;&#24517;&#39035;&#28385;&#36275;&#20004;&#20010;&#22522;&#26412;&#26465;&#20214;&#65306;1&#65289;&#29992;&#25143;&#24517;&#39035;&#30693;&#36947;&#25152;&#38656;&#23545;&#35937;&#30340;&#21517;&#31216;&#65307;2&#65289;&#29992;&#25143;&#25351;&#23450;&#30340;&#23545;&#35937;&#24517;&#39035;&#30830;&#23454;&#23384;&#22312;&#20110;&#22330;&#26223;&#20013;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#65292;&#27169;&#25311;&#22120;&#21487;&#20197;&#23558;&#39044;&#23450;&#20041;&#30340;&#23545;&#35937;&#21517;&#31216;&#21644;&#20301;&#32622;&#32435;&#20837;&#22330;&#26223;&#30340;&#20803;&#25968;&#25454;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#30830;&#20445;&#22987;&#32456;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#24448;&#24448;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#38476;&#29983;&#30340;&#29615;&#22659;&#20013;&#65292;&#20154;&#20204;&#21487;&#33021;&#19981;&#30693;&#36947;&#22330;&#26223;&#20013;&#23384;&#22312;&#21738;&#20123;&#23545;&#35937;&#65292;&#25110;&#32773;&#20182;&#20204;&#21487;&#33021;&#38169;&#35823;&#22320;&#25351;&#23450;&#19968;&#20010;&#23454;&#38469;&#19978;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#25361;&#25112;&#65292;&#20154;&#20204;&#20173;&#28982;&#21487;&#33021;&#23545;&#19968;&#20010;&#23545;&#35937;&#26377;&#38656;&#27714;&#65292;&#36825;&#20010;&#38656;&#27714;&#21487;&#33021;&#21487;&#20197;&#36890;&#36807;&#22330;&#26223;&#20013;&#23384;&#22312;&#30340;&#20854;&#20182;&#23545;&#35937;&#20197;&#31561;&#25928;&#30340;&#26041;&#24335;&#26469;&#28385;&#36275;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38656;&#27714;&#39537;&#21160;&#23548;&#33322;&#65288;DDN&#65289;&#65292;&#23427;&#21033;&#29992;&#29992;&#25143;&#30340;&#38656;&#27714;&#19982;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#23646;&#24615;&#31354;&#38388;&#36827;&#34892;&#23548;&#33322;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of Visual Object Navigation (VON) involves an agent's ability to locate a particular object within a given scene. In order to successfully accomplish the VON task, two essential conditions must be fulfilled:1) the user must know the name of the desired object; and 2) the user-specified object must actually be present within the scene. To meet these conditions, a simulator can incorporate pre-defined object names and positions into the metadata of the scene. However, in real-world scenarios, it is often challenging to ensure that these conditions are always met. Human in an unfamiliar environment may not know which objects are present in the scene, or they may mistakenly specify an object that is not actually present. Nevertheless, despite these challenges, human may still have a demand for an object, which could potentially be fulfilled by other objects present within the scene in an equivalent manner. Hence, we propose Demand-driven Navigation (DDN), which leverages the user'
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#36341;&#25945;&#31243;&#30340;&#35774;&#35745;&#65292;&#25552;&#39640;&#20102;AI&#21019;&#20316;&#32773;&#23545;&#24615;&#21035;&#20559;&#35265;&#30340;&#35748;&#35782;&#21644;&#30693;&#35782;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#25945;&#32946;&#30340;&#19981;&#36275;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#21644;&#35774;&#35745;&#24037;&#20316;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2309.08121</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#36341;&#25945;&#31243;&#21521;AI&#21019;&#20316;&#32773;&#20256;&#25480;&#24615;&#21035;&#20559;&#35265;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#20182;&#20204;&#23545;&#24615;&#21035;&#20559;&#35265;&#30340;&#35748;&#35782;
&lt;/p&gt;
&lt;p&gt;
"I'm Not Confident in Debiasing AI Systems Since I Know Too Little": Teaching AI Creators About Gender Bias Through Hands-on Tutorials. (arXiv:2309.08121v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#36341;&#25945;&#31243;&#30340;&#35774;&#35745;&#65292;&#25552;&#39640;&#20102;AI&#21019;&#20316;&#32773;&#23545;&#24615;&#21035;&#20559;&#35265;&#30340;&#35748;&#35782;&#21644;&#30693;&#35782;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#25945;&#32946;&#30340;&#19981;&#36275;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#21644;&#35774;&#35745;&#24037;&#20316;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#21035;&#20559;&#35265;&#22312;AI&#31995;&#32479;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#32473;&#22899;&#24615;&#29992;&#25143;&#24102;&#26469;&#20102;&#31967;&#31957;&#30340;&#20307;&#39564;&#65292;&#20135;&#29983;&#20102;&#19981;&#20844;&#27491;&#21644;&#24515;&#29702;&#19978;&#30340;&#20260;&#23475;&#12290;&#23398;&#26657;&#25945;&#32946;&#26410;&#33021;&#32473;AI&#21019;&#20316;&#32773;&#25552;&#20379;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#25945;&#32946;&#65292;&#20351;&#20182;&#20204;&#26080;&#27861;&#26377;&#25928;&#22320;&#20943;&#23569;AI&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#26412;&#25991;&#38024;&#23545;AI&#21019;&#20316;&#32773;&#35774;&#35745;&#20102;&#23454;&#36341;&#25945;&#31243;&#65292;&#25552;&#39640;&#20182;&#20204;&#23545;AI&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#35748;&#35782;&#65292;&#24182;&#22686;&#24378;&#20182;&#20204;&#23545;&#24615;&#21035;&#20559;&#35265;&#26469;&#28304;&#21644;&#21435;&#20559;&#26041;&#27861;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;18&#21517;AI&#21019;&#20316;&#32773;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;AI&#30740;&#31350;&#20154;&#21592;&#12289;AI&#24037;&#19994;&#20174;&#19994;&#32773;&#65288;&#24320;&#21457;&#20154;&#21592;&#21644;&#20135;&#21697;&#32463;&#29702;&#65289;&#21644;&#23398;&#20064;&#36807;AI&#30340;&#23398;&#29983;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25945;&#31243;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#20123;&#25945;&#31243;&#26377;&#26395;&#24357;&#34917;CS/AI&#35838;&#31243;&#20013;&#19981;&#36275;&#30340;AI&#24615;&#21035;&#20559;&#35265;&#25945;&#32946;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#32508;&#21512;&#20102;&#35774;&#35745;&#24847;&#20041;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#12289;&#25945;&#32946;&#21644;&#35774;&#35745;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gender bias is rampant in AI systems, causing bad user experience, injustices, and mental harm to women. School curricula fail to educate AI creators on this topic, leaving them unprepared to mitigate gender bias in AI. In this paper, we designed hands-on tutorials to raise AI creators' awareness of gender bias in AI and enhance their knowledge of sources of gender bias and debiasing techniques. The tutorials were evaluated with 18 AI creators, including AI researchers, AI industrial practitioners (i.e., developers and product managers), and students who had learned AI. Their improved awareness and knowledge demonstrated the effectiveness of our tutorials, which have the potential to complement the insufficient AI gender bias education in CS/AI courses. Based on the findings, we synthesize design implications and a rubric to guide future research, education, and design efforts.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#36807;&#31243;&#25366;&#25496;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#32945;&#19978;&#20551;&#32930;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#30446;&#26631;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#19978;&#26174;&#33879;&#20248;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#38169;&#35823;&#26102;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#33258;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.08106</link><description>&lt;p&gt;
&#20351;&#29992;&#36807;&#31243;&#25366;&#25496;&#25216;&#26415;&#22312;&#32945;&#19978;&#20551;&#32930;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#30446;&#26631;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Goal Recognition in Transhumeral Prostheses Using Process Mining Techniques. (arXiv:2309.08106v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#36807;&#31243;&#25366;&#25496;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#32945;&#19978;&#20551;&#32930;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#30446;&#26631;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#19978;&#26174;&#33879;&#20248;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#38169;&#35823;&#26102;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#33258;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32945;&#19978;&#20551;&#32930;&#33021;&#22815;&#24674;&#22797;&#25163;&#20197;&#19979;&#32570;&#22833;&#30340;&#35299;&#21078;&#32467;&#26500;&#12290;&#20027;&#21160;&#24335;&#20551;&#32930;&#21033;&#29992;&#36830;&#32493;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#35782;&#21035;&#24739;&#32773;&#30340;&#30446;&#26631;&#23039;&#21183;&#65292;&#24182;&#20027;&#21160;&#31227;&#21160;&#20154;&#24037;&#32930;&#20307;&#12290;&#20197;&#24448;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#22914;&#20309;&#21033;&#29992;&#38745;&#27490;&#23039;&#21183;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#21306;&#20998;&#30446;&#26631;&#65292;&#32780;&#24573;&#30053;&#20102;&#26102;&#38388;&#27493;&#38271;&#30340;&#32771;&#34385;&#12290;&#22312;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#26469;&#33258;&#32908;&#30005;&#22270;&#30005;&#26497;&#21644;&#36816;&#21160;&#20256;&#24863;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#39034;&#24207;&#35782;&#21035;&#24739;&#32773;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23558;&#25968;&#25454;&#36716;&#21270;&#20026;&#31163;&#25955;&#20107;&#20214;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;&#36807;&#31243;&#25366;&#25496;&#30340;&#29616;&#26377;&#30446;&#26631;&#35782;&#21035;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;&#34394;&#25311;&#29616;&#23454;&#29615;&#22659;&#20013;&#25910;&#38598;&#20102;&#21313;&#20010;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20854;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#26174;&#33879;&#25552;&#39640;&#65292;&#24182;&#19988;&#22312;&#38169;&#35823;&#26102;&#19981;&#22826;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
A transhumeral prosthesis restores missing anatomical segments below the shoulder, including the hand. Active prostheses utilize real-valued, continuous sensor data to recognize patient target poses, or goals, and proactively move the artificial limb. Previous studies have examined how well the data collected in stationary poses, without considering the time steps, can help discriminate the goals. In this case study paper, we focus on using time series data from surface electromyography electrodes and kinematic sensors to sequentially recognize patients' goals. Our approach involves transforming the data into discrete events and training an existing process mining-based goal recognition system. Results from data collected in a virtual reality setting with ten subjects demonstrate the effectiveness of our proposed goal recognition approach, which achieves significantly better precision and recall than the state-of-the-art machine learning techniques and is less confident when wrong, whi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#23454;&#20307;&#37051;&#22495;&#20449;&#24687;&#21644;&#25551;&#36848;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#32534;&#31243;&#35774;&#35745;&#35838;&#31243;&#30693;&#35782;&#22270;&#35889;&#20013;&#23884;&#20837;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.08100</link><description>&lt;p&gt;
&#12298;&#22522;&#20110;&#23454;&#20307;&#37051;&#22495;&#20449;&#24687;&#21644;&#25551;&#36848;&#20449;&#24687;&#30340;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#12299;
&lt;/p&gt;
&lt;p&gt;
Research on Joint Representation Learning Methods for Entity Neighborhood Information and Description Information. (arXiv:2309.08100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08100
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#23454;&#20307;&#37051;&#22495;&#20449;&#24687;&#21644;&#25551;&#36848;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#32534;&#31243;&#35774;&#35745;&#35838;&#31243;&#30693;&#35782;&#22270;&#35889;&#20013;&#23884;&#20837;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;&#32534;&#31243;&#35774;&#35745;&#35838;&#31243;&#30693;&#35782;&#22270;&#35889;&#20013;&#23884;&#20837;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23454;&#20307;&#37051;&#22495;&#20449;&#24687;&#21644;&#25551;&#36848;&#20449;&#24687;&#30340;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#37319;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#33719;&#21462;&#23454;&#20307;&#37051;&#22495;&#33410;&#28857;&#30340;&#29305;&#24449;&#65292;&#32467;&#21512;&#20851;&#31995;&#29305;&#24449;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;BERT-WWM&#27169;&#22411;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#33719;&#21462;&#23454;&#20307;&#25551;&#36848;&#20449;&#24687;&#30340;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23558;&#23454;&#20307;&#37051;&#22495;&#20449;&#24687;&#21644;&#25551;&#36848;&#20449;&#24687;&#30340;&#21521;&#37327;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#26368;&#32456;&#30340;&#23454;&#20307;&#21521;&#37327;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#32534;&#31243;&#35774;&#35745;&#35838;&#31243;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the issue of poor embedding performance in the knowledge graph of a programming design course, a joint represen-tation learning model that combines entity neighborhood infor-mation and description information is proposed. Firstly, a graph at-tention network is employed to obtain the features of entity neigh-boring nodes, incorporating relationship features to enrich the structural information. Next, the BERT-WWM model is utilized in conjunction with attention mechanisms to obtain the representation of entity description information. Finally, the final entity vector representation is obtained by combining the vector representations of entity neighborhood information and description information. Experimental results demonstrate that the proposed model achieves favorable performance on the knowledge graph dataset of the pro-gramming design course, outperforming other baseline models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LCR-Net&#30340;&#22810;&#22836;&#32593;&#32476;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#23039;&#24577;&#24863;&#30693;&#26426;&#21046;&#26469;&#24555;&#36895;&#20934;&#30830;&#22320;&#22788;&#29702;&#24490;&#29615;&#20851;&#38381;&#21644;&#20301;&#32622;&#20877;&#23450;&#20301;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LCR-Net&#22312;&#20505;&#36873;&#26816;&#32034;&#12289;&#38381;&#29615;&#28857;&#20113;&#37197;&#20934;&#21644;&#22810;&#25968;&#25454;&#38598;&#36830;&#32493;&#20877;&#23450;&#20301;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08086</link><description>&lt;p&gt;
&#24555;&#36895;&#20934;&#30830;&#30340;&#28145;&#24230;&#24490;&#29615;&#20851;&#38381;&#21644;&#20301;&#32622;&#20877;&#23450;&#20301;&#20197;&#23454;&#29616;&#21487;&#38752;&#30340;LiDAR SLAM
&lt;/p&gt;
&lt;p&gt;
Fast and Accurate Deep Loop Closing and Relocalization for Reliable LiDAR SLAM. (arXiv:2309.08086v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LCR-Net&#30340;&#22810;&#22836;&#32593;&#32476;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#23039;&#24577;&#24863;&#30693;&#26426;&#21046;&#26469;&#24555;&#36895;&#20934;&#30830;&#22320;&#22788;&#29702;&#24490;&#29615;&#20851;&#38381;&#21644;&#20301;&#32622;&#20877;&#23450;&#20301;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LCR-Net&#22312;&#20505;&#36873;&#26816;&#32034;&#12289;&#38381;&#29615;&#28857;&#20113;&#37197;&#20934;&#21644;&#22810;&#25968;&#25454;&#38598;&#36830;&#32493;&#20877;&#23450;&#20301;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#20851;&#38381;&#21644;&#20301;&#32622;&#20877;&#23450;&#20301;&#26159;&#24314;&#31435;&#21487;&#38752;&#21644;&#31283;&#23450;&#30340;&#38271;&#26399;SLAM&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#23039;&#24577;&#20272;&#35745;&#28418;&#31227;&#21644;&#36864;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#34920;&#36848;&#20102;&#24490;&#29615;&#20851;&#38381;&#21644;&#20301;&#32622;&#20877;&#23450;&#20301;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22836;&#32593;&#32476;LCR-Net&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;&#23427;&#21033;&#29992;&#20102;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#23039;&#24577;&#24863;&#30693;&#26426;&#21046;&#26469;&#31934;&#30830;&#20272;&#35745;LiDAR&#25195;&#25551;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;6&#33258;&#30001;&#24230;&#23039;&#24577;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;LCR-Net&#25972;&#21512;&#21040;&#19968;&#20010;SLAM&#31995;&#32479;&#20013;&#65292;&#22312;&#23460;&#22806;&#39550;&#39542;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#31283;&#20581;&#20934;&#30830;&#30340;&#22312;&#32447;LiDAR SLAM&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#20174;&#24490;&#29615;&#20851;&#38381;&#21644;&#20301;&#32622;&#20877;&#23450;&#20301;&#24471;&#20986;&#30340;&#35774;&#32622;&#65292;&#21253;&#25324;&#20505;&#36873;&#26816;&#32034;&#12289;&#38381;&#29615;&#28857;&#20113;&#37197;&#20934;&#21644;&#22810;&#25968;&#25454;&#38598;&#30340;&#36830;&#32493;&#20877;&#23450;&#20301;&#65292;&#23545;LCR-Net&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LCR-Net&#22312;&#36825;&#19977;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Loop closing and relocalization are crucial techniques to establish reliable and robust long-term SLAM by addressing pose estimation drift and degeneration. This article begins by formulating loop closing and relocalization within a unified framework. Then, we propose a novel multi-head network LCR-Net to tackle both tasks effectively. It exploits novel feature extraction and pose-aware attention mechanism to precisely estimate similarities and 6-DoF poses between pairs of LiDAR scans. In the end, we integrate our LCR-Net into a SLAM system and achieve robust and accurate online LiDAR SLAM in outdoor driving environments. We thoroughly evaluate our LCR-Net through three setups derived from loop closing and relocalization, including candidate retrieval, closed-loop point cloud registration, and continuous relocalization using multiple datasets. The results demonstrate that LCR-Net excels in all three tasks, surpassing the state-of-the-art methods and exhibiting a remarkable generalizati
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#30456;&#20851;&#25991;&#26412;-&#38899;&#39057;&#25968;&#25454;&#20316;&#20026;&#39069;&#22806;&#26465;&#20214;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;AudioCaps&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.08051</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#22411;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Text-to-Audio Generation. (arXiv:2309.08051v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08051
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#30456;&#20851;&#25991;&#26412;-&#38899;&#39057;&#25968;&#25454;&#20316;&#20026;&#39069;&#22806;&#26465;&#20214;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;AudioCaps&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25991;&#26412;&#21040;&#38899;&#39057;(TTA)&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#25105;&#20204;&#21457;&#29616;&#29366;&#24577;-&#33402;&#26415;&#27169;&#22411;&#65292;&#22914;AudioLDM&#65292;&#22312;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31867;&#21035;&#20998;&#24067;&#19981;&#24179;&#34913;&#65288;&#22914;AudioCaps&#65289;&#30340;&#35757;&#32451;&#20013;&#65292;&#20854;&#29983;&#25104;&#24615;&#33021;&#23384;&#22312;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#20204;&#22312;&#29983;&#25104;&#24120;&#35265;&#38899;&#39057;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#22312;&#32597;&#35265;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#25972;&#20307;&#29983;&#25104;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#27492;&#38382;&#39064;&#31216;&#20026;&#38271;&#23614;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#26469;&#36827;&#34892;TTA&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#25991;&#26412;&#36755;&#20837;&#25552;&#31034;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#23545;&#27604;&#35821;&#38899;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;CLAP&#65289;&#27169;&#22411;&#26469;&#26816;&#32034;&#30456;&#20851;&#30340;&#25991;&#26412;-&#38899;&#39057;&#23545;&#12290;&#28982;&#21518;&#20351;&#29992;&#26816;&#32034;&#21040;&#30340;&#38899;&#39057;-&#25991;&#26412;&#25968;&#25454;&#30340;&#29305;&#24449;&#20316;&#20026;&#39069;&#22806;&#26465;&#20214;&#26469;&#25351;&#23548;TTA&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;AudioLDM&#65292;&#24182;&#23558;&#25152;&#24471;&#21040;&#30340;&#22686;&#24378;&#31995;&#32479;&#31216;&#20026;Re-AudioLDM&#12290;&#22312;AudioCaps&#25968;&#25454;&#38598;&#19978;&#65292;Re-AudioLDM&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Frechet&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in text-to-audio (TTA) generation, we show that the state-of-the-art models, such as AudioLDM, trained on datasets with an imbalanced class distribution, such as AudioCaps, are biased in their generation performance. Specifically, they excel in generating common audio classes while underperforming in the rare ones, thus degrading the overall generation performance. We refer to this problem as long-tailed text-to-audio generation. To address this issue, we propose a simple retrieval-augmented approach for TTA models. Specifically, given an input text prompt, we first leverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve relevant text-audio pairs. The features of the retrieved audio-text data are then used as additional conditions to guide the learning of TTA models. We enhance AudioLDM with our proposed approach and denote the resulting augmented system as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a state-of-the-art Frechet 
&lt;/p&gt;</description></item><item><title>&#22635;&#20805;&#24863;&#30693;&#31070;&#32463;&#20803;&#65288;PANs&#65289;&#26159;&#19968;&#31181;&#21367;&#31215;&#27169;&#22411;&#20013;&#24120;&#35265;&#30340;&#36807;&#28388;&#22120;&#65292;&#23427;&#19987;&#27880;&#20110;&#23545;&#36755;&#20837;&#36793;&#30028;&#20301;&#32622;&#30340;&#29305;&#24449;&#21270;&#21644;&#35782;&#21035;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.08048</link><description>&lt;p&gt;
Padding Aware Neurons&#65288;&#22635;&#20805;&#24863;&#30693;&#31070;&#32463;&#20803;&#65289;
&lt;/p&gt;
&lt;p&gt;
Padding Aware Neurons. (arXiv:2309.08048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08048
&lt;/p&gt;
&lt;p&gt;
&#22635;&#20805;&#24863;&#30693;&#31070;&#32463;&#20803;&#65288;PANs&#65289;&#26159;&#19968;&#31181;&#21367;&#31215;&#27169;&#22411;&#20013;&#24120;&#35265;&#30340;&#36807;&#28388;&#22120;&#65292;&#23427;&#19987;&#27880;&#20110;&#23545;&#36755;&#20837;&#36793;&#30028;&#20301;&#32622;&#30340;&#29305;&#24449;&#21270;&#21644;&#35782;&#21035;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#23618;&#26159;&#22823;&#22810;&#25968;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#27169;&#22411;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#23618;&#36890;&#24120;&#40664;&#35748;&#23454;&#29616;&#19968;&#31181;&#38745;&#24577;&#22635;&#20805;&#31574;&#30053;&#65288;&#22914;&#38646;&#22635;&#20805;&#65289;&#65292;&#20197;&#25511;&#21046;&#20869;&#37096;&#34920;&#31034;&#30340;&#23610;&#24230;&#65292;&#24182;&#20801;&#35768;&#20197;&#36793;&#30028;&#20026;&#20013;&#24515;&#30340;&#20869;&#26680;&#28608;&#27963;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#22635;&#20805;&#24863;&#30693;&#31070;&#32463;&#20803;&#65288;PANs&#65289;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#36807;&#28388;&#22120;&#22312;&#20351;&#29992;&#38745;&#24577;&#22635;&#20805;&#35757;&#32451;&#30340;&#22823;&#22810;&#25968;&#65288;&#22914;&#26524;&#19981;&#26159;&#20840;&#37096;&#65289;&#21367;&#31215;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#12290;PANs&#19987;&#27880;&#20110;&#23545;&#36755;&#20837;&#36793;&#30028;&#20301;&#32622;&#30340;&#29305;&#24449;&#21270;&#21644;&#35782;&#21035;&#65292;&#20026;&#27169;&#22411;&#24341;&#20837;&#20102;&#31354;&#38388;&#24402;&#32435;&#20559;&#24046;&#65288;&#20363;&#22914;&#65292;&#27169;&#24335;&#36890;&#24120;&#36317;&#31163;&#36755;&#20837;&#36793;&#30028;&#22810;&#36817;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23427;&#20204;&#30340;&#28608;&#27963;&#26469;&#35782;&#21035;PANs&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#22312;&#20960;&#31181;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#23384;&#22312;&#65292;&#21457;&#29616;&#20174;&#20960;&#21313;&#20010;&#21040;&#20960;&#30334;&#20010;&#30340;PANs&#12290;&#25105;&#20204;&#35752;&#35770;&#24182;&#35828;&#26126;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;PANs&#12289;&#23427;&#20204;&#30340;&#20869;&#26680;&#21644;&#34892;&#20026;&#12290;&#20026;&#20102;&#20102;&#35299;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22635;&#20805;&#21644;PA&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional layers are a fundamental component of most image-related models. These layers often implement by default a static padding policy (\eg zero padding), to control the scale of the internal representations, and to allow kernel activations centered on the border regions. In this work we identify Padding Aware Neurons (PANs), a type of filter that is found in most (if not all) convolutional models trained with static padding. PANs focus on the characterization and recognition of input border location, introducing a spatial inductive bias into the model (e.g., how close to the input's border a pattern typically is). We propose a method to identify PANs through their activations, and explore their presence in several popular pre-trained models, finding PANs on all models explored, from dozens to hundreds. We discuss and illustrate different types of PANs, their kernels and behaviour. To understand their relevance, we test their impact on model performance, and find padding and PA
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Wave-RNN (wRNN)&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26053;&#34892;&#27874;&#26426;&#21046;&#22914;&#20309;&#26377;&#25928;&#22320;&#32534;&#30721;&#26368;&#36817;&#30340;&#36807;&#21435;&#65292;&#24182;&#22312;&#21512;&#25104;&#35760;&#24518;&#20219;&#21153;&#20013;&#27604;&#27874;&#21160;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.08045</link><description>&lt;p&gt;
&#26053;&#34892;&#27874;&#32534;&#30721;&#26368;&#36817;&#30340;&#36807;&#21435;&#24182;&#22686;&#24378;&#24207;&#21015;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Traveling Waves Encode the Recent Past and Enhance Sequence Learning. (arXiv:2309.08045v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Wave-RNN (wRNN)&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26053;&#34892;&#27874;&#26426;&#21046;&#22914;&#20309;&#26377;&#25928;&#22320;&#32534;&#30721;&#26368;&#36817;&#30340;&#36807;&#21435;&#65292;&#24182;&#22312;&#21512;&#25104;&#35760;&#24518;&#20219;&#21153;&#20013;&#27604;&#27874;&#21160;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#27963;&#21160;&#30340;&#26053;&#34892;&#27874;&#29616;&#35937;&#22312;&#22823;&#33041;&#30340;&#19981;&#21516;&#21306;&#22495;&#21644;&#23610;&#24230;&#19978;&#37117;&#26377;&#25152;&#35266;&#23519;&#21040;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#35282;&#33394;&#19978;&#30340;&#20855;&#20307;&#20316;&#29992;&#20173;&#23384;&#22312;&#20105;&#35758;&#12290;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30340;&#20551;&#35774;&#35748;&#20026;&#65292;&#30382;&#36136;&#23618;&#21487;&#20197;&#20687;&#27874;&#21160;&#22330;&#19968;&#26679;&#65292;&#36890;&#36807;&#27839;&#30528;&#30382;&#36136;&#34920;&#38754;&#20256;&#25773;&#30340;&#27874;&#21160;&#26469;&#23384;&#20648;&#39034;&#24207;&#21050;&#28608;&#30340;&#30701;&#26399;&#35760;&#24518;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#19968;&#20010;&#31616;&#21333;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#33021;&#22815;&#23637;&#29616;&#20986;&#36825;&#31181;&#27874;&#21160;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20010;&#24819;&#27861;&#30340;&#35745;&#31639;&#24847;&#20041;&#19968;&#30452;&#26159;&#20551;&#35774;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Wave-RNN (wRNN)&#65292;&#24182;&#23637;&#31034;&#20102;&#36830;&#36890;&#24615;&#32422;&#26463;&#21644;&#21021;&#22987;&#21270;&#22312;&#27874;&#21160;&#21160;&#21147;&#23398;&#20986;&#29616;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#20102;&#36825;&#26679;&#30340;&#26550;&#26500;&#30340;&#30830;&#36890;&#36807;&#19968;&#31995;&#21015;&#21512;&#25104;&#35760;&#24518;&#20219;&#21153;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#26368;&#36817;&#30340;&#36807;&#21435;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;wRNN&#27604;&#27874;&#21160;&#27169;&#22411;&#23398;&#20064;&#26356;&#24555;&#12289;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traveling waves of neural activity have been observed throughout the brain at a diversity of regions and scales; however, their precise computational role is still debated. One physically grounded hypothesis suggests that the cortical sheet may act like a wave-field capable of storing a short-term memory of sequential stimuli through induced waves traveling across the cortical surface. To date, however, the computational implications of this idea have remained hypothetical due to the lack of a simple recurrent neural network architecture capable of exhibiting such waves. In this work, we introduce a model to fill this gap, which we denote the Wave-RNN (wRNN), and demonstrate how both connectivity constraints and initialization play a crucial role in the emergence of wave-like dynamics. We then empirically show how such an architecture indeed efficiently encodes the recent past through a suite of synthetic memory tasks where wRNNs learn faster and perform significantly better than wave-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Flickr&#22270;&#20687;&#21644;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#23558;&#20247;&#21253;&#34903;&#26223;&#22270;&#20687;&#24212;&#29992;&#20110;&#24314;&#31569;&#23646;&#24615;&#26144;&#23556;&#30340;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#27492;&#20219;&#21153;&#30456;&#20851;&#30340;&#37325;&#22823;&#38382;&#39064;&#65292;&#21253;&#25324;&#23567;&#25991;&#26412;&#21306;&#22495;&#12289;&#32570;&#20047;&#30495;&#23454;&#26631;&#31614;&#21644;&#24314;&#31569;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08042</link><description>&lt;p&gt;
&#20351;&#29992;&#20247;&#21253;&#22270;&#20687;&#36827;&#34892;&#22823;&#35268;&#27169;&#24314;&#31569;&#23646;&#24615;&#26144;&#23556;&#65306;&#22522;&#20110;Flickr&#30340;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#21644;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Towards Large-scale Building Attribute Mapping using Crowdsourced Images: Scene Text Recognition on Flickr and Problems to be Solved. (arXiv:2309.08042v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Flickr&#22270;&#20687;&#21644;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#23558;&#20247;&#21253;&#34903;&#26223;&#22270;&#20687;&#24212;&#29992;&#20110;&#24314;&#31569;&#23646;&#24615;&#26144;&#23556;&#30340;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#27492;&#20219;&#21153;&#30456;&#20851;&#30340;&#37325;&#22823;&#38382;&#39064;&#65292;&#21253;&#25324;&#23567;&#25991;&#26412;&#21306;&#22495;&#12289;&#32570;&#20047;&#30495;&#23454;&#26631;&#31614;&#21644;&#24314;&#31569;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#21253;&#24179;&#21488;&#25552;&#20379;&#20102;&#22823;&#37327;&#21253;&#21547;&#26377;&#20215;&#20540;&#24314;&#31569;&#20449;&#24687;&#30340;&#34903;&#26223;&#22270;&#20687;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#24212;&#29992;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#65288;STR&#65289;&#36827;&#34892;&#24314;&#31569;&#23646;&#24615;&#26144;&#23556;&#26102;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;Flickr&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#30740;&#31350;&#20102;&#24314;&#31569;&#22806;&#22681;&#19978;&#30340;&#25991;&#26412;&#12290;&#21019;&#24314;&#20102;&#19968;&#20010;&#26575;&#26519;Flickr&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;STR&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#12290;&#23545;STR&#35782;&#21035;&#22270;&#20687;&#30340;&#23376;&#38598;&#36827;&#34892;&#25163;&#21160;&#26816;&#26597;&#65292;&#32467;&#26524;&#26174;&#31034;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;STR&#32467;&#26524;&#19982;&#24314;&#31569;&#21151;&#33021;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#20303;&#23429;&#24314;&#31569;&#19978;&#35782;&#21035;&#21040;&#25991;&#26412;&#20294;&#22312;&#21830;&#19994;&#24314;&#31569;&#19978;&#26410;&#35782;&#21035;&#21040;&#30340;&#24773;&#20917;&#12290;&#36827;&#19968;&#27493;&#35843;&#26597;&#21457;&#29616;&#20102;&#19982;&#36825;&#19968;&#20219;&#21153;&#30456;&#20851;&#30340;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#34903;&#26223;&#22270;&#20687;&#20013;&#30340;&#23567;&#25991;&#26412;&#21306;&#22495;&#65292;&#32570;&#20047;&#30495;&#23454;&#26631;&#31614;&#20197;&#21450;Flickr&#22270;&#20687;&#20013;&#30340;&#24314;&#31569;&#19982;OpenStreetMap&#65288;OSM&#65289;&#20013;&#30340;&#24314;&#31569;&#36275;&#36857;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#24320;&#21457;&#22478;&#24066;&#33539;&#22260;&#30340;&#26144;&#23556;&#65292;&#36229;&#36234;&#22478;&#24066;&#28909;&#28857;&#21306;&#22495;&#65292;
&lt;/p&gt;
&lt;p&gt;
Crowdsourced platforms provide huge amounts of street-view images that contain valuable building information. This work addresses the challenges in applying Scene Text Recognition (STR) in crowdsourced street-view images for building attribute mapping. We use Flickr images, particularly examining texts on building facades. A Berlin Flickr dataset is created, and pre-trained STR models are used for text detection and recognition. Manual checking on a subset of STR-recognized images demonstrates high accuracy. We examined the correlation between STR results and building functions, and analysed instances where texts were recognized on residential buildings but not on commercial ones. Further investigation revealed significant challenges associated with this task, including small text regions in street-view images, the absence of ground truth labels, and mismatches in buildings in Flickr images and building footprints in OpenStreetMap (OSM). To develop city-wide mapping beyond urban hotspo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#31616;&#21270;&#30340;&#38598;&#21512;&#32467;&#26500;BEA&#29992;&#20110;&#38170;&#28857;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26657;&#20934;&#19982;&#38477;&#20302;&#19981;&#30830;&#23450;&#24615;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08036</link><description>&lt;p&gt;
BEA: &#37325;&#26032;&#23457;&#35270;&#20351;&#29992;Budding Ensemble Architecture&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#30446;&#26631;&#26816;&#27979;DNN
&lt;/p&gt;
&lt;p&gt;
BEA: Revisiting anchor-based object detection DNN using Budding Ensemble Architecture. (arXiv:2309.08036v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#31616;&#21270;&#30340;&#38598;&#21512;&#32467;&#26500;BEA&#29992;&#20110;&#38170;&#28857;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26657;&#20934;&#19982;&#38477;&#20302;&#19981;&#30830;&#23450;&#24615;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Budding Ensemble Architecture&#65288;BEA&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#38170;&#28857;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#26032;&#22411;&#31616;&#21270;&#38598;&#21512;&#32467;&#26500;&#12290;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#12290;&#23427;&#20204;&#24212;&#35813;&#25552;&#20379;&#31934;&#30830;&#30340;&#36793;&#30028;&#26694;&#26816;&#27979;&#65292;&#24182;&#26657;&#20934;&#20854;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20551;&#38451;&#24615;&#25509;&#25910;&#21040;&#39640;&#20998;&#25110;&#30495;&#38451;&#24615;&#30001;&#20110;&#20302;&#20998;&#32780;&#34987;&#20002;&#24323;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20570;&#20986;&#38169;&#35823;&#30340;&#20915;&#31574;&#12290;BEA&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;BEA&#30340;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#25913;&#21892;&#20102;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26657;&#20934;&#21644;&#38477;&#20302;&#20102;&#19981;&#30830;&#23450;&#24615;&#35823;&#24046;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21306;&#20998;&#30495;&#38451;&#24615;&#21644;&#20551;&#38451;&#24615;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;BEA&#26041;&#27861;&#21644;&#20854;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;Base-YOLOv3&#21644;SSD&#27169;&#22411;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;BEA&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;Base-YOLOv3&#32467;&#26524;&#20013;&#65292;&#31934;&#24230;&#20998;&#21035;&#25552;&#39640;&#20102;6%&#21644;3.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Budding Ensemble Architecture (BEA), a novel reduced ensemble architecture for anchor-based object detection models. Object detection models are crucial in vision-based tasks, particularly in autonomous systems. They should provide precise bounding box detections while also calibrating their predicted confidence scores, leading to higher-quality uncertainty estimates. However, current models may make erroneous decisions due to false positives receiving high scores or true positives being discarded due to low scores. BEA aims to address these issues. The proposed loss functions in BEA improve the confidence score calibration and lower the uncertainty error, which results in a better distinction of true and false positives and, eventually, higher accuracy of the object detection models. Both Base-YOLOv3 and SSD models were enhanced using the BEA method and its proposed loss functions. The BEA on Base-YOLOv3 trained on the KITTI dataset results in a 6% and 3.7% i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#20256;&#24863;&#22120;&#65292;&#30740;&#31350;&#20102;&#24613;&#24615;&#37202;&#31934;&#25668;&#20837;&#23545;&#39550;&#39542;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#37202;&#39550;&#34892;&#20026;&#26469;&#20943;&#23569;&#37202;&#39550;&#20107;&#25925;&#12290;</title><link>http://arxiv.org/abs/2309.08021</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#37202;&#39550;&#21496;&#26426;&#34892;&#20026;&#21644;&#39550;&#39542;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Vision-based Analysis of Driver Activity and Driving Performance Under the Influence of Alcohol. (arXiv:2309.08021v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08021
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#20256;&#24863;&#22120;&#65292;&#30740;&#31350;&#20102;&#24613;&#24615;&#37202;&#31934;&#25668;&#20837;&#23545;&#39550;&#39542;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#37202;&#39550;&#34892;&#20026;&#26469;&#20943;&#23569;&#37202;&#39550;&#20107;&#25925;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#32422;30%&#30340;&#20132;&#36890;&#20107;&#25925;&#27515;&#20129;&#28041;&#21450;&#37202;&#39550;&#65292;&#22240;&#27492;&#22312;&#32654;&#22269;&#21644;&#20854;&#20182;&#39640;&#37202;&#39550;&#24739;&#30149;&#29575;&#22320;&#21306;&#65292;&#38450;&#27490;&#37202;&#39550;&#23545;&#36710;&#36742;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20027;&#21160;&#20351;&#29992;&#20256;&#24863;&#22120;&#65288;&#35201;&#27714;&#39550;&#39542;&#21592;&#25552;&#20379;&#21628;&#27668;&#26679;&#26412;&#32473;&#36710;&#36742;&#20202;&#22120;&#25110;&#34987;&#35686;&#23519;&#25318;&#19979;&#26102;&#65289;&#65292;&#21487;&#20197;&#30417;&#27979;&#39550;&#39542;&#33021;&#21147;&#21463;&#25439;&#65292;&#20294;&#20351;&#29992;&#19968;&#31181;&#26356;&#34987;&#21160;&#19988;&#31283;&#20581;&#30340;&#24863;&#30693;&#26426;&#21046;&#21487;&#33021;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#24212;&#29992;&#26234;&#33021;&#31995;&#32479;&#65292;&#20174;&#32780;&#20943;&#23569;&#37202;&#39550;&#20107;&#25925;&#12290;&#36825;&#21487;&#20197;&#24110;&#21161;&#22312;&#39550;&#39542;&#21069;&#25110;&#39550;&#39542;&#36807;&#31243;&#26089;&#26399;&#65288;&#22312;&#20107;&#25925;&#25110;&#34987;&#25191;&#27861;&#37096;&#38376;&#21457;&#29616;&#20043;&#21069;&#65289;&#35782;&#21035;&#20986;&#21463;&#25439;&#39550;&#39542;&#21592;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#39033;&#20351;&#29992;&#35270;&#35273;&#12289;&#28909;&#24863;&#12289;&#38899;&#39057;&#21644;&#21270;&#23398;&#20256;&#24863;&#22120;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;&#65292;&#20197;(1)&#22312;&#39550;&#39542;&#27169;&#25311;&#22120;&#20013;&#30740;&#31350;&#24613;&#24615;&#37202;&#31934;&#25668;&#20837;&#23545;&#39550;&#39542;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;(2)&#35782;&#21035;&#37202;&#39550;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
About 30% of all traffic crash fatalities in the United States involve drunk drivers, making the prevention of drunk driving paramount to vehicle safety in the US and other locations which have a high prevalence of driving while under the influence of alcohol. Driving impairment can be monitored through active use of sensors (when drivers are asked to engage in providing breath samples to a vehicle instrument or when pulled over by a police officer), but a more passive and robust mechanism of sensing may allow for wider adoption and benefit of intelligent systems that reduce drunk driving accidents. This could assist in identifying impaired drivers before they drive, or early in the driving process (before a crash or detection by law enforcement). In this research, we introduce a study which adopts a multi-modal ensemble of visual, thermal, audio, and chemical sensors to (1) examine the impact of acute alcohol administration on driving performance in a driving simulator, and (2) identi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#20116;&#20010;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25928;&#26524;&#65292;&#20026;&#35299;&#38145;&#20020;&#24202;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2309.08008</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#25552;&#31034;&#31574;&#30053;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing. (arXiv:2309.08008v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08008
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#20116;&#20010;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25928;&#26524;&#65292;&#20026;&#35299;&#38145;&#20020;&#24202;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#25110;&#26114;&#36149;&#30340;&#39046;&#22495;&#65292;&#22914;&#20020;&#24202;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#35201;&#35299;&#38145;&#36825;&#20123;LLMs&#20013;&#38544;&#34255;&#30340;&#20020;&#24202;&#30693;&#35782;&#65292;&#25105;&#20204;&#38656;&#35201;&#35774;&#35745;&#26377;&#25928;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#24341;&#23548;&#23427;&#20204;&#22312;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#29305;&#23450;&#30340;&#20020;&#24202;NLP&#20219;&#21153;&#12290;&#36825;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#38376;&#38656;&#35201;&#20102;&#35299;&#19981;&#21516;LLMs&#21644;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#30340;&#33402;&#26415;&#21644;&#31185;&#23398;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#32780;&#31995;&#32479;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#38024;&#23545;&#20116;&#20010;&#20020;&#24202;NLP&#20219;&#21153;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#30340;&#35780;&#20272;&#65306;&#20020;&#24202;&#24847;&#20041;&#28040;&#27495;&#12289;&#29983;&#29289;&#21307;&#23398;&#35777;&#25454;&#25552;&#21462;&#12289;&#20849;&#25351;&#28040;&#35299;&#12289;&#33647;&#29289;&#29366;&#24577;&#25552;&#21462;&#21644;&#33647;&#29289;&#23646;&#24615;&#25552;&#21462;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#36817;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21253;&#25324;&#31616;&#21333;&#21069;&#32512;&#12289;&#31616;&#21333;&#22635;&#31354;&#12289;&#24605;&#32500;&#38142;&#21644;&#39044;&#26399;&#25552;&#31034;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#30740;&#31350;&#27969;&#22495;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#23792;&#20540;&#27169;&#24335;&#12290;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#24102;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#21644;&#33258;&#21160;&#36229;&#21442;&#25968;&#20248;&#21270;&#26426;&#21046;&#65292;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#26631;&#35760;&#25968;&#25454;&#21644;&#36873;&#25321;&#21512;&#36866;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07992</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#20869;&#30740;&#31350;&#27969;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#23792;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
An Automated Machine Learning Approach for Detecting Anomalous Peak Patterns in Time Series Data from a Research Watershed in the Northeastern United States Critical Zone. (arXiv:2309.07992v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#30740;&#31350;&#27969;&#22495;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#23792;&#20540;&#27169;&#24335;&#12290;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#24102;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#21644;&#33258;&#21160;&#36229;&#21442;&#25968;&#20248;&#21270;&#26426;&#21046;&#65292;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#26631;&#35760;&#25968;&#25454;&#21644;&#36873;&#25321;&#21512;&#36866;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#27700;&#25991;&#23398;&#23478;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#30740;&#31350;&#27969;&#22495;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#35782;&#21035;&#23792;&#20540;&#27169;&#24335;&#24322;&#24120;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#20256;&#24863;&#22120;&#25925;&#38556;&#25110;&#33258;&#28982;&#29616;&#35937;&#24341;&#36215;&#30340;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#23384;&#22312;&#25361;&#25112;&#65292;&#20363;&#22914;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#22522;&#20934;&#21644;&#36873;&#25321;&#26368;&#36866;&#21512;&#32473;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23558;&#21512;&#25104;&#30340;&#23792;&#20540;&#27169;&#24335;&#27880;&#20837;&#21040;&#21512;&#25104;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#29983;&#25104;&#24102;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#33258;&#21160;&#21270;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26426;&#21046;&#12290;&#35813;&#26426;&#21046;&#20174;&#20116;&#31181;&#36873;&#25321;&#30340;&#27169;&#22411;&#20013;&#29983;&#25104;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#26550;&#26500;&#21644;&#35757;&#32451;&#21442;&#25968;&#30340;&#20248;&#21270;&#27169;&#22411;&#23454;&#20363;&#65292;&#21363;&#26102;&#24207;&#21367;&#31215;&#32593;&#32476;&#65288;
&lt;/p&gt;
&lt;p&gt;
This paper presents an automated machine learning framework designed to assist hydrologists in detecting anomalies in time series data generated by sensors in a research watershed in the northeastern United States critical zone. The framework specifically focuses on identifying peak-pattern anomalies, which may arise from sensor malfunctions or natural phenomena. However, the use of classification methods for anomaly detection poses challenges, such as the requirement for labeled data as ground truth and the selection of the most suitable deep learning model for the given task and dataset. To address these challenges, our framework generates labeled datasets by injecting synthetic peak patterns into synthetically generated time series data and incorporates an automated hyperparameter optimization mechanism. This mechanism generates an optimized model instance with the best architectural and training parameters from a pool of five selected models, namely Temporal Convolutional Network (
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;2D&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20986;3D&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#35813;&#20449;&#24687;&#36827;&#34892;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#36890;&#36807;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#65292;&#26377;&#25928;&#35299;&#20915;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#35270;&#22270;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07986</link><description>&lt;p&gt;
&#35266;&#28857;&#25991;&#26412;&#20498;&#32622;&#65306;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#37322;&#25918;&#26032;&#39062;&#30340;&#35270;&#22270;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Viewpoint Textual Inversion: Unleashing Novel View Synthesis with Pretrained 2D Diffusion Models. (arXiv:2309.07986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;2D&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20986;3D&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#35813;&#20449;&#24687;&#36827;&#34892;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#36890;&#36807;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#65292;&#26377;&#25928;&#35299;&#20915;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#35270;&#22270;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29702;&#35299;&#23545;&#35937;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#20165;&#36890;&#36807;2D&#30417;&#30563;&#26469;&#34920;&#31034;&#19990;&#30028;&#30340;&#30495;&#23454;3D&#32467;&#26500;&#65311;&#25105;&#20204;&#35777;&#26126;&#65292;&#26159;&#30340;&#65292;3D&#30693;&#35782;&#34987;&#32534;&#30721;&#22312;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65289;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#65292;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#31070;&#32463;&#26144;&#23556;&#22120;&#65292;&#29992;&#20110;&#33719;&#21462;&#30456;&#26426;&#35270;&#28857;&#21442;&#25968;&#24182;&#39044;&#27979;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#21521;&#37327;&#65307;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#28508;&#22312;&#21521;&#37327;&#26469;&#35843;&#25972;&#25193;&#25955;&#29983;&#25104;&#36807;&#31243;&#65292;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#30456;&#26426;&#35270;&#28857;&#30340;&#22270;&#20687;&#12290;ViewNeTI&#33258;&#28982;&#35299;&#20915;&#20102;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#65288;NVS&#65289;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#34987;&#20923;&#32467;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#24456;&#23569;&#30340;&#36755;&#20837;&#35270;&#22270;&#26469;&#35299;&#20915;NVS&#38382;&#39064;&#65307;&#25105;&#20204;&#29978;&#33267;&#21487;&#20197;&#36827;&#34892;&#21333;&#35270;&#22270;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#21333;&#35270;&#22270;NVS&#39044;&#27979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models understand spatial relationship between objects, but do they represent the true 3D structure of the world from only 2D supervision? We demonstrate that yes, 3D knowledge is encoded in 2D image diffusion models like Stable Diffusion, and we show that this structure can be exploited for 3D vision tasks. Our method, Viewpoint Neural Textual Inversion (ViewNeTI), controls the 3D viewpoint of objects in generated images from frozen diffusion models. We train a small neural mapper to take camera viewpoint parameters and predict text encoder latents; the latents then condition the diffusion generation process to produce images with the desired camera viewpoint.  ViewNeTI naturally addresses Novel View Synthesis (NVS). By leveraging the frozen diffusion model as a prior, we can solve NVS with very few input views; we can even do single-view novel view synthesis. Our single-view NVS predictions have good semantic details and photorealism compared to prior methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#20307;&#39564;&#26234;&#33021;&#20307;&#38598;&#25104;&#30340;&#26032;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#26426;&#22120;&#25512;&#29702;&#12290;&#35813;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#21253;&#25324;&#27169;&#26495;&#21270;&#30340;&#25991;&#26412;&#26597;&#35810;&#21644;&#31572;&#26696;&#65292;&#24182;&#19982;&#32534;&#30721;&#20026;&#25968;&#25454;&#24211;&#30340;&#19990;&#30028;&#29366;&#24577;&#30456;&#21305;&#37197;&#12290;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#24403;&#21069;&#27169;&#22411;&#21487;&#20197;&#22238;&#31572;&#19968;&#20123;&#20851;&#20110;&#19990;&#30028;&#29366;&#24577;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#20854;&#20182;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.07974</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#29702;&#20307;&#39564;&#26234;&#33021;&#20307;&#30340;&#25968;&#25454;&#28304;
&lt;/p&gt;
&lt;p&gt;
A Data Source for Reasoning Embodied Agents. (arXiv:2309.07974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#20307;&#39564;&#26234;&#33021;&#20307;&#38598;&#25104;&#30340;&#26032;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#26426;&#22120;&#25512;&#29702;&#12290;&#35813;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#21253;&#25324;&#27169;&#26495;&#21270;&#30340;&#25991;&#26412;&#26597;&#35810;&#21644;&#31572;&#26696;&#65292;&#24182;&#19982;&#32534;&#30721;&#20026;&#25968;&#25454;&#24211;&#30340;&#19990;&#30028;&#29366;&#24577;&#30456;&#21305;&#37197;&#12290;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#24403;&#21069;&#27169;&#22411;&#21487;&#20197;&#22238;&#31572;&#19968;&#20123;&#20851;&#20110;&#19990;&#30028;&#29366;&#24577;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#20854;&#20182;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36825;&#24471;&#30410;&#20110;&#26032;&#39062;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#21327;&#35758;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#24494;&#35843;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#21160;&#36825;&#20123;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19982;&#20307;&#39564;&#26234;&#33021;&#20307;&#38598;&#25104;&#30340;&#26032;&#25968;&#25454;&#29983;&#25104;&#22120;&#29992;&#20110;&#26426;&#22120;&#25512;&#29702;&#12290;&#29983;&#25104;&#30340;&#25968;&#25454;&#21253;&#25324;&#27169;&#26495;&#21270;&#30340;&#25991;&#26412;&#26597;&#35810;&#21644;&#31572;&#26696;&#65292;&#19982;&#32534;&#30721;&#20026;&#25968;&#25454;&#24211;&#30340;&#19990;&#30028;&#29366;&#24577;&#30456;&#21305;&#37197;&#12290;&#36825;&#20123;&#19990;&#30028;&#29366;&#24577;&#26159;&#19990;&#30028;&#21160;&#24577;&#21644;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#31181;&#22522;&#20934;&#27169;&#22411;&#22312;&#35757;&#32451;&#38598;&#23454;&#20363;&#21270;&#19978;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#22522;&#20934;&#27169;&#22411;&#21253;&#25324;&#22312;&#25968;&#25454;&#24211;&#30340;&#25991;&#26412;&#26684;&#24335;&#21270;&#34920;&#31034;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#22312;&#30693;&#35782;&#22270;&#34920;&#31034;&#30340;&#22270;&#32467;&#26500;Transformer&#19978;&#25805;&#20316;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22238;&#31572;&#19968;&#20123;&#20851;&#20110;&#19990;&#30028;&#29366;&#24577;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#20854;&#20182;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20123;&#32467;&#26524;&#26263;&#31034;&#20102;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in using machine learning models for reasoning tasks has been driven by novel model architectures, large-scale pre-training protocols, and dedicated reasoning datasets for fine-tuning. In this work, to further pursue these advances, we introduce a new data generator for machine reasoning that integrates with an embodied agent. The generated data consists of templated text queries and answers, matched with world-states encoded into a database. The world-states are a result of both world dynamics and the actions of the agent. We show the results of several baseline models on instantiations of train sets. These include pre-trained language models fine-tuned on a text-formatted representation of the database, and graph-structured Transformers operating on a knowledge-graph representation of the database. We find that these models can answer some questions about the world-state, but struggle with others. These results hint at new research directions in designing neural reaso
&lt;/p&gt;</description></item><item><title>TiBGL&#26159;&#19968;&#31181;&#27169;&#26495;&#24341;&#23548;&#30340;&#33041;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21151;&#33021;&#24615;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;&#12290;&#23427;&#20855;&#26377;&#21028;&#21035;&#21644;&#21487;&#35299;&#37322;&#33021;&#21147;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#21151;&#33021;&#36830;&#25509;&#25968;&#25454;&#30340;&#26377;&#29992;&#29305;&#24449;&#26469;&#25913;&#36827;&#31070;&#32463;&#30142;&#30149;&#30340;&#35786;&#26029;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07947</link><description>&lt;p&gt;
TiBGL: &#27169;&#26495;&#24341;&#23548;&#30340;&#33041;&#22270;&#23398;&#20064;&#29992;&#20110;&#21151;&#33021;&#24615;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TiBGL: Template-induced Brain Graph Learning for Functional Neuroimaging Analysis. (arXiv:2309.07947v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07947
&lt;/p&gt;
&lt;p&gt;
TiBGL&#26159;&#19968;&#31181;&#27169;&#26495;&#24341;&#23548;&#30340;&#33041;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21151;&#33021;&#24615;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;&#12290;&#23427;&#20855;&#26377;&#21028;&#21035;&#21644;&#21487;&#35299;&#37322;&#33021;&#21147;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#21151;&#33021;&#36830;&#25509;&#25968;&#25454;&#30340;&#26377;&#29992;&#29305;&#24449;&#26469;&#25913;&#36827;&#31070;&#32463;&#30142;&#30149;&#30340;&#35786;&#26029;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#24050;&#25104;&#20026;&#30740;&#31350;&#20154;&#31867;&#22823;&#33041;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#30456;&#20851;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#33041;&#30340;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#30142;&#30149;&#35786;&#26029;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#21151;&#33021;&#24615;&#31070;&#32463;&#24433;&#20687;&#39046;&#22495;&#20173;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#38480;&#21046;&#30528;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#21151;&#33021;&#36830;&#25509;&#25968;&#25454;&#20013;&#23384;&#22312;&#22823;&#37327;&#22122;&#38899;&#21644;&#20887;&#20313;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#33041;&#32593;&#32476;&#27169;&#22411;&#24448;&#24448;&#20559;&#21521;&#20110;&#20998;&#31867;&#24615;&#33021;&#25110;&#23545;&#23398;&#20064;&#27169;&#22411;&#32972;&#21518;&#30340;&#31070;&#32463;&#31185;&#23398;&#21457;&#29616;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33041;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;&#27169;&#26495;&#24341;&#23548;&#30340;&#33041;&#22270;&#23398;&#20064;&#65288;TiBGL&#65289;&#65292;&#20855;&#26377;&#21028;&#21035;&#21644;&#21487;&#35299;&#37322;&#33021;&#21147;&#12290;&#21463;&#21040;&#19982;&#21151;&#33021;&#36830;&#25509;&#30456;&#20851;&#30340;&#21307;&#23398;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;TiBGL&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#27169;&#26495;&#24341;&#23548;&#26041;&#27861;&#26469;&#23398;&#20064;&#21151;&#33021;&#36830;&#25509;&#25968;&#25454;&#30340;&#26377;&#29992;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, functional magnetic resonance imaging has emerged as a powerful tool for investigating the human brain's functional connectivity networks. Related studies demonstrate that functional connectivity networks in the human brain can help to improve the efficiency of diagnosing neurological disorders. However, there still exist two challenges that limit the progress of functional neuroimaging. Firstly, there exists an abundance of noise and redundant information in functional connectivity data, resulting in poor performance. Secondly, existing brain network models have tended to prioritize either classification performance or the interpretation of neuroscience findings behind the learned models. To deal with these challenges, this paper proposes a novel brain graph learning framework called Template-induced Brain Graph Learning (TiBGL), which has both discriminative and interpretable abilities. Motivated by the related medical findings on functional connectivites, TiBGL prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#37319;&#26679;&#26041;&#26696; (ESS)&#65292;&#29992;&#20110;&#25513;&#30721;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#24314;&#27169;&#12290;&#35813;&#26041;&#26696;&#33021;&#22815;&#30830;&#20445;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#24182;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#12289;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#12290;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#29992;&#20110;&#37319;&#26679;&#26631;&#35760;&#38598;&#65292;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#29992;&#20110;&#25513;&#30422;&#19981;&#30495;&#23454;&#30340;&#26631;&#35760;&#24182;&#37325;&#24314;&#34987;&#25513;&#30422;&#30340;&#26631;&#35760;&#65292;&#20197;&#25552;&#39640;&#37319;&#26679;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07945</link><description>&lt;p&gt;
&#22686;&#24378;&#37319;&#26679;&#26041;&#26696;&#30340;&#25513;&#30721;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Masked Generative Modeling with Enhanced Sampling Scheme. (arXiv:2309.07945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#37319;&#26679;&#26041;&#26696; (ESS)&#65292;&#29992;&#20110;&#25513;&#30721;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#24314;&#27169;&#12290;&#35813;&#26041;&#26696;&#33021;&#22815;&#30830;&#20445;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#24182;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#12289;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#12290;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#29992;&#20110;&#37319;&#26679;&#26631;&#35760;&#38598;&#65292;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#29992;&#20110;&#25513;&#30422;&#19981;&#30495;&#23454;&#30340;&#26631;&#35760;&#24182;&#37325;&#24314;&#34987;&#25513;&#30422;&#30340;&#26631;&#35760;&#65292;&#20197;&#25552;&#39640;&#37319;&#26679;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25513;&#30721;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#24314;&#27169;&#30340;&#26032;&#22411;&#37319;&#26679;&#26041;&#26696;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;TimeVQVAE&#12289;MaskGIT&#21644;Token-Critic&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#24378;&#37319;&#26679;&#26041;&#26696; (ESS) &#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;ESS&#26126;&#30830;&#30830;&#20445;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#12289;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#21644;&#20851;&#38190;&#37325;&#37319;&#26679;&#12290;ESS&#39318;&#20808;&#20351;&#29992;MaskGIT&#20013;&#25552;&#20986;&#30340;&#31616;&#21333;&#36845;&#20195;&#35299;&#30721;&#26469;&#37319;&#26679;&#19968;&#20010;&#26631;&#35760;&#38598;&#65292;&#20197;&#30830;&#20445;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#21518;&#65292;&#26631;&#35760;&#38598;&#32463;&#36807;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#65292;&#25513;&#30422;&#23548;&#33268;&#19981;&#30495;&#23454;&#26679;&#26412;&#30340;&#26631;&#35760;&#12290;&#22312;&#27492;&#20043;&#21518;&#65292;&#20851;&#38190;&#37325;&#37319;&#26679;&#37325;&#24314;&#34987;&#25513;&#30422;&#30340;&#26631;&#35760;&#65292;&#30452;&#21040;&#36798;&#21040;&#26368;&#32456;&#37319;&#26679;&#27493;&#39588;&#20197;&#30830;&#20445;&#39640;&#24230;&#20445;&#30495;&#24230;&#12290;&#20851;&#38190;&#37325;&#37319;&#26679;&#20351;&#29992;&#26469;&#33258;&#33258;&#25105;Token-Critic&#33719;&#24471;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26356;&#22909;&#22320;&#34913;&#37327;&#37319;&#26679;&#26631;&#35760;&#30340;&#30495;&#23454;&#24615;&#65292;&#32780;&#20851;&#38190;&#21453;&#21521;&#37319;&#26679;&#20351;&#29992;&#37327;&#21270;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel sampling scheme for masked non-autoregressive generative modeling. We identify the limitations of TimeVQVAE, MaskGIT, and Token-Critic in their sampling processes, and propose Enhanced Sampling Scheme (ESS) to overcome these limitations. ESS explicitly ensures both sample diversity and fidelity, and consists of three stages: Naive Iterative Decoding, Critical Reverse Sampling, and Critical Resampling. ESS starts by sampling a token set using the naive iterative decoding as proposed in MaskGIT, ensuring sample diversity. Then, the token set undergoes the critical reverse sampling, masking tokens leading to unrealistic samples. After that, critical resampling reconstructs masked tokens until the final sampling step is reached to ensure high fidelity. Critical resampling uses confidence scores obtained from a self-Token-Critic to better measure the realism of sampled tokens, while critical reverse sampling uses the structure of the quantized latent vector space
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#26085;&#24535;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#26377;&#38480;&#65292;&#22238;&#22797;&#32570;&#20047;&#19968;&#33268;&#24615;&#19988;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07938</link><description>&lt;p&gt;
ChatGPT&#22312;&#26085;&#24535;&#25968;&#25454;&#19978;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Assessment of ChatGPT on Log Data. (arXiv:2309.07938v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07938
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#26085;&#24535;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#26377;&#38480;&#65292;&#22238;&#22797;&#32570;&#20047;&#19968;&#33268;&#24615;&#19988;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#27604;&#22914;ChatGPT&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#12290;&#35768;&#22810;&#35770;&#25991;&#24050;&#32463;&#25253;&#36947;&#20102;ChatGPT&#22312;&#32534;&#20889;&#20195;&#30721;&#12289;&#25688;&#35201;&#12289;&#25991;&#26412;&#29983;&#25104;&#31561;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#30340;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26085;&#24535;&#22788;&#29702;&#30340;ChatGPT&#30340;&#24403;&#21069;&#29366;&#24577;&#30340;&#20998;&#26512;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#22823;&#35268;&#27169;&#36719;&#20214;&#31995;&#32479;&#29983;&#25104;&#30340;&#26085;&#24535;&#22797;&#26434;&#19988;&#38590;&#20197;&#29702;&#35299;&#12290;&#23613;&#31649;&#22797;&#26434;&#65292;&#20294;&#23427;&#20204;&#20026;&#20027;&#39064;&#19987;&#23478;&#25552;&#20379;&#20102;&#29702;&#35299;&#31995;&#32479;&#29366;&#24577;&#21644;&#35786;&#26029;&#31995;&#32479;&#38382;&#39064;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#22312;&#26085;&#24535;&#25968;&#25454;&#19978;&#25191;&#34892;&#20960;&#20010;&#26377;&#36259;&#20219;&#21153;&#30340;&#24403;&#21069;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#35797;&#22270;&#25214;&#20986;&#20854;&#20027;&#35201;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#21069;&#29256;&#26412;&#30340;ChatGPT&#22312;&#26085;&#24535;&#22788;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#26377;&#38480;&#65292;&#22238;&#22797;&#32570;&#20047;&#19968;&#33268;&#24615;&#19988;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#20851;&#20110;&#25913;&#36827;ChatGPT&#22312;&#26085;&#24535;&#22788;&#29702;&#20013;&#30340;&#35266;&#28857;&#36827;&#34892;&#20102;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent development of large language models (LLMs), such as ChatGPT has been widely applied to a wide range of software engineering tasks. Many papers have reported their analysis on the potential advantages and limitations of ChatGPT for writing code, summarization, text generation, etc. However, the analysis of the current state of ChatGPT for log processing has received little attention. Logs generated by large-scale software systems are complex and hard to understand. Despite their complexity, they provide crucial information for subject matter experts to understand the system status and diagnose problems of the systems. In this paper, we investigate the current capabilities of ChatGPT to perform several interesting tasks on log data, while also trying to identify its main shortcomings. Our findings show that the performance of the current version of ChatGPT for log processing is limited, with a lack of consistency in responses and scalability issues. We also outline our views on h
&lt;/p&gt;</description></item><item><title>Landscape-Sketch-Step&#26159;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#31105;&#27490;&#30340;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07936</link><description>&lt;p&gt;
Landscape-Sketch-Step: &#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#35299;&#20915;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate Optimization Problems. (arXiv:2309.07936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07936
&lt;/p&gt;
&lt;p&gt;
Landscape-Sketch-Step&#26159;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#31105;&#27490;&#30340;&#20195;&#29702;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25104;&#26412;&#20989;&#25968;&#30340;&#35780;&#20272;&#38750;&#24120;&#26114;&#36149;&#12289;&#19981;&#21487;&#35775;&#38382;&#25110;&#29978;&#33267;&#31105;&#27490;&#30340;&#22330;&#26223;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;Landscape-Sketch-Step&#65288;LSS&#65289;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#38543;&#26426;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20381;&#36182;&#20110;&#20808;&#21069;&#37319;&#26679;&#28857;&#30340;&#21382;&#21490;&#20449;&#24687;&#65292;&#20197;&#26126;&#26234;&#22320;&#36873;&#25321;&#24212;&#35780;&#20272;&#25104;&#26412;&#20989;&#25968;&#30340;&#21442;&#25968;&#20540;&#12290;&#19982;&#22797;&#21046;&#20132;&#25442;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#25152;&#38656;&#30340;&#25104;&#26412;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#19982;&#27169;&#25311;&#36864;&#28779;&#26041;&#27861;&#30456;&#24403;&#65292;&#36825;&#22312;&#39640;&#36890;&#37327;&#35745;&#31639;&#25110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#31561;&#29615;&#22659;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#35780;&#20272;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#35201;&#20040;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#25165;&#33021;&#23436;&#25104;&#12290;&#35813;&#26041;&#27861;&#19982;&#26631;&#20934;&#30340;&#20195;&#29702;&#20248;&#21270;&#25216;&#26415;&#20063;&#19981;&#21516;&#65292;&#22240;&#20026;&#23427;&#19981;&#26500;&#24314;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new heuristics for global optimization in scenarios where extensive evaluations of the cost function are expensive, inaccessible, or even prohibitive. The method, which we call Landscape-Sketch-and-Step (LSS), combines Machine Learning, Stochastic Optimization, and Reinforcement Learning techniques, relying on historical information from previously sampled points to make judicious choices of parameter values where the cost function should be evaluated at. Unlike optimization by Replica Exchange Monte Carlo methods, the number of evaluations of the cost function required in this approach is comparable to that used by Simulated Annealing, quality that is especially important in contexts like high-throughput computing or high-performance computing tasks, where evaluations are either computationally expensive or take a long time to be performed. The method also differs from standard Surrogate Optimization techniques, for it does not construct a surrogate model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Racing Control Variable Genetic Programming (Racing-CVGP) &#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#35745;&#21010;&#26469;&#21152;&#36895;&#31526;&#21495;&#22238;&#24402;&#36807;&#31243;&#65292;&#24182;&#20811;&#26381;&#20102;&#22266;&#23450;&#23454;&#39564;&#35745;&#21010;&#36873;&#25321;&#19981;&#20339;&#23548;&#33268;&#21457;&#29616;&#36807;&#31243;&#24310;&#36831;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.07934</link><description>&lt;p&gt;
&#20351;&#29992;&#31454;&#36895;&#25511;&#21046;&#21464;&#37327;&#36951;&#20256;&#32534;&#31243;&#36827;&#34892;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Racing Control Variable Genetic Programming for Symbolic Regression. (arXiv:2309.07934v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07934
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Racing Control Variable Genetic Programming (Racing-CVGP) &#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#35745;&#21010;&#26469;&#21152;&#36895;&#31526;&#21495;&#22238;&#24402;&#36807;&#31243;&#65292;&#24182;&#20811;&#26381;&#20102;&#22266;&#23450;&#23454;&#39564;&#35745;&#21010;&#36873;&#25321;&#19981;&#20339;&#23548;&#33268;&#21457;&#29616;&#36807;&#31243;&#24310;&#36831;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#23427;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#12290;&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#12289;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#25110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27969;&#34892;&#26041;&#27861;&#21487;&#20197;&#20174;&#22266;&#23450;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31526;&#21495;&#22238;&#24402;&#12290;&#23588;&#20854;&#26159;&#22312;&#23398;&#20064;&#28041;&#21450;&#22810;&#20010;&#21464;&#37327;&#30340;&#22797;&#26434;&#26041;&#31243;&#26102;&#65292;&#23427;&#20204;&#38656;&#35201;&#28023;&#37327;&#30340;&#25968;&#25454;&#38598;&#21644;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#25511;&#21046;&#21464;&#37327;&#36951;&#20256;&#32534;&#31243;&#65288;CVGP&#65289;&#65292;&#23427;&#36890;&#36807;&#20174;&#35774;&#35745;&#30340;&#25511;&#21046;&#21464;&#37327;&#23454;&#39564;&#20013;&#21457;&#29616;&#26041;&#31243;&#26469;&#21152;&#36895;&#22238;&#24402;&#36807;&#31243;&#12290;&#20294;&#26159;&#65292;&#22312;CVGP&#20013;&#23454;&#39564;&#38598;&#26159;&#20808;&#39564;&#22266;&#23450;&#30340;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23454;&#39564;&#35745;&#21010;&#30340;&#27425;&#20248;&#36873;&#25321;&#20250;&#26174;&#33879;&#24310;&#36831;&#21457;&#29616;&#36807;&#31243;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31454;&#36895;&#25511;&#21046;&#21464;&#37327;&#36951;&#20256;&#32534;&#31243;&#65288;Racing-CVGP&#65289;&#65292;&#23427;&#21516;&#26102;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#35745;&#21010;&#12290;&#31867;&#20284;&#20110;&#36873;&#25321;&#22909;&#30340;&#31526;&#21495;&#26041;&#31243;&#30340;&#36873;&#25321;&#26041;&#26696;&#34987;&#29992;&#20110;&#36873;&#25321;&#23454;&#39564;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression, as one of the most crucial tasks in AI for science, discovers governing equations from experimental data. Popular approaches based on genetic programming, Monte Carlo tree search, or deep reinforcement learning learn symbolic regression from a fixed dataset. They require massive datasets and long training time especially when learning complex equations involving many variables. Recently, Control Variable Genetic Programming (CVGP) has been introduced which accelerates the regression process by discovering equations from designed control variable experiments. However, the set of experiments is fixed a-priori in CVGP and we observe that sub-optimal selection of experiment schedules delay the discovery process significantly. To overcome this limitation, we propose Racing Control Variable Genetic Programming (Racing-CVGP), which carries out multiple experiment schedules simultaneously. A selection scheme similar to that used in selecting good symbolic equations in the 
&lt;/p&gt;</description></item><item><title>"&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;"&#25351;&#30340;&#26159;&#33021;&#22815;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#29983;&#25104;&#26032;&#39062;&#26377;&#24847;&#20041;&#20869;&#23481;&#30340;&#35745;&#31639;&#25216;&#26415;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#38899;&#39057;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#30340;&#27010;&#24565;&#65292;&#24182;&#20171;&#32461;&#20102;&#27169;&#22411;&#12289;&#31995;&#32479;&#21644;&#24212;&#29992;&#30340;&#31034;&#20363;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#24403;&#21069;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#21830;&#19994;&#19982;&#20449;&#24687;&#31995;&#32479;&#24037;&#31243;&#30740;&#31350;&#30340;&#35758;&#31243;&#65292;&#21253;&#25324;&#30740;&#31350;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07930</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Generative AI. (arXiv:2309.07930v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07930
&lt;/p&gt;
&lt;p&gt;
"&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;"&#25351;&#30340;&#26159;&#33021;&#22815;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#29983;&#25104;&#26032;&#39062;&#26377;&#24847;&#20041;&#20869;&#23481;&#30340;&#35745;&#31639;&#25216;&#26415;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#38899;&#39057;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#30340;&#27010;&#24565;&#65292;&#24182;&#20171;&#32461;&#20102;&#27169;&#22411;&#12289;&#31995;&#32479;&#21644;&#24212;&#29992;&#30340;&#31034;&#20363;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#24403;&#21069;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#21830;&#19994;&#19982;&#20449;&#24687;&#31995;&#32479;&#24037;&#31243;&#30740;&#31350;&#30340;&#35758;&#31243;&#65292;&#21253;&#25324;&#30740;&#31350;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;"&#19968;&#35789;&#25351;&#30340;&#26159;&#33021;&#22815;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#29983;&#25104;&#30475;&#20284;&#26032;&#39062;&#26377;&#24847;&#20041;&#30340;&#20869;&#23481;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#38899;&#39057;&#30340;&#35745;&#31639;&#25216;&#26415;&#12290;&#36825;&#31181;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20363;&#22914;Dall-E 2&#65292;GPT-4&#21644;Copilot&#65292;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#24037;&#20316;&#21644;&#19982;&#20182;&#20154;&#20132;&#27969;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24418;&#23481;&#20026;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#30340;&#19968;&#31181;&#23454;&#20307;&#65292;&#24182;&#25552;&#20379;&#20102;&#27169;&#22411;&#12289;&#31995;&#32479;&#21644;&#24212;&#29992;&#30340;&#31034;&#20363;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24403;&#21069;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#21830;&#19994;&#19982;&#20449;&#24687;&#31995;&#32479;&#24037;&#31243;&#65288;BISE&#65289;&#30740;&#31350;&#30340;&#35758;&#31243;&#12290;&#19982;&#20197;&#24448;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#37325;&#28857;&#35752;&#35770;&#20102;&#20449;&#24687;&#31995;&#32479;&#32972;&#26223;&#19979;&#30340;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#35752;&#35770;&#20102;BISE&#31038;&#21306;&#29420;&#29305;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;BISE&#30740;&#31350;&#30340;&#26377;&#24433;&#21709;&#30340;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term "generative AI" refers to computational techniques that are capable of generating seemingly new, meaningful content such as text, images, or audio from training data. The widespread diffusion of this technology with examples such as Dall-E 2, GPT-4, and Copilot is currently revolutionizing the way we work and communicate with each other. In this article, we provide a conceptualization of generative AI as an entity in socio-technical systems and provide examples of models, systems, and applications. Based on that, we introduce limitations of current generative AI and provide an agenda for Business &amp; Information Systems Engineering (BISE) research. Different from previous works, we focus on generative AI in the context of information systems, and, to this end, we discuss several opportunities and challenges that are unique to the BISE community and make suggestions for impactful directions for BISE research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#31163;&#25955;&#21644;&#32500;&#24230;&#24773;&#32490;&#65292;&#24182;&#22312;MER 2023&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#31532;&#19977;&#30340;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2309.07925</link><description>&lt;p&gt;
MER 2023&#20013;&#22522;&#20110;&#22810;&#26631;&#31614;&#32852;&#21512;&#35299;&#30721;&#30340;&#20998;&#23618;&#38899;&#39057;-&#35270;&#35273;&#20449;&#24687;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Audio-Visual Information Fusion with Multi-label Joint Decoding for MER 2023. (arXiv:2309.07925v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#31163;&#25955;&#21644;&#32500;&#24230;&#24773;&#32490;&#65292;&#24182;&#22312;MER 2023&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#31532;&#19977;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#31163;&#25955;&#21644;&#32500;&#24230;&#24773;&#32490;&#30340;&#21019;&#26032;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20174;&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#28145;&#23618;&#29305;&#24449;&#34987;&#29992;&#20316;&#21407;&#22987;&#35270;&#39057;&#30340;&#31283;&#20581;&#22768;&#38899;&#21644;&#35270;&#35273;&#34920;&#31034;&#12290;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#29305;&#24449;&#32858;&#38598;&#30340;&#19977;&#31181;&#19981;&#21516;&#32467;&#26500;&#34987;&#35774;&#35745;&#29992;&#20110;&#28145;&#23618;&#29305;&#24449;&#34701;&#21512;&#12290;&#28982;&#21518;&#65292;&#22312;&#35299;&#30721;&#38454;&#27573;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32852;&#21512;&#35299;&#30721;&#32467;&#26500;&#65292;&#29992;&#20110;&#24773;&#32490;&#20998;&#31867;&#21644;&#20215;&#20540;&#22238;&#24402;&#12290;&#36824;&#35774;&#35745;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22810;&#20219;&#21153;&#25439;&#22833;&#26469;&#20248;&#21270;&#25972;&#20010;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#22312;&#21518;&#39564;&#27010;&#29575;&#27700;&#24179;&#19978;&#32467;&#21512;&#19977;&#31181;&#19981;&#21516;&#32467;&#26500;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#31163;&#25955;&#21644;&#32500;&#24230;&#24773;&#32490;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#25361;&#25112;&#65288;MER 2023&#65289;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#24773;&#32490;&#20998;&#31867;&#21644;&#20215;&#20540;&#22238;&#24402;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#31995;&#32479;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#24182;&#22312;MER-M&#25490;&#34892;&#27036;&#19978;&#25490;&#21517;&#31532;&#19977;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel framework for recognizing both discrete and dimensional emotions. In our framework, deep features extracted from foundation models are used as robust acoustic and visual representations of raw video. Three different structures based on attention-guided feature gathering (AFG) are designed for deep feature fusion. Then, we introduce a joint decoding structure for emotion classification and valence regression in the decoding stage. A multi-task loss based on uncertainty is also designed to optimize the whole process. Finally, by combining three different structures on the posterior probability level, we obtain the final predictions of discrete and dimensional emotions. When tested on the dataset of multimodal emotion recognition challenge (MER 2023), the proposed framework yields consistent improvements in both emotion classification and valence regression. Our final system achieves state-of-the-art performance and ranks third on the leaderboard on MER-M
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.07864</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Rise and Potential of Large Language Model Based Agents: A Survey. (arXiv:2309.07864v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07864
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#31867;&#19968;&#30452;&#36861;&#27714;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36798;&#21040;&#25110;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#30446;&#26631;&#65292;&#32780;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26377;&#26395;&#26041;&#24335;&#30340;AI&#20195;&#29702;&#12290;AI&#20195;&#29702;&#26159;&#33021;&#24863;&#30693;&#29615;&#22659;&#12289;&#20570;&#20986;&#20915;&#31574;&#21644;&#37319;&#21462;&#34892;&#21160;&#30340;&#20154;&#24037;&#23454;&#20307;&#12290;&#33258;20&#19990;&#32426;&#20013;&#21494;&#20197;&#26469;&#65292;&#20154;&#20204;&#20026;&#24320;&#21457;&#26234;&#33021;AI&#20195;&#29702;&#36827;&#34892;&#20102;&#35768;&#22810;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#31639;&#27861;&#25110;&#35757;&#32451;&#31574;&#30053;&#30340;&#36827;&#27493;&#19978;&#65292;&#20197;&#22686;&#24378;&#29305;&#23450;&#33021;&#21147;&#25110;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#23454;&#38469;&#19978;&#65292;&#31038;&#21306;&#25152;&#32570;&#20047;&#30340;&#26159;&#19968;&#20010;&#36275;&#22815;&#36890;&#29992;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#35774;&#35745;&#33021;&#36866;&#24212;&#21508;&#31181;&#22330;&#26223;&#30340;AI&#20195;&#29702;&#30340;&#36215;&#28857;&#12290;&#30001;&#20110;&#23637;&#31034;&#20986;&#30340;&#22810;&#21151;&#33021;&#21644;&#26174;&#33879;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#20026;&#26500;&#24314;&#36890;&#29992;AI&#20195;&#29702;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#35768;&#22810;&#30740;&#31350;&#24037;&#20316;&#21033;&#29992;LLMs&#20316;&#20026;&#26500;&#24314;AI&#20195;&#29702;&#30340;&#22522;&#30784;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent AI agents since the mid-20th century. However, these efforts have mainly focused on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a sufficiently general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile and remarkable capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many research efforts have leveraged LLMs as the foundation to build AI agents and ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38598;&#21512;&#21407;&#21017;&#30340;&#23398;&#20064;&#21327;&#35758;&#30340;&#24605;&#36335;&#65292;&#20854;&#20013;&#26426;&#22120;&#22312;&#29615;&#22659;&#20132;&#20114;&#32972;&#26223;&#20013;&#25484;&#25569;&#35748;&#30693;&#25216;&#33021;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#38598;&#38598;&#20013;&#21270;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.06938</link><description>&lt;p&gt;
&#26080;&#38598;&#21512;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Collectionless Artificial Intelligence. (arXiv:2309.06938v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38598;&#21512;&#21407;&#21017;&#30340;&#23398;&#20064;&#21327;&#35758;&#30340;&#24605;&#36335;&#65292;&#20854;&#20013;&#26426;&#22120;&#22312;&#29615;&#22659;&#20132;&#20114;&#32972;&#26223;&#20013;&#25484;&#25569;&#35748;&#30693;&#25216;&#33021;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#38598;&#38598;&#20013;&#21270;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#20307;&#19978;&#65292;&#22788;&#29702;&#24222;&#22823;&#25968;&#25454;&#38598;&#34987;&#35748;&#20026;&#26159;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#22766;&#35266;&#32467;&#26524;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#20110;&#36825;&#31181;&#25968;&#25454;&#38598;&#30340;&#38598;&#20013;&#21270;&#23384;&#22312;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#39118;&#38505;&#24847;&#35782;&#12290;&#26412;&#25991;&#25903;&#25345;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#21327;&#35758;&#24605;&#36335;&#65292;&#20854;&#20013;&#26426;&#22120;&#22312;&#30495;&#27491;&#20197;&#29615;&#22659;&#20132;&#20114;&#20026;&#20013;&#24515;&#30340;&#31867;&#20154;&#35748;&#30693;&#32972;&#26223;&#19979;&#25484;&#25569;&#35748;&#30693;&#25216;&#33021;&#12290;&#36825;&#24847;&#21619;&#30528;&#23398;&#20064;&#21327;&#35758;&#38656;&#35201;&#36981;&#24490;&#26080;&#38598;&#21512;&#21407;&#21017;&#65292;&#21363;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#65292;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#30340;&#25968;&#25454;&#34987;&#29992;&#20110;&#26356;&#26032;&#24403;&#21069;&#29615;&#22659;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#19988;&#20195;&#29702;&#19981;&#33021;&#23545;&#26102;&#38388;&#27969;&#36827;&#34892;&#35760;&#24405;&#12290;&#22522;&#26412;&#19978;&#65292;&#19981;&#33021;&#23384;&#20648;&#26469;&#33258;&#20256;&#24863;&#22120;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#26080;&#38598;&#21512;&#21407;&#21017;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
By and large, the professional handling of huge data collections is regarded as a fundamental ingredient of the progress of machine learning and of its spectacular results in related disciplines, with a growing agreement on risks connected to the centralization of such data collections. This paper sustains the position that the time has come for thinking of new learning protocols where machines conquer cognitive skills in a truly human-like context centered on environmental interactions. This comes with specific restrictions on the learning protocol according to the collectionless principle, which states that, at each time instant, data acquired from the environment is processed with the purpose of contributing to update the current internal representation of the environment, and that the agent is not given the privilege of recording the temporal stream. Basically, there is neither permission to store the temporal information coming from the sensors, thus promoting the development of s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06800</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#65292;&#22240;&#20026;&#23427;&#22312;&#20132;&#36890;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36817;&#26399;&#65292;&#35768;&#22810;&#30740;&#31350;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#39044;&#27979;&#20301;&#32622;&#26377;&#23436;&#25972;&#25110;&#33267;&#23569;&#37096;&#20998;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#19981;&#33021;&#25193;&#23637;&#21040;&#26080;&#21382;&#21490;&#35760;&#24405;&#30340;&#20301;&#32622;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#39044;&#31639;&#38480;&#21046;&#21644;&#23433;&#35013;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#21487;&#33021;&#21463;&#38480;&#65292;&#36825;&#20351;&#24471;&#22823;&#22810;&#25968;&#24403;&#21069;&#27169;&#22411;&#19981;&#36866;&#29992;&#12290;&#34429;&#28982;&#23569;&#25968;&#25991;&#29486;&#23581;&#35797;&#22312;&#32570;&#22833;&#20301;&#32622;&#19978;&#25554;&#34917;&#20132;&#36890;&#29366;&#24577;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19982;&#20256;&#24863;&#22120;&#20301;&#32622;&#21516;&#26102;&#35266;&#27979;&#30340;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#39044;&#27979;&#20219;&#21153;&#12290;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#32570;&#20047;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#24471;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#20219;&#21153;&#25110;&#28041;&#21450;&#20915;&#31574;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#21463;&#21040;&#20808;&#21069;&#30340;&#24402;&#32435;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-awa
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#39057;&#35889;&#28151;&#21512;&#22120;&#65288;DSM&#65289;&#26159;&#19968;&#31181;&#20869;&#23481;&#33258;&#36866;&#24212;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#34920;&#31034;&#20196;&#29260;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#33021;&#22815;&#23398;&#20064;&#38271;&#26399;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#23427;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#39057;&#35889;&#26435;&#37325;&#29983;&#25104;&#23618;&#20316;&#20026;&#39057;&#35889;&#24102;&#36873;&#25321;&#22120;&#65292;&#20197;&#24378;&#35843;&#20449;&#24687;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.06721</link><description>&lt;p&gt;
&#21160;&#24577;&#39057;&#35889;&#28151;&#21512;&#22120;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Dynamic Spectrum Mixer for Visual Recognition. (arXiv:2309.06721v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06721
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#39057;&#35889;&#28151;&#21512;&#22120;&#65288;DSM&#65289;&#26159;&#19968;&#31181;&#20869;&#23481;&#33258;&#36866;&#24212;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#34920;&#31034;&#20196;&#29260;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#33021;&#22815;&#23398;&#20064;&#38271;&#26399;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#23427;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#39057;&#35889;&#26435;&#37325;&#29983;&#25104;&#23618;&#20316;&#20026;&#39057;&#35889;&#24102;&#36873;&#25321;&#22120;&#65292;&#20197;&#24378;&#35843;&#20449;&#24687;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;&#35270;&#35273;&#20027;&#24178;&#22312;&#20960;&#20010;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;MLP&#30340;&#26041;&#27861;&#30452;&#25509;&#20351;&#29992;&#38745;&#24577;&#26435;&#37325;&#32858;&#21512;&#20196;&#20854;&#26080;&#27861;&#36866;&#24212;&#19981;&#21516;&#30340;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;MLP-Transformer&#22312;&#21019;&#24314;&#36828;&#31243;&#20381;&#36182;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25429;&#25417;&#20027;&#35201;&#20256;&#36755;&#23616;&#37096;&#20449;&#24687;&#30340;&#39640;&#39057;&#29575;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#20351;&#20854;&#26080;&#27861;&#24212;&#29992;&#20110;&#19979;&#28216;&#30340;&#31264;&#23494;&#39044;&#27979;&#20219;&#21153;&#65292;&#22914;&#35821;&#20041;&#20998;&#21106;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23481;&#33258;&#36866;&#24212;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32467;&#26500;&#65292;&#31216;&#20026;&#21160;&#24577;&#39057;&#35889;&#28151;&#21512;&#22120;&#65288;DSM&#65289;&#12290;DSM&#36890;&#36807;&#24212;&#29992;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#22312;&#39057;&#22495;&#20013;&#34920;&#31034;&#20196;&#29260;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#21487;&#20197;&#20197;&#23545;&#25968;&#32447;&#24615;&#22797;&#26434;&#24230;&#23398;&#20064;&#38271;&#26399;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#39057;&#35889;&#26435;&#37325;&#29983;&#25104;&#23618;&#20316;&#20026;&#39057;&#35889;&#24102;&#36873;&#25321;&#22120;&#65292;&#33021;&#22815;&#24378;&#35843;&#20449;&#24687;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, MLP-based vision backbones have achieved promising performance in several visual recognition tasks. However, the existing MLP-based methods directly aggregate tokens with static weights, leaving the adaptability to different images untouched. Moreover, Recent research demonstrates that MLP-Transformer is great at creating long-range dependencies but ineffective at catching high frequencies that primarily transmit local information, which prevents it from applying to the downstream dense prediction tasks, such as semantic segmentation. To address these challenges, we propose a content-adaptive yet computationally efficient structure, dubbed Dynamic Spectrum Mixer (DSM). The DSM represents token interactions in the frequency domain by employing the Discrete Cosine Transform, which can learn long-term spatial dependencies with log-linear complexity. Furthermore, a dynamic spectrum weight generation layer is proposed as the spectrum bands selector, which could emphasize the infor
&lt;/p&gt;</description></item><item><title>R^3&#26159;&#19968;&#31181;&#22312;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;&#35774;&#22791;&#30340;&#23454;&#26102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21160;&#24577;&#25209;&#37327;&#22823;&#23567;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#22823;&#23567;&#30340;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#26102;&#38388;&#21644;&#31639;&#27861;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#26377;&#25928;&#22320;&#31649;&#29702;&#20102;&#20869;&#23384;&#21644;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15039</link><description>&lt;p&gt;
R^3: &#22522;&#20110;&#35774;&#22791;&#30340;&#23454;&#26102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
R^3: On-device Real-Time Deep Reinforcement Learning for Autonomous Robotics. (arXiv:2308.15039v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15039
&lt;/p&gt;
&lt;p&gt;
R^3&#26159;&#19968;&#31181;&#22312;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;&#35774;&#22791;&#30340;&#23454;&#26102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21160;&#24577;&#25209;&#37327;&#22823;&#23567;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#22823;&#23567;&#30340;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#26102;&#38388;&#21644;&#31639;&#27861;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#26377;&#25928;&#22320;&#31649;&#29702;&#20102;&#20869;&#23384;&#21644;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#26426;&#22120;&#20154;&#25628;&#25937;&#31995;&#32479;&#65292;&#38656;&#35201;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36830;&#32493;&#36866;&#24212;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#27169;&#22411;&#30340;&#39640;&#25928;&#35774;&#22791;&#19978;&#35757;&#32451;&#12290;&#26412;&#30740;&#31350;&#30340;&#22522;&#26412;&#21160;&#26426;&#26159;&#29702;&#35299;&#21644;&#24212;&#23545;&#22522;&#20110;&#35774;&#22791;&#30340;&#23454;&#26102;DRL&#30340;&#25361;&#25112;&#65292;&#36825;&#28041;&#21450;&#22312;&#20869;&#23384;&#32422;&#26463;&#19979;&#24179;&#34913;&#26102;&#38388;&#21644;&#31639;&#27861;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#12290;&#36825;&#31181;&#24494;&#22937;&#30340;&#24179;&#34913;&#38656;&#35201;&#20849;&#21516;&#20248;&#21270;DRL&#35757;&#32451;&#30340;&#20004;&#20010;&#20851;&#38190;&#21442;&#25968;--&#25209;&#37327;&#22823;&#23567;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#22823;&#23567;&#12290;&#37197;&#32622;&#36825;&#20123;&#21442;&#25968;&#23545;&#26102;&#38388;&#21644;&#31639;&#27861;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#28982;&#32780;&#20004;&#32773;&#37117;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20869;&#23384;&#20998;&#37197;&#25165;&#33021;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;R^3&#65292;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#31649;&#29702;&#26102;&#38388;&#12289;&#20869;&#23384;&#21644;&#31639;&#27861;&#24615;&#33021;&#30340;&#25972;&#20307;&#35299;&#20915;&#26041;&#26696;&#12290;R^3&#37319;&#29992;&#65288;i&#65289;&#19968;&#20010;&#20197;&#25130;&#27490;&#26085;&#26399;&#20026;&#39537;&#21160;&#30340;&#21453;&#39304;&#24490;&#29615;&#65292;&#24102;&#26377;&#21160;&#24577;&#25209;&#37327;&#22823;&#23567;&#65292;
&lt;/p&gt;
&lt;p&gt;
Autonomous robotic systems, like autonomous vehicles and robotic search and rescue, require efficient on-device training for continuous adaptation of Deep Reinforcement Learning (DRL) models in dynamic environments. This research is fundamentally motivated by the need to understand and address the challenges of on-device real-time DRL, which involves balancing timing and algorithm performance under memory constraints, as exposed through our extensive empirical studies. This intricate balance requires co-optimizing two pivotal parameters of DRL training -- batch size and replay buffer size. Configuring these parameters significantly affects timing and algorithm performance, while both (unfortunately) require substantial memory allocation to achieve near-optimal performance.  This paper presents R^3, a holistic solution for managing timing, memory, and algorithm performance in on-device real-time DRL training. R^3 employs (i) a deadline-driven feedback loop with dynamic batch sizing for 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.09729</link><description>&lt;p&gt;
MindMap&#65306;&#30693;&#35782;&#22270;&#35889;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#22270;&#24605;&#32771;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26080;&#27861;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25972;&#21512;&#26368;&#26032;&#30693;&#35782;&#21644;&#24341;&#21457;&#27169;&#22411;&#24605;&#32500;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25552;&#31034;&#31649;&#36947;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;KG&#36755;&#20837;&#24182;&#21033;&#29992;&#38544;&#21547;&#30693;&#35782;&#21644;&#26816;&#32034;&#21040;&#30340;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#32500;&#23548;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#24605;&#32500;&#23548;&#22270;&#22522;&#20110;&#30693;&#35782;&#30340;&#26412;&#20307;&#35770;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MindMap&#25552;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question &amp; answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#23427;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;DCNFIS&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#65292;&#24182;&#19988;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#36890;&#36807;&#27169;&#31946;&#35268;&#21017;&#25552;&#21462;&#30340;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06378</link><description>&lt;p&gt;
DCNFIS&#65306;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System. (arXiv:2308.06378v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#23427;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;DCNFIS&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#65292;&#24182;&#19988;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#36890;&#36807;&#27169;&#31946;&#35268;&#21017;&#25552;&#21462;&#30340;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#36879;&#26126;&#24230;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#33879;&#21517;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#32593;&#32476;&#35774;&#35745;&#65292;&#36890;&#36807;&#23558;&#27169;&#31946;&#36923;&#36753;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#36879;&#26126;&#24230;&#20294;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;DCNFIS&#65289;&#65292;&#24182;&#22312;&#22235;&#20010;&#33879;&#21517;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23427;&#19982;&#19977;&#20010;&#29616;&#26377;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#21516;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;DCNFIS&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#31946;&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#27169;&#31946;&#36923;&#36753;&#30340;&#36879;&#26126;&#24230;&#65292;&#20174;DCNFIS&#20013;&#32534;&#30721;&#30340;&#27169;&#31946;&#35268;&#21017;&#20013;&#25552;&#21462;&#35299;&#37322;&#65292;&#20197;&#28176;&#21464;&#26144;&#23556;&#30340;&#24418;&#24335;&#23637;&#31034;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;Fashion-MNIST&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#35299;&#37322;&#30340;&#29305;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#65292;&#29992;&#20110;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#30340;&#29983;&#29289;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;30&#19975;&#31181;&#20505;&#36873;&#33647;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#25351;&#32441;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.01921</link><description>&lt;p&gt;
&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#29983;&#29289;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats. (arXiv:2308.01921v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01921
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#65292;&#29992;&#20110;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#30340;&#29983;&#29289;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;30&#19975;&#31181;&#20505;&#36873;&#33647;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#25351;&#32441;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#33647;&#29289;&#20998;&#23376;&#24555;&#36895;&#31579;&#36873;&#26159;&#33647;&#29289;&#21457;&#29616;&#31649;&#32447;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#22270;&#31070;&#32463;&#25351;&#32441;&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#39640;&#36890;&#37327;&#21644;&#39640;&#20934;&#30830;&#24615;&#20998;&#23376;&#23545;&#25509;&#20195;&#29702;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#32422;30&#19975;&#31181;&#33647;&#29289;&#20505;&#36873;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#22270;&#31070;&#32463;&#25351;&#32441;&#23545;&#25509;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#12290;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23545;&#22823;&#22810;&#25968;&#23545;&#25509;&#38774;&#28857;&#30340;&#22343;&#26041;&#35823;&#24046;&#20302;&#20110;0.21 kcal/mol&#65292;&#30456;&#27604;&#20256;&#32479;&#22278;&#24418;&#25351;&#32441;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#20026;&#20102;&#20351;&#31070;&#32463;&#25351;&#32441;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20010;&#38774;&#28857;&#19978;&#35757;&#32451;&#30340;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast screening of drug molecules based on the ligand binding affinity is an important step in the drug discovery pipeline. Graph neural fingerprint is a promising method for developing molecular docking surrogates with high throughput and great fidelity. In this study, we built a COVID-19 drug docking dataset of about 300,000 drug candidates on 23 coronavirus protein targets. With this dataset, we trained graph neural fingerprint docking models for high-throughput virtual COVID-19 drug screening. The graph neural fingerprint models yield high prediction accuracy on docking scores with the mean squared error lower than $0.21$ kcal/mol for most of the docking targets, showing significant improvement over conventional circular fingerprint methods. To make the neural fingerprints transferable for unknown targets, we also propose a transferable graph neural fingerprint method trained on multiple targets. With comparable accuracy to target-specific graph neural fingerprint models, the transf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16387</link><description>&lt;p&gt;
Relation-Oriented: &#36808;&#21521;&#19982;&#30693;&#35782;&#23545;&#20934;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24212;&#29992;&#19968;&#20010;&#35266;&#23519;&#23548;&#21521;&#30340;&#21407;&#21017;&#65292;&#20854;&#20013;&#35266;&#23519;&#21464;&#37327;&#20808;&#23384;&#22312;&#24182;&#20026;&#26500;&#24314;&#20851;&#31995;&#22880;&#23450;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#26469;&#35828;&#36275;&#22815;&#20102;&#65292;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#25972;&#21512;&#26292;&#38706;&#20102;&#35266;&#23519;&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#23454;&#38469;&#29702;&#35299;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22609;&#36896;&#20102;&#30001;&#20851;&#31995;&#23450;&#20041;&#30340;&#35748;&#30693;&#23454;&#20307;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36328;&#36234;&#26102;&#38388;&#21644;&#36229;&#32500;&#24230;&#31354;&#38388;&#21046;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#34987;&#38480;&#21046;&#22312;&#35266;&#23519;&#26500;&#24314;&#20013;&#12290;&#20174;&#19968;&#31181;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20581;&#24247;&#20449;&#24687;&#23398;&#30340;&#30452;&#35266;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22312;&#25105;&#20204;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#36825;&#31181;&#19981;&#23545;&#40784;&#30340;&#26681;&#28304;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#19968;&#31181;&#23454;&#38469;&#23454;&#26045;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, we naturally apply an Observation-Oriented principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the misalignment between the observational models and our actual comprehension. Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs. From an innovative Relation-Oriented perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the relation-defined representation learning methodology as a practical implementation of Relation-Oriented modeling, supported by extensive experimental validation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#23398;&#20064;&#20195;&#29702;&#21644;&#23398;&#20064;&#32422;&#26463;&#30456;&#32467;&#21512;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;&#23545;&#27969;&#20307;&#27969;&#21160;&#24615;&#36136;&#30340;&#21453;&#28436;&#31934;&#24230;&#65292;&#32780;&#19988;&#20026;&#21453;&#28436;&#22810;&#27169;&#24577;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.11099</link><description>&lt;p&gt;
&#29992;&#23398;&#20064;&#30340;&#20195;&#29702;&#21644;&#32422;&#26463;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving multiphysics-based inverse problems with learned surrogates and constraints. (arXiv:2307.11099v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#23398;&#20064;&#20195;&#29702;&#21644;&#23398;&#20064;&#32422;&#26463;&#30456;&#32467;&#21512;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;&#23545;&#27969;&#20307;&#27969;&#21160;&#24615;&#36136;&#30340;&#21453;&#28436;&#31934;&#24230;&#65292;&#32780;&#19988;&#20026;&#21453;&#28436;&#22810;&#27169;&#24577;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#36136;&#30899;&#23553;&#23384;&#30417;&#27979;&#20013;&#65292;&#24403;&#22810;&#27169;&#24577;&#26102;&#21464;&#25968;&#25454;&#26114;&#36149;&#19988;&#25968;&#20540;&#27169;&#25311;&#25104;&#26412;&#39640;&#26114;&#26102;&#65292;&#35299;&#20915;&#22522;&#20110;&#22810;&#29289;&#29702;&#30340;&#21453;&#38382;&#39064;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#30340;&#23398;&#20064;&#20195;&#29702;&#19982;&#23398;&#20064;&#32422;&#26463;&#30456;&#32467;&#21512;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#36825;&#31181;&#32452;&#21512;&#19981;&#20165;&#33021;&#22815;&#22823;&#22823;&#25913;&#21892;&#23545;&#37325;&#35201;&#27969;&#20307;&#27969;&#21160;&#24615;&#36136;&#65288;&#28183;&#36879;&#29575;&#65289;&#30340;&#21453;&#28436;&#65292;&#36824;&#33021;&#20026;&#21453;&#28436;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#21253;&#25324;&#20117;&#27979;&#37327;&#21644;&#20027;&#21160;&#28304;&#26102;&#21464;&#22320;&#38663;&#25968;&#25454;&#65289;&#25552;&#20379;&#19968;&#20010;&#33258;&#28982;&#30340;&#24179;&#21488;&#12290;&#36890;&#36807;&#28155;&#21152;&#23398;&#20064;&#32422;&#26463;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#35745;&#31639;&#21487;&#34892;&#30340;&#21453;&#28436;&#26041;&#27861;&#65292;&#20854;&#31934;&#24230;&#20173;&#28982;&#20934;&#30830;&#12290;&#36825;&#36890;&#36807;&#21253;&#21547;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;&#31216;&#20026;&#24402;&#19968;&#21270;&#27969;&#65289;&#65292;&#20351;&#27169;&#22411;&#36845;&#20195;&#20445;&#25345;&#22312;&#20998;&#24067;&#20869;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#20316;&#20026;&#20195;&#29702;&#30340;&#32463;&#36807;&#35757;&#32451;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#20123;&#31639;&#23376;&#29992;&#20110;&#20195;&#26367;&#28041;&#21450;&#37096;&#20998;&#35745;&#31639;&#26114;&#36149;&#30340;&#22810;&#30456;&#27969;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving multiphysics-based inverse problems for geological carbon storage monitoring can be challenging when multimodal time-lapse data are expensive to collect and costly to simulate numerically. We overcome these challenges by combining computationally cheap learned surrogates with learned constraints. Not only does this combination lead to vastly improved inversions for the important fluid-flow property, permeability, it also provides a natural platform for inverting multimodal data including well measurements and active-source time-lapse seismic data. By adding a learned constraint, we arrive at a computationally feasible inversion approach that remains accurate. This is accomplished by including a trained deep neural network, known as a normalizing flow, which forces the model iterates to remain in-distribution, thereby safeguarding the accuracy of trained Fourier neural operators that act as surrogates for the computationally expensive multiphase flow simulations involving partia
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#34987;&#24191;&#27867;&#29992;&#20110;&#22686;&#24378;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25918;&#22823;&#31232;&#26377;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#20197;&#21450;&#21019;&#24314;&#21453;&#20107;&#23454;&#24773;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#28385;&#36275;&#32422;&#26463;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#19988;&#35745;&#31639;&#20195;&#20215;&#39640;&#65292;&#25110;&#32773;&#22312;&#22797;&#26434;&#32422;&#26463;&#26465;&#20214;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;</title><link>http://arxiv.org/abs/2307.01717</link><description>&lt;p&gt;
&#20851;&#20110;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Constrained Time-Series Generation Problem. (arXiv:2307.01717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#34987;&#24191;&#27867;&#29992;&#20110;&#22686;&#24378;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25918;&#22823;&#31232;&#26377;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#20197;&#21450;&#21019;&#24314;&#21453;&#20107;&#23454;&#24773;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#28385;&#36275;&#32422;&#26463;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#19988;&#35745;&#31639;&#20195;&#20215;&#39640;&#65292;&#25110;&#32773;&#22312;&#22797;&#26434;&#32422;&#26463;&#26465;&#20214;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#32463;&#24120;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#29992;&#20110;&#22686;&#21152;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25918;&#22823;&#31232;&#26377;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#24182;&#21019;&#24314;&#30001;&#26102;&#38388;&#24207;&#21015;&#25551;&#36848;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#12290;&#20998;&#24067;&#30456;&#20284;&#24615;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30495;&#23454;&#24615;&#65289;&#20197;&#21450;&#28385;&#36275;&#19968;&#23450;&#25968;&#20540;&#32422;&#26463;&#26159;&#21453;&#20107;&#23454;&#26102;&#38388;&#24207;&#21015;&#22330;&#26223;&#29983;&#25104;&#35831;&#27714;&#20013;&#24120;&#35265;&#30340;&#35201;&#27714;&#12290;&#20363;&#22914;&#65292;&#32654;&#32852;&#20648;&#21457;&#24067;&#20102;&#32473;&#23450;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#30340;&#21512;&#25104;&#24066;&#22330;&#21387;&#21147;&#24773;&#26223;&#65292;&#20379;&#37329;&#34701;&#26426;&#26500;&#35780;&#20272;&#20854;&#22312;&#20551;&#35774;&#24615;&#34928;&#36864;&#20013;&#30340;&#34920;&#29616;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24809;&#32602;&#26469;&#24378;&#21046;&#28385;&#36275;&#32422;&#26463;&#65292;&#24182;&#25298;&#32477;&#19981;&#31526;&#21512;&#32422;&#26463;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#25913;&#21464;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#32780;&#25298;&#32477;&#25277;&#26679;&#21487;&#33021;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#25110;&#32773;&#22312;&#22797;&#26434;&#32422;&#26463;&#26465;&#20214;&#19979;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic time series are often used in practical applications to augment the historical time series dataset for better performance of machine learning algorithms, amplify the occurrence of rare events, and also create counterfactual scenarios described by the time series. Distributional-similarity (which we refer to as realism) as well as the satisfaction of certain numerical constraints are common requirements in counterfactual time series scenario generation requests. For instance, the US Federal Reserve publishes synthetic market stress scenarios given by the constrained time series for financial institutions to assess their performance in hypothetical recessions. Existing approaches for generating constrained time series usually penalize training loss to enforce constraints, and reject non-conforming samples. However, these approaches would require re-training if we change constraints, and rejection sampling can be computationally expensive, or impractical for complex constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36741;&#21161;&#29305;&#24449;&#38388;&#21327;&#20316;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#24635;&#32467;&#20102;&#19981;&#21516;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#30340;&#36129;&#29486;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.16559</link><description>&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#65306;&#23545;&#23646;&#24615;&#38388;&#21327;&#20316;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Feature Selection: A perspective on inter-attribute cooperation. (arXiv:2306.16559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36741;&#21161;&#29305;&#24449;&#38388;&#21327;&#20316;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#24635;&#32467;&#20102;&#19981;&#21516;&#26041;&#27861;&#22312;&#25991;&#29486;&#20013;&#30340;&#36129;&#29486;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#23545;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23398;&#20064;&#20219;&#21153;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#29305;&#24449;&#36873;&#25321;&#26159;&#22788;&#29702;&#32500;&#24230;&#32553;&#20943;&#30340;&#19968;&#31181;&#26377;&#25928;&#25216;&#26415;&#65292;&#36890;&#24120;&#26159;&#22312;&#24212;&#29992;&#23398;&#20064;&#31639;&#27861;&#20043;&#21069;&#30340;&#37325;&#35201;&#25968;&#25454;&#22788;&#29702;&#27493;&#39588;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20174;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#30456;&#20851;&#24615;&#25490;&#24207;&#31639;&#27861;&#21457;&#23637;&#21040;&#26356;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;-&#20887;&#20313;&#26435;&#34913;&#21644;&#22522;&#20110;&#22810;&#20803;&#20381;&#36182;&#24615;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#25429;&#25417;&#22810;&#21464;&#37327;&#20381;&#36182;&#30340;&#36235;&#21183;&#26088;&#22312;&#36890;&#36807;&#29305;&#24449;&#38388;&#30340;&#20114;&#30456;&#21512;&#20316;&#33719;&#21462;&#20851;&#20110;&#31867;&#21035;&#30340;&#29420;&#29305;&#20449;&#24687;&#12290;&#26412;&#25991;&#23545;&#36741;&#21161;&#29305;&#24449;&#38388;&#21327;&#20316;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#24182;&#24635;&#32467;&#20102;&#25991;&#29486;&#20013;&#19981;&#21516;&#26041;&#27861;&#30340;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional datasets depict a challenge for learning tasks in data mining and machine learning. Feature selection is an effective technique in dealing with dimensionality reduction. It is often an essential data processing step prior to applying a learning algorithm. Over the decades, filter feature selection methods have evolved from simple univariate relevance ranking algorithms to more sophisticated relevance-redundancy trade-offs and to multivariate dependencies-based approaches in recent years. This tendency to capture multivariate dependence aims at obtaining unique information about the class from the intercooperation among features. This paper presents a comprehensive survey of the state-of-the-art work on filter feature selection methods assisted by feature intercooperation, and summarizes the contributions of different approaches found in the literature. Furthermore, current issues and challenges are introduced to identify promising future research and development.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2306.14096</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#23454;&#20307;&#32423;&#21035;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#26159;&#24773;&#24863;&#20998;&#26512;&#30340;&#37325;&#35201;&#23376;&#20219;&#21153;&#65292;&#30446;&#21069;&#38754;&#20020;&#30528;&#20247;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26469;&#33258;&#20110;&#32570;&#20047;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#37329;&#34701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#30340;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#36825;&#38480;&#21046;&#20102;&#24320;&#21457;&#26377;&#25928;&#25991;&#26412;&#22788;&#29702;&#25216;&#26415;&#25152;&#38656;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#35328;&#27169;&#24335;&#21305;&#37197;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;&#29616;&#26377;&#24320;&#28304;LLMs&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#21160;&#30495;&#23454;&#19990;&#30028;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which shoul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23545;&#31995;&#32479;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29983;&#25104;&#30340;&#27874;&#24418;&#12290;</title><link>http://arxiv.org/abs/2306.10359</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#39537;&#21160;Foley&#38899;&#25928;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text-Driven Foley Sound Generation With Latent Diffusion Model. (arXiv:2306.10359v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23545;&#31995;&#32479;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29983;&#25104;&#30340;&#27874;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Foley&#38899;&#25928;&#29983;&#25104;&#26088;&#22312;&#20026;&#22810;&#23186;&#20307;&#20869;&#23481;&#29983;&#25104;&#32972;&#26223;&#38899;&#25928;&#12290;&#20808;&#21069;&#30340;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22823;&#37327;&#26377;&#26631;&#31614;&#30340;&#24320;&#21457;&#38598;&#20316;&#20026;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;&#25968;&#23383;&#25110;one-hot&#21521;&#37327;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#20026;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#37197;&#23545;&#65288;CLAP&#65289;&#25216;&#26415;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26469;&#23545;&#35813;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25991;&#26412;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#23884;&#20837;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#32534;&#30721;&#22120;&#20043;&#21518;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#25991;&#26412;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#38899;&#39057;&#29255;&#27573;&#24182;&#36873;&#25321;&#26368;&#20339;&#29255;&#27573;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#29983;&#25104;&#30340;&#27874;&#24418;&#65292;&#26368;&#20339;&#29255;&#27573;&#26159;&#26681;&#25454;&#23884;&#20837;&#20043;&#38388;&#30456;&#20284;&#24615;&#24471;&#20998;&#30830;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foley sound generation aims to synthesise the background sound for multimedia content. Previous models usually employ a large development set with labels as input (e.g., single numbers or one-hot vector). In this work, we propose a diffusion model based system for Foley sound generation with text conditions. To alleviate the data scarcity issue, our model is initially pre-trained with large-scale datasets and fine-tuned to this task via transfer learning using the contrastive language-audio pertaining (CLAP) technique. We have observed that the feature embedding extracted by the text encoder can significantly affect the performance of the generation model. Hence, we introduce a trainable layer after the encoder to improve the text embedding produced by the encoder. In addition, we further refine the generated waveform by generating multiple candidate audio clips simultaneously and selecting the best one, which is determined in terms of the similarity score between the embedding of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#36807;&#31243;&#36827;&#34892;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;&#21644;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05554</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#20013;&#30340;&#24212;&#29992;&#21644;&#39044;&#27979;&#65306;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Simulation and Prediction of Countercurrent Spontaneous Imbibition at Early and Late Times Using Physics-Informed Neural Networks. (arXiv:2306.05554v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#36807;&#31243;&#36827;&#34892;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;&#21644;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#65288;COUCSI&#65289;&#26159;&#19968;&#31181;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36807;&#31243;&#65292;&#20854;&#20013;&#28070;&#28287;&#30456;&#21462;&#20195;&#20102;&#38750;&#28070;&#28287;&#30456;&#30340;&#20301;&#32622;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#35299;&#20915;&#26089;&#26399;&#65288;ET&#65289;&#21644;&#26202;&#26399;&#65288;LT&#65289;COUCSI&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#20197;&#25913;&#36827;PINNs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#33258;&#21464;&#37327;&#23558;COUCSI&#38382;&#39064;&#20998;&#21035;&#29992;XT-&#65292;XY-&#21644;Z-&#19977;&#31181;&#31561;&#25928;&#24418;&#24335;&#36827;&#34892;&#25551;&#36848;&#65306;&#31532;&#19968;&#20010;&#25551;&#36848;&#20102;&#39281;&#21644;&#24230;&#20316;&#20026;&#35268;&#33539;&#21270;&#20301;&#32622;X&#21644;&#26102;&#38388;T&#30340;&#20989;&#25968;;&#31532;&#20108;&#20010;&#25551;&#36848;&#20102;X&#21644;Y=T^0.5&#20316;&#20026;&#20989;&#25968;&#30340;&#39281;&#21644;&#24230;;&#31532;&#19977;&#20010;&#20316;&#20026;Z=X/T^0.5&#30340;&#21807;&#19968;&#20989;&#25968;&#65288;&#20165;&#22312;ET&#19979;&#26377;&#25928;&#65289;&#12290;&#35813;PINN&#27169;&#22411;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#65292;&#24182;&#22522;&#20110;&#26368;&#23567;&#21270;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#29289;&#29702;&#20449;&#24687;&#20002;&#22833;&#39033;&#21644;&#19982;&#21021;&#22987;&#36793;&#30028;&#26465;&#20214;&#30456;&#23545;&#24212;&#30340;&#39033;&#12290;&#27809;&#26377;&#21512;&#25104;&#25110;&#23454;&#39564;&#25968;&#25454;&#34987;&#35843;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Countercurrent spontaneous imbibition (COUCSI) is a process in porous materials in which a wetting phase displaces non-wetting phase. In this work, we investigate for the first time the application of Physics-Informed Neural Networks (PINNs) in solving the 1D COUCSI problem in both early (ET) and late (LT) times. Also novel, we examine the Change-of-Variables technique for improving the performance of PINNs. We formulated the COUCSI problem in three equivalent forms by changing the independent variables: XT-, XY-, and Z-formulations. The first describes saturation as function of normalized position X and time T; the second as function of X and Y=T^0.5; and the third as a sole function of Z=X/T^0.5 (valid only at ET). The PINN model was generated using a feed-forward neural network and trained based on minimizing a weighted loss function, including the physics-informed loss term and terms corresponding to the initial and boundary conditions. No synthetical or experimental data were invo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.16044</link><description>&lt;p&gt;
&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#23558;&#22122;&#22768;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#26159;&#22823;&#33041;&#38750;&#20961;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#30340;&#22522;&#30784;&#65292;&#24182;&#24050;&#25104;&#20026;&#31070;&#32463;&#24418;&#24577;&#26234;&#33021;&#30340;&#25903;&#26609;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#37319;&#29992;&#24102;&#26377;&#22122;&#22768;&#31070;&#32463;&#20803;&#21160;&#21147;&#23398;&#30340;&#33033;&#20914;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#26174;&#31034;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#29702;&#35770;&#19978;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;NDL&#20026;&#20195;&#29702;&#26799;&#24230;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#12290;&#36890;&#36807;&#23558;&#21508;&#31181;SNN&#26550;&#26500;&#21644;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#30830;&#23450;&#24615;SNNs&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#24230;&#25513;&#30721;&#29575;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;MLM&#39044;&#35757;&#32451;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#25513;&#30721;&#29575;&#65292;&#36798;&#21040;&#20102;&#23545;BERT-base&#21644;BERT-large&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;0.46%&#21644;0.25%&#30340;&#24179;&#22343;GLUE&#20934;&#30830;&#29575;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21152;&#24555;&#20102;BERT-base&#30340;&#39044;&#35757;&#32451;&#36895;&#24230;&#65292;&#36824;&#23454;&#29616;&#20102;&#23545;BERT-large&#30340;&#24085;&#32047;&#25176;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2305.15096</link><description>&lt;p&gt;
MLM&#39044;&#35757;&#32451;&#30340;&#21160;&#24577;&#25513;&#30721;&#29575;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Dynamic Masking Rate Schedules for MLM Pretraining. (arXiv:2305.15096v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#24230;&#25513;&#30721;&#29575;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;MLM&#39044;&#35757;&#32451;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#25513;&#30721;&#29575;&#65292;&#36798;&#21040;&#20102;&#23545;BERT-base&#21644;BERT-large&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;0.46%&#21644;0.25%&#30340;&#24179;&#22343;GLUE&#20934;&#30830;&#29575;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21152;&#24555;&#20102;BERT-base&#30340;&#39044;&#35757;&#32451;&#36895;&#24230;&#65292;&#36824;&#23454;&#29616;&#20102;&#23545;BERT-large&#30340;&#24085;&#32047;&#25176;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#30340;transformer&#27169;&#22411;&#20351;&#29992;&#20102;&#21407;&#22987;BERT&#27169;&#22411;&#30340;&#22266;&#23450;&#25513;&#30721;&#29575;15%&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25513;&#30721;&#29575;&#26469;&#26367;&#20195;&#22266;&#23450;&#29575;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#32447;&#24615;&#38477;&#20302;&#25513;&#30721;&#29575;&#21487;&#20197;&#27604;&#22266;&#23450;&#29575;&#22522;&#20934;&#20998;&#21035;&#25552;&#39640;BERT-base&#21644;BERT-large&#30340;&#24179;&#22343;GLUE&#20934;&#30830;&#29575;0.46%&#21644;0.25%&#12290;&#36825;&#20123;&#25552;&#21319;&#26469;&#33258;&#20110;&#25509;&#35302;&#39640;&#21644;&#20302;&#25513;&#30721;&#29575;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#22312;&#20004;&#31181;&#35774;&#32622;&#20013;&#37117;&#24102;&#26469;&#20102;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25513;&#30721;&#29575;&#35843;&#24230;&#26159;&#25552;&#39640;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;BERT-base&#30340;&#39044;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;1.89&#20493;&#65292;&#24182;&#23545;BERT-large&#23454;&#29616;&#20102;&#24085;&#32047;&#25176;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most works on transformers trained with the Masked Language Modeling (MLM) objective use the original BERT model's fixed masking rate of 15%. We propose to instead dynamically schedule the masking rate throughout training. We find that linearly decreasing the masking rate over the course of pretraining improves average GLUE accuracy by up to 0.46% and 0.25% in BERT-base and BERT-large, respectively, compared to fixed rate baselines. These gains come from exposure to both high and low masking rate regimes, providing benefits from both settings. Our results demonstrate that masking rate scheduling is a simple way to improve the quality of masked language models, achieving up to a 1.89x speedup in pretraining for BERT-base as well as a Pareto improvement for BERT-large.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#26631;&#27880;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;&#20154;&#31867;&#26631;&#27880;&#21592;&#12290;</title><link>http://arxiv.org/abs/2305.08339</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#36741;&#21161;&#27880;&#37322;&#36827;&#34892;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#65306;&#26412;&#22320;&#35821;&#27861;&#20998;&#26512;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis. (arXiv:2305.08339v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#26631;&#27880;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;&#20154;&#31867;&#26631;&#27880;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;LLMs&#22312;&#21327;&#21161;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#33258;&#21160;&#26631;&#27880;&#20026;&#29305;&#23450;&#35821;&#35328;&#20449;&#24687;&#31867;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#30340;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#12289;&#22522;&#20110;GPT-4&#30340;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#32534;&#30721;&#22120;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#30528;&#20248;&#20110;ChatGPT&#12290;&#19982;&#20154;&#31867;&#26631;&#27880;&#21592;&#30456;&#27604;&#65292;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25972;&#20307;&#34920;&#29616;&#30053;&#20302;&#20110;&#20154;&#31867;&#26631;&#27880;&#21592;&#30340;&#34920;&#29616;&#65292;&#20294;&#24050;&#32463;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1&#24471;&#20998;:&#36947;&#27465;&#26631;&#35760;99.95&#65285;&#65292;&#21407;&#22240;&#26631;&#35760;91.91&#65285;&#65292;&#36947;&#27465;&#32773;&#26631;&#35760;95.35&#65285;&#65292;&#34987;&#36947;&#27465;&#32773;&#26631;&#35760;89.74&#65285;&#21644;&#21152;&#24378;&#26631;&#35760;96.47&#65285;&#12290;&#36825;&#34920;&#26126;&#65292;&#22312;&#35821;&#35328;&#31867;&#21035;&#28165;&#26224;&#19988;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;LLM&#36741;&#21161;&#27880;&#37322;&#36827;&#34892;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots based on Large Language Models (LLMs) have shown strong capabilities in language understanding. In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information. Specifically, we examined to what extent LLMs understand the functional elements constituting the speech act of apology from a local grammar perspective, by comparing the performance of ChatGPT (powered by GPT-3.5), the Bing chatbot (powered by GPT-4), and a human coder in the annotation task. The results demonstrate that the Bing chatbot significantly outperformed ChatGPT in the task. Compared to human annotator, the overall performance of the Bing chatbot was slightly less satisfactory. However, it already achieved high F1 scores: 99.95% for the tag of APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for APOLOGISEE, and 96.47% for INTENSIFIER. This suggests that it is feasible to use LLM-
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#20307;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.14633</link><description>&lt;p&gt;
CVRecon: &#37325;&#26032;&#24605;&#32771;&#31070;&#32463;&#37325;&#24314;&#30340;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction. (arXiv:2304.14633v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14633
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#20307;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20351;&#29992;&#22270;&#20687;&#24207;&#21015;&#36827;&#34892;&#31070;&#32463;&#37325;&#24314;&#30340;&#36827;&#23637;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32570;&#20047;&#28145;&#24230;&#20449;&#24687;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20307;&#31215;&#30340;&#25216;&#26415;&#20165;&#27839;&#25972;&#20010;&#30456;&#26426;&#20809;&#32447;&#22797;&#21046;&#23545;&#35937;&#34920;&#38754;&#30340;2D&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#22797;&#21046;&#20250;&#22312;&#31354;&#27934;&#21644;&#36974;&#25377;&#31354;&#38388;&#20013;&#24341;&#20837;&#22122;&#22768;&#65292;&#20174;&#32780;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;3D&#20960;&#20309;&#20307;&#25104;&#24418;&#26041;&#38754;&#20135;&#29983;&#25361;&#25112;&#12290;&#21463;&#20256;&#32479;&#22810;&#35270;&#35282;&#31435;&#20307;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;3D&#31070;&#32463;&#37325;&#24314;&#26694;&#26550;CVRecon&#65292;&#26088;&#22312;&#21033;&#29992;&#20195;&#20215;&#20307;&#20013;&#20016;&#23500;&#30340;&#20960;&#20309;&#23884;&#20837;&#26469;&#20419;&#36827;3D&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#20960;&#20309;&#29305;&#24449;&#34920;&#31034;&#27861;&#8212;&#8212;&#23556;&#32447;&#19978;&#19979;&#25991;&#34917;&#20607;&#20195;&#20215;&#20307;&#65288;RCCV&#65289;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#23436;&#25972;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#32534;&#30721;&#35270;&#35282;&#30456;&#20851;&#20449;&#24687;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26174;&#30528;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#24674;&#22797;&#20102;&#28165;&#26224;&#30340;
&lt;/p&gt;
&lt;p&gt;
Recent advances in neural reconstruction using posed image sequences have made remarkable progress. However, due to the lack of depth information, existing volumetric-based techniques simply duplicate 2D image features of the object surface along the entire camera ray. We contend this duplication introduces noise in empty and occluded spaces, posing challenges for producing high-quality 3D geometry. Drawing inspiration from traditional multi-view stereo methods, we propose an end-to-end 3D neural reconstruction framework CVRecon, designed to exploit the rich geometric embedding in the cost volumes to facilitate 3D geometric feature learning. Furthermore, we present Ray-contextual Compensated Cost Volume (RCCV), a novel 3D geometric feature representation that encodes view-dependent information with improved integrity and robustness. Through comprehensive experiments, we demonstrate that our approach significantly improves the reconstruction quality in various metrics and recovers clear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#36141;&#29289;&#36873;&#25321;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.06016</link><description>&lt;p&gt;
Probe&#65306;&#23398;&#20064;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#25414;&#32465;&#36873;&#25321;&#20013;&#30340;&#20010;&#24615;&#21270;&#25237;&#24433;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Probe: Learning Users' Personalized Projection Bias in Intertemporal Bundle Choices. (arXiv:2303.06016v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#36141;&#29289;&#36873;&#25321;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#36328;&#24230;&#30340;&#36873;&#25321;&#38656;&#35201;&#26435;&#34913;&#29616;&#22312;&#30340;&#25104;&#26412;&#21644;&#26410;&#26469;&#30340;&#25910;&#30410;&#12290;&#20854;&#20013;&#19968;&#31181;&#20855;&#20307;&#30340;&#36873;&#25321;&#26159;&#20915;&#23450;&#36141;&#20080;&#21333;&#20010;&#29289;&#21697;&#36824;&#26159;&#36873;&#25321;&#21253;&#21547;&#35813;&#29289;&#21697;&#30340;&#25414;&#32465;&#38144;&#21806;&#26041;&#24335;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20551;&#35774;&#20010;&#20154;&#23545;&#36825;&#20123;&#36873;&#25321;&#20013;&#28041;&#21450;&#30340;&#22240;&#32032;&#26377;&#20934;&#30830;&#30340;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#29992;&#25143;&#23545;&#36825;&#20123;&#22240;&#32032;&#30340;&#24863;&#30693;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#65292;&#23548;&#33268;&#20102;&#38750;&#29702;&#24615;&#21644;&#27425;&#20248;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#24120;&#35265;&#30340;&#20559;&#24046;&#65306;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#24182;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21152;&#26435;&#20989;&#25968;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#25237;&#24433;&#20559;&#24046;&#65292;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#26469;&#32771;&#34385;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#24182;&#24341;&#20837;&#34892;&#20026;&#32463;&#27982;&#23398;&#20013;&#30340;&#21069;&#26223;&#29702;&#35770;&#26469;&#32452;&#21512;&#21152;&#26435;&#21644;&#20215;&#20540;&#20989;&#25968;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#29992;&#25143;&#36141;&#20080;&#25414;&#32465;&#38144;&#21806;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intertemporal choices involve making decisions that require weighing the costs in the present against the benefits in the future. One specific type of intertemporal choice is the decision between purchasing an individual item or opting for a bundle that includes that item. Previous research assumes that individuals have accurate expectations of the factors involved in these choices. However, in reality, users' perceptions of these factors are often biased, leading to irrational and suboptimal decision-making. In this work, we specifically focus on two commonly observed biases: projection bias and the reference-point effect. To address these biases, we propose a novel bias-embedded preference model called Probe. The Probe incorporates a weight function to capture users' projection bias and a value function to account for the reference-point effect, and introduce prospect theory from behavioral economics to combine the weight and value functions. This allows us to determine the probabili
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#31163;&#25955;&#29305;&#24449;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#30340;&#28145;&#24230;&#29983;&#25104;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#39640;&#32500;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#20013;&#36890;&#29992;&#12289;&#20934;&#30830;&#21644;&#32500;&#24230;&#26080;&#20851;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13221</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65306;&#36890;&#36807;&#31163;&#25955;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#23454;&#29616;&#28145;&#24230;&#29983;&#25104;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Data-Centric AI: Deep Generative Differentiable Feature Selection via Discrete Subsetting as Continuous Embedding Space Optimization. (arXiv:2302.13221v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#31163;&#25955;&#29305;&#24449;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#30340;&#28145;&#24230;&#29983;&#25104;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#39640;&#32500;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#20013;&#36890;&#29992;&#12289;&#20934;&#30830;&#21644;&#32500;&#24230;&#26080;&#20851;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#65288;FS&#65289;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#25214;&#21040;&#26368;&#20339;&#29305;&#24449;&#23376;&#38598;&#65292;&#20363;&#22914;&#36807;&#28388;&#22120;&#12289;&#21253;&#35013;&#22120;&#21644;&#23884;&#20837;&#24335;&#26041;&#27861;&#12290;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;FS&#30340;&#26631;&#20934;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21464;&#21270;&#65292;&#24182;&#19988;&#24403;&#25968;&#25454;&#26159;&#39640;&#32500;&#21644;&#23567;&#26679;&#26412;&#26102;&#65292;FS&#23481;&#26131;&#20986;&#29616;&#38382;&#39064;&#12290;&#36873;&#25321;&#30340;&#29305;&#24449;&#23376;&#38598;&#26159;&#21542;&#21487;&#20197;&#26356;&#36890;&#29992;&#12289;&#20934;&#30830;&#21644;&#32500;&#24230;&#26080;&#20851;&#65311;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#27867;&#21270;&#20026;&#19968;&#20010;&#28145;&#24230;&#21487;&#24494;&#20998;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65306;&#23558;&#31163;&#25955;&#29305;&#24449;&#23376;&#38598;&#20316;&#20026;&#36830;&#32493;&#23884;&#20837;&#31354;&#38388;&#20248;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#21644;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#28145;&#24230;&#29305;&#24449;&#23376;&#38598;&#32534;&#30721;&#22120;&#12289;&#20934;&#30830;&#24615;&#35780;&#20272;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#26799;&#24230;&#19978;&#21319;&#20248;&#21270;&#22120;&#12290;&#36825;&#20010;&#26694;&#26550;&#23454;&#29616;&#20102;&#22235;&#20010;&#27493;&#39588;&#65306;1) &#29305;&#24449;-&#20934;&#30830;&#24615;&#35757;&#32451;&#25968;&#25454;&#20934;&#22791;&#65307;2) &#28145;&#24230;&#29305;&#24449;&#23376;&#38598;&#23884;&#20837;&#65307;3) &#26799;&#24230;&#20248;&#21270;&#25628;&#32034;&#65307;4) &#29305;&#24449;&#23376;&#38598;&#37325;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25216;&#26415;&#27934;&#35265;&#65306;&#23558;&#24378;&#21270;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#27169;&#22411;&#35270;&#20026;&#25628;&#32034;&#21152;&#36895;&#22120;&#12289;&#22810;&#23610;&#24230;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#36880;&#28176;&#22686;&#24378;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Feature Selection (FS), such as filter, wrapper, and embedded methods, aims to find the optimal feature subset for a given downstream task. However, in many real-world practices, 1) the criteria of FS vary across domains; 2) FS is brittle when data is a high-dimensional and small sample size. Can selected feature subsets be more generalized, accurate, and input dimensionality agnostic? We generalize this problem into a deep differentiable feature selection task and propose a new perspective: discrete feature subsetting as continuous embedding space optimization. We develop a generic and principled framework including a deep feature subset encoder, accuracy evaluator, decoder, and gradient ascent optimizer. This framework implements four steps: 1) features-accuracy training data preparation; 2) deep feature subset embedding; 3) gradient-optimized search; 4) feature subset reconstruction. We develop new technical insights: reinforcement as a training data generator, ensembles of diverse 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#30340;&#38656;&#27714;&#12289;&#33539;&#24335;&#21644;&#25216;&#26415;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#20445;&#25252;&#38544;&#31169;&#12289;&#23454;&#29616;&#20302;&#24310;&#36831;&#30340;&#23454;&#26102;&#24615;&#33021;&#21644;&#36164;&#28304;&#20248;&#21270;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.08571</link><description>&lt;p&gt;
&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#30340;&#32508;&#36848;&#19982;&#20998;&#31867;&#65306;&#38656;&#27714;&#65292;&#33539;&#24335;&#21644;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review and a Taxonomy of Edge Machine Learning: Requirements, Paradigms, and Techniques. (arXiv:2302.08571v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08571
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#30340;&#38656;&#27714;&#12289;&#33539;&#24335;&#21644;&#25216;&#26415;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#20445;&#25252;&#38544;&#31169;&#12289;&#23454;&#29616;&#20302;&#24310;&#36831;&#30340;&#23454;&#26102;&#24615;&#33021;&#21644;&#36164;&#28304;&#20248;&#21270;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35745;&#31639;(EC)&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#32467;&#21512;&#25552;&#20986;&#20102;&#36793;&#32536;AI&#30340;&#27010;&#24565;&#65292;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#65292;&#23454;&#29616;&#20302;&#24310;&#36831;&#30340;&#23454;&#26102;&#24615;&#33021;&#21644;&#36164;&#28304;&#20248;&#21270;&#65292;&#25552;&#20379;&#20102;&#25509;&#36817;&#26368;&#32456;&#29992;&#25143;&#29615;&#22659;&#30340;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#26426;&#22120;&#23398;&#20064;(ML)&#20316;&#20026;&#36817;&#24180;&#26469;AI&#20013;&#26368;&#20808;&#36827;&#30340;&#20998;&#25903;&#65292;&#22312;&#36793;&#32536;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;&#39537;&#21160;&#30340;ML&#35299;&#20915;&#26041;&#26696;&#26356;&#21152;&#22797;&#26434;&#65292;&#22240;&#20026;&#23427;&#21516;&#26102;&#32771;&#34385;&#21040;&#36793;&#32536;&#35745;&#31639;&#21644;AI&#39046;&#22495;&#30340;&#32422;&#26463;&#65292;&#24182;&#19988;&#26399;&#26395;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#22312;&#36793;&#32536;ML&#30340;&#38656;&#27714;&#26041;&#38754;&#39640;&#25928;&#19988;&#36866;&#24212;&#24615;&#24378;&#65292;&#22914;&#25968;&#25454;&#22788;&#29702;&#65292;&#27169;&#22411;&#21387;&#32553;&#65292;&#20998;&#24067;&#24335;&#25512;&#29702;&#21644;&#39640;&#32423;&#23398;&#20064;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#21463;&#21040;&#20102;&#36793;&#32536;ML&#30340;&#20851;&#27880;&#65292;&#20294;&#25105;&#20204;&#27880;&#24847;&#21040;&#32570;&#20047;&#23545;&#29616;&#26377;&#36793;&#32536;ML&#25216;&#26415;&#30340;&#23436;&#25972;&#35843;&#26597;&#65292;&#20197;&#25552;&#20379;&#19968;&#20010;&#20849;&#21516;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The union of Edge Computing (EC) and Artificial Intelligence (AI) has brought forward the Edge AI concept to provide intelligent solutions close to the end-user environment, for privacy preservation, low latency to real-time performance, and resource optimization. Machine Learning (ML), as the most advanced branch of AI in the past few years, has shown encouraging results and applications in the edge environment. Nevertheless, edge-powered ML solutions are more complex to realize due to the joint constraints from both edge computing and AI domains, and the corresponding solutions are expected to be efficient and adapted in technologies such as data processing, model compression, distributed inference, and advanced learning paradigms for Edge ML requirements. Despite the fact that a great deal of the attention garnered by Edge ML is gained in both the academic and industrial communities, we noticed the lack of a complete survey on existing Edge ML technologies to provide a common unders
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22870;&#21169;&#20195;&#29702;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#24314;&#35758;&#30340;&#30446;&#26631;&#26469;&#22609;&#36896;&#25506;&#32034;&#31574;&#30053;&#65292;&#20351;&#20195;&#29702;&#26397;&#30528;&#20154;&#31867;&#26377;&#24847;&#20041;&#19988;&#21487;&#33021;&#26377;&#29992;&#30340;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#65292;&#26080;&#38656;&#20154;&#31867;&#30340;&#20171;&#20837;&#12290;</title><link>http://arxiv.org/abs/2302.06692</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Guiding Pretraining in Reinforcement Learning with Large Language Models. (arXiv:2302.06692v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06692
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22870;&#21169;&#20195;&#29702;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#24314;&#35758;&#30340;&#30446;&#26631;&#26469;&#22609;&#36896;&#25506;&#32034;&#31574;&#30053;&#65292;&#20351;&#20195;&#29702;&#26397;&#30528;&#20154;&#31867;&#26377;&#24847;&#20041;&#19988;&#21487;&#33021;&#26377;&#29992;&#30340;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#65292;&#26080;&#38656;&#20154;&#31867;&#30340;&#20171;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#27809;&#26377;&#23494;&#38598;&#19988;&#24418;&#29366;&#33391;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#36890;&#24120;&#24456;&#22256;&#38590;&#12290;&#36890;&#36807;&#22870;&#21169;&#20195;&#29702;&#35775;&#38382;&#26032;&#39062;&#29366;&#24577;&#25110;&#36716;&#25442;&#30340;&#20869;&#22312;&#21160;&#26426;&#25506;&#32034;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#20294;&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#26377;&#38480;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#32972;&#26223;&#30693;&#35782;&#26469;&#22609;&#36896;&#25506;&#32034;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;ELLM&#65288;&#20351;&#29992;LLMs&#36827;&#34892;&#25506;&#32034;&#65289;&#65292;&#36890;&#36807;&#32473;&#20195;&#29702;&#22870;&#21169;&#20854;&#36798;&#25104;&#30001;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#20195;&#29702;&#24403;&#21069;&#29366;&#24577;&#25551;&#36848;&#25152;&#25552;&#20986;&#30340;&#30446;&#26631;&#65292;&#24341;&#23548;&#20195;&#29702;&#26397;&#30528;&#20154;&#31867;&#26377;&#24847;&#20041;&#19988;&#21487;&#33021;&#26377;&#29992;&#30340;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#65292;&#26080;&#38656;&#20154;&#31867;&#30340;&#20171;&#20837;&#12290;&#25105;&#20204;&#22312;Crafter&#28216;&#25103;&#29615;&#22659;&#21644;Housekeep&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#20013;&#35780;&#20272;&#20102;ELLM&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;ELLM&#35757;&#32451;&#30340;&#20195;&#29702;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26377;&#26356;&#22909;&#30340;&#24120;&#35782;&#34892;&#20026;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#36890;&#24120;&#19982;&#20154;&#31867;&#34892;&#20026;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning algorithms typically struggle in the absence of a dense, well-shaped reward function. Intrinsically motivated exploration methods address this limitation by rewarding agents for visiting novel states or transitions, but these methods offer limited benefits in large environments where most discovered novelty is irrelevant for downstream tasks. We describe a method that uses background knowledge from text corpora to shape exploration. This method, called ELLM (Exploring with LLMs) rewards an agent for achieving goals suggested by a language model prompted with a description of the agent's current state. By leveraging large-scale language model pretraining, ELLM guides agents toward human-meaningful and plausibly useful behaviors without requiring a human in the loop. We evaluate ELLM in the Crafter game environment and the Housekeep robotic simulator, showing that ELLM-trained agents have better coverage of common-sense behaviors during pretraining and usually matc
&lt;/p&gt;</description></item><item><title>&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.02173</link><description>&lt;p&gt;
&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning based Time Series Analysis with Frequency Transformation. (arXiv:2302.02173v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02173
&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#39057;&#29575;&#21464;&#25442;&#30340;&#20248;&#21183;&#65292;&#22914;&#39640;&#25928;&#24615;&#21644;&#20840;&#23616;&#35270;&#35282;&#65292;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#34987;&#36805;&#36895;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#23637;&#31034;&#20102;&#39057;&#29575;&#21464;&#25442;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#21644;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#31995;&#32479;&#22238;&#39038;&#21644;&#28145;&#20837;&#20998;&#26512;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#39057;&#29575;&#21464;&#25442;&#21487;&#20197;&#25552;&#21319;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25928;&#26524;&#65292;&#20197;&#21450;&#23427;&#22312;&#35813;&#39046;&#22495;&#30340;&#38480;&#21046;&#26159;&#20160;&#20040;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#31995;&#32479;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20027;&#35201;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, frequency transformation (FT) has been increasingly incorporated into deep learning models to significantly enhance state-of-the-art accuracy and efficiency in time series analysis. The advantages of FT, such as high efficiency and a global view, have been rapidly explored and exploited in various time series tasks and applications, demonstrating the promising potential of FT as a new deep learning paradigm for time series analysis. Despite the growing attention and the proliferation of research in this emerging field, there is currently a lack of a systematic review and in-depth analysis of deep learning-based time series models with FT. It is also unclear why FT can enhance time series analysis and what its limitations in the field are. To address these gaps, we present a comprehensive review that systematically investigates and summarizes the recent research advancements in deep learning-based time series analysis with FT. Specifically, we explore the primary approaches us
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#25506;&#35752;&#20102;&#22240;&#26524;&#24615;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#26032;&#20852;&#21327;&#21516;&#20316;&#29992;&#65292;&#38416;&#26126;&#20102;&#23558;&#22240;&#26524;&#24615;&#21407;&#21017;&#34701;&#20837;DGM&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#24212;&#29992;&#22240;&#26524;&#24615;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;</title><link>http://arxiv.org/abs/2301.12351</link><description>&lt;p&gt;
&#22240;&#26524;&#24615;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26032;&#20852;&#21327;&#21516;&#20316;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Emerging Synergies in Causality and Deep Generative Models: A Survey. (arXiv:2301.12351v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12351
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#25506;&#35752;&#20102;&#22240;&#26524;&#24615;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#26032;&#20852;&#21327;&#21516;&#20316;&#29992;&#65292;&#38416;&#26126;&#20102;&#23558;&#22240;&#26524;&#24615;&#21407;&#21017;&#34701;&#20837;DGM&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#24212;&#29992;&#22240;&#26524;&#24615;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#20102;&#35299;&#21644;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65288;DGP&#65289;&#30340;&#36861;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGM&#65289;&#22312;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36890;&#24120;&#22312;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;&#32780;&#22240;&#26524;&#24615;&#21017;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#39537;&#21160;&#25968;&#25454;&#29983;&#25104;&#30340;&#26426;&#21046;&#65292;&#24182;&#31361;&#26174;&#20102;&#36825;&#20123;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#22240;&#26524;&#25928;&#24212;&#21160;&#21147;&#23398;&#12290;&#34429;&#28982;&#22240;&#26524;&#24615;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#22806;&#25512;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#21364;&#38754;&#20020;&#30528;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;&#24847;&#35782;&#21040;&#23427;&#20204;&#20043;&#38388;&#30340;&#21327;&#21516;&#28508;&#21147;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22240;&#26524;&#24615;&#21644;DGM&#30340;&#20132;&#27719;&#28857;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#22240;&#26524;&#24615;&#21407;&#21017;&#22312;DGM&#20013;&#30340;&#25972;&#21512;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;DGM&#36827;&#34892;&#22240;&#26524;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#22240;&#26524;&#24615;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26032;&#20852;&#30740;&#31350;&#21069;&#27839;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#29983;&#25104;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26041;&#27861;&#35770;&#65292;&#31361;&#20986;&#20102;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of artificial intelligence (AI), the quest to understand and model data-generating processes (DGPs) is of paramount importance. Deep generative models (DGMs) have proven adept in capturing complex data distributions but often fall short in generalization and interpretability. On the other hand, causality offers a structured lens to comprehend the mechanisms driving data generation and highlights the causal-effect dynamics inherent in these processes. While causality excels in interpretability and the ability to extrapolate, it grapples with intricacies of high-dimensional spaces. Recognizing the synergistic potential, we delve into the confluence of causality and DGMs. We elucidate the integration of causal principles within DGMs, investigate causal identification using DGMs, and navigate an emerging research frontier of causality in large-scale generative models, particularly generative large language models (LLMs). We offer insights into methodologies, highlight open cha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04688</link><description>&lt;p&gt;
BAFFLE: &#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#35797;&#38169;&#32463;&#39564;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#31163;&#32447;RL&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;RL&#33539;&#20363;&#65292;&#22240;&#20026;&#23427;&#33410;&#30465;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447;RL&#20013;&#65292;&#25968;&#25454;&#25552;&#20379;&#32773;&#20849;&#20139;&#22823;&#35268;&#27169;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20182;&#20154;&#21487;&#20197;&#22312;&#19981;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#33539;&#20363;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#12289;&#33258;&#21160;&#39550;&#39542;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36739;&#23569;&#20851;&#27880;&#30740;&#31350;&#31163;&#32447;RL&#31995;&#32479;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#20851;&#27880;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#19968;&#20123;&#25200;&#21160;&#34987;&#28155;&#21152;&#21040;&#25968;&#25454;&#65288;&#35266;&#27979;&#20540;&#65289;&#20013;&#65292;&#20351;&#24471;&#22312;&#32473;&#23450;&#27491;&#24120;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#37319;&#21462;&#39640;&#22870;&#21169;&#30340;&#21160;&#20316;&#65292;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BAFFLE&#65288;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research has focused on the Reinforcement Learning (RL) methods which allow the agent to learn from trial-and-error experiences gathered during the interaction with the environment. Recently, offline RL becomes a popular RL paradigm because it saves the interactions with environments. In offline RL, data providers share large pre-collected datasets, and others can train high-quality agents without interacting with the environments. This paradigm has demonstrated effectiveness in critical tasks like robot control, autonomous driving, etc. However, less attention is paid to investigating the security threats to the offline RL system. This paper focuses on backdoor attacks, where some perturbations are added to the data (observations) such that given normal observations, the agent takes high-rewards actions, and low-reward actions on observations injected with triggers. In this paper, we propose Baffle (Backdoor Attack for Offline Reinforcement Learning), an approach tha
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#22810;&#24863;&#23448;&#38598;&#25104;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#25509;&#21463;&#36866;&#24403;&#30456;&#20851;&#20449;&#21495;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#24178;&#25200;&#23398;&#20064;&#36807;&#31243;&#21487;&#33021;&#20250;&#27704;&#20037;&#25439;&#23475;&#25216;&#33021;&#30340;&#21457;&#23637;&#12290;&#26089;&#26399;&#30636;&#24577;&#21160;&#21147;&#23398;&#23545;&#26368;&#32456;&#30340;&#31995;&#32479;&#24615;&#33021;&#21644;&#23398;&#20064;&#34920;&#31034;&#20855;&#26377;&#20915;&#23450;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2210.04643</link><description>&lt;p&gt;
&#22810;&#24863;&#23448;&#38598;&#25104;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#23398;&#20064;&#26399;
&lt;/p&gt;
&lt;p&gt;
Critical Learning Periods for Multisensory Integration in Deep Networks. (arXiv:2210.04643v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04643
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#24863;&#23448;&#38598;&#25104;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#25509;&#21463;&#36866;&#24403;&#30456;&#20851;&#20449;&#21495;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#24178;&#25200;&#23398;&#20064;&#36807;&#31243;&#21487;&#33021;&#20250;&#27704;&#20037;&#25439;&#23475;&#25216;&#33021;&#30340;&#21457;&#23637;&#12290;&#26089;&#26399;&#30636;&#24577;&#21160;&#21147;&#23398;&#23545;&#26368;&#32456;&#30340;&#31995;&#32479;&#24615;&#33021;&#21644;&#23398;&#20064;&#34920;&#31034;&#20855;&#26377;&#20915;&#23450;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#20449;&#24687;&#30340;&#33021;&#21147;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#25509;&#21463;&#36866;&#24403;&#30456;&#20851;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#33267;&#20851;&#37325;&#35201;&#12290;&#24178;&#25200;&#23398;&#20064;&#36807;&#31243;&#21487;&#33021;&#20250;&#27704;&#20037;&#25439;&#23475;&#25216;&#33021;&#30340;&#21457;&#23637;&#65292;&#26080;&#35770;&#26159;&#22312;&#20154;&#36896;&#31995;&#32479;&#36824;&#26159;&#29983;&#29289;&#31995;&#32479;&#20013;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#20851;&#38190;&#23398;&#20064;&#26399;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20851;&#38190;&#23398;&#20064;&#26399;&#28304;&#20110;&#22797;&#26434;&#32780;&#19981;&#31283;&#23450;&#30340;&#26089;&#26399;&#30636;&#24577;&#21160;&#21147;&#23398;&#65292;&#36825;&#23545;&#35757;&#32451;&#31995;&#32479;&#30340;&#26368;&#32456;&#24615;&#33021;&#21644;&#23398;&#20064;&#34920;&#31034;&#20855;&#26377;&#20915;&#23450;&#24615;&#24433;&#21709;&#12290;&#36825;&#19968;&#35777;&#25454;&#25361;&#25112;&#20102;&#36890;&#36807;&#20998;&#26512;&#23485;&#32780;&#27973;&#30340;&#32593;&#32476;&#24471;&#20986;&#30340;&#35748;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26089;&#26399;&#23398;&#20064;&#21160;&#24577;&#26159;&#31616;&#21333;&#30340;&#12289;&#31867;&#20284;&#20110;&#32447;&#24615;&#27169;&#22411;&#30340;&#35266;&#28857;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#22312;&#22810;&#28304;&#38598;&#25104;&#26041;&#38754;&#20063;&#20250;&#20986;&#29616;&#20851;&#38190;&#23398;&#20064;&#26399;&#65292;&#32780;&#27973;&#23618;&#32593;&#32476;&#21017;&#19981;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the ability of a neural network to integrate information from diverse sources hinges critically on being exposed to properly correlated signals during the early phases of training. Interfering with the learning process during this initial stage can permanently impair the development of a skill, both in artificial and biological systems where the phenomenon is known as a critical learning period. We show that critical periods arise from the complex and unstable early transient dynamics, which are decisive of final performance of the trained system and their learned representations. This evidence challenges the view, engendered by analysis of wide and shallow networks, that early learning dynamics of neural networks are simple, akin to those of a linear model. Indeed, we show that even deep linear networks exhibit critical learning periods for multi-source integration, while shallow networks do not. To better understand how the internal representations change according to di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#31995;&#32479;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#20013;&#22269;&#19971;&#20010;&#20379;&#24212;&#21830;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#31995;&#32479;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#22312;&#27169;&#22411;&#29256;&#26435;&#20445;&#25252;&#21644;&#25968;&#23383;&#35777;&#25454;&#21462;&#35777;&#31561;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2208.10489</link><description>&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#31995;&#32479;&#25351;&#32441;&#35782;&#21035;&#65306;&#21021;&#22987;&#25968;&#25454;&#38598;&#19982;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
System Fingerprint Recognition for Deepfake Audio: An Initial Dataset and Investigation. (arXiv:2208.10489v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#31995;&#32479;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#20013;&#22269;&#19971;&#20010;&#20379;&#24212;&#21830;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#31995;&#32479;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#22312;&#27169;&#22411;&#29256;&#26435;&#20445;&#25252;&#21644;&#25968;&#23383;&#35777;&#25454;&#21462;&#35777;&#31561;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#37325;&#22823;&#23041;&#32961;&#65292;&#20363;&#22914;&#24694;&#24847;&#20869;&#23481;&#25805;&#32437;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#20986;&#29616;&#20102;&#65292;&#26088;&#22312;&#26816;&#27979;&#25152;&#35859;&#30340;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#23545;&#30495;&#23454;&#38899;&#39057;&#21644;&#20266;&#36896;&#38899;&#39057;&#36827;&#34892;&#20108;&#20803;&#26816;&#27979;&#12290;&#22312;&#27169;&#22411;&#29256;&#26435;&#20445;&#25252;&#21644;&#25968;&#23383;&#35777;&#25454;&#21462;&#35777;&#31561;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#30693;&#36947;&#29983;&#25104;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#24037;&#20855;&#25110;&#27169;&#22411;&#26469;&#35299;&#37322;&#20915;&#31574;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#35782;&#21035;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#31995;&#32479;&#25351;&#32441;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31995;&#32479;&#25351;&#32441;&#35782;&#21035;&#65288;SFR&#65289;&#30340;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#25105;&#20204;&#20174;&#20351;&#29992;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#19971;&#20010;&#20013;&#22269;&#20379;&#24212;&#21830;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#20013;&#25910;&#38598;&#20102;&#35813;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#28165;&#26224;&#21644;&#21387;&#32553;&#38598;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20419;&#36827;&#31995;&#32479;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22806;&#37096;&#21442;&#32771;&#38899;&#39057;&#65292;&#20197;&#20415;&#36827;&#34892;&#35780;&#20272;&#21644;&#23545;&#27604;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of deep speech synthesis models has posed significant threats to society such as malicious content manipulation. Therefore, many studies have emerged to detect the so-called deepfake audio. However, existing works focus on the binary detection of real audio and fake audio. In real-world scenarios such as model copyright protection and digital evidence forensics, it is needed to know what tool or model generated the deepfake audio to explain the decision. This motivates us to ask: Can we recognize the system fingerprints of deepfake audio? In this paper, we present the first deepfake audio dataset for system fingerprint recognition (SFR) and conduct an initial investigation. We collected the dataset from the speech synthesis systems of seven Chinese vendors that use the latest state-of-the-art deep learning technologies, including both clean and compressed sets. In addition, to facilitate the further development of system fingerprint recognition methods, we provide ex
&lt;/p&gt;</description></item><item><title>VQA-GNN&#26159;&#19968;&#31181;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#22810;&#27169;&#24577;&#30693;&#35782;&#20043;&#38388;&#36827;&#34892;&#21452;&#21521;&#34701;&#21512;&#30340;&#26032;&#30340;VQA&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2205.11501</link><description>&lt;p&gt;
VQA-GNN: &#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#22810;&#27169;&#24577;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks for Visual Question Answering. (arXiv:2205.11501v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11501
&lt;/p&gt;
&lt;p&gt;
VQA-GNN&#26159;&#19968;&#31181;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#22810;&#27169;&#24577;&#30693;&#35782;&#20043;&#38388;&#36827;&#34892;&#21452;&#21521;&#34701;&#21512;&#30340;&#26032;&#30340;VQA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572; (VQA) &#38656;&#35201;&#31995;&#32479;&#36890;&#36807;&#32479;&#19968;&#38750;&#32467;&#26500;&#21270;&#65288;&#20363;&#22914;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#19978;&#19979;&#25991; "QA&#19978;&#19979;&#25991;"&#65289;&#21644;&#32467;&#26500;&#21270;&#65288;&#20363;&#22914;QA&#19978;&#19979;&#25991;&#21644;&#22330;&#26223;&#30340;&#30693;&#35782;&#22270; "&#27010;&#24565;&#22270;"&#65289;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#36827;&#34892;&#27010;&#24565;&#32423;&#21035;&#30340;&#25512;&#29702;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#36830;&#25509;&#30456;&#24212;&#30340;&#35270;&#35273;&#33410;&#28857;&#21644;&#27010;&#24565;&#33410;&#28857;&#26469;&#21512;&#24182;&#22330;&#26223;&#22270;&#21644;&#27010;&#24565;&#22270;&#65292;&#28982;&#21518;&#23558;QA&#19978;&#19979;&#25991;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#33021;&#20174;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#21040;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#21333;&#21521;&#34701;&#21512;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#25417;&#22810;&#27169;&#24577;&#30693;&#35782;&#30340;&#24322;&#26500;&#32852;&#21512;&#25512;&#29702;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#36827;&#34892;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VQA-GNN&#65292;&#19968;&#31181;&#26032;&#30340;VQA&#27169;&#22411;&#65292;&#23427;&#22312;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#22810;&#27169;&#24577;&#30693;&#35782;&#20043;&#38388;&#36827;&#34892;&#21452;&#21521;&#34701;&#21512;&#65292;&#20197;&#33719;&#24471;&#32479;&#19968;&#30340;&#30693;&#35782;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36229;&#38142;&#25509;&#36830;&#25509;&#22330;&#26223;&#22270;&#21644;&#27010;&#24565;&#22270;&#65292;&#23454;&#29616;&#20102;&#20114;&#36830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) requires systems to perform concept-level reasoning by unifying unstructured (e.g., the context in question and answer; "QA context") and structured (e.g., knowledge graph for the QA context and scene; "concept graph") multimodal knowledge. Existing works typically combine a scene graph and a concept graph of the scene by connecting corresponding visual nodes and concept nodes, then incorporate the QA context representation to perform question answering. However, these methods only perform a unidirectional fusion from unstructured knowledge to structured knowledge, limiting their potential to capture joint reasoning over the heterogeneous modalities of knowledge. To perform more expressive reasoning, we propose VQA-GNN, a new VQA model that performs bidirectional fusion between unstructured and structured multimodal knowledge to obtain unified knowledge representations. Specifically, we inter-connect the scene graph and the concept graph through a super 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20845;&#20010;&#24230;&#37327;&#26469;&#35780;&#20272;&#22522;&#20110;&#26799;&#24230;&#12289;&#20256;&#25773;&#25110;&#24178;&#25200;&#30340;&#20107;&#21518;&#21487;&#35270;&#21270;&#35299;&#37322;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#20449;&#24230;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.07861</link><description>&lt;p&gt;
&#19981;&#35201;&#35823;&#20250;&#25105;&#65306;&#22914;&#20309;&#23558;&#28145;&#24230;&#35270;&#35273;&#35299;&#37322;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Don't Get Me Wrong: How to Apply Deep Visual Interpretations to Time Series. (arXiv:2203.07861v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07861
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20845;&#20010;&#24230;&#37327;&#26469;&#35780;&#20272;&#22522;&#20110;&#26799;&#24230;&#12289;&#20256;&#25773;&#25110;&#24178;&#25200;&#30340;&#20107;&#21518;&#21487;&#35270;&#21270;&#35299;&#37322;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#20449;&#24230;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#27491;&#30830;&#35299;&#37322;&#21644;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#12290;&#38024;&#23545;&#22270;&#20687;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35299;&#37322;&#24615;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;&#20801;&#35768;&#39046;&#22495;&#19987;&#23478;&#39564;&#35777;&#21644;&#29702;&#35299;&#20960;&#20046;&#20219;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#25512;&#24191;&#21040;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#23427;&#20204;&#22312;&#26412;&#36136;&#19978;&#26356;&#21152;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#12290;&#19968;&#20010;&#21487;&#35270;&#21270;&#35299;&#37322;&#26159;&#21542;&#35299;&#37322;&#20102;&#26377;&#25928;&#30340;&#25512;&#29702;&#25110;&#25429;&#25417;&#20102;&#23454;&#38469;&#29305;&#24449;&#26159;&#38590;&#20197;&#21028;&#26029;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#23458;&#35266;&#35780;&#20272;&#26469;&#33719;&#24471;&#21487;&#20449;&#30340;&#36136;&#37327;&#25351;&#26631;&#65292;&#32780;&#19981;&#26159;&#30450;&#30446;&#20449;&#20219;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#20845;&#20010;&#27491;&#20132;&#24230;&#37327;&#65292;&#29992;&#20110;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#30340;&#22522;&#20110;&#26799;&#24230;&#12289;&#20256;&#25773;&#25110;&#24178;&#25200;&#30340;&#20107;&#21518;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;&#12290;&#23454;&#39564;&#30740;&#31350;&#21253;&#25324;&#20102;&#24120;&#35265;&#30340;&#26102;&#38388;&#24207;&#21015;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#20061;&#31181;&#21487;&#35270;&#21270;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;UCR r&#31561;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#36825;&#20123;&#21487;&#35270;&#21270;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The correct interpretation and understanding of deep learning models are essential in many applications. Explanatory visual interpretation approaches for image, and natural language processing allow domain experts to validate and understand almost any deep learning model. However, they fall short when generalizing to arbitrary time series, which is inherently less intuitive and more diverse. Whether a visualization explains valid reasoning or captures the actual features is difficult to judge. Hence, instead of blind trust, we need an objective evaluation to obtain trustworthy quality metrics. We propose a framework of six orthogonal metrics for gradient-, propagation- or perturbation-based post-hoc visual interpretation methods for time series classification and segmentation tasks. An experimental study includes popular neural network architectures for time series and nine visual interpretation methods. We evaluate the visual interpretation methods with diverse datasets from the UCR r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#27979;&#24230;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#25972;&#20307;&#27979;&#24230;&#26469;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#23545;&#35937;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#29983;&#25104;&#31639;&#27861;&#20013;&#23545;&#35937;&#37325;&#22797;&#21644;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.09573</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diversity in deep generative models and generative AI. (arXiv:2202.09573v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#27979;&#24230;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#25972;&#20307;&#27979;&#24230;&#26469;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#23545;&#35937;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#29983;&#25104;&#31639;&#27861;&#20013;&#23545;&#35937;&#37325;&#22797;&#21644;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#29983;&#25104;&#31639;&#27861;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#22312;&#26500;&#24314;&#19982;&#35757;&#32451;&#38598;&#20013;&#30456;&#20284;&#30340;&#23545;&#35937;&#26102;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#26032;&#23545;&#35937;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38544;&#34255;&#32467;&#26500;&#30340;&#29702;&#35299;&#65292;&#28982;&#21518;&#20174;&#22810;&#32500;&#27491;&#24577;&#21464;&#37327;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#23588;&#20854;&#26159;&#65292;&#27599;&#20010;&#26679;&#26412;&#37117;&#26159;&#29420;&#31435;&#30340;&#65292;&#21487;&#33021;&#20250;&#37325;&#22797;&#25552;&#20986;&#30456;&#21516;&#31867;&#22411;&#30340;&#23545;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#27979;&#24230;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23558;&#32473;&#23450;&#30446;&#26631;&#27979;&#24230;&#36817;&#20284;&#20026;&#25972;&#20307;&#26469;&#29983;&#25104;&#26032;&#23545;&#35937;&#65292;&#24182;&#19988;&#33021;&#22815;&#36991;&#20813;&#20174;&#35813;&#20998;&#24067;&#20013;&#24050;&#32463;&#32472;&#21046;&#30340;&#20803;&#32032;&#12290;&#36825;&#30830;&#20445;&#20102;&#29983;&#25104;&#23545;&#35937;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine learning generative algorithms such as Generative Adversarial Networks (GAN) and Variational Auto-Encoders (VAE) show impressive results when constructing objects similar to those in a training ensemble. However, the generation of new objects builds mainly on the understanding of the hidden structure of the training dataset followed by a sampling from a multi-dimensional normal variable. In particular each sample is independent from the others and can repeatedly propose same type of objects. To cure this drawback we introduce a kernel-based measure quantization method that can produce new objects from a given target measure by approximating it as a whole and even staying away from elements already drawn from that distribution. This ensures a better diversity of the produced objects. The method is tested on classic machine learning benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HAKE&#30340;&#30693;&#35782;&#24341;&#25806;&#65292;&#29992;&#20110;&#20154;&#31867;&#27963;&#21160;&#29702;&#35299;&#12290;&#35813;&#24341;&#25806;&#36890;&#36807;&#23558;&#20687;&#32032;&#26144;&#23556;&#21040;&#20013;&#38388;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#25512;&#26029;&#35821;&#20041;&#65292;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2202.06851</link><description>&lt;p&gt;
HAKE:&#20154;&#31867;&#27963;&#21160;&#29702;&#35299;&#30340;&#30693;&#35782;&#24341;&#25806;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
HAKE: A Knowledge Engine Foundation for Human Activity Understanding. (arXiv:2202.06851v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HAKE&#30340;&#30693;&#35782;&#24341;&#25806;&#65292;&#29992;&#20110;&#20154;&#31867;&#27963;&#21160;&#29702;&#35299;&#12290;&#35813;&#24341;&#25806;&#36890;&#36807;&#23558;&#20687;&#32032;&#26144;&#23556;&#21040;&#20013;&#38388;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#25512;&#26029;&#35821;&#20041;&#65292;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#29702;&#35299;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#28041;&#21450;&#21040;&#20581;&#24247;&#25252;&#29702;&#21644;&#34892;&#20026;&#20998;&#26512;&#31561;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#36890;&#24120;&#65292;&#20687;&#29289;&#20307;&#35782;&#21035;&#19968;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#35797;&#22270;&#30452;&#25509;&#23558;&#20687;&#32032;&#26144;&#23556;&#21040;&#35821;&#20041;&#65292;&#20294;&#27963;&#21160;&#27169;&#24335;&#19982;&#29289;&#20307;&#27169;&#24335;&#38750;&#24120;&#19981;&#21516;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#23558;&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#23558;&#20687;&#32032;&#26144;&#23556;&#21040;&#30001;&#21407;&#23376;&#27963;&#21160;&#22522;&#20803;&#26500;&#25104;&#30340;&#20013;&#38388;&#31354;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#36923;&#36753;&#35268;&#21017;&#23545;&#26816;&#27979;&#21040;&#30340;&#22522;&#20803;&#36827;&#34892;&#32534;&#31243;&#20197;&#25512;&#26029;&#35821;&#20041;&#12290;&#20026;&#20102;&#24471;&#21040;&#19968;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#22522;&#20803;&#31354;&#38388;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;26+M&#20010;&#22522;&#20803;&#26631;&#31614;&#21644;&#36923;&#36753;&#35268;&#21017;&#30340;&#30693;&#35782;&#24211;&#65292;&#36825;&#20123;&#35268;&#21017;&#26159;&#36890;&#36807;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#25110;&#33258;&#21160;&#21457;&#29616;&#24471;&#21040;&#30340;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#20154;&#31867;&#27963;&#21160;&#30693;&#35782;&#24341;&#25806;&#65288;HAKE&#65289;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity understanding is of widespread interest in artificial intelligence and spans diverse applications like health care and behavior analysis. Although there have been advances in deep learning, it remains challenging. The object recognition-like solutions usually try to map pixels to semantics directly, but activity patterns are much different from object patterns, thus hindering success. In this work, we propose a novel paradigm to reformulate this task in two stages: first mapping pixels to an intermediate space spanned by atomic activity primitives, then programming detected primitives with interpretable logic rules to infer semantics. To afford a representative primitive space, we build a knowledge base including 26+ M primitive labels and logic rules from human priors or automatic discovering. Our framework, the Human Activity Knowledge Engine (HAKE), exhibits superior generalization ability and performance upon canonical methods on challenging benchmarks. Code and data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#27169;&#22411;&#26469;&#30740;&#31350;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#25112;&#30053;&#20114;&#21160;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#23548;&#33268;&#31639;&#27861;&#21246;&#32467;&#20986;&#29616;&#30340;&#26426;&#21046;&#12290;&#36890;&#36807;&#33258;&#21457;&#32806;&#21512;&#65292;&#31639;&#27861;&#21608;&#26399;&#24615;&#22320;&#21327;&#35843;&#34892;&#21160;&#65292;&#36798;&#21040;&#26356;&#39640;&#21033;&#28070;&#12290;&#35813;&#27169;&#22411;&#30340;&#21442;&#25968;&#21487;&#39044;&#27979;&#32479;&#35745;&#20851;&#32852;&#30340;&#20986;&#29616;&#21644;&#26377;&#21033;&#20110;&#31639;&#27861;&#21246;&#32467;&#30340;&#24066;&#22330;&#32467;&#26500;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#33258;&#21457;&#32806;&#21512;&#22914;&#20309;&#22312;&#20215;&#26684;&#21644;&#24066;&#22330;&#20221;&#39069;&#19978;&#32500;&#25345;&#21246;&#32467;&#65292;&#24182;&#24212;&#29992;&#20110;&#35774;&#35745;&#31639;&#27861;&#24066;&#22330;&#12290;</title><link>http://arxiv.org/abs/2202.05946</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#32479;&#35745;&#21246;&#32467;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence and Statistical Collusion. (arXiv:2202.05946v4 [econ.TH] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#27169;&#22411;&#26469;&#30740;&#31350;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#25112;&#30053;&#20114;&#21160;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#23548;&#33268;&#31639;&#27861;&#21246;&#32467;&#20986;&#29616;&#30340;&#26426;&#21046;&#12290;&#36890;&#36807;&#33258;&#21457;&#32806;&#21512;&#65292;&#31639;&#27861;&#21608;&#26399;&#24615;&#22320;&#21327;&#35843;&#34892;&#21160;&#65292;&#36798;&#21040;&#26356;&#39640;&#21033;&#28070;&#12290;&#35813;&#27169;&#22411;&#30340;&#21442;&#25968;&#21487;&#39044;&#27979;&#32479;&#35745;&#20851;&#32852;&#30340;&#20986;&#29616;&#21644;&#26377;&#21033;&#20110;&#31639;&#27861;&#21246;&#32467;&#30340;&#24066;&#22330;&#32467;&#26500;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#33258;&#21457;&#32806;&#21512;&#22914;&#20309;&#22312;&#20215;&#26684;&#21644;&#24066;&#22330;&#20221;&#39069;&#19978;&#32500;&#25345;&#21246;&#32467;&#65292;&#24182;&#24212;&#29992;&#20110;&#35774;&#35745;&#31639;&#27861;&#24066;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#27169;&#22411;&#26469;&#30740;&#31350;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#25112;&#30053;&#20114;&#21160;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#23548;&#33268;&#31639;&#27861;&#21246;&#32467;&#20986;&#29616;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#31639;&#27861;&#21608;&#26399;&#24615;&#22320;&#21327;&#35843;&#34892;&#21160;&#65292;&#36825;&#20123;&#34892;&#21160;&#27604;&#38745;&#24577;&#32435;&#20160;&#22343;&#34913;&#26356;&#20855;&#21033;&#28070;&#24615;&#12290;&#36825;&#31181;&#26032;&#30340;&#21246;&#32467;&#28192;&#36947;&#20381;&#36182;&#20110;&#31639;&#27861;&#20272;&#35745;&#20013;&#30340;&#20869;&#29983;&#32479;&#35745;&#20851;&#32852;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#33258;&#21457;&#32806;&#21512;&#12290;&#27169;&#22411;&#30340;&#21442;&#25968;&#39044;&#27979;&#20102;&#32479;&#35745;&#20851;&#32852;&#26159;&#21542;&#20250;&#20986;&#29616;&#65292;&#20197;&#21450;&#20160;&#20040;&#24066;&#22330;&#32467;&#26500;&#26377;&#21161;&#20110;&#31639;&#27861;&#21246;&#32467;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#21457;&#32806;&#21512;&#22914;&#20309;&#32500;&#25345;&#20215;&#26684;&#21644;&#24066;&#22330;&#20221;&#39069;&#19978;&#30340;&#21246;&#32467;&#65292;&#36825;&#19982;&#25991;&#29486;&#20013;&#30340;&#23454;&#39564;&#35777;&#25454;&#30456;&#34917;&#20805;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#32467;&#26524;&#24212;&#29992;&#20110;&#35774;&#35745;&#31639;&#27861;&#24066;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a tractable model for studying strategic interactions between learning algorithms. We uncover a mechanism responsible for the emergence of algorithmic collusion. We observe that algorithms periodically coordinate on actions that are more profitable than static Nash equilibria. This novel collusive channel relies on an endogenous statistical linkage in the algorithms' estimates which we call spontaneous coupling. The model's parameters predict whether the statistical linkage will appear, and what market structures facilitate algorithmic collusion. We show that spontaneous coupling can sustain collusion in prices and market shares, complementing experimental findings in the literature. Finally, we apply our results to design algorithmic markets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#31639;&#27861;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#34892;&#20316;&#29289;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20302;&#33539;&#22260;&#20256;&#24863;&#22120;&#21644;&#23395;&#33410;&#21464;&#21270;&#30340;&#20449;&#24687;&#65292;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#29983;&#25104;&#36335;&#24452;&#65292;&#24182;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#25216;&#26415;&#21644;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#19988;&#25104;&#26412;&#25928;&#30410;&#30340;&#33258;&#20027;&#23548;&#33322;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2112.03816</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#31639;&#27861;&#27969;&#27700;&#32447;&#29992;&#20110;&#34892;&#20316;&#29289;&#33258;&#20027;&#23548;&#33322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops. (arXiv:2112.03816v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#31639;&#27861;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#34892;&#20316;&#29289;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20302;&#33539;&#22260;&#20256;&#24863;&#22120;&#21644;&#23395;&#33410;&#21464;&#21270;&#30340;&#20449;&#24687;&#65292;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#29983;&#25104;&#36335;&#24452;&#65292;&#24182;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#25216;&#26415;&#21644;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#19988;&#25104;&#26412;&#25928;&#30410;&#30340;&#33258;&#20027;&#23548;&#33322;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26114;&#36149;&#30340;&#20256;&#24863;&#22120;&#21644;&#20302;&#25928;&#30340;&#31639;&#27861;&#27969;&#31243;&#26174;&#33879;&#24433;&#21709;&#33258;&#20027;&#26426;&#22120;&#30340;&#24635;&#20307;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#32463;&#27982;&#23454;&#24800;&#30340;&#26426;&#22120;&#20154;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#36130;&#21153;&#24433;&#21709;&#26500;&#25104;&#20102;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#39046;&#22495;&#20013;&#37319;&#29992;&#26381;&#21153;&#26426;&#22120;&#20154;&#30340;&#22522;&#26412;&#35201;&#27714;&#12290;&#20854;&#20013;&#65292;&#22312;&#31934;&#30830;&#20892;&#19994;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21147;&#22270;&#35774;&#35745;&#20986;&#31283;&#20581;&#19988;&#25104;&#26412;&#25928;&#30410;&#30340;&#33258;&#20027;&#24179;&#21488;&#65292;&#20197;&#25552;&#20379;&#30495;&#27491;&#30340;&#22823;&#35268;&#27169;&#31454;&#20105;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#38024;&#23545;&#34892;&#20316;&#29289;&#33258;&#20027;&#23548;&#33322;&#30340;&#31639;&#27861;&#27969;&#27700;&#32447;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24212;&#23545;&#20302;&#33539;&#22260;&#20256;&#24863;&#22120;&#21644;&#23395;&#33410;&#21464;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#31283;&#20581;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#29983;&#25104;&#36866;&#29992;&#20110;&#33258;&#27835;&#26426;&#22120;&#30340;&#21487;&#34892;&#36335;&#24452;&#65292;&#20165;&#21033;&#29992;&#30000;&#22320;&#30340;&#21344;&#29992;&#26629;&#26684;&#22320;&#22270;&#20449;&#24687;&#35206;&#30422;&#25972;&#20010;&#20316;&#29289;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#25216;&#26415;&#21644;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expensive sensors and inefficient algorithmic pipelines significantly affect the overall cost of autonomous machines. However, affordable robotic solutions are essential to practical usage, and their financial impact constitutes a fundamental requirement to employ service robotics in most fields of application. Among all, researchers in the precision agriculture domain strive to devise robust and cost-effective autonomous platforms in order to provide genuinely large-scale competitive solutions. In this article, we present a complete algorithmic pipeline for row-based crops autonomous navigation, specifically designed to cope with low-range sensors and seasonal variations. Firstly, we build on a robust data-driven methodology to generate a viable path for the autonomous machine, covering the full extension of the crop with only the occupancy grid map information of the field. Moreover, our solution leverages on latest advancement of deep learning optimization techniques and synthetic g
&lt;/p&gt;</description></item><item><title>MixStyle&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#39046;&#22495;&#36716;&#31227;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23427;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28151;&#21512;&#20004;&#20010;&#38543;&#26426;&#23454;&#20363;&#30340;&#29305;&#24449;&#32479;&#35745;&#26469;&#21512;&#25104;&#26032;&#39046;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#12290;MixStyle&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#20110;&#21508;&#31867;&#23398;&#20064;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2107.02053</link><description>&lt;p&gt;
MixStyle&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#21644;&#36866;&#24212;&#24615;&#30340;&#32763;&#35793;&#21644;&#25688;&#35201;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
MixStyle Neural Networks for Domain Generalization and Adaptation. (arXiv:2107.02053v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.02053
&lt;/p&gt;
&lt;p&gt;
MixStyle&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#39046;&#22495;&#36716;&#31227;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23427;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28151;&#21512;&#20004;&#20010;&#38543;&#26426;&#23454;&#20363;&#30340;&#29305;&#24449;&#32479;&#35745;&#26469;&#21512;&#25104;&#26032;&#39046;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#12290;MixStyle&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#20110;&#21508;&#31867;&#23398;&#20064;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#20855;&#26377;&#39046;&#22495;&#36716;&#31227;&#30340;&#26410;&#35265;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MixStyle&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#21363;&#25554;&#21363;&#29992;&#65292;&#26080;&#38656;&#21442;&#25968;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#27867;&#21270;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#25910;&#38598;&#26356;&#22810;&#30340;&#25968;&#25454;&#25110;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#12290;MixStyle&#30340;&#35774;&#35745;&#24456;&#31616;&#21333;&#65306;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#22312;&#19968;&#20010;&#21069;&#21521;&#20256;&#25773;&#20013;&#23558;&#20004;&#20010;&#38543;&#26426;&#23454;&#20363;&#30340;&#29305;&#24449;&#32479;&#35745;&#28151;&#21512;&#12290;&#36825;&#20010;&#24819;&#27861;&#26159;&#22522;&#20110;&#26368;&#26032;&#30340;&#39118;&#26684;&#36716;&#25442;&#30740;&#31350;&#21457;&#29616;&#30340;&#65292;&#29305;&#24449;&#32479;&#35745;&#25429;&#25417;&#21040;&#22270;&#20687;&#39118;&#26684;&#20449;&#24687;&#65292;&#32780;&#22270;&#20687;&#39118;&#26684;&#26412;&#36136;&#19978;&#23450;&#20041;&#20102;&#35270;&#35273;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#28151;&#21512;&#29305;&#24449;&#32479;&#35745;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#21512;&#25104;&#26032;&#39046;&#22495;&#30340;&#19968;&#31181;&#39640;&#25928;&#26041;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#12290;MixStyle&#24456;&#23481;&#26131;&#29992;&#20960;&#34892;&#20195;&#30721;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#20462;&#25913;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#21253;&#25324;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#65292;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks do not generalize well to unseen data with domain shifts -- a longstanding problem in machine learning and AI. To overcome the problem, we propose MixStyle, a simple plug-and-play, parameter-free module that can improve domain generalization performance without the need to collect more data or increase model capacity. The design of MixStyle is simple: it mixes the feature statistics of two random instances in a single forward pass during training. The idea is grounded by the finding from recent style transfer research that feature statistics capture image style information, which essentially defines visual domains. Therefore, mixing feature statistics can be seen as an efficient way to synthesize new domains in the feature space, thus achieving data augmentation. MixStyle is easy to implement with a few lines of code, does not require modification to training objectives, and can fit a variety of learning paradigms including supervised domain generalization, semi-supervi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#29992;&#20110;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21547;&#31946;&#24615;&#22768;&#26126;&#26680;&#26597;&#38382;&#39064;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#35777;&#25454;&#27880;&#37322;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#36719;&#26631;&#31614;&#35777;&#25454;&#26680;&#26597;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#20998;&#26512;&#20013;&#21457;&#29616;&#20102;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2104.00640</link><description>&lt;p&gt;
AmbiFC: &#29992;&#35777;&#25454;&#26816;&#39564;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
AmbiFC: Fact-Checking Ambiguous Claims with Evidence. (arXiv:2104.00640v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.00640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#29992;&#20110;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21547;&#31946;&#24615;&#22768;&#26126;&#26680;&#26597;&#38382;&#39064;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#35777;&#25454;&#27880;&#37322;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#36719;&#26631;&#31614;&#35777;&#25454;&#26680;&#26597;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#20998;&#26512;&#20013;&#21457;&#29616;&#20102;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#24517;&#39035;&#23558;&#22768;&#26126;&#19982;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#36827;&#34892;&#27604;&#36739;&#20197;&#39044;&#27979;&#30495;&#23454;&#24615;&#12290;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#21487;&#33021;&#26080;&#27861;&#26126;&#30830;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#65292;&#24182;&#20135;&#29983;&#21508;&#31181;&#26377;&#25928;&#35299;&#37322;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;&#38656;&#35201;&#27169;&#22411;&#20026;&#27599;&#20010;&#22768;&#26126;&#39044;&#27979;&#21333;&#20010;&#30495;&#23454;&#24615;&#26631;&#31614;&#65292;&#24182;&#19988;&#32570;&#20047;&#31649;&#29702;&#27492;&#31867;&#27169;&#31946;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#23436;&#25972;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#20013;&#33719;&#21462;&#30340;&#32463;&#36807;&#32454;&#31890;&#24230;&#35777;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#38656;&#27714;&#30340;&#29616;&#23454;&#22768;&#26126;&#12290;&#25105;&#20204;&#24443;&#24213;&#20998;&#26512;&#20102;AmbiFC&#20013;&#28041;&#21450;&#21547;&#31946;&#22768;&#26126;&#24341;&#36215;&#30340;&#20105;&#35758;&#65292;&#35266;&#23519;&#21040;&#19982;&#27880;&#37322;&#20154;&#21592;&#30340;&#33258;&#25105;&#35780;&#20272;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;&#35821;&#35328;&#29616;&#35937;&#24378;&#28872;&#30456;&#20851;&#30340;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#12290;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#35777;&#25454;&#30340;&#21547;&#31946;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#26680;&#26597;&#20219;&#21153;&#65292;&#27604;&#36739;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#27880;&#37322;&#20449;&#21495;&#21644;&#21333;&#26631;&#31614;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated fact-checking systems in real-world scenarios must compare claims with retrieved evidence to predict the veracity. The retrieved evidence may not unambiguously support or refute the claim and yield diverse valid interpretations. Existing fact-checking datasets necessitate that models predict a single veracity label for each claim and lack the ability to manage such ambiguity. We present AmbiFC, a large-scale fact-checking dataset with realistic claims derived from real-world information needs. Our dataset contains fine-grained evidence annotations of passages from complete Wikipedia pages. We thoroughly analyze disagreements arising from ambiguous claims in AmbiFC, observing a strong correlation of annotator disagreement with their self-assessment and expert-annotated linguistic phenomena. We introduce the task of evidence-based fact-checking for ambiguous claims with soft labels, and compare three methodologies incorporating annotation signals with a single-label classificat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#28041;&#21450;&#21453;&#39304;&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#65292;&#24212;&#29992;Fast Causal Inference (FCI)&#31639;&#27861;&#21487;&#20197;&#24471;&#21040;&#27491;&#30830;&#30340;&#32467;&#26524;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#34987;&#29992;&#20110;&#19968;&#33268;&#22320;&#20272;&#35745;&#22240;&#26524;&#20851;&#31995;&#30340;&#23384;&#22312;&#21644;&#32570;&#22833;&#12289;&#30452;&#25509;&#22240;&#26524;&#20851;&#31995;&#30340;&#23384;&#22312;&#21644;&#32570;&#22833;&#12289;&#28151;&#28102;&#22240;&#32032;&#30340;&#32570;&#22833;&#20197;&#21450;&#22240;&#26524;&#22270;&#20013;&#29305;&#23450;&#24490;&#29615;&#30340;&#32570;&#22833;&#12290;</title><link>http://arxiv.org/abs/2005.00610</link><description>&lt;p&gt;
&#22312;&#24490;&#29615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#25512;&#26029;&#21033;&#29992;&#37096;&#20998;&#31062;&#20808;&#22270;
&lt;/p&gt;
&lt;p&gt;
Constraint-Based Causal Discovery using Partial Ancestral Graphs in the presence of Cycles. (arXiv:2005.00610v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.00610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#28041;&#21450;&#21453;&#39304;&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#65292;&#24212;&#29992;Fast Causal Inference (FCI)&#31639;&#27861;&#21487;&#20197;&#24471;&#21040;&#27491;&#30830;&#30340;&#32467;&#26524;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#34987;&#29992;&#20110;&#19968;&#33268;&#22320;&#20272;&#35745;&#22240;&#26524;&#20851;&#31995;&#30340;&#23384;&#22312;&#21644;&#32570;&#22833;&#12289;&#30452;&#25509;&#22240;&#26524;&#20851;&#31995;&#30340;&#23384;&#22312;&#21644;&#32570;&#22833;&#12289;&#28151;&#28102;&#22240;&#32032;&#30340;&#32570;&#22833;&#20197;&#21450;&#22240;&#26524;&#22270;&#20013;&#29305;&#23450;&#24490;&#29615;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21453;&#39304;&#22238;&#36335;&#22312;&#35768;&#22810;&#22797;&#26434;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#22312;&#22823;&#37096;&#20998;&#22240;&#26524;&#25512;&#26029;&#25991;&#29486;&#20013;&#24573;&#35270;&#20102;&#23427;&#20204;&#30340;&#23384;&#22312;&#65292;&#22240;&#20026;&#36890;&#24120;&#20551;&#35774;&#31995;&#32479;&#20174;&#19968;&#24320;&#22987;&#23601;&#26159;&#38750;&#24490;&#29615;&#30340;&#12290;&#24403;&#23558;&#20026;&#38750;&#24490;&#29615;&#29615;&#22659;&#35774;&#35745;&#30340;&#22240;&#26524;&#25512;&#26029;&#31639;&#27861;&#24212;&#29992;&#20110;&#28041;&#21450;&#21453;&#39304;&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#19981;&#20250;&#26399;&#26395;&#24471;&#21040;&#27491;&#30830;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20986;&#20154;&#24847;&#26009;&#30340;&#26159;&#65292;&#24555;&#36895;&#22240;&#26524;&#25512;&#26029;&#65288;FCI&#65289;&#31639;&#27861;&#22312;&#24212;&#29992;&#20110;&#28041;&#21450;&#21453;&#39304;&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#35266;&#23519;&#25968;&#25454;&#26102;&#30340;&#36755;&#20986;&#26159;&#27491;&#30830;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#30001;&#31616;&#21333;&#19988;$\sigma$-&#21487;&#20449;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#29983;&#25104;&#30340;&#35266;&#23519;&#25968;&#25454;&#65292;FCI&#26159;&#21487;&#38752;&#32780;&#23436;&#25972;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#19968;&#33268;&#22320;&#20272;&#35745;&#65306;&#65288;i&#65289;&#22240;&#26524;&#20851;&#31995;&#30340;&#23384;&#22312;&#21644;&#32570;&#22833;&#65292;&#65288;ii&#65289;&#30452;&#25509;&#22240;&#26524;&#20851;&#31995;&#30340;&#23384;&#22312;&#21644;&#32570;&#22833;&#65292;&#65288;iii&#65289;&#28151;&#28102;&#22240;&#32032;&#30340;&#32570;&#22833;&#65292;&#20197;&#21450;&#65288;iv&#65289;&#22240;&#26524;&#22270;&#20013;&#29305;&#23450;&#24490;&#29615;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
While feedback loops are known to play important roles in many complex systems, their existence is ignored in a large part of the causal discovery literature, as systems are typically assumed to be acyclic from the outset. When applying causal discovery algorithms designed for the acyclic setting on data generated by a system that involves feedback, one would not expect to obtain correct results. In this work, we show that -- surprisingly -- the output of the Fast Causal Inference (FCI) algorithm is correct if it is applied to observational data generated by a system that involves feedback. More specifically, we prove that for observational data generated by a simple and $\sigma$-faithful Structural Causal Model (SCM), FCI is sound and complete, and can be used to consistently estimate (i) the presence and absence of causal relations, (ii) the presence and absence of direct causal relations, (iii) the absence of confounders, and (iv) the absence of specific cycles in the causal graph o
&lt;/p&gt;</description></item></channel></rss>