<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21019;&#36896;&#24615;&#25903;&#25345;&#24037;&#20855;&#65288;CSTs&#65289;&#20351;&#29992;&#26102;&#65292;&#34429;&#28982;&#21487;&#20197;&#22686;&#21152;&#29992;&#25143;&#20135;&#29983;&#35814;&#32454;&#30340;&#24819;&#27861;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#20250;&#23548;&#33268;&#29992;&#25143;&#25552;&#20986;&#30340;&#24819;&#27861;&#21516;&#36136;&#21270;&#12290;&#23545;&#20110;&#22522;&#20110;LLMs&#30340;CSTs&#30340;&#29992;&#25143;&#12289;&#35774;&#35745;&#24072;&#21644;&#24320;&#21457;&#32773;&#26469;&#35828;&#65292;&#36825;&#20123;&#32467;&#26524;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01536</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20154;&#31867;&#21019;&#36896;&#24615;&#24605;&#32500;&#30340;&#21516;&#36136;&#21270;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Homogenization Effects of Large Language Models on Human Creative Ideation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01536
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21019;&#36896;&#24615;&#25903;&#25345;&#24037;&#20855;&#65288;CSTs&#65289;&#20351;&#29992;&#26102;&#65292;&#34429;&#28982;&#21487;&#20197;&#22686;&#21152;&#29992;&#25143;&#20135;&#29983;&#35814;&#32454;&#30340;&#24819;&#27861;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#20250;&#23548;&#33268;&#29992;&#25143;&#25552;&#20986;&#30340;&#24819;&#27861;&#21516;&#36136;&#21270;&#12290;&#23545;&#20110;&#22522;&#20110;LLMs&#30340;CSTs&#30340;&#29992;&#25143;&#12289;&#35774;&#35745;&#24072;&#21644;&#24320;&#21457;&#32773;&#26469;&#35828;&#65292;&#36825;&#20123;&#32467;&#26524;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29616;&#22312;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#34987;&#20351;&#29992;&#65292;&#21253;&#25324;&#20316;&#20026;&#21019;&#36896;&#24615;&#25903;&#25345;&#24037;&#20855;(CSTs)&#26469;&#24110;&#21161;&#29992;&#25143;&#20135;&#29983;&#26032;&#30340;&#24819;&#27861;&#12290;&#20294;&#26159;LLMs&#30495;&#30340;&#33021;&#22815;&#25903;&#25345;&#29992;&#25143;&#30340;&#21019;&#36896;&#21147;&#21527;&#65311;&#25105;&#20204;&#20551;&#35774;LLMs&#20316;&#20026;CSTs&#30340;&#20351;&#29992;&#21487;&#33021;&#20351;LLMs&#30340;&#29992;&#25143;&#24863;&#21040;&#26356;&#26377;&#21019;&#36896;&#21147;&#65292;&#24182;&#19988;&#25193;&#22823;&#27599;&#20301;&#29992;&#25143;&#25552;&#20986;&#30340;&#24819;&#27861;&#30340;&#33539;&#22260;&#65292;&#20294;&#20063;&#21516;&#36136;&#21270;&#20102;&#19981;&#21516;&#29992;&#25143;&#25152;&#25552;&#20986;&#30340;&#24819;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;36&#20301;&#21442;&#19982;&#32773;&#30340;&#27604;&#36739;&#24615;&#29992;&#25143;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#21516;&#36136;&#21270;&#20551;&#35774;&#21457;&#29616;&#65292;&#19982;&#21478;&#19968;&#31181;CST&#30456;&#27604;&#65292;&#19981;&#21516;&#29992;&#25143;&#20542;&#21521;&#20110;&#20351;&#29992;ChatGPT&#25552;&#20986;&#36739;&#23569;&#35821;&#20041;&#19978;&#29420;&#31435;&#30340;&#24819;&#27861;&#12290;&#27492;&#22806;&#65292;ChatGPT&#29992;&#25143;&#20135;&#29983;&#20102;&#26356;&#22810;&#35814;&#32454;&#30340;&#24819;&#27861;&#65292;&#20294;&#23545;&#25152;&#29983;&#25104;&#30340;&#24819;&#27861;&#24863;&#21040;&#36131;&#20219;&#26356;&#23569;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20123;&#21457;&#29616;&#23545;&#22522;&#20110;LLMs&#30340;CSTs&#30340;&#29992;&#25143;&#12289;&#35774;&#35745;&#24072;&#21644;&#24320;&#21457;&#32773;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are now being used in a wide variety of contexts, including as creativity support tools (CSTs) intended to help their users come up with new ideas. But do LLMs actually support user creativity? We hypothesized that the use of an LLM as a CST might make the LLM's users feel more creative, and even broaden the range of ideas suggested by each individual user, but also homogenize the ideas suggested by different users. We conducted a 36-participant comparative user study and found, in accordance with the homogenization hypothesis, that different users tended to produce less semantically distinct ideas with ChatGPT than with an alternative CST. Additionally, ChatGPT users generated a greater number of more detailed ideas, but felt less responsible for the ideas they generated. We discuss potential implications of these findings for users, designers, and developers of LLM-based CSTs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#23398;&#20064;&#23436;&#20840;&#21487;&#35266;&#23519;&#12289;&#38750;&#30830;&#23450;&#24615;&#35745;&#21010;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#22312;&#19968;&#20123; FOND &#35745;&#21010;&#22522;&#20934;&#39046;&#22495;&#20013;&#20135;&#29983;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02499</link><description>&lt;p&gt;
&#23398;&#20064;&#38754;&#21521;&#23436;&#20840;&#21487;&#35266;&#23519;&#38750;&#30830;&#23450;&#24615;&#35745;&#21010;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#23398;&#20064;&#23436;&#20840;&#21487;&#35266;&#23519;&#12289;&#38750;&#30830;&#23450;&#24615;&#35745;&#21010;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#22312;&#19968;&#20123; FOND &#35745;&#21010;&#22522;&#20934;&#39046;&#22495;&#20013;&#20135;&#29983;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#31574;&#30053;&#20195;&#34920;&#35299;&#20915;&#22823;&#37327;&#35745;&#21010;&#38382;&#39064;&#30340;&#21453;&#24212;&#24615;&#31574;&#30053;&#65292;&#20363;&#22914;&#20174;&#32473;&#23450;&#39046;&#22495;&#20013;&#26080;&#38480;&#21487;&#35299;&#23454;&#20363;&#30340;&#38598;&#21512;&#12290; &#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19968;&#31995;&#21015;&#23567;&#35757;&#32451;&#23454;&#20363;&#20013;&#23398;&#20064;&#36825;&#31181;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#32463;&#20856;&#39046;&#22495;&#12290; &#26412;&#25991;&#25193;&#23637;&#20102;&#23398;&#20064;&#38754;&#21521;&#23436;&#20840;&#21487;&#35266;&#23519;&#12289;&#38750;&#30830;&#23450;&#24615;&#65288;FOND&#65289;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#20844;&#24335;&#21644;&#23548;&#33268;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015; FOND &#35745;&#21010;&#22522;&#20934;&#39046;&#22495;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#19968;&#20123;&#39046;&#22495;&#20013;&#20135;&#29983;&#30340;&#27867;&#21270;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#27491;&#30830;&#24615;&#12290; &#23398;&#20064; FOND &#35745;&#21010;&#30340;&#27867;&#21270;&#31574;&#30053;&#26041;&#27861;&#23454;&#38469;&#19978;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#31181;&#25628;&#32034;&#32467;&#26524;&#30340;&#21478;&#19968;&#31181; FOND &#35745;&#21010;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#26159;&#22312;&#32473;&#23450;&#29366;&#24577;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26159;&#22312;&#30001;&#24517;&#39035;&#23398;&#20064;&#30340;&#29305;&#24449;&#23450;&#20041;&#30340;&#25277;&#35937;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02499v1 Announce Type: new  Abstract: General policies represent reactive strategies for solving large families of planning problems like the infinite collection of solvable instances from a given domain. Methods for learning such policies from a collection of small training instances have been developed successfully for classical domains. In this work, we extend the formulations and the resulting combinatorial methods for learning general policies over fully observable, non-deterministic (FOND) domains. We also evaluate the resulting approach experimentally over a number of benchmark domains in FOND planning, present the general policies that result in some of these domains, and prove their correctness. The method for learning general policies for FOND planning can actually be seen as an alternative FOND planning method that searches for solutions, not in the given state space but in an abstract space defined by features that must be learned as well.
&lt;/p&gt;</description></item><item><title>&#36866;&#20013;&#30340;&#31181;&#32676;&#35268;&#27169;&#21487;&#20197;&#22312;&#20808;&#39564;&#20301;&#22122;&#22768;&#23384;&#22312;&#26102;&#20445;&#25345;&#24378;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#22312;OneMax&#22522;&#20934;&#19978;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;</title><link>https://arxiv.org/abs/2404.02090</link><description>&lt;p&gt;
&#24050;&#32463;&#36866;&#20013;&#30340;&#31181;&#32676;&#35268;&#27169;&#21487;&#35777;&#26126;&#23545;&#22122;&#22768;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Already Moderate Population Sizes Provably Yield Strong Robustness to Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02090
&lt;/p&gt;
&lt;p&gt;
&#36866;&#20013;&#30340;&#31181;&#32676;&#35268;&#27169;&#21487;&#20197;&#22312;&#20808;&#39564;&#20301;&#22122;&#22768;&#23384;&#22312;&#26102;&#20445;&#25345;&#24378;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#22312;OneMax&#22522;&#20934;&#19978;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#34920;&#26126;&#65292;&#20856;&#22411;&#30340;&#36827;&#21270;&#31639;&#27861;&#21487;&#20197;&#24456;&#22909;&#22320;&#24212;&#23545;&#35832;&#22914;&#22024;&#26434;&#30340;&#20989;&#25968;&#35780;&#20272;&#31561;&#38543;&#26426;&#24178;&#25200;&#12290;&#22312;&#31532;&#19968;&#27425;&#38024;&#23545;$(1+\lambda)$&#21644;$(1,\lambda)$&#36827;&#21270;&#31639;&#27861;&#22312;&#20808;&#39564;&#20301;&#22122;&#22768;&#23384;&#22312;&#26102;&#30340;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#20004;&#31181;&#31639;&#27861;&#37117;&#33021;&#23481;&#24525;&#24658;&#23450;&#30340;&#22122;&#22768;&#27010;&#29575;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#22312;OneMax&#22522;&#20934;&#19978;&#30340;&#28176;&#36817;&#36816;&#34892;&#26102;&#38388;&#12290;&#20026;&#27492;&#65292;&#31181;&#32676;&#35268;&#27169;$\lambda$&#24212;&#33267;&#23569;&#20026;&#38382;&#39064;&#35268;&#27169;$n$&#30340;&#23545;&#25968;&#12290;&#22312;&#36825;&#26041;&#21521;&#19978;&#30340;&#21807;&#19968;&#20808;&#21069;&#32467;&#26524;&#28041;&#21450;&#19981;&#22826;&#29616;&#23454;&#30340;&#19968;&#20301;&#22122;&#22768;&#27169;&#22411;&#65292;&#38656;&#35201;&#36229;&#32447;&#24615;&#30340;&#38382;&#39064;&#35268;&#27169;&#31181;&#32676;&#22823;&#23567;&#65292;&#24182;&#19988;&#23545;&#20110;OneMax&#22522;&#20934;&#35777;&#26126;&#20102;&#22823;&#33268;&#26159;&#26080;&#22122;&#22768;&#36816;&#34892;&#26102;&#38388;&#30340;&#19977;&#27425;&#26041;&#30340;&#36816;&#34892;&#26102;&#38388;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#26174;&#30528;&#26356;&#24378;&#32467;&#26524;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#21363;&#26080;&#22122;&#22768;&#21518;&#20195;&#21487;&#20197;&#30475;&#20316;&#26159;&#29238;&#20195;&#21644;&#26377;&#22122;&#22768;&#30340;&#21518;&#20195;&#20043;&#38388;&#30340;&#26377;&#20559;&#32479;&#19968;&#20132;&#21449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02090v1 Announce Type: cross  Abstract: Experience shows that typical evolutionary algorithms can cope well with stochastic disturbances such as noisy function evaluations.   In this first mathematical runtime analysis of the $(1+\lambda)$ and $(1,\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise, we show that both algorithms can tolerate constant noise probabilities without increasing the asymptotic runtime on the OneMax benchmark. For this, a population size $\lambda$ suffices that is at least logarithmic in the problem size $n$. The only previous result in this direction regarded the less realistic one-bit noise model, required a population size super-linear in the problem size, and proved a runtime guarantee roughly cubic in the noiseless runtime for the OneMax benchmark. Our significantly stronger results are based on the novel proof argument that the noiseless offspring can be seen as a biased uniform crossover between the parent and the noisy o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TOLERANCE&#30340;&#26032;&#22411;&#25511;&#21046;&#26550;&#26500;&#65292;&#36890;&#36807;&#20004;&#32423;&#26368;&#20248;&#25511;&#21046;&#35299;&#20915;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#25928;&#31639;&#27861;&#26469;&#25913;&#21892;&#26381;&#21153;&#21487;&#29992;&#24615;&#21644;&#38477;&#20302;&#25805;&#20316;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.01741</link><description>&lt;p&gt;
&#36890;&#36807;&#20004;&#32423;&#21453;&#39304;&#25511;&#21046;&#23454;&#29616;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;
&lt;/p&gt;
&lt;p&gt;
Intrusion Tolerance for Networked Systems through Two-Level Feedback Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TOLERANCE&#30340;&#26032;&#22411;&#25511;&#21046;&#26550;&#26500;&#65292;&#36890;&#36807;&#20004;&#32423;&#26368;&#20248;&#25511;&#21046;&#35299;&#20915;&#32593;&#32476;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#25928;&#31639;&#27861;&#26469;&#25913;&#21892;&#26381;&#21153;&#21487;&#29992;&#24615;&#21644;&#38477;&#20302;&#25805;&#20316;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#26381;&#21153;&#22797;&#21046;&#21697;&#31995;&#32479;&#30340;&#20837;&#20405;&#23481;&#24525;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#20004;&#32423;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#26412;&#22320;&#32423;&#21035;&#65292;&#33410;&#28857;&#25511;&#21046;&#22120;&#25191;&#34892;&#20837;&#20405;&#24674;&#22797;&#65292;&#22312;&#20840;&#23616;&#32423;&#21035;&#65292;&#31995;&#32479;&#25511;&#21046;&#22120;&#31649;&#29702;&#22797;&#21046;&#22240;&#23376;&#12290;&#26412;&#22320;&#21644;&#20840;&#23616;&#25511;&#21046;&#38382;&#39064;&#21487;&#20197;&#34987;&#21046;&#23450;&#20026;&#36816;&#31609;&#23398;&#20013;&#30340;&#32463;&#20856;&#38382;&#39064;&#65292;&#21363;&#26426;&#22120;&#26356;&#25442;&#38382;&#39064;&#21644;&#24211;&#23384;&#34917;&#32473;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#19968;&#27169;&#24335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21517;&#20026;TOLERANCE&#30340;&#20837;&#20405;&#23481;&#24525;&#31995;&#32479;&#25511;&#21046;&#26550;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#23618;&#38754;&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20855;&#26377;&#38408;&#20540;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#29992;&#20110;&#35745;&#31639;&#23427;&#20204;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20223;&#30495;&#29615;&#22659;&#20013;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;TOLERANCE&#65292;&#20854;&#20013;&#36816;&#34892;&#20102;10&#31181;&#32593;&#32476;&#20837;&#20405;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#20837;&#20405;&#23481;&#24525;&#31995;&#32479;&#30456;&#27604;&#65292;TOLERANCE&#33021;&#22815;&#25552;&#39640;&#26381;&#21153;&#21487;&#29992;&#24615;&#24182;&#20943;&#23569;&#25805;&#20316;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01741v1 Announce Type: cross  Abstract: We formulate intrusion tolerance for a system with service replicas as a two-level optimal control problem. On the local level node controllers perform intrusion recovery, and on the global level a system controller manages the replication factor. The local and global control problems can be formulated as classical problems in operations research, namely, the machine replacement problem and the inventory replenishment problem. Based on this formulation, we design TOLERANCE, a novel control architecture for intrusion-tolerant systems. We prove that the optimal control strategies on both levels have threshold structure and design efficient algorithms for computing them. We implement and evaluate TOLERANCE in an emulation environment where we run 10 types of network intrusions. The results show that TOLERANCE can improve service availability and reduce operational cost compared with state-of-the-art intrusion-tolerant systems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2404.01714</link><description>&lt;p&gt;
&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#30340;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01714
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#21152;&#24555;&#22521;&#35757;&#36895;&#24230;&#24182;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#20849;&#36717;&#26799;&#24230;&#20462;&#27491;&#20026;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#65292;&#24182;&#23558;&#20854;&#24182;&#20837;&#36890;&#29992;Adam&#20013;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CG-like-Adam&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#29992;Adam&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#20272;&#35745;&#22343;&#30001;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#26367;&#25442;&#12290;&#25910;&#25947;&#20998;&#26512;&#22788;&#29702;&#20102;&#19968;&#38454;&#30697;&#20272;&#35745;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#31995;&#25968;&#20026;&#24120;&#25968;&#19988;&#19968;&#38454;&#30697;&#20272;&#35745;&#26080;&#20559;&#30340;&#24773;&#20917;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#20102;&#22522;&#20110;CIFAR10/100&#25968;&#25454;&#38598;&#30340;&#25152;&#25552;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01714v1 Announce Type: cross  Abstract: Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20027;&#39064;&#20998;&#26512;&#23545;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#30340;&#24187;&#35273;&#36827;&#34892;&#20102;&#24635;&#32467;&#21644;&#20998;&#31867;&#65292;&#24314;&#31435;&#20102;&#20195;&#30721;&#20013;&#24187;&#35273;&#30340;&#20840;&#38754;&#20998;&#31867;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00971</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#35780;&#20272;LLM&#39537;&#21160;&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Exploring and Evaluating Hallucinations in LLM-Powered Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20027;&#39064;&#20998;&#26512;&#23545;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#30340;&#24187;&#35273;&#36827;&#34892;&#20102;&#24635;&#32467;&#21644;&#20998;&#31867;&#65292;&#24314;&#31435;&#20102;&#20195;&#30721;&#20013;&#24187;&#35273;&#30340;&#20840;&#38754;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#24050;&#32463;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#35768;&#22810;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#12290;&#23613;&#31649;&#34920;&#29616;&#20986;&#33394;&#65292;LLMs&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65292;&#21363;LLMs&#21487;&#33021;&#20135;&#29983;&#19982;&#29992;&#25143;&#24847;&#22270;&#20559;&#31163;&#12289;&#34920;&#29616;&#20986;&#20869;&#37096;&#19981;&#19968;&#33268;&#25110;&#19982;&#20107;&#23454;&#30693;&#35782;&#19981;&#31526;&#30340;&#36755;&#20986;&#65292;&#20351;&#24471;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#37096;&#32626;LLMs&#21487;&#33021;&#23384;&#22312;&#39118;&#38505;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#39046;&#22495;&#30340;&#24187;&#35273;&#65292;&#32570;&#20047;&#23545;&#20195;&#30721;&#29983;&#25104;&#29615;&#22659;&#20013;&#24187;&#35273;&#31867;&#22411;&#21644;&#31243;&#24230;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#36827;&#34892;&#20102;&#20027;&#39064;&#20998;&#26512;&#65292;&#24635;&#32467;&#21644;&#24402;&#31867;&#20854;&#20013;&#23384;&#22312;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#24187;&#35273;&#30340;&#20840;&#38754;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#20102;5&#20010;&#20027;&#35201;&#24187;&#35273;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00971v1 Announce Type: cross  Abstract: The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinatio
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;PSPEM&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#21069;&#32512;&#25552;&#31034;&#26469;&#32534;&#36753;&#35821;&#35328;Lodel&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20013;&#30340;&#20302;&#25928;&#24615;&#12289;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#25552;&#31034;&#24037;&#31243;&#30340;&#19981;&#36879;&#26126;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14381</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#21069;&#32512;&#25552;&#31034;&#26469;&#32534;&#36753;&#35821;&#35328;Lodel&#30340;&#30693;&#35782;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14381
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;PSPEM&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#21069;&#32512;&#25552;&#31034;&#26469;&#32534;&#36753;&#35821;&#35328;Lodel&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20013;&#30340;&#20302;&#25928;&#24615;&#12289;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#25552;&#31034;&#24037;&#31243;&#30340;&#19981;&#36879;&#26126;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24050;&#22312;&#24191;&#27867;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#22521;&#35757;&#65292;&#20197;&#23384;&#20648;&#20851;&#20110;&#25991;&#26412;&#25551;&#36848;&#30340;&#19990;&#30028;&#21508;&#20010;&#26041;&#38754;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#24403;&#21069;&#25216;&#26415;&#36890;&#24120;&#37319;&#29992;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#25110;&#29305;&#23450;&#25552;&#31034;&#26469;&#20462;&#25913;LM&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#25104;&#26412;&#39640;&#26114;&#19988;&#20302;&#25928;&#65292;&#38590;&#20197;&#20135;&#29983;&#36866;&#24403;&#30340;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#25552;&#31034;&#24037;&#31243;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#38656;&#35201;&#22823;&#37327;&#21162;&#21147;&#25214;&#21040;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;PSPEM&#65288;&#21069;&#32512;&#36719;&#25552;&#31034;&#32534;&#36753;&#26041;&#27861;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#19968;&#27425;&#35757;&#32451;&#32780;&#32456;&#36523;&#20351;&#29992;&#12290;&#23427;&#35299;&#20915;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20013;&#30340;&#20302;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#23547;&#25214;&#26368;&#20339;&#36719;&#25552;&#31034;&#26469;&#20811;&#26381;&#25552;&#31034;&#24037;&#31243;&#30340;&#19981;&#36879;&#26126;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PSPEM&#21033;&#29992;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#32534;&#30721;&#36716;&#25442;&#22120;&#26469;&#31934;&#28860;&#25552;&#31034;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#25552;&#31034;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14381v1 Announce Type: cross  Abstract: Neural language models (LMs) have been extensively trained on vast corpora to store factual knowledge about various aspects of the world described in texts. Current technologies typically employ knowledge editing methods or specific prompts to modify LM outputs. However, existing knowledge editing methods are costly and inefficient, struggling to produce appropriate text. Additionally, prompt engineering is opaque and requires significant effort to find suitable prompts. To address these issues, we introduce a new method called PSPEM (Prefix Soft Prompt Editing Method), that can be used for a lifetime with just one training. It resolves the inefficiencies and generalizability issues in knowledge editing methods and overcomes the opacity of prompt engineering by automatically seeking optimal soft prompts. Specifically, PSPEM utilizes a prompt encoder and an encoding converter to refine key information in prompts and uses prompt alignmen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22320;&#29699;&#35266;&#27979;&#24212;&#29992;&#20013;&#32570;&#22833;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38598;&#25104;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;100%&#30340;&#39044;&#27979;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#32570;&#22833;&#24773;&#26223;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27604;&#20998;&#31867;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#19988;&#20809;&#23398;&#35270;&#35282;&#26159;&#26368;&#20851;&#38190;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.14297</link><description>&lt;p&gt;
&#22320;&#29699;&#35266;&#27979;&#24212;&#29992;&#20013;&#32570;&#22833;&#25968;&#25454;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Impact Assessment of Missing Data in Model Predictions for Earth Observation Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22320;&#29699;&#35266;&#27979;&#24212;&#29992;&#20013;&#32570;&#22833;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38598;&#25104;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;100%&#30340;&#39044;&#27979;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#32570;&#22833;&#24773;&#26223;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27604;&#20998;&#31867;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#19988;&#20809;&#23398;&#35270;&#35282;&#26159;&#26368;&#20851;&#38190;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#24212;&#29992;&#28041;&#21450;&#22797;&#26434;&#21644;&#24322;&#26500;&#25968;&#25454;&#28304;&#65292;&#36890;&#24120;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#26222;&#36941;&#20551;&#35774;&#25968;&#25454;&#28304;&#23558;&#25345;&#32493;&#21487;&#29992;&#12290;&#19981;&#21516;&#24773;&#20917;&#21487;&#33021;&#24433;&#21709;EO&#25968;&#25454;&#28304;&#30340;&#21487;&#29992;&#24615;&#65292;&#22914;&#22122;&#22768;&#12289;&#20113;&#23618;&#25110;&#21355;&#26143;&#20219;&#21153;&#22833;&#36133;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#32570;&#22833;&#26102;&#38388;&#24615;&#21644;&#38745;&#24577;EO&#25968;&#25454;&#28304;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#39044;&#27979;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#19968;&#20123;&#26041;&#27861;&#22312;&#38754;&#23545;&#32570;&#22833;&#25968;&#25454;&#26102;&#33258;&#28982;&#26356;&#21152;&#31283;&#20581;&#12290;&#29305;&#21035;&#26159;&#38598;&#25104;&#31574;&#30053;&#23454;&#29616;&#20102;&#39640;&#36798;100%&#30340;&#39044;&#27979;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#32570;&#22833;&#24773;&#26223;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27604;&#20998;&#31867;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#20809;&#23398;&#35270;&#35282;&#22312;&#21333;&#29420;&#32570;&#22833;&#26102;&#26159;&#26368;&#20851;&#38190;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14297v1 Announce Type: cross  Abstract: Earth observation (EO) applications involving complex and heterogeneous data sources are commonly approached with machine learning models. However, there is a common assumption that data sources will be persistently available. Different situations could affect the availability of EO sources, like noise, clouds, or satellite mission failures. In this work, we assess the impact of missing temporal and static EO sources in trained models across four datasets with classification and regression tasks. We compare the predictive quality of different methods and find that some are naturally more robust to missing data. The Ensemble strategy, in particular, achieves a prediction robustness up to 100%. We evidence that missing scenarios are significantly more challenging in regression than classification tasks. Finally, we find that the optical view is the most critical view when it is missing individually.
&lt;/p&gt;</description></item><item><title>STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.12418</link><description>&lt;p&gt;
STG-Mamba: &#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12418
&lt;/p&gt;
&lt;p&gt;
STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Graph&#65288;STG&#65289;&#25968;&#25454;&#20855;&#26377;&#21160;&#24577;&#24615;&#12289;&#24322;&#36136;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#29305;&#28857;&#65292;&#23548;&#33268;&#26102;&#31354;&#22270;&#23398;&#20064;&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#38598;&#20013;&#20110;&#27169;&#25311;STG&#32593;&#32476;&#20013;&#33410;&#28857;&#20010;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#38543;&#26102;&#38388;&#23384;&#22312;&#30340;STG&#31995;&#32479;&#26412;&#36136;&#29305;&#24449;&#30340;&#24314;&#27169;&#37325;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#29616;&#20195;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSSMs&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#31934;&#24515;&#25506;&#32034;&#20102;STG&#31995;&#32479;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#21160;&#24577;&#29366;&#24577;&#28436;&#21464;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Spatial-Temporal Graph Mamba&#65288;STG-Mamba&#65289;&#65292;&#20316;&#20026;&#39318;&#20010;&#21033;&#29992;&#24378;&#22823;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;STG&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12418v1 Announce Type: cross  Abstract: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of ST
&lt;/p&gt;</description></item><item><title>DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.10903</link><description>&lt;p&gt;
DTOR&#65306;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#29992;&#20110;&#35299;&#37322;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
DTOR: Decision Tree Outlier Regressor to explain anomalies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10903
&lt;/p&gt;
&lt;p&gt;
DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24322;&#24120;&#20540;&#30340;&#20986;&#29616;&#20197;&#21450;&#20854;&#20135;&#29983;&#26426;&#21046;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#21487;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#25925;&#38556;&#12289;&#27450;&#35784;&#12289;&#23041;&#32961;&#31561;&#38382;&#39064;&#65292;&#38500;&#20102;&#34987;&#27491;&#30830;&#35782;&#21035;&#20043;&#22806;&#65292;&#36890;&#24120;&#38656;&#35201;&#26377;&#25928;&#30340;&#35299;&#37322;&#20197;&#26377;&#25928;&#25191;&#34892;&#21487;&#25805;&#20316;&#30340;&#23545;&#25239;&#25514;&#26045;&#12290;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#20351;&#29992;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35782;&#21035;&#24322;&#24120;&#20540;&#65292;&#20351;&#24471;&#36825;&#26679;&#30340;&#35299;&#37322;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65288;DTOR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20026;&#21333;&#20010;&#25968;&#25454;&#28857;&#29983;&#25104;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#30340;&#25216;&#26415;&#12290;&#36825;&#26159;&#36890;&#36807;&#39318;&#20808;&#24212;&#29992;&#20915;&#31574;&#26641;&#22238;&#24402;&#22120;&#26469;&#35745;&#31639;&#20272;&#35745;&#20998;&#25968;&#65292;&#28982;&#21518;&#25552;&#21462;&#19982;&#25968;&#25454;&#28857;&#20998;&#25968;&#30456;&#20851;&#32852;&#30340;&#30456;&#23545;&#36335;&#24452;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;DTOR&#30340;&#40065;&#26834;&#24615;&#20063;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10903v1 Announce Type: cross  Abstract: Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approac
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;ARC&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#21644;&#25104;&#24180;&#20154;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23545;LLMs&#21644;&#24180;&#24188;&#20799;&#31461;&#38169;&#35823;&#20998;&#26512;&#25581;&#31034;&#20102;&#31867;&#20284;&#30340;&#35299;&#20915;&#31574;&#30053;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#20026;&#25105;&#20204;&#29702;&#35299;LLMs&#22914;&#20309;&#35299;&#20915;&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.09734</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20687;&#20154;&#19968;&#26679;&#35299;&#20915;ARC&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models Solve ARC Visual Analogies Like People Do?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;ARC&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#21644;&#25104;&#24180;&#20154;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23545;LLMs&#21644;&#24180;&#24188;&#20799;&#31461;&#38169;&#35823;&#20998;&#26512;&#25581;&#31034;&#20102;&#31867;&#20284;&#30340;&#35299;&#20915;&#31574;&#30053;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#20026;&#25105;&#20204;&#29702;&#35299;LLMs&#22914;&#20309;&#35299;&#20915;&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#21046;&#35770;&#25991;&#65288;Chollet, 2019&#65289;&#24418;&#24335;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20799;&#31461;&#21451;&#22909;&#30340;ARC&#39033;&#30446;&#19978;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#20799;&#31461;&#36824;&#26159;&#25104;&#24180;&#20154;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#37117;&#32988;&#36807;&#22823;&#22810;&#25968;LLMs&#12290;&#38169;&#35823;&#20998;&#26512;&#25581;&#31034;&#20102;LLMs&#21644;&#24180;&#24188;&#20799;&#31461;&#20043;&#38388;&#31867;&#20284;&#30340;&#8220;&#20498;&#36864;&#8221;&#35299;&#20915;&#31574;&#30053;&#65292;&#20854;&#20013;&#31867;&#27604;&#30340;&#19968;&#37096;&#20998;&#34987;&#31616;&#21333;&#22797;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#20182;&#20004;&#31181;&#38169;&#35823;&#31867;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#34920;&#38754;&#25484;&#25569;&#20851;&#38190;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#20869;&#22806;&#20851;&#31995;&#65289;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#36755;&#20837;&#30697;&#38453;&#30340;&#31616;&#21333;&#32452;&#21512;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#8220;&#27010;&#24565;&#8221;&#38169;&#35823;&#22312;&#20154;&#31867;&#20013;&#26356;&#24120;&#35265;&#65292;&#8220;&#30697;&#38453;&#8221;&#38169;&#35823;&#22312;LLMs&#20013;&#26356;&#24120;&#35265;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#38169;&#35823;&#20998;&#26512;&#20197;&#21450;&#19982;&#20154;&#31867;&#21457;&#23637;&#30340;&#27604;&#36739;&#26469;&#29702;&#35299;LLMs&#22914;&#20309;&#35299;&#20915;&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09734v1 Announce Type: cross  Abstract: The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test designed for humans and machines (Chollet, 2019). We compared human and large language model (LLM) performance on a new child-friendly set of ARC items. Results show that both children and adults outperform most LLMs on these tasks. Error analysis revealed a similar "fallback" solution strategy in LLMs and young children, where part of the analogy is simply copied. In addition, we found two other error types, one based on seemingly grasping key concepts (e.g., Inside-Outside) and the other based on simple combinations of analogy input matrices. On the whole, "concept" errors were more common in humans, and "matrix" errors were more common in LLMs. This study sheds new light on LLM reasoning ability and the extent to which we can use error analyses and comparisons with human development to understand how LLMs solve visual analogies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.14528</link><description>&lt;p&gt;
ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24573;&#35270;&#20102;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21516;&#21407;&#22987;&#34892;&#20026;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#21160;&#20316;&#32500;&#24230;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35780;&#20272;&#35757;&#32451;&#36807;&#31243;&#20013;&#21508;&#31181;&#21407;&#22987;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22240;&#26524;&#24863;&#30693;&#29109;&#39033;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#24182;&#20248;&#20808;&#22788;&#29702;&#20855;&#26377;&#39640;&#28508;&#22312;&#24433;&#21709;&#30340;&#34892;&#21160;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#38450;&#27490;&#23545;&#29305;&#23450;&#21407;&#22987;&#34892;&#20026;&#36807;&#24230;&#20851;&#27880;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65292;&#22312;&#36328;7&#20010;&#39046;&#22495;&#30340;29&#20010;&#19981;&#21516;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#65292;&#30456;&#36739;&#20110;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#65292;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14528v1 Announce Type: cross  Abstract: The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#24110;&#21161;&#27169;&#22411;&#35780;&#20272;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#24182;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.12563</link><description>&lt;p&gt;
&#20449;&#24515;&#33267;&#20851;&#37325;&#35201;&#65306;&#37325;&#26032;&#23457;&#35270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#24110;&#21161;&#27169;&#22411;&#35780;&#20272;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#24182;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#28608;&#21457;&#20102;&#23545;&#23427;&#20204;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#35797;&#22270;&#35299;&#20915;&#20851;&#20110;&#20854;&#21487;&#34892;&#24615;&#30340;&#25345;&#32493;&#20105;&#35770;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28508;&#22312;&#22240;&#32032; - LLMs&#30340;&#8220;&#20449;&#24515;&#8221; - &#22312;&#33258;&#25105;&#26657;&#27491;&#36807;&#31243;&#20013;&#12290;&#24573;&#35270;&#36825;&#19968;&#22240;&#32032;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25209;&#35780;&#33258;&#24049;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#33258;&#26657;&#27491;&#25928;&#26524;&#30340;&#21487;&#38752;&#32467;&#35770;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#23454;&#39564;&#35266;&#23519;&#21040;LLMs&#20855;&#26377;&#29702;&#35299;&#20854;&#33258;&#36523;&#22238;&#24212;&#8220;&#20449;&#24515;&#8221;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21169;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24341;&#23548;LLMs&#35780;&#20272;&#20854;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#65292;&#20419;&#36827;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#22522;&#20110;IoE&#30340;&#25552;&#31034;&#21487;&#20197;&#23454;&#29616;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12563v1 Announce Type: cross  Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a co
&lt;/p&gt;</description></item><item><title>HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10228</link><description>&lt;p&gt;
HyperAgent&#65306;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#29992;&#20110;&#22797;&#26434;&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10228
&lt;/p&gt;
&lt;p&gt;
HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#19981;&#26029;&#31215;&#32047;&#30340;&#20132;&#20114;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyperAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36229;&#27169;&#22411;&#12289;&#32034;&#24341;&#25277;&#26679;&#26041;&#26696;&#21644;&#22686;&#37327;&#26356;&#26032;&#26426;&#21046;&#30340;RL&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#20013;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#39034;&#24207;&#21518;&#39564;&#36924;&#36817;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#36229;&#36234;&#20102;&#20849;&#36717;&#24615;&#12290;HyperAgent&#30340;&#23454;&#29616;&#31616;&#21333;&#65292;&#21482;&#38656;&#35201;&#22312;DDQN&#20013;&#28155;&#21152;&#19968;&#20010;&#27169;&#22359;&#21644;&#19968;&#34892;&#39069;&#22806;&#20195;&#30721;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;HyperAgent&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#25968;&#25454;&#36824;&#26159;&#35745;&#31639;&#26041;&#38754;&#37117;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#22312;&#23454;&#38469;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#20013;&#65292;HyperAgent&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00447</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Data-Efficient Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00447
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#31038;&#20132;&#32593;&#32476;&#21040;&#29983;&#29289;&#21270;&#23398;&#20998;&#26512;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#26159;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24314;&#27169;&#36825;&#31181;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#21151;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#22312;&#26631;&#27880;&#36164;&#28304;&#26377;&#38480;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;(DEGL)&#30340;&#30740;&#31350;&#21069;&#27839;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;DEGL&#24403;&#21069;&#36827;&#23637;&#30340;&#39318;&#27425;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#20026;&#25105;&#20204;&#23545;DEGL&#30340;&#25506;&#32034;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#20960;&#20010;&#20851;&#38190;&#26041;&#38754;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#19968;&#20027;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#30340;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#20102;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#30340;&#36801;&#31227;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2401.17173</link><description>&lt;p&gt;
&#36890;&#36807;&#20989;&#25968;&#32534;&#30721;&#22120;&#23454;&#29616;&#38646;-shot&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Reinforcement Learning via Function Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#30340;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#20102;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#30340;&#36801;&#31227;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#35299;&#20915;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#20294;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#38590;&#28857;&#22312;&#20110;&#23547;&#25214;&#19968;&#20010;&#33391;&#22909;&#30340;&#34920;&#31034;&#26469;&#34920;&#36798;&#24403;&#21069;&#20219;&#21153;&#65292;&#20197;&#20415;&#20195;&#29702;&#31243;&#24207;&#29702;&#35299;&#23427;&#19982;&#20808;&#21069;&#30475;&#21040;&#30340;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#20989;&#25968;&#32534;&#30721;&#22120;&#26469;&#34920;&#31034;&#22870;&#21169;&#20989;&#25968;&#25110;&#36716;&#31227;&#20989;&#25968;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#19968;&#20010;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#26377;&#20851;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#30340;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#36801;&#31227;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#23558;&#22522;&#26412;RL&#31639;&#27861;&#19982;&#20989;&#25968;&#32534;&#30721;&#22120;&#32467;&#21512;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;RL&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#25928;&#29575;&#12289;&#28176;&#36817;&#24615;&#33021;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although reinforcement learning (RL) can solve many challenging sequential decision making problems, achieving zero-shot transfer across related tasks remains a challenge. The difficulty lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks. To achieve zero-shot transfer, we introduce the function encoder, a representation learning algorithm which represents a function as a weighted combination of learned, non-linear basis functions. By using a function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation. Thus, the agent is able to achieve transfer between related tasks at run time with no additional training. We demonstrate state-of-the-art data efficiency, asymptotic performance, and training stability in three RL fields by augmenting basic RL algorithms with a function encod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;90&#20010;&#36523;&#20307;&#27573;&#12289;206&#20010;&#20851;&#33410;&#21644;700&#20010;&#32908;&#33137;&#21333;&#20301;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#20302;&#32500;&#34920;&#31034;&#21644;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20840;&#36523;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2312.05473</link><description>&lt;p&gt;
&#33258;&#25105;&#27169;&#22411;&#29992;&#20110;&#20855;&#36523;&#26234;&#33021;&#65306;&#29992;&#20998;&#23618;&#20302;&#32500;&#34920;&#31034;&#24314;&#27169;&#20840;&#36523;&#20154;&#20307;&#39592;&#39612;&#32908;&#32905;&#31995;&#32479;&#21644;&#36816;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Self Model for Embodied Intelligence: Modeling Full-Body Human Musculoskeletal System and Locomotion Control with Hierarchical Low-Dimensional Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;90&#20010;&#36523;&#20307;&#27573;&#12289;206&#20010;&#20851;&#33410;&#21644;700&#20010;&#32908;&#33137;&#21333;&#20301;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#20302;&#32500;&#34920;&#31034;&#21644;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20840;&#36523;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#32908;&#32905;&#39592;&#39612;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#25511;&#21046;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#36816;&#21160;&#21151;&#33021;&#12289;&#24320;&#21457;&#20855;&#36523;&#26234;&#33021;&#20197;&#21450;&#20248;&#21270;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#38024;&#23545;&#30446;&#21069;&#24320;&#28304;&#27169;&#22411;&#20165;&#38480;&#20110;&#23569;&#25968;&#36523;&#20307;&#37096;&#20301;&#19988;&#36890;&#24120;&#32908;&#32905;&#25968;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;90&#20010;&#36523;&#20307;&#27573;&#12289;206&#20010;&#20851;&#33410;&#21644;700&#20010;&#32908;&#33137;&#21333;&#20301;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#65292;&#33021;&#22815;&#27169;&#25311;&#20840;&#36523;&#21160;&#24577;&#24182;&#19982;&#21508;&#31181;&#35774;&#22791;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#32500;&#34920;&#31034;&#21644;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20840;&#36523;&#25511;&#21046;&#12290;&#36890;&#36807;&#27169;&#25311;&#30495;&#23454;&#20154;&#31867;&#27493;&#24577;&#25968;&#25454;&#39564;&#35777;&#20102;&#25105;&#20204;&#27169;&#22411;&#21644;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05473v2 Announce Type: replace  Abstract: Modeling and control of the human musculoskeletal system is important for understanding human motor functions, developing embodied intelligence, and optimizing human-robot interaction systems. However, current open-source models are restricted to a limited range of body parts and often with a reduced number of muscles. There is also a lack of algorithms capable of controlling over 600 muscles to generate reasonable human movements. To fill this gap, we build a musculoskeletal model with 90 body segments, 206 joints, and 700 muscle-tendon units, allowing simulation of full-body dynamics and interaction with various devices. We develop a new algorithm using low-dimensional representation and hierarchical deep reinforcement learning to achieve state-of-the-art full-body control. We validate the effectiveness of our model and algorithm in simulations with real human locomotion data. The musculoskeletal model, along with its control algor
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.02129</link><description>&lt;p&gt;
&#25581;&#31034;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Pitfalls of Knowledge Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#19981;&#26029;&#19978;&#21319;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#36716;&#21521;&#24320;&#21457;&#32534;&#36753;LLMs&#20869;&#22312;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#19968;&#20010;&#38452;&#20113;&#24748;&#22312;&#22836;&#39030;&#19978; - &#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#35302;&#21457;&#34676;&#34678;&#25928;&#24212;&#65311;&#22240;&#20026;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#24341;&#20837;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#30340;&#21103;&#20316;&#29992;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#19982;LLMs&#30693;&#35782;&#32534;&#36753;&#30456;&#20851;&#30340;&#28508;&#22312;&#38519;&#38449;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#30693;&#35782;&#20914;&#31361;&#65306;&#32534;&#36753;&#36923;&#36753;&#20914;&#31361;&#30340;&#20107;&#23454;&#32452;&#21487;&#33021;&#20250;&#25918;&#22823;LLMs&#22266;&#26377;&#30340;&#19981;&#19968;&#33268;&#24615; - &#36825;&#26159;&#20197;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#65288;2&#65289;&#30693;&#35782;&#25197;&#26354;&#65306;&#20026;&#20102;&#32534;&#36753;&#20107;&#23454;&#30693;&#35782;&#32780;&#26356;&#25913;&#21442;&#25968;&#21487;&#33021;&#20250;&#19981;&#21487;&#36870;&#22320;&#25197;&#26354;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#30340;&#20381;&#36182;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36866;&#24403;&#20381;&#36182;&#12290;&#35813;&#23450;&#20041;&#20998;&#31163;&#20102;&#20381;&#36182;&#30340;&#27010;&#24565;&#21644;&#20154;&#31867;&#22312;&#24418;&#25104;&#20934;&#30830;&#20449;&#24565;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20026;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20114;&#34917;&#24615;&#21644;&#20381;&#36182;&#24615;&#30340;&#30740;&#31350;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.15356</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20381;&#36182;&#30340;&#32479;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Statistical Framework for Measuring AI Reliance. (arXiv:2401.15356v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15356
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#30340;&#20381;&#36182;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36866;&#24403;&#20381;&#36182;&#12290;&#35813;&#23450;&#20041;&#20998;&#31163;&#20102;&#20381;&#36182;&#30340;&#27010;&#24565;&#21644;&#20154;&#31867;&#22312;&#24418;&#25104;&#20934;&#30830;&#20449;&#24565;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20026;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20114;&#34917;&#24615;&#21644;&#20381;&#36182;&#24615;&#30340;&#30740;&#31350;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#32463;&#24120;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24110;&#21161;&#19979;&#20570;&#20915;&#31574;&#12290;&#19968;&#20010;&#24120;&#35265;&#27169;&#24335;&#26159;&#20154;&#24037;&#26234;&#33021;&#21521;&#20154;&#31867;&#25512;&#33616;&#34892;&#21160;&#65292;&#32780;&#20154;&#31867;&#20445;&#30041;&#23545;&#26368;&#32456;&#20915;&#31574;&#30340;&#25511;&#21046;&#26435;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30830;&#35748;&#65292;&#30830;&#20445;&#20154;&#31867;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36866;&#24403;&#20381;&#36182;&#26159;&#23454;&#29616;&#20114;&#34917;&#24615;&#33021;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#30446;&#21069;&#22312;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#36866;&#24403;&#20381;&#36182;&#30340;&#23450;&#20041;&#32570;&#20047;&#24418;&#24335;&#21270;&#30340;&#32479;&#35745;&#22522;&#30784;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#30683;&#30462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#30340;&#20381;&#36182;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#23427;&#23558;&#20381;&#36182;&#30340;&#27010;&#24565;&#19982;&#20154;&#31867;&#22312;&#21306;&#20998;&#20449;&#21495;&#24182;&#24418;&#25104;&#20934;&#30830;&#20449;&#24565;&#30340;&#25361;&#25112;&#20998;&#24320;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#20135;&#29983;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#26469;&#25351;&#23548;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20114;&#34917;&#24615;&#21644;&#20381;&#36182;&#24615;&#30340;&#30740;&#31350;&#35774;&#35745;&#21644;&#35299;&#37322;&#12290;&#21033;&#29992;&#26368;&#36817;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#20915;&#31574;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Humans frequently make decisions with the aid of artificially intelligent (AI) systems. A common pattern is for the AI to recommend an action to the human who retains control over the final decision. Researchers have identified ensuring that a human has appropriate reliance on an AI as a critical component of achieving complementary performance. We argue that the current definition of appropriate reliance used in such research lacks formal statistical grounding and can lead to contradictions. We propose a formal definition of reliance, based on statistical decision theory, which separates the concepts of reliance as the probability the decision-maker follows the AI's prediction from challenges a human may face in differentiating the signals and forming accurate beliefs about the situation. Our definition gives rise to a framework that can be used to guide the design and interpretation of studies on human-AI complementarity and reliance. Using recent AI-advised decision making studies f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#30340;&#26041;&#24335;&#26469;&#20943;&#36731;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14589</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24230;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20943;&#23569;&#35748;&#30693;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using Large Language Models to Mitigate Cognitive Bias. (arXiv:2401.14589v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#30340;&#26041;&#24335;&#26469;&#20943;&#36731;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#26174;&#33879;&#23548;&#33268;&#35786;&#26029;&#38169;&#35823;&#21644;&#27425;&#20248;&#24739;&#32773;&#32467;&#26524;&#12290;&#35299;&#20915;&#36825;&#20123;&#20559;&#24046;&#38382;&#39064;&#22312;&#21307;&#30103;&#39046;&#22495;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#20013;&#20943;&#36731;&#36825;&#20123;&#20559;&#24046;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#27169;&#25311;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#25913;&#21892;&#35786;&#26029;&#20934;&#30830;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;&#26041;&#27861;&#65306;&#20174;&#25991;&#29486;&#20013;&#25214;&#21040;&#20102;&#24635;&#20849;16&#20010;&#24050;&#21457;&#34920;&#21644;&#26410;&#21457;&#34920;&#30340;&#30149;&#20363;&#25253;&#21578;&#65292;&#20854;&#20013;&#35748;&#30693;&#20559;&#24046;&#23548;&#33268;&#35823;&#35786;&#12290;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992; GPT-4 Turbo &#20419;&#36827;&#22235;&#20010;&#27169;&#25311;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20197;&#22797;&#21046;&#20020;&#24202;&#22242;&#38431;&#21160;&#24577;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#26377;&#29420;&#29305;&#30340;&#35282;&#33394;&#65306;1) &#22312;&#32771;&#34385;&#35752;&#35770;&#21518;&#36827;&#34892;&#21021;&#27493;&#21644;&#26368;&#32456;&#35786;&#26029;&#12290;2) &#20805;&#24403;&#39764;&#39740;&#30340;&#20195;&#35328;&#20154;&#65292;&#20197;&#32416;&#27491;&#30830;&#35748;&#20559;&#24046;&#21644;&#38170;&#23450;&#20559;&#24046;&#12290;3) &#20805;&#24403;&#23548;&#24072;&#21644;&#20419;&#36827;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Cognitive biases in clinical decision-making significantly contribute to errors in diagnosis and suboptimal patient outcomes. Addressing these biases presents a formidable challenge in the medical field. This study explores the role of large language models (LLMs) in mitigating these biases through the utilization of a multi-agent framework. We simulate the clinical decision-making processes through multi-agent conversation and evaluate its efficacy in improving diagnostic accuracy. Methods: A total of 16 published and unpublished case reports where cognitive biases have resulted in misdiagnoses were identified from the literature. In the multi-agent system, we leveraged GPT-4 Turbo to facilitate interactions among four simulated agents to replicate clinical team dynamics. Each agent has a distinct role: 1) To make the initial and final diagnosis after considering the discussions, 2) The devil's advocate and correct confirmation and anchoring bias, 3) The tutor and facilita
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#40657;&#30418;&#23457;&#35745;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#30333;&#30418;&#21644;&#36229;&#36234;&#26694;&#26550;&#23457;&#35745;&#30340;&#20248;&#21183;&#65292;&#40657;&#30418;&#35775;&#38382;&#23545;&#20110;&#20005;&#26684;&#30340;&#20154;&#24037;&#26234;&#33021;&#23457;&#35745;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.14446</link><description>&lt;p&gt;
&#40657;&#30418;&#35775;&#38382;&#23545;&#20110;&#20005;&#26684;&#30340;&#20154;&#24037;&#26234;&#33021;&#23457;&#35745;&#26159;&#19981;&#20805;&#20998;&#30340;
&lt;/p&gt;
&lt;p&gt;
Black-Box Access is Insufficient for Rigorous AI Audits. (arXiv:2401.14446v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#40657;&#30418;&#23457;&#35745;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#30333;&#30418;&#21644;&#36229;&#36234;&#26694;&#26550;&#23457;&#35745;&#30340;&#20248;&#21183;&#65292;&#40657;&#30418;&#35775;&#38382;&#23545;&#20110;&#20005;&#26684;&#30340;&#20154;&#24037;&#26234;&#33021;&#23457;&#35745;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23457;&#35745;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#35748;&#20026;&#26159;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#26426;&#21046;&#12290;&#23457;&#35745;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#23457;&#35745;&#21592;&#34987;&#25480;&#20104;&#30340;&#31995;&#32479;&#35775;&#38382;&#31243;&#24230;&#12290;&#36817;&#26399;&#23545;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23457;&#35745;&#20027;&#35201;&#20381;&#36182;&#20110;&#40657;&#30418;&#35775;&#38382;&#65292;&#23457;&#35745;&#21592;&#21482;&#33021;&#26597;&#35810;&#31995;&#32479;&#24182;&#35266;&#23519;&#20854;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#23545;&#31995;&#32479;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#65288;&#20363;&#22914;&#26435;&#37325;&#12289;&#28608;&#27963;&#12289;&#26799;&#24230;&#65289;&#30340;&#36879;&#26126;&#35775;&#38382;&#20801;&#35768;&#23457;&#35745;&#21592;&#36827;&#34892;&#26356;&#24378;&#30340;&#25915;&#20987;&#65292;&#26356;&#20840;&#38754;&#22320;&#35299;&#37322;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#12290;&#21516;&#26102;&#65292;&#23545;&#20854;&#22521;&#35757;&#21644;&#37096;&#32626;&#20449;&#24687;&#30340;&#36229;&#36234;&#26694;&#26550;&#35775;&#38382;&#65288;&#20363;&#22914;&#26041;&#27861;&#35770;&#12289;&#20195;&#30721;&#12289;&#25991;&#26723;&#12289;&#36229;&#21442;&#25968;&#12289;&#25968;&#25454;&#12289;&#37096;&#32626;&#32454;&#33410;&#12289;&#20869;&#37096;&#35780;&#20272;&#32467;&#26524;&#65289;&#20801;&#35768;&#23457;&#35745;&#21592;&#23457;&#26597;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#35774;&#35745;&#26356;&#20855;&#38024;&#23545;&#24615;&#30340;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#40657;&#30418;&#23457;&#35745;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#30333;&#30418;&#21644;&#36229;&#36234;&#26694;&#26550;&#23457;&#35745;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25216;&#26415;&#21644;&#29983;&#29702;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
External audits of AI systems are increasingly recognized as a key mechanism for AI governance. The effectiveness of an audit, however, depends on the degree of system access granted to auditors. Recent audits of state-of-the-art AI systems have primarily relied on black-box access, in which auditors can only query the system and observe its outputs. However, white-box access to the system's inner workings (e.g., weights, activations, gradients) allows an auditor to perform stronger attacks, more thoroughly interpret models, and conduct fine-tuning. Meanwhile, outside-the-box access to its training and deployment information (e.g., methodology, code, documentation, hyperparameters, data, deployment details, findings from internal evaluations) allows for auditors to scrutinize the development process and design more targeted evaluations. In this paper, we examine the limitations of black-box audits and the advantages of white- and outside-the-box audits. We also discuss technical, physi
&lt;/p&gt;</description></item><item><title>CompactifAI&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2401.14109</link><description>&lt;p&gt;
CompactifAI: &#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks. (arXiv:2401.14109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14109
&lt;/p&gt;
&lt;p&gt;
CompactifAI&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;LlaMA&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#24102;&#26469;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#22914;&#24040;&#22823;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#25104;&#26412;&#12289;&#36739;&#22823;&#30340;&#33021;&#28304;&#38656;&#27714;&#20197;&#21450;&#29616;&#22330;&#37096;&#32626;&#30340;&#38480;&#21046;&#12290;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#22914;&#21098;&#26525;&#12289;&#33976;&#39311;&#21644;&#20302;&#31209;&#36924;&#36817;&#20027;&#35201;&#20851;&#27880;&#20943;&#23569;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#26377;&#25928;&#25968;&#37327;&#65292;&#32780;&#37327;&#21270;&#26041;&#27861;&#21017;&#20391;&#37325;&#20110;&#38477;&#20302;&#21333;&#20010;&#26435;&#37325;&#30340;&#25968;&#20540;&#31934;&#24230;&#65292;&#20197;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21516;&#26102;&#20445;&#25345;&#31070;&#32463;&#20803;&#25968;&#30446;&#19981;&#21464;&#12290;&#34429;&#28982;&#36825;&#20123;&#21387;&#32553;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#30456;&#23545;&#25104;&#21151;&#65292;&#20294;&#27809;&#26377;&#20196;&#20154;&#20449;&#26381;&#30340;&#29702;&#30001;&#35748;&#20026;&#25130;&#26029;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#26159;&#19968;&#31181;&#26368;&#20248;&#31574;&#30053;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;LLM&#21387;&#32553;&#26041;&#27861;CompactifAI&#65292;&#23427;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment. Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed. While these compression methods have been relatively successful in practice, there's no compelling reason to believe that truncating the number of neurons is an optimal strategy. In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model's correlation space instead, allowing for a more controlled, refined an
&lt;/p&gt;</description></item><item><title>LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13920</link><description>&lt;p&gt;
LocMoE: &#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#20302;&#24320;&#38144;MoE
&lt;/p&gt;
&lt;p&gt;
LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13920
&lt;/p&gt;
&lt;p&gt;
LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65288;MoE&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#20998;&#24067;&#24335;&#21644;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#26377;&#25928;&#31232;&#30095;&#21644;&#25193;&#23637;&#27169;&#22411;&#65292;&#22240;&#27492;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;MoE&#30340;&#24615;&#33021;&#21463;&#21040;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20840;&#23545;&#20840;&#36890;&#20449;&#30340;&#39640;&#24310;&#36831;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#30001;&#20110;&#22823;&#37327;&#30340;&#19987;&#23478;&#23481;&#37327;&#23548;&#33268;&#30456;&#23545;&#20887;&#20313;&#30340;&#35745;&#31639;&#12290;&#36127;&#36733;&#19981;&#24179;&#34913;&#21487;&#33021;&#26159;&#30001;&#20110;&#29616;&#26377;&#36335;&#30001;&#31574;&#30053;&#22987;&#32456;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#30340;&#19987;&#23478;&#23548;&#33268;&#30340;&#12290;&#20840;&#23545;&#20840;&#36807;&#31243;&#20013;&#39057;&#32321;&#30340;&#33410;&#28857;&#38388;&#36890;&#20449;&#20063;&#26174;&#33879;&#24310;&#38271;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#24615;&#33021;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#19987;&#23478;&#23481;&#37327;&#30340;&#26368;&#23567;&#38408;&#20540;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#30340;&#38376;&#25511;&#26435;&#37325;&#19982;&#20998;&#37197;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#26368;&#22823;&#35282;&#20559;&#24046;&#35745;&#31639;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We por
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;Split Learning&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36807;&#31243;&#25552;&#39640;&#20102;&#20551;&#32930;&#25511;&#21046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03233</link><description>&lt;p&gt;
&#22522;&#20110;Split Learning&#30340;&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices. (arXiv:2401.03233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;Split Learning&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36807;&#31243;&#25552;&#39640;&#20102;&#20551;&#32930;&#25511;&#21046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Split Learning (SL)&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#24212;&#29992;&#20110;&#22522;&#20110;&#32908;&#30005;&#30340;&#20551;&#32930;&#25511;&#21046;&#12290;&#19982;&#28145;&#24230;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#31561;&#20854;&#20182;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;SL&#33021;&#22815;&#25552;&#20379;&#26356;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#20551;&#32930;&#35774;&#22791;&#22312;&#22788;&#29702;&#33021;&#21147;&#21644;&#30005;&#27744;&#23551;&#21629;&#26041;&#38754;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;SL&#30340;&#21487;&#34892;&#24615;&#28304;&#20110;&#20854;&#22266;&#26377;&#30340;&#27169;&#22411;&#20998;&#21106;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#25191;&#34892;&#36739;&#23567;&#30340;&#27169;&#22411;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#19981;&#24688;&#24403;&#30340;&#20999;&#23618;&#20250;&#38459;&#30861;SL&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#12290;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#25913;&#21892;&#20551;&#32930;&#25511;&#21046;&#30340;&#32908;&#30005;&#27169;&#24335;&#35782;&#21035;&#20219;&#21153;&#20013;&#26174;&#33879;&#21152;&#36895;&#20102;&#25910;&#25947;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split Learning (SL) is a promising Distributed Learning approach in electromyography (EMG) based prosthetic control, due to its applicability within resource-constrained environments. Other learning approaches, such as Deep Learning and Federated Learning (FL), provide suboptimal solutions, since prosthetic devices are extremely limited in terms of processing power and battery life. The viability of implementing SL in such scenarios is caused by its inherent model partitioning, with clients executing the smaller model segment. However, selecting an inadequate cut layer hinders the training process in SL systems. This paper presents an algorithm for optimal cut layer selection in terms of maximizing the convergence rate of the model. The performance evaluation demonstrates that the proposed algorithm substantially accelerates the convergence in an EMG pattern recognition task for improving prosthetic device control.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24102;&#22827;&#22971;&#30340;&#21307;&#38498;/&#23621;&#27665;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#20197;&#21450;&#23545;&#31283;&#23450;B&#21305;&#37197;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#25509;&#36817;&#21487;&#34892;&#30340;&#31283;&#23450;&#21305;&#37197;&#65292;&#24182;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2311.00405</link><description>&lt;p&gt;
&#22827;&#22971;&#21487;&#20197;&#34987;&#35299;&#20915;&#65306;&#26032;&#30340;&#31639;&#27861;&#21644;&#23545;&#24102;&#22827;&#22971;&#30340;&#21307;&#38498;/&#23621;&#27665;&#38382;&#39064;&#30340;&#38590;&#24230;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Couples can be tractable: New algorithms and hardness results for the Hospitals / Residents problem with Couples. (arXiv:2311.00405v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24102;&#22827;&#22971;&#30340;&#21307;&#38498;/&#23621;&#27665;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#20197;&#21450;&#23545;&#31283;&#23450;B&#21305;&#37197;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#25509;&#36817;&#21487;&#34892;&#30340;&#31283;&#23450;&#21305;&#37197;&#65292;&#24182;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#22827;&#22971;&#30340;&#21307;&#38498;/&#23621;&#27665;&#38382;&#39064;&#65288;Hospitals / Residents problem with Couples&#65292;&#31616;&#31216;HRC&#65289;&#65292;&#20854;&#20013;&#35299;&#30340;&#19968;&#31181;&#26159;&#31283;&#23450;&#21305;&#37197;&#65292;&#25110;&#32773;&#25253;&#21578;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#24102;&#22827;&#22971;&#30340;HRC&#23454;&#20363;&#20013;&#25214;&#21040;&#19968;&#20010;&#25509;&#36817;&#21487;&#34892;&#30340;&#31283;&#23450;&#21305;&#37197;&#65288;&#36890;&#36807;&#26368;&#22810;&#35843;&#25972;&#21307;&#38498;&#30340;&#23481;&#37327;1&#20010;&#21333;&#20301;&#65289;&#65292;&#20854;&#20013;&#22827;&#22971;&#30340;&#20559;&#22909;&#26159;&#23376;&#21709;&#24212;&#24615;&#30340;&#65288;&#21363;&#65292;&#22914;&#26524;&#19968;&#20010;&#25104;&#21592;&#36716;&#31227;&#21040;&#19968;&#20010;&#26356;&#22909;&#30340;&#21307;&#38498;&#65292;&#37027;&#20040;&#22827;&#22971;&#20063;&#20250;&#21464;&#24471;&#26356;&#22909;&#65289;&#21644;&#23376;&#23436;&#22791;&#24615;&#30340;&#65288;&#21363;&#65292;&#23545;&#20110;&#27599;&#23545;&#20010;&#21035;&#21487;&#25509;&#21463;&#30340;&#21307;&#38498;&#37117;&#26159;&#22827;&#22971;&#19968;&#36215;&#21487;&#25509;&#21463;&#30340;&#65289;&#65292;&#36890;&#36807;&#23558;&#20854;&#35268;&#32422;&#21040;&#31283;&#23450;&#22266;&#23450;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;HRC&#23376;&#21709;&#24212;&#24615;&#12289;&#23376;&#23436;&#22791;&#23454;&#20363;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#35813;&#23454;&#20363;&#26159;&#19968;&#20010;&#21452;&#37325;&#24066;&#22330;&#65292;&#25110;&#32773;&#25152;&#26377;&#22827;&#22971;&#23646;&#20110;&#20960;&#31181;&#21487;&#33021;&#31867;&#22411;&#20043;&#19968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20063;&#24847;&#21619;&#30528;&#31283;&#23450;B&#21305;&#37197;&#38382;&#39064;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#65292;&#20854;&#20013;&#24213;&#23618;&#22270;&#26159;&#24102;&#26377;&#29615;&#30340;&#22810;&#37325;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we study the {\sc Hospitals / Residents problem with Couples} ({\sc hrc}), where a solution is a stable matching or a report that none exists. We present a novel polynomial-time algorithm that can find a near-feasible stable matching (adjusting the hospitals' capacities by at most 1) in an {\sc hrc} instance where the couples' preferences are sub-responsive (i.e., if one member switches to a better hospital, than the couple also improves) and sub-complete (i.e., each pair of hospitals that are individually acceptable to both members are jointly acceptable for the couple) by reducing it to an instance of the {\sc Stable Fixtures} problem. We also present a polynomial-time algorithm for {\sc hrc} in a sub-responsive, sub-complete instance that is a Dual Market, or where all couples are one of several possible types. We show that our algorithm also implies the polynomial-time solvability of a stable b-matching problem, where the underlying graph is a multigraph with loops.  
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.17894</link><description>&lt;p&gt;
&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey. (arXiv:2310.17894v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#29992;&#25143;&#19982;&#34920;&#26684;&#25968;&#25454;&#30340;&#20132;&#20114;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#20174;&#20256;&#32479;&#30340;&#26597;&#35810;&#35821;&#35328;&#21644;&#25163;&#21160;&#32472;&#22270;&#36716;&#21521;&#26356;&#30452;&#35266;&#12289;&#22522;&#20110;&#35821;&#35328;&#30340;&#30028;&#38754;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21450;&#20854;&#21518;&#32487;&#32773;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#36825;&#20123;&#30028;&#38754;&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#19982;&#25968;&#25454;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20123;&#30028;&#38754;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#25216;&#26415;&#65292;&#29305;&#21035;&#24378;&#35843;&#35821;&#20041;&#35299;&#26512;&#65292;&#36825;&#26159;&#23454;&#29616;&#20174;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#26597;&#35810;&#25110;&#25968;&#25454;&#21487;&#35270;&#21270;&#21629;&#20196;&#36716;&#21270;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#21518;&#20174;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#35770;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#31995;&#32479;&#35774;&#35745;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. Thi
&lt;/p&gt;</description></item><item><title>AdaptiX&#26159;&#19968;&#20010;&#36807;&#28193;&#24615;&#30340;XR&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#21161;&#21160;&#21147;&#26426;&#22120;&#20154;&#20013;&#30340;&#20849;&#21516;&#25511;&#21046;&#24212;&#29992;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#20223;&#30495;&#29615;&#22659;&#65292;&#24182;&#32467;&#21512;&#20102;&#29992;&#25143;&#33258;&#20027;&#24615;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#12290;</title><link>http://arxiv.org/abs/2310.15887</link><description>&lt;p&gt;
AdaptiX - &#19968;&#20010;&#29992;&#20110;&#21161;&#21160;&#21147;&#26426;&#22120;&#20154;&#20013;&#24320;&#21457;&#21644;&#35780;&#20272;&#20849;&#21516;&#25511;&#21046;&#24212;&#29992;&#30340;&#36807;&#28193;&#24615;XR&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AdaptiX -- A Transitional XR Framework for Development and Evaluation of Shared Control Applications in Assistive Robotics. (arXiv:2310.15887v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15887
&lt;/p&gt;
&lt;p&gt;
AdaptiX&#26159;&#19968;&#20010;&#36807;&#28193;&#24615;&#30340;XR&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#21161;&#21160;&#21147;&#26426;&#22120;&#20154;&#20013;&#30340;&#20849;&#21516;&#25511;&#21046;&#24212;&#29992;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#20223;&#30495;&#29615;&#22659;&#65292;&#24182;&#32467;&#21512;&#20102;&#29992;&#25143;&#33258;&#20027;&#24615;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#20204;&#25480;&#26435;&#34892;&#21160;&#21463;&#38480;&#21644;&#25216;&#26415;&#25509;&#21463;&#24230;&#30340;&#25552;&#39640;&#65292;&#22914;&#21512;&#20316;&#26426;&#22120;&#33218;&#31561;&#21161;&#21160;&#25216;&#26415;&#27491;&#22312;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26222;&#21450;&#25104;&#21151;&#21463;&#21040;&#21487;&#29992;&#24615;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#29992;&#25143;&#36755;&#20837;&#19982;&#36719;&#20214;&#25511;&#21046;&#22312;&#33258;&#20027;&#24615;&#36830;&#32493;&#24615;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20849;&#21516;&#25511;&#21046;&#27010;&#24565;&#25552;&#20379;&#20102;&#23558;&#26377;&#38024;&#23545;&#24615;&#22320;&#22686;&#21152;&#29992;&#25143;&#33258;&#20027;&#24615;&#19982;&#19968;&#23450;&#31243;&#24230;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#30456;&#32467;&#21512;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;AdaptiX&#30340;&#20813;&#36153;&#24320;&#28304;XR&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39640;&#20998;&#36776;&#29575;&#20223;&#30495;&#29615;&#22659;&#20013;&#24320;&#21457;&#21644;&#35780;&#20272;&#20849;&#21516;&#25511;&#21046;&#24212;&#29992;&#12290;&#21021;&#22987;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#20013;&#30340;&#31034;&#20363;&#24773;&#26223;&#19979;&#30340;&#27169;&#25311;&#26426;&#22120;&#33218;&#12289;&#22810;&#31181;&#26631;&#20934;&#25511;&#21046;&#25509;&#21475;&#21644;&#19968;&#20010;&#19987;&#38376;&#30340;&#35760;&#24405;/&#22238;&#25918;&#31995;&#32479;&#12290;AdaptiX&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20197;&#28385;&#36275;&#29305;&#23450;&#30340;&#30740;&#31350;&#38656;&#27714;&#65292;&#20801;&#35768;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#65288;HRI&#65289;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the ongoing efforts to empower people with mobility impairments and the increase in technological acceptance by the general public, assistive technologies, such as collaborative robotic arms, are gaining popularity. Yet, their widespread success is limited by usability issues, specifically the disparity between user input and software control along the autonomy continuum. To address this, shared control concepts provide opportunities to combine the targeted increase of user autonomy with a certain level of computer assistance. This paper presents the free and open-source AdaptiX XR framework for developing and evaluating shared control applications in a high-resolution simulation environment. The initial framework consists of a simulated robotic arm with an example scenario in Virtual Reality (VR), multiple standard control interfaces, and a specialized recording/replay system. AdaptiX can easily be extended for specific research needs, allowing Human-Robot Interaction (HRI) resea
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08644</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#24314;&#27169;&#20013;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems. (arXiv:2310.08644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25968;&#21313;&#24180;&#26469;&#33268;&#21147;&#20110;&#26500;&#24314;&#29992;&#20110;&#39044;&#27979;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#26102;&#38388;&#24207;&#21015;&#28436;&#21270;&#30340;&#29289;&#29702;-&#27010;&#24565; (PC) &#27169;&#22411;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064; (ML) &#30340;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20174;ML&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#29289;&#29702;&#29702;&#35299;&#30340;&#22256;&#38590;&#20351;&#24471;&#20854;&#22312;&#22686;&#24378;&#23545;&#31995;&#32479;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#31185;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#24212;&#29992;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35299;&#29289;&#29702;&#24615;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120; (MCP) &#20316;&#20026;&#24357;&#21512;PC&#27169;&#22411;&#21644;ML&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;MCP&#21033;&#29992;PC&#27169;&#22411;&#21644;GRNNs&#32972;&#21518;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#20869;&#22312;&#21516;&#26500;&#24615;&#65292;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26126;&#30830;&#34920;&#31034;&#29289;&#29702;&#36807;&#31243;&#30340;&#36136;&#37327;&#20445;&#25345;&#24615;&#36136;&#65292;&#21516;&#26102;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#21644;&#29616;&#25104;&#30340;ML&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36825;&#31181;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#65288;&#21487;&#35299;&#37322;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although decades of effort have been devoted to building Physical-Conceptual (PC) models for predicting the time-series evolution of geoscientific systems, recent work shows that Machine Learning (ML) based Gated Recurrent Neural Network technology can be used to develop models that are much more accurate. However, the difficulty of extracting physical understanding from ML-based models complicates their utility for enhancing scientific knowledge regarding system structure and function. Here, we propose a physically-interpretable Mass Conserving Perceptron (MCP) as a way to bridge the gap between PC-based and ML-based modeling approaches. The MCP exploits the inherent isomorphism between the directed graph structures underlying both PC models and GRNNs to explicitly represent the mass-conserving nature of physical processes while enabling the functional nature of such processes to be directly learned (in an interpretable manner) from available data using off-the-shelf ML technology. As
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#30456;&#20301;&#31354;&#38388;&#20013;&#26500;&#24314;&#36335;&#24452;&#27979;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.07805</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20301;&#38543;&#26426;&#26725;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling with Phase Stochastic Bridges. (arXiv:2310.07805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#30456;&#20301;&#31354;&#38388;&#20013;&#26500;&#24314;&#36335;&#24452;&#27979;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#26159;&#29992;&#20110;&#36830;&#32493;&#36755;&#20837;&#30340;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;DMs&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#65288;&#21363;&#20301;&#32622;&#31354;&#38388;&#65289;&#20013;&#26500;&#24314;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21453;&#28436;&#26469;&#24037;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20301;&#31354;&#38388;&#21160;&#21147;&#23398;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#20854;&#20013;&#30456;&#20301;&#31354;&#38388;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#21253;&#25324;&#20301;&#32622;&#21644;&#36895;&#24230;&#30340;&#22686;&#24378;&#31354;&#38388;&#12290;&#21033;&#29992;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#30456;&#20301;&#31354;&#38388;&#20013;&#30340;&#36335;&#24452;&#27979;&#24230;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#37319;&#26679;&#12290;&#19982;DMs&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#23601;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#12290;&#36825;&#31181;&#26089;&#26399;&#39044;&#27979;&#20026;&#36890;&#36807;&#27839;&#36712;&#36857;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23567;&#20989;&#25968;&#35780;&#20272;&#25968;&#37327;&#30340;&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#24182;&#25506;&#32034;&#20102;&#24503;&#35821;&#21644;&#33521;&#35821;&#30340;ChatGPT&#22238;&#24212;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#31995;&#32479;&#22810;&#27425;&#25552;&#20379;&#30456;&#21516;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#24212;&#23384;&#22312;&#24046;&#24322;&#12290;&#20351;&#29992;ChatGPT&#26469;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#24037;&#20316;&#25991;&#26412;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#31995;&#32479;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.03031</link><description>&lt;p&gt;
ChatGPT&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#26377;&#22810;&#26222;&#36941;&#65311;&#8212;&#8212; &#25506;&#32034;&#24503;&#35821;&#21644;&#33521;&#35821;ChatGPT&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses. (arXiv:2310.03031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#24182;&#25506;&#32034;&#20102;&#24503;&#35821;&#21644;&#33521;&#35821;&#30340;ChatGPT&#22238;&#24212;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#31995;&#32479;&#22810;&#27425;&#25552;&#20379;&#30456;&#21516;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#24212;&#23384;&#22312;&#24046;&#24322;&#12290;&#20351;&#29992;ChatGPT&#26469;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#24037;&#20316;&#25991;&#26412;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#31995;&#32479;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#30340;&#25512;&#20986;&#65292;OpenAI&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20379;&#20855;&#26377;&#26377;&#38480;IT&#19987;&#19994;&#30693;&#35782;&#30340;&#29992;&#25143;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#32972;&#26223;&#30340;&#29992;&#25143;&#21487;&#33021;&#32570;&#20047;&#23545;LLM&#30340;&#36866;&#24403;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#65292;&#32570;&#20047;&#23545;&#20854;&#22266;&#26377;&#38480;&#21046;&#30340;&#24847;&#35782;&#65292;&#23558;&#25509;&#21463;&#31995;&#32479;&#36755;&#20986;&#30340;&#34920;&#38754;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#36755;&#20837;&#25552;&#31034;&#21644;&#29983;&#25104;&#30340;&#22238;&#24212;&#65292;&#20197;&#35782;&#21035;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#65292;&#29992;&#25143;&#22312;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#38656;&#35201;&#24847;&#35782;&#21040;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;ChatGPT&#22312;&#33521;&#35821;&#21644;&#24503;&#35821;&#20013;&#30340;&#21453;&#24212;&#65292;&#24182;&#25552;&#20379;&#20102;&#22899;&#24615;&#12289;&#30007;&#24615;&#25110;&#20013;&#31435;&#35282;&#24230;&#30340;&#25351;&#20196;&#26102;&#65292;&#22238;&#22797;&#30340;&#26159;&#21542;&#26377;&#24046;&#24322;&#12290;&#36890;&#36807;&#28145;&#20837;&#35843;&#26597;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#36873;&#25321;&#30340;&#25552;&#31034;&#65292;&#24182;&#20998;&#26512;&#20102;&#31995;&#32479;&#22312;&#30456;&#21516;&#26041;&#24335;&#19979;&#22810;&#27425;&#25552;&#20379;&#25351;&#20196;&#26102;&#22238;&#24212;&#30340;&#24046;&#24322;&#31243;&#24230;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#26085;&#24120;&#24037;&#20316;&#25991;&#26412;&#65292;ChatGPT&#30830;&#23454;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#28982;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#35201;&#24847;&#35782;&#21040;&#65292;&#24403;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#65292;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#21040;&#20854;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the introduction of ChatGPT, OpenAI made large language models (LLM) accessible to users with limited IT expertise. However, users with no background in natural language processing (NLP) might lack a proper understanding of LLMs. Thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. We explore how ChatGPT reacts in English and German if prompted to answer from a female, male, or neutral perspective. In an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. On this basis, we show that ChatGPT is indeed useful for helping non-IT users draft texts for their daily work. However, it is absolutely crucial to 
&lt;/p&gt;</description></item><item><title>zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02554</link><description>&lt;p&gt;
zkFL: &#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02554
&lt;/p&gt;
&lt;p&gt;
zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#22312;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#32452;&#32455;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#23545;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#20449;&#20219;&#65292;&#23427;&#20197;&#20844;&#24179;&#35802;&#23454;&#30340;&#26041;&#24335;&#24418;&#25104;&#23458;&#25143;&#31471;&#30340;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#24694;&#24847;&#30340;&#21327;&#35843;&#32773;&#21487;&#33021;&#20250;&#25918;&#24323;&#24182;&#26367;&#25442;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#25110;&#32773;&#21457;&#21160;&#34394;&#20551;&#23458;&#25143;&#31471;&#30340;&#32902;&#24847;&#25915;&#20987;&#12290;&#36825;&#31181;&#24694;&#24847;&#34892;&#20026;&#35753;&#21327;&#35843;&#32773;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#25317;&#26377;&#26356;&#22810;&#25511;&#21046;&#23458;&#25143;&#31471;&#21644;&#20915;&#23450;&#26368;&#32456;&#35757;&#32451;&#32467;&#26524;&#30340;&#26435;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;zkFL&#65292;&#23427;&#21033;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;(ZKPs)&#26469;&#35299;&#20915;&#35757;&#32451;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#24694;&#24847;&#21327;&#35843;&#32773;&#38382;&#39064;&#12290;&#20026;&#20102;&#20445;&#35777;&#27491;&#30830;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#21327;&#35843;&#32773;&#38656;&#35201;&#27599;&#36718;&#25552;&#20379;&#19968;&#20010;&#35777;&#26126;&#12290;&#36825;&#20010;&#35777;&#26126;&#21487;&#20197;&#21521;&#23458;&#25143;&#31471;&#35777;&#26126;&#21327;&#35843;&#32773;&#24544;&#23454;&#25191;&#34892;&#39044;&#26399;&#34892;&#20026;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#25252;&#23458;&#25143;&#31471;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#24182;&#23545;zkFL&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
&lt;/p&gt;</description></item><item><title>MoDem-V2&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#30452;&#25509;&#23398;&#20064;&#25509;&#35302;&#20016;&#23500;&#25805;&#20316;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.14236</link><description>&lt;p&gt;
MoDem-V2: &#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#35270;&#35273;-&#36816;&#21160;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation. (arXiv:2309.14236v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14236
&lt;/p&gt;
&lt;p&gt;
MoDem-V2&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#30452;&#25509;&#23398;&#20064;&#25509;&#35302;&#20016;&#23500;&#25805;&#20316;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24076;&#26395;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#24517;&#39035;&#36890;&#36807;&#26426;&#36733;&#20256;&#24863;&#22120;&#30452;&#25509;&#24863;&#30693;&#19990;&#30028;&#12290;&#22522;&#20110;&#35270;&#35273;&#30340;&#23398;&#20064;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#21407;&#22987;&#20687;&#32032;&#30340;&#38544;&#24335;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#65292;&#28040;&#38500;&#29615;&#22659;&#35013;&#32622;&#30340;&#38656;&#27714;&#65292;&#20294;&#20165;&#20165;&#20381;&#38752;&#31232;&#30095;&#30340;&#35270;&#35273;&#22870;&#21169;&#20449;&#21495;&#22312;&#25509;&#35302;&#20016;&#23500;&#30340;&#39640;&#32500;&#25628;&#32034;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#26174;&#33879;&#21152;&#21095;&#20102;&#25506;&#32034;&#30340;&#38590;&#24230;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#31995;&#32479;&#30340;&#36866;&#29992;&#24615;&#36890;&#24120;&#23616;&#38480;&#20110;&#27169;&#25311;&#25110;&#20005;&#26684;&#24037;&#31243;&#21270;&#30340;&#29615;&#22659;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#29366;&#24577;&#20272;&#35745;&#21644;&#31264;&#23494;&#22870;&#21169;&#30340;&#25351;&#23548;&#19979;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#20195;&#29702;&#30340;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#19981;&#23433;&#20840;&#34892;&#20026;&#21644;&#37325;&#22823;&#23433;&#20840;&#25925;&#38556;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#31163;&#20102;&#36825;&#20123;&#38480;&#21046;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;MoDem-V2&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#30452;&#25509;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#23398;&#20064;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#12290;&#22312;&#26368;&#26032;&#30340;&#31639;&#27861;&#36827;&#23637;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;
&lt;/p&gt;
&lt;p&gt;
Robotic systems that aspire to operate in uninstrumented real-world environments must perceive the world directly via onboard sensing. Vision-based learning systems aim to eliminate the need for environment instrumentation by building an implicit understanding of the world based on raw pixels, but navigating the contact-rich high-dimensional search space from solely sparse visual reward signals significantly exacerbates the challenge of exploration. The applicability of such systems is thus typically restricted to simulated or heavily engineered environments since agent exploration in the real-world without the guidance of explicit state estimation and dense rewards can lead to unsafe behavior and safety faults that are catastrophic. In this study, we isolate the root causes behind these limitations to develop a system, called MoDem-V2, capable of learning contact-rich manipulation directly in the uninstrumented real world. Building on the latest algorithmic advancements in model-based
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#32423;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#21516;&#26102;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.11876</link><description>&lt;p&gt;
&#22810;&#32423;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-level Asymmetric Contrastive Learning for Medical Image Segmentation Pre-training. (arXiv:2309.11876v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#32423;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#21516;&#26102;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#22270;&#20687;&#32423;&#34920;&#31034;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#20026;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21644;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#20043;&#38388;&#30340;&#22256;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#20027;&#35201;&#38024;&#23545;&#33258;&#28982;&#22270;&#20687;&#30340;&#19979;&#28216;&#20219;&#21153;&#35774;&#35745;&#65292;&#22240;&#27492;&#24403;&#30452;&#25509;&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#65288;&#20854;&#19979;&#28216;&#20219;&#21153;&#36890;&#24120;&#26159;&#20998;&#21106;&#65289;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#26159;&#27425;&#20248;&#30340;&#29978;&#33267;&#19981;&#22914;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JCL&#30340;&#26032;&#22411;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#21516;&#26102;&#22312;&#19968;&#38454;&#27573;&#20869;&#23545;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290; &#65288;2&#65289;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#32423;&#23545;&#27604;&#25439;&#22833;&#65292;&#29992;&#20110;&#32771;&#34385;&#29305;&#24449;&#32423;&#21035;&#12289;&#22270;&#20687;&#32423;&#21035;&#21644;&#20687;&#32032;&#32423;&#21035;&#25237;&#24433;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning, which is a powerful technique for learning image-level representations from unlabeled data, leads a promising direction to dealing with the dilemma between large-scale pre-training and limited labeled data. However, most existing contrastive learning strategies are designed mainly for downstream tasks of natural images, therefore they are sub-optimal and even worse than learning from scratch when directly applied to medical images whose downstream tasks are usually segmentation. In this work, we propose a novel asymmetric contrastive learning framework named JCL for medical image segmentation with self-supervised pre-training. Specifically, (1) A novel asymmetric contrastive learning strategy is proposed to pre-train both encoder and decoder simultaneously in one-stage to provide better initialization for segmentation models. (2) A multi-level contrastive loss is designed to take the correspondence among feature-level, image-level and pixel-level projections, resp
&lt;/p&gt;</description></item><item><title>C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2309.07597</link><description>&lt;p&gt;
C-Pack: &#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#25171;&#21253;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07597
&lt;/p&gt;
&lt;p&gt;
C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;C-Pack&#65292;&#36825;&#26159;&#19968;&#22871;&#26174;&#33879;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#12290;C-Pack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#36164;&#28304;&#12290;1&#65289;C-MTEB&#26159;&#19968;&#20010;&#28085;&#30422;6&#20010;&#20219;&#21153;&#21644;35&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12290;2&#65289;C-MTP&#26159;&#19968;&#20010;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#27721;&#35821;&#35821;&#26009;&#24211;&#20013;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12290;3&#65289;C-TEM&#26159;&#19968;&#20010;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;C-MTEB&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#25152;&#26377;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#36798;&#21040;&#20102;&#21457;&#24067;&#26102;&#30340;&#26368;&#39640;+10%&#12290;&#25105;&#20204;&#36824;&#25972;&#21512;&#21644;&#20248;&#21270;&#20102;C-TEM&#30340;&#25972;&#22871;&#35757;&#32451;&#26041;&#27861;&#12290;&#38500;&#20102;&#25105;&#20204;&#20851;&#20110;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#36164;&#28304;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#36825;&#20123;&#33521;&#35821;&#27169;&#22411;&#22312;MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#24067;&#30340;&#33521;&#35821;&#25968;&#25454;&#27604;&#27721;&#35821;&#25968;&#25454;&#22823;2&#20493;&#12290;&#25152;&#26377;&#36825;&#20123;&#36164;&#28304;&#37117;&#21487;&#20197;&#22312;https://github.com/FlagOpen/FlagEmbedding&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36890;&#29992;&#38899;&#39057;&#34920;&#31034;LOA&#65292;&#23558;&#20219;&#20309;&#38899;&#39057;&#36716;&#25442;&#20026;LOA&#65292;&#24182;&#21033;&#29992;&#20197;LOA&#20026;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#38899;&#39057;&#29983;&#25104;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2308.05734</link><description>&lt;p&gt;
AudioLDM 2: &#21033;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#36827;&#34892;&#25972;&#20307;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining. (arXiv:2308.05734v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36890;&#29992;&#38899;&#39057;&#34920;&#31034;LOA&#65292;&#23558;&#20219;&#20309;&#38899;&#39057;&#36716;&#25442;&#20026;LOA&#65292;&#24182;&#21033;&#29992;&#20197;LOA&#20026;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#38899;&#39057;&#29983;&#25104;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38899;&#39057;&#29983;&#25104;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38899;&#39057;&#20013;&#20849;&#20139;&#19968;&#20123;&#20849;&#24615;&#65292;&#27604;&#22914;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#65292;&#20294;&#20026;&#27599;&#31181;&#31867;&#22411;&#35774;&#35745;&#27169;&#22411;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#29305;&#23450;&#30340;&#30446;&#26631;&#21644;&#20559;&#24046;&#65292;&#36825;&#20123;&#20559;&#24046;&#21487;&#33021;&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#30446;&#26631;&#26377;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23454;&#29616;&#38899;&#39057;&#29983;&#25104;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#29983;&#25104;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#35821;&#35328;&#38899;&#39057;&#65288;LOA&#65289;&#8221;&#30340;&#38899;&#39057;&#36890;&#29992;&#34920;&#31034;&#12290;&#20219;&#20309;&#38899;&#39057;&#37117;&#21487;&#20197;&#22522;&#20110;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#27169;&#22411;AudioMAE&#36716;&#25442;&#20026;LOA&#12290;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-2&#27169;&#22411;&#23558;&#20219;&#20309;&#24418;&#24335;&#30340;&#38899;&#39057;&#36716;&#25442;&#20026;LOA&#65292;&#24182;&#21033;&#29992;&#20197;LOA&#20026;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#38899;&#39057;&#29983;&#25104;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33258;&#28982;&#22320;&#24102;&#26469;&#20102;&#35832;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21644;&#21487;&#37325;&#29992;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;AudioMAE&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called language of audio (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09913</link><description>&lt;p&gt;
&#25506;&#32034;&#20855;&#26377;&#25551;&#36848;&#36923;&#36753;&#29305;&#24449;&#30340;&#21629;&#39064;&#21160;&#24577;&#36923;&#36753;&#30340;&#38750;&#27491;&#21017;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Exploring Non-Regular Extensions of Propositional Dynamic Logic with Description-Logics Features. (arXiv:2307.09913v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09913
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#30340;&#23545;&#35937;&#26159;ALCreg&#21644;ALCvpl&#65292;&#20998;&#21035;&#26159;&#20351;&#29992;&#27491;&#21017;&#21644;&#21487;&#35265;&#25512;&#19979;&#35821;&#35328;&#30340;&#36335;&#24452;&#34920;&#36798;&#24335;&#30340;&#25193;&#23637;&#12290;&#31532;&#19968;&#20010;ALCreg&#26159;Fischer&#21644;Ladner&#25152;&#29087;&#30693;&#30340;&#21629;&#39064;&#21160;&#24577;&#36923;&#36753;&#30340;&#19968;&#31181;&#21464;&#31181;&#12290;&#31532;&#20108;&#20010;ALCvpl&#26159;&#30001;Loding&#21644;Serre&#22312;2007&#24180;&#24341;&#20837;&#21644;&#30740;&#31350;&#30340;&#12290;ALCvpl&#36923;&#36753;&#24191;&#20041;&#19978;&#25512;&#24191;&#20102;&#35768;&#22810;&#24050;&#30693;&#30340;&#21487;&#20915;&#23450;&#24615;&#38750;&#27491;&#21017;&#25193;&#23637;&#30340;ALCreg&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28155;&#21152;&#30475;&#20284;&#26080;&#23475;&#30340;Self&#25805;&#20316;&#31526;&#21518;&#65292;&#23545;&#20110;ALCvpl&#20013;&#30340;&#27010;&#24565;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#21487;&#20915;&#23450;&#24615;&#20007;&#22833;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#20110;&#22312;ALCvpl&#20013;&#28155;&#21152;&#20010;&#20307;&#35789;&#30340;&#27010;&#24565;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#19981;&#21487;&#20915;&#23450;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#19981;&#21487;&#20915;&#23450;&#24615;&#35777;&#26126;&#21482;&#20381;&#36182;&#20110;&#19968;&#20010;&#21333;&#19968;&#30340;&#38750;&#27491;&#21017;&#65288;&#21487;&#35265;&#25512;&#19979;&#65289;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the impact of non-regular path expressions on the decidability of satisfiability checking and querying in description logics extending ALC. Our primary objects of interest are ALCreg and ALCvpl, the extensions of with path expressions employing, respectively, regular and visibly-pushdown languages. The first one, ALCreg, is a notational variant of the well-known Propositional Dynamic Logic of Fischer and Ladner. The second one, ALCvpl, was introduced and investigated by Loding and Serre in 2007. The logic ALCvpl generalises many known decidable non-regular extensions of ALCreg.  We provide a series of undecidability results. First, we show that decidability of the concept satisfiability problem for ALCvpl is lost upon adding the seemingly innocent Self operator. Second, we establish undecidability for the concept satisfiability problem for ALCvpl extended with nominals. Interestingly, our undecidability proof relies only on one single non-regular (visibly-pushdown) langu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#31934;&#24230;&#30697;&#38453;&#65292;&#20174;&#35757;&#32451;&#23436;&#25104;&#21518;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#21253;&#25324;&#27963;&#36291;&#23376;&#31354;&#38388;&#30340;&#26041;&#21521;&#21644;&#36755;&#20837;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.05639</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#27963;&#36291;&#23376;&#31354;&#38388;&#24182;&#21457;&#29616;&#37325;&#35201;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks. (arXiv:2307.05639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#31934;&#24230;&#30697;&#38453;&#65292;&#20174;&#35757;&#32451;&#23436;&#25104;&#21518;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#21253;&#25324;&#27963;&#36291;&#23376;&#31354;&#38388;&#30340;&#26041;&#21521;&#21644;&#36755;&#20837;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#19968;&#20010;&#26082;&#33021;&#36798;&#21040;&#24378;&#22823;&#39044;&#27979;&#24615;&#33021;&#65292;&#21448;&#33021;&#34987;&#20154;&#31867;&#35299;&#37322;&#30340;&#27169;&#22411;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#22256;&#38590;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#30001;&#20110;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#20914;&#31361;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20026;&#20854;&#39640;&#26031;&#26680;&#28155;&#21152;&#21487;&#23398;&#20064;&#30340;&#31934;&#24230;&#30697;&#38453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35757;&#32451;&#23436;&#25104;&#21518;&#21487;&#20197;&#20174;&#31934;&#24230;&#30697;&#38453;&#30340;&#35889;&#20013;&#25552;&#21462;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;&#29305;&#24449;&#21521;&#37327;&#35299;&#37322;&#20102;&#27169;&#22411;&#26368;&#25935;&#24863;&#30340;&#26041;&#21521;&#65292;&#25581;&#31034;&#20102;&#27963;&#36291;&#23376;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#30417;&#30563;&#38477;&#32500;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#29305;&#24449;&#21521;&#37327;&#20984;&#26174;&#20102;&#36755;&#20837;&#21644;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#30340;&#32477;&#23545;&#21464;&#21270;&#20851;&#31995;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#22522;&#20110;&#20854;&#23545;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#25552;&#21462;&#36755;&#20837;&#21464;&#37327;&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing a model that achieves a strong predictive performance and at the same time is interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the Radial Basis Function Neural Network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction. At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the predi
&lt;/p&gt;</description></item><item><title>&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#29305;&#24449;&#20449;&#24687;&#26469;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21487;&#20197;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.16950</link><description>&lt;p&gt;
&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Alternative Telescopic Displacement: An Efficient Multimodal Alignment Method. (arXiv:2306.16950v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16950
&lt;/p&gt;
&lt;p&gt;
&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#29305;&#24449;&#20449;&#24687;&#26469;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21487;&#20197;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23545;&#40784;&#26159;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#65292;&#21487;&#20197;&#23436;&#20840;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#26469;&#23454;&#29616;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#19968;&#33268;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;&#23545;ETT&#21644;MIT-BIH-Arrhythmia&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature alignment is the primary means of fusing multimodal data. We propose a feature alignment method that fully fuses multimodal information, which alternately shifts and expands feature information from different modalities to have a consistent representation in a feature space. The proposed method can robustly capture high-level interactions between features of different modalities, thus significantly improving the performance of multimodal learning. We also show that the proposed method outperforms other popular multimodal schemes on multiple tasks. Experimental evaluation of ETT and MIT-BIH-Arrhythmia, datasets shows that the proposed method achieves state of the art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#24615;&#33021;&#21487;&#33021;&#20986;&#29616;&#36870;&#21521;&#32553;&#25918;&#29616;&#35937;&#12290;&#36825;&#19968;&#36870;&#21521;&#32553;&#25918;&#30340;&#21407;&#22240;&#21487;&#33021;&#26377;&#22235;&#31181;&#65306;&#35760;&#24518;&#37325;&#29616;&#12289;&#23398;&#20064;&#26679;&#26412;&#38169;&#35823;&#12289;&#20219;&#21153;&#26131;&#20110;&#24178;&#25200;&#12289;&#21644;&#20219;&#21153;&#31034;&#33539;&#30340;&#35823;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.09479</link><description>&lt;p&gt;
&#36870;&#21521;&#32553;&#25918;&#65306;&#21464;&#24471;&#26356;&#22823;&#24182;&#19981;&#24847;&#21619;&#30528;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Inverse Scaling: When Bigger Isn't Better. (arXiv:2306.09479v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#24615;&#33021;&#21487;&#33021;&#20986;&#29616;&#36870;&#21521;&#32553;&#25918;&#29616;&#35937;&#12290;&#36825;&#19968;&#36870;&#21521;&#32553;&#25918;&#30340;&#21407;&#22240;&#21487;&#33021;&#26377;&#22235;&#31181;&#65306;&#35760;&#24518;&#37325;&#29616;&#12289;&#23398;&#20064;&#26679;&#26412;&#38169;&#35823;&#12289;&#20219;&#21153;&#26131;&#20110;&#24178;&#25200;&#12289;&#21644;&#20219;&#21153;&#31034;&#33539;&#30340;&#35823;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#12289;&#35745;&#31639;&#37327;&#30340;&#22686;&#21152;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#25439;&#22833;&#27604;&#20363;&#26377;&#21487;&#39044;&#27979;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;LMs&#20063;&#21487;&#33021;&#26174;&#31034;&#36870;&#21521;&#32553;&#25918;&#65292;&#21363;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#20219;&#21153;&#24615;&#33021;&#36234;&#26469;&#36234;&#24046;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#35757;&#32451;&#30446;&#26631;&#21644;&#25968;&#25454;&#30340;&#32570;&#38519;&#25152;&#33268;&#12290;&#26412;&#25991;&#36890;&#36807;&#20844;&#24320;&#27604;&#36187;&#65292;Inverse Scaling Prize&#65292;&#22312;11&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#36870;&#21521;&#32553;&#25918;&#29616;&#35937;&#12290;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#38598;&#21450;&#20854;&#20182;&#23454;&#20363;&#65292;&#25105;&#20204;&#35748;&#20026;&#36870;&#21521;&#32553;&#25918;&#30340;&#21407;&#22240;&#21487;&#33021;&#26377;&#22235;&#31181;&#65306;&#65288;i&#65289;&#20542;&#21521;&#20110;&#37325;&#22797;&#35760;&#24518;&#30340;&#24207;&#21015;&#32780;&#38750;&#36319;&#38543;&#19978;&#19979;&#25991;&#25351;&#31034;&#65292;&#65288;ii&#65289;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#27169;&#20223;&#19981;&#33391;&#27169;&#24335;&#65292;&#65288;iii&#65289;&#20219;&#21153;&#20013;&#26377;&#19968;&#20010;&#26131;&#20110;&#24178;&#25200;LMs&#30340;&#20219;&#21153;&#65292;&#23558;&#20854;&#27880;&#24847;&#21147;&#36716;&#31227;&#21040;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#32780;&#38750;&#36739;&#38590;&#30340;&#20219;&#21153;&#65292;&#65288;iv&#65289;&#20219;&#21153;&#30340;&#27491;&#30830;&#31034;&#33539;&#35823;&#23548;LMs&#12290;&#20316;&#32773;&#36824;&#20844;&#24067;&#20102;&#27604;&#36187;&#30340;&#33719;&#32988;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#20855;&#26377;&#21452;&#37325;&#25928;&#24212;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#21644;&#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#65292;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2306.03481</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#32416;&#32544;&#25968;&#25454;&#30340;&#36716;&#25442;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transition role of entangled data in quantum machine learning. (arXiv:2306.03481v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#20855;&#26377;&#21452;&#37325;&#25928;&#24212;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#21644;&#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#65292;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32416;&#32544;&#20316;&#20026;&#22686;&#24378;&#37327;&#23376;&#35745;&#31639;&#30340;&#36164;&#28304;&#65292;&#24050;&#32463;&#22312;&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#23558;&#32416;&#32544;&#34701;&#20837;&#21040;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25805;&#20316;&#25110;&#27979;&#37327;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#65292;&#21516;&#26102;&#22312;&#36798;&#21040;&#25351;&#23450;&#39044;&#27979;&#35823;&#24046;&#38408;&#20540;&#26102;&#21462;&#24471;&#20102;&#26356;&#20248;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#32416;&#32544;&#31243;&#24230;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#30446;&#21069;&#20173;&#32570;&#20047;&#20998;&#26512;&#24615;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#20013;&#20351;&#29992;&#32416;&#32544;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#37327;&#23376;&#19981;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#12290;&#19982;&#20197;&#24448;&#21457;&#29616;&#30340;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#23545;&#39044;&#27979;&#35823;&#24046;&#30340;&#24433;&#21709;&#21576;&#29616;&#20986;&#21452;&#37325;&#25928;&#24212;&#65292;&#21462;&#20915;&#20110;&#20801;&#35768;&#30340;&#27979;&#37327;&#27425;&#25968;&#12290;&#22312;&#26377;&#20805;&#20998;&#30340;&#27979;&#37327;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#32416;&#32544;&#24230;&#21487;&#20197;&#25345;&#32493;&#38477;&#20302;&#39044;&#27979;&#35823;&#24046;&#65292;&#25110;&#20943;&#23569;&#36798;&#21040;&#32473;&#23450;&#35823;&#24046;&#38408;&#20540;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#12290;&#26412;&#30740;&#31350;&#38416;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#36716;&#25442;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#36827;&#24615;&#33021;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entanglement serves as the resource to empower quantum computing. Recent progress has highlighted its positive impact on learning quantum dynamics, wherein the integration of entanglement into quantum operations or measurements of quantum machine learning (QML) models leads to substantial reductions in training data size, surpassing a specified prediction error threshold. However, an analytical understanding of how the entanglement degree in data affects model performance remains elusive. In this study, we address this knowledge gap by establishing a quantum no-free-lunch (NFL) theorem for learning quantum dynamics using entangled data. Contrary to previous findings, we prove that the impact of entangled data on prediction error exhibits a dual effect, depending on the number of permitted measurements. With a sufficient number of measurements, increasing the entanglement of training data consistently reduces the prediction error or decreases the required size of the training data to ac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.02865</link><description>&lt;p&gt;
&#25235;&#20303;&#24847;&#22806;&#25910;&#33719;&#65306;&#22312;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#20215;&#20540;(arXiv:2306.02865v2 [cs.LG]&#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. (arXiv:2306.02865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39640;&#36136;&#37327;&#30340; Q &#20540;&#20989;&#25968;&#22312;&#35768;&#22810;&#29616;&#20195;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (RL) &#31639;&#27861;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#35299;&#20915;&#37319;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#21644;&#31163;&#32447;&#23398;&#20064;&#25152;&#23548;&#33268;&#30340;&#20540;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#19982;&#36825;&#31181;&#26222;&#36941;&#35266;&#28857;&#19981;&#21516;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040; Q &#20540;&#22312; RL &#35757;&#32451;&#36807;&#31243;&#30340;&#21518;&#26399;&#23454;&#38469;&#19978;&#34987;&#20302;&#20272;&#20102;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#36125;&#23572;&#26364;&#26356;&#26032;&#20013;&#65292;&#24403;&#21069;&#31574;&#30053;&#20351;&#29992;&#27604;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#26356;&#20248;&#30340;&#21160;&#20316;&#26679;&#26412;&#24046;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20010;&#38271;&#26399;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#21487;&#33021;&#38459;&#30861;&#20102;&#31574;&#30053;&#23398;&#20064;&#65292;&#38477;&#20302;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#22312;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#30340;&#21516;&#26102;&#65292;&#32467;&#21512;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#21033;&#29992;&#21644;&#25506;&#32034; (BEE) &#25805;&#20316;&#31526;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21382;&#21490;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#21160;&#20316;&#21644;&#24403;&#21069;&#31574;&#30053;&#29983;&#25104;&#30340;&#21160;&#20316;&#26469;&#26356;&#26032; Q &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning high-quality Q-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that Q-values are indeed underestimated in the latter stage of the RL training process, primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency. Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates Q-value using both historical best-performing actions and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#65288;VPT&#65289;&#25216;&#26415;&#20013;&#25552;&#31034;&#25968;&#37327;&#23545;&#24494;&#35843;&#24615;&#33021;&#21644;&#33258;&#25105;&#20851;&#27880;&#25805;&#20316;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;Prompt Condensation&#65288;PC&#65289;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#23558;&#25552;&#31034;&#25968;&#37327;&#20943;&#23569;&#32422;70&#65285;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17223</link><description>&lt;p&gt;
&#25105;&#20204;&#30495;&#30340;&#38656;&#35201;&#22823;&#37327;&#30340;&#35270;&#35273;&#25552;&#31034;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do We Really Need a Large Number of Visual Prompts?. (arXiv:2305.17223v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#65288;VPT&#65289;&#25216;&#26415;&#20013;&#25552;&#31034;&#25968;&#37327;&#23545;&#24494;&#35843;&#24615;&#33021;&#21644;&#33258;&#25105;&#20851;&#27880;&#25805;&#20316;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;Prompt Condensation&#65288;PC&#65289;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#23558;&#25552;&#31034;&#25968;&#37327;&#20943;&#23569;&#32422;70&#65285;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#19978;&#36866;&#24212;&#27169;&#22411;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#21487;&#35270;&#25552;&#31034;&#35843;&#25972;&#65288;VPT&#65289;&#23558;&#21487;&#23398;&#20064;&#25552;&#31034;&#21152;&#21040;&#36755;&#20837;&#31354;&#38388;&#20013;&#65292;&#19982;&#20840;&#32593;&#32476;&#21442;&#25968;&#30340;&#35757;&#32451;&#30456;&#27604;&#65292;&#26174;&#31034;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24494;&#35843;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;VPT&#22686;&#21152;&#20102;&#36755;&#20837;&#26631;&#35760;&#30340;&#25968;&#37327;&#65292;&#23548;&#33268;&#39069;&#22806;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25552;&#31034;&#25968;&#37327;&#23545;&#35270;&#35273;&#21464;&#25442;&#22120;&#20307;&#31995;&#32467;&#26500;&#20013;&#24494;&#35843;&#24615;&#33021;&#21644;&#33258;&#25105;&#20851;&#27880;&#25805;&#20316;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#34920;&#26126;&#28155;&#21152;&#26356;&#22810;&#25552;&#31034;&#19981;&#20250;&#23548;&#33268;&#32447;&#24615;&#24615;&#33021;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Prompt Condensation&#65288;PC&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#38450;&#27490;&#20351;&#29992;&#23569;&#37327;&#25552;&#31034;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#22312;FGVC&#21644;VTAB-1k&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#25552;&#31034;&#25968;&#37327;&#20943;&#23569;&#32422;70&#65285;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to increasing interest in adapting models on resource-constrained edges, parameter-efficient transfer learning has been widely explored. Among various methods, Visual Prompt Tuning (VPT), prepending learnable prompts to input space, shows competitive fine-tuning performance compared to training of full network parameters. However, VPT increases the number of input tokens, resulting in additional computational overhead. In this paper, we analyze the impact of the number of prompts on fine-tuning performance and self-attention operation in a vision transformer architecture. Through theoretical and empirical analysis we show that adding more prompts does not lead to linear performance improvement. Further, we propose a Prompt Condensation (PC) technique that aims to prevent performance degradation from using a small number of prompts. We validate our methods on FGVC and VTAB-1k tasks and show that our approach reduces the number of prompts by ~70% while maintaining accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#24471;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#22312;&#26356;&#22823;&#33539;&#22260;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#20915;&#31574;&#29615;&#22659;&#20013;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13138</link><description>&lt;p&gt;
&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#30340;&#26356;&#26032;&#31561;&#20215;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Update Equivalence Framework for Decision-Time Planning. (arXiv:2304.13138v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#24471;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#22312;&#26356;&#22823;&#33539;&#22260;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#20915;&#31574;&#29615;&#22659;&#20013;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26827;&#31867;&#28216;&#25103;&#31561;&#23436;&#20840;&#20449;&#24687;&#29615;&#22659;&#20013;&#65292;&#21363;&#26102;&#20462;&#27491;&#65288;&#25110;&#26500;&#24314;&#65289;&#31574;&#30053;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26159;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#30340;&#20851;&#38190;&#12290;&#19968;&#20123;&#30740;&#31350;&#23558;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#25193;&#23637;&#21040;&#26356;&#26222;&#36941;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25169;&#20811;&#20013;&#30340;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#32771;&#34385;&#38543;&#30528;&#38750;&#20844;&#20849;&#20449;&#24687;&#37327;&#30340;&#22686;&#21152;&#32780;&#24555;&#36895;&#22686;&#38271;&#30340;&#23376;&#28216;&#25103;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#38750;&#20844;&#20849;&#20449;&#24687;&#37327;&#36739;&#22823;&#26102;&#19981;&#36215;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#32780;&#19981;&#26159;&#23376;&#28216;&#25103;&#27010;&#24565;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#27169;&#25311;&#21516;&#27493;&#23398;&#20064;&#31639;&#27861;&#30340;&#26356;&#26032;&#12290;&#36825;&#20010;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#24341;&#20837;&#19968;&#31995;&#21015;&#21407;&#21017;&#19978;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#24182;&#20026;&#26032;&#30340;&#19968;&#20010;&#31995;&#21015;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of revising (or constructing) a policy immediately prior to execution -- known as decision-time planning -- is key to achieving superhuman performance in perfect-information settings like chess and Go. A recent line of work has extended decision-time planning to more general imperfect-information settings, leading to superhuman performance in poker. However, these methods requires considering subgames whose sizes grow quickly in the amount of non-public information, making them unhelpful when the amount of non-public information is large. Motivated by this issue, we introduce an alternative framework for decision-time planning that is not based on subgames but rather on the notion of update equivalence. In this framework, decision-time planning algorithms simulate updates of synchronous learning algorithms. This framework enables us to introduce a new family of principled decision-time planning algorithms that do not rely on public information, opening the door to sound and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05215</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Billion-scale Foundation Model for Remote Sensing Images. (arXiv:2304.05215v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20808;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#39044;&#35757;&#32451;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20197;&#21450;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#26368;&#36817;&#65292;&#36965;&#24863;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#22522;&#30784;&#27169;&#22411;&#22312;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#26469;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#21442;&#25968;&#65288;&#21253;&#25324;86M&#12289;605.26M&#12289;1.3B&#21644;2.4B&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#30830;&#23450;&#21442;&#25968;&#22686;&#21152;&#26159;&#21542;&#20250;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10&#20159;&#20010;&#36965;&#24863;&#22270;&#20687;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21521;&#30740;&#31350;&#31038;&#21306;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation mod
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#30340;&#26032;&#22411;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#24515;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#32570;&#20047;&#22810;&#20010;&#23610;&#24230;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.00979</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Wasserstein Shortest-path Filtration Kernels on Graphs. (arXiv:2206.00979v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00979
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#30340;&#26032;&#22411;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#24515;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#32570;&#20047;&#22810;&#20010;&#23610;&#24230;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65288;SP&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#22270;&#26680;&#24515;&#20043;&#19968;&#12290;&#23427;&#23558;&#22270;&#20998;&#35299;&#20026;&#26368;&#30701;&#36335;&#24452;&#65292;&#24182;&#35745;&#31639;&#27599;&#20010;&#22270;&#20013;&#26368;&#30701;&#36335;&#24452;&#30340;&#39057;&#29575;&#12290;&#28982;&#32780;&#65292;SP&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#26368;&#30701;&#36335;&#24452;&#30340;&#19977;&#20803;&#34920;&#31034;&#22833;&#21435;&#20102;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;SP&#27604;&#36739;&#22270;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#22270;&#32467;&#26500;&#30340;&#22810;&#20010;&#19981;&#21516;&#23610;&#24230;&#65292;&#32780;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#24456;&#24120;&#35265;&#65292;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#38142;&#29366;&#32467;&#26500;&#12289;&#29615;&#29366;&#32467;&#26500;&#21644;&#26143;&#29366;&#32467;&#26500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#31216;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#12290;&#23427;&#20351;&#29992;&#20197;&#27599;&#20010;&#39030;&#28857;&#20026;&#26681;&#30340;&#26576;&#20010;&#28145;&#24230;&#30340;BFS&#26641;&#26469;&#38480;&#21046;&#32771;&#34385;&#26368;&#30701;&#36335;&#24452;&#30340;&#26368;&#22823;&#38271;&#24230;&#65292;&#32771;&#34385;&#21040;&#23567;&#19990;&#30028;&#29305;&#24615;&#12290;&#23427;&#32771;&#34385;&#20102;&#26368;&#30701;&#36335;&#24452;&#20013;&#25152;&#26377;&#39030;&#28857;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#26041;&#20415;&#22312;&#22810;&#20010;&#19981;&#21516;&#23610;&#24230;&#19978;&#27604;&#36739;&#22270;&#65292;&#23427;&#20174;&#39030;&#28857;&#21644;
&lt;/p&gt;
&lt;p&gt;
The traditional shortest-path graph kernel (SP) is one of the most popular graph kernels. It decomposes graphs into shortest paths and computes their frequencies in each graph. However, SP has two main challenges: Firstly, the triplet representation of the shortest path loses information. Secondly, SP compares graphs without considering the multiple different scales of the graph structure which is common in real-world graphs, e.g., the chain-, ring-, and star-structures in social networks. To overcome these two challenges, we develop a novel shortest-path graph kernel called the Multi-scale Wasserstein Shortest-Path Filtration graph kernel (MWSPF). It uses a BFS tree of a certain depth rooted at each vertex to restrict the maximum length of the shortest path considering the small world property. It considers the labels of all the vertices in the shortest path. To facilitate the comparison of graphs at multiple different scales, it augments graphs from both the aspects of the vertex and
&lt;/p&gt;</description></item></channel></rss>