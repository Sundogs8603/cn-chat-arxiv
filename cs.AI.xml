<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#20449;&#21495;&#20256;&#25773;&#30340;&#20844;&#24335;&#65292;&#25552;&#20986;&#20102;DeepScaleLM&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#28145;&#23618;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#27973;&#23618;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09635</link><description>&lt;p&gt;
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
&lt;/p&gt;
&lt;p&gt;
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09635
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#20449;&#21495;&#20256;&#25773;&#30340;&#20844;&#24335;&#65292;&#25552;&#20986;&#20102;DeepScaleLM&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#28145;&#23618;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#27973;&#23618;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;transformer&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#28145;&#24230;&#26041;&#38754;&#20173;&#28982;&#24456;&#38590;&#25193;&#23637;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#21069;&#21521;&#21644;&#21453;&#21521;&#20449;&#21495;&#30697;&#30340;&#20844;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#29702;&#35299;&#21644;&#32531;&#35299;&#19982;&#39640;&#27880;&#24847;&#21147;&#20998;&#25968;&#30456;&#20851;&#30340;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#12289;&#31209;&#22349;&#32553;&#21644;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;DeepScaleLM&#65292;&#19968;&#31181;&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#36890;&#36807;&#35813;&#26041;&#26696;&#33021;&#22815;&#22312;&#27169;&#22411;&#20013;&#20445;&#25345;&#21333;&#20301;&#36755;&#20986;/&#26799;&#24230;&#30697;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#20855;&#26377;100&#22810;&#23618;&#30340;&#38750;&#24120;&#28145;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;transformer&#27169;&#22411;&#21487;&#20197;&#26356;&#28145; - &#25105;&#20204;&#30340;&#28145;&#23618;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#21253;&#25324;&#20165;&#32534;&#30721;&#22120;&#12289;&#20165;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#20307;&#65292;&#36866;&#29992;&#20110;Pre-LN&#21644;Post-LN transformers&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09635v1 Announce Type: cross  Abstract: In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These imp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;3D-VLA&#65292;&#36890;&#36807;&#23558;3D&#24863;&#30693;&#12289;&#25512;&#29702;&#21644;&#21160;&#20316;&#26080;&#32541;&#36830;&#25509;&#65292;&#24314;&#31435;&#19968;&#20010;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;VLA&#27169;&#22411;&#21482;&#33021;&#22788;&#29702;2D&#36755;&#20837;&#19988;&#24573;&#35270;&#19990;&#30028;&#21160;&#24577;&#19982;&#21160;&#20316;&#20043;&#38388;&#20851;&#31995;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.09631</link><description>&lt;p&gt;
3D-VLA: &#19968;&#20010;3D&#35270;&#35273;-&#35821;&#35328;-&#21160;&#20316;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
3D-VLA: A 3D Vision-Language-Action Generative World Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;3D-VLA&#65292;&#36890;&#36807;&#23558;3D&#24863;&#30693;&#12289;&#25512;&#29702;&#21644;&#21160;&#20316;&#26080;&#32541;&#36830;&#25509;&#65292;&#24314;&#31435;&#19968;&#20010;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;VLA&#27169;&#22411;&#21482;&#33021;&#22788;&#29702;2D&#36755;&#20837;&#19988;&#24573;&#35270;&#19990;&#30028;&#21160;&#24577;&#19982;&#21160;&#20316;&#20043;&#38388;&#20851;&#31995;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#35273;-&#35821;&#35328;-&#21160;&#20316;&#65288;VLA&#65289;&#27169;&#22411;&#20381;&#36182;&#20110;2D&#36755;&#20837;&#65292;&#32570;&#20047;&#19982;&#26356;&#24191;&#38420;&#30340;3D&#29289;&#29702;&#19990;&#30028;&#34701;&#21512;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#36807;&#23398;&#20064;&#20174;&#24863;&#30693;&#21040;&#21160;&#20316;&#30340;&#30452;&#25509;&#26144;&#23556;&#26469;&#25191;&#34892;&#21160;&#20316;&#39044;&#27979;&#65292;&#24573;&#30053;&#20102;&#19990;&#30028;&#30340;&#24191;&#27867;&#21160;&#24577;&#21644;&#21160;&#20316;&#19982;&#21160;&#24577;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#25317;&#26377;&#25551;&#32472;&#20851;&#20110;&#26410;&#26469;&#22330;&#26223;&#30340;&#24819;&#35937;&#65292;&#20197;&#30456;&#24212;&#22320;&#35268;&#21010;&#34892;&#21160;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31995;&#21015;&#26032;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#65292;&#26080;&#32541;&#22320;&#23558;3D&#24863;&#30693;&#12289;&#25512;&#29702;&#21644;&#21160;&#20316;&#36890;&#36807;&#19968;&#20010;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#30456;&#36830;&#65292;&#25552;&#20986;&#20102;3D-VLA&#12290;&#20855;&#20307;&#22320;&#65292;3D-VLA&#24314;&#31435;&#22312;&#22522;&#20110;3D&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20043;&#19978;&#65292;&#24182;&#24341;&#20837;&#19968;&#32452;&#20132;&#20114;&#26631;&#35760;&#20197;&#19982;&#20855;&#36523;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23558;&#29983;&#25104;&#33021;&#21147;&#27880;&#20837;&#27169;&#22411;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#20855;&#36523;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;LLM&#23545;&#40784;&#20197;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09631v1 Announce Type: cross  Abstract: Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting
&lt;/p&gt;</description></item><item><title>Quiet-STaR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27867;&#21270;&#29256;&#26412;&#65292;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.09629</link><description>&lt;p&gt;
Quiet-STaR: &#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#24049;&#23398;&#20250;&#24605;&#32771;&#21518;&#20877;&#35828;&#35805;
&lt;/p&gt;
&lt;p&gt;
Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09629
&lt;/p&gt;
&lt;p&gt;
Quiet-STaR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27867;&#21270;&#29256;&#26412;&#65292;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#21644;&#20132;&#35848;&#26102;&#65292;&#20154;&#20204;&#26377;&#26102;&#20250;&#20572;&#19979;&#26469;&#24605;&#32771;&#12290;&#23613;&#31649;&#20197;&#25512;&#29702;&#20026;&#37325;&#28857;&#30340;&#20316;&#21697;&#36890;&#24120;&#23558;&#25512;&#29702;&#26694;&#23450;&#20026;&#22238;&#31572;&#38382;&#39064;&#25110;&#23436;&#25104;&#20195;&#29702;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20294;&#25512;&#29702;&#20960;&#20046;&#37117;&#38544;&#21547;&#22312;&#25152;&#26377;&#20070;&#38754;&#25991;&#26412;&#20013;&#12290;&#20363;&#22914;&#65292;&#36825;&#36866;&#29992;&#20110;&#35777;&#26126;&#20013;&#26410;&#26126;&#30830;&#35828;&#26126;&#30340;&#27493;&#39588;&#65292;&#20197;&#21450;&#25903;&#25745;&#23545;&#35805;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#22312;&#33258;&#23398;&#20064;&#25512;&#29702;&#32773;&#65288;STaR&#65292;Zelikman&#31561;&#65292;2022&#65289;&#20013;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#25512;&#26029;&#26469;&#33258;&#38382;&#31572;&#20013;&#26377;&#29992;&#30340;&#24605;&#32771;&#65292;&#24182;&#23398;&#20064;&#37027;&#20123;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#30340;&#24605;&#32771;&#12290;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;--&#29702;&#24819;&#24773;&#20917;&#19979;, &#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20250;&#20174;&#20219;&#24847;&#25991;&#26412;&#20013;&#25512;&#26029;&#26410;&#26126;&#30830;&#35828;&#26126;&#30340;&#24605;&#32771;&#12290;&#25105;&#20204;&#25552;&#20986;Quiet-STaR&#65292;&#36825;&#26159;STaR&#30340;&#19968;&#20010;&#27867;&#21270;&#29256;&#26412;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#20854;&#39044;&#27979;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;1&#65289;&#29983;&#25104;&#36830;&#32493;&#30340;&#35745;&#31639;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09629v1 Announce Type: cross  Abstract: When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continu
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#23567;&#21270;&#26368;&#20248;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20026;&#40065;&#26834;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#24102;&#26469;&#26032;&#39062;&#35270;&#35282;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#19982;&#26631;&#20934;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20989;&#25968;&#36924;&#36817;&#30340;&#21306;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.09621</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;&#26368;&#20248;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#20998;&#24067;&#40065;&#26834;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09621
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#23567;&#21270;&#26368;&#20248;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20026;&#40065;&#26834;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#24102;&#26469;&#26032;&#39062;&#35270;&#35282;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#19982;&#26631;&#20934;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20989;&#25968;&#36924;&#36817;&#30340;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23547;&#27714;&#38024;&#23545;&#29615;&#22659;&#25200;&#21160;&#30340;&#40065;&#26834;&#31574;&#30053;&#35757;&#32451;&#65292;&#36890;&#36807;&#24314;&#27169;&#21160;&#24577;&#19981;&#30830;&#23450;&#24615;&#26469;&#35843;&#29992;&#20989;&#25968;&#36924;&#36817;&#65292;&#24403;&#38754;&#23545;&#24222;&#22823;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#26102;&#65292;&#36825;&#31181;RL&#38656;&#35201;&#32771;&#34385;&#21040;&#21160;&#24577;&#19981;&#30830;&#23450;&#24615;&#65292;&#24341;&#20837;&#20102;&#22522;&#26412;&#30340;&#38750;&#32447;&#24615;&#21644;&#35745;&#31639;&#36127;&#25285;&#65292;&#36825;&#32473;&#20998;&#26512;&#21644;&#23454;&#38469;&#24212;&#29992;&#20989;&#25968;&#36924;&#36817;&#25552;&#20986;&#20102;&#29420;&#29305;&#25361;&#25112;&#12290;&#22312;&#22522;&#26412;&#35774;&#32622;&#19979;&#65292;&#25552;&#35758;&#26368;&#23567;&#21270;&#26368;&#20248;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20989;&#25968;&#36924;&#36817;&#65292;&#24182;&#22312;&#40065;&#26834;&#31163;&#32447;RL&#30340;&#32972;&#26223;&#19979;&#21551;&#21160;&#23545;&#23454;&#20363;&#30456;&#20851;&#27425;&#20248;&#24615;&#20998;&#26512;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#40065;&#26834;&#31163;&#32447;RL&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#26412;&#36136;&#19978;&#19982;&#26631;&#20934;&#31163;&#32447;RL&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#26377;&#26126;&#26174;&#21306;&#21035;&#65292;&#21487;&#33021;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#29702;&#35770;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#22320;&#20381;&#36182;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09621v1 Announce Type: cross  Abstract: Distributionally robust offline reinforcement learning (RL), which seeks robust policy training against environment perturbation by modeling dynamics uncertainty, calls for function approximations when facing large state-action spaces. However, the consideration of dynamics uncertainty introduces essential nonlinearity and computational burden, posing unique challenges for analyzing and practically employing function approximation. Focusing on a basic setting where the nominal model and perturbed models are linearly parameterized, we propose minimax optimal and computationally efficient algorithms realizing function approximation and initiate the study on instance-dependent suboptimality analysis in the context of robust offline RL. Our results uncover that function approximation in robust offline RL is essentially distinct from and probably harder than that in standard offline RL. Our algorithms and theoretical results crucially depen
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#26497;&#22823;&#24433;&#21709;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32780;&#26412;&#32508;&#36848;&#21017;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#26029;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12289;&#35299;&#20915;&#20844;&#24179;&#21644;&#23433;&#20840;&#38382;&#39064;&#12289;&#25552;&#20379;&#35299;&#37322;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.09606</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21327;&#20316;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09606
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#26497;&#22823;&#24433;&#21709;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32780;&#26412;&#32508;&#36848;&#21017;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#26029;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12289;&#35299;&#20915;&#20844;&#24179;&#21644;&#23433;&#20840;&#38382;&#39064;&#12289;&#25552;&#20379;&#35299;&#37322;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36890;&#36807;&#25429;&#25417;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#24433;&#21709;&#20102;&#21508;&#31181;NLP&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#35843;&#26597;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;LLMs&#30340;&#22240;&#26524;&#35270;&#35282;&#65292;&#22312;&#20197;&#19979;&#39046;&#22495;&#23637;&#24320;&#65306;&#29702;&#35299;&#21644;&#25913;&#36827;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;LLMs&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#20026;LLMs&#25552;&#20379;&#35299;&#37322;&#65292;&#24182;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;&#21516;&#26102;&#65292;LLMs&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21453;&#36807;&#26469;&#21487;&#20197;&#36890;&#36807;&#24110;&#21161;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#21644;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26469;&#20419;&#36827;&#22240;&#26524;&#25512;&#26029;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#19982;LLMs&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#38598;&#20307;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09606v1 Announce Type: cross  Abstract: Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables. The emergence of generative Large Language Models (LLMs) has significantly impacted various NLP domains, particularly through their advanced reasoning capabilities. This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs' reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality. Meanwhile, LLMs' strong reasoning capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CF-SimCLR&#65292;&#19968;&#31181;&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#20284;&#21453;&#20107;&#23454;&#25512;&#26029;&#21019;&#36896;&#27491;&#26679;&#26412;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#37319;&#38598;&#20559;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09605</link><description>&lt;p&gt;
&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#65306;&#36890;&#36807;&#22240;&#26524;&#22270;&#20687;&#21512;&#25104;&#33719;&#24471;&#31283;&#20581;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Counterfactual contrastive learning: robust representations via causal image synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CF-SimCLR&#65292;&#19968;&#31181;&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#20284;&#21453;&#20107;&#23454;&#25512;&#26029;&#21019;&#36896;&#27491;&#26679;&#26412;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#37319;&#38598;&#20559;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#39044;&#35757;&#32451;&#24050;&#34987;&#24191;&#27867;&#35748;&#20026;&#33021;&#22815;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#26631;&#31614;&#35774;&#32622;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#22686;&#24378;&#31649;&#36947;&#30340;&#36873;&#25321;&#25935;&#24863;&#12290;&#27491;&#26679;&#26412;&#24212;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#21516;&#26102;&#30772;&#22351;&#22495;&#29305;&#23450;&#20449;&#24687;&#12290;&#26631;&#20934;&#22686;&#24378;&#31649;&#36947;&#36890;&#36807;&#39044;&#23450;&#20041;&#30340;&#20809;&#24230;&#21464;&#25442;&#27169;&#25311;&#22495;&#29305;&#23450;&#21464;&#21270;&#65292;&#20294;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#27169;&#25311;&#30495;&#23454;&#30340;&#39046;&#22495;&#21464;&#21270;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#36817;&#22312;&#21453;&#20107;&#23454;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CF-SimCLR&#65292;&#19968;&#31181;&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#36817;&#20284;&#21453;&#20107;&#23454;&#25512;&#26029;&#36827;&#34892;&#27491;&#26679;&#26412;&#21019;&#24314;&#12290;&#23545;&#33016;&#37096;X&#20809;&#21644;&#20083;&#33146;X&#20809;&#31561;&#20116;&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;CF-SimCLR&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#33719;&#21462;&#20559;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#22312;&#20004;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09605v1 Announce Type: cross  Abstract: Contrastive pretraining is well-known to improve downstream task performance and model generalisation, especially in limited label settings. However, it is sensitive to the choice of augmentation pipeline. Positive pairs should preserve semantic information while destroying domain-specific information. Standard augmentation pipelines emulate domain-specific changes with pre-defined photometric transformations, but what if we could simulate realistic domain changes instead? In this work, we show how to utilise recent progress in counterfactual image generation to this effect. We propose CF-SimCLR, a counterfactual contrastive learning approach which leverages approximate counterfactual inference for positive pair creation. Comprehensive evaluation across five datasets, on chest radiography and mammography, demonstrates that CF-SimCLR substantially improves robustness to acquisition shift with higher downstream performance on both in- an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#31934;&#24230;&#19979;&#36827;&#34892;&#35757;&#32451;&#12289;&#22312;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#36827;&#34892;&#22235;&#33293;&#20116;&#20837;&#65292;&#24182;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#65292;&#20197;&#24212;&#23545;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.09603</link><description>&lt;p&gt;
&#25511;&#21046;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#36827;&#34892;&#20048;&#35266;&#21487;&#39564;&#35777;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Optimistic Verifiable Training by Controlling Hardware Nondeterminism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09603
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#31934;&#24230;&#19979;&#36827;&#34892;&#35757;&#32451;&#12289;&#22312;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#36827;&#34892;&#22235;&#33293;&#20116;&#20837;&#65292;&#24182;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#65292;&#20197;&#24212;&#23545;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#26085;&#30410;&#22686;&#21152;&#30340;&#35745;&#31639;&#38656;&#27714;&#23548;&#33268;&#20102;&#20026;&#32570;&#20047;&#24517;&#35201;&#36164;&#28304;&#30340;&#23458;&#25143;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#26381;&#21153;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#35757;&#32451;&#30340;&#27491;&#30830;&#24615;&#24182;&#38450;&#33539;&#28508;&#22312;&#30340;&#35757;&#32451;&#26102;&#25915;&#20987;&#65292;&#20363;&#22914;&#25968;&#25454;&#27602;&#21270;&#65292;&#37117;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#21487;&#39564;&#35777;&#35757;&#32451;&#30340;&#24037;&#20316;&#20027;&#35201;&#20998;&#20026;&#20004;&#31867;&#65306;&#22522;&#20110;&#35777;&#26126;&#30340;&#31995;&#32479;&#65292;&#30001;&#20110;&#38656;&#35201;&#21152;&#23494;&#25216;&#26415;&#32780;&#38590;&#20197;&#25193;&#23637;&#65292;&#20197;&#21450;&#32771;&#34385;&#21040;&#19968;&#20010;&#21487;&#20449;&#31532;&#19977;&#26041;&#23457;&#35745;&#21592;&#22797;&#21046;&#35757;&#32451;&#36807;&#31243;&#30340;&#8220;&#20048;&#35266;&#8221;&#26041;&#27861;&#12290; &#21518;&#32773;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;GPU&#31867;&#22411;&#20043;&#38388;&#30340;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#38459;&#27490;&#23457;&#35745;&#21592;&#31934;&#30830;&#22797;&#21046;&#35757;&#32451;&#36807;&#31243;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#26041;&#26696;&#19981;&#22815;&#20581;&#22766;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#30340;&#31934;&#24230;&#19979;&#36827;&#34892;&#65292;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#22235;&#33293;&#20116;&#20837;&#65292;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09603v1 Announce Type: cross  Abstract: The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and "optimistic" methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#27010;&#29575;&#35770;&#20026;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#30340;&#26367;&#20195;&#22522;&#30784;&#65292;&#21487;&#20197;&#25193;&#23637;&#22240;&#26524;&#35782;&#21035;&#25216;&#26415;&#21040;&#26356;&#22810;&#22240;&#26524;&#35774;&#32622;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.09580</link><description>&lt;p&gt;
&#31639;&#27861;&#21477;&#27861;&#22240;&#26524;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Algorithmic syntactic causal identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09580
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#27010;&#29575;&#35770;&#20026;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#30340;&#26367;&#20195;&#22522;&#30784;&#65292;&#21487;&#20197;&#25193;&#23637;&#22240;&#26524;&#35782;&#21035;&#25216;&#26415;&#21040;&#26356;&#22810;&#22240;&#26524;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;CBN&#65289;&#20013;&#36827;&#34892;&#22240;&#26524;&#35782;&#21035;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#39033;&#37325;&#35201;&#24037;&#20855;&#65292;&#20801;&#35768;&#20174;&#29702;&#35770;&#19978;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#30340;&#35266;&#27979;&#20998;&#24067;&#25512;&#23548;&#24178;&#39044;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22240;&#26524;&#35782;&#21035;&#24418;&#24335;&#65292;&#22914;&#20351;&#29992;d&#20998;&#31163;&#21644;do-&#28436;&#31639;&#30340;&#25216;&#26415;&#37117;&#26159;&#22312;CBN&#19978;&#21033;&#29992;&#32463;&#20856;&#27010;&#29575;&#35770;&#30340;&#25968;&#23398;&#35821;&#35328;&#34920;&#36798;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#22240;&#26524;&#35774;&#32622;&#20013;&#65292;&#27010;&#29575;&#35770;&#21644;&#22240;&#27492;&#30446;&#21069;&#30340;&#22240;&#26524;&#35782;&#21035;&#25216;&#26415;&#19981;&#36866;&#29992;&#65292;&#22914;&#20851;&#31995;&#25968;&#25454;&#24211;&#12289;&#25968;&#25454;&#27969;&#31243;&#24207;&#65288;&#20363;&#22914;&#30828;&#20214;&#25551;&#36848;&#35821;&#35328;&#65289;&#12289;&#20998;&#24067;&#24335;&#31995;&#32479;&#21644;&#22823;&#22810;&#25968;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#29992;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#30340;&#26367;&#20195;&#20844;&#29702;&#22522;&#30784;&#26469;&#28040;&#38500;&#36825;&#31181;&#38480;&#21046;&#12290;&#22312;&#36825;&#31181;&#26367;&#20195;&#20844;&#29702;&#21270;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#33719;&#24471;&#19968;&#20010;&#26126;&#30830;&#19988;&#28165;&#26224;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09580v1 Announce Type: new  Abstract: Causal identification in causal Bayes nets (CBNs) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle. However, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on CBNs. However, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. We show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories. In this alternative axiomatization, we show how an unambiguous and clean
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21306;&#22359;&#38142;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36131;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26550;&#26500;&#65292;&#25552;&#39640;&#33258;&#20027;&#20195;&#29702;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#65292;&#22686;&#24378;&#20195;&#29702;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#27807;&#36890;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.09567</link><description>&lt;p&gt;
&#36890;&#36807;&#21306;&#22359;&#38142;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#33258;&#20027;&#20195;&#29702;&#30340;&#20449;&#20219;&#65306;&#19968;&#31181;&#36890;&#36807;&#21306;&#22359;&#38142;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36131;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09567
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21306;&#22359;&#38142;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36131;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26550;&#26500;&#65292;&#25552;&#39640;&#33258;&#20027;&#20195;&#29702;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#65292;&#22686;&#24378;&#20195;&#29702;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#27807;&#36890;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20195;&#29702;&#22312;&#28041;&#21450;&#20154;&#31867;&#20114;&#21160;&#30340;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#26085;&#30410;&#24341;&#36215;&#23433;&#20840;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#20107;&#20214;&#32972;&#21518;&#30340;&#24773;&#20917;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#21521;&#38750;&#19987;&#23478;&#29992;&#25143;&#35299;&#37322;&#20854;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#35299;&#37322;&#22312;&#25552;&#39640;&#21487;&#20449;&#24230;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#65292;&#20316;&#20026;&#38450;&#33539;&#22833;&#36133;&#12289;&#38169;&#35823;&#21644;&#35823;&#35299;&#30340;&#25514;&#26045;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26377;&#21161;&#20110;&#25913;&#21892;&#27807;&#36890;&#65292;&#24357;&#21512;&#20195;&#29702;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#30340;&#25928;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#22522;&#20110;ROS&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#23454;&#26045;&#30340;&#36131;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#31867;&#20284;&#40657;&#30418;&#30340;&#20803;&#32032;&#29992;&#20110;&#25552;&#20379;&#38382;&#36131;&#21046;&#65292;&#20855;&#26377;&#36890;&#36807;&#21306;&#22359;&#38142;&#25216;&#26415;&#23454;&#29616;&#30340;&#38450;&#31713;&#25913;&#23646;&#24615;&#12290;&#20854;&#27425;&#65292;&#19968;&#20010;&#36127;&#36131;&#30340;&#32452;&#20214;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09567v1 Announce Type: cross  Abstract: The deployment of autonomous agents in environments involving human interaction has increasingly raised security concerns. Consequently, understanding the circumstances behind an event becomes critical, requiring the development of capabilities to justify their behaviors to non-expert users. Such explanations are essential in enhancing trustworthiness and safety, acting as a preventive measure against failures, errors, and misunderstandings. Additionally, they contribute to improving communication, bridging the gap between the agent and the user, thereby improving the effectiveness of their interactions. This work presents an accountability and explainability architecture implemented for ROS-based mobile robots. The proposed solution consists of two main components. Firstly, a black box-like element to provide accountability, featuring anti-tampering properties achieved through blockchain technology. Secondly, a component in charge of 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#24037;&#31243;&#39046;&#22495;&#30340;&#21361;&#38505;&#20998;&#26512;&#19982;&#39118;&#38505;&#35780;&#20272;&#33258;&#21160;&#21270;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;SafetyOps&#21608;&#26399;&#20013;&#20851;&#38190;&#27493;&#39588;&#30340;&#36827;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.09565</link><description>&lt;p&gt;
&#27426;&#36814;&#24744;&#30340;&#26032;AI&#22242;&#38431;&#65306;&#20851;&#20110;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#24037;&#31243;&#39046;&#22495;&#30340;&#21361;&#38505;&#20998;&#26512;&#19982;&#39118;&#38505;&#35780;&#20272;&#33258;&#21160;&#21270;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;SafetyOps&#21608;&#26399;&#20013;&#20851;&#38190;&#27493;&#39588;&#30340;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DevOps&#22312;&#35768;&#22810;&#34892;&#19994;&#20013;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#26377;&#19968;&#20123;&#36845;&#20195;&#27963;&#21160;&#20943;&#24930;&#20102;SafetyOps&#21608;&#26399;&#30340;&#36895;&#24230;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#8220;&#21361;&#38505;&#20998;&#26512;&#19982;&#39118;&#38505;&#35780;&#20272;&#8221;&#65288;HARA&#65289;&#65292;&#36825;&#26159;&#24320;&#22987;&#23433;&#20840;&#35201;&#27714;&#35268;&#33539;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#20316;&#20026;&#22686;&#21152;SafetyOps&#20013;&#36825;&#19968;&#27493;&#36895;&#24230;&#30340;&#28508;&#22312;&#36884;&#24452;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#31995;&#32479;&#35780;&#20272;&#23427;&#20204;&#22312;&#23433;&#20840;&#24037;&#31243;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#25903;&#25345;&#20351;&#29992;LLMs&#26356;&#39640;&#31243;&#24230;&#30340;&#33258;&#21160;&#21270;HARA&#12290;&#23613;&#31649;&#25105;&#20204;&#21162;&#21147;&#33258;&#21160;&#21270;&#23613;&#21487;&#33021;&#22810;&#30340;&#27969;&#31243;&#65292;&#20294;&#19987;&#23478;&#23457;&#26597;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#20998;&#26512;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#21644;&#27491;&#30830;&#24615;&#65292;&#24182;&#30456;&#24212;&#36827;&#34892;&#24517;&#35201;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09565v1 Announce Type: cross  Abstract: DevOps is a necessity in many industries, including the development of Autonomous Vehicles. In those settings, there are iterative activities that reduce the speed of SafetyOps cycles. One of these activities is "Hazard Analysis &amp; Risk Assessment" (HARA), which is an essential step to start the safety requirements specification. As a potential approach to increase the speed of this step in SafetyOps, we have delved into the capabilities of Large Language Models (LLMs).   Our objective is to systematically assess their potential for application in the field of safety engineering. To that end, we propose a framework to support a higher degree of automation of HARA with LLMs. Despite our endeavors to automate as much of the process as possible, expert review remains crucial to ensure the validity and correctness of the analysis results, with necessary modifications made accordingly.
&lt;/p&gt;</description></item><item><title>&#23558;&#21435;&#22122;&#26041;&#27861;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#65292;&#20174;&#32780;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#23545;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#29702;&#35299;&#20197;&#21450;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.09549</link><description>&lt;p&gt;
&#23558;&#21435;&#22122;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#20197;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09549
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21435;&#22122;&#26041;&#27861;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#65292;&#20174;&#32780;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#23545;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#29702;&#35299;&#20197;&#21450;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21407;&#23376;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22914;3D&#21407;&#23376;&#20307;&#31995;&#20013;&#30340;&#21147;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#38656;&#35201;&#35745;&#31639;&#23494;&#38598;&#30340;&#20174;&#22836;&#31639;&#35745;&#31639;&#65292;&#22240;&#27492;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21435;&#22122;&#38750;&#24179;&#34913;&#32467;&#26500;&#65288;DeNS&#65289;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;DeNS&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21521;&#20854;3D&#22352;&#26631;&#28155;&#21152;&#22122;&#22768;&#26469;&#30772;&#22351;3D&#32467;&#26500;&#65292;&#28982;&#21518;&#39044;&#27979;&#22122;&#22768;&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#20165;&#38480;&#20110;&#24179;&#34913;&#32467;&#26500;&#30340;&#21435;&#22122;&#24037;&#20316;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#21435;&#22122;&#27867;&#21270;&#21040;&#26356;&#22823;&#33539;&#22260;&#30340;&#38750;&#24179;&#34913;&#32467;&#26500;&#12290;&#20027;&#35201;&#21306;&#21035;&#22312;&#20110;&#38750;&#24179;&#34913;&#32467;&#26500;&#19981;&#23545;&#24212;&#20110;&#23616;&#37096;&#33021;&#37327;&#26368;&#23567;&#20540;&#65292;&#20855;&#26377;&#38750;&#38646;&#21147;&#65292;&#22240;&#27492;&#21487;&#33021;&#20855;&#26377;&#35768;&#22810;&#21487;&#33021;&#30340;&#21407;&#23376;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09549v1 Announce Type: cross  Abstract: Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamental to many applications like molecular dynamics and catalyst design. However, simulating these interactions requires compute-intensive ab initio calculations and thus results in limited data for training neural networks. In this paper, we propose to use denoising non-equilibrium structures (DeNS) as an auxiliary task to better leverage training data and improve performance. For training with DeNS, we first corrupt a 3D structure by adding noise to its 3D coordinates and then predict the noise. Different from previous works on denoising, which are limited to equilibrium structures, the proposed method generalizes denoising to a much larger set of non-equilibrium structures. The main difference is that a non-equilibrium structure does not correspond to local energy minima and has non-zero forces, and therefore it can have many possible atomic posit
&lt;/p&gt;</description></item><item><title>&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;</title><link>https://arxiv.org/abs/2403.09539</link><description>&lt;p&gt;
API&#20445;&#25252;&#30340;LLMs&#30340;&#26631;&#24535;&#27844;&#38706;&#19987;&#26377;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Logits of API-Protected LLMs Leak Proprietary Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09539
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21830;&#19994;&#21270;&#23548;&#33268;&#20102;&#39640;&#32423;API-only&#25509;&#20837;&#19987;&#26377;&#27169;&#22411;&#30340;&#24120;&#35265;&#23454;&#36341;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#27169;&#22411;&#26550;&#26500;&#26377;&#20445;&#23432;&#30340;&#20551;&#35774;&#65292;&#20063;&#21487;&#20197;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;API&#26597;&#35810;&#20013;&#23398;&#20064;&#20851;&#20110;API&#20445;&#25252;&#30340;LLM&#30340;&#22823;&#37327;&#38750;&#20844;&#24320;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;OpenAI&#30340;gpt-3.5-turbo&#20165;&#33457;&#36153;&#19981;&#21040;1000&#32654;&#20803;&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#38598;&#20013;&#22312;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#19978;&#65306;&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;&#20102;softmax&#29942;&#39048;&#30340;&#24433;&#21709;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#36755;&#20986;&#21040;&#23436;&#25972;&#36755;&#20986;&#31354;&#38388;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#27169;&#22411;&#22270;&#20687;&#25110;&#27169;&#22411;&#31614;&#21517;&#65292;&#20174;&#32780;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#35299;&#38145;&#20102;&#20960;&#31181;&#21151;&#33021;&#65306;&#26377;&#25928;&#21457;&#29616;LLM&#30340;&#38544;&#34255;&#22823;&#23567;&#65292;&#33719;&#21462;&#23436;&#25972;&#35789;&#27719;&#36755;&#20986;&#65292;&#26816;&#27979;&#21644;&#28040;&#38500;&#19981;&#21516;&#27169;&#22411;&#26356;&#26032;&#65292;&#35782;&#21035;&#32473;&#23450;&#21333;&#20010;&#23436;&#25972;LLM&#36755;&#20986;&#30340;&#28304;LLM&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09539v1 Announce Type: cross  Abstract: The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and eve
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;VisionGPT-3D&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#23545;&#20110;3D&#35270;&#35273;&#29702;&#35299;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.09530</link><description>&lt;p&gt;
VisionGPT-3D:&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;3D&#35270;&#35273;&#29702;&#35299;&#30340;&#36890;&#29992;&#22810;&#27169;&#24577;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09530
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;VisionGPT-3D&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#23545;&#20110;3D&#35270;&#35273;&#29702;&#35299;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21521;&#35270;&#35273;&#32452;&#20214;&#30340;&#28436;&#36827;&#20419;&#36827;&#20102;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#20415;&#21033;&#65292;&#20363;&#22914;&#20174;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#12289;&#35270;&#39057;&#24182;&#35782;&#21035;&#22270;&#20687;&#20013;&#25152;&#38656;&#30340;&#20803;&#32032;&#12290;&#20197;&#21069;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#19987;&#27880;&#20110;&#22522;&#20110;&#26126;&#30830;&#23450;&#20041;&#23545;&#35937;&#30340;&#22270;&#20687;&#26816;&#27979;&#12289;&#20998;&#31867;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;&#35270;&#35273;&#23545;&#35937;&#65292;&#20026;&#25991;&#26412;&#32972;&#26223;&#25552;&#20379;&#20102;&#35270;&#35273;&#24067;&#23616;&#12290;OpenAI GPT-4&#24050;&#25104;&#20026;LLMs&#30340;&#39030;&#23792;&#65292;&#32780;&#35745;&#31639;&#26426;&#35270;&#35273;(CV)&#39046;&#22495;&#25317;&#26377;&#22823;&#37327;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#21487;&#23558;2D&#22270;&#20687;&#36716;&#25442;&#20026;&#23427;&#20204;&#30340;3D&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#31639;&#27861;&#19982;&#38382;&#39064;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#19981;&#33391;&#32467;&#26524;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;VisionGPT-3D&#26694;&#26550;&#65292; conslidate&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09530v1 Announce Type: cross  Abstract: The evolution of text to visual components facilitates people's daily lives, such as generating image, videos from text and identifying the desired elements within the images. Computer vision models involving the multimodal abilities in the previous days are focused on image detection, classification based on well-defined objects. Large language models (LLMs) introduces the transformation from nature language to visual objects, which present the visual layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs, while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models and algorithms to convert 2D images to their 3D representations. However, the mismatching between the algorithms with the problem could lead to undesired results. In response to this challenge, we propose an unified VisionGPT-3D framework to consolidate the state-of-the-art vision models, thereby facilitating the development
&lt;/p&gt;</description></item><item><title>AdaShield &#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#30462;&#29260;&#25552;&#31034;&#26041;&#27861;&#65292;&#26080;&#38656;&#24494;&#35843;&#25110;&#39069;&#22806;&#35757;&#32451;&#27169;&#22359;&#65292;&#21363;&#21487;&#20445;&#25252;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#22522;&#20110;&#32467;&#26500;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.09513</link><description>&lt;p&gt;
AdaShield&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#30462;&#29260;&#25552;&#31034;&#20445;&#25252;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#22522;&#20110;&#32467;&#26500;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09513
&lt;/p&gt;
&lt;p&gt;
AdaShield &#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#30462;&#29260;&#25552;&#31034;&#26041;&#27861;&#65292;&#26080;&#38656;&#24494;&#35843;&#25110;&#39069;&#22806;&#35757;&#32451;&#27169;&#22359;&#65292;&#21363;&#21487;&#20445;&#25252;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#22522;&#20110;&#32467;&#26500;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20986;&#29616;&#21644;&#24191;&#27867;&#37096;&#32626;&#65292;&#30830;&#20445;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#39069;&#22806;&#27169;&#24577;&#30340;&#25972;&#21512;&#65292;MLLMs&#26292;&#38706;&#20110;&#26032;&#30340;&#28431;&#27934;&#65292;&#20351;&#20854;&#23481;&#26131;&#36973;&#21463;&#22522;&#20110;&#32467;&#26500;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#21363;&#21521;&#22270;&#20687;&#20013;&#27880;&#20837;&#35821;&#20041;&#20869;&#23481;&#65288;&#20363;&#22914;&#8220;&#26377;&#23475;&#25991;&#26412;&#8221;&#65289;&#20197;&#35823;&#23548;MLLMs&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25269;&#24481;&#27492;&#31867;&#23041;&#32961;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\textbf{Ada}ptive \textbf{Shield} Prompting&#65288;\textbf{AdaShield}&#65289;&#65292;&#23427;&#22312;&#36755;&#20837;&#21069;&#28155;&#21152;&#38450;&#24481;&#25552;&#31034;&#65292;&#20197;&#22312;&#19981;&#24494;&#35843;MLLMs&#25110;&#35757;&#32451;&#39069;&#22806;&#27169;&#22359;&#65288;&#20363;&#22914;&#21518;&#38454;&#27573;&#20869;&#23481;&#26816;&#27979;&#22120;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#25252;MLLMs&#20813;&#21463;&#22522;&#20110;&#32467;&#26500;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#19968;&#20010;&#25163;&#21160;&#35774;&#35745;&#30340;&#38745;&#24577;&#38450;&#24481;&#25552;&#31034;&#65292;&#23427;&#36880;&#27493;&#24443;&#24213;&#26816;&#26597;&#22270;&#20687;&#21644;&#25351;&#20196;&#20869;&#23481;&#65292;&#24182;&#25351;&#23450;&#21709;&#24212;&#24694;&#24847;&#26597;&#35810;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09513v1 Announce Type: cross  Abstract: With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g., "harmful text") has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \textbf{Ada}ptive \textbf{Shield} Prompting (\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector). Initially, we present a manually designed static defense prompt, which thoroughly examines the image and instruction content step by step and specifies response methods to malicious queries. Fu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28436;&#21270;&#21338;&#24328;&#35770;&#37327;&#21270;&#27169;&#25311;&#29992;&#25143;&#12289;AI&#21019;&#20316;&#32773;&#21644;&#30417;&#31649;&#32773;&#38754;&#20020;&#30340;&#22256;&#22659;&#65292;&#25552;&#20986;&#25919;&#24220;&#35748;&#21487;&#21644;&#22870;&#21169;&#30417;&#31649;&#32773;&#21487;&#20197;&#28608;&#21169;&#26377;&#25928;&#30417;&#31649;&#30340;&#26426;&#21046;&#65292;&#24110;&#21161;&#24314;&#31435;&#20540;&#24471;&#20449;&#36182;&#30340;AI&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2403.09510</link><description>&lt;p&gt;
AI&#30417;&#31649;&#20449;&#20219;? &#36776;&#35782;&#29992;&#25143;&#23545;&#20110;&#24314;&#31435;&#20449;&#20219;&#21644;&#26377;&#25928;&#30340;AI&#30417;&#31649;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Trust AI Regulation? Discerning users are vital to build trust and effective AI regulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09510
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28436;&#21270;&#21338;&#24328;&#35770;&#37327;&#21270;&#27169;&#25311;&#29992;&#25143;&#12289;AI&#21019;&#20316;&#32773;&#21644;&#30417;&#31649;&#32773;&#38754;&#20020;&#30340;&#22256;&#22659;&#65292;&#25552;&#20986;&#25919;&#24220;&#35748;&#21487;&#21644;&#22870;&#21169;&#30417;&#31649;&#32773;&#21487;&#20197;&#28608;&#21169;&#26377;&#25928;&#30417;&#31649;&#30340;&#26426;&#21046;&#65292;&#24110;&#21161;&#24314;&#31435;&#20540;&#24471;&#20449;&#36182;&#30340;AI&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26222;&#36941;&#20849;&#35782;&#35748;&#20026;&#65292;AI&#21019;&#20316;&#32773;&#38656;&#35201;&#21463;&#21040;&#26576;&#31181;&#24418;&#24335;&#30340;&#30417;&#31649;&#20197;&#24320;&#21457;&#20540;&#24471;&#20449;&#36182;&#30340;&#31995;&#32479;&#65292;&#29992;&#25143;&#20063;&#38656;&#35201;&#30495;&#27491;&#30456;&#20449;&#36825;&#20123;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#30417;&#31649;&#24212;&#35813;&#37319;&#21462;&#20160;&#20040;&#24418;&#24335;&#20197;&#21450;&#22914;&#20309;&#23454;&#26045;&#23427;&#20204;&#23384;&#22312;&#24456;&#22823;&#20105;&#35758;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#26159;&#23450;&#24615;&#30340;&#65292;&#24182;&#19988;&#19981;&#33021;&#36827;&#34892;&#27491;&#24335;&#39044;&#27979;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#28436;&#21270;&#21338;&#24328;&#35770;&#21487;&#20197;&#29992;&#20110;&#37327;&#21270;&#27169;&#25311;&#29992;&#25143;&#12289;AI&#21019;&#20316;&#32773;&#21644;&#30417;&#31649;&#32773;&#38754;&#20020;&#30340;&#22256;&#22659;&#65292;&#24182;&#20026;&#19981;&#21516;&#30417;&#31649;&#21046;&#24230;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#25552;&#20379;&#35265;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21019;&#24314;&#20540;&#24471;&#20449;&#36182;&#30340;AI&#21644;&#29992;&#25143;&#20449;&#20219;&#38656;&#35201;&#30417;&#31649;&#32773;&#21463;&#21040;&#26377;&#25928;&#30417;&#31649;&#30340;&#28608;&#21169;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#28857;&#30340;&#20004;&#31181;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#31532;&#19968;&#31181;&#26426;&#21046;&#26159;&#25919;&#24220;&#21487;&#20197;&#35748;&#21487;&#24182;&#22870;&#21169;&#20570;&#22909;&#24037;&#20316;&#30340;&#30417;&#31649;&#32773;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;AI&#31995;&#32479;&#23545;&#29992;&#25143;&#26469;&#35828;&#19981;&#22826;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09510v1 Announce Type: new  Abstract: There is general agreement that some form of regulation is necessary both for AI creators to be incentivised to develop trustworthy systems, and for users to actually trust those systems. But there is much debate about what form these regulations should take and how they should be implemented. Most work in this area has been qualitative, and has not been able to make formal predictions. Here, we propose that evolutionary game theory can be used to quantitatively model the dilemmas faced by users, AI creators, and regulators, and provide insights into the possible effects of different regulatory regimes. We show that creating trustworthy AI and user trust requires regulators to be incentivised to regulate effectively. We demonstrate the effectiveness of two mechanisms that can achieve this. The first is where governments can recognise and reward regulators that do a good job. In that case, if the AI system is not too risky for users then 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;&#65288;MCA&#65289;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#35266;&#21464;&#21270;&#26469;&#40723;&#21169;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#35270;&#39057;&#20013;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#38745;&#24577;&#22806;&#35266;&#12290;</title><link>https://arxiv.org/abs/2403.09506</link><description>&lt;p&gt;
&#19981;&#35201;&#20197;&#22806;&#34920;&#21028;&#26029;: &#29992;&#20110;&#35270;&#39057;&#35782;&#21035;&#30340;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;&#65288;MCA&#65289;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#35266;&#21464;&#21270;&#26469;&#40723;&#21169;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#35270;&#39057;&#20013;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#38745;&#24577;&#22806;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30446;&#26631;&#35782;&#21035;&#20013;&#30340;&#35757;&#32451;&#27969;&#31243;&#22312;&#25968;&#25454;&#22686;&#24378;&#26102;&#24573;&#30053;&#20102;&#33394;&#35843;&#25238;&#21160;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#20250;&#24102;&#26469;&#23545;&#20998;&#31867;&#26377;&#23475;&#30340;&#22806;&#35266;&#21464;&#21270;&#65292;&#32780;&#19988;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#20063;&#26159;&#20302;&#25928;&#30340;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33394;&#35843;&#21464;&#21270;&#22312;&#35270;&#39057;&#35782;&#21035;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#21464;&#21270;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#23545;&#20110;&#21253;&#21547;&#36816;&#21160;&#20449;&#24687;&#30340;&#35270;&#39057;&#26469;&#35828;&#65292;&#38745;&#24577;&#22806;&#35266;&#19981;&#26159;&#37027;&#20040;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#39057;&#35782;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21517;&#20026;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;&#65288;MCA&#65289;&#65292;&#23427;&#22312;&#35270;&#39057;&#20013;&#24341;&#20837;&#22806;&#35266;&#21464;&#21270;&#65292;&#38544;&#24335;&#40723;&#21169;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#36816;&#21160;&#27169;&#24335;&#65292;&#32780;&#19981;&#26159;&#38745;&#24577;&#22806;&#35266;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SwapMix&#30340;&#25805;&#20316;&#65292;&#29992;&#20110;&#39640;&#25928;&#20462;&#25913;&#35270;&#39057;&#26679;&#26412;&#30340;&#22806;&#35266;&#65292;&#24182;&#24341;&#20837;&#20102;&#21464;&#24322;&#23545;&#40784;&#65288;VA&#65289;&#26469;&#35299;&#20915;SwapMix&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36843;&#20351;&#27169;&#22411;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09506v1 Announce Type: cross  Abstract: Current training pipelines in object recognition neglect Hue Jittering when doing data augmentation as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice. In this study, we investigate the effect of hue variance in the context of video recognition and find this variance to be beneficial since static appearances are less important in videos that contain motion information. Based on this observation, we propose a data augmentation method for video recognition, named Motion Coherent Augmentation (MCA), that introduces appearance variation in videos and implicitly encourages the model to prioritize motion patterns, rather than static appearances. Concretely, we propose an operation SwapMix to efficiently modify the appearance of video samples, and introduce Variation Alignment (VA) to resolve the distribution shift caused by SwapMix, enforcing the model to le
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#23454;&#29616;&#29305;&#24449;&#32858;&#21512;&#21644;&#23884;&#20837;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#30417;&#30563;&#65292;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.09502</link><description>&lt;p&gt;
EquiAV: &#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09502
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#23454;&#29616;&#29305;&#24449;&#32858;&#21512;&#21644;&#23884;&#20837;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#30417;&#30563;&#65292;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#38899;&#39057;-&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26368;&#36817;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23637;&#31034;&#20986;&#25429;&#25417;&#20016;&#23500;&#32508;&#21512;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25968;&#25454;&#22686;&#24378;&#22312;&#35768;&#22810;&#23398;&#20064;&#26041;&#27861;&#20013;&#24050;&#32463;&#24471;&#21040;&#39564;&#35777;&#65292;&#38899;&#39057;-&#35270;&#35273;&#23398;&#20064;&#20173;&#28982;&#24456;&#38590;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#20248;&#21183;&#65292;&#22240;&#20026;&#22686;&#24378;&#21487;&#33021;&#20250;&#36731;&#26131;&#30772;&#22351;&#36755;&#20837;&#23545;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;EquiAV&#65292;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#25193;&#23637;&#31561;&#21464;&#24615;&#24320;&#22987;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23398;&#20064;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#20419;&#36827;&#12290;&#23427;&#20351;&#24471;&#26469;&#33258;&#19981;&#21516;&#22686;&#24378;&#30340;&#29305;&#24449;&#33021;&#22815;&#32858;&#21512;&#21040;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#23884;&#20837;&#20013;&#65292;&#25552;&#20379;&#24378;&#22823;&#30340;&#30417;&#30563;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#26159;&#22312;&#26368;&#23567;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30340;&#12290;&#22823;&#37327;&#28040;&#34701;&#30740;&#31350;&#21644;&#23450;&#24615;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09502v1 Announce Type: cross  Abstract: Recent advancements in self-supervised audio-visual representation learning have demonstrated its potential to capture rich and comprehensive representations. However, despite the advantages of data augmentation verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs. To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning. Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor. It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision. Notably, this is achieved with minimal computational overhead. Extensive ablation studies and qualitative results verify the effectiveness of our method. 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#22902;&#29275;&#20859;&#27542;&#20013;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#20197;&#25913;&#21892;&#30005;&#27744;&#31649;&#29702;&#65292;&#24212;&#23545;&#30005;&#33021;&#28040;&#32791;&#27874;&#21160;&#21644;&#33021;&#28304;&#20215;&#26684;&#27874;&#21160;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09499</link><description>&lt;p&gt;
&#20351;&#29992;Q&#23398;&#20064;&#30340;&#22902;&#29275;&#20859;&#27542;&#22330;&#30005;&#27744;&#31649;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#22902;&#29275;&#20859;&#27542;&#20013;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#20197;&#25913;&#21892;&#30005;&#27744;&#31649;&#29702;&#65292;&#24212;&#23545;&#30005;&#33021;&#28040;&#32791;&#27874;&#21160;&#21644;&#33021;&#28304;&#20215;&#26684;&#27874;&#21160;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22902;&#29275;&#20859;&#27542;&#28040;&#32791;&#22823;&#37327;&#33021;&#28304;&#65292;&#26159;&#20892;&#19994;&#20013;&#19968;&#20010;&#33021;&#28304;&#23494;&#38598;&#22411;&#30340;&#37096;&#38376;&#12290;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#38598;&#25104;&#21040;&#22902;&#29275;&#20859;&#27542;&#20013;&#21487;&#20197;&#24110;&#21161;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#26377;&#25928;&#30340;&#30005;&#27744;&#31649;&#29702;&#23545;&#20110;&#25972;&#21512;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#33267;&#20851;&#37325;&#35201;&#12290;&#31649;&#29702;&#30005;&#27744;&#30340;&#20805;&#30005;&#21644;&#25918;&#30005;&#30001;&#20110;&#30005;&#33021;&#28040;&#32791;&#30340;&#27874;&#21160;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#30340;&#38388;&#27463;&#24615;&#20197;&#21450;&#33021;&#28304;&#20215;&#26684;&#30340;&#27874;&#21160;&#32780;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26377;&#28508;&#21147;&#26174;&#33879;&#25913;&#21892;&#22902;&#29275;&#20859;&#27542;&#20013;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#21033;&#29992;&#65292;&#28982;&#32780;&#22312;&#36825;&#19968;&#29305;&#23450;&#39046;&#22495;&#20013;&#36827;&#34892;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#20197;&#29233;&#23572;&#20848;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#20854;&#20197;&#21487;&#20877;&#29983;&#33021;&#28304;&#21033;&#29992;&#20026;&#26680;&#24515;&#30340;2030&#24180;&#33021;&#28304;&#25112;&#30053;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23433;&#25490;&#30005;&#27744;&#30340;&#20805;&#30005;&#21644;&#25918;&#30005;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09499v1 Announce Type: cross  Abstract: Dairy farming consumes a significant amount of energy, making it an energy-intensive sector within agriculture. Integrating renewable energy generation into dairy farming could help address this challenge. Effective battery management is important for integrating renewable energy generation. Managing battery charging and discharging poses significant challenges because of fluctuations in electrical consumption, the intermittent nature of renewable energy generation, and fluctuations in energy prices. Artificial Intelligence (AI) has the potential to significantly improve the use of renewable energy in dairy farming, however, there is limited research conducted in this particular domain. This research considers Ireland as a case study as it works towards attaining its 2030 energy strategy centered on the utilization of renewable sources. This study proposes a Q-learning-based algorithm for scheduling battery charging and discharging in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#20223;&#30495;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#30340;&#36235;&#21183;&#21644;&#25511;&#21046;&#65292;&#27599;&#20010;&#20195;&#29702;&#20154;&#22312;&#20223;&#30495;&#20013;&#20195;&#34920;&#20855;&#26377;&#29420;&#29305;&#20010;&#24615;&#30340;&#20010;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.09498</link><description>&lt;p&gt;
&#20174;&#24576;&#30097;&#21040;&#25509;&#21463;&#65306;&#27169;&#25311;&#23545;&#34394;&#20551;&#26032;&#38395;&#24577;&#24230;&#21160;&#24577;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#20223;&#30495;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#30340;&#36235;&#21183;&#21644;&#25511;&#21046;&#65292;&#27599;&#20010;&#20195;&#29702;&#20154;&#22312;&#20223;&#30495;&#20013;&#20195;&#34920;&#20855;&#26377;&#29420;&#29305;&#20010;&#24615;&#30340;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#34394;&#20551;&#26032;&#38395;&#21644;&#35875;&#35328;&#36890;&#36807;&#31038;&#20132;&#32593;&#32476;&#36805;&#36895;&#20256;&#25773;&#65292;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#31038;&#20250;&#25361;&#25112;&#65292;&#24433;&#21709;&#30528;&#20844;&#20247;&#33286;&#35770;&#12290;&#20256;&#32479;&#30340;&#34394;&#20551;&#26032;&#38395;&#24314;&#27169;&#36890;&#24120;&#39044;&#27979;&#19981;&#21516;&#32676;&#20307;&#30340;&#26222;&#36941;&#27969;&#34892;&#36235;&#21183;&#25110;&#25968;&#23383;&#21270;&#20195;&#34920;&#24847;&#35265;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#36807;&#20110;&#31616;&#21270;&#29616;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#24615;&#65292;&#24573;&#35270;&#20102;&#26032;&#38395;&#25991;&#26412;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#27169;&#25311;&#24494;&#22937;&#24847;&#35265;&#21160;&#24577;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;LLM&#30340;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#20223;&#30495;&#26694;&#26550;&#65288;FPS&#65289;&#65292;&#35814;&#32454;&#30740;&#31350;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#30340;&#36235;&#21183;&#21644;&#25511;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#20223;&#30495;&#20013;&#30340;&#27599;&#20010;&#20195;&#29702;&#20154;&#20195;&#34920;&#20855;&#26377;&#29420;&#29305;&#20010;&#24615;&#30340;&#20010;&#20154;&#12290;&#20182;&#20204;&#37197;&#22791;&#20102;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#21450;&#21453;&#24605;&#26426;&#21046;&#26469;&#27169;&#20223;&#31867;&#20154;&#24605;&#32500;&#12290;&#27599;&#22825;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09498v1 Announce Type: cross  Abstract: In the digital era, the rapid propagation of fake news and rumors via social networks brings notable societal challenges and impacts public opinion regulation. Traditional fake news modeling typically forecasts the general popularity trends of different groups or numerically represents opinions shift. However, these methods often oversimplify real-world complexities and overlook the rich semantic information of news text. The advent of large language models (LLMs) provides the possibility of modeling subtle dynamics of opinion. Consequently, in this work, we introduce a Fake news Propagation Simulation framework (FPS) based on LLM, which studies the trends and control of fake news propagation in detail. Specifically, each agent in the simulation represents an individual with a distinct personality. They are equipped with both short-term and long-term memory, as well as a reflective mechanism to mimic human-like thinking. Every day, the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26126;&#31034;&#24847;&#35782;&#26657;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09488</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#32416;&#27491;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Rectifying Demonstration Shortcut in In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26126;&#31034;&#24847;&#35782;&#26657;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#20165;&#20973;&#23569;&#37327;&#28436;&#31034;&#20415;&#33021;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;LLMs&#24120;&#24120;&#20381;&#36182;&#20110;&#23427;&#20204;&#23545;&#28436;&#31034;&#30340;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#20041;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#26681;&#25454;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#32487;&#32493;&#36827;&#34892;ICL&#39044;&#27979;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#29616;&#35937;&#31216;&#20026;&#8220;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#8221;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#25913;&#36827;&#39044;&#23450;&#20041;&#20219;&#21153;&#30340;ICL&#39044;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#32416;&#27491;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#20174;&#32780;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#30340;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26126;&#31034;&#24847;&#35782;&#30340;&#26657;&#20934;&#26041;&#27861;&#65306;In-Context Calibration&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#65288;1&#65289;&#20351;&#29992;&#26631;&#20934;&#26631;&#31614;&#31354;&#38388;&#30340;&#21407;&#22987;ICL&#20219;&#21153;&#20197;&#21450;&#65288;2&#65289;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#26631;&#31614;&#31354;&#38388;&#34987;&#35821;&#20041;&#26080;&#20851;&#30340;&#26631;&#35760;&#26367;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09488v1 Announce Type: cross  Abstract: Large language models (LLMs) are able to solve various tasks with only a few demonstrations utilizing their in-context learning (ICL) abilities. However, LLMs often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with ICL prediction. In this work, we term this phenomenon as the `Demonstration Shortcut'. While previous works have primarily focused on improving ICL prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM to effectively learn new input-label relationships from demonstrations. To achieve this, we introduce In-Context Calibration, a demonstration-aware calibration method. We evaluate the effectiveness of the proposed method in two settings: (1) the Original ICL Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#21644;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#36125;&#21494;&#26031;&#32593;&#32476;&#19982;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#25913;&#36827;&#20020;&#24202;&#25512;&#29702;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2403.09481</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#34920;&#26684;&#25968;&#25454;&#21644;&#25991;&#26412;&#30340;&#20020;&#24202;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Clinical Reasoning over Tabular Data and Text with Bayesian Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#21644;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#36125;&#21494;&#26031;&#32593;&#32476;&#19982;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#25913;&#36827;&#20020;&#24202;&#25512;&#29702;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#38750;&#24120;&#36866;&#21512;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20020;&#24202;&#25512;&#29702;&#65292;&#20294;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#26469;&#35828;&#21364;&#19981;&#22815;&#20860;&#23481;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#21017;&#20026;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#25552;&#20379;&#20102;&#25104;&#21151;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#27604;&#36739;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#20197;&#29983;&#25104;&#24615;&#21644;&#21028;&#21035;&#24615;&#26041;&#24335;&#22686;&#24378;&#36125;&#21494;&#26031;&#32593;&#32476;&#19982;&#31070;&#32463;&#25991;&#26412;&#34920;&#31034;&#65292;&#32467;&#21512;&#27169;&#25311;&#32467;&#26524;&#20197;&#19968;&#20010;&#22522;&#30784;&#21307;&#30103;&#26696;&#20363;&#65288;&#32954;&#28814;&#35786;&#26029;&#65289;&#36827;&#34892;&#35828;&#26126;&#65292;&#24182;&#22312;&#26356;&#24191;&#27867;&#30340;&#20020;&#24202;&#32972;&#26223;&#19979;&#36827;&#34892;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09481v1 Announce Type: new  Abstract: Bayesian networks are well-suited for clinical reasoning on tabular data, but are less compatible with natural language data, for which neural networks provide a successful framework. This paper compares and discusses strategies to augment Bayesian networks with neural text representations, both in a generative and discriminative manner. This is illustrated with simulation results for a primary care use case (diagnosis of pneumonia) and discussed in a broader clinical context.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#33609;&#22270;&#35299;&#37322;&#30340;&#29420;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#35299;&#37322;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#22235;&#20010;&#24212;&#29992;&#39046;&#22495;&#65306;&#26816;&#32034;&#12289;&#29983;&#25104;&#12289;&#36741;&#21161;&#32472;&#22270;&#20197;&#21450;&#33609;&#22270;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.09480</link><description>&lt;p&gt;
Sketch&#35299;&#37322;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#21547;&#20041;
&lt;/p&gt;
&lt;p&gt;
What Sketch Explainability Really Means for Downstream Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#33609;&#22270;&#35299;&#37322;&#30340;&#29420;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#35299;&#37322;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#22235;&#20010;&#24212;&#29992;&#39046;&#22495;&#65306;&#26816;&#32034;&#12289;&#29983;&#25104;&#12289;&#36741;&#21161;&#32472;&#22270;&#20197;&#21450;&#33609;&#22270;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#28085;&#30422;&#33609;&#22270;&#35299;&#37322;&#24615;&#30340;&#29420;&#29305;&#27169;&#24577;&#65292;&#24378;&#35843;&#20154;&#31867;&#31508;&#35302;&#30456;&#23545;&#20110;&#20256;&#32479;&#22522;&#20110;&#20687;&#32032;&#30340;&#30740;&#31350;&#25152;&#20135;&#29983;&#30340;&#28145;&#36828;&#24433;&#21709;&#12290;&#38500;&#20102;&#35299;&#37322;&#32593;&#32476;&#34892;&#20026;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#36776;&#21035;&#20102;&#35299;&#37322;&#24615;&#22312;&#19981;&#21516;&#19979;&#28216;&#19982;&#33609;&#22270;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#30495;&#27491;&#21547;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#20415;&#25658;&#30340;&#35299;&#37322;&#24615;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#19968;&#20010;&#26080;&#32541;&#25554;&#20214;&#65292;&#33021;&#22815;&#36731;&#26494;&#19982;&#20219;&#20309;&#39044;&#35757;&#32451;&#27169;&#22411;&#38598;&#25104;&#65292;&#28040;&#38500;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#36866;&#24212;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#24212;&#29992;&#65306;&#24191;&#21463;&#20851;&#27880;&#30340;&#26816;&#32034;&#21644;&#29983;&#25104;&#65292;&#20197;&#21450;&#20840;&#26032;&#30340;&#36741;&#21161;&#32472;&#22270;&#21644;&#33609;&#22270;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#31508;&#35302;&#32423;&#21035;&#30340;&#24402;&#22240;&#22320;&#22270;&#65292;&#22312;&#19982;&#19979;&#28216;&#20219;&#21153;&#38142;&#25509;&#26102;&#21576;&#29616;&#19981;&#21516;&#24418;&#24335;&#12290;&#36890;&#36807;&#35299;&#20915;&#20809;&#26629;&#21270;&#30340;&#22266;&#26377;&#19981;&#21487;&#24494;&#24615;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#31895;&#30053;&#31508;&#35302;&#32423;&#21035;&#65288;SLA&#65289;&#21644;&#37096;&#20998;&#31508;&#35302;&#32423;&#21035;&#65288;P-S&#65289;&#36827;&#34892;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09480v1 Announce Type: cross  Abstract: In this paper, we explore the unique modality of sketch for explainability, emphasising the profound impact of human strokes compared to conventional pixel-oriented studies. Beyond explanations of network behavior, we discern the genuine implications of explainability across diverse downstream sketch-related tasks. We propose a lightweight and portable explainability solution -- a seamless plugin that integrates effortlessly with any pre-trained model, eliminating the need for re-training. Demonstrating its adaptability, we present four applications: highly studied retrieval and generation, and completely novel assisted drawing and sketch adversarial attacks. The centrepiece to our solution is a stroke-level attribution map that takes different forms when linked with downstream tasks. By addressing the inherent non-differentiability of rasterisation, we enable explanations at both coarse stroke level (SLA) and partial stroke level (P-S
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25913;&#21892;&#29992;&#25143;&#25925;&#20107;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;LLMs&#23545;&#25552;&#39640;&#29992;&#25143;&#25925;&#20107;&#36136;&#37327;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#25935;&#25463;&#24320;&#21457;&#20013;&#23637;&#31034;&#20102;AI&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.09442</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20307;&#29992;&#20110;&#33258;&#21160;&#25552;&#21319;&#29992;&#25143;&#25925;&#20107;&#36136;&#37327;&#65306;&#19968;&#20010;&#26089;&#26399;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
LLM-based agents for automating the enhancement of user story quality: An early report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25913;&#21892;&#29992;&#25143;&#25925;&#20107;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;LLMs&#23545;&#25552;&#39640;&#29992;&#25143;&#25925;&#20107;&#36136;&#37327;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#25935;&#25463;&#24320;&#21457;&#20013;&#23637;&#31034;&#20102;AI&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25935;&#25463;&#36719;&#20214;&#24320;&#21457;&#20013;&#65292;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#29992;&#25143;&#25925;&#20107;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25552;&#21319;&#22885;&#22320;&#21033;&#37038;&#25919;&#38598;&#22242;IT&#25935;&#25463;&#22242;&#38431;&#20013;&#29992;&#25143;&#25925;&#20107;&#36136;&#37327;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#20027;LLM&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#21442;&#32771;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#23454;&#26045;&#22312;&#20844;&#21496;&#20013;&#12290;&#36890;&#36807;11&#20301;&#21442;&#19982;&#32773;&#36328;&#36234;&#20845;&#20010;&#25935;&#25463;&#22242;&#38431;&#23545;&#30740;&#31350;&#20013;&#30340;&#29992;&#25143;&#25925;&#20107;&#36136;&#37327;&#20197;&#21450;&#36825;&#20123;&#26234;&#33021;&#20307;&#22312;&#25552;&#39640;&#29992;&#25143;&#25925;&#20107;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;LLMs&#22312;&#25552;&#39640;&#29992;&#25143;&#25925;&#20107;&#36136;&#37327;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#23545;AI&#22312;&#25935;&#25463;&#24320;&#21457;&#20013;&#30340;&#35282;&#33394;&#36827;&#34892;&#20102;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;AI&#22312;&#34892;&#19994;&#29615;&#22659;&#20013;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#30340;&#23454;&#38469;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09442v1 Announce Type: cross  Abstract: In agile software development, maintaining high-quality user stories is crucial, but also challenging. This study explores the use of large language models to automatically improve the user story quality in Austrian Post Group IT agile teams. We developed a reference model for an Autonomous LLM-based Agent System and implemented it at the company. The quality of user stories in the study and the effectiveness of these agents for user story quality improvement was assessed by 11 participants across six agile teams. Our findings demonstrate the potential of LLMs in improving user story quality, contributing to the research on AI role in agile development, and providing a practical example of the transformative impact of AI in an industry setting.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20840;&#23616;3D&#20449;&#24687;&#21644;&#29983;&#25104;&#24615;&#32454;&#21270;&#32593;&#32476;&#65292;&#32467;&#21512;NeRF&#27169;&#22411;&#21644;2D&#25193;&#25955;&#27169;&#22411;&#20808;&#39564;&#65292;&#25552;&#20986;&#20102;3D-SceneDreamer&#36825;&#19968;&#25991;&#26412;&#39537;&#21160;&#30340;&#19968;&#33268;3D&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09439</link><description>&lt;p&gt;
3D-SceneDreamer: &#25991;&#26412;&#39537;&#21160;&#30340;3D&#19968;&#33268;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09439
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20840;&#23616;3D&#20449;&#24687;&#21644;&#29983;&#25104;&#24615;&#32454;&#21270;&#32593;&#32476;&#65292;&#32467;&#21512;NeRF&#27169;&#22411;&#21644;2D&#25193;&#25955;&#27169;&#22411;&#20808;&#39564;&#65292;&#25552;&#20986;&#20102;3D-SceneDreamer&#36825;&#19968;&#25991;&#26412;&#39537;&#21160;&#30340;&#19968;&#33268;3D&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#39537;&#21160;&#30340;3D&#22330;&#26223;&#29983;&#25104;&#25216;&#26415;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#22240;&#20110;&#21033;&#29992;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#25197;&#26354;&#21644;&#20462;&#34917;&#65292;&#29983;&#25104;3D&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#22312;&#20960;&#20309;&#21644;&#22806;&#35266;&#19978;&#20250;&#23548;&#33268;&#38169;&#35823;&#32047;&#31215;&#65292;&#38459;&#30861;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;&#22330;&#26223;&#65288;&#20363;&#22914;&#25143;&#22806;&#21644;&#34394;&#25311;&#22330;&#26223;&#65289;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#26597;&#35810;&#21644;&#32858;&#21512;&#20840;&#23616;3D&#20449;&#24687;&#65292;&#23545;&#26032;&#29983;&#25104;&#30340;&#23616;&#37096;&#35270;&#22270;&#36827;&#34892;&#29983;&#25104;&#24615;&#32454;&#21270;&#65292;&#28982;&#21518;&#36880;&#27493;&#29983;&#25104;3D&#22330;&#26223;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#19977;&#24179;&#38754;&#29305;&#24449;&#30340;NeRF&#20316;&#20026;3D&#22330;&#26223;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#20197;&#32422;&#26463;&#20840;&#23616;3D&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#29983;&#25104;&#24615;&#32454;&#21270;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;2D&#25193;&#25955;&#27169;&#22411;&#30340;&#33258;&#28982;&#22270;&#20687;&#20808;&#39564;&#20197;&#21450;&#20840;&#23616;&#20449;&#24687;&#26469;&#21512;&#25104;&#36136;&#37327;&#26356;&#39640;&#30340;&#26032;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09439v1 Announce Type: cross  Abstract: Text-driven 3D scene generation techniques have made rapid progress in recent years. Their success is mainly attributed to using existing generative models to iteratively perform image warping and inpainting to generate 3D scenes. However, these methods heavily rely on the outputs of existing models, leading to error accumulation in geometry and appearance that prevent the models from being used in various scenarios (e.g., outdoor and unreal scenarios). To address this limitation, we generatively refine the newly generated local views by querying and aggregating global 3D information, and then progressively generate the 3D scene. Specifically, we employ a tri-plane features-based NeRF as a unified representation of the 3D scene to constrain global 3D consistency, and propose a generative refinement network to synthesize new contents with higher quality by exploiting the natural image prior from 2D diffusion model as well as the global 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21453;&#20107;&#23454;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#29983;&#25104;&#22270;&#20687;&#26377;&#25928;&#24615;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#20943;&#23569;&#23646;&#24615;&#25918;&#22823;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.09422</link><description>&lt;p&gt;
&#22312;&#21453;&#20107;&#23454;&#22270;&#20687;&#29983;&#25104;&#20013;&#32531;&#35299;&#23646;&#24615;&#25918;&#22823;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating attribute amplification in counterfactual image generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09422
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21453;&#20107;&#23454;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#29983;&#25104;&#22270;&#20687;&#26377;&#25928;&#24615;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#20943;&#23569;&#23646;&#24615;&#25918;&#22823;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#33021;&#22815;&#22238;&#31572;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#26597;&#35810;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#20391;&#37325;&#20110;&#29983;&#25104;&#30475;&#36215;&#26469;&#21512;&#29702;&#30340;&#21453;&#20107;&#23454;&#22270;&#20687;&#65292;&#20351;&#29992;&#36741;&#21161;&#20998;&#31867;&#22120;&#26469;&#24378;&#21270;&#27169;&#25311;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#31181;&#26041;&#27861;&#20013;&#30340;&#32570;&#38519;&#65292;&#21457;&#29616;&#20102;&#23646;&#24615;&#25918;&#22823;&#38382;&#39064;&#65292;&#21363;&#22312;&#24178;&#39044;&#36807;&#31243;&#20013;&#20250;&#35823;&#20260;&#26080;&#20851;&#23646;&#24615;&#65292;&#23548;&#33268;&#21463;&#20445;&#25252;&#29305;&#24449;&#21644;&#30142;&#30149;&#29366;&#24577;&#20043;&#38388;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23646;&#24615;&#25918;&#22823;&#26159;&#30001;&#21453;&#20107;&#23454;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30828;&#26631;&#31614;&#24341;&#36215;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#36719;&#21453;&#20107;&#23454;&#24494;&#35843;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#25918;&#22823;&#25928;&#24212;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#22823;&#22411;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#23454;&#29616;&#26356;&#21152;&#24544;&#23454;&#21644;&#26080;&#20559;&#21521;&#30340;&#22240;&#26524;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09422v1 Announce Type: cross  Abstract: Causal generative modelling is gaining interest in medical imaging due to its ability to answer interventional and counterfactual queries. Most work focuses on generating counterfactual images that look plausible, using auxiliary classifiers to enforce effectiveness of simulated interventions. We investigate pitfalls in this approach, discovering the issue of attribute amplification, where unrelated attributes are spuriously affected during interventions, leading to biases across protected characteristics and disease status. We show that attribute amplification is caused by the use of hard labels in the counterfactual training process and propose soft counterfactual fine-tuning to mitigate this issue. Our method substantially reduces the amplification effect while maintaining effectiveness of generated images, demonstrated on a large chest X-ray dataset. Our work makes an important advancement towards more faithful and unbiased causal 
&lt;/p&gt;</description></item><item><title>&#24320;&#25918;&#22270;&#35889;&#26159;&#38024;&#23545;&#22823;&#35268;&#27169;&#23460;&#22806;&#29615;&#22659;&#35774;&#35745;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#23618;&#22270;&#32467;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#22320;&#22270;&#21463;&#38480;&#20110;&#23460;&#20869;&#22330;&#26223;&#21644;VLM&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35270;&#35273;&#22270;&#20687;&#25552;&#21462;&#23454;&#20363;&#21644;&#26631;&#39064;&#65292;&#24182;&#21152;&#24378;&#25991;&#23383;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.09412</link><description>&lt;p&gt;
&#24320;&#25918;&#22270;&#35889;&#65306;&#22823;&#35268;&#27169;&#23460;&#22806;&#29615;&#22659;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#23618;3D&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09412
&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22270;&#35889;&#26159;&#38024;&#23545;&#22823;&#35268;&#27169;&#23460;&#22806;&#29615;&#22659;&#35774;&#35745;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#23618;&#22270;&#32467;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#22320;&#22270;&#21463;&#38480;&#20110;&#23460;&#20869;&#22330;&#26223;&#21644;VLM&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35270;&#35273;&#22270;&#20687;&#25552;&#21462;&#23454;&#20363;&#21644;&#26631;&#39064;&#65292;&#24182;&#21152;&#24378;&#25991;&#23383;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#22797;&#26434;&#35821;&#20041;&#30340;&#29615;&#22659;&#22320;&#22270;&#23545;&#20110;&#20419;&#36827;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#26080;&#32541;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#24320;&#25918;&#35789;&#27719;&#22320;&#22270;&#65292;&#30001;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#39537;&#21160;&#65292;&#20855;&#26377;&#22266;&#26377;&#20248;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#26816;&#32034;&#21644;&#24320;&#25918;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24320;&#25918;&#35789;&#27719;&#22320;&#22270;&#21463;&#38480;&#20110;&#23553;&#38381;&#30340;&#23460;&#20869;&#22330;&#26223;&#21644;VLM&#29305;&#24449;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#21487;&#29992;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#25299;&#25169;&#20851;&#31995;&#36827;&#19968;&#27493;&#20351;&#24471;&#23545;&#29305;&#23450;&#23454;&#20363;&#30340;&#20934;&#30830;&#26597;&#35810;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenGraph&#65292;&#36825;&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#23460;&#22806;&#29615;&#22659;&#35774;&#35745;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#23618;&#22270;&#32467;&#26500;&#34920;&#31034;&#12290;OpenGraph&#39318;&#20808;&#21033;&#29992;2D&#22522;&#30784;&#27169;&#22411;&#20174;&#35270;&#35273;&#22270;&#20687;&#20013;&#25552;&#21462;&#23454;&#20363;&#21450;&#20854;&#26631;&#39064;&#65292;&#24182;&#23545;&#26631;&#39064;&#36827;&#34892;&#29305;&#24449;&#32534;&#30721;&#20197;&#22686;&#24378;&#25991;&#23383;&#25512;&#29702;&#12290;&#38543;&#21518;&#65292;3D&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09412v1 Announce Type: cross  Abstract: Environment maps endowed with sophisticated semantics are pivotal for facilitating seamless interaction between robots and humans, enabling them to effectively carry out various tasks. Open-vocabulary maps, powered by Visual-Language models (VLMs), possess inherent advantages, including multimodal retrieval and open-set classes. However, existing open-vocabulary maps are constrained to closed indoor scenarios and VLM features, thereby diminishing their usability and inference capabilities. Moreover, the absence of topological relationships further complicates the accurate querying of specific instances. In this work, we propose OpenGraph, a representation of open-vocabulary hierarchical graph structure designed for large-scale outdoor environments. OpenGraph initially extracts instances and their captions from visual images using 2D foundation models, encoding the captions with features to enhance textual reasoning. Subsequently, 3D in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#40784;&#22270;&#20687;&#12289;&#21487;&#23398;&#20064;&#25552;&#31034;&#21644;&#20020;&#24202;&#27010;&#24565;&#39537;&#21160;&#25552;&#31034;&#30340;&#35821;&#20041;&#65292;&#21033;&#29992;&#21307;&#23398;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.09410</link><description>&lt;p&gt;
XCoOp&#65306;&#36890;&#36807;&#27010;&#24565;&#24341;&#23548;&#30340;&#19978;&#19979;&#25991;&#20248;&#21270;&#20026;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09410
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#40784;&#22270;&#20687;&#12289;&#21487;&#23398;&#20064;&#25552;&#31034;&#21644;&#20020;&#24202;&#27010;&#24565;&#39537;&#21160;&#25552;&#31034;&#30340;&#35821;&#20041;&#65292;&#21033;&#29992;&#21307;&#23398;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24378;&#22823;&#30340;&#34920;&#31034;&#26469;&#23436;&#25104;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#36719;&#25552;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20195;&#34920;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#36866;&#24212;VLMs&#65292;&#20363;&#22914;CLIP&#65292;&#20197;&#25191;&#34892;&#35832;&#22914;&#22270;&#20687;&#20998;&#31867;&#20043;&#31867;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#30340;&#25991;&#26412;&#26631;&#35760;&#26159;&#26080;&#27861;&#35299;&#37322;&#30340;&#65292;&#36825;&#19981;&#33021;&#28385;&#36275;&#20687;&#21307;&#30103;&#20445;&#20581;&#31561;&#39640;&#39118;&#38505;&#22330;&#26223;&#20013;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#20005;&#26684;&#21487;&#35299;&#37322;&#24615;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#31890;&#24230;&#19978;&#23545;&#40784;&#22270;&#20687;&#12289;&#21487;&#23398;&#20064;&#25552;&#31034;&#21644;&#20020;&#24202;&#27010;&#24565;&#39537;&#21160;&#25552;&#31034;&#30340;&#35821;&#20041;&#65292;&#21033;&#29992;&#21307;&#23398;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#21462;&#30693;&#35782;&#26469;&#35299;&#20915;&#32570;&#20047;&#26377;&#20215;&#20540;&#30340;&#27010;&#24565;&#27880;&#37322;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#19978;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09410v1 Announce Type: cross  Abstract: Utilizing potent representations of the large vision-language models (VLMs) to accomplish various downstream tasks has attracted increasing attention. Within this research field, soft prompt learning has become a representative approach for efficiently adapting VLMs such as CLIP, to tasks like image classification. However, most existing prompt learning methods learn text tokens that are unexplainable, which cannot satisfy the stringent interpretability requirements of Explainable Artificial Intelligence (XAI) in high-stakes scenarios like healthcare. To address this issue, we propose a novel explainable prompt learning framework that leverages medical knowledge by aligning the semantics of images, learnable prompts, and clinical concept-driven prompts at multiple granularities. Moreover, our framework addresses the lack of valuable concept annotations by eliciting knowledge from large language models and offers both visual and textual
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;ChatGPT&#65292;&#30740;&#31350;&#20102;&#36229;&#36807;350&#21517;&#19968;&#24180;&#32423;&#35745;&#31639;&#26426;&#23398;&#29983;&#29983;&#25104;&#30340;&#36882;&#24402;&#31867;&#27604;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#31867;&#27604;&#24110;&#21161;&#29702;&#35299;&#22797;&#26434;&#30340;&#35745;&#31639;&#27010;&#24565;</title><link>https://arxiv.org/abs/2403.09409</link><description>&lt;p&gt;
"&#20687;&#22871;&#23043;&#19968;&#26679;"&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#29983;&#25104;&#30340;&#36882;&#24402;&#31867;&#27604;
&lt;/p&gt;
&lt;p&gt;
"Like a Nesting Doll": Analyzing Recursion Analogies Generated by CS Students using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09409
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;ChatGPT&#65292;&#30740;&#31350;&#20102;&#36229;&#36807;350&#21517;&#19968;&#24180;&#32423;&#35745;&#31639;&#26426;&#23398;&#29983;&#29983;&#25104;&#30340;&#36882;&#24402;&#31867;&#27604;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#31867;&#27604;&#24110;&#21161;&#29702;&#35299;&#22797;&#26434;&#30340;&#35745;&#31639;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25226;&#22797;&#26434;&#30340;&#35745;&#31639;&#27010;&#24565;&#34701;&#20250;&#36143;&#36890;&#24120;&#24120;&#26159;&#23398;&#29983;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20182;&#20204;&#24448;&#24448;&#38590;&#20197;&#23558;&#36825;&#20123;&#26032;&#24819;&#27861;&#38170;&#23450;&#22312;&#29087;&#24713;&#30340;&#32463;&#39564;&#21644;&#29702;&#35299;&#20043;&#19978;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#19968;&#20010;&#22909;&#30340;&#31867;&#27604;&#21487;&#20197;&#24357;&#21512;&#38476;&#29983;&#27010;&#24565;&#19982;&#29087;&#24713;&#27010;&#24565;&#20043;&#38388;&#30340;&#40511;&#27807;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#24341;&#20154;&#20837;&#32988;&#30340;&#26041;&#24335;&#26469;&#24110;&#21161;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#32463;&#39564;&#20016;&#23500;&#30340;&#25945;&#24072;&#26469;&#35828;&#65292;&#21019;&#36896;&#26377;&#25928;&#30340;&#25945;&#32946;&#31867;&#27604;&#20063;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;ChatGPT&#65292;&#21040;&#24213;&#33021;&#22815;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#25552;&#20379;&#25353;&#38656;&#35775;&#38382;&#20010;&#20154;&#30456;&#20851;&#31867;&#27604;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36882;&#24402;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38376;&#27099;&#27010;&#24565;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35843;&#26597;&#65292;&#20998;&#26512;&#20102;350&#22810;&#21517;&#19968;&#24180;&#32423;&#35745;&#31639;&#26426;&#23398;&#29983;&#29983;&#25104;&#30340;&#31867;&#27604;&#12290;&#20182;&#20204;&#34987;&#35201;&#27714;&#20351;&#29992;ChatGPT&#29983;&#25104;&#22522;&#20110;&#36882;&#24402;&#30340;&#31867;&#27604;&#65292;&#20854;&#20013;&#21487;&#20197;&#36873;&#25321;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#20010;&#20154;&#30456;&#20851;&#20027;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29983;&#25104;&#30340;&#31867;&#27604;&#21576;&#29616;&#20986;&#26497;&#22823;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09409v1 Announce Type: cross  Abstract: Grasping complex computing concepts often poses a challenge for students who struggle to anchor these new ideas to familiar experiences and understandings. To help with this, a good analogy can bridge the gap between unfamiliar concepts and familiar ones, providing an engaging way to aid understanding. However, creating effective educational analogies is difficult even for experienced instructors. We investigate to what extent large language models (LLMs), specifically ChatGPT, can provide access to personally relevant analogies on demand. Focusing on recursion, a challenging threshold concept, we conducted an investigation analyzing the analogies generated by more than 350 first-year computing students. They were provided with a code snippet and tasked to generate their own recursion-based analogies using ChatGPT, optionally including personally relevant topics in their prompts. We observed a great deal of diversity in the analogies p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LM2D&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#25193;&#25955;&#27169;&#22411;&#21644;&#19968;&#33268;&#24615;&#33976;&#39311;&#30340;&#26032;&#39062;&#27010;&#29575;&#26550;&#26500;&#65292;&#26088;&#22312;&#22312;&#38899;&#20048;&#21644;&#27468;&#35789;&#26465;&#20214;&#19979;&#36827;&#34892;&#33310;&#36424;&#29983;&#25104;&#65307;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#28085;&#30422;&#38899;&#20048;&#21644;&#27468;&#35789;&#30340;3D&#33310;&#36424;&#36816;&#21160;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.09407</link><description>&lt;p&gt;
LM2D&#65306;&#20197;&#27468;&#35789;&#21644;&#38899;&#20048;&#39537;&#21160;&#30340;&#33310;&#36424;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
LM2D: Lyrics- and Music-Driven Dance Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09407
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LM2D&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#25193;&#25955;&#27169;&#22411;&#21644;&#19968;&#33268;&#24615;&#33976;&#39311;&#30340;&#26032;&#39062;&#27010;&#29575;&#26550;&#26500;&#65292;&#26088;&#22312;&#22312;&#38899;&#20048;&#21644;&#27468;&#35789;&#26465;&#20214;&#19979;&#36827;&#34892;&#33310;&#36424;&#29983;&#25104;&#65307;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#28085;&#30422;&#38899;&#20048;&#21644;&#27468;&#35789;&#30340;3D&#33310;&#36424;&#36816;&#21160;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33310;&#36424;&#36890;&#24120;&#28041;&#21450;&#19987;&#19994;&#32534;&#33310;&#65292;&#21253;&#21547;&#25353;&#29031;&#38899;&#20048;&#33410;&#22863;&#36827;&#34892;&#30340;&#22797;&#26434;&#21160;&#20316;&#65292;&#36824;&#21487;&#33021;&#21463;&#21040;&#27468;&#35789;&#20869;&#23481;&#30340;&#24433;&#21709;&#12290;&#23558;&#27468;&#35789;&#25972;&#21512;&#21040;&#21548;&#35273;&#32500;&#24230;&#20043;&#22806;&#65292;&#20016;&#23500;&#20102;&#22522;&#30784;&#38899;&#33394;&#65292;&#24182;&#20351;&#36816;&#21160;&#29983;&#25104;&#26356;&#26131;&#20110;&#35821;&#20041;&#21547;&#20041;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33310;&#36424;&#21512;&#25104;&#26041;&#27861;&#24448;&#24448;&#21482;&#24314;&#27169;&#20110;&#38899;&#39057;&#20449;&#21495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#28857;&#36129;&#29486;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LM2D&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#25193;&#25955;&#27169;&#22411;&#21644;&#19968;&#33268;&#24615;&#33976;&#39311;&#30340;&#26032;&#39062;&#27010;&#29575;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#25193;&#25955;&#29983;&#25104;&#27493;&#39588;&#22312;&#38899;&#20048;&#21644;&#27468;&#35789;&#30340;&#26465;&#20214;&#19979;&#21019;&#24314;&#33310;&#36424;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#28085;&#30422;&#38899;&#20048;&#21644;&#27468;&#35789;&#30340;3D&#33310;&#36424;&#36816;&#21160;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23039;&#21183;&#20272;&#35745;&#25216;&#26415;&#33719;&#24471;&#12290;&#25105;&#20204;&#36890;&#36807;&#23458;&#35266;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20215;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20165;&#26377;&#38899;&#20048;&#30340;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09407v1 Announce Type: cross  Abstract: Dance typically involves professional choreography with complex movements that follow a musical rhythm and can also be influenced by lyrical content. The integration of lyrics in addition to the auditory dimension, enriches the foundational tone and makes motion generation more amenable to its semantic meanings. However, existing dance synthesis methods tend to model motions only conditioned on audio signals. In this work, we make two contributions to bridge this gap. First, we propose LM2D, a novel probabilistic architecture that incorporates a multimodal diffusion model with consistency distillation, designed to create dance conditioned on both music and lyrics in one diffusion generation step. Second, we introduce the first 3D dance-motion dataset that encompasses both music and lyrics, obtained with pose estimation technologies. We evaluate our model against music-only baseline models with objective metrics and human evaluations, i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#26032;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#31934;&#24230;&#26368;&#22823;&#21270;&#21644;&#21162;&#21147;&#20943;&#23569;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#22312;&#35814;&#23613;&#30340;&#36923;&#36753;&#22788;&#29702;&#21644;&#20351;&#29992;&#35748;&#30693;&#24555;&#25463;&#26041;&#24335;&#65288;&#21551;&#21457;&#24335;&#65289;&#20043;&#38388;&#36716;&#25442;&#30340;&#26465;&#20214;&#65292;&#24182;&#21306;&#20998;&#20102;&#21551;&#21457;&#24335;&#30340;&#24037;&#20855;&#24615;&#20351;&#29992;&#21644;&#27169;&#20223;&#21560;&#25910;&#20004;&#31181;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.09404</link><description>&lt;p&gt;
AI&#20013;&#30340;&#21551;&#21457;&#24335;&#25512;&#29702;: &#24037;&#20855;&#24615;&#20351;&#29992;&#19982;&#27169;&#20223;&#21560;&#25910;
&lt;/p&gt;
&lt;p&gt;
Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09404
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#26032;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#31934;&#24230;&#26368;&#22823;&#21270;&#21644;&#21162;&#21147;&#20943;&#23569;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#22312;&#35814;&#23613;&#30340;&#36923;&#36753;&#22788;&#29702;&#21644;&#20351;&#29992;&#35748;&#30693;&#24555;&#25463;&#26041;&#24335;&#65288;&#21551;&#21457;&#24335;&#65289;&#20043;&#38388;&#36716;&#25442;&#30340;&#26465;&#20214;&#65292;&#24182;&#21306;&#20998;&#20102;&#21551;&#21457;&#24335;&#30340;&#24037;&#20855;&#24615;&#20351;&#29992;&#21644;&#27169;&#20223;&#21560;&#25910;&#20004;&#31181;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21551;&#21457;&#24335;&#25512;&#29702;&#26041;&#26696;&#65292;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#21019;&#26032;&#24615;&#23454;&#39564;&#65292;&#21253;&#25324;&#23545;&#32463;&#20856;&#29747;&#36798;&#38382;&#39064;&#30340;&#21464;&#20307;&#21644;&#23545;&#32654;&#20029;&#27604;&#36187;&#28216;&#25103;&#30340;&#26032;&#24212;&#29992;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#31934;&#24230;&#26368;&#22823;&#21270;&#21644;&#21162;&#21147;&#20943;&#23569;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#22609;&#36896;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#20309;&#31181;&#26465;&#20214;&#19979;&#22312;&#35814;&#23613;&#30340;&#36923;&#36753;&#22788;&#29702;&#21644;&#20351;&#29992;&#35748;&#30693;&#24555;&#25463;&#26041;&#24335;&#65288;&#21551;&#21457;&#24335;&#65289;&#20043;&#38388;&#36716;&#25442;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#21551;&#21457;&#24335;&#30340;'&#24037;&#20855;&#24615;'&#20351;&#29992;&#65292;&#20197;&#21305;&#37197;&#36164;&#28304;&#19982;&#30446;&#26631;&#65292;&#20197;&#21450;'&#27169;&#20223;&#21560;&#25910;'&#65292;&#21363;&#20174;&#20154;&#31867;&#37027;&#37324;&#23398;&#21040;&#30340;&#21551;&#21457;&#24335;&#65292;&#24182;&#34920;&#29616;&#20026;&#38543;&#26426;&#19988;&#26222;&#36941;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23613;&#31649;&#32570;&#20047;&#20869;&#22312;&#30446;&#26631;&#25110;&#33258;&#25105;&#24847;&#35782;&#65292;&#20154;&#24037;&#26234;&#33021;&#34920;&#29616;&#20986;&#31934;&#24230;&#21644;&#25928;&#29575;&#30340;&#33258;&#36866;&#24212;&#24179;&#34913;&#65292;&#31526;&#21512;&#36164;&#28304;&#29702;&#24615;&#20154;&#31867;&#35748;&#30693;&#30340;&#21407;&#21017;&#65292;&#36825;&#26159;&#21463;&#38480;&#29702;&#24615;&#21644;&#21452;&#31995;&#32479;&#29702;&#35770;&#32463;&#20856;&#29702;&#35770;&#30340;&#26126;&#25991;&#38416;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09404v1 Announce Type: new  Abstract: We propose a novel program of heuristic reasoning within artificial intelligence (AI) systems. Through a series of innovative experiments, including variations of the classic Linda problem and a novel application of the Beauty Contest game, we uncover trade-offs between accuracy maximization and effort reduction that shape the conditions under which AIs transition between exhaustive logical processing and the use of cognitive shortcuts (heuristics). We distinguish between the 'instrumental' use of heuristics to match resources with objectives, and 'mimetic absorption,' whereby heuristics are learned from humans, and manifest randomly and universally. We provide evidence that AI, despite lacking intrinsic goals or self-awareness, manifests an adaptive balancing of precision and efficiency, consistent with principles of resource-rational human cognition as explicated in classical theories of bounded rationality and dual-process theory.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#31181;&#32676;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20179;&#24211;&#36793;&#35013;&#37197;&#20132;&#21449;&#21644;&#26377;&#25928;&#23616;&#37096;&#25628;&#32034;&#31561;&#25163;&#27573;&#65292;&#23454;&#29616;&#20102;&#23545;&#23481;&#37327;&#20301;&#32622;&#36335;&#24452;&#38382;&#39064;&#30340;&#26377;&#25928;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.09361</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#23481;&#37327;&#20301;&#32622;&#36335;&#24452;&#38382;&#39064;&#30340;&#22810;&#31181;&#32676;&#32508;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Multi-population Integrated Approach for Capacitated Location Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09361
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#31181;&#32676;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20179;&#24211;&#36793;&#35013;&#37197;&#20132;&#21449;&#21644;&#26377;&#25928;&#23616;&#37096;&#25628;&#32034;&#31561;&#25163;&#27573;&#65292;&#23454;&#29616;&#20102;&#23545;&#23481;&#37327;&#20301;&#32622;&#36335;&#24452;&#38382;&#39064;&#30340;&#26377;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23481;&#37327;&#20301;&#32622;&#36335;&#24452;&#38382;&#39064;&#28041;&#21450;&#30830;&#23450;&#20174;&#19968;&#32452;&#20505;&#36873;&#23481;&#37327;&#37197;&#36865;&#20013;&#24515;&#20301;&#32622;&#20013;&#36873;&#21462;&#20179;&#24211;&#65292;&#24182;&#25214;&#21040;&#20174;&#36873;&#23450;&#30340;&#20179;&#24211;&#21040;&#19968;&#32452;&#23458;&#25143;&#30340;&#26381;&#21153;&#25152;&#38656;&#36335;&#24452;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#21253;&#25324;&#24320;&#35774;&#36873;&#23450;&#20179;&#24211;&#30340;&#25104;&#26412;&#12289;&#27599;&#36742;&#36710;&#22266;&#23450;&#21033;&#29992;&#25104;&#26412;&#21644;&#36335;&#24452;&#24635;&#25104;&#26412;&#65288;&#36317;&#31163;&#65289;&#22312;&#20869;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#31181;&#32676;&#32508;&#21512;&#26694;&#26550;&#65292;&#20854;&#20013;&#22810;&#20179;&#24211;&#36793;&#35013;&#37197;&#20132;&#21449;&#29983;&#25104;&#26377;&#21069;&#26223;&#30340;&#21518;&#20195;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#20179;&#24211;&#20301;&#32622;&#21644;&#36335;&#24452;&#36793;&#32536;&#32452;&#35013;&#30340;&#35282;&#24230;&#26469;&#30475;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#37051;&#22495;&#30340;&#26377;&#25928;&#23616;&#37096;&#25628;&#32034;&#12289;&#19968;&#20010;&#24674;&#22797;&#21487;&#34892;&#24615;&#30340;&#31243;&#24207;&#21644;&#19968;&#20010;&#22810;&#26679;&#21270;&#23548;&#21521;&#30340;&#31361;&#21464;&#12290;&#29305;&#21035;&#26377;&#36259;&#30340;&#26159;&#22810;&#31181;&#32676;&#26041;&#26696;&#65292;&#23427;&#26681;&#25454;&#20179;&#24211;&#37197;&#32622;&#23558;&#20154;&#21475;&#32452;&#32455;&#25104;&#22810;&#20010;&#23376;&#20154;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09361v1 Announce Type: new  Abstract: The capacitated location-routing problem involves determining the depots from a set of candidate capacitated depot locations and finding the required routes from the selected depots to serve a set of customers whereas minimizing a cost function that includes the cost of opening the chosen depots, the fixed utilization cost per vehicle used, and the total cost (distance) of the routes. This paper presents a multi-population integrated framework in which a multi-depot edge assembly crossover generates promising offspring solutions from the perspective of both depot location and route edge assembly. The method includes an effective neighborhood-based local search, a feasibility-restoring procedure and a diversification-oriented mutation. Of particular interest is the multi-population scheme which organizes the population into multiple subpopulations based on depot configurations. Extensive experiments on 281 benchmark instances from the lit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Distinctive Dual-Domain Teacher (D3T) &#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#35757;&#32451;&#33539;&#24335;&#20197;&#21450;&#22312;&#21452;&#25945;&#24072;&#20043;&#38388;&#30340;&#26354;&#25240;&#23398;&#20064;&#26041;&#27861;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#21487;&#35265;&#21040;&#28909;&#32418;&#22806;&#39046;&#22495;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.09359</link><description>&lt;p&gt;
D3T: RGB-&#28909;&#32418;&#22806;&#39046;&#22495;&#36328;&#22495;&#30446;&#26631;&#26816;&#27979;&#20013;&#29420;&#29305;&#30340;&#21452;&#22495;&#25945;&#24072;&#20043;&#26053;
&lt;/p&gt;
&lt;p&gt;
D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Distinctive Dual-Domain Teacher (D3T) &#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#35757;&#32451;&#33539;&#24335;&#20197;&#21450;&#22312;&#21452;&#25945;&#24072;&#20043;&#38388;&#30340;&#26354;&#25240;&#23398;&#20064;&#26041;&#27861;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#21487;&#35265;&#21040;&#28909;&#32418;&#22806;&#39046;&#22495;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#36890;&#24120;&#28041;&#21450;&#23558;&#30693;&#35782;&#20174;&#19968;&#20010;&#21487;&#35265;&#39046;&#22495;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#21487;&#35265;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20174;&#21487;&#35265;&#21040;&#28909;&#32418;&#22806;&#39046;&#22495;&#30340;&#36866;&#24212;&#30740;&#31350;&#26377;&#38480;&#65292;&#22240;&#20026;&#21487;&#35265;&#21644;&#28909;&#32418;&#22806;&#39046;&#22495;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#36828;&#36828;&#22823;&#20110;&#39044;&#26399;&#65292;&#20256;&#32479;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26080;&#27861;&#25104;&#21151;&#22320;&#20419;&#36827;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Distinctive Dual-Domain Teacher (D3T) &#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21508;&#33258;&#39046;&#22495;&#30340;&#29420;&#29305;&#35757;&#32451;&#33539;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#28304;&#35757;&#32451;&#38598;&#21644;&#30446;&#26631;&#35757;&#32451;&#38598;&#20998;&#24320;&#26500;&#24314;&#21452;&#25945;&#24072;&#65292;&#24182;&#36880;&#27493;&#23545;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65292;&#20197;&#36866;&#24212;&#21508;&#20010;&#39046;&#22495;&#30340;&#25945;&#24072;&#12290;&#35813;&#26694;&#26550;&#36827;&#19968;&#27493;&#34701;&#20837;&#20102;&#21452;&#25945;&#24072;&#20043;&#38388;&#30340;&#26354;&#25240;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20419;&#36827;&#20102;&#20174;&#21487;&#35265;&#21040;&#28909;&#32418;&#22806;&#39046;&#22495;&#30340;&#28176;&#36827;&#36807;&#28193;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#20854;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09359v1 Announce Type: cross  Abstract: Domain adaptation for object detection typically entails transferring knowledge from one visible domain to another visible domain. However, there are limited studies on adapting from the visible to the thermal domain, because the domain gap between the visible and thermal domains is much larger than expected, and traditional domain adaptation can not successfully facilitate learning in this situation. To overcome this challenge, we propose a Distinctive Dual-Domain Teacher (D3T) framework that employs distinct training paradigms for each domain. Specifically, we segregate the source and target training sets for building dual-teachers and successively deploy exponential moving average to the student model to individual teachers of each domain. The framework further incorporates a zigzag learning method between dual teachers, facilitating a gradual transition from the visible to thermal domains during training. We validate the superiorit
&lt;/p&gt;</description></item><item><title>AVIBench&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#21508;&#31181;&#24418;&#24335;&#30340;&#23545;&#25239;&#24615;&#35270;&#35273;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#25991;&#26412;&#25915;&#20987;&#20197;&#21450;&#20869;&#23481;&#20559;&#35265;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.09346</link><description>&lt;p&gt;
AVIBench&#65306;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#25239;&#24615;&#35270;&#35273;&#25351;&#23548;&#19978;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09346
&lt;/p&gt;
&lt;p&gt;
AVIBench&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#21508;&#31181;&#24418;&#24335;&#30340;&#23545;&#25239;&#24615;&#35270;&#35273;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#25991;&#26412;&#25915;&#20987;&#20197;&#21450;&#20869;&#23481;&#20559;&#35265;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#22312;&#23545;&#29992;&#25143;&#30340;&#35270;&#35273;&#25351;&#20196;&#20316;&#20986;&#33391;&#22909;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25351;&#20196;&#28085;&#30422;&#22270;&#20687;&#21644;&#25991;&#26412;&#65292;&#23481;&#26131;&#21463;&#21040;&#26377;&#24847;&#21644;&#26080;&#24847;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;LVLMs&#23545;&#25239;&#27492;&#31867;&#23041;&#32961;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AVIBench&#65292;&#19968;&#20010;&#26088;&#22312;&#20998;&#26512;LVLMs&#22312;&#38754;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#35270;&#35273;&#25351;&#20196;&#65288;AVIs&#65289;&#26102;&#30340;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#22235;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;AVIs&#12289;&#21313;&#31181;&#22522;&#20110;&#25991;&#26412;&#30340;AVIs&#21644;&#20061;&#31181;&#20869;&#23481;&#20559;&#35265;AVIs&#65288;&#22914;&#24615;&#21035;&#12289;&#26292;&#21147;&#12289;&#25991;&#21270;&#21644;&#31181;&#26063;&#20559;&#35265;&#31561;&#65289;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;26&#19975;&#20010;AVIs&#65292;&#28085;&#30422;&#20116;&#31867;&#22810;&#27169;&#24577;&#33021;&#21147;&#65288;&#20061;&#39033;&#20219;&#21153;&#65289;&#21644;&#20869;&#23481;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;14&#20010;&#24320;&#28304;LVLMs&#22312;&#20869;&#30340;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#20197;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;AVIBench&#36824;&#21487;&#20316;&#20026;&#19968;&#20010;&#20415;&#21033;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09346v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) have shown significant progress in well responding to visual-instructions from users. However, these instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks. Despite the critical importance of LVLMs' robustness against such threats, current research in this area remains limited. To bridge this gap, we introduce AVIBench, a framework designed to analyze the robustness of LVLMs when facing various adversarial visual-instructions (AVIs), including four types of image-based AVIs, ten types of text-based AVIs, and nine types of content bias AVIs (such as gender, violence, cultural, and racial biases, among others). We generate 260K AVIs encompassing five categories of multimodal capabilities (nine tasks) and content bias. We then conduct a comprehensive evaluation involving 14 open-source LVLMs to assess their performance. AVIBench also serves as a convenie
&lt;/p&gt;</description></item><item><title>SketchINR&#23558;&#30690;&#37327;&#32032;&#25551;&#21387;&#32553;&#21040;&#22266;&#23450;&#32500;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#38544;&#24335;&#31070;&#32463;&#27169;&#22411;&#32534;&#30721;&#32032;&#25551;&#30340;&#24418;&#29366;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#21253;&#25324;&#25968;&#25454;&#21387;&#32553;&#12289;&#39640;&#20445;&#30495;&#24230;&#30340;&#34920;&#31034;&#20197;&#21450;&#24555;&#36895;&#35299;&#30721;&#28210;&#26579;&#12290;</title><link>https://arxiv.org/abs/2403.09344</link><description>&lt;p&gt;
SketchINR&#65306;&#23558;&#32032;&#25551;&#20316;&#20026;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
SketchINR: A First Look into Sketches as Implicit Neural Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09344
&lt;/p&gt;
&lt;p&gt;
SketchINR&#23558;&#30690;&#37327;&#32032;&#25551;&#21387;&#32553;&#21040;&#22266;&#23450;&#32500;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#38544;&#24335;&#31070;&#32463;&#27169;&#22411;&#32534;&#30721;&#32032;&#25551;&#30340;&#24418;&#29366;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#21253;&#25324;&#25968;&#25454;&#21387;&#32553;&#12289;&#39640;&#20445;&#30495;&#24230;&#30340;&#34920;&#31034;&#20197;&#21450;&#24555;&#36895;&#35299;&#30721;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SketchINR&#65292;&#26088;&#22312;&#36890;&#36807;&#38544;&#24335;&#31070;&#32463;&#27169;&#22411;&#25512;&#36827;&#30690;&#37327;&#32032;&#25551;&#30340;&#34920;&#31034;&#12290;&#21487;&#21464;&#38271;&#24230;&#30340;&#30690;&#37327;&#32032;&#25551;&#34987;&#21387;&#32553;&#21040;&#22266;&#23450;&#32500;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#38544;&#24335;&#22320;&#23558;&#22522;&#30784;&#24418;&#29366;&#32534;&#30721;&#20026;&#26102;&#38388;&#21644;&#31508;&#30011;&#30340;&#20989;&#25968;&#12290;&#23398;&#20064;&#30340;&#20989;&#25968;&#22312;&#27599;&#20010;&#26102;&#38388;&#21644;&#31508;&#30011;&#19978;&#39044;&#27979;&#32032;&#25551;&#20013;&#30340;$ xy $&#28857;&#22352;&#26631;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;SketchINR&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#34920;&#31034;&#65306;&#65288;i&#65289;&#23558;&#25972;&#20010;&#32032;&#25551;&#25968;&#25454;&#38598;&#32534;&#30721;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#28508;&#22312;&#30690;&#37327;&#26102;&#65292;SketchINR&#30456;&#27604;&#20809;&#26629;&#21644;&#30690;&#37327;&#32032;&#25551;&#65292;&#20998;&#21035;&#25552;&#20379;&#20102;$60\times$&#21644;$10\times$&#30340;&#25968;&#25454;&#21387;&#32553;&#12290; &#65288;ii&#65289;SketchINR&#30340;&#33258;&#21160;&#35299;&#30721;&#22120;&#25552;&#20379;&#27604;&#20854;&#20182;&#23398;&#20064;&#30340;&#30690;&#37327;&#32032;&#25551;&#34920;&#31034;&#26356;&#39640;&#20445;&#30495;&#24230;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#33021;&#22815;&#29420;&#29305;&#22320;&#36866;&#24212;&#22797;&#26434;&#30340;&#30690;&#37327;&#32032;&#25551;&#65292;&#22914;FS-COCO&#12290; &#65288;iii&#65289;SketchINR&#25903;&#25345;&#21487;&#20197;&#27604;&#20854;&#20182;&#23398;&#20064;&#30340;&#30690;&#37327;&#34920;&#31034;&#35299;&#30721;/&#28210;&#26579;&#24555;&#32422;&#32422;$\sim$$100\times$&#30340;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09344v1 Announce Type: cross  Abstract: We propose SketchINR, to advance the representation of vector sketches with implicit neural models. A variable length vector sketch is compressed into a latent space of fixed dimension that implicitly encodes the underlying shape as a function of time and strokes. The learned function predicts the $xy$ point coordinates in a sketch at each time and stroke. Despite its simplicity, SketchINR outperforms existing representations at multiple tasks: (i) Encoding an entire sketch dataset into a fixed size latent vector, SketchINR gives $60\times$ and $10\times$ data compression over raster and vector sketches, respectively. (ii) SketchINR's auto-decoder provides a much higher-fidelity representation than other learned vector sketch representations, and is uniquely able to scale to complex vector sketches such as FS-COCO. (iii) SketchINR supports parallelisation that can decode/render $\sim$$100\times$ faster than other learned vector represe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23616;&#37096;&#25195;&#25551;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#24341;&#20837;&#31383;&#21475;&#21010;&#20998;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25429;&#25417;&#23616;&#37096;&#20381;&#36182;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20840;&#23616;&#35270;&#35282;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#35270;&#35273;Mamba&#27169;&#22411;&#30340;&#24207;&#21015;&#24314;&#27169;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09338</link><description>&lt;p&gt;
LocalMamba: &#24102;&#31383;&#21475;&#36873;&#25321;&#25195;&#25551;&#30340;&#35270;&#35273;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LocalMamba: Visual State Space Model with Windowed Selective Scan
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23616;&#37096;&#25195;&#25551;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#24341;&#20837;&#31383;&#21475;&#21010;&#20998;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25429;&#25417;&#23616;&#37096;&#20381;&#36182;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20840;&#23616;&#35270;&#35282;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#35270;&#35273;Mamba&#27169;&#22411;&#30340;&#24207;&#21015;&#24314;&#27169;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;Mamba&#65292;&#24050;&#32463;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20363;&#22914;&#35821;&#35328;&#29702;&#35299;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#24182;&#27809;&#26377;&#26126;&#26174;&#36229;&#36234;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#21644;&#35270;&#35273;Transformer(ViTs)&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#35748;&#20026;&#22686;&#24378;&#35270;&#35273;Mamba(ViM)&#30340;&#20851;&#38190;&#22312;&#20110;&#20248;&#21270;&#24207;&#21015;&#24314;&#27169;&#30340;&#25195;&#25551;&#26041;&#21521;&#12290;&#20256;&#32479;&#30340;ViM&#26041;&#27861;&#23558;&#31354;&#38388;&#20196;&#29260;&#23637;&#24179;&#65292;&#24573;&#35270;&#20102;&#23616;&#37096;2D&#20381;&#36182;&#24615;&#30340;&#20445;&#30041;&#65292;&#20174;&#32780;&#25289;&#38271;&#20102;&#30456;&#37051;&#20196;&#29260;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23616;&#37096;&#25195;&#25551;&#31574;&#30053;&#65292;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#19981;&#21516;&#31383;&#21475;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#23616;&#37096;&#20381;&#36182;&#24615;&#21516;&#26102;&#20445;&#25345;&#20840;&#23616;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#19981;&#21516;&#32593;&#32476;&#23618;&#20043;&#38388;&#25195;&#25551;&#27169;&#24335;&#30340;&#20559;&#22909;&#21487;&#33021;&#19981;&#21516;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#26041;&#27861;&#65292;&#29420;&#31435;&#25628;&#32034;&#26368;&#20339;&#30340;&#25195;&#25551;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09338v1 Announce Type: cross  Abstract: Recent advancements in state space models, notably Mamba, have demonstrated significant progress in modeling long sequences for tasks like language understanding. Yet, their application in vision tasks has not markedly surpassed the performance of traditional Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). This paper posits that the key to enhancing Vision Mamba (ViM) lies in optimizing scan directions for sequence modeling. Traditional ViM approaches, which flatten spatial tokens, overlook the preservation of local 2D dependencies, thereby elongating the distance between adjacent tokens. We introduce a novel local scanning strategy that divides images into distinct windows, effectively capturing local dependencies while maintaining a global perspective. Additionally, acknowledging the varying preferences for scan patterns across different network layers, we propose a dynamic method to independently search for the 
&lt;/p&gt;</description></item><item><title>Griffon v2&#36890;&#36807;&#24341;&#20837;&#39640;&#20998;&#36776;&#29575;&#32553;&#25918;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20849;&#25351;&#65292;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;&#24863;&#30693;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#23567;&#23545;&#35937;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09333</link><description>&lt;p&gt;
Griffon v2&#65306;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#32553;&#25918;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20849;&#25351;&#25512;&#36827;&#22810;&#27169;&#24577;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09333
&lt;/p&gt;
&lt;p&gt;
Griffon v2&#36890;&#36807;&#24341;&#20837;&#39640;&#20998;&#36776;&#29575;&#32553;&#25918;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20849;&#25351;&#65292;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;&#24863;&#30693;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#23567;&#23545;&#35937;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#23545;&#35937;&#24863;&#30693;&#65292;&#20294;&#22270;&#20687;&#20998;&#36776;&#29575;&#30340;&#38480;&#21046;&#20173;&#28982;&#26159;&#36229;&#36234;&#22797;&#26434;&#21644;&#23494;&#38598;&#22330;&#26223;&#20013;&#29305;&#23450;&#20219;&#21153;&#19987;&#23478;&#34920;&#29616;&#30340;&#37325;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#39640;&#20998;&#36776;&#29575;&#36890;&#29992;&#27169;&#22411;&#65292;Griffon v2&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#35270;&#35273;&#21644;&#25991;&#26412;&#25552;&#31034;&#30340;&#28789;&#27963;&#23545;&#35937;&#24341;&#29992;&#12290;&#20026;&#20102;&#26377;&#25928;&#25552;&#39640;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#36731;&#37327;&#32423;&#30340;&#19979;&#37319;&#26679;&#25237;&#24433;&#20202;&#65292;&#20197;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36755;&#20837;&#26631;&#35760;&#30340;&#32422;&#26463;&#12290;&#36825;&#31181;&#35774;&#35745;&#22266;&#26377;&#22320;&#20445;&#30041;&#20102;&#23436;&#25972;&#30340;&#19978;&#19979;&#25991;&#21644;&#32454;&#33410;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#24863;&#30693;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23567;&#23545;&#35937;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20026;&#27169;&#22411;&#37197;&#32622;&#20102;&#35270;&#35273;-&#35821;&#35328;&#20849;&#25351;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09333v1 Announce Type: cross  Abstract: Large Vision Language Models have achieved fine-grained object perception, but the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios. Such limitation further restricts the model's potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and \etc. To address this issue, we introduce a unified high-resolution generalist model, Griffon v2, enabling flexible object referring with visual and textual prompts. To efficiently scaling up image resolution, we design a simple and lightweight down-sampling projector to overcome the input tokens constraint in Large Language Models. This design inherently preserves the complete contexts and fine details, and significantly improves multimodal perception ability especially for small objects. Building upon this, we further equip the model with visual-language co-refer
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#23616;&#37096;&#32593;&#26684;&#21464;&#24418;&#25216;&#26415;&#65292;HeadEvolver&#26694;&#26550;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22836;&#37096;&#22836;&#20687;&#65292;&#20445;&#30041;&#32454;&#33410;&#24182;&#25903;&#25345;&#32534;&#36753;&#21644;&#21160;&#30011;&#12290;</title><link>https://arxiv.org/abs/2403.09326</link><description>&lt;p&gt;
HeadEvolver&#65306;&#36890;&#36807;&#26412;&#22320;&#21487;&#23398;&#20064;&#32593;&#26684;&#21464;&#24418;&#23454;&#29616;&#25991;&#26412;&#21040;&#22836;&#37096;&#22836;&#20687;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09326
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#23616;&#37096;&#32593;&#26684;&#21464;&#24418;&#25216;&#26415;&#65292;HeadEvolver&#26694;&#26550;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22836;&#37096;&#22836;&#20687;&#65292;&#20445;&#30041;&#32454;&#33410;&#24182;&#25903;&#25345;&#32534;&#36753;&#21644;&#21160;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;HeadEvolver&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#39118;&#26684;&#21270;&#30340;&#22836;&#37096;&#22836;&#20687;&#12290;HeadEvolver&#20351;&#29992;&#27169;&#26495;&#22836;&#37096;&#32593;&#26684;&#30340;&#26412;&#22320;&#21487;&#23398;&#20064;&#32593;&#26684;&#21464;&#24418;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#23383;&#36164;&#20135;&#65292;&#20197;&#23454;&#29616;&#20445;&#30041;&#32454;&#33410;&#30340;&#32534;&#36753;&#21644;&#21160;&#30011;&#12290;&#20026;&#20102;&#35299;&#20915;&#20840;&#23616;&#21464;&#24418;&#20013;&#32570;&#20047;&#32454;&#31890;&#24230;&#21644;&#35821;&#20041;&#24863;&#30693;&#26412;&#22320;&#24418;&#29366;&#25511;&#21046;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#20316;&#20026;&#27599;&#20010;&#19977;&#35282;&#24418;&#30340;Jacobi&#30697;&#38453;&#30340;&#21152;&#26435;&#22240;&#23376;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#26412;&#22320;&#24418;&#29366;&#21516;&#26102;&#20445;&#25345;&#20840;&#23616;&#23545;&#24212;&#21644;&#38754;&#37096;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30830;&#20445;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#32467;&#26524;&#24418;&#29366;&#21644;&#22806;&#35266;&#30340;&#36830;&#36143;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21487;&#24494;&#20998;&#28210;&#26579;&#65292;&#24182;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#20197;&#22312;&#25991;&#26412;&#24341;&#23548;&#19979;&#20248;&#21270;&#21464;&#24418;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#20851;&#33410;&#32593;&#26684;&#30340;&#22810;&#26679;&#21270;&#22836;&#37096;&#22836;&#20687;&#65292;&#21487;&#26080;&#32541;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09326v1 Announce Type: cross  Abstract: We present HeadEvolver, a novel framework to generate stylized head avatars from text guidance. HeadEvolver uses locally learnable mesh deformation from a template head mesh, producing high-quality digital assets for detail-preserving editing and animation. To tackle the challenges of lacking fine-grained and semantic-aware local shape control in global deformation through Jacobians, we introduce a trainable parameter as a weighting factor for the Jacobian at each triangle to adaptively change local shapes while maintaining global correspondences and facial features. Moreover, to ensure the coherence of the resulting shape and appearance from different viewpoints, we use pretrained image diffusion models for differentiable rendering with regularization terms to refine the deformation under text guidance. Extensive experiments demonstrate that our method can generate diverse head avatars with an articulated mesh that can be edited seaml
&lt;/p&gt;</description></item><item><title>SD-Net&#36890;&#36807;&#23545;&#31216;&#24863;&#30693;&#20851;&#38190;&#28857;&#39044;&#27979;&#21644;&#33258;&#35757;&#32451;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;&#23545;&#31216;&#29289;&#20307;&#21644;&#30495;&#23454;&#22330;&#26223;6D&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.09317</link><description>&lt;p&gt;
SD-Net&#65306;&#23545;&#31216;&#24863;&#30693;&#20851;&#38190;&#28857;&#39044;&#27979;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;&#25235;&#21462;&#22330;&#26223;&#20013;&#30340;6D&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09317
&lt;/p&gt;
&lt;p&gt;
SD-Net&#36890;&#36807;&#23545;&#31216;&#24863;&#30693;&#20851;&#38190;&#28857;&#39044;&#27979;&#21644;&#33258;&#35757;&#32451;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;&#23545;&#31216;&#29289;&#20307;&#21644;&#30495;&#23454;&#22330;&#26223;6D&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25235;&#21462;&#22330;&#26223;&#20013;&#30340;6D&#23039;&#24577;&#20272;&#35745;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#20026;&#23545;&#31216;&#29289;&#20307;&#21644;&#30495;&#23454;&#22330;&#26223;&#20135;&#29983;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20027;&#35201;&#29942;&#39048;&#21253;&#25324;1&#65289;&#30001;&#29289;&#20307;&#23545;&#31216;&#24341;&#36215;&#30340;&#20851;&#38190;&#28857;&#27495;&#20041;&#65307;2&#65289;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#20043;&#38388;&#30340;&#22495;&#24046;&#36317;&#12290;&#20026;&#20102;&#35268;&#36991;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#23545;&#31216;&#24863;&#30693;&#20851;&#38190;&#28857;&#39044;&#27979;&#21644;&#33258;&#35757;&#32451;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;SD-Net&#65289;&#30340;6D&#23039;&#24577;&#20272;&#35745;&#32593;&#32476;&#12290;SD-Net&#24314;&#31435;&#22312;&#28857;&#23545;&#28857;&#20851;&#38190;&#28857;&#22238;&#24402;&#21644;&#28145;&#24230;&#38669;&#22827;&#25237;&#31080;&#22522;&#30784;&#19978;&#65292;&#21487;&#20197;&#22312;&#26434;&#20081;&#21644;&#36974;&#25377;&#19979;&#21487;&#38752;&#22320;&#26816;&#27979;&#20851;&#38190;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#20851;&#38190;&#28857;&#39044;&#27979;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#19977;&#32500;&#20851;&#38190;&#28857;&#36873;&#25321;&#31574;&#30053;&#65292;&#32771;&#34385;&#20102;&#23545;&#35937;&#30340;&#23545;&#31216;&#31867;&#21035;&#21644;&#31561;&#25928;&#20851;&#38190;&#28857;&#65292;&#36825;&#26377;&#21161;&#20110;&#21363;&#20351;&#22312;&#39640;&#24230;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#23450;&#20301;3D&#20851;&#38190;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#39044;&#27979;&#20851;&#38190;&#28857;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#28388;&#27874;&#31639;&#27861;&#20197;&#21160;&#24577;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09317v1 Announce Type: cross  Abstract: Despite the success in 6D pose estimation in bin-picking scenarios, existing methods still struggle to produce accurate prediction results for symmetry objects and real world scenarios. The primary bottlenecks include 1) the ambiguity keypoints caused by object symmetries; 2) the domain gap between real and synthetic data. To circumvent these problem, we propose a new 6D pose estimation network with symmetric-aware keypoint prediction and self-training domain adaptation (SD-Net). SD-Net builds on pointwise keypoint regression and deep hough voting to perform reliable detection keypoint under clutter and occlusion. Specifically, at the keypoint prediction stage, we designe a robust 3D keypoints selection strategy considering the symmetry class of objects and equivalent keypoints, which facilitate locating 3D keypoints even in highly occluded scenes. Additionally, we build an effective filtering algorithm on predicted keypoint to dynamic
&lt;/p&gt;</description></item><item><title>YOLOX-ViT&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#26377;&#25928;&#20943;&#23569;&#20102;&#20391;&#25195;&#22768;&#32435;&#30446;&#26631;&#26816;&#27979;&#20013;&#22681;&#20307;&#35823;&#25253;&#65292;&#24182;&#24341;&#20837;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#23618;&#26174;&#33879;&#25552;&#39640;&#20102;&#27700;&#19979;&#29615;&#22659;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09313</link><description>&lt;p&gt;
YOLOX-ViT&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#22312;&#20391;&#25195;&#22768;&#32435;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09313
&lt;/p&gt;
&lt;p&gt;
YOLOX-ViT&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#26377;&#25928;&#20943;&#23569;&#20102;&#20391;&#25195;&#22768;&#32435;&#30446;&#26631;&#26816;&#27979;&#20013;&#22681;&#20307;&#35823;&#25253;&#65292;&#24182;&#24341;&#20837;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#23618;&#26174;&#33879;&#25552;&#39640;&#20102;&#27700;&#19979;&#29615;&#22659;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;YOLOX-ViT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#30693;&#35782;&#33976;&#39311;&#22312;&#27169;&#22411;&#23610;&#23544;&#20943;&#23567;&#20294;&#19981;&#25439;&#22833;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#27700;&#19979;&#26426;&#22120;&#20154;&#65292;&#25506;&#35752;&#20102;&#20851;&#20110;&#26356;&#23567;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;YOLOX&#20013;&#35270;&#35273;&#21464;&#25442;&#22120;&#23618;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20391;&#25195;&#22768;&#32435;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#29992;&#23427;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30693;&#35782;&#33976;&#39311;&#26377;&#25928;&#20943;&#23569;&#20102;&#22681;&#20307;&#26816;&#27979;&#20013;&#30340;&#35823;&#25253;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#23618;&#26174;&#33879;&#25552;&#39640;&#20102;&#27700;&#19979;&#29615;&#22659;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#30693;&#35782;&#33976;&#39311;&#22312;YOLOX-ViT&#20013;&#30340;&#28304;&#20195;&#30721;&#22312;https://github.com/remaro-network/KD-YOLOX-ViT&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09313v1 Announce Type: cross  Abstract: In this paper we present YOLOX-ViT, a novel object detection model, and investigate the efficacy of knowledge distillation for model size reduction without sacrificing performance. Focused on underwater robotics, our research addresses key questions about the viability of smaller models and the impact of the visual transformer layer in YOLOX. Furthermore, we introduce a new side-scan sonar image dataset, and use it to evaluate our object detector's performance. Results show that knowledge distillation effectively reduces false positives in wall detection. Additionally, the introduced visual transformer layer significantly improves object detection accuracy in the underwater environment. The source code of the knowledge distillation in the YOLOX-ViT is at https://github.com/remaro-network/KD-YOLOX-ViT.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SELECTOR&#30340;&#24322;&#26500;&#22270;&#32593;&#32476;&#65292;&#21033;&#29992;&#21367;&#31215;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#30284;&#30151;&#24739;&#32773;&#29983;&#23384;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#39044;&#27979;</title><link>https://arxiv.org/abs/2403.09290</link><description>&lt;p&gt;
SELECTOR&#65306;&#20855;&#26377;&#21367;&#31215;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#24322;&#26500;&#22270;&#32593;&#32476;&#65292;&#29992;&#20110;&#30284;&#30151;&#29983;&#23384;&#40065;&#26834;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SELECTOR&#30340;&#24322;&#26500;&#22270;&#32593;&#32476;&#65292;&#21033;&#29992;&#21367;&#31215;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#30284;&#30151;&#24739;&#32773;&#29983;&#23384;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#30284;&#30151;&#24739;&#32773;&#30340;&#29983;&#23384;&#29575;&#23545;&#20110;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#21046;&#23450;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#26696;&#65292;&#38477;&#20302;&#19982;&#30284;&#30151;&#30456;&#20851;&#30340;&#21307;&#30103;&#36153;&#29992;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#30284;&#30151;&#24739;&#32773;&#29983;&#23384;&#30340;&#22810;&#27169;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20840;&#38754;&#12289;&#26356;&#31934;&#30830;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#30528;&#19982;&#32570;&#22833;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#27169;&#24577;&#20869;&#20449;&#24687;&#20132;&#20114;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SELECTOR&#65292;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#25513;&#30721;&#32534;&#30721;&#22120;&#30340;&#24322;&#26500;&#22270;&#24863;&#30693;&#32593;&#32476;&#65292;&#29992;&#20110;&#30284;&#30151;&#24739;&#32773;&#29983;&#23384;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#39044;&#27979;&#12290;SELECTOR&#21253;&#25324;&#29305;&#24449;&#36793;&#37325;&#26500;&#12289;&#21367;&#31215;&#25513;&#30721;&#32534;&#30721;&#22120;&#12289;&#29305;&#24449;&#20132;&#21449;&#34701;&#21512;&#21644;&#22810;&#27169;&#24577;&#29983;&#23384;&#39044;&#27979;&#27169;&#22359;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#24322;&#26500;&#22270;&#65292;&#24182;&#37319;&#29992;&#20803;&#36335;&#24452;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#36793;&#37325;&#26500;&#65292;&#30830;&#20445;&#29305;&#24449;&#20449;&#24687;&#30340;&#20840;&#38754;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09290v1 Announce Type: cross  Abstract: Accurately predicting the survival rate of cancer patients is crucial for aiding clinicians in planning appropriate treatment, reducing cancer-related medical expenses, and significantly enhancing patients' quality of life. Multimodal prediction of cancer patient survival offers a more comprehensive and precise approach. However, existing methods still grapple with challenges related to missing multimodal data and information interaction within modalities. This paper introduces SELECTOR, a heterogeneous graph-aware network based on convolutional mask encoders for robust multimodal prediction of cancer patient survival. SELECTOR comprises feature edge reconstruction, convolutional mask encoder, feature cross-fusion, and multimodal survival prediction modules. Initially, we construct a multimodal heterogeneous graph and employ the meta-path method for feature edge reconstruction, ensuring comprehensive incorporation of feature informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#22312;&#25317;&#26377;&#22810;&#20010;&#29420;&#31435;AI&#20195;&#29702;&#30340;&#29615;&#22659;&#20013;&#30340;&#31934;&#31070;&#29702;&#35770;&#65288;ToM&#65289;&#65292;&#24182;&#21457;&#29616;&#24403;&#20195;AI&#23637;&#29616;&#20986;&#25509;&#36817;&#23436;&#32654;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09289</link><description>&lt;p&gt;
&#30789;&#20013;&#24515;&#29702;&#24565;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Silico-centric Theory of Mind
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#22312;&#25317;&#26377;&#22810;&#20010;&#29420;&#31435;AI&#20195;&#29702;&#30340;&#29615;&#22659;&#20013;&#30340;&#31934;&#31070;&#29702;&#35770;&#65288;ToM&#65289;&#65292;&#24182;&#21457;&#29616;&#24403;&#20195;AI&#23637;&#29616;&#20986;&#25509;&#36817;&#23436;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#31070;&#29702;&#35770;&#65288;ToM&#65289;&#25351;&#30340;&#26159;&#23558;&#20449;&#24565;&#12289;&#27442;&#26395;&#12289;&#24847;&#22270;&#21644;&#30693;&#35782;&#31561;&#31934;&#31070;&#29366;&#24577;&#24402;&#22240;&#20110;&#33258;&#24049;&#21644;&#20182;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#29702;&#35299;&#36825;&#20123;&#31934;&#31070;&#29366;&#24577;&#21487;&#33021;&#19981;&#21516;&#20110;&#33258;&#24049;&#21644;&#29616;&#23454;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25317;&#26377;&#22810;&#20010;&#12289;&#19981;&#21516;&#12289;&#29420;&#31435;AI&#20195;&#29702;&#30340;&#29615;&#22659;&#20013;&#30340;ToM&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#20869;&#37096;&#29366;&#24577;&#12289;&#20449;&#24687;&#21644;&#30446;&#26631;&#12290;&#21463;&#20154;&#31867;&#38169;&#35823;&#20449;&#24565;&#23454;&#39564;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#24773;&#22659;&#30340;AI&#65288;&#8220;&#28966;&#28857;AI&#8221;&#65289;&#65292;&#20854;&#20013;&#20854;&#20811;&#38534;&#20307;&#32463;&#21382;&#20102;&#20197;&#20154;&#31867;&#20026;&#20013;&#24515;&#30340;ToM&#35780;&#20272;&#12290;&#25105;&#20204;&#20419;&#20351;&#28966;&#28857;AI&#35780;&#20272;&#20854;&#20811;&#38534;&#20307;&#26159;&#21542;&#20250;&#20174;&#39069;&#22806;&#30340;&#25351;&#23548;&#20013;&#21463;&#30410;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35753;&#20811;&#38534;&#20307;&#36827;&#34892;ToM&#35780;&#20272;&#65292;&#26377;&#21644;&#27809;&#26377;&#25351;&#23548;&#65292;&#20174;&#32780;&#20351;&#28966;&#28857;AI&#21442;&#19982;&#31867;&#20284;&#20110;&#20154;&#31867;&#24515;&#26234;&#21270;&#30340;&#39640;&#38454;&#21453;&#20107;&#23454;&#25512;&#29702;--&#38024;&#23545;&#19968;&#39033;&#27979;&#35797;&#20013;&#30340;&#20154;&#31867;&#21644;&#21478;&#19968;&#39033;&#27979;&#35797;&#20013;&#30340;&#20854;&#20182;AI&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#19981;&#19968;&#33268;&#65306;&#24403;&#20195;AI&#34920;&#29616;&#20986;&#25509;&#36817;&#23436;&#32654;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09289v1 Announce Type: new  Abstract: Theory of Mind (ToM) refers to the ability to attribute mental states, such as beliefs, desires, intentions, and knowledge, to oneself and others, and to understand that these mental states can differ from one's own and from reality. We investigate ToM in environments with multiple, distinct, independent AI agents, each possessing unique internal states, information, and objectives. Inspired by human false-belief experiments, we present an AI ('focal AI') with a scenario where its clone undergoes a human-centric ToM assessment. We prompt the focal AI to assess whether its clone would benefit from additional instructions. Concurrently, we give its clones the ToM assessment, both with and without the instructions, thereby engaging the focal AI in higher-order counterfactual reasoning akin to human mentalizing--with respect to humans in one test and to other AI in another. We uncover a discrepancy: Contemporary AI demonstrates near-perfect 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31354;&#38388;&#24863;&#30693;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#26550;&#26500;&#65292;&#36890;&#36807;Adversarial OCR Enhancement&#65288;AOE&#65289;&#27169;&#22359;&#21644;&#31354;&#38388;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;SASA&#65289;&#26426;&#21046;&#65292;&#21487;&#20197;&#22686;&#24378;OCR&#25991;&#26412;&#30340;&#23481;&#38169;&#24615;&#65292;&#24182;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#25429;&#25417;OCR&#26631;&#35760;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.09288</link><description>&lt;p&gt;
&#20855;&#26377;OCR&#27169;&#24577;&#25200;&#21160;&#30340;&#23545;&#25239;&#35757;&#32451;&#29992;&#20110;&#22330;&#26223;&#25991;&#26412;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09288
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31354;&#38388;&#24863;&#30693;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#26550;&#26500;&#65292;&#36890;&#36807;Adversarial OCR Enhancement&#65288;AOE&#65289;&#27169;&#22359;&#21644;&#31354;&#38388;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;SASA&#65289;&#26426;&#21046;&#65292;&#21487;&#20197;&#22686;&#24378;OCR&#25991;&#26412;&#30340;&#23481;&#38169;&#24615;&#65292;&#24182;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#25429;&#25417;OCR&#26631;&#35760;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Scene-Text Visual Question Answering&#65288;ST-VQA&#65289;&#26088;&#22312;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#22330;&#26223;&#25991;&#26412;&#24182;&#22238;&#31572;&#19982;&#25991;&#26412;&#20869;&#23481;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31354;&#38388;&#24863;&#30693;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#23545;&#25239;&#35757;&#32451;&#26550;&#26500;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;Adversarial OCR Enhancement&#65288;AOE&#65289;&#27169;&#22359;&#65292;&#22312;OCR&#27169;&#24577;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#26469;&#22686;&#24378;OCR&#25991;&#26412;&#30340;&#23481;&#38169;&#34920;&#31034;&#65292;&#20174;&#32780;&#20943;&#23569;OCR&#38169;&#35823;&#24341;&#36215;&#30340;&#22122;&#38899;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#19968;&#20010;&#31354;&#38388;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;SASA&#65289;&#26426;&#21046;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#25429;&#25417;OCR&#26631;&#35760;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#21508;&#31181;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09288v1 Announce Type: cross  Abstract: Scene-Text Visual Question Answering (ST-VQA) aims to understand scene text in images and answer questions related to the text content. Most existing methods heavily rely on the accuracy of Optical Character Recognition (OCR) systems, and aggressive fine-tuning based on limited spatial location information and erroneous OCR text information often leads to inevitable overfitting. In this paper, we propose a multimodal adversarial training architecture with spatial awareness capabilities. Specifically, we introduce an Adversarial OCR Enhancement (AOE) module, which leverages adversarial training in the embedding space of OCR modality to enhance fault-tolerant representation of OCR texts, thereby reducing noise caused by OCR errors. Simultaneously, We add a Spatial-Aware Self-Attention (SASA) mechanism to help the model better capture the spatial relationships among OCR tokens. Various experiments demonstrate that our method achieves sign
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23558;&#32422;&#26463;&#32534;&#31243;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20351;&#29992;&#32422;&#26463;&#32534;&#31243;&#29983;&#25104;&#30340;&#26368;&#20248;&#35299;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#24191;&#27867;&#25506;&#32034;&#30340;&#38656;&#35201;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09249</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#35299;&#20915;&#26580;&#24615;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20013;&#21033;&#29992;&#32422;&#26463;&#32534;&#31243;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Constraint Programming in a Deep Learning Approach for Dynamically Solving the Flexible Job-Shop Scheduling Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09249
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23558;&#32422;&#26463;&#32534;&#31243;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20351;&#29992;&#32422;&#26463;&#32534;&#31243;&#29983;&#25104;&#30340;&#26368;&#20248;&#35299;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#24191;&#27867;&#25506;&#32034;&#30340;&#38656;&#35201;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26580;&#24615;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;FJSSP&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#65292;&#22240;&#20854;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#23454;&#26102;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;DRL&#26041;&#27861;&#24120;&#24120;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#29616;&#26377;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#22914;&#30830;&#20999;&#26041;&#27861;&#25110;&#32422;&#26463;&#32534;&#31243;&#65288;CP&#65289;&#65292;&#21518;&#32773;&#33021;&#22815;&#22312;&#36739;&#23567;&#23454;&#20363;&#20013;&#25214;&#21040;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#32422;&#26463;&#32534;&#31243;&#38598;&#25104;&#21040;&#20197;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#20013;&#65292;&#20805;&#20998;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09249v1 Announce Type: new  Abstract: Recent advancements in the flexible job-shop scheduling problem (FJSSP) are primarily based on deep reinforcement learning (DRL) due to its ability to generate high-quality, real-time solutions. However, DRL approaches often fail to fully harness the strengths of existing techniques such as exact methods or constraint programming (CP), which can excel at finding optimal or near-optimal solutions for smaller instances. This paper aims to integrate CP within a deep learning (DL) based methodology, leveraging the benefits of both. In this paper, we introduce a method that involves training a DL model using optimal solutions generated by CP, ensuring the model learns from high-quality data, thereby eliminating the need for the extensive exploration typical in DRL and enhancing overall performance. Further, we integrate CP into our DL framework to jointly construct solutions, utilizing DL for the initial complex stages and transitioning to CP
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861; REVISEDplus&#65292;&#26088;&#22312;&#29983;&#25104;&#26356;&#21487;&#34892;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#24207;&#21015;&#24615;&#36136;&#19994;&#21153;&#27969;&#31243;&#26696;&#20363;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09232</link><description>&lt;p&gt;
&#29983;&#25104;&#19994;&#21153;&#27969;&#31243;&#32467;&#26524;&#39044;&#27979;&#30340;&#21487;&#34892;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09232
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861; REVISEDplus&#65292;&#26088;&#22312;&#29983;&#25104;&#26356;&#21487;&#34892;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#24207;&#21015;&#24615;&#36136;&#19994;&#21153;&#27969;&#31243;&#26696;&#20363;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#25104;&#21151;&#22320;&#24341;&#20837;&#20102;&#39044;&#27979;&#24615;&#27969;&#31243;&#20998;&#26512;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#22266;&#26377;&#30340;&#19981;&#36879;&#26126;&#24615;&#32473;&#20154;&#31867;&#20915;&#31574;&#32773;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#20182;&#20204;&#29702;&#35299;&#39044;&#27979;&#32972;&#21518;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#36825;&#19968;&#26085;&#30410;&#20851;&#27880;&#30340;&#38382;&#39064;&#24341;&#21457;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24341;&#20837;&#65292;&#35774;&#35745;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#20551;&#35774;&#22330;&#26223;&#65292;&#20197;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#20915;&#31574;&#32972;&#21518;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#22788;&#29702;&#36890;&#24120;&#29992;&#20110;&#39044;&#27979;&#24615;&#27969;&#31243;&#20998;&#26512;&#30340;&#24207;&#21015;&#24615;&#36136;&#65288;&#19994;&#21153;&#65289;&#27969;&#31243;&#26696;&#20363;&#26102;&#36935;&#21040;&#29305;&#23450;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861; REVISEDplus &#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#29983;&#25104;&#26356;&#21487;&#34892;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09232v1 Announce Type: new  Abstract: In recent years, various machine and deep learning architectures have been successfully introduced to the field of predictive process analytics. Nevertheless, the inherent opacity of these algorithms poses a significant challenge for human decision-makers, hindering their ability to understand the reasoning behind the predictions. This growing concern has sparked the introduction of counterfactual explanations, designed as human-understandable what if scenarios, to provide clearer insights into the decision-making process behind undesirable predictions. The generation of counterfactual explanations, however, encounters specific challenges when dealing with the sequential nature of the (business) process cases typically used in predictive process analytics. Our paper tackles this challenge by introducing a data-driven approach, REVISEDplus, to generate more feasible and plausible counterfactual explanations. First, we restrict the counter
&lt;/p&gt;</description></item><item><title>BEHAVIOR-1K&#26159;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26426;&#22120;&#20154;&#25216;&#26415;&#32508;&#21512;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;1,000&#20010;&#26085;&#24120;&#27963;&#21160;&#21644;&#29992;&#20110;&#25903;&#25345;&#36825;&#20123;&#27963;&#21160;&#30340;&#36924;&#30495;&#27169;&#25311;&#29615;&#22659;&#12290;&#36825;&#20123;&#27963;&#21160;&#20855;&#26377;&#38271;&#35270;&#37326;&#65292;&#22797;&#26434;&#25805;&#20316;&#25216;&#33021;&#20381;&#36182;&#65292;&#23545;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09227</link><description>&lt;p&gt;
BEHAVIOR-1K&#65306;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#65292;&#20855;&#26377;1,000&#20010;&#26085;&#24120;&#27963;&#21160;&#21644;&#36924;&#30495;&#27169;&#25311;&#30340;AI&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09227
&lt;/p&gt;
&lt;p&gt;
BEHAVIOR-1K&#26159;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26426;&#22120;&#20154;&#25216;&#26415;&#32508;&#21512;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;1,000&#20010;&#26085;&#24120;&#27963;&#21160;&#21644;&#29992;&#20110;&#25903;&#25345;&#36825;&#20123;&#27963;&#21160;&#30340;&#36924;&#30495;&#27169;&#25311;&#29615;&#22659;&#12290;&#36825;&#20123;&#27963;&#21160;&#20855;&#26377;&#38271;&#35270;&#37326;&#65292;&#22797;&#26434;&#25805;&#20316;&#25216;&#33021;&#20381;&#36182;&#65292;&#23545;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;BEHAVIOR-1K&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26426;&#22120;&#20154;&#25216;&#26415;&#32780;&#35774;&#35745;&#30340;&#32508;&#21512;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#12290;BEHAVIOR-1K&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#21463;&#21040;&#24191;&#27867;&#35843;&#26597;&#32467;&#26524;&#30340;&#25351;&#23548;&#21644;&#21551;&#21457;&#65292;&#35843;&#26597;&#38382;&#39064;&#20026;&#8220;&#20320;&#24076;&#26395;&#26426;&#22120;&#20154;&#20026;&#20320;&#20570;&#20160;&#20040;&#65311;&#8221;&#12290;&#31532;&#19968;&#20010;&#37096;&#20998;&#23450;&#20041;&#20102;1,000&#20010;&#26085;&#24120;&#27963;&#21160;&#65292;&#22522;&#20110;50&#20010;&#22330;&#26223;&#65288;&#25151;&#23627;&#65292;&#33457;&#22253;&#65292;&#39184;&#21381;&#65292;&#21150;&#20844;&#23460;&#31561;&#65289;&#65292;&#36229;&#36807;9,000&#20010;&#23545;&#35937;&#24102;&#26377;&#20016;&#23500;&#30340;&#29289;&#29702;&#21644;&#35821;&#20041;&#23646;&#24615;&#30340;&#26631;&#27880;&#12290;&#31532;&#20108;&#20010;&#37096;&#20998;&#26159;OMNIGIBSON&#65292;&#36825;&#26159;&#19968;&#20010;&#25903;&#25345;&#36825;&#20123;&#27963;&#21160;&#30340;&#26032;&#39062;&#20223;&#30495;&#29615;&#22659;&#65292;&#36890;&#36807;&#23545;&#21018;&#20307;&#12289;&#21487;&#21464;&#24418;&#20307;&#21644;&#28082;&#20307;&#30340;&#36924;&#30495;&#29289;&#29702;&#27169;&#25311;&#21644;&#28210;&#26579;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BEHAVIOR-1K&#20013;&#30340;&#27963;&#21160;&#20855;&#26377;&#38271;&#35270;&#37326;&#65292;&#24182;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#25805;&#20316;&#25216;&#33021;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20063;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#26657;&#20934;BEHAVIOR-1K&#30340;&#20223;&#30495;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#36716;&#31227;&#26041;&#26696;&#23398;&#20064;&#30340;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09227v1 Announce Type: cross  Abstract: We present BEHAVIOR-1K, a comprehensive simulation benchmark for human-centered robotics. BEHAVIOR-1K includes two components, guided and motivated by the results of an extensive survey on "what do you want robots to do for you?". The first is the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 9,000 objects annotated with rich physical and semantic properties. The second is OMNIGIBSON, a novel simulation environment that supports these activities via realistic physics simulation and rendering of rigid bodies, deformable bodies, and liquids. Our experiments indicate that the activities in BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both of which remain a challenge for even state-of-the-art robot learning solutions. To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an initial study on transferring solutions lear
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Laplace&#36924;&#36817;&#30340;&#22810;&#31181;&#24230;&#37327;&#26631;&#20934;&#65292;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#36873;&#25321;&#20013;&#36935;&#21040;&#30340;&#24615;&#33021;&#19981;&#20339;&#21644;&#36816;&#34892;&#26102;&#38388;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26032;&#30340;&#26631;&#20934;&#22312;&#36136;&#37327;&#19978;&#19982;&#40644;&#37329;&#26631;&#20934;&#21160;&#24577;&#23884;&#22871;&#25277;&#26679;&#30456;&#24403;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.09215</link><description>&lt;p&gt;
Laplace&#36924;&#36817;&#20316;&#20026;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Laplace Approximation as Model Selection Criterion for Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09215
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Laplace&#36924;&#36817;&#30340;&#22810;&#31181;&#24230;&#37327;&#26631;&#20934;&#65292;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#36873;&#25321;&#20013;&#36935;&#21040;&#30340;&#24615;&#33021;&#19981;&#20339;&#21644;&#36816;&#34892;&#26102;&#38388;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26032;&#30340;&#26631;&#20934;&#22312;&#36136;&#37327;&#19978;&#19982;&#40644;&#37329;&#26631;&#20934;&#21160;&#24577;&#23884;&#22871;&#25277;&#26679;&#30456;&#24403;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36873;&#25321;&#26088;&#22312;&#25214;&#21040;&#22312;&#20934;&#30830;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#25110;&#31616;&#26131;&#24615;&#26041;&#38754;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#26368;&#22909;&#26159;&#19977;&#32773;&#20860;&#39038;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#20110;&#35780;&#20272;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#21363;&#25214;&#21040;&#19968;&#20010;&#25552;&#20379;&#25152;&#26377;&#36825;&#20123;&#20934;&#21017;&#20043;&#38388;&#26368;&#20339;&#26435;&#34913;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;Laplace&#36924;&#36817;&#30340;&#22810;&#31181;&#24230;&#37327;&#26631;&#20934;&#26469;&#24212;&#23545;&#20197;&#24448;&#24037;&#20316;&#20013;&#23384;&#22312;&#30340;&#24615;&#33021;&#19981;&#20339;&#25110;&#20855;&#26377;&#20005;&#37325;&#36816;&#34892;&#26102;&#38388;&#38382;&#39064;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#36866;&#29992;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#22312;&#36136;&#37327;&#19978;&#19982;&#40644;&#37329;&#26631;&#20934;&#21160;&#24577;&#23884;&#22871;&#25277;&#26679;&#30456;&#24403;&#65292;&#32780;&#21448;&#19981;&#20250;&#24433;&#21709;&#35745;&#31639;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36873;&#25321;&#26631;&#20934;&#20351;&#24471;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#27169;&#22411;&#36873;&#25321;&#33021;&#22815;&#26356;&#24555;&#36895;&#22320;&#19988;&#29983;&#25104;&#36136;&#37327;&#26356;&#39640;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09215v1 Announce Type: cross  Abstract: Model selection aims to find the best model in terms of accuracy, interpretability or simplicity, preferably all at once. In this work, we focus on evaluating model performance of Gaussian process models, i.e. finding a metric that provides the best trade-off between all those criteria. While previous work considers metrics like the likelihood, AIC or dynamic nested sampling, they either lack performance or have significant runtime issues, which severely limits applicability. We address these challenges by introducing multiple metrics based on the Laplace approximation, where we overcome a severe inconsistency occuring during naive application of the Laplace approximation. Experiments show that our metrics are comparable in quality to the gold standard dynamic nested sampling without compromising for computational speed. Our model selection criteria allow significantly faster and high quality model selection of Gaussian process models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30340;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LAN&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#27963;&#21160;&#32423;&#21035;&#36827;&#34892;&#20869;&#37096;&#23041;&#32961;&#26816;&#27979;&#65292;&#24182;&#23398;&#20064;&#27963;&#21160;&#24207;&#21015;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.09209</link><description>&lt;p&gt;
&#23398;&#20064;&#33258;&#36866;&#24212;&#37051;&#23621;&#20197;&#23454;&#26102;&#26816;&#27979;&#20869;&#37096;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30340;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LAN&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#27963;&#21160;&#32423;&#21035;&#36827;&#34892;&#20869;&#37096;&#23041;&#32961;&#26816;&#27979;&#65292;&#24182;&#23398;&#20064;&#27963;&#21160;&#24207;&#21015;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#21644;&#32452;&#32455;&#38754;&#20020;&#26469;&#33258;&#20869;&#37096;&#21592;&#24037;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#30340;&#28508;&#22312;&#23041;&#32961;&#12290;&#20808;&#21069;&#20851;&#20110;&#20869;&#37096;&#23041;&#32961;&#26816;&#27979;&#65288;ITD&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26816;&#27979;&#24322;&#24120;&#29992;&#25143;&#25110;&#24322;&#24120;&#26102;&#38388;&#27573;&#65288;&#20363;&#22914;&#65292;&#19968;&#21608;&#25110;&#19968;&#22825;&#65289;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#21487;&#33021;&#22312;&#26085;&#24535;&#20013;&#26377;&#25968;&#21313;&#19975;&#26465;&#27963;&#21160;&#65292;&#21363;&#20351;&#22312;&#19968;&#22825;&#20869;&#65292;&#19968;&#20010;&#29992;&#25143;&#20063;&#21487;&#33021;&#23384;&#22312;&#25968;&#21315;&#26465;&#27963;&#21160;&#65292;&#36825;&#38656;&#35201;&#39640;&#26114;&#30340;&#35843;&#26597;&#39044;&#31639;&#26469;&#39564;&#35777;&#24322;&#24120;&#29992;&#25143;&#25110;&#27963;&#21160;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#26159;&#20107;&#21518;&#26041;&#27861;&#32780;&#19981;&#26159;&#23454;&#26102;&#26816;&#27979;&#65292;&#26080;&#27861;&#21450;&#26102;&#25253;&#21578;&#20869;&#37096;&#23041;&#32961;&#22312;&#24341;&#36215;&#25439;&#22833;&#20043;&#21069;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#38024;&#23545;&#27963;&#21160;&#32423;&#21035;&#23454;&#26102;ITD&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#31890;&#24230;&#21644;&#39640;&#25928;&#30340;&#26694;&#26550;LAN&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LAN&#21516;&#26102;&#23398;&#20064;&#27963;&#21160;&#24207;&#21015;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#27963;&#21160;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09209v1 Announce Type: cross  Abstract: Enterprises and organizations are faced with potential threats from insider employees that may lead to serious consequences. Previous studies on insider threat detection (ITD) mainly focus on detecting abnormal users or abnormal time periods (e.g., a week or a day). However, a user may have hundreds of thousands of activities in the log, and even within a day there may exist thousands of activities for a user, requiring a high investigation budget to verify abnormal users or activities given the detection results. On the other hand, existing works are mainly post-hoc methods rather than real-time detection, which can not report insider threats in time before they cause loss. In this paper, we conduct the first study towards real-time ITD at activity level, and present a fine-grained and efficient framework LAN. Specifically, LAN simultaneously learns the temporal dependencies within an activity sequence and the relationships between ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#19977;&#23618;&#32447;&#24615;&#32467;&#26500;&#30340;&#37096;&#20998;CBM&#20013;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#27010;&#21270;&#38169;&#35823;&#30340;&#19978;&#30028;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#37096;&#20998;CBM&#20248;&#20110;&#26420;&#32032;CBM&#12290;</title><link>https://arxiv.org/abs/2403.09206</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#27010;&#21270;&#38169;&#35823;&#22312;&#37096;&#20998;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20013;&#30340;&#19978;&#30028;&#65306;&#37096;&#20998;CBM&#32988;&#36807;&#26420;&#32032;CBM
&lt;/p&gt;
&lt;p&gt;
Upper Bound of Bayesian Generalization Error in Partial Concept Bottleneck Model (CBM): Partial CBM outperforms naive CBM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#19977;&#23618;&#32447;&#24615;&#32467;&#26500;&#30340;&#37096;&#20998;CBM&#20013;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#27010;&#21270;&#38169;&#35823;&#30340;&#19978;&#30028;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#37096;&#20998;CBM&#20248;&#20110;&#26420;&#32032;CBM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.09206v1 &#31867;&#22411;&#36890;&#21578;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#26159;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#22312;CBM&#20013;&#65292;&#23545;&#24212;&#20110;&#36755;&#20986;&#21407;&#22240;&#30340;&#27010;&#24565;&#34987;&#25554;&#20837;&#21040;&#26368;&#21518;&#19968;&#20010;&#20013;&#38388;&#23618;&#20316;&#20026;&#35266;&#23519;&#20540;&#12290;&#20154;&#20204;&#39044;&#26399;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#36755;&#20986;&#21644;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#31867;&#20284;&#20110;&#32447;&#24615;&#22238;&#24402;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#37322;&#38656;&#35201;&#35266;&#23519;&#25152;&#26377;&#27010;&#24565;&#65292;&#24182;&#19988;&#38477;&#20302;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#37096;&#20998;CBM&#65288;PCBM&#65289;&#20351;&#29992;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#12290;&#23613;&#31649;&#19968;&#20123;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;PCBM&#30340;&#27867;&#21270;&#24615;&#33021;&#20960;&#20046;&#19982;&#21407;&#22987;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#39640;&#65292;&#20294;&#30001;&#20110;PCBM&#26159;&#22855;&#24322;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#20854;&#27867;&#21270;&#38169;&#35823;&#30340;&#29702;&#35770;&#34892;&#20026;&#23578;&#26410;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20855;&#26377;&#19977;&#23618;&#32447;&#24615;&#26550;&#26500;&#30340;PCBM&#20013;&#30340;&#36125;&#21494;&#26031;&#27867;&#21270;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09206v1 Announce Type: cross  Abstract: Concept Bottleneck Model (CBM) is a methods for explaining neural networks. In CBM, concepts which correspond to reasons of outputs are inserted in the last intermediate layer as observed values. It is expected that we can interpret the relationship between the output and concept similar to linear regression. However, this interpretation requires observing all concepts and decreases the generalization performance of neural networks. Partial CBM (PCBM), which uses partially observed concepts, has been devised to resolve these difficulties. Although some numerical experiments suggest that the generalization performance of PCBMs is almost as high as that of the original neural networks, the theoretical behavior of its generalization error has not been yet clarified since PCBM is singular statistical model. In this paper, we reveal the Bayesian generalization error in PCBM with a three-layered and linear architecture. The result indcates t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;Segment Anything Model (SAM)&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#22312;&#24212;&#29992;&#20110;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#26102;&#38754;&#20020;&#30340;&#36755;&#20837;&#25552;&#31034;&#27169;&#31946;&#24615;&#21644;&#39069;&#22806;&#35757;&#32451;&#38656;&#27714;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.09199</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23450;&#21046;&#21270;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Customizing Segmentation Foundation Model via Prompt Learning for Instance Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09199
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;Segment Anything Model (SAM)&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#22312;&#24212;&#29992;&#20110;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#26102;&#38754;&#20020;&#30340;&#36755;&#20837;&#25552;&#31034;&#27169;&#31946;&#24615;&#21644;&#39069;&#22806;&#35757;&#32451;&#38656;&#27714;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24471;&#21040;&#31215;&#26497;&#25506;&#35752;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;Segment Anything Model (SAM)&#22240;&#20854;&#22312;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#32780;&#33073;&#39062;&#32780;&#20986;&#65292;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#23545;&#35937;&#25513;&#27169;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SAM&#20855;&#26377;&#36825;&#20123;&#20248;&#21183;&#65292;&#22312;&#24212;&#29992;&#20110;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#26102;&#65288;&#23545;&#29305;&#23450;&#23545;&#35937;&#25110;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#36890;&#24120;&#19981;&#23384;&#22312;&#30340;&#29420;&#29305;&#29615;&#22659;&#20013;&#36827;&#34892;&#20998;&#21106;&#65289;&#65292;SAM&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;1&#65289;&#36755;&#20837;&#25552;&#31034;&#20013;&#22266;&#26377;&#30340;&#27169;&#31946;&#24615;&#65292;2&#65289;&#20026;&#23454;&#29616;&#26368;&#20339;&#20998;&#21106;&#38656;&#35201;&#22823;&#37327;&#39069;&#22806;&#35757;&#32451;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#65292;&#38024;&#23545;SAM&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#21547;&#19968;&#20010;&#25552;&#31034;&#23398;&#20064;&#27169;&#22359;&#65288;PLM&#65289;&#65292;&#21487;&#20197;&#35843;&#25972;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09199v1 Announce Type: cross  Abstract: Recently, foundation models trained on massive datasets to adapt to a wide range of domains have attracted considerable attention and are actively being explored within the computer vision community. Among these, the Segment Anything Model (SAM) stands out for its remarkable progress in generalizability and flexibility for image segmentation tasks, achieved through prompt-based object mask generation. However, despite its strength, SAM faces two key limitations when applied to customized instance segmentation that segments specific objects or those in unique environments not typically present in the training data: 1) the ambiguity inherent in input prompts and 2) the necessity for extensive additional training to achieve optimal segmentation. To address these challenges, we propose a novel method, customized instance segmentation via prompt learning tailored to SAM. Our method involves a prompt learning module (PLM), which adjusts inpu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#27604;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#24418;&#29366;&#65292;&#26263;&#31034;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20250;&#21463;&#21040;&#25991;&#26412;&#30340;&#35843;&#33410;</title><link>https://arxiv.org/abs/2403.09193</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#32441;&#29702;&#20559;&#35265;&#36824;&#26159;&#24418;&#29366;&#20559;&#35265;&#65292;&#25105;&#20204;&#21487;&#20197;&#24341;&#23548;&#23427;&#20204;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Vision Language Models Texture or Shape Biased and Can We Steer Them?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#27604;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#24418;&#29366;&#65292;&#26263;&#31034;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20250;&#21463;&#21040;&#25991;&#26412;&#30340;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09193v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#30701;&#30701;&#20960;&#24180;&#20869;&#24443;&#24213;&#25913;&#21464;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#26684;&#23616;&#65292;&#24320;&#21551;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#24212;&#29992;&#65292;&#20174;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#21040;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#65292;&#20877;&#21040;&#35270;&#35273;&#38382;&#31572;&#12290;&#19982;&#32431;&#35270;&#35273;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#36890;&#36807;&#35821;&#35328;&#25552;&#31034;&#35775;&#38382;&#35270;&#35273;&#20869;&#23481;&#30340;&#30452;&#35266;&#26041;&#24335;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#24341;&#21457;&#25105;&#20204;&#24605;&#32771;&#23427;&#20204;&#26159;&#21542;&#20063;&#19982;&#20154;&#31867;&#35270;&#35273;&#19968;&#33268; - &#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#26377;&#22810;&#22823;&#31243;&#24230;&#22320;&#37319;&#29992;&#20102;&#20154;&#31867;&#24341;&#23548;&#30340;&#35270;&#35273;&#20559;&#35265;&#65292;&#25110;&#32773;&#23427;&#20204;&#26159;&#21542;&#21482;&#26159;&#20174;&#32431;&#35270;&#35273;&#27169;&#22411;&#20013;&#32487;&#25215;&#20102;&#20559;&#35265;&#12290;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#35270;&#35273;&#20559;&#35265;&#26159;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21363;&#23616;&#37096;&#20449;&#24687;&#30340;&#20027;&#23548;&#22320;&#20301;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#27969;&#34892;&#30340;VLMs&#20013;&#30340;&#36825;&#31181;&#20559;&#35265;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;VLMs&#36890;&#24120;&#27604;&#23427;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#20110;&#24418;&#29366;&#65292;&#36825;&#34920;&#26126;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#36890;&#36807;&#25991;&#26412;&#36827;&#34892;&#35843;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09193v1 Announce Type: cross  Abstract: Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20256;&#32479;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#21463;&#38480;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#24847;&#22270;&#24863;&#30693;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;IDM&#65289;&#29983;&#25104;&#26410;&#26469;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.09190</link><description>&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#24847;&#22270;&#24863;&#30693;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Intention-aware Denoising Diffusion Model for Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20256;&#32479;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#21463;&#38480;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#24847;&#22270;&#24863;&#30693;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;IDM&#65289;&#29983;&#25104;&#26410;&#26469;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#23588;&#20854;&#23545;&#20110;&#30896;&#25758;&#22238;&#36991;&#31995;&#32479;&#12290;&#32771;&#34385;&#21040;&#20219;&#21153;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#65292;&#35768;&#22810;&#30740;&#31350;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20026;&#27599;&#20010;&#21442;&#19982;&#32773;&#20135;&#29983;&#22810;&#20010;&#21487;&#33021;&#30340;&#26410;&#26469;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#34920;&#31034;&#33021;&#21147;&#21463;&#38480;&#25110;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26410;&#26469;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#23454;&#29616;&#36825;&#19968;&#24819;&#27861;&#38656;&#35299;&#20915;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#24847;&#22270;&#30340;&#22810;&#26679;&#24615;&#19982;&#19981;&#30830;&#23450;&#29615;&#22659;&#30456;&#20114;&#20132;&#32455;&#65292;&#20351;&#24471;&#30495;&#23454;&#20998;&#24067;&#38590;&#20197;&#21442;&#25968;&#21270;&#12290;&#20854;&#27425;&#65292;&#25193;&#25955;&#36807;&#31243;&#22312;&#25512;&#26029;&#38454;&#27573;&#32791;&#26102;&#65292;&#20351;&#24471;&#22312;&#23454;&#26102;&#39550;&#39542;&#31995;&#32479;&#20013;&#23454;&#29616;&#19981;&#20999;&#23454;&#38469;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24847;&#22270;&#24863;&#30693;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;IDM&#65289;&#26469;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09190v1 Announce Type: cross  Abstract: Trajectory prediction is an essential component in autonomous driving, particularly for collision avoidance systems. Considering the inherent uncertainty of the task, numerous studies have utilized generative models to produce multiple plausible future trajectories for each agent. However, most of them suffer from restricted representation ability or unstable training issues. To overcome these limitations, we propose utilizing the diffusion model to generate the distribution of future trajectories. Two cruxes are to be settled to realize such an idea. First, the diversity of intention is intertwined with the uncertain surroundings, making the true distribution hard to parameterize. Second, the diffusion process is time-consuming during the inference phase, rendering it unrealistic to implement in a real-time driving system. We propose an Intention-aware denoising Diffusion Model (IDM), which tackles the above two problems. We decouple 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#23398;&#20064;&#31639;&#27861;&#21644;&#21551;&#21457;&#24335;&#24341;&#23548;&#24212;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#39564;&#35777;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#65292;&#36991;&#20813;&#23545;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#31351;&#23613;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2403.09184</link><description>&lt;p&gt;
&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#39564;&#35777;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Algorithms for Verification of Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#23398;&#20064;&#31639;&#27861;&#21644;&#21551;&#21457;&#24335;&#24341;&#23548;&#24212;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#39564;&#35777;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#65292;&#36991;&#20813;&#23545;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#31351;&#23613;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#23398;&#20064;&#31639;&#27861;&#21644;&#21551;&#21457;&#24335;&#24341;&#23548;&#24212;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#39564;&#35777;&#65292;&#22522;&#20110;Br\'azdil, T.&#31561;&#20154;&#65288;2014&#65289;&#30340;&#24819;&#27861;&#12290;&#35813;&#26694;&#26550;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#36991;&#20813;&#23545;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#31351;&#23613;&#25506;&#32034;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#26159;&#20381;&#38752;&#21551;&#21457;&#24335;&#12290;&#26412;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#25193;&#23637;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#23545;&#22522;&#30784;&#29702;&#35770;&#30340;&#20960;&#20010;&#32454;&#33410;&#36827;&#34892;&#20102;&#25913;&#36827;&#21644;&#38169;&#35823;&#20462;&#27491;&#12290;&#31532;1.3&#33410;&#25552;&#20379;&#20102;&#25152;&#26377;&#24046;&#24322;&#30340;&#27010;&#36848;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#27010;&#29575;&#21487;&#36798;&#24615;&#65292;&#36825;&#26159;&#39564;&#35777;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#24182;&#20855;&#20307;&#21270;&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#31532;&#19968;&#20010;&#20551;&#35774;&#23436;&#20840;&#20102;&#35299;MDP&#65292;&#23588;&#20854;&#26159;&#31934;&#30830;&#30340;&#36716;&#31227;&#27010;&#29575;&#12290;&#23427;&#25191;&#34892;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#27169;&#22411;&#37096;&#20998;&#25506;&#32034;&#65292;&#20135;&#29983;&#31934;&#20934;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09184v1 Announce Type: cross  Abstract: We present a general framework for applying learning algorithms and heuristical guidance to the verification of Markov decision processes (MDPs), based on the ideas of Br\'azdil, T. et al. (2014). Verification of Markov Decision Processes Using Learning Algorithms. The primary goal of the techniques presented in that work is to improve performance by avoiding an exhaustive exploration of the state space, guided by heuristics. This approach is significantly extended in this work. Several details of the base theory are refined and errors are fixed. Section 1.3 provides an overview of all differences.   The presented framework focuses on probabilistic reachability, which is a core problem in verification, and is instantiated in two distinct scenarios. The first assumes that full knowledge of the MDP is available, in particular precise transition probabilities. It performs a heuristic-driven partial exploration of the model, yielding preci
&lt;/p&gt;</description></item><item><title>ADEdgeDrop&#25552;&#20986;&#20102;&#19968;&#31181;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#36793;&#32536;&#39044;&#27979;&#22120;&#25351;&#23548;&#36793;&#32536;&#21024;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09171</link><description>&lt;p&gt;
ADEdgeDrop&#65306;&#29992;&#20110;&#24378;&#20581;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;
&lt;/p&gt;
&lt;p&gt;
ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09171
&lt;/p&gt;
&lt;p&gt;
ADEdgeDrop&#25552;&#20986;&#20102;&#19968;&#31181;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#36793;&#32536;&#39044;&#27979;&#22120;&#25351;&#23548;&#36793;&#32536;&#21024;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#21508;&#31181;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#23637;&#31034;&#20102;&#20174;&#37051;&#36817;&#33410;&#28857;&#25910;&#38598;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#22024;&#26434;&#21644;&#20887;&#20313;&#30340;&#22270;&#25968;&#25454;&#36896;&#25104;&#30340;&#24046;&#30340;&#27867;&#21270;&#21644;&#33030;&#24369;&#30340;&#31283;&#20581;&#24615;&#38480;&#21046;&#20102;GNNs&#30340;&#24615;&#33021;&#12290;&#22312;Graph Augmentation Learning&#65288;GAL&#65289;&#20013;&#65292;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;GNNs&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#21024;&#38500;&#36793;&#32536;&#36890;&#24120;&#20250;&#32469;&#36807;&#20851;&#38190;&#36793;&#32536;&#65292;&#20174;&#32780;&#21066;&#24369;&#28040;&#24687;&#20256;&#36882;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#65288;ADEdgeDrop&#65289;&#65292;&#21033;&#29992;&#25932;&#23545;&#36793;&#32536;&#39044;&#27979;&#22120;&#24341;&#23548;&#36793;&#32536;&#21024;&#38500;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#25972;&#21512;&#21040;&#19981;&#21516;&#30340;GNN&#20027;&#24178;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09171v1 Announce Type: cross  Abstract: Although Graph Neural Networks (GNNs) have exhibited the powerful ability to gather graph-structured information from neighborhood nodes via various message-passing mechanisms, the performance of GNNs is limited by poor generalization and fragile robustness caused by noisy and redundant graph data. As a prominent solution, Graph Augmentation Learning (GAL) has recently received increasing attention. Among prior GAL approaches, edge-dropping methods that randomly remove edges from a graph during training are effective techniques to improve the robustness of GNNs. However, randomly dropping edges often results in bypassing critical edges, consequently weakening the effectiveness of message passing. In this paper, we propose a novel adversarial edge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor guiding the removal of edges, which can be flexibly incorporated into diverse GNN backbones. Employing an adversarial 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#25628;&#32034;&#34892;&#20026;&#27169;&#25311;&#22120; USimAgent&#65292;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#26597;&#35810;&#12289;&#28857;&#20987;&#21644;&#20572;&#27490;&#34892;&#20026;&#65292;&#23454;&#29616;&#29983;&#25104;&#29305;&#23450;&#25628;&#32034;&#30340;&#23436;&#25972;&#25628;&#32034;&#20250;&#35805;&#12290;</title><link>https://arxiv.org/abs/2403.09142</link><description>&lt;p&gt;
USimAgent&#65306;&#29992;&#20110;&#27169;&#25311;&#25628;&#32034;&#29992;&#25143;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
USimAgent: Large Language Models for Simulating Search Users
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09142
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#25628;&#32034;&#34892;&#20026;&#27169;&#25311;&#22120; USimAgent&#65292;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#26597;&#35810;&#12289;&#28857;&#20987;&#21644;&#20572;&#27490;&#34892;&#20026;&#65292;&#23454;&#29616;&#29983;&#25104;&#29305;&#23450;&#25628;&#32034;&#30340;&#23436;&#25972;&#25628;&#32034;&#20250;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25104;&#26412;&#25928;&#30410;&#21644;&#21487;&#37325;&#29616;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#29992;&#25143;&#27169;&#25311;&#24050;&#25104;&#20026;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#29992;&#25143;&#20026;&#20013;&#24515;&#35780;&#20272;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#27169;&#25311;&#29992;&#25143;&#25628;&#32034;&#34892;&#20026;&#19968;&#30452;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#29992;&#25143;&#22312;&#25628;&#32034;&#20013;&#30340;&#34892;&#20026;&#38750;&#24120;&#22797;&#26434;&#65292;&#21463;&#21040;&#23398;&#20064;&#12289;&#25512;&#29702;&#21644;&#35268;&#21010;&#31561;&#38169;&#32508;&#22797;&#26434;&#35748;&#30693;&#36807;&#31243;&#30340;&#39537;&#21160;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#20154;&#31867;&#32423;&#26234;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#24050;&#34987;&#29992;&#20110;&#26500;&#24314;&#21508;&#31181;&#20219;&#21153;&#30340;&#33258;&#20027;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#20351;&#29992;LLM&#27169;&#25311;&#25628;&#32034;&#34892;&#20026;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#29992;&#25143;&#25628;&#32034;&#34892;&#20026;&#27169;&#25311;&#22120;&#65292;&#21363;USimAgent&#12290;&#25552;&#20986;&#30340;&#27169;&#25311;&#22120;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#26597;&#35810;&#12289;&#28857;&#20987;&#21644;&#20572;&#27490;&#34892;&#20026;&#65292;&#22240;&#27492;&#33021;&#22815;&#29983;&#25104;&#29305;&#23450;&#25628;&#32034;&#30340;&#23436;&#25972;&#25628;&#32034;&#20250;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09142v1 Announce Type: cross  Abstract: Due to the advantages in the cost-efficiency and reproducibility, user simulation has become a promising solution to the user-centric evaluation of information retrieval systems. Nonetheless, accurately simulating user search behaviors has long been a challenge, because users' actions in search are highly complex and driven by intricate cognitive processes such as learning, reasoning, and planning. Recently, Large Language Models (LLMs) have demonstrated remarked potential in simulating human-level intelligence and have been used in building autonomous agents for various tasks. However, the potential of using LLMs in simulating search behaviors has not yet been fully explored. In this paper, we introduce a LLM-based user search behavior simulator, USimAgent. The proposed simulator can simulate users' querying, clicking, and stopping behaviors during search, and thus, is capable of generating complete search sessions for specific search
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25903;&#25345;AI&#30340;&#36793;&#32536;&#35774;&#22791;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#25968;&#25454;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#22312;&#29420;&#31435;&#20195;&#29702;&#36935;&#21040;&#30340;&#25968;&#25454;&#38598;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#21464;&#24322;&#24615;&#20013;&#30830;&#23450;&#23398;&#20064;&#32467;&#26524;&#30340;&#32622;&#20449;&#27700;&#24179;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09141</link><description>&lt;p&gt;
&#22312;&#25903;&#25345;AI&#30340;&#36793;&#32536;&#35774;&#22791;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation in Multi-Agent Distributed Learning for AI-Enabled Edge Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09141
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25903;&#25345;AI&#30340;&#36793;&#32536;&#35774;&#22791;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#25968;&#25454;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#22312;&#29420;&#31435;&#20195;&#29702;&#36935;&#21040;&#30340;&#25968;&#25454;&#38598;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#21464;&#24322;&#24615;&#20013;&#30830;&#23450;&#23398;&#20064;&#32467;&#26524;&#30340;&#32622;&#20449;&#27700;&#24179;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#21021;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#26377;&#38480;&#33258;&#20027;&#22788;&#29702;&#33021;&#21147;&#30340;&#20302;&#21151;&#29575;&#21333;&#20803;&#65292;&#36793;&#32536;&#29289;&#32852;&#32593;&#35774;&#22791;&#38543;&#30528;FPGA&#21644;AI&#21152;&#36895;&#22120;&#30340;&#24341;&#20837;&#32780;&#21457;&#29983;&#20102;&#33539;&#24335;&#36716;&#21464;&#12290;&#36825;&#19968;&#36827;&#27493;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#31361;&#26174;&#20102;&#36793;&#32536;AI&#30340;&#23454;&#29992;&#24615;&#12290;&#36825;&#31181;&#36827;&#27493;&#24341;&#20837;&#20102;&#26032;&#25361;&#25112;&#65292;&#21363;&#22914;&#20309;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#33021;&#28304;&#21644;&#32593;&#32476;&#36164;&#28304;&#30340;&#38480;&#21046;&#20248;&#21270;AI&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#25903;&#25345;AI&#30340;&#36793;&#32536;&#35774;&#22791;&#23454;&#29616;&#20998;&#24067;&#24335;&#25968;&#25454;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#21327;&#20316;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#37325;&#28857;&#20043;&#19968;&#26159;&#35299;&#20915;&#30830;&#23450;&#23398;&#20064;&#32467;&#26524;&#30340;&#32622;&#20449;&#27700;&#24179;&#30340;&#25361;&#25112;&#65292;&#32771;&#34385;&#29420;&#31435;&#20195;&#29702;&#36935;&#21040;&#30340;&#25968;&#25454;&#38598;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#21464;&#24322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#31649;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09141v1 Announce Type: cross  Abstract: Initially considered as low-power units with limited autonomous processing, Edge IoT devices have seen a paradigm shift with the introduction of FPGAs and AI accelerators. This advancement has vastly amplified their computational capabilities, emphasizing the practicality of edge AI. Such progress introduces new challenges of optimizing AI tasks for the limitations of energy and network resources typical in Edge computing environments. Our study explores methods that enable distributed data processing through AI-enabled edge devices, enhancing collaborative learning capabilities. A key focus of our research is the challenge of determining confidence levels in learning outcomes, considering the spatial and temporal variability of data sets encountered by independent agents. To address this issue, we investigate the application of Bayesian neural networks, proposing a novel approach to manage uncertainty in distributed learning environme
&lt;/p&gt;</description></item><item><title>ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09131</link><description>&lt;p&gt;
ProSwitch&#65306;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09131
&lt;/p&gt;
&lt;p&gt;
ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25991;&#26412;&#25688;&#35201;&#21644;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#36890;&#36807;&#24494;&#35843;&#22312;&#19981;&#21516;&#39118;&#26684;&#38388;&#20999;&#25442;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#25991;&#26412;&#19987;&#19994;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ProSwitch&#65292;&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#22238;&#22797;&#30340;&#33021;&#21147;&#12290;ProSwitch&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#25968;&#25454;&#20934;&#22791;&#65292;&#29992;&#20110;&#25910;&#38598;&#39046;&#22495;&#30693;&#35782;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#65307;&#25351;&#20196;&#24494;&#35843;&#65292;&#29992;&#20110;&#20248;&#21270;&#24102;&#26377;&#22810;&#31181;&#25351;&#20196;&#26684;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;&#20840;&#38754;&#35780;&#20272;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#19987;&#19994;&#24615;&#21306;&#20998;&#33021;&#21147;&#21644;&#22522;&#20110;&#21442;&#32771;&#30340;&#36136;&#37327;&#12290; ProSwitch&#30456;&#23545;&#20110;&#36890;&#29992;&#21644;&#19987;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09131v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning. ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text. Comparative analysis of ProSwitch against both general and specialized language models reveals that our appro
&lt;/p&gt;</description></item><item><title>AutoLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#65292;&#20197;&#35299;&#20915;LoRA&#20013;&#31209;&#20998;&#37197;&#21644;&#31209;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09113</link><description>&lt;p&gt;
AutoLoRA&#65306;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#33258;&#21160;&#35843;&#25972;&#30697;&#38453;&#31209;&#22312;&#20302;&#31209;&#36866;&#24212;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09113
&lt;/p&gt;
&lt;p&gt;
AutoLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#65292;&#20197;&#35299;&#20915;LoRA&#20013;&#31209;&#20998;&#37197;&#21644;&#31209;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20043;&#21518;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#23384;&#22312;&#30528;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25361;&#25112;&#65292;&#22240;&#27492;&#30740;&#21457;&#20102;&#20960;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#36890;&#36807;&#22312;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#20043;&#19978;&#24494;&#35843;&#20302;&#31209;&#22686;&#37327;&#26356;&#26032;&#30697;&#38453;&#65292;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;LoRA&#22312;&#25152;&#26377;&#23618;&#20013;&#22343;&#21248;&#20998;&#37197;&#31209;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#31351;&#20030;&#25628;&#32034;&#26469;&#25214;&#21040;&#26368;&#20339;&#31209;&#65292;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24494;&#35843;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoLoRA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#12290;AutoLoRA&#23558;&#20302;&#31209;&#26356;&#26032;&#30697;&#38453;&#20013;&#30340;&#27599;&#20010;&#31209;&#20026;1&#30340;&#30697;&#38453;&#19982;&#36873;&#25321;&#21464;&#37327;&#30456;&#20851;&#32852;&#65292;&#35813;&#21464;&#37327;&#20915;&#23450;&#20102;&#31209;&#20026;1&#30340;&#30697;&#38453;&#26159;&#21542;&#24212;&#35813;&#34987;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09113v1 Announce Type: cross  Abstract: Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should b
&lt;/p&gt;</description></item><item><title>MCFEND&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#21333;&#19968;&#26469;&#28304;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#22810;&#28304;&#26032;&#38395;&#25968;&#25454;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.09092</link><description>&lt;p&gt;
MCFEND&#65306;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09092
&lt;/p&gt;
&lt;p&gt;
MCFEND&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#21333;&#19968;&#26469;&#28304;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#22810;&#28304;&#26032;&#38395;&#25968;&#25454;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#26032;&#38395;&#22312;&#21508;&#20010;&#22312;&#32447;&#26469;&#28304;&#30340;&#26222;&#36941;&#20256;&#25773;&#23545;&#20844;&#20247;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#25968;&#25454;&#38598;&#20165;&#38480;&#20110;&#26469;&#33258;&#24494;&#21338;&#30340;&#26032;&#38395;&#12290;&#28982;&#32780;&#65292;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#34394;&#20551;&#26032;&#38395;&#22312;&#20869;&#23481;&#21644;&#31038;&#20250;&#32972;&#26223;&#31561;&#21508;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#22810;&#26679;&#24615;&#12290;&#20165;&#22312;&#21333;&#19968;&#26032;&#38395;&#26469;&#28304;&#19978;&#35757;&#32451;&#30340;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#36866;&#29992;&#20110;&#29616;&#23454;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#23398;&#20064;&#33258;&#19968;&#20010;&#22823;&#22411;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#25968;&#25454;&#38598;Weibo-21&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;F1&#20998;&#25968;&#65292;&#24403;&#27979;&#35797;&#25968;&#25454;&#25913;&#21464;&#20026;&#22810;&#28304;&#26032;&#38395;&#25968;&#25454;&#26102;&#65292;&#20174;0.943&#24613;&#21095;&#19979;&#38477;&#21040;0.470&#65292;&#26410;&#33021;&#35782;&#21035;&#36229;&#36807;&#19977;&#20998;&#20043;&#19968;&#30340;&#22810;&#28304;&#34394;&#20551;&#26032;&#38395;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;MCFEND&#65292;&#30001;&#25105;&#20204;&#20174;&#21508;&#31181;&#26469;&#28304;&#25910;&#38598;&#30340;&#26032;&#38395;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09092v1 Announce Type: cross  Abstract: The prevalence of fake news across various online sources has had a significant influence on the public. Existing Chinese fake news detection datasets are limited to news sourced solely from Weibo. However, fake news originating from multiple sources exhibits diversity in various aspects, including its content and social context. Methods trained on purely one single news source can hardly be applicable to real-world scenarios. Our pilot experiment demonstrates that the F1 score of the state-of-the-art method that learns from a large Chinese fake news detection dataset, Weibo-21, drops significantly from 0.943 to 0.470 when the test data is changed to multi-source news data, failing to identify more than one-third of the multi-source fake news. To address this limitation, we constructed the first multi-source benchmark dataset for Chinese fake news detection, termed MCFEND, which is composed of news we collected from diverse sources suc
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#20010;&#25277;&#35937;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#26377;&#24847;&#20041;&#23398;&#20064;&#33539;&#24335;&#65292;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#36890;&#29992;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09085</link><description>&lt;p&gt;
&#26377;&#24847;&#20041;&#23398;&#20064;&#65306;&#36890;&#36807;&#36890;&#29992;&#20107;&#23454;&#24341;&#23548;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25277;&#35937;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09085
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#20010;&#25277;&#35937;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#26377;&#24847;&#20041;&#23398;&#20064;&#33539;&#24335;&#65292;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#36890;&#29992;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25512;&#29702;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#21644;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#26631;&#24535;&#30528;&#26397;&#30528;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#30001;&#36890;&#29992;&#20107;&#23454;&#25903;&#25345;&#30340;&#31616;&#21333;&#38382;&#39064;&#26102;&#65292;LLMs&#32463;&#24120;&#26410;&#33021;&#25552;&#20379;&#19968;&#33268;&#21644;&#20934;&#30830;&#30340;&#31572;&#26696;&#65292;&#34920;&#26126;&#20854;&#23384;&#22312;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;LLMs&#21040;&#24213;&#26159;&#22312;&#30495;&#27491;&#25512;&#29702;&#36824;&#26159;&#20165;&#20165;&#22312;&#35760;&#24518;&#30340;&#28608;&#28872;&#20105;&#35770;&#12290;&#37492;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21021;&#27493;&#30740;&#31350;&#26469;&#37327;&#21270;&#24182;&#28145;&#20837;&#25506;&#35752;&#29616;&#26377;LLMs&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#26174;&#31034;&#20986;&#23427;&#20204;&#30340;&#19968;&#33324;&#25512;&#29702;&#21644;&#25277;&#35937;&#25512;&#29702;&#34920;&#29616;&#20043;&#38388;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#24322;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#20102;&#19968;&#20010;&#25277;&#35937;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;AbsR&#65289;&#65292;&#32467;&#21512;&#26377;&#24847;&#20041;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#25945;&#20250;LLMs&#22914;&#20309;&#21033;&#29992;&#36890;&#29992;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#30528;&#25913;&#21892;LLMs&#22312;&#25277;&#35937;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09085v1 Announce Type: cross  Abstract: Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities. This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs. Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances. To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes. The results show that our app
&lt;/p&gt;</description></item><item><title>UniCode&#25552;&#20986;&#19968;&#31181;&#23398;&#20064;&#32479;&#19968;&#30721;&#20070;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#35270;&#35273;&#21644;&#25991;&#26412;&#36827;&#34892;&#26631;&#35760;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#21387;&#32553;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09072</link><description>&lt;p&gt;
UniCode: &#23398;&#20064;&#29992;&#20110;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#30721;&#20070;
&lt;/p&gt;
&lt;p&gt;
UniCode: Learning a Unified Codebook for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09072
&lt;/p&gt;
&lt;p&gt;
UniCode&#25552;&#20986;&#19968;&#31181;&#23398;&#20064;&#32479;&#19968;&#30721;&#20070;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#35270;&#35273;&#21644;&#25991;&#26412;&#36827;&#34892;&#26631;&#35760;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UniCode&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23646;&#20110;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#39046;&#22495;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#30721;&#20070;&#26469;&#39640;&#25928;&#22320;&#26631;&#35760;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#28508;&#22312;&#20854;&#20182;&#31867;&#22411;&#30340;&#20449;&#21495;&#12290;&#36825;&#19968;&#21019;&#26032;&#35299;&#20915;&#20102;&#29616;&#26377;MLLMs&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#23616;&#38480;&#65306;&#23427;&#20204;&#20381;&#36182;&#20110;&#20165;&#38480;&#20110;&#25991;&#26412;&#30340;&#30721;&#20070;&#65292;&#36825;&#38480;&#21046;&#20102;MLLM&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#29983;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#35821;&#35328;&#39537;&#21160;&#30340;&#36845;&#20195;&#35757;&#32451;&#33539;&#24335;&#65292;&#32467;&#21512;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#22270;&#20687;&#35299;&#21387;&#32553;&#8221;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#35299;&#37322;&#21387;&#32553;&#30340;&#35270;&#35273;&#25968;&#25454;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#32479;&#19968;&#30340;&#30721;&#20070;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#23558;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25193;&#23637;&#21040;&#38750;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;UniCode&#36866;&#24212;&#20102;&#21508;&#31181;&#21472;&#21152;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#23558;&#35270;&#35273;&#20449;&#21495;&#21387;&#32553;&#20026;&#26356;&#32039;&#20945;&#30340;&#26631;&#35760;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09072v1 Announce Type: cross  Abstract: In this paper, we propose \textbf{UniCode}, a novel approach within the domain of multimodal large language models (MLLMs) that learns a unified codebook to efficiently tokenize visual, text, and potentially other types of signals. This innovation addresses a critical limitation in existing MLLMs: their reliance on a text-only codebook, which restricts MLLM's ability to generate images and texts in a multimodal context. Towards this end, we propose a language-driven iterative training paradigm, coupled with an in-context pre-training task we term ``image decompression'', enabling our model to interpret compressed visual data and generate high-quality images.The unified codebook empowers our model to extend visual instruction tuning to non-linguistic generation tasks. Moreover, UniCode is adaptable to diverse stacked quantization approaches in order to compress visual signals into a more compact token representation. Despite using signi
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#20998;&#24067;&#21644;&#28145;&#24230;&#24863;&#30693;&#20154;&#20307;&#32593;&#26684;&#24674;&#22797;&#65288;D2A-HMR&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#31471;&#21040;&#31471;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#26368;&#23567;&#21270;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#24182;&#25972;&#21512;&#22330;&#26223;&#28145;&#24230;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26126;&#30830;&#21644;&#31283;&#20581;&#30340;&#20154;&#20307;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2403.09063</link><description>&lt;p&gt;
&#20998;&#24067;&#21644;&#28145;&#24230;&#24863;&#30693;&#21464;&#21387;&#22120;&#29992;&#20110;3D&#20154;&#20307;&#32593;&#26684;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09063
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#20998;&#24067;&#21644;&#28145;&#24230;&#24863;&#30693;&#20154;&#20307;&#32593;&#26684;&#24674;&#22797;&#65288;D2A-HMR&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#31471;&#21040;&#31471;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#26368;&#23567;&#21270;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#24182;&#25972;&#21512;&#22330;&#26223;&#28145;&#24230;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26126;&#30830;&#21644;&#31283;&#20581;&#30340;&#20154;&#20307;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#37326;&#22806;&#25968;&#25454;&#31934;&#30830;&#36827;&#34892;&#20154;&#20307;&#32593;&#26684;&#24674;&#22797;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#25361;&#25112;&#65292;&#24448;&#24448;&#21463;&#21040;&#28145;&#24230;&#19981;&#26126;&#30830;&#21644;&#38477;&#20302;&#31934;&#24230;&#30340;&#38459;&#30861;&#12290;&#29616;&#26377;&#20316;&#21697;&#35201;&#20040;&#20511;&#21161;&#23039;&#21183;&#20808;&#39564;&#65292;&#35201;&#20040;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#22914;&#22810;&#35270;&#22270;&#25110;&#28857;&#20113;&#20449;&#24687;&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#26041;&#27861;&#32463;&#24120;&#24573;&#35270;&#21333;&#20010;&#22270;&#20687;&#20013;&#22825;&#28982;&#23384;&#22312;&#30340;&#23453;&#36149;&#22330;&#26223;&#28145;&#24230;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23039;&#21183;&#12289;&#24418;&#29366;&#21644;&#28145;&#24230;&#30340;&#22266;&#26377;&#21464;&#21270;&#65292;&#38024;&#23545;&#20998;&#24067;&#22806;&#25968;&#25454;&#65288;OOD&#65289;&#23454;&#29616;&#31283;&#20581;&#30340;HMR&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#20154;&#20307;&#24418;&#24577;&#24314;&#27169;&#30340;&#28508;&#22312;&#20998;&#24067;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#23376;&#38382;&#39064;&#12290;&#20986;&#20110;&#23545;&#26126;&#30830;&#21644;&#31283;&#20581;&#20154;&#20307;&#24314;&#27169;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#24067;&#21644;&#28145;&#24230;&#24863;&#30693;&#20154;&#20307;&#32593;&#26684;&#24674;&#22797;&#65288;D2A-HMR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#24182;&#25972;&#21512;&#22330;&#26223;&#28145;&#24230;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#28145;&#24230;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09063v1 Announce Type: cross  Abstract: Precise Human Mesh Recovery (HMR) with in-the-wild data is a formidable challenge and is often hindered by depth ambiguities and reduced precision. Existing works resort to either pose priors or multi-modal data such as multi-view or point cloud information, though their methods often overlook the valuable scene-depth information inherently present in a single image. Moreover, achieving robust HMR for out-of-distribution (OOD) data is exceedingly challenging due to inherent variations in pose, shape and depth. Consequently, understanding the underlying distribution becomes a vital subproblem in modeling human forms. Motivated by the need for unambiguous and robust human modeling, we introduce Distribution and depth-aware human mesh recovery (D2A-HMR), an end-to-end transformer architecture meticulously designed to minimize the disparity between distributions and incorporate scene-depth leveraging prior depth information. Our approach d
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#35760;&#24405;&#29983;&#25104;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;&#65292;&#22312;PubMedQA&#26041;&#38754;&#24615;&#33021;&#20248;&#20110;GPT-4&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36807;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;</title><link>https://arxiv.org/abs/2403.09057</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Continued Pretrained LLM Approach for Automatic Medical Note Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09057
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#35760;&#24405;&#29983;&#25104;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;&#65292;&#22312;PubMedQA&#26041;&#38754;&#24615;&#33021;&#20248;&#20110;GPT-4&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36807;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#27491;&#22312;&#38761;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#26368;&#24378;&#22823;&#30340;LLM&#23545;&#20110;&#22823;&#22810;&#25968;&#39046;&#22495;&#29305;&#23450;&#22330;&#26223;&#26469;&#35828;&#25104;&#26412;&#22826;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36830;&#32493;&#35757;&#32451;&#30340;130&#20159;&#21442;&#25968; Llama2-basd LLM&#65292;&#19987;&#20026;&#21307;&#30103;&#23545;&#35805;&#32780;&#35774;&#35745;&#65292;&#24182;&#22312;&#33258;&#21160;&#35760;&#24405;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;PubMedQA&#20013;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;76.6&#65285;&#65292;&#22312;&#24635;&#32467;&#21307;&#30103;&#23545;&#35805;&#20026;SOAP&#31508;&#35760;&#26041;&#38754;&#19982;GPT-4&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#26041;&#38754;&#36229;&#36807;&#20102;GPT-4&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09057v1 Announce Type: cross  Abstract: LLMs are revolutionizing NLP tasks. However, the most powerful LLM, like GPT-4, is too costly for most domain-specific scenarios. We present the first continuously trained 13B Llama2-based LLM that is purpose-built for medical conversations and measured on automated scribing. Our results show that our model outperforms GPT-4 in PubMedQA with 76.6\% accuracy and matches its performance in summarizing medical conversations into SOAP notes. Notably, our model exceeds GPT-4 in capturing a higher number of correct medical concepts and outperforms human scribes with higher correctness and completeness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Keyformer&#8221;&#30340;&#21019;&#26032;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#20851;&#38190;&#26631;&#35760;&#26469;&#20943;&#23569;KV&#32531;&#23384;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09054</link><description>&lt;p&gt;
Keyformer&#65306;&#36890;&#36807;&#20851;&#38190;&#26631;&#35760;&#36873;&#25321;&#20943;&#23569;KV&#32531;&#23384;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#29983;&#25104;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Keyformer&#8221;&#30340;&#21019;&#26032;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#20851;&#38190;&#26631;&#35760;&#26469;&#20943;&#23569;KV&#32531;&#23384;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#22522;&#30784;&#26550;&#26500;&#12290;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25512;&#26029;&#36807;&#31243;&#28041;&#21450;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;&#25552;&#31034;&#22788;&#29702;&#21644;&#26631;&#35760;&#29983;&#25104;&#12290;&#26631;&#35760;&#29983;&#25104;&#65292;&#26500;&#25104;&#20102;&#22823;&#37096;&#20998;&#35745;&#31639;&#24037;&#20316;&#37327;&#65292;&#20027;&#35201;&#28041;&#21450;&#21521;&#37327;-&#30697;&#38453;&#20056;&#27861;&#21644;&#19982;&#38190;-&#20540;(KV)&#32531;&#23384;&#20132;&#20114;&#12290;&#30001;&#20110;&#20174;&#23384;&#20648;&#31995;&#32479;&#20256;&#36755;&#26435;&#37325;&#21644;KV&#32531;&#23384;&#20540;&#21040;&#35745;&#31639;&#21333;&#20803;&#30340;&#24320;&#38144;&#65292;&#36825;&#19968;&#38454;&#27573;&#21463;&#21040;&#20869;&#23384;&#24102;&#23485;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#20869;&#23384;&#29942;&#39048;&#22312;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#21644;&#22823;&#37327;&#25991;&#26412;&#29983;&#25104;&#30340;&#24212;&#29992;&#20013;&#23588;&#20026;&#31361;&#20986;&#65292;&#36825;&#20004;&#32773;&#23545;LLMs&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;  &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#8220;Keyformer&#8221;&#65292;&#20197;&#32531;&#35299;&#19982;KV&#32531;&#23384;&#22823;&#23567;&#21644;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;Keyformer&#21033;&#29992;&#20102;&#36825;&#26679;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#22823;&#32422;90
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09054v1 Announce Type: cross  Abstract: Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   This paper introduces "Keyformer", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27169;&#22411;&#33976;&#39311;&#30340;&#19968;&#33324;&#29702;&#35770;&#65292;&#36890;&#36807;PAC-&#33976;&#39311;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#25277;&#21462;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26435;&#37325;&#30693;&#35782;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#33976;&#39311;&#27604;&#20174;&#22836;&#23398;&#20064;&#26356;&#20415;&#23452;&#19988;&#26377;&#21161;&#20110;&#29702;&#35299;&#20854;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09053</link><description>&lt;p&gt;
&#36208;&#21521;&#27169;&#22411;&#33976;&#39311;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Towards a theory of model distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09053
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22411;&#33976;&#39311;&#30340;&#19968;&#33324;&#29702;&#35770;&#65292;&#36890;&#36807;PAC-&#33976;&#39311;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#25277;&#21462;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26435;&#37325;&#30693;&#35782;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#33976;&#39311;&#27604;&#20174;&#22836;&#23398;&#20064;&#26356;&#20415;&#23452;&#19988;&#26377;&#21161;&#20110;&#29702;&#35299;&#20854;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33976;&#39311;&#26159;&#23558;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26367;&#25442;&#20026;&#31616;&#21270;&#27169;&#22411;&#26469;&#36817;&#20284;&#21407;&#27169;&#22411;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#20851;&#20110;&#27169;&#22411;&#33976;&#39311;&#30340;&#31243;&#24230;&#12289;&#25152;&#38656;&#36816;&#34892;&#26102;&#38388;&#21644;&#25968;&#25454;&#37327;&#30340;&#22522;&#26412;&#38382;&#39064;&#20173;&#28982;&#22823;&#22810;&#26410;&#35299;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#22987;&#20102;&#33976;&#39311;&#30340;&#19968;&#33324;&#29702;&#35770;&#65292;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;PAC-&#33976;&#39311; [Val84]&#65292;&#25552;&#20986;&#20102;&#25552;&#21462;&#35757;&#32451;&#26435;&#37325;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#30340;&#26032;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#8220;&#32447;&#24615;&#34920;&#31034;&#20551;&#35774;&#8221;&#23558;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#22320;&#33976;&#39311;&#25104;&#31616;&#26126;&#26126;&#20102;&#30340;&#20915;&#31574;&#26641;&#34920;&#31034;&#65292;&#36824;&#35777;&#26126;&#20102;&#33976;&#39311;&#21487;&#20197;&#27604;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#20415;&#23452;&#24471;&#22810;&#65292;&#24182;&#22312;&#34920;&#24449;&#20854;&#22797;&#26434;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09053v1 Announce Type: cross  Abstract: Distillation is the task of replacing a complicated machine learning model with a simpler model that approximates the original [BCNM06,HVD15]. Despite many practical applications, basic questions about the extent to which models can be distilled, and the runtime and amount of data needed to distill, remain largely open.   To study these questions, we initiate a general theory of distillation, defining PAC-distillation in an analogous way to PAC-learning [Val84]. As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently distill neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that distillation can be much cheaper than learning from scratch, and make progress on characterizing its complexity.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;STRIPE&#65289;&#29992;&#20110;&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38376;&#25511;&#26102;&#38388;&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.09039</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09039
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;STRIPE&#65289;&#29992;&#20110;&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38376;&#25511;&#26102;&#38388;&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38754;&#20020;&#36739;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#26102;&#38388;&#28436;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;STRIPE&#65289;&#12290;STRIPE&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#38376;&#25511;&#26102;&#38388;&#21367;&#31215;&#23618;&#20998;&#21035;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09039v1 Announce Type: cross  Abstract: Anomaly detection in dynamic graphs presents a significant challenge due to the temporal evolution of graph structures and attributes. The conventional approaches that tackle this problem typically employ an unsupervised learning framework, capturing normality patterns with exclusive normal data during training and identifying deviations as anomalies during testing. However, these methods face critical drawbacks: they either only depend on proxy tasks for general representation without directly pinpointing normal patterns, or they neglect to differentiate between spatial and temporal normality patterns, leading to diminished efficacy in anomaly detection. To address these challenges, we introduce a novel Spatial-Temporal memories-enhanced graph autoencoder (STRIPE). Initially, STRIPE employs Graph Neural Networks (GNNs) and gated temporal convolution layers to extract spatial features and temporal features, respectively. Then STRIPE in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102; WebSight &#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545; VLM &#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#23558;&#32593;&#39029;&#25130;&#22270;&#36716;&#25442;&#20026;&#21151;&#33021;&#24615; HTML &#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#20026;&#35299;&#20915;&#23631;&#24149;&#25130;&#22270;&#36716;&#25442;&#20026; HTML &#20195;&#30721;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.09029</link><description>&lt;p&gt;
&#20351;&#29992; WebSight &#25968;&#25454;&#38598;&#23454;&#29616;&#23558; Web &#23631;&#24149;&#25130;&#22270;&#36716;&#25442;&#20026; HTML &#20195;&#30721;&#30340;&#35299;&#38145;
&lt;/p&gt;
&lt;p&gt;
Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102; WebSight &#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545; VLM &#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#23558;&#32593;&#39029;&#25130;&#22270;&#36716;&#25442;&#20026;&#21151;&#33021;&#24615; HTML &#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#20026;&#35299;&#20915;&#23631;&#24149;&#25130;&#22270;&#36716;&#25442;&#20026; HTML &#20195;&#30721;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312; web &#24320;&#21457;&#20013;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#24182;&#35299;&#38145;&#26080;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#25552;&#20379; UI &#30340;&#23631;&#24149;&#25130;&#22270;&#25110;&#33609;&#22270;&#65292;VLM &#21487;&#20197;&#29983;&#25104;&#22797;&#21046;&#23427;&#30340;&#20195;&#30721;&#65292;&#20363;&#22914;&#20197; HTML &#35821;&#35328;&#12290;&#23613;&#31649; VLMs &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23558;&#23631;&#24149;&#25130;&#22270;&#36716;&#25442;&#20026;&#30456;&#24212;&#30340; HTML &#30340;&#20855;&#20307;&#25361;&#25112;&#20960;&#20046;&#26410;&#32463;&#25506;&#32034;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#19968;&#20010;&#21512;&#36866;&#30340;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; WebSight&#65292;&#19968;&#20010;&#30001; 200 &#19975;&#23545; HTML &#20195;&#30721;&#21644;&#23427;&#20204;&#23545;&#24212;&#30340;&#23631;&#24149;&#25130;&#22270;&#32452;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#22522;&#30784; VLM &#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#32593;&#39029;&#25130;&#22270;&#36716;&#25442;&#20026;&#21151;&#33021;&#24615; HTML &#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102; WebSight.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09029v1 Announce Type: cross  Abstract: Using vision-language models (VLMs) in web development presents a promising strategy to increase efficiency and unblock no-code solutions: by providing a screenshot or a sketch of a UI, a VLM could generate the code to reproduce it, for instance in a language like HTML. Despite the advancements in VLMs for various tasks, the specific challenge of converting a screenshot into a corresponding HTML has been minimally explored. We posit that this is mainly due to the absence of a suitable, high-quality dataset. This work introduces WebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and their corresponding screenshots. We fine-tune a foundational VLM on our dataset and show proficiency in converting webpage screenshots to functional HTML code. To accelerate the research in this area, we open-source WebSight.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#21442;&#25968;&#20196;&#29260;&#24207;&#21015;&#20849;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#20256;&#32479;&#30340;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#25439;&#22833;&#21644;&#19979;&#19968;&#20010;&#24207;&#21015;&#39044;&#27979;&#25439;&#22833;&#26469;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09024</link><description>&lt;p&gt;
&#21322;&#21442;&#25968;&#20196;&#29260;&#24207;&#21015;&#20849;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
Semiparametric Token-Sequence Co-Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09024
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#21442;&#25968;&#20196;&#29260;&#24207;&#21015;&#20849;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#20256;&#32479;&#30340;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#25439;&#22833;&#21644;&#19979;&#19968;&#20010;&#24207;&#21015;&#39044;&#27979;&#25439;&#22833;&#26469;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#21442;&#25968;&#20196;&#29260;&#24207;&#21015;&#20849;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#25968;&#21270;&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#35745;&#31639;&#30340;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#25439;&#22833;&#21644;&#22522;&#20110;&#38750;&#21442;&#25968;&#21270;&#24207;&#21015;&#23884;&#20837;&#31354;&#38388;&#35745;&#31639;&#30340;&#19979;&#19968;&#20010;&#24207;&#21015;&#39044;&#27979;&#25439;&#22833;&#26469;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#38750;&#21442;&#25968;&#24207;&#21015;&#23884;&#20837;&#31354;&#38388;&#26159;&#30001;&#19968;&#20010;&#21333;&#29420;&#30340;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#65292;&#20854;&#20219;&#21153;&#26159;&#23558;&#36755;&#20837;&#25991;&#26412;&#21387;&#32553;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#20195;&#34920;&#24615;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#36825;&#20004;&#31181;&#30417;&#30563;&#35757;&#32451;&#30340;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#21333;&#29420;&#36890;&#36807;&#27599;&#31181;&#30417;&#30563;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#20849;&#30417;&#30563;&#40723;&#21169;&#27169;&#22411;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#39044;&#35757;&#32451;&#27493;&#39588;&#20013;&#24314;&#31435;&#30340;&#21442;&#25968;&#21270;&#26631;&#35760;&#31354;&#38388;&#30340;&#40065;&#26834;&#24615;&#20542;&#21521;&#20110;&#26377;&#25928;&#22686;&#24378;&#38750;&#21442;&#25968;&#21270;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09024v1 Announce Type: cross  Abstract: In this work, we introduce a semiparametric token-sequence co-supervision training method. It trains a language model by simultaneously leveraging supervision from the traditional next token prediction loss which is calculated over the parametric token embedding space and the next sequence prediction loss which is calculated over the nonparametric sequence embedding space. The nonparametric sequence embedding space is constructed by a separate language model tasked to condense an input text into a single representative embedding. Our experiments demonstrate that a model trained via both supervisions consistently surpasses models trained via each supervision independently. Analysis suggests that this co-supervision encourages a broader generalization capability across the model. Especially, the robustness of parametric token space which is established during the pretraining step tends to effectively enhance the stability of nonparametri
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#65292;&#25903;&#25345;&#33258;&#20027;&#36718;&#26885;&#21644;&#39134;&#34892;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#23433;&#20840;&#36807;&#39532;&#36335;&#20915;&#31574;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#22810;&#20256;&#24863;&#22120;&#21487;&#20197;&#25552;&#39640;&#20915;&#31574;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#25903;&#25345;&#23433;&#20840;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.08984</link><description>&lt;p&gt;
&#33258;&#20027;&#36718;&#26885;&#30340;&#23433;&#20840;&#36807;&#39532;&#36335;&#65306;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#21450;&#20854;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Safe Road-Crossing by Autonomous Wheelchairs: a Novel Dataset and its Experimental Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08984
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#65292;&#25903;&#25345;&#33258;&#20027;&#36718;&#26885;&#21644;&#39134;&#34892;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#23433;&#20840;&#36807;&#39532;&#36335;&#20915;&#31574;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#22810;&#20256;&#24863;&#22120;&#21487;&#20197;&#25552;&#39640;&#20915;&#31574;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#25903;&#25345;&#23433;&#20840;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#22320;&#36807;&#39532;&#36335;&#26159;&#26234;&#33021;&#22478;&#24066;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#33258;&#20027;&#36718;&#26885;&#21644;&#37197;&#22791;&#22810;&#26679;&#21270;&#21644;&#20887;&#20313;&#32452;&#20214;&#30340;&#39134;&#34892;&#26080;&#20154;&#26426;&#31995;&#32479;&#20013;&#30340;&#36807;&#39532;&#36335;&#20915;&#31574;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#29289;&#29702;&#26465;&#20214;&#30340;&#20998;&#26512;&#21361;&#38505;&#20989;&#25968;&#65292;&#36890;&#36807;&#21333;&#19968;&#20256;&#24863;&#22120;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#35270;&#35273;&#30340;&#20256;&#24863;&#22120;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#22810;&#20256;&#24863;&#22120;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#25552;&#39640;&#20915;&#31574;&#20934;&#30830;&#24615;&#65292;&#26377;&#25928;&#25903;&#25345;&#23433;&#20840;&#35780;&#20272;&#12290;&#25105;&#20204;&#21521;&#31185;&#23398;&#30028;&#25552;&#20379;&#20102;&#35813;&#25968;&#25454;&#38598;&#20197;&#20379;&#36827;&#19968;&#27493;&#35797;&#39564;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#22312;&#19968;&#20010;&#21517;&#20026;REXASI-PRO&#30340;&#27431;&#27954;&#39033;&#30446;&#30340;&#26694;&#26550;&#19979;&#24320;&#21457;&#30340;&#65292;&#26088;&#22312;&#24320;&#21457;&#20540;&#24471;&#20449;&#36182;&#30340;&#33402;&#26415;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08984v1 Announce Type: cross  Abstract: Safe road-crossing by self-driving vehicles is a crucial problem to address in smart-cities. In this paper, we introduce a multi-sensor fusion approach to support road-crossing decisions in a system composed by an autonomous wheelchair and a flying drone featuring a robust sensory system made of diverse and redundant components. To that aim, we designed an analytical danger function based on explainable physical conditions evaluated by single sensors, including those using machine learning and artificial vision. As a proof-of-concept, we provide an experimental evaluation in a laboratory environment, showing the advantages of using multiple sensors, which can improve decision accuracy and effectively support safety assessment. We made the dataset available to the scientific community for further experimentation. The work has been developed in the context of an European project named REXASI-PRO, which aims to develop trustworthy artific
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#21435;&#22122;&#25193;&#25955;&#26469;&#20934;&#30830;&#25429;&#25417;&#35299;&#21078;&#26641;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.08974</link><description>&lt;p&gt;
&#29992;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#31070;&#32463;&#22330;&#34920;&#31034;&#35299;&#21078;&#26641;
&lt;/p&gt;
&lt;p&gt;
Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08974
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#21435;&#22122;&#25193;&#25955;&#26469;&#20934;&#30830;&#25429;&#25417;&#35299;&#21078;&#26641;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#21078;&#26641;&#22312;&#20020;&#24202;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35299;&#21078;&#26641;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#20960;&#20309;&#24418;&#29366;&#22810;&#26679;&#19988;&#22797;&#26434;&#65292;&#20934;&#30830;&#34920;&#31034;&#35299;&#21078;&#26641;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#26469;&#34920;&#31034;&#35299;&#21078;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;INR&#31354;&#38388;&#20013;&#36827;&#34892;&#21435;&#22122;&#25193;&#25955;&#26469;&#25429;&#25417;&#19968;&#32452;&#26641;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#21487;&#20197;&#22312;&#20219;&#20309;&#25152;&#38656;&#20998;&#36776;&#29575;&#19979;&#20934;&#30830;&#25429;&#25417;&#35299;&#21078;&#26641;&#30340;&#22797;&#26434;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08974v1 Announce Type: cross  Abstract: Anatomical trees play a central role in clinical diagnosis and treatment planning. However, accurately representing anatomical trees is challenging due to their varying and complex topology and geometry. Traditional methods for representing tree structures, captured using medical imaging, while invaluable for visualizing vascular and bronchial networks, exhibit drawbacks in terms of limited resolution, flexibility, and efficiency. Recently, implicit neural representations (INRs) have emerged as a powerful tool for representing shapes accurately and efficiently. We propose a novel approach for representing anatomical trees using INR, while also capturing the distribution of a set of trees via denoising diffusion in the space of INRs. We accurately capture the intricate geometries and topologies of anatomical trees at any desired resolution. Through extensive qualitative and quantitative evaluation, we demonstrate high-fidelity tree reco
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181; PathM3 &#26694;&#26550;&#65292;&#29992;&#20110;WSI&#20998;&#31867;&#21644;&#23383;&#24149;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#24335;transformer&#26377;&#25928;&#23545;&#40784;WSIs&#19982;&#35786;&#26029;&#24615;&#23383;&#24149;&#12290;</title><link>https://arxiv.org/abs/2403.08967</link><description>&lt;p&gt;
PathM3: &#19968;&#31181;&#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#21644;&#23383;&#24149;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#23454;&#20363;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PathM3: A Multimodal Multi-Task Multiple Instance Learning Framework for Whole Slide Image Classification and Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08967
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181; PathM3 &#26694;&#26550;&#65292;&#29992;&#20110;WSI&#20998;&#31867;&#21644;&#23383;&#24149;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#24335;transformer&#26377;&#25928;&#23545;&#40784;WSIs&#19982;&#35786;&#26029;&#24615;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#32452;&#32455;&#30149;&#29702;&#23398;&#39046;&#22495;&#65292;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#21644;&#35786;&#26029;&#24615;&#23383;&#24149;&#37117;&#20026;&#20570;&#20986;&#35786;&#26029;&#20915;&#31574;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#23558;WSIs&#19982;&#35786;&#26029;&#24615;&#23383;&#24149;&#23545;&#40784;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#33258;&#20004;&#20010;&#22240;&#32032;&#65306;1&#65289;&#24040;&#22411;&#20687;&#32032;WSIs&#19981;&#36866;&#21512;&#30452;&#25509;&#36755;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#22270;&#22359;&#38388;&#30340;&#20887;&#20313;&#24615;&#21644;&#30456;&#20851;&#24615;&#35201;&#27714;&#26356;&#22810;&#27880;&#24847;&#65307;2&#65289;&#30495;&#23454;&#30340;WSI&#35786;&#26029;&#24615;&#23383;&#24149;&#26497;&#20854;&#26377;&#38480;&#65292;&#20351;&#24471;&#38590;&#20197;&#35757;&#32451;&#20986;&#26377;&#25928;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08967v1 Announce Type: cross  Abstract: In the field of computational histopathology, both whole slide images (WSIs) and diagnostic captions provide valuable insights for making diagnostic decisions. However, aligning WSIs with diagnostic captions presents a significant challenge. This difficulty arises from two main factors: 1) Gigapixel WSIs are unsuitable for direct input into deep learning models, and the redundancy and correlation among the patches demand more attention; and 2) Authentic WSI diagnostic captions are extremely limited, making it difficult to train an effective model. To overcome these obstacles, we present PathM3, a multimodal, multi-task, multiple instance learning (MIL) framework for WSI classification and captioning. PathM3 adapts a query-based transformer to effectively align WSIs with diagnostic captions. Given that histopathology visual patterns are redundantly distributed across WSIs, we aggregate each patch feature with MIL method that considers t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;D-CNN&#32593;&#32476;&#36827;&#34892;&#29482;&#31867;&#24418;&#24577;&#20998;&#31867;&#65292;&#37325;&#28857;&#20851;&#27880;&#21355;&#29983;&#30417;&#27979;&#29305;&#24449;&#65292;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08962</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#29482;&#31867;&#24418;&#24577;&#20998;&#31867;&#65292;&#37325;&#28857;&#20851;&#27880;&#21355;&#29983;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Using Deep Learning for Morphological Classification in Pigs with a Focus on Sanitary Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;D-CNN&#32593;&#32476;&#36827;&#34892;&#29482;&#31867;&#24418;&#24577;&#20998;&#31867;&#65292;&#37325;&#28857;&#20851;&#27880;&#21355;&#29983;&#30417;&#27979;&#29305;&#24449;&#65292;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;D-CNN&#65289;&#31639;&#27861;&#26469;&#23545;&#29482;&#30340;&#36523;&#20307;&#26465;&#20214;&#36827;&#34892;&#27491;&#24120;&#25110;&#24322;&#24120;&#20998;&#31867;&#65292;&#37325;&#28857;&#20851;&#27880;&#21355;&#29983;&#30417;&#27979;&#20013;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#26469;&#23436;&#25104;&#27492;&#20219;&#21153;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;&#20116;&#20010;&#29482;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#23614;&#24052;&#21827;&#39135;&#12289;&#32819;&#37096;&#34880;&#32959;&#12289;&#36523;&#20307;&#25235;&#30165;&#12289;&#32418;&#26001;&#21644;&#33258;&#28982;&#27745;&#28173;&#65288;&#26837;&#33394;&#25110;&#40657;&#33394;&#65289;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;D-CNN&#22312;&#23545;&#19982;&#30382;&#32932;&#29305;&#24449;&#30456;&#20851;&#30340;&#29482;&#20307;&#24418;&#24577;&#20559;&#24046;&#36827;&#34892;&#20998;&#31867;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#35780;&#20272;&#36890;&#36807;&#20998;&#26512;&#20934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F&#20998;&#25968;&#31561;&#24615;&#33021;&#25351;&#26631;&#20197;&#21450;&#32479;&#35745;&#20998;&#26512;ANOVA&#21644;Scott-Knott&#27979;&#35797;&#23436;&#25104;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20351;&#29992;D-CNN&#32593;&#32476;&#36827;&#34892;&#29482;&#31867;&#24418;&#24577;&#20998;&#31867;&#65292;&#37325;&#28857;&#20851;&#27880;&#21355;&#29983;&#30417;&#27979;&#20013;&#35782;&#21035;&#21040;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08962v1 Announce Type: cross  Abstract: The aim of this paper is to evaluate the use of D-CNN (Deep Convolutional Neural Networks) algorithms to classify pig body conditions in normal or not normal conditions, with a focus on characteristics that are observed in sanitary monitoring, and were used six different algorithms to do this task. The study focused on five pig characteristics, being these caudophagy, ear hematoma, scratches on the body, redness, and natural stains (brown or black). The results of the study showed that D-CNN was effective in classifying deviations in pig body morphologies related to skin characteristics. The evaluation was conducted by analyzing the performance metrics Precision, Recall, and F-score, as well as the statistical analyses ANOVA and the Scott-Knott test. The contribution of this article is characterized by the proposal of using D-CNN networks for morphological classification in pigs, with a focus on characteristics identified in sanitary m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36816;&#29992;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21078;&#26512;&#32701;&#27611;&#29699;&#27604;&#36187;&#35270;&#39057;&#65292;&#25552;&#21462;&#29699;&#21592;&#21160;&#21147;&#23398;&#21644;&#29983;&#29289;&#21147;&#23398;&#35265;&#35299;&#65292;&#20197;&#25512;&#23548;&#25913;&#36827;&#25216;&#26415;&#30340;&#39044;&#27979;&#27169;&#22411;</title><link>https://arxiv.org/abs/2403.08956</link><description>&lt;p&gt;
&#32701;&#27611;&#29699;AI&#25945;&#32451;
&lt;/p&gt;
&lt;p&gt;
AI coach for badminton
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08956
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36816;&#29992;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21078;&#26512;&#32701;&#27611;&#29699;&#27604;&#36187;&#35270;&#39057;&#65292;&#25552;&#21462;&#29699;&#21592;&#21160;&#21147;&#23398;&#21644;&#29983;&#29289;&#21147;&#23398;&#35265;&#35299;&#65292;&#20197;&#25512;&#23548;&#25913;&#36827;&#25216;&#26415;&#30340;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31454;&#25216;&#20307;&#32946;&#39046;&#22495;&#65292;&#26368;&#20339;&#34920;&#29616;&#38656;&#35201;&#23545;&#33829;&#20859;&#21644;&#36523;&#20307;&#29366;&#20917;&#36827;&#34892;&#20005;&#26684;&#31649;&#29702;&#12290;&#22312;&#32701;&#27611;&#29699;&#36816;&#21160;&#20013;&#65292;&#25152;&#38656;&#30340;&#25935;&#25463;&#24615;&#21644;&#31934;&#20934;&#24615;&#20351;&#20854;&#25104;&#20026;&#36890;&#36807;&#35270;&#39057;&#20998;&#26512;&#36827;&#34892;&#36816;&#21160;&#20998;&#26512;&#30340;&#29702;&#24819;&#20505;&#36873;&#32773;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#21078;&#26512;&#32701;&#27611;&#29699;&#27604;&#36187;&#30340;&#35270;&#39057;&#38236;&#22836;&#65292;&#26088;&#22312;&#25552;&#21462;&#26377;&#20851;&#29699;&#21592;&#21160;&#21147;&#23398;&#21644;&#29983;&#29289;&#21147;&#23398;&#30340;&#35814;&#32454;&#35265;&#35299;&#12290;&#36890;&#36807;&#23545;&#20987;&#29699;&#21147;&#23398;&#30340;&#20998;&#26512;&#65292;&#21253;&#25324;&#25163;&#33218;-&#33216;&#37096;&#21327;&#35843;&#65292;&#33151;&#37096;&#23450;&#20301;&#21644;&#20987;&#29699;&#30340;&#25191;&#34892;&#35282;&#24230;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#25512;&#23548;&#20986;&#21487;&#20197;&#24314;&#35758;&#25913;&#36827;&#31449;&#23039;&#12289;&#25216;&#26415;&#21644;&#32908;&#32905;&#26041;&#21521;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20123;&#24314;&#35758;&#26088;&#22312;&#20943;&#23569;&#38169;&#35823;&#25216;&#26415;&#65292;&#38477;&#20302;&#20851;&#33410;&#30130;&#21171;&#39118;&#38505;&#65292;&#24182;&#22686;&#24378;&#25972;&#20307;&#34920;&#29616;&#12290;&#21033;&#29992;&#22312;&#32447;&#21487;&#33719;&#24471;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#35813;&#30740;&#31350;&#23558;&#29699;&#21592;&#30340;&#36523;&#20307;&#23646;&#24615;&#19982;&#20854;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08956v1 Announce Type: cross  Abstract: In the competitive realm of sports, optimal performance necessitates rigorous management of nutrition and physical conditioning. Specifically, in badminton, the agility and precision required make it an ideal candidate for motion analysis through video analytics. This study leverages advanced neural network methodologies to dissect video footage of badminton matches, aiming to extract detailed insights into player kinetics and biomechanics. Through the analysis of stroke mechanics, including hand-hip coordination, leg positioning, and the execution angles of strokes, the research aims to derive predictive models that can suggest improvements in stance, technique, and muscle orientation. These recommendations are designed to mitigate erroneous techniques, reduce the risk of joint fatigue, and enhance overall performance. Utilizing a vast array of data available online, this research correlates players' physical attributes with their in-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#36798;&#21040;&#36739;&#20302;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.08955</link><description>&lt;p&gt;
&#26397;&#21521;&#39640;&#25928;&#30340;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#65306;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#36798;&#21040;&#36739;&#20302;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#24471;&#33258;&#20027;&#26234;&#33021;&#20307;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#36845;&#20195;&#22797;&#26434;&#24230;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#24179;&#34913;&#20102;&#26399;&#26395;&#22238;&#25253;&#21644;&#39118;&#38505;&#65292;&#20855;&#26377;&#20135;&#29983;&#27010;&#29575;&#40065;&#26834;&#31574;&#30053;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;REINFORCE&#31639;&#27861;&#24182;&#37319;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;$\mathcal{O}(\epsilon^{-2})$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20197;&#36798;&#21040;$\epsilon$-&#36817;&#20284;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65288;FOSP&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#27604;&#39118;&#38505;&#20013;&#24615;&#31639;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08955v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration complexity and robustness. Risk-sensitive RL, which balances expected return and risk, has been explored for its potential to yield probabilistically robust policies, yet its iteration complexity analysis remains underexplored. In this study, we conduct a thorough iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm and employing the exponential utility function. We obtain an iteration complexity of $\mathcal{O}(\epsilon^{-2})$ to reach an $\epsilon$-approximate first-order stationary point (FOSP). We investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutr
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20998;&#26512;&#20102;&#25552;&#31034;&#32534;&#36753;&#34892;&#20026;&#30340;&#20250;&#35805;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#25552;&#31034;&#24037;&#31243;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2403.08950</link><description>&lt;p&gt;
&#25506;&#31350;&#20225;&#19994;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Exploring Prompt Engineering Practices in the Enterprise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08950
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20998;&#26512;&#20102;&#25552;&#31034;&#32534;&#36753;&#34892;&#20026;&#30340;&#20250;&#35805;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#25552;&#31034;&#24037;&#31243;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20114;&#21160;&#20027;&#35201;&#36890;&#36807;&#25552;&#31034;&#26469;&#23454;&#29616;&#12290; &#25552;&#31034;&#26159;&#19968;&#31181;&#26088;&#22312;&#20174;&#27169;&#22411;&#20013;&#24341;&#20986;&#29305;&#23450;&#34892;&#20026;&#25110;&#36755;&#20986;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12290; &#29702;&#35770;&#19978;&#65292;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20351;&#38750;&#19987;&#23478;&#33021;&#22815;&#19982;LLMs&#36827;&#34892;&#20114;&#21160;&#21644;&#21033;&#29992;&#12290; &#20294;&#26159;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#20855;&#26377;&#29305;&#23450;&#35201;&#27714;&#30340;&#20219;&#21153;&#65292;&#25552;&#31034;&#35774;&#35745;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290; &#21019;&#24314;&#26377;&#25928;&#30340;&#25552;&#31034;&#38656;&#35201;&#25216;&#33021;&#21644;&#30693;&#35782;&#65292;&#20197;&#21450;&#22823;&#37327;&#36845;&#20195;&#65292;&#20197;&#30830;&#23450;&#27169;&#22411;&#34892;&#20026;&#65292;&#24182;&#24341;&#23548;&#27169;&#22411;&#23454;&#29616;&#29305;&#23450;&#30446;&#26631;&#12290; &#25105;&#20204;&#20551;&#35774;&#29992;&#25143;&#22312;&#25552;&#31034;&#19978;&#36827;&#34892;&#36845;&#20195;&#30340;&#26041;&#24335;&#21487;&#20197;&#25581;&#31034;&#20182;&#20204;&#22914;&#20309;&#35748;&#20026;&#25552;&#31034;&#21644;&#27169;&#22411;&#24037;&#20316;&#65292;&#20197;&#21450;&#26356;&#39640;&#25928;&#30340;&#25552;&#31034;&#24037;&#31243;&#38656;&#35201;&#21738;&#31181;&#25903;&#25345;&#12290; &#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#25552;&#31034;&#24037;&#31243;&#23454;&#36341;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25552;&#31034;&#32534;&#36753;&#34892;&#20026;&#30340;&#20250;&#35805;&#65292;&#23545;&#29992;&#25143;&#36845;&#20195;&#30340;&#25552;&#31034;&#37096;&#20998;&#21644;&#26356;&#25913;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08950v1 Announce Type: cross  Abstract: Interaction with Large Language Models (LLMs) is primarily carried out via prompting. A prompt is a natural language instruction designed to elicit certain behaviour or output from a model. In theory, natural language prompts enable non-experts to interact with and leverage LLMs. However, for complex tasks and tasks with specific requirements, prompt design is not trivial. Creating effective prompts requires skill and knowledge, as well as significant iteration in order to determine model behavior, and guide the model to accomplish a particular goal. We hypothesize that the way in which users iterate on their prompts can provide insight into how they think prompting and models work, as well as the kinds of support needed for more efficient prompt engineering. To better understand prompt engineering practices, we analyzed sessions of prompt editing behavior, categorizing the parts of prompts users iterated on and the types of changes th
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#23454;&#39564;&#30740;&#31350;&#25581;&#31034;&#20102;&#35821;&#35328;&#20869;&#23481;&#26174;&#33879;&#24433;&#21709;&#20915;&#31574;&#65292;&#20419;&#20351;&#20174;&#22522;&#20110;&#32467;&#26524;&#36716;&#21521;&#22522;&#20110;&#35821;&#35328;&#30340;&#25928;&#29992;&#20989;&#25968;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.08944</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#21338;&#24328;&#35770;
&lt;/p&gt;
&lt;p&gt;
Language-based game theory in the age of artificial intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08944
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#39564;&#30740;&#31350;&#25581;&#31034;&#20102;&#35821;&#35328;&#20869;&#23481;&#26174;&#33879;&#24433;&#21709;&#20915;&#31574;&#65292;&#20419;&#20351;&#20174;&#22522;&#20110;&#32467;&#26524;&#36716;&#21521;&#22522;&#20110;&#35821;&#35328;&#30340;&#25928;&#29992;&#20989;&#25968;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20915;&#31574;&#38382;&#39064;&#21644;&#25112;&#30053;&#20114;&#21160;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#22312;&#32463;&#27982;&#23398;&#12289;&#24515;&#29702;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#21338;&#24328;&#35770;&#25552;&#20379;&#20102;&#36825;&#31181;&#29702;&#35299;&#30340;&#22362;&#23454;&#22522;&#30784;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#20010;&#20307;&#26088;&#22312;&#26368;&#22823;&#21270;&#25928;&#29992;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#24433;&#21709;&#31574;&#30053;&#36873;&#25321;&#30340;&#30830;&#20999;&#22240;&#32032;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#23613;&#31649;&#20256;&#32479;&#27169;&#22411;&#35797;&#22270;&#23558;&#20154;&#31867;&#34892;&#20026;&#35299;&#37322;&#20026;&#21487;&#29992;&#34892;&#21160;&#32467;&#26524;&#30340;&#20989;&#25968;&#65292;&#20294;&#26368;&#36817;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#20869;&#23481;&#26174;&#33879;&#24433;&#21709;&#20915;&#31574;&#65292;&#22240;&#27492;&#20419;&#20351;&#20174;&#22522;&#20110;&#32467;&#26524;&#36716;&#21521;&#22522;&#20110;&#35821;&#35328;&#30340;&#25928;&#29992;&#20989;&#25968;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#37492;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#65292;&#36825;&#31181;&#36716;&#21464;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#21152;&#32039;&#36843;&#65292;&#21518;&#32773;&#26377;&#28508;&#21147;&#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#30340;&#20114;&#21160;&#25903;&#25345;&#20154;&#31867;&#20570;&#20986;&#20851;&#38190;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#24773;&#24863;&#20998;&#26512;&#20316;&#20026;&#36825;&#31181;&#36716;&#21464;&#30340;&#22522;&#30784;&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#20854;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08944v1 Announce Type: cross  Abstract: Understanding human behaviour in decision problems and strategic interactions has wide-ranging applications in economics, psychology, and artificial intelligence. Game theory offers a robust foundation for this understanding, based on the idea that individuals aim to maximize a utility function. However, the exact factors influencing strategy choices remain elusive. While traditional models try to explain human behaviour as a function of the outcomes of available actions, recent experimental research reveals that linguistic content significantly impacts decision-making, thus prompting a paradigm shift from outcome-based to language-based utility functions. This shift is more urgent than ever, given the advancement of generative AI, which has the potential to support humans in making critical decisions through language-based interactions. We propose sentiment analysis as a fundamental tool for this shift and take an initial step by anal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19977;&#31181;&#20027;&#35201;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#25910;&#38598;&#30340;333&#20010;&#32570;&#38519;&#26679;&#26412;&#65292;&#24182;&#35782;&#21035;&#20102;10&#31181;&#29420;&#29305;&#30340;&#38169;&#35823;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.08937</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#30340;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Bugs in Large Language Models Generated Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19977;&#31181;&#20027;&#35201;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#25910;&#38598;&#30340;333&#20010;&#32570;&#38519;&#26679;&#26412;&#65292;&#24182;&#35782;&#21035;&#20102;10&#31181;&#29420;&#29305;&#30340;&#38169;&#35823;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29992;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#23427;&#20204;&#21487;&#20197;&#22522;&#20110;&#25552;&#20379;&#30340;&#25552;&#31034;&#22312;&#19981;&#21516;&#30340;&#32534;&#31243;&#35821;&#35328;&#20013;&#29983;&#25104;&#20195;&#30721;&#65292;&#23454;&#29616;&#20102;&#36719;&#20214;&#24037;&#31243;&#65288;SE&#65289;&#20013;&#38271;&#26399;&#20197;&#26469;&#30340;&#19968;&#20010;&#26790;&#24819;&#65292;&#21363;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#12290;&#19982;&#20154;&#31867;&#32534;&#20889;&#30340;&#20195;&#30721;&#31867;&#20284;&#65292;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#65292;&#20294;&#31038;&#21306;&#23578;&#26410;&#23545;&#36825;&#20123;&#38169;&#35823;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;&#37492;&#20110;&#36719;&#20214;&#24037;&#31243;&#27963;&#21160;&#20013;LLM-based&#20195;&#30721;&#29983;&#25104;&#24037;&#20855;&#65288;&#20363;&#22914;GitHub Copilot&#65289;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#20102;&#35299;LLMs&#29983;&#25104;&#20195;&#30721;&#20013;&#21253;&#21547;&#30340;&#32570;&#38519;&#29305;&#24449;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19977;&#31181;&#20027;&#35201;LLM&#65288;&#21363;CodeGen&#12289;PanGu-Coder&#21644;Codex&#65289;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#25910;&#38598;&#30340;333&#20010;&#32570;&#38519;&#26679;&#26412;&#65292;&#24182;&#35782;&#21035;&#20102;&#20197;&#19979;10&#31181;&#29420;&#29305;&#30340;&#38169;&#35823;&#27169;&#24335;&#65306;&#35823;&#35299;&#12289;&#35821;&#27861;&#38169;&#35823;&#12289;&#24858;&#34850;&#38169;&#35823;&#12289;&#25552;&#31034;&#20559;&#21521;&#20195;&#30721;&#12289;&#36951;&#28431;&#35282;&#33853;&#26696;&#20363;&#12289;&#38169;&#35823;&#30340;&#36755;&#20837;&#31867;&#22411;&#12289;&#20135;&#29983;&#24187;&#35937;&#23545;&#35937;&#12289;&#38169;&#35823;&#30340;&#23646;&#24615;&#12289;&#19981;&#23436;&#25972;&#65288;&#26410;&#23436;&#32467;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08937v1 Announce Type: cross  Abstract: Large Language Models (LLMs) for code have gained significant attention recently. They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation. Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs. This paper examines a sample of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomple
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#30340;&#27010;&#24565;&#65292;&#20026;&#27599;&#20010;&#26234;&#20307;&#25110;&#19981;&#21516;&#31867;&#22411;&#30340;&#26234;&#20307;&#25552;&#20379;&#38024;&#23545;&#20010;&#20154;&#30446;&#26631;&#30340;&#25351;&#23548;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#32852;&#21512;&#31034;&#33539;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08936</link><description>&lt;p&gt;
&#36229;&#36234;&#32852;&#21512;&#31034;&#33539;&#65306;&#20010;&#24615;&#21270;&#19987;&#23478;&#25351;&#23548;&#29992;&#20110;&#39640;&#25928;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08936
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#30340;&#27010;&#24565;&#65292;&#20026;&#27599;&#20010;&#26234;&#20307;&#25110;&#19981;&#21516;&#31867;&#22411;&#30340;&#26234;&#20307;&#25552;&#20379;&#38024;&#23545;&#20010;&#20154;&#30446;&#26631;&#30340;&#25351;&#23548;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#32852;&#21512;&#31034;&#33539;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#26377;&#25928;&#25506;&#32034;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#32852;&#21512;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#34429;&#28982;&#31034;&#33539;&#24341;&#23548;&#23398;&#20064;&#22312;&#21333;&#26234;&#20307;&#29615;&#22659;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#20854;&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#21463;&#21040;&#33719;&#24471;&#32852;&#21512;&#19987;&#23478;&#31034;&#33539;&#30340;&#23454;&#38469;&#22256;&#38590;&#30340;&#38459;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#30340;&#26032;&#27010;&#24565;&#65292;&#38024;&#23545;&#27599;&#20010;&#21333;&#20010;&#26234;&#20307;&#25110;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#22242;&#38431;&#20013;&#27599;&#31181;&#31867;&#22411;&#30340;&#26234;&#20307;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#36825;&#20123;&#31034;&#33539;&#20165;&#28041;&#21450;&#21333;&#26234;&#20307;&#34892;&#20026;&#20197;&#21450;&#27599;&#20010;&#26234;&#20307;&#22914;&#20309;&#23454;&#29616;&#20010;&#20154;&#30446;&#26631;&#65292;&#32780;&#19981;&#28041;&#21450;&#20219;&#20309;&#21512;&#20316;&#20803;&#32032;&#65292;&#22240;&#27492;&#30450;&#30446;&#27169;&#20223;&#23427;&#20204;&#19981;&#20250;&#23454;&#29616;&#21512;&#20316;&#30001;&#20110;&#28508;&#22312;&#20914;&#31361;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#20316;&#20026;&#25351;&#23548;&#65292;&#24182;&#20801;&#35768;&#26234;&#20307;&#23398;&#20064;&#21327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08936v1 Announce Type: cross  Abstract: Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of efficient exploration due to the exponential increase in the size of the joint state-action space. While demonstration-guided learning has proven beneficial in single-agent settings, its direct applicability to MARL is hindered by the practical difficulty of obtaining joint expert demonstrations. In this work, we introduce a novel concept of personalized expert demonstrations, tailored for each individual agent or, more broadly, each individual type of agent within a heterogeneous team. These demonstrations solely pertain to single-agent behaviors and how each agent can achieve personal goals without encompassing any cooperative elements, thus naively imitating them will not achieve cooperation due to potential conflicts. To this end, we propose an approach that selectively utilizes personalized expert demonstrations as guidance and allows agents to learn to coo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#31867;&#35821;&#20041;&#30693;&#35782;&#30740;&#31350;&#20102;&#34394;&#20551;&#22270;&#20687;&#26816;&#27979;&#26694;&#26550;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#25910;&#38598;&#26032;&#25968;&#25454;&#38598;&#21644;&#36827;&#34892;&#30524;&#21160;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#30524;&#21160;&#22312;&#27492;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.08933</link><description>&lt;p&gt;
&#25581;&#31034;&#30495;&#30456;&#65306;&#25506;&#32034;&#20154;&#31867;&#20957;&#35270;&#20551;&#22270;&#20013;&#30340;&#30524;&#21160;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Truth: Exploring Human Gaze Patterns in Fake Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#31867;&#35821;&#20041;&#30693;&#35782;&#30740;&#31350;&#20102;&#34394;&#20551;&#22270;&#20687;&#26816;&#27979;&#26694;&#26550;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#25910;&#38598;&#26032;&#25968;&#25454;&#38598;&#21644;&#36827;&#34892;&#30524;&#21160;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#30524;&#21160;&#22312;&#27492;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#30340;&#28145;&#36828;&#36827;&#27493;&#65292;&#29616;&#22312;&#21487;&#20197;&#21019;&#24314;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25152;&#38656;&#36755;&#20986;&#65292;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#21497;&#20026;&#35266;&#27490;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#20351;&#29992;&#22686;&#22810;&#65292;&#20154;&#20204;&#23545;&#24694;&#24847;&#20869;&#23481;&#21644;&#38169;&#35823;&#20449;&#24687;&#20256;&#25773;&#30340;&#25285;&#24551;&#20063;&#22312;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#30028;&#27491;&#22312;&#31215;&#26497;&#33268;&#21147;&#20110;&#24320;&#21457;&#26032;&#22411;&#34394;&#20551;&#26816;&#27979;&#25216;&#26415;&#65292;&#20027;&#35201;&#20851;&#27880;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#29983;&#25104;&#27169;&#22411;&#21487;&#33021;&#30041;&#19979;&#30340;&#20302;&#32423;&#29305;&#24449;&#21644;&#21487;&#33021;&#30340;&#25351;&#32441;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20511;&#21161;&#20154;&#31867;&#35821;&#20041;&#30693;&#35782;&#25506;&#35752;&#21487;&#33021;&#32435;&#20837;&#34394;&#20551;&#22270;&#20687;&#26816;&#27979;&#26694;&#26550;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#37096;&#20998;&#25805;&#32437;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#30524;&#21160;&#23454;&#39564;&#65292;&#35760;&#24405;&#20102;&#19981;&#21516;&#35266;&#23519;&#32773;&#22312;&#35266;&#30475;&#36807;&#31243;&#20013;&#30340;&#30524;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08933v1 Announce Type: cross  Abstract: Creating high-quality and realistic images is now possible thanks to the impressive advancements in image generation. A description in natural language of your desired output is all you need to obtain breathtaking results. However, as the use of generative models grows, so do concerns about the propagation of malicious content and misinformation. Consequently, the research community is actively working on the development of novel fake detection techniques, primarily focusing on low-level features and possible fingerprints left by generative models during the image generation process. In a different vein, in our work, we leverage human semantic knowledge to investigate the possibility of being included in frameworks of fake image detection. To achieve this, we collect a novel dataset of partially manipulated images using diffusion models and conduct an eye-tracking experiment to record the eye movements of different observers while view
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35748;&#30495;&#31579;&#36873;&#21644;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;Flickr&#22270;&#20687;&#29305;&#24449;&#19982;&#31354;&#20013;&#22270;&#20687;&#29305;&#24449;&#30456;&#32467;&#21512;&#33021;&#22815;&#23558;&#24615;&#33021;&#24046;&#36317;&#20174;30%&#38477;&#20302;&#21040;15%</title><link>https://arxiv.org/abs/2403.08915</link><description>&lt;p&gt;
&#38463;&#22982;&#26031;&#29305;&#20025;&#20303;&#25151;&#36136;&#37327;&#30340;&#36328;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Learning of Housing Quality in Amsterdam
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08915
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35748;&#30495;&#31579;&#36873;&#21644;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;Flickr&#22270;&#20687;&#29305;&#24449;&#19982;&#31354;&#20013;&#22270;&#20687;&#29305;&#24449;&#30456;&#32467;&#21512;&#33021;&#22815;&#23558;&#24615;&#33021;&#24046;&#36317;&#20174;30%&#38477;&#20302;&#21040;15%
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#20174;&#22320;&#38754;&#21644;&#31354;&#20013;&#22270;&#20687;&#35782;&#21035;&#38463;&#22982;&#26031;&#29305;&#20025;&#22478;&#24066;&#20303;&#25151;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#23545;&#20110;&#22320;&#38754;&#22270;&#20687;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#35895;&#27468;&#34903;&#26223;&#22270;&#65288;GSV&#65289;&#21644;Flickr&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;GSV&#33021;&#22815;&#39044;&#27979;&#26368;&#20934;&#30830;&#30340;&#24314;&#31569;&#36136;&#37327;&#20998;&#25968;&#65292;&#27604;&#21333;&#29420;&#20351;&#29992;&#31354;&#20013;&#22270;&#20687;&#25552;&#39640;&#20102;&#32422;30%&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#20180;&#32454;&#31579;&#36873;&#24182;&#20351;&#29992;&#27491;&#30830;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;Flickr&#22270;&#20687;&#29305;&#24449;&#19982;&#31354;&#20013;&#22270;&#20687;&#29305;&#24449;&#30456;&#32467;&#21512;&#33021;&#22815;&#23558;&#24615;&#33021;&#24046;&#36317;&#20174;30%&#38477;&#20302;&#21040;15%&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#29983;&#27963;&#36136;&#37327;&#22240;&#32032;&#39044;&#27979;&#65292;&#23384;&#22312;&#21487;&#34892;&#30340;GSV&#26367;&#20195;&#26041;&#26696;&#65292;&#36825;&#26159;&#19968;&#20010;&#40723;&#33310;&#20154;&#24515;&#30340;&#28040;&#24687;&#65292;&#22240;&#20026;GSV&#22270;&#20687;&#26356;&#38590;&#33719;&#21462;&#24182;&#19988;&#24182;&#38750;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08915v1 Announce Type: cross  Abstract: In our research we test data and models for the recognition of housing quality in the city of Amsterdam from ground-level and aerial imagery. For ground-level images we compare Google StreetView (GSV) to Flickr images. Our results show that GSV predicts the most accurate building quality scores, approximately 30% better than using only aerial images. However, we find that through careful filtering and by using the right pre-trained model, Flickr image features combined with aerial image features are able to halve the performance gap to GSV features from 30% to 15%. Our results indicate that there are viable alternatives to GSV for liveability factor prediction, which is encouraging as GSV images are more difficult to acquire and not always available.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#20803;&#31639;&#23376;&#30340;&#27010;&#24565;&#65292;&#20351;&#24471;RL&#21160;&#20316;&#31354;&#38388;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#35268;&#21010;&#25805;&#20316;&#31526;&#65292;&#20174;&#32780;&#23454;&#29616;&#26032;&#30340;&#35268;&#21010;&#35270;&#35282;&#65292;&#20363;&#22914;&#24182;&#34892;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.08910</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#24182;&#34892;&#35268;&#21010;&#30340;&#20803;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Meta-operators for Enabling Parallel Planning Using Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08910
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#20803;&#31639;&#23376;&#30340;&#27010;&#24565;&#65292;&#20351;&#24471;RL&#21160;&#20316;&#31354;&#38388;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#35268;&#21010;&#25805;&#20316;&#31526;&#65292;&#20174;&#32780;&#23454;&#29616;&#26032;&#30340;&#35268;&#21010;&#35270;&#35282;&#65292;&#20363;&#22914;&#24182;&#34892;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#20204;&#23545;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#36827;&#34892;AI&#35268;&#21010;&#30340;&#24212;&#29992;&#36234;&#21457;&#24863;&#20852;&#36259;&#65292;&#26088;&#22312;&#25552;&#20986;&#36890;&#29992;&#31574;&#30053;&#12290;&#36890;&#24120;&#65292;&#36890;&#36807;&#20551;&#35774;&#21508;&#33258;&#21160;&#20316;&#31354;&#38388;&#20855;&#26377;&#19968;&#19968;&#23545;&#24212;&#20851;&#31995;&#65292;&#23558;AI&#35268;&#21010;&#30340;&#36716;&#31227;&#27169;&#22411;&#26144;&#23556;&#21040;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#29366;&#24577;&#36716;&#31227;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#31639;&#23376;&#30340;&#27010;&#24565;&#65292;&#23427;&#26159;&#21516;&#26102;&#24212;&#29992;&#22810;&#20010;&#35268;&#21010;&#31639;&#23376;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20803;&#31639;&#23376;&#21253;&#25324;&#22312;RL&#21160;&#20316;&#31354;&#38388;&#20013;&#22914;&#20309;&#20351;&#24471;&#26032;&#30340;&#35268;&#21010;&#35270;&#35282;&#33021;&#22815;&#36890;&#36807;RL&#26469;&#35299;&#20915;&#65292;&#20363;&#22914;&#24182;&#34892;&#35268;&#21010;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#22312;RL&#36807;&#31243;&#20013;&#21253;&#25324;&#20803;&#31639;&#23376;&#30340;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#20197;&#24448;&#20351;&#29992;&#20256;&#32479;&#24191;&#20041;&#35268;&#21010;&#27169;&#22411;&#26410;&#26366;&#21462;&#24471;&#28385;&#24847;&#32467;&#26524;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#37325;&#23450;&#20041;&#36328;&#28145;&#23618;&#35268;&#21010;&#21644;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#30028;&#38480;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08910v1 Announce Type: new  Abstract: There is a growing interest in the application of Reinforcement Learning (RL) techniques to AI planning with the aim to come up with general policies. Typically, the mapping of the transition model of AI planning to the state transition system of a Markov Decision Process is established by assuming a one-to-one correspondence of the respective action spaces. In this paper, we introduce the concept of meta-operator as the result of simultaneously applying multiple planning operators, and we show that including meta-operators in the RL action space enables new planning perspectives to be addressed using RL, such as parallel planning. Our research aims to analyze the performance and complexity of including meta-operators in the RL process, concretely in domains where satisfactory outcomes have not been previously achieved using usual generalized planning models. The main objective of this article is thus to pave the way towards a redefiniti
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#25506;&#35752;&#20102;Q-learning&#31639;&#27861;&#22312;&#28216;&#25103;&#20013;&#21463;&#21040;&#31574;&#30053;&#24615;&#23545;&#25163;&#30340;&#25805;&#32437;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08906</link><description>&lt;p&gt;
&#38024;&#23545;Q-&#23398;&#20064;&#32773;&#30340;&#31574;&#30053;&#21270;&#23545;&#25239;&#65306;&#19968;&#31181;&#25511;&#21046;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Strategizing against Q-learners: A Control-theoretical Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08906
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#25506;&#35752;&#20102;Q-learning&#31639;&#27861;&#22312;&#28216;&#25103;&#20013;&#21463;&#21040;&#31574;&#30053;&#24615;&#23545;&#25163;&#30340;&#25805;&#32437;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;Q-learning&#31639;&#27861;(&#19968;&#31181;&#32463;&#20856;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;)&#22312;&#28216;&#25103;&#20013;&#23545;&#31574;&#30053;&#24615;&#23545;&#25163;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#22914;&#26524;&#31574;&#30053;&#24615;&#23545;&#25163;&#20102;&#35299;&#23545;&#25163;&#30340;Q-learning&#31639;&#27861;&#65292;&#22905;&#21487;&#20197;&#21033;&#29992;&#19968;&#20010;&#22825;&#30495;&#30340;Q-&#23398;&#20064;&#32773;&#22810;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#31574;&#30053;&#34892;&#20026;&#32773;&#30340;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(&#20855;&#26377;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;Q&#20540;&#30340;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;)&#65292;&#23601;&#22909;&#20687;Q-&#23398;&#20064;&#31639;&#27861;&#26159;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#19968;&#26679;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37327;&#21270;&#30340;&#36817;&#20284;&#26041;&#26696;&#26469;&#22788;&#29702;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#20998;&#26512;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08906v1 Announce Type: cross  Abstract: In this paper, we explore the susceptibility of the Q-learning algorithm (a classical and widely used reinforcement learning method) to strategic manipulation of sophisticated opponents in games. We quantify how much a strategically sophisticated agent can exploit a naive Q-learner if she knows the opponent's Q-learning algorithm. To this end, we formulate the strategic actor's problem as a Markov decision process (with a continuum state space encompassing all possible Q-values) as if the Q-learning algorithm is the underlying dynamical system. We also present a quantization-based approximation scheme to tackle the continuum state space and analyze its performance both analytically and numerically.
&lt;/p&gt;</description></item><item><title>SLCF-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#20041;&#22330;&#26223;&#23436;&#21892;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#34701;&#21512;&#28608;&#20809;&#38647;&#36798;&#21644;&#30456;&#26426;&#25968;&#25454;&#65292;&#32852;&#21512;&#20272;&#35745;&#32570;&#22833;&#30340;&#20960;&#20309;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#39640;&#26031;&#34928;&#20943;&#28145;&#24230;&#20808;&#39564;&#25237;&#24433;&#27169;&#22359;&#20197;&#23454;&#29616;2D&#22270;&#20687;&#29305;&#24449;&#21644;3D&#22330;&#26223;&#20307;&#31215;&#30340;&#20851;&#32852;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#30830;&#20445;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08885</link><description>&lt;p&gt;
SLCF-Net&#65306;&#20351;&#29992;3D&#24490;&#29615;U-Net&#36827;&#34892;&#24207;&#21015;&#24335;&#28608;&#20809;&#38647;&#36798;-&#30456;&#26426;&#34701;&#21512;&#30340;&#35821;&#20041;&#22330;&#26223;&#23436;&#21892;
&lt;/p&gt;
&lt;p&gt;
SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion using a 3D Recurrent U-Net
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08885
&lt;/p&gt;
&lt;p&gt;
SLCF-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#20041;&#22330;&#26223;&#23436;&#21892;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#34701;&#21512;&#28608;&#20809;&#38647;&#36798;&#21644;&#30456;&#26426;&#25968;&#25454;&#65292;&#32852;&#21512;&#20272;&#35745;&#32570;&#22833;&#30340;&#20960;&#20309;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#39640;&#26031;&#34928;&#20943;&#28145;&#24230;&#20808;&#39564;&#25237;&#24433;&#27169;&#22359;&#20197;&#23454;&#29616;2D&#22270;&#20687;&#29305;&#24449;&#21644;3D&#22330;&#26223;&#20307;&#31215;&#30340;&#20851;&#32852;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#30830;&#20445;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SLCF-Net&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#29992;&#20110;&#35821;&#20041;&#22330;&#26223;&#23436;&#21892;&#65288;SSC&#65289;&#20219;&#21153;&#65292;&#36890;&#36807;&#24207;&#21015;&#34701;&#21512;&#28608;&#20809;&#38647;&#36798;&#21644;&#30456;&#26426;&#25968;&#25454;&#12290;&#23427;&#36890;&#36807;RGB&#22270;&#20687;&#24207;&#21015;&#21644;&#31232;&#30095;&#28608;&#20809;&#38647;&#36798;&#27979;&#37327;&#25968;&#25454;&#32852;&#21512;&#20272;&#35745;&#22330;&#26223;&#20013;&#32570;&#22833;&#30340;&#20960;&#20309;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#22270;&#20687;&#32463;&#36807;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;2D U-Net&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#24182;&#20174;Depth Anything&#25552;&#20379;&#30340;&#28145;&#24230;&#26465;&#20214;&#31649;&#32447;&#20013;&#20272;&#35745;&#20986;&#23494;&#38598;&#28145;&#24230;&#20808;&#39564;&#12290;&#20026;&#20102;&#23558;2D&#22270;&#20687;&#29305;&#24449;&#19982;3D&#22330;&#26223;&#20307;&#31215;&#20851;&#32852;&#36215;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#26031;&#34928;&#20943;&#28145;&#24230;&#20808;&#39564;&#25237;&#24433;&#65288;GDP&#65289;&#12290;&#35813;&#27169;&#22359;&#20351;&#29992;&#39640;&#26031;&#34928;&#20943;&#20989;&#25968;&#27839;&#30528;&#20197;&#28145;&#24230;&#20808;&#39564;&#20026;&#20013;&#24515;&#30340;&#35270;&#32447;&#23558;2D&#29305;&#24449;&#25237;&#24433;&#21040;3D&#20307;&#31215;&#20013;&#12290;&#20307;&#31215;&#35821;&#20041;&#30001;3D U-Net&#35745;&#31639;&#12290;&#25105;&#20204;&#21033;&#29992;&#20256;&#24863;&#22120;&#36816;&#21160;&#20256;&#25773;&#38544;&#34255;&#30340;3D U-Net&#29366;&#24577;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#30830;&#20445;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;SemanticKITTI&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#39046;&#20808;&#30340;SSC&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08885v1 Announce Type: cross  Abstract: We introduce SLCF-Net, a novel approach for the Semantic Scene Completion (SSC) task that sequentially fuses LiDAR and camera data. It jointly estimates missing geometry and semantics in a scene from sequences of RGB images and sparse LiDAR measurements. The images are semantically segmented by a pre-trained 2D U-Net and a dense depth prior is estimated from a depth-conditioned pipeline fueled by Depth Anything. To associate the 2D image features with the 3D scene volume, we introduce Gaussian-decay Depth-prior Projection (GDP). This module projects the 2D features into the 3D volume along the line of sight with a Gaussian-decay function, centered around the depth prior. Volumetric semantics is computed by a 3D U-Net. We propagate the hidden 3D U-Net state using the sensor motion and design a novel loss to ensure temporal consistency. We evaluate our approach on the SemanticKITTI dataset and compare it with leading SSC approaches. The 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#20223;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#25991;&#21270;&#28436;&#21270;&#20013;&#30001;&#36827;&#21270;&#30340;&#35748;&#30693;&#26426;&#21046;&#24341;&#36215;&#30340;&#31038;&#20250;&#20449;&#24687;&#36716;&#21464;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.08882</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Cultural evolution in populations of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08882
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#20223;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#25991;&#21270;&#28436;&#21270;&#20013;&#30001;&#36827;&#21270;&#30340;&#35748;&#30693;&#26426;&#21046;&#24341;&#36215;&#30340;&#31038;&#20250;&#20449;&#24687;&#36716;&#21464;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#21270;&#28436;&#21270;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#25991;&#21270;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#22240;&#26524;&#35299;&#37322;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#65292;&#36825;&#19968;&#39046;&#22495;&#24050;&#32463;&#20135;&#29983;&#20102;&#37325;&#35201;&#30340;&#30693;&#35782;&#20307;&#31995;&#65292;&#21033;&#29992;&#20102;&#23454;&#39564;&#12289;&#21382;&#21490;&#21644;&#35745;&#31639;&#26041;&#27861;&#12290;&#34429;&#28982;&#35745;&#31639;&#27169;&#22411;&#22312;&#29983;&#25104;&#20851;&#20110;&#35832;&#22810;&#22240;&#32032;&#22914;&#20154;&#21475;&#32467;&#26500;&#25110;&#20256;&#25773;&#20559;&#24046;&#24433;&#21709;&#30340;&#21487;&#26816;&#39564;&#20551;&#35774;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#26576;&#20123;&#29616;&#35937;&#36804;&#20170;&#20026;&#27490;&#26356;&#38590;&#20197;&#20351;&#29992;&#22522;&#20110;&#20195;&#29702;&#20154;&#21644;&#24418;&#24335;&#27169;&#22411;&#26469;&#25429;&#25417;&#65292;&#23588;&#20854;&#26159;&#30001;&#36827;&#21270;&#30340;&#35748;&#30693;&#26426;&#21046;&#24341;&#36215;&#30340;&#31038;&#20250;&#20449;&#24687;&#36716;&#21464;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27169;&#20223;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#21487;&#33021;&#26377;&#21161;&#20110;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#12290;&#20316;&#20026;&#20154;&#31867;&#25991;&#21270;&#21160;&#24577;&#30340;&#26377;&#29992;&#36817;&#20284;&#65292;&#20197;&#29983;&#25104;&#20195;&#29702;&#20026;&#29305;&#24449;&#30340;&#22810;&#20195;&#29702;&#27169;&#22411;&#20063;&#26159;&#30740;&#31350;&#33258;&#36523;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08882v1 Announce Type: cross  Abstract: Research in cultural evolution aims at providing causal explanations for the change of culture over time. Over the past decades, this field has generated an important body of knowledge, using experimental, historical, and computational methods. While computational models have been very successful at generating testable hypotheses about the effects of several factors, such as population structure or transmission biases, some phenomena have so far been more complex to capture using agent-based and formal models. This is in particular the case for the effect of the transformations of social information induced by evolved cognitive mechanisms. We here propose that leveraging the capacity of Large Language Models (LLMs) to mimic human behavior may be fruitful to address this gap. On top of being an useful approximation of human cultural dynamics, multi-agents models featuring generative agents are also important to study for their own sake.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#29615;&#22659;&#19979;&#36827;&#34892;&#22810;&#30446;&#26631;&#12289;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20302;&#35745;&#31639;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.08879</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08879
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#29615;&#22659;&#19979;&#36827;&#34892;&#22810;&#30446;&#26631;&#12289;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20302;&#35745;&#31639;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#29615;&#22659;&#34987;&#35748;&#20026;&#26159;&#21160;&#24577;&#21644;&#20998;&#24067;&#24335;&#30340;&#65292;&#21442;&#19982;&#32773;&#65288;&#36710;&#36742;&#29992;&#25143;&#65292;&#36816;&#33829;&#21830;&#31561;&#65289;&#20855;&#26377;&#22810;&#20010;&#12289;&#19981;&#26029;&#21464;&#21270;&#19988;&#21487;&#33021;&#30456;&#20114;&#20914;&#31361;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#36890;&#24120;&#29992;&#20110;&#20248;&#21270;ITS&#24212;&#29992;&#65292;&#22914;&#36164;&#28304;&#31649;&#29702;&#21644;&#21368;&#36733;&#65292;&#20294;&#22823;&#22810;&#25968;RL&#31639;&#27861;&#19987;&#27880;&#20110;&#21333;&#19968;&#30446;&#26631;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23558;&#22810;&#30446;&#26631;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#19968;&#30446;&#26631;&#26159;&#19981;&#21487;&#33021;&#30340;&#12289;&#26840;&#25163;&#30340;&#25110;&#19981;&#36275;&#30340;&#65292;&#36825;&#20351;&#24471;&#36825;&#31181;RL&#31639;&#27861;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20302;&#35745;&#31639;&#35201;&#27714;&#30340;&#22810;&#30446;&#26631;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#21160;&#24577;&#12289;&#20998;&#24067;&#24335;&#21644;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#33258;&#21160;&#35302;&#21457;&#33258;&#36866;&#24212;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#20855;&#26377;&#31232;&#30095;&#21644;&#24310;&#36831;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#36793;&#32536;&#20113;&#35745;&#31639;&#30340;ITS&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08879v1 Announce Type: cross  Abstract: The Intelligent Transportation System (ITS) environment is known to be dynamic and distributed, where participants (vehicle users, operators, etc.) have multiple, changing and possibly conflicting objectives. Although Reinforcement Learning (RL) algorithms are commonly applied to optimize ITS applications such as resource management and offloading, most RL algorithms focus on single objectives. In many situations, converting a multi-objective problem into a single-objective one is impossible, intractable or insufficient, making such RL algorithms inapplicable. We propose a multi-objective, multi-agent reinforcement learning (MARL) algorithm with high learning efficiency and low computational requirements, which automatically triggers adaptive few-shot learning in a dynamic, distributed and noisy environment with sparse and delayed reward. We test our algorithm in an ITS environment with edge cloud computing. Empirical results show that
&lt;/p&gt;</description></item><item><title>&#20998;&#21449;&#27880;&#24847;&#21147;&#26159;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#21333;&#19978;&#19979;&#25991;&#25209;&#37327;&#25277;&#26679;&#29615;&#22659;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20998;&#25104;&#20004;&#20010;&#29420;&#31435;&#30340;&#25805;&#20316;&#26469;&#20943;&#23569;&#20887;&#20313;&#20869;&#23384;IO&#25104;&#26412;&#65292;&#25552;&#39640;&#25928;&#29575;&#24182;&#38477;&#20302;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2403.08845</link><description>&lt;p&gt;
&#21333;&#19978;&#19979;&#25991;&#22823;&#25209;&#37327;&#25277;&#26679;&#30340;&#20998;&#21449;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Bifurcated Attention for Single-Context Large-Batch Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08845
&lt;/p&gt;
&lt;p&gt;
&#20998;&#21449;&#27880;&#24847;&#21147;&#26159;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#21333;&#19978;&#19979;&#25991;&#25209;&#37327;&#25277;&#26679;&#29615;&#22659;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20998;&#25104;&#20004;&#20010;&#29420;&#31435;&#30340;&#25805;&#20316;&#26469;&#20943;&#23569;&#20887;&#20313;&#20869;&#23384;IO&#25104;&#26412;&#65292;&#25552;&#39640;&#25928;&#29575;&#24182;&#38477;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#21449;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21333;&#19978;&#19979;&#25991;&#25209;&#37327;&#25277;&#26679;&#29615;&#22659;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20943;&#23569;&#20887;&#20313;&#30340;&#20869;&#23384;IO&#25104;&#26412;&#65292;&#36825;&#26159;&#39640;&#25209;&#37327;&#22823;&#23567;&#21644;&#38271;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#24310;&#36831;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#20998;&#21449;&#27880;&#24847;&#21147;&#36890;&#36807;&#22312;&#22686;&#37327;&#35299;&#30721;&#26399;&#38388;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#21010;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;GEMM&#25805;&#20316;&#65292;&#20998;&#21035;&#19987;&#27880;&#20110;&#26469;&#33258;&#39044;&#22635;&#20805;&#30340;KV&#32531;&#23384;&#20197;&#21450;&#35299;&#30721;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#30830;&#20445;&#20102;&#31934;&#30830;&#30340;&#35745;&#31639;&#65292;&#24182;&#32500;&#25345;&#24120;&#35268;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35745;&#31639;&#36127;&#36733;&#65288;FLOPs&#65289;&#65292;&#20294;&#20943;&#23569;&#20102;&#20869;&#23384;IO&#12290;&#20998;&#21449;&#27880;&#24847;&#21147;&#36824;&#19982;&#20943;&#23569;KV&#32531;&#23384;&#20869;&#23384;IO&#24050;&#30693;&#30340;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#26426;&#21046;&#20860;&#23481;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#26356;&#39640;&#30340;&#25209;&#37327;&#22823;&#23567;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#30001;&#27492;&#24102;&#26469;&#30340;&#25928;&#29575;&#23548;&#33268;&#26356;&#20302;&#30340;&#24310;&#36831;&#65292;&#25913;&#21892;&#20102;&#23454;&#26102;&#24212;&#29992;&#30340;&#36866;&#29992;&#24615;&#65292;&#20363;&#22914;&#23454;&#29616;&#22823;&#35268;&#27169;&#24182;&#34892;&#30340;&#31572;&#26696;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08845v1 Announce Type: cross  Abstract: In our study, we present bifurcated attention, a method developed for language model inference in single-context batch sampling contexts. This approach aims to reduce redundant memory IO costs, a significant factor in latency for high batch sizes and long context lengths. Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process. This method ensures precise computation and maintains the usual computational load (FLOPs) of standard attention mechanisms, but with reduced memory IO. Bifurcated attention is also compatible with multi-query attention mechanism known for reduced memory IO for KV cache, further enabling higher batch size and context length. The resulting efficiency leads to lower latency, improving suitability for real-time applications, e.g., enabling massively-parallel answer generation 
&lt;/p&gt;</description></item><item><title>AcademiaOS&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#23450;&#24615;&#30740;&#31350;&#20013;&#30340;&#29702;&#35770;&#24314;&#26500;&#65292;&#20026;&#23398;&#26415;&#30028;&#25552;&#20379;&#26032;&#39062;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.08844</link><description>&lt;p&gt;
AcademiaOS&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#23450;&#24615;&#30740;&#31350;&#20013;&#30340;&#29702;&#35770;&#24314;&#26500;
&lt;/p&gt;
&lt;p&gt;
AcademiaOS: Automating Grounded Theory Development in Qualitative Research with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08844
&lt;/p&gt;
&lt;p&gt;
AcademiaOS&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#23450;&#24615;&#30740;&#31350;&#20013;&#30340;&#29702;&#35770;&#24314;&#26500;&#65292;&#20026;&#23398;&#26415;&#30028;&#25552;&#20379;&#26032;&#39062;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AcademiaOS&#26159;&#31532;&#19968;&#20010;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#23450;&#24615;&#30740;&#31350;&#20013;&#30340;&#29702;&#35770;&#24314;&#26500;&#30340;&#31995;&#32479;&#12290;&#21033;&#29992;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;AcademiaOS&#23545;&#31579;&#36873;&#36807;&#30340;&#23450;&#24615;&#21407;&#22987;&#25968;&#25454;&#65288;&#22914;&#35775;&#35848;&#25991;&#26412;&#65289;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#21457;&#23637;&#20027;&#39064;&#21644;&#32500;&#24230;&#20197;&#36827;&#19968;&#27493;&#26500;&#24314;&#19968;&#20010;&#29702;&#35770;&#27169;&#22411;&#65292;&#25552;&#20379;&#26032;&#39062;&#30340;&#35265;&#35299;&#12290;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65288;n=19&#65289;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;&#23398;&#26415;&#30028;&#20013;&#24471;&#21040;&#35748;&#21487;&#65292;&#24182;&#20855;&#26377;&#22686;&#24378;&#20154;&#31867;&#36827;&#34892;&#23450;&#24615;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;AcademiaOS&#24050;&#32463;&#24320;&#28304;&#20379;&#20182;&#20154;&#36827;&#34892;&#26500;&#24314;&#24182;&#36866;&#24212;&#20854;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08844v1 Announce Type: cross  Abstract: AcademiaOS is a first attempt to automate grounded theory development in qualitative research with large language models. Using recent large language models' language understanding, generation, and reasoning capabilities, AcademiaOS codes curated qualitative raw data such as interview transcripts and develops themes and dimensions to further develop a grounded theoretical model, affording novel insights. A user study (n=19) suggests that the system finds acceptance in the academic community and exhibits the potential to augment humans in qualitative research. AcademiaOS has been made open-source for others to build upon and adapt to their use cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#35880;&#30340;&#27169;&#31946;&#19981;&#21487;&#38752;&#24615;&#20540;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#31639;&#27861;&#26469;&#26377;&#25928;&#35745;&#31639;&#31995;&#32479;&#30340;&#27169;&#31946;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08843</link><description>&lt;p&gt;
&#27169;&#31946;&#25925;&#38556;&#26641;&#30340;&#24418;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Fault Trees Formalized
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#35880;&#30340;&#27169;&#31946;&#19981;&#21487;&#38752;&#24615;&#20540;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#31639;&#27861;&#26469;&#26377;&#25928;&#35745;&#31639;&#31995;&#32479;&#30340;&#27169;&#31946;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#26641;&#20998;&#26512;&#26159;&#35780;&#20272;&#23433;&#20840;&#39118;&#38505;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#23427;&#26377;&#21161;&#20110;&#35782;&#21035;&#20107;&#25925;&#28508;&#22312;&#30340;&#21407;&#22240;&#65292;&#35780;&#20272;&#20854;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#21644;&#20005;&#37325;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#39044;&#38450;&#25514;&#26045;&#12290;&#25925;&#38556;&#26641;&#30340;&#23450;&#37327;&#20998;&#26512;&#36890;&#24120;&#36890;&#36807;&#21487;&#38752;&#24615;&#24230;&#37327;&#26469;&#23436;&#25104;&#65292;&#35813;&#24230;&#37327;&#35745;&#31639;&#31995;&#32479;&#38543;&#26102;&#38388;&#30340;&#25925;&#38556;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#31934;&#30830;&#25968;&#25454;&#26159;&#23450;&#37327;&#20998;&#26512;&#20197;&#21450;&#21487;&#38752;&#24615;&#20998;&#26512;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#27169;&#31946;&#36923;&#36753;&#26159;&#22788;&#29702;&#27169;&#31946;&#20540;&#30340;&#24120;&#29992;&#26694;&#26550;&#65292;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#24212;&#29992;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#27169;&#31946;&#26041;&#27861;&#29992;&#20110;&#25925;&#38556;&#26641;&#20998;&#26512;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#31181;&#25552;&#20379;&#29992;&#20110;&#35745;&#31639;&#27169;&#31946;&#19981;&#21487;&#38752;&#24615;&#20540;&#30340;&#20005;&#26684;&#23450;&#20041;&#25110;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#27169;&#31946;&#19981;&#21487;&#38752;&#24615;&#20540;&#30340;&#20005;&#35880;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35745;&#31639;&#31995;&#32479;&#30340;&#27169;&#31946;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08843v1 Announce Type: new  Abstract: Fault tree analysis is a vital method of assessing safety risks. It helps to identify potential causes of accidents, assess their likelihood and severity, and suggest preventive measures. Quantitative analysis of fault trees is often done via the dependability metrics that compute the system's failure behaviour over time. However, the lack of precise data is a major obstacle to quantitative analysis, and so to reliability analysis. Fuzzy logic is a popular framework for dealing with ambiguous values and has applications in many domains. A number of fuzzy approaches have been proposed to fault tree analysis, but -- to the best of our knowledge -- none of them provide rigorous definitions or algorithms for computing fuzzy unreliability values. In this paper, we define a rigorous framework for fuzzy unreliability values. In addition, we provide a bottom-up algorithm to efficiently calculate fuzzy reliability for a system. The algorithm inco
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoiseDiffusion&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#26657;&#27491;&#22270;&#20687;&#25554;&#20540;&#20013;&#30340;&#22122;&#22768;&#65292;&#23558;&#26080;&#25928;&#30340;&#22122;&#22768;&#36924;&#36817;&#21040;&#39044;&#26399;&#30340;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#32422;&#26463;&#26465;&#20214;&#26469;&#25233;&#21046;&#26497;&#31471;&#20540;&#30340;&#22122;&#22768;&#12290;</title><link>https://arxiv.org/abs/2403.08840</link><description>&lt;p&gt;
NoiseDiffusion&#65306;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26657;&#27491;&#22270;&#20687;&#25554;&#20540;&#20013;&#30340;&#22122;&#22768;&#65292;&#36229;&#36234;&#29699;&#38754;&#32447;&#24615;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
NoiseDiffusion: Correcting Noise for Image Interpolation with Diffusion Models beyond Spherical Linear Interpolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08840
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoiseDiffusion&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#26657;&#27491;&#22270;&#20687;&#25554;&#20540;&#20013;&#30340;&#22122;&#22768;&#65292;&#23558;&#26080;&#25928;&#30340;&#22122;&#22768;&#36924;&#36817;&#21040;&#39044;&#26399;&#30340;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#32422;&#26463;&#26465;&#20214;&#26469;&#25233;&#21046;&#26497;&#31471;&#20540;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#25554;&#20540;&#22312;&#21019;&#24314;&#26032;&#39062;&#26377;&#36259;&#30340;&#22270;&#20687;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#28508;&#21147;&#12290;&#39640;&#32423;&#25554;&#20540;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#29699;&#38754;&#32447;&#24615;&#25554;&#20540;&#19978;&#65292;&#20854;&#20013;&#22270;&#20687;&#34987;&#32534;&#30721;&#21040;&#22122;&#22768;&#31354;&#38388;&#65292;&#28982;&#21518;&#36827;&#34892;&#25554;&#20540;&#20197;&#23558;&#21435;&#22122;&#36716;&#25442;&#22238;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#25928;&#25554;&#20540;&#33258;&#28982;&#22270;&#20687;&#65288;&#32780;&#38750;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#65289;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35843;&#26597;&#26174;&#31034;&#65292;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#32534;&#30721;&#22122;&#22768;&#30340;&#26080;&#25928;&#24615;&#65292;&#21487;&#33021;&#19981;&#20877;&#36981;&#24490;&#39044;&#26399;&#30340;&#22122;&#22768;&#20998;&#24067;&#65292;&#22914;&#27491;&#24577;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26657;&#27491;&#22270;&#20687;&#25554;&#20540;&#20013;&#30340;&#22122;&#22768;&#65292;NoiseDiffusion&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NoiseDiffusion&#36890;&#36807;&#24341;&#20837;&#24494;&#22937;&#30340;&#39640;&#26031;&#22122;&#22768;&#23558;&#26080;&#25928;&#30340;&#22122;&#22768;&#36924;&#36817;&#21040;&#39044;&#26399;&#30340;&#20998;&#24067;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#32422;&#26463;&#26465;&#20214;&#26469;&#25233;&#21046;&#20855;&#26377;&#26497;&#31471;&#20540;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08840v1 Announce Type: cross  Abstract: Image interpolation based on diffusion models is promising in creating fresh and interesting images. Advanced interpolation methods mainly focus on spherical linear interpolation, where images are encoded into the noise space and then interpolated for denoising to images. However, existing methods face challenges in effectively interpolating natural images (not generated by diffusion models), thereby restricting their practical applicability. Our experimental investigations reveal that these challenges stem from the invalidity of the encoding noise, which may no longer obey the expected noise distribution, e.g., a normal distribution. To address these challenges, we propose a novel approach to correct noise for image interpolation, NoiseDiffusion. Specifically, NoiseDiffusion approaches the invalid noise to the expected distribution by introducing subtle Gaussian noise and introduces a constraint to suppress noise with extreme values. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#36712;&#36857;&#34920;&#31034;&#30340;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#32858;&#31867;&#21644;&#28508;&#22312;&#32534;&#30721;&#65292;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#32858;&#31867;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08838</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#36712;&#36857;&#34920;&#31034;&#30340;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Predictive Clustering of Vessel Behavior Based on Hierarchical Trajectory Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08838
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#36712;&#36857;&#34920;&#31034;&#30340;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#32858;&#31867;&#21644;&#28508;&#22312;&#32534;&#30721;&#65292;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#32858;&#31867;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33337;&#33334;&#36712;&#36857;&#32858;&#31867;&#26088;&#22312;&#23547;&#25214;&#30456;&#20284;&#30340;&#36712;&#36857;&#27169;&#24335;&#65292;&#22312;&#28023;&#19978;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#22823;&#22810;&#25968;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#35268;&#21017;&#21644;&#38408;&#20540;&#26469;&#35782;&#21035;&#31163;&#25955;&#30340;&#33337;&#33334;&#34892;&#20026;&#65292;&#20294;&#23384;&#22312;&#26080;&#27861;&#34920;&#31034;&#28436;&#21464;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#33337;&#33334;&#34892;&#20026;&#39044;&#27979;&#32858;&#31867;&#65288;PC-HiV&#65289;&#30340;&#26041;&#27861;&#12290;PC-HiV&#39318;&#20808;&#20351;&#29992;&#20998;&#23618;&#34920;&#31034;&#23558;&#27599;&#26465;&#36712;&#36857;&#36716;&#25442;&#20026;&#34892;&#20026;&#24207;&#21015;&#65292;&#28982;&#21518;&#22522;&#20110;&#36825;&#20123;&#34920;&#31034;&#22312;&#27599;&#20010;&#26102;&#38388;&#25139;&#39044;&#27979;&#28436;&#21270;&#12290;&#36890;&#36807;&#24212;&#29992;&#39044;&#27979;&#32858;&#31867;&#21644;&#28508;&#22312;&#32534;&#30721;&#65292;PC-HiV&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#32858;&#31867;&#21644;&#39044;&#27979;&#12290;&#22312;&#30495;&#23454;AIS&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;PC-HiV&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25429;&#25417;&#33337;&#33334;&#34892;&#20026;&#27169;&#24335;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08838v1 Announce Type: cross  Abstract: Vessel trajectory clustering, which aims to find similar trajectory patterns, has been widely leveraged in overwater applications. Most traditional methods use predefined rules and thresholds to identify discrete vessel behaviors. They aim for high-quality clustering and conduct clustering on entire sequences, whether the original trajectory or its sub-trajectories, failing to represent their evolution. To resolve this problem, we propose a Predictive Clustering of Hierarchical Vessel Behavior (PC-HiV). PC-HiV first uses hierarchical representations to transform every trajectory into a behavioral sequence. Then, it predicts evolution at each timestamp of the sequence based on the representations. By applying predictive clustering and latent encoding, PC-HiV improves clustering and predictions simultaneously. Experiments on real AIS datasets demonstrate PC-HiV's superiority over existing methods, showcasing its effectiveness in capturin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#24490;&#29615;&#25968;&#25454;&#24182;&#34892;&#24615;&#65292;&#36890;&#36807;&#23558;&#24494;&#25209;&#37327;&#25191;&#34892;&#20174;&#21516;&#26102;&#25913;&#20026;&#39034;&#24207;&#25191;&#34892;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#24182;&#34892;&#21270;&#20013;&#28608;&#27963;&#20869;&#23384;&#23792;&#20540;&#21644;&#26799;&#24230;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#33021;&#20943;&#23569;&#25152;&#38656;GPU&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.08837</link><description>&lt;p&gt;
&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#24182;&#34892;&#21270;&#30340;&#24490;&#29615;&#25968;&#25454;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cyclic Data Parallelism for Efficient Parallelism of Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08837
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#24490;&#29615;&#25968;&#25454;&#24182;&#34892;&#24615;&#65292;&#36890;&#36807;&#23558;&#24494;&#25209;&#37327;&#25191;&#34892;&#20174;&#21516;&#26102;&#25913;&#20026;&#39034;&#24207;&#25191;&#34892;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#24182;&#34892;&#21270;&#20013;&#28608;&#27963;&#20869;&#23384;&#23792;&#20540;&#21644;&#26799;&#24230;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#33021;&#20943;&#23569;&#25152;&#38656;GPU&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#24182;&#34892;&#21270;&#25216;&#26415;&#20197;&#25193;&#23637;&#35268;&#27169;&#12290;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#22914;&#25968;&#25454;&#24182;&#34892;&#24615;&#25110;ZeRO-DP&#65292;&#24494;&#25209;&#37327;&#25968;&#25454;&#34987;&#24182;&#34892;&#22788;&#29702;&#65292;&#36825;&#20135;&#29983;&#20102;&#20004;&#20010;&#32570;&#28857;&#65306;&#22312;&#21069;&#21521;&#20256;&#36882;&#32467;&#26463;&#26102;&#27169;&#22411;&#28608;&#27963;&#25152;&#38656;&#30340;&#24635;&#20869;&#23384;&#23792;&#20540;&#65292;&#24182;&#19988;&#26799;&#24230;&#24517;&#39035;&#22312;&#21453;&#21521;&#20256;&#25773;&#27493;&#39588;&#32467;&#26463;&#26102;&#21516;&#26102;&#24179;&#22343;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#29615;&#25968;&#25454;&#24182;&#34892;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33539;&#24335;&#65292;&#23558;&#24494;&#25209;&#37327;&#30340;&#25191;&#34892;&#20174;&#21516;&#26102;&#21464;&#20026;&#39034;&#24207;&#25191;&#34892;&#65292;&#24102;&#26377;&#22343;&#21248;&#30340;&#24310;&#36831;&#12290;&#20197;&#30053;&#24494;&#26799;&#24230;&#24310;&#36831;&#20026;&#20195;&#20215;&#65292;&#28608;&#27963;&#25152;&#21344;&#30340;&#24635;&#20869;&#23384;&#26159;&#24658;&#23450;&#30340;&#65292;&#24182;&#19988;&#26799;&#24230;&#36890;&#20449;&#22312;&#35757;&#32451;&#27493;&#39588;&#26399;&#38388;&#26159;&#24179;&#34913;&#30340;&#12290;&#36890;&#36807;&#27169;&#22411;&#24182;&#34892;&#24615;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;GPU&#25968;&#37327;&#65292;&#36890;&#36807;&#22312;&#24494;&#25209;&#37327;&#20043;&#38388;&#20849;&#20139;GPU&#12290;&#22312;ZeRO-DP&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#20801;&#35768;&#20351;&#29992;&#28857;&#23545;&#28857;&#25805;&#20316;&#36827;&#34892;&#27169;&#22411;&#29366;&#24577;&#30340;&#36890;&#20449;&#65292;&#32780;&#38750; t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08837v1 Announce Type: cross  Abstract: Training large deep learning models requires parallelization techniques to scale. In existing methods such as Data Parallelism or ZeRO-DP, micro-batches of data are processed in parallel, which creates two drawbacks: the total memory required to store the model's activations peaks at the end of the forward pass, and gradients must be simultaneously averaged at the end of the backpropagation step. We propose Cyclic Data Parallelism, a novel paradigm shifting the execution of the micro-batches from simultaneous to sequential, with a uniform delay. At the cost of a slight gradient delay, the total memory taken by activations is constant, and the gradient communications are balanced during the training step. With Model Parallelism, our technique reduces the number of GPUs needed, by sharing GPUs across micro-batches. Within the ZeRO-DP framework, our technique allows communication of the model states with point-to-point operations rather t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#21307;&#23398;&#36807;&#31243;&#30417;&#27979;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#24418;&#20301;&#32622;&#32534;&#30721;&#25216;&#26415;&#34701;&#20837;&#26412;&#20307;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#33719;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08836</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#20301;&#32622;&#32534;&#30721;&#30340;&#21464;&#21387;&#22120;&#22312;&#21307;&#30103;&#36807;&#31243;&#30417;&#27979;&#20013;&#30340;&#30693;&#35782;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Structural Positional Encoding for knowledge integration in transformer-based medical process monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#21307;&#23398;&#36807;&#31243;&#30417;&#27979;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#24418;&#20301;&#32622;&#32534;&#30721;&#25216;&#26415;&#34701;&#20837;&#26412;&#20307;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#33719;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#27979;&#26159;&#19968;&#39033;&#26088;&#22312;&#39044;&#27979;&#27491;&#22312;&#36816;&#34892;&#30340;&#36807;&#31243;&#36319;&#36394;&#20449;&#24687;&#30340;&#36807;&#31243;&#25366;&#25496;&#20219;&#21153;&#65292;&#20363;&#22914;&#26368;&#27491;&#30830;&#30340;&#19979;&#19968;&#20010;&#35201;&#25191;&#34892;&#30340;&#27963;&#21160;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#27979;&#21487;&#20197;&#22312;&#38750;&#20856;&#22411;&#21644;&#38750;&#24179;&#20961;&#24773;&#20917;&#19979;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20110;&#21464;&#21387;&#22120;&#30340;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;&#22270;&#24418;&#20301;&#32622;&#32534;&#30721;&#25216;&#26415;&#34701;&#20837;&#26412;&#20307;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#25991;&#31456;&#20171;&#32461;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#22312;&#21307;&#23398;&#36807;&#31243;&#30417;&#27979;&#20013;&#27491;&#22312;&#25910;&#38598;&#30340;&#20196;&#20154;&#40723;&#33310;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08836v1 Announce Type: cross  Abstract: Predictive process monitoring is a process mining task aimed at forecasting information about a running process trace, such as the most correct next activity to be executed. In medical domains, predictive process monitoring can provide valuable decision support in atypical and nontrivial situations. Decision support and quality assessment in medicine cannot ignore domain knowledge, in order to be grounded on all the available information (which is not limited to data) and to be really acceptable by end users.   In this paper, we propose a predictive process monitoring approach relying on the use of a {\em transformer}, a deep learning architecture based on the attention mechanism. A major contribution of our work lies in the incorporation of ontological domain-specific knowledge, carried out through a graph positional encoding technique. The paper presents and discusses the encouraging experimental result we are collecting in the domai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22534;&#21472;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#36275;&#29699;&#39046;&#22495;&#20013;&#29992;&#20110;&#26816;&#27979;&#39640;&#28508;&#21147;&#29699;&#21592;&#65292;&#30456;&#27604;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08835</link><description>&lt;p&gt;
&#22522;&#20110;&#22534;&#21472;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36275;&#29699;&#29699;&#21592;&#25628;&#23547;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stacking-based deep neural network for player scouting in football 1
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22534;&#21472;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#36275;&#29699;&#39046;&#22495;&#20013;&#29992;&#20110;&#26816;&#27979;&#39640;&#28508;&#21147;&#29699;&#21592;&#65292;&#30456;&#27604;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DataScouting&#26159;&#19987;&#19994;&#20307;&#32946;&#30028;&#26368;&#30693;&#21517;&#30340;&#25968;&#25454;&#24212;&#29992;&#20043;&#19968;&#65292;&#29305;&#21035;&#26159;&#22312;&#36275;&#29699;&#39046;&#22495;&#12290;&#23427;&#30340;&#30446;&#26631;&#26159;&#20998;&#26512;&#24222;&#22823;&#30340;&#29699;&#21592;&#25968;&#25454;&#24211;&#65292;&#20197;&#20415;&#26816;&#27979;&#28508;&#21147;&#24040;&#22823;&#30340;&#29699;&#21592;&#65292;&#28982;&#21518;&#30001;&#20154;&#31867;&#29699;&#25506;&#36827;&#34892;&#36827;&#19968;&#27493;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22534;&#21472;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#39640;&#28508;&#21147;&#30340;&#36275;&#29699;&#29699;&#21592;&#12290;&#22312;&#24320;&#28304;&#25968;&#25454;&#24211;&#19978;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21462;&#24471;&#20102;&#27604;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;&#26174;&#33879;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08835v1 Announce Type: cross  Abstract: Datascouting is one of the most known data applications in professional sport, and specifically football. Its objective is to analyze huge database of players in order to detect high potentials that can be then individually considered by human scouts. In this paper, we propose a stacking-based deep learning model to detect high potential football players. Applied on open-source database, our model obtains significantly better results that classical statistical methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#34920;&#26684;&#25968;&#25454;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#32467;&#26680;&#30149;&#27835;&#30103;&#32467;&#26524;&#65292;&#24182;&#22312;&#39564;&#35777;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08834</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#32467;&#26680;&#30149;&#27835;&#30103;&#32467;&#26524;&#30340;&#39044;&#27979;&#20998;&#26512;&#65306;&#21345;&#32435;&#22612;&#20811;&#37030;&#35268;&#27169;&#30340;&#32467;&#26680;&#30149;&#25968;&#25454;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Predictive Analysis of Tuberculosis Treatment Outcomes Using Machine Learning: A Karnataka TB Data Study at a Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#34920;&#26684;&#25968;&#25454;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#32467;&#26680;&#30149;&#27835;&#30103;&#32467;&#26524;&#65292;&#24182;&#22312;&#39564;&#35777;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26680;&#30149;(TB)&#20173;&#28982;&#26159;&#20840;&#29699;&#20581;&#24247;&#23041;&#32961;&#20043;&#19968;&#65292;&#22312;&#20840;&#29699;&#33268;&#27515;&#21407;&#22240;&#20013;&#25490;&#21517;&#38752;&#21069;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#26426;&#22120;&#23398;&#20064;(ML)&#24050;&#32463;&#25104;&#20026;&#19968;&#32929;&#21464;&#38761;&#24615;&#21147;&#37327;&#65292;&#20026;&#19982;TB&#27835;&#30103;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#32467;&#26680;&#30149;(TB)&#27835;&#30103;&#32467;&#26524;&#12290;&#23427;&#23558;&#36825;&#20010;&#39044;&#27979;&#20219;&#21153;&#36716;&#21270;&#20026;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#20174;&#21360;&#24230;&#30340;&#22269;&#23478;&#32467;&#26680;&#30149;&#25511;&#21046;&#39033;&#30446;NIKSHAY&#20013;&#33719;&#21462;&#30340;&#24739;&#32773;&#25968;&#25454;&#29983;&#25104;&#39118;&#38505;&#20998;&#25968;&#65292;&#35813;&#39033;&#30446;&#21253;&#25324;&#36229;&#36807;50&#19975;&#24739;&#32773;&#35760;&#24405;&#12290;&#25968;&#25454;&#39044;&#22788;&#29702;&#26159;&#30740;&#31350;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#35813;&#27169;&#22411;&#22312;&#21253;&#21547;2&#19975;&#21517;&#24739;&#32773;&#35760;&#24405;&#30340;&#39564;&#35777;&#38598;&#19978;&#23454;&#29616;&#20102;98%&#30340;&#21484;&#22238;&#29575;&#21644;0.95&#30340;AUC-ROC&#20998;&#25968;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#22312;&#25913;&#36827;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#32463;&#36807;&#20102;&#21508;&#31181;&#32771;&#34385;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08834v1 Announce Type: cross  Abstract: Tuberculosis (TB) remains a global health threat, ranking among the leading causes of mortality worldwide. In this context, machine learning (ML) has emerged as a transformative force, providing innovative solutions to the complexities associated with TB treatment.This study explores how machine learning, especially with tabular data, can be used to predict Tuberculosis (TB) treatment outcomes more accurately. It transforms this prediction task into a binary classification problem, generating risk scores from patient data sourced from NIKSHAY, India's national TB control program, which includes over 500,000 patient records.   Data preprocessing is a critical component of the study, and the model achieved an recall of 98% and an AUC-ROC score of 0.95 on the validation set, which includes 20,000 patient records.We also explore the use of Natural Language Processing (NLP) for improved model learning. Our results, corroborated by various m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20195;&#29702;&#65292;&#24182;&#20171;&#32461;&#20102;&#24605;&#32771;&#12289;&#20114;&#21160;&#21644;&#34892;&#21160;(TINA)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#30340;&#38382;&#31572;&#27169;&#22359;&#24357;&#34917;&#20102;&#29615;&#22659;&#24863;&#30693;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.08833</link><description>&lt;p&gt;
TINA: &#38646;&#26679;&#26412;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#30340;&#24605;&#32771;&#12289;&#20132;&#20114;&#21644;&#34892;&#21160;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20195;&#29702;&#65292;&#24182;&#20171;&#32461;&#20102;&#24605;&#32771;&#12289;&#20114;&#21160;&#21644;&#34892;&#21160;(TINA)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#30340;&#38382;&#31572;&#27169;&#22359;&#24357;&#34917;&#20102;&#29615;&#22659;&#24863;&#30693;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23548;&#33322;&#26159;&#35270;&#35273;-&#35821;&#35328;&#23548;&#33322;(VLN)&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#36825;&#37324;&#36866;&#24212;&#38476;&#29983;&#25351;&#20196;&#24182;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#34892;&#21160;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#22686;&#24378;&#23398;&#20064;&#20351;&#29992;&#24102;&#26631;&#27880;&#25968;&#25454;&#35757;&#32451;&#30340;&#29616;&#26377;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#20854;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#32039;&#24613;&#25512;&#29702;&#33021;&#21147;&#65292;&#20026;&#23454;&#29616;&#38646;&#26679;&#26412;&#23548;&#33322;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;VLN&#20195;&#29702;&#65292;&#25506;&#32034;&#20102;&#35299;&#20915;&#38646;&#26679;&#26412;&#23548;&#33322;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#20026;&#24357;&#34917;LLMs&#22312;&#29615;&#22659;&#24863;&#30693;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24605;&#32771;&#12289;&#20114;&#21160;&#21644;&#34892;&#21160;(TINA)&#26694;&#26550;&#12290;TINA&#20351;&#20195;&#29702;&#33021;&#22815;&#23457;&#26597;&#24863;&#30693;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#30340;&#38382;&#31572;&#27169;&#22359;&#33258;&#20027;&#26597;&#35810;&#29615;&#22659;&#20013;&#30340;&#20851;&#38190;&#32447;&#32034;&#65292;&#20174;&#32780;&#23558;&#25351;&#20196;&#19982;&#29615;&#22659;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08833v1 Announce Type: cross  Abstract: Zero-shot navigation is a critical challenge in Vision-Language Navigation (VLN) tasks, where the ability to adapt to unfamiliar instructions and to act in unknown environments is essential. Existing supervised learning-based models, trained using annotated data through reinforcement learning, exhibit limitations in generalization capabilities. Large Language Models (LLMs), with their extensive knowledge and emergent reasoning abilities, present a potential pathway for achieving zero-shot navigation. This paper presents a VLN agent based on LLMs, exploring approaches to the zero-shot navigation problem. To compensate for the shortcomings of LLMs in environmental perception, we propose the Thinking, Interacting, and Action (TINA) framework. TINA enables the agent to scrutinize perceptual information and autonomously query key clues within the environment through an introduced question-answering module, thereby aligning instructions with
&lt;/p&gt;</description></item><item><title>&#20154;&#20204;&#20250;&#32473;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#36171;&#20104;&#30446;&#30340;&#23646;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#35299;&#37322;&#21644;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#26102;&#34920;&#29616;&#20986;&#23545;&#30446;&#30340;&#35770;&#35299;&#37322;&#30340;&#20542;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.08828</link><description>&lt;p&gt;
&#24403;&#35299;&#37322;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#26102;&#65292;&#20154;&#20204;&#20250;&#32473;&#20104;&#20854;&#23646;&#24615;&#30446;&#30340;
&lt;/p&gt;
&lt;p&gt;
People Attribute Purpose to Autonomous Vehicles When Explaining Their Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08828
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#20250;&#32473;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#36171;&#20104;&#30446;&#30340;&#23646;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#35299;&#37322;&#21644;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#26102;&#34920;&#29616;&#20986;&#23545;&#30446;&#30340;&#35770;&#35299;&#37322;&#30340;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27454;&#20248;&#31168;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26631;&#24535;&#26159;&#29992;&#25143;&#21487;&#20197;&#29702;&#35299;&#24182;&#37319;&#21462;&#34892;&#21160;&#30340;&#35299;&#37322;&#12290;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#38656;&#35201;&#31995;&#32479;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#22240;&#26524;&#25110;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#35748;&#30693;&#31185;&#23398;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#29992;&#25143;&#21487;&#33021;&#26399;&#26395;&#30340;&#35299;&#37322;&#31867;&#22411;&#65292;&#20197;&#21450;&#22312;&#21738;&#31181;&#26684;&#24335;&#19979;&#21576;&#29616;&#36825;&#20123;&#35299;&#37322;&#12290;&#26412;&#25991;&#31616;&#35201;&#22238;&#39038;&#20102;&#35748;&#30693;&#31185;&#23398;&#35299;&#37322;&#26041;&#38754;&#30340;&#30456;&#20851;&#25991;&#29486;&#65292;&#29305;&#21035;&#20851;&#27880;&#30446;&#30340;&#35770;&#65292;&#21363;&#20197;&#36798;&#21040;&#30446;&#30340;&#20026;&#35299;&#37322;&#20915;&#31574;&#30340;&#20542;&#21521;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#20154;&#20204;&#22914;&#20309;&#20026;&#33258;&#20027;&#36710;&#36742;&#30340;&#34892;&#20026;&#20135;&#29983;&#35299;&#37322;&#20197;&#21450;&#20182;&#20204;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#30340;&#32463;&#39564;&#25968;&#25454;&#12290;&#22312;&#31532;&#19968;&#39033;&#35843;&#26597;&#20013;&#65292;&#21442;&#19982;&#32773;&#65288;n = 54&#65289;&#35266;&#30475;&#20102;&#36947;&#36335;&#22330;&#26223;&#30340;&#35270;&#39057;&#65292;&#24182;&#34987;&#35201;&#27714;&#20026;&#36710;&#36742;&#30340;&#34892;&#20026;&#29983;&#25104;&#26426;&#26800;&#30340;&#12289;&#21453;&#20107;&#23454;&#30340;&#25110;&#30446;&#30340;&#35770;&#30340;&#35328;&#35821;&#35299;&#37322;&#12290;&#22312;&#31532;&#20108;&#39033;&#35843;&#26597;&#20013;&#65292;&#21478;&#19968;&#32452;&#21442;&#19982;&#32773;&#65288;n = 356&#65289;&#23545;&#36825;&#20123;&#36827;&#34892;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08828v1 Announce Type: cross  Abstract: A hallmark of a good XAI system is explanations that users can understand and act on. In many cases, this requires a system to offer causal or counterfactual explanations that are intelligible. Cognitive science can help us understand what kinds of explanations users might expect, and in which format to frame these explanations. We briefly review relevant literature from the cognitive science of explanation, particularly as it concerns teleology, the tendency to explain a decision in terms of the purpose it was meant to achieve. We then report empirical data on how people generate explanations for the behavior of autonomous vehicles, and how they evaluate these explanations. In a first survey, participants (n=54) were shown videos of a road scene and asked to generate either mechanistic, counterfactual, or teleological verbal explanations for a vehicle's actions. In the second survey, a different set of participants (n=356) rated these
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#26159;&#31532;&#19968;&#27425;&#21516;&#26102;&#25506;&#32034;&#29992;&#20110;&#20998;&#26512;&#21387;&#21147;&#12289;&#25233;&#37057;&#21644;&#25237;&#20837;&#24230;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#12289;&#36755;&#20837;&#27169;&#24577;&#12289;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.08824</link><description>&lt;p&gt;
&#34913;&#37327;&#38750;&#20856;&#22411;&#24773;&#32490;&#23545;&#24515;&#29702;&#20581;&#24247;&#30340;&#24433;&#21709;&#65306;&#35745;&#31639;&#26041;&#27861;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Measuring Non-Typical Emotions for Mental Health: A Survey of Computational Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#26159;&#31532;&#19968;&#27425;&#21516;&#26102;&#25506;&#32034;&#29992;&#20110;&#20998;&#26512;&#21387;&#21147;&#12289;&#25233;&#37057;&#21644;&#25237;&#20837;&#24230;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#12289;&#36755;&#20837;&#27169;&#24577;&#12289;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20856;&#22411;&#24773;&#32490;&#65288;&#22914;&#21387;&#21147;&#12289;&#25233;&#37057;&#21644;&#25237;&#20837;&#24230;&#20998;&#26512;&#65289;&#30456;&#36739;&#20110;&#32463;&#24120;&#35752;&#35770;&#30340;&#24773;&#32490;&#65288;&#22914;&#24555;&#20048;&#12289;&#24754;&#20260;&#12289;&#24656;&#24807;&#21644;&#24868;&#24594;&#65289;&#26469;&#35828;&#65292;&#26356;&#20026;&#32597;&#35265;&#19988;&#22797;&#26434;&#12290;&#30001;&#20110;&#23545;&#24515;&#29702;&#20581;&#24247;&#21644;&#24184;&#31119;&#30340;&#24433;&#21709;&#65292;&#20154;&#20204;&#23545;&#36825;&#20123;&#38750;&#20856;&#22411;&#24773;&#32490;&#30340;&#37325;&#35201;&#24615;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#12290;&#21387;&#21147;&#21644;&#25233;&#37057;&#24433;&#21709;&#20102;&#26085;&#24120;&#20219;&#21153;&#30340;&#21442;&#19982;&#65292;&#31361;&#26174;&#20102;&#29702;&#35299;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#35843;&#26597;&#26159;&#31532;&#19968;&#27425;&#21516;&#26102;&#25506;&#32034;&#29992;&#20110;&#20998;&#26512;&#21387;&#21147;&#12289;&#25233;&#37057;&#21644;&#25237;&#20837;&#24230;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#29992;&#20110;&#35745;&#31639;&#20998;&#26512;&#21387;&#21147;&#12289;&#25233;&#37057;&#21644;&#25237;&#20837;&#24230;&#30340;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#12289;&#36755;&#20837;&#27169;&#24577;&#12289;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#38750;&#20856;&#22411;&#24773;&#32490;&#20998;&#26512;&#26041;&#27861;&#30340;&#26102;&#38388;&#34920;&#21644;&#20998;&#31867;&#27861;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#36890;&#29992;&#27969;&#31243;&#21644;&#31867;&#21035;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08824v1 Announce Type: cross  Abstract: Analysis of non-typical emotions, such as stress, depression and engagement is less common and more complex compared to that of frequently discussed emotions like happiness, sadness, fear, and anger. The importance of these non-typical emotions has been increasingly recognized due to their implications on mental health and well-being. Stress and depression impact the engagement in daily tasks, highlighting the need to understand their interplay. This survey is the first to simultaneously explore computational methods for analyzing stress, depression, and engagement. We discuss the most commonly used datasets, input modalities, data processing techniques, and information fusion methods used for the computational analysis of stress, depression and engagement. A timeline and taxonomy of non-typical emotion analysis approaches along with their generic pipeline and categories are presented. Subsequently, we describe state-of-the-art computa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#26041;&#38754;&#30340;&#33203;&#39135;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;Diet-ODIN&#26694;&#26550;&#65292;&#26088;&#22312;&#25506;&#32034;&#33203;&#39135;&#27169;&#24335;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2403.08820</link><description>&lt;p&gt;
Diet-ODIN&#65306;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#24615;&#33203;&#39135;&#27169;&#24335;&#19979;&#30340;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Diet-ODIN: A Novel Framework for Opioid Misuse Detection with Interpretable Dietary Patterns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#26041;&#38754;&#30340;&#33203;&#39135;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;Diet-ODIN&#26694;&#26550;&#65292;&#26088;&#22312;&#25506;&#32034;&#33203;&#39135;&#27169;&#24335;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#29255;&#31867;&#33647;&#29289;&#21361;&#26426;&#19968;&#30452;&#26159;&#32654;&#22269;&#31038;&#20250;&#26368;&#20851;&#38190;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#23613;&#31649;&#33647;&#29289;&#36741;&#21161;&#27835;&#30103;&#65288;MAT&#65289;&#34987;&#35748;&#20026;&#26159;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#21644;&#25104;&#30270;&#26368;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20294;&#21508;&#31181;&#21103;&#20316;&#29992;&#21487;&#33021;&#24341;&#21457;&#38463;&#29255;&#31867;&#33647;&#29289;&#20877;&#27425;&#28389;&#29992;&#12290;&#38500;MAT&#22806;&#65292;&#33203;&#39135;&#33829;&#20859;&#24178;&#39044;&#22312;&#38450;&#27490;&#21644;&#24247;&#22797;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26377;&#20851;&#33203;&#39135;&#27169;&#24335;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#20043;&#38388;&#20196;&#20154;&#25285;&#24551;&#30340;&#20851;&#32852;&#30340;&#30740;&#31350;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#39318;&#27425;&#24314;&#31435;&#20102;&#19968;&#20010;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#32773;&#30456;&#20851;&#30340;&#22823;&#35268;&#27169;&#22810;&#26041;&#38754;&#33203;&#39135;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#26694;&#26550;&#65292;&#21363;&#35299;&#37322;&#24615;&#33203;&#39135;&#27169;&#24335;&#19979;&#30340;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#26816;&#27979;&#65288;Diet-ODIN&#65289;&#65292;&#29992;&#20110;&#36830;&#25509;&#24322;&#36136;&#22270;&#65288;HG&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;&#35782;&#21035;&#38463;&#29255;&#31867;&#33647;&#29289;&#35823;&#29992;&#32773;&#24182;&#35299;&#37322;&#20854;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08820v1 Announce Type: cross  Abstract: The opioid crisis has been one of the most critical society concerns in the United States. Although the medication assisted treatment (MAT) is recognized as the most effective treatment for opioid misuse and addiction, the various side effects can trigger opioid relapse. In addition to MAT, the dietary nutrition intervention has been demonstrated its importance in opioid misuse prevention and recovery. However, research on the alarming connections between dietary patterns and opioid misuse remain under-explored. In response to this gap, in this paper, we first establish a large-scale multifaceted dietary benchmark dataset related to opioid users at the first attempt and then develop a novel framework - i.e., namely Opioid Misuse Detection with Interpretable Dietary Patterns (Diet-ODIN) - to bridge heterogeneous graph (HG) and large language model (LLM) for the identification of users with opioid misuse and the interpretation of their a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MINGLE&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#32423;&#27880;&#20837;&#31574;&#30053;&#23558;&#21307;&#23398;&#27010;&#24565;&#35821;&#20041;&#21644;&#20020;&#24202;&#31508;&#35760;&#35821;&#20041;&#34701;&#21512;&#21040;&#36229;&#22270;&#20013;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#32467;&#26500;&#21644;&#35821;&#20041;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.08818</link><description>&lt;p&gt;
&#32467;&#26500;&#21644;&#35821;&#20041;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22810;&#27169;&#24577;&#34701;&#21512;&#65306;&#23558;&#20020;&#24202;&#35760;&#24405;&#21644;&#31508;&#35760;&#19982;&#36229;&#22270;&#21644;LLM&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08818
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MINGLE&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#32423;&#27880;&#20837;&#31574;&#30053;&#23558;&#21307;&#23398;&#27010;&#24565;&#35821;&#20041;&#21644;&#20020;&#24202;&#31508;&#35760;&#35821;&#20041;&#34701;&#21512;&#21040;&#36229;&#22270;&#20013;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#32467;&#26500;&#21644;&#35821;&#20041;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#22312;&#36817;&#20960;&#21313;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;EHRs&#36890;&#24120;&#21253;&#21547;&#24322;&#26500;&#20449;&#24687;&#65292;&#22914;&#34920;&#26684;&#24418;&#24335;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#25991;&#26412;&#31508;&#35760;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;EHRs&#20013;&#30340;&#19981;&#21516;&#31867;&#22411;&#20449;&#24687;&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#65292;&#25552;&#20379;&#24739;&#32773;&#20581;&#24247;&#29366;&#24577;&#30340;&#26356;&#23436;&#25972;&#22270;&#29255;&#12290;&#23613;&#31649;&#23545;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#19981;&#21516;&#31867;&#22411;EHR&#25968;&#25454;&#30340;&#34701;&#21512;&#65288;&#22810;&#27169;&#24577;&#34701;&#21512;&#65289;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#21307;&#30103;&#32534;&#30721;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#20070;&#38754;&#31508;&#35760;&#20013;&#23384;&#22312;&#30340;&#22122;&#38899;&#21644;&#20887;&#20313;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MINGLE&#30340;&#26032;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#23558;EHR&#20013;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#20004;&#32423;&#27880;&#20837;&#31574;&#30053;&#23558;&#21307;&#23398;&#27010;&#24565;&#35821;&#20041;&#21644;&#20020;&#24202;&#31508;&#35760;&#35821;&#20041;&#34701;&#21512;&#21040;&#36229;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08818v1 Announce Type: cross  Abstract: Electronic Health Records (EHRs) have become increasingly popular to support clinical decision-making and healthcare in recent decades. EHRs usually contain heterogeneous information, such as structural data in tabular form and unstructured data in textual notes. Different types of information in EHRs can complement each other and provide a more complete picture of the health status of a patient. While there has been a lot of research on representation learning of structured EHR data, the fusion of different types of EHR data (multimodal fusion) is not well studied. This is mostly because of the complex medical coding systems used and the noise and redundancy present in the written notes. In this work, we propose a new framework called MINGLE, which integrates both structures and semantics in EHR effectively. Our framework uses a two-level infusion strategy to combine medical concept semantics and clinical note semantics into hypergrap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#20010;&#32852;&#21512;&#28145;&#24230;Q&#23398;&#20064;&#36127;&#36733;&#24179;&#34913;&#31995;&#32479;&#65292;&#21033;&#29992;Open-RAN xAPP&#26694;&#26550;&#21644;&#36817;&#23454;&#26102;&#23556;&#39057;&#25509;&#21475;&#25511;&#21046;&#22120;&#65288;near-RT RIC&#65289;&#23454;&#26045;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#30446;&#21069;UE&#20351;&#29992;&#30340;&#26368;&#22823;&#20449;&#22122;&#27604;&#65288;MAX-SINR&#65289;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#25345;&#32493;&#25552;&#20379;&#26356;&#22909;&#30340;&#36127;&#36733;&#24179;&#34913;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08813</link><description>&lt;p&gt;
&#32852;&#21512;&#28145;&#24230;Q&#23398;&#20064;&#19982;5G&#36127;&#36733;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Federated Deep Q-Learning and 5G load balancing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#20010;&#32852;&#21512;&#28145;&#24230;Q&#23398;&#20064;&#36127;&#36733;&#24179;&#34913;&#31995;&#32479;&#65292;&#21033;&#29992;Open-RAN xAPP&#26694;&#26550;&#21644;&#36817;&#23454;&#26102;&#23556;&#39057;&#25509;&#21475;&#25511;&#21046;&#22120;&#65288;near-RT RIC&#65289;&#23454;&#26045;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#30446;&#21069;UE&#20351;&#29992;&#30340;&#26368;&#22823;&#20449;&#22122;&#27604;&#65288;MAX-SINR&#65289;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#25345;&#32493;&#25552;&#20379;&#26356;&#22909;&#30340;&#36127;&#36733;&#24179;&#34913;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#34562;&#31389;&#32593;&#32476;&#25216;&#26415;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#22522;&#31449;&#65288;BS&#65289;&#36127;&#36733;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#20037;&#24615;&#38382;&#39064;&#12290;&#34429;&#28982;&#38598;&#20013;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#65292;&#20294;&#20173;&#28982;&#26159;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#32852;&#21512;&#28145;&#24230;Q&#23398;&#20064;&#26469;&#36890;&#30693;&#27599;&#20010;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#21508;&#20010;BS&#30340;&#36127;&#36733;&#24773;&#20917;&#12290;&#32852;&#21512;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#36127;&#36733;&#24179;&#34913;&#20351;&#24471;&#26234;&#33021;UE&#21487;&#20197;&#29420;&#31435;&#36873;&#25321;&#26368;&#20339;BS&#65292;&#21516;&#26102;&#38480;&#21046;&#20102;&#21521;&#32593;&#32476;&#26292;&#38706;&#30340;&#31169;&#20154;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08813v1 Announce Type: cross  Abstract: Despite advances in cellular network technology, base station (BS) load balancing remains a persistent problem. Although centralized resource allocation methods can address the load balancing problem, it still remains an NP-hard problem. In this research, we study how federated deep Q learning can be used to inform each user equipment (UE) of the each BS's load conditions. Federated deep Q learning's load balancing enables intelligent UEs to independently select the best BS while also limiting the amount of private information exposed to the network.   In this study, we propose and analyze a federated deep Q learning load balancing system, which is implemented using the Open-RAN xAPP framework and the near-Real Time Radio Interface Controller (near-RT RIC). Our simulation results indicate that compared to the maximum Signal-To-Noise-Ratio (MAX-SINR) method currently used by UEs, our proposed deep Q learning model can consistently provi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20302;&#25104;&#26412;&#36793;&#32536;&#29289;&#32852;&#32593;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20272;&#35745;&#23460;&#20869;&#29615;&#22659;&#36136;&#37327;&#21442;&#25968;&#30340;&#36731;&#37327;&#32423;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#29289;&#32852;&#32593;&#26550;&#26500;&#20013;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#22788;&#29702;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.08810</link><description>&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#26550;&#26500;&#20013;&#27604;&#36739;&#36793;&#32536;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20272;&#35745;&#23460;&#20869;&#29615;&#22659;&#21442;&#25968;&#19982;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Comparison of edge computing methods in Internet of Things architectures for efficient estimation of indoor environmental parameters with Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20302;&#25104;&#26412;&#36793;&#32536;&#29289;&#32852;&#32593;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20272;&#35745;&#23460;&#20869;&#29615;&#22659;&#36136;&#37327;&#21442;&#25968;&#30340;&#36731;&#37327;&#32423;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#29289;&#32852;&#32593;&#26550;&#26500;&#20013;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#22788;&#29702;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#29289;&#32852;&#35774;&#22791;&#25968;&#37327;&#30340;&#22823;&#24133;&#22686;&#21152;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#25968;&#25454;&#22788;&#29702;&#26041;&#24335;&#65292;&#21152;&#19978;&#30446;&#21069;&#20174;&#20113;&#35745;&#31639;&#36716;&#21521;&#36793;&#32536;&#35745;&#31639;&#30340;&#36235;&#21183;&#36843;&#20351;&#25105;&#20204;&#38656;&#35201;&#20351;&#29992;&#33021;&#28304;&#39640;&#25928;&#30340;&#35774;&#22791;&#22312;&#25968;&#25454;&#28304;&#38468;&#36817;&#36827;&#34892;&#39640;&#25928;&#21487;&#38752;&#30340;&#25968;&#25454;&#22788;&#29702;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20302;&#25104;&#26412;&#36793;&#32536;&#29289;&#32852;&#32593;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#36731;&#37327;&#32423;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20197;&#20272;&#35745;&#23460;&#20869;&#29615;&#22659;&#36136;&#37327;&#21442;&#25968;&#65292;&#27604;&#22914;&#22810;&#23618;&#24863;&#30693;&#22120;&#31867;&#22411;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#38598;&#20013;&#24335;&#21644;&#20998;&#24067;&#24335;&#24182;&#34892;&#29289;&#32852;&#32593;&#26550;&#26500;&#23454;&#29616;&#65292;&#36890;&#36807;&#26080;&#32447;&#36830;&#25509;&#65292;&#20849;&#20139;&#21830;&#29992;&#27169;&#22359;&#36827;&#34892;&#25968;&#25454;&#37319;&#38598;&#21644;&#20256;&#24863;&#65292;&#27604;&#22914;&#28201;&#24230;&#12289;&#28287;&#24230;&#12289;&#29031;&#24230;&#12289;CO2&#21644;&#20854;&#20182;&#27668;&#20307;&#20256;&#24863;&#22120;&#12290;&#38598;&#20013;&#24335;&#26041;&#27861;&#20351;&#29992;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#21644;&#28040;&#24687;&#38431;&#21015;&#36965;&#27979;&#20256;&#36755;&#21327;&#35758;&#65292;&#32780;&#20998;&#24067;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08810v1 Announce Type: cross  Abstract: The large increase in the number of Internet of Things (IoT) devices have revolutionised the way data is processed, which added to the current trend from cloud to edge computing has resulted in the need for efficient and reliable data processing near the data sources using energy-efficient devices. Two methods based on low-cost edge-IoT architectures are proposed to implement lightweight Machine Learning (ML) models that estimate indoor environmental quality (IEQ) parameters, such as Artificial Neural Networks of Multilayer Perceptron type. Their implementation is based on centralised and distributed parallel IoT architectures, connected via wireless, which share commercial off-the-self modules for data acquisition and sensing, such as sensors for temperature, humidity, illuminance, CO2, and other gases. The centralised method uses a Graphics Processing Unit and the Message Queuing Telemetry Transport protocol, but the distributed meth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20223;&#29983;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#38271;&#36317;&#31163;&#27700;&#19979;&#23548;&#33322;&#26041;&#27861;&#65292;&#21033;&#29992;&#22320;&#30913;&#25968;&#25454;&#36827;&#34892;&#23548;&#33322;&#24182;&#36890;&#36807;&#24320;&#21457;&#26426;&#21046;&#21644;&#27169;&#22411;&#26657;&#20934;&#26469;&#22686;&#24378;&#25239;&#24322;&#24120;&#24615;</title><link>https://arxiv.org/abs/2403.08808</link><description>&lt;p&gt;
&#19968;&#31181;&#20223;&#29983;&#25968;&#25454;&#39537;&#21160;&#38271;&#36317;&#31163;&#27700;&#19979;&#23548;&#33322;&#26041;&#27861;&#21450;&#20854;&#25239;&#24322;&#24120;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Bionic Data-driven Approach for Long-distance Underwater Navigation with Anomaly Resistance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08808
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20223;&#29983;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#38271;&#36317;&#31163;&#27700;&#19979;&#23548;&#33322;&#26041;&#27861;&#65292;&#21033;&#29992;&#22320;&#30913;&#25968;&#25454;&#36827;&#34892;&#23548;&#33322;&#24182;&#36890;&#36807;&#24320;&#21457;&#26426;&#21046;&#21644;&#27169;&#22411;&#26657;&#20934;&#26469;&#22686;&#24378;&#25239;&#24322;&#24120;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#21160;&#29289;&#21033;&#29992;&#29615;&#22659;&#32447;&#32034;&#23637;&#29616;&#20102;&#20934;&#30830;&#23548;&#33322;&#30340;&#33021;&#21147;&#12290;&#22320;&#29699;&#30340;&#30913;&#22330;&#24050;&#34987;&#35777;&#26126;&#26159;&#38271;&#36317;&#31163;&#21160;&#29289;&#36801;&#24473;&#20013;&#21487;&#38752;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#21463;&#21160;&#29289;&#23548;&#33322;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#36317;&#31163;&#27700;&#19979;&#23548;&#33322;&#30340;&#20223;&#29983;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#27979;&#37327;&#30340;&#22320;&#30913;&#25968;&#25454;&#36827;&#34892;&#23548;&#33322;&#65292;&#26080;&#38656;GPS&#31995;&#32479;&#25110;&#22320;&#29702;&#22320;&#22270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26500;&#24314;&#21644;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#21147;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;TA-LSTM&#65289;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#23548;&#33322;&#36807;&#31243;&#20013;&#39044;&#27979;&#33322;&#21521;&#35282;&#12290;&#20026;&#20102;&#20943;&#36731;&#22320;&#30913;&#24322;&#24120;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#26816;&#27979;&#21644;&#37327;&#21270;&#24322;&#24120;&#12290;&#25105;&#20204;&#23558;&#24320;&#21457;&#30340;&#26426;&#21046;&#19982;TA-LSTM&#38598;&#25104;&#65292;&#24182;&#26657;&#20934;&#39044;&#27979;&#30340;&#33322;&#21521;&#35282;&#20197;&#22686;&#24378;&#23545;&#22320;&#30913;&#24322;&#24120;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;&#21033;&#29992;&#20174;WMM&#27169;&#22411;&#33719;&#21462;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08808v1 Announce Type: cross  Abstract: Various animals exhibit accurate navigation using environment cues. The Earth's magnetic field has been proved a reliable information source in long-distance fauna migration. Inspired by animal navigation, this work proposes a bionic and data-driven approach for long-distance underwater navigation. The proposed approach uses measured geomagnetic data for the navigation, and requires no GPS systems or geographical maps. Particularly, we construct and train a Temporal Attention-based Long Short-Term Memory (TA-LSTM) network to predict the heading angle during the navigation. To mitigate the impact of geomagnetic anomalies, we develop the mechanism to detect and quantify the anomalies based on Maximum Likelihood Estimation. We integrate the developed mechanism with the TA-LSTM, and calibrate the predicted heading angles to gain resistance against geomagnetic anomalies. Using the retrieved data from the WMM model, we conduct numerical simu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#30830;&#21363;&#26102;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#65292;&#32467;&#21512;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#24605;&#24819;&#20197;&#22686;&#24378;&#21363;&#26102;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.08807</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26377;&#25928;&#21363;&#26102;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Effective anytime algorithm for multiobjective combinatorial optimization problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#30830;&#21363;&#26102;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#65292;&#32467;&#21512;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#24605;&#24819;&#20197;&#22686;&#24378;&#21363;&#26102;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#65292;&#20248;&#21270;&#31639;&#27861;&#30340;&#32467;&#26524;&#26159;&#19968;&#32452;&#26377;&#25928;&#35299;&#65292;&#20915;&#31574;&#32773;&#20174;&#20013;&#36873;&#25321;&#19968;&#20010;&#12290;&#24120;&#35265;&#24773;&#20917;&#26159;&#65292;&#24182;&#38750;&#25152;&#26377;&#26377;&#25928;&#35299;&#37117;&#33021;&#22312;&#30701;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26469;&#65292;&#25628;&#32034;&#31639;&#27861;&#24517;&#39035;&#36807;&#26089;&#20572;&#27490;&#20197;&#20998;&#26512;&#21040;&#30446;&#21069;&#20026;&#27490;&#25214;&#21040;&#30340;&#35299;&#12290;&#20915;&#31574;&#32773;&#26356;&#21916;&#27426;&#22312;&#23458;&#35266;&#31354;&#38388;&#20013;&#20998;&#24067;&#33391;&#22909;&#30340;&#19968;&#32452;&#26377;&#25928;&#35299;&#65292;&#20197;&#25552;&#20379;&#21508;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#21482;&#26377;&#23569;&#25968;&#20960;&#31181;&#30830;&#20999;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#20219;&#20309;&#26102;&#21051;&#25552;&#20379;&#36825;&#26679;&#19968;&#32452;&#20998;&#24067;&#33391;&#22909;&#30340;&#35299;&#65306;&#25105;&#20204;&#31216;&#20043;&#20026;&#21363;&#26102;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#30830;&#21363;&#26102;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#65292;&#32467;&#21512;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#24605;&#24819;&#20197;&#22686;&#24378;&#21363;&#26102;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#19982;&#29616;&#26377;&#20110;&#24403;&#20170;&#26368;&#20808;&#36827;&#30340;&#21363;&#26102;&#22810;&#30446;&#26631;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20351;&#29992;&#19968;&#32452;480&#20010;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08807v1 Announce Type: cross  Abstract: In multiobjective optimization, the result of an optimization algorithm is a set of efficient solutions from which the decision maker selects one. It is common that not all the efficient solutions can be computed in a short time and the search algorithm has to be stopped prematurely to analyze the solutions found so far. A set of efficient solutions that are well-spread in the objective space is preferred to provide the decision maker with a great variety of solutions. However, just a few exact algorithms in the literature exist with the ability to provide such a well-spread set of solutions at any moment: we call them anytime algorithms. We propose a new exact anytime algorithm for multiobjective combinatorial optimization combining three novel ideas to enhance the anytime behavior. We compare the proposed algorithm with those in the state-of-the-art for anytime multiobjective combinatorial optimization using a set of 480 instances fr
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#26377;&#20851;&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27835;&#29702;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.08802</link><description>&lt;p&gt;
&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;
&lt;/p&gt;
&lt;p&gt;
Governance of Generative Artificial Intelligence for Companies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#26377;&#20851;&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27835;&#29702;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#65292;&#29305;&#21035;&#26159;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#36805;&#36895;&#36827;&#20837;&#20225;&#19994;&#65292;&#20294;&#32570;&#20047;&#20805;&#20998;&#30340;&#27835;&#29702;&#65292;&#24102;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#23613;&#31649;&#23545;GenAI&#20855;&#26377;&#21464;&#38761;&#24615;&#36136;&#21644;&#30417;&#31649;&#25514;&#26045;&#30340;&#24191;&#27867;&#35752;&#35770;&#65292;&#20294;&#26377;&#38480;&#30340;&#30740;&#31350;&#28041;&#21450;&#32452;&#32455;&#27835;&#29702;&#65292;&#21253;&#25324;&#25216;&#26415;&#21644;&#19994;&#21153;&#35270;&#35282;&#12290;&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#12290;&#23427;&#19981;&#20165;&#20165;&#26159;&#24635;&#32467;&#65292;&#36824;&#36890;&#36807;&#21046;&#23450;&#36866;&#29992;&#20110;&#20225;&#19994;&#20869;&#30340;GenAI&#27835;&#29702;&#26694;&#26550;&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35814;&#32454;&#25551;&#36848;&#20102;&#33539;&#22260;&#12289;&#30446;&#26631;&#21644;&#27835;&#29702;&#26426;&#21046;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#19987;&#27880;&#20110;GenAI&#27835;&#29702;&#30340;&#26041;&#27861;&#65292;&#20026;&#20225;&#19994;&#22312;&#36127;&#36131;&#20219;&#30340;AI&#37319;&#29992;&#25361;&#25112;&#20013;&#25552;&#20379;&#20102;&#23454;&#29992;&#35265;&#35299;&#12290;&#23545;&#20110;&#25216;&#26415;&#20154;&#21592;&#26469;&#35828;&#65292;&#20063;&#26377;&#21161;&#20110;&#25299;&#23485;&#20182;&#20204;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08802v1 Announce Type: new  Abstract: Generative Artificial Intelligence (GenAI), specifically large language models like ChatGPT, has swiftly entered organizations without adequate governance, posing both opportunities and risks. Despite extensive debates on GenAI's transformative nature and regulatory measures, limited research addresses organizational governance, encompassing technical and business perspectives. This review paper fills this gap by surveying recent works. It goes beyond mere summarization by developing a framework for GenAI governance within companies. Our framework outlines the scope, objectives, and governance mechanisms tailored to harness business opportunities and mitigate risks associated with GenAI integration. This research contributes a focused approach to GenAI governance, offering practical insights for companies navigating the challenges of responsible AI adoption. It is also valuable for a technical audience to broaden their perspective as inc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#36827;&#21270;&#31639;&#27861;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26041;&#27861;&#26469;&#25193;&#23637;&#33258;&#28982;&#34507;&#30333;&#36136;&#8220;&#35789;&#27719;&#37327;&#8221;&#65292;&#20197;&#24320;&#21457;&#20174;&#26410;&#23384;&#22312;&#36807;&#30340;&#20840;&#26032;&#34507;&#30333;&#36136;&#12290;</title><link>https://arxiv.org/abs/2403.08797</link><description>&lt;p&gt;
&#27169;&#25311;&#20998;&#23376;&#36827;&#21270;&#30340;&#36827;&#21270;&#31639;&#27861;&#65306;&#19968;&#20010;&#26032;&#39046;&#22495;&#30340;&#25552;&#35758;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Algorithms Simulating Molecular Evolution: A New Field Proposal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08797
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#36827;&#21270;&#31639;&#27861;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26041;&#27861;&#26469;&#25193;&#23637;&#33258;&#28982;&#34507;&#30333;&#36136;&#8220;&#35789;&#27719;&#37327;&#8221;&#65292;&#20197;&#24320;&#21457;&#20174;&#26410;&#23384;&#22312;&#36807;&#30340;&#20840;&#26032;&#34507;&#30333;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#21629;&#27963;&#21160;&#30340;&#22522;&#26412;&#21151;&#33021;&#22522;&#22240;&#34013;&#22270;&#32534;&#30721;&#22312;DNA&#20013;&#65292;&#36716;&#35793;&#20026;&#34507;&#30333;&#36136;--&#25512;&#21160;&#22823;&#22810;&#25968;&#20195;&#35874;&#36807;&#31243;&#30340;&#24341;&#25806;&#12290;&#26368;&#36817;&#30340;&#22522;&#22240;&#32452;&#27979;&#24207;&#36827;&#23637;&#25581;&#31034;&#20102;&#22823;&#37327;&#30340;&#34507;&#30333;&#36136;&#23478;&#26063;&#65292;&#20294;&#19982;&#25152;&#26377;&#21487;&#33021;&#30340;&#27688;&#22522;&#37240;&#24207;&#21015;&#30340;&#24040;&#22823;&#25628;&#32034;&#31354;&#38388;&#30456;&#27604;&#65292;&#24050;&#30693;&#30340;&#21151;&#33021;&#23478;&#26063;&#38598;&#21512;&#24456;&#23567;&#12290;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#33258;&#28982;&#20855;&#26377;&#26377;&#38480;&#30340;&#34507;&#30333;&#36136;&#8220;&#35789;&#27719;&#37327;&#8221;&#12290;&#22240;&#27492;&#65292;&#35745;&#31639;&#29983;&#29289;&#23398;&#23478;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#36825;&#20010;&#35789;&#27719;&#37327;&#26159;&#21542;&#21487;&#20197;&#25193;&#23637;&#65292;&#20197;&#21253;&#25324;&#24456;&#20037;&#20197;&#21069;&#28781;&#32477;&#25110;&#32773;&#20174;&#26410;&#22312;&#31532;&#19968;&#27425;&#36827;&#21270;&#20013;&#20986;&#29616;&#30340;&#26377;&#29992;&#34507;&#30333;&#36136;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#36890;&#36807;&#34701;&#21512;&#36827;&#21270;&#31639;&#27861;&#12289;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#65292;&#25105;&#20204;&#21487;&#20197;&#20419;&#36827;&#24320;&#21457;&#20174;&#26410;&#23384;&#22312;&#36807;&#30340;&#20840;&#26032;&#34507;&#30333;&#36136;&#12290;&#25105;&#20204;&#35774;&#24819;&#36825;&#39033;&#24037;&#20316;&#23558;&#24418;&#25104;&#35745;&#31639;&#36827;&#21270;&#30340;&#26032;&#30340;&#23376;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08797v1 Announce Type: cross  Abstract: The genetic blueprint for the essential functions of life is encoded in DNA, which is translated into proteins -- the engines driving most of our metabolic processes. Recent advancements in genome sequencing have unveiled a vast diversity of protein families, but compared to the massive search space of all possible amino acid sequences, the set of known functional families is minimal. One could say nature has a limited protein "vocabulary." The major question for computational biologists, therefore, is whether this vocabulary can be expanded to include useful proteins that went extinct long ago, or maybe never evolved in the first place. We outline a computational approach to solving this problem. By merging evolutionary algorithms, machine learning (ML), and bioinformatics, we can facilitate the development of completely novel proteins which have never existed before. We envision this work forming a new sub-field of computational evol
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#39034;&#24207;&#29256;&#26412;&#30340;&#36816;&#34892;&#26102;&#34892;&#20026;&#26469;&#39044;&#27979;&#32473;&#23450;&#31639;&#27861;&#30340;&#24182;&#34892;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39034;&#24207;&#36816;&#34892;&#26102;&#38388;&#20998;&#24067;&#36827;&#34892;SAT&#26412;&#22320;&#25628;&#32034;&#24182;&#34892;&#21152;&#36895;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08790</link><description>&lt;p&gt;
&#20351;&#29992;&#39034;&#24207;&#36816;&#34892;&#26102;&#38388;&#20998;&#24067;&#23545;SAT&#26412;&#22320;&#25628;&#32034;&#30340;&#24182;&#34892;&#21152;&#36895;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Using Sequential Runtime Distributions for the Parallel Speedup Prediction of SAT Local Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08790
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#39034;&#24207;&#29256;&#26412;&#30340;&#36816;&#34892;&#26102;&#34892;&#20026;&#26469;&#39044;&#27979;&#32473;&#23450;&#31639;&#27861;&#30340;&#24182;&#34892;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39034;&#24207;&#36816;&#34892;&#26102;&#38388;&#20998;&#24067;&#36827;&#34892;SAT&#26412;&#22320;&#25628;&#32034;&#24182;&#34892;&#21152;&#36895;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#26412;&#22320;&#25628;&#32034;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24182;&#34892;&#21270;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#36890;&#36807;&#20998;&#26512;&#20854;&#39034;&#24207;&#29256;&#26412;&#30340;&#36816;&#34892;&#26102;&#34892;&#20026;&#26469;&#20272;&#35745;&#32473;&#23450;&#31639;&#27861;&#30340;&#24182;&#34892;&#24615;&#33021;&#12290;&#36890;&#36807;&#29992;&#32479;&#35745;&#26041;&#27861;&#36817;&#20284;&#39034;&#24207;&#36807;&#31243;&#30340;&#36816;&#34892;&#26102;&#38388;&#20998;&#24067;&#65292;&#21487;&#20197;&#22522;&#20110;&#39034;&#24207;&#32479;&#35745;&#37327;&#24314;&#31435;&#27169;&#22411;&#26469;&#39044;&#27979;&#24182;&#34892;&#36807;&#31243;&#30340;&#36816;&#34892;&#26102;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#30740;&#31350;&#20004;&#31181;SAT&#26412;&#22320;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#21363;Sparrow&#21644;CCASAT&#30340;&#24182;&#34892;&#24615;&#33021;&#65292;&#24182;&#23558;&#39044;&#27979;&#30340;&#24615;&#33021;&#19982;&#22312;&#39640;&#36798;384&#20010;&#26680;&#24515;&#30340;&#24182;&#34892;&#30828;&#20214;&#19978;&#36827;&#34892;&#30340;&#23454;&#38469;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#39044;&#27979;&#24615;&#33021;&#25509;&#36817;&#32463;&#39564;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#24403;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20363;&#65288;&#38543;&#26426;&#21644;&#31934;&#24515;&#35774;&#35745;&#65289;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26412;&#22320;&#25628;&#32034;&#27714;&#35299;&#22120;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08790v1 Announce Type: cross  Abstract: This paper presents a detailed analysis of the scalability and parallelization of local search algorithms for the Satisfiability problem. We propose a framework to estimate the parallel performance of a given algorithm by analyzing the runtime behavior of its sequential version. Indeed, by approximating the runtime distribution of the sequential process with statistical methods, the runtime behavior of the parallel process can be predicted by a model based on order statistics. We apply this approach to study the parallel performance of two SAT local search solvers, namely Sparrow and CCASAT, and compare the predicted performances to the results of an actual experimentation on parallel hardware up to 384 cores. We show that the model is accurate and predicts performance close to the empirical data. Moreover, as we study different types of instances (random and crafted), we observe that the local search solvers exhibit different behavior
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#35745;&#31639;&#26426;&#21644;&#20154;&#31867;&#35270;&#35273;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20154;&#33080;&#39564;&#35777;&#31639;&#27861;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;</title><link>https://arxiv.org/abs/2403.08789</link><description>&lt;p&gt;
&#23558;&#20154;&#31867;&#27010;&#24565;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20154;&#33080;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Bridging Human Concepts and Computer Vision for Explainable Face Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#35745;&#31639;&#26426;&#21644;&#20154;&#31867;&#35270;&#35273;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20154;&#33080;&#39564;&#35777;&#31639;&#27861;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;(AI)&#24433;&#21709;&#20102;&#25935;&#24863;&#24212;&#29992;&#31243;&#24207;(&#22914;&#20154;&#33080;&#39564;&#35777;)&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#22240;&#27492;&#30830;&#20445;&#20915;&#31574;&#30340;&#36879;&#26126;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#38382;&#36131;&#21046;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#23384;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;(XAI)&#25216;&#26415;&#29992;&#20110;&#28548;&#28165;AI&#20915;&#31574;&#65292;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#20026;&#20154;&#31867;&#25552;&#20379;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#35745;&#31639;&#26426;&#21644;&#20154;&#31867;&#35270;&#35273;&#65292;&#22686;&#21152;&#20102;&#20154;&#33080;&#39564;&#35777;&#31639;&#27861;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21463;&#21040;&#20154;&#31867;&#24863;&#30693;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#20197;&#20102;&#35299;&#26426;&#22120;&#22312;&#20154;&#33080;&#27604;&#36739;&#20219;&#21153;&#26399;&#38388;&#22914;&#20309;&#24863;&#30693;&#20154;&#31867;&#35821;&#20041;&#21306;&#22495;&#12290;&#25105;&#20204;&#20351;&#29992;Mediapipe&#25552;&#20379;&#30340;&#20998;&#21106;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#19981;&#21516;&#30340;&#20154;&#31867;&#35821;&#20041;&#38754;&#37096;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#26426;&#22120;&#30340;&#24863;&#30693;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35843;&#25972;&#20102;&#20004;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#31639;&#27861;&#65292;&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08789v1 Announce Type: cross  Abstract: With Artificial Intelligence (AI) influencing the decision-making process of sensitive applications such as Face Verification, it is fundamental to ensure the transparency, fairness, and accountability of decisions. Although Explainable Artificial Intelligence (XAI) techniques exist to clarify AI decisions, it is equally important to provide interpretability of these decisions to humans. In this paper, we present an approach to combine computer and human vision to increase the explanation's interpretability of a face verification algorithm. In particular, we are inspired by the human perceptual process to understand how machines perceive face's human-semantic areas during face comparison tasks. We use Mediapipe, which provides a segmentation technique that identifies distinct human-semantic facial regions, enabling the machine's perception analysis. Additionally, we adapted two model-agnostic algorithms to provide human-interpretable i
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#26032;&#39062;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#65288;IBP&#65289;&#26041;&#27861;&#65292;IBP IoU&#22312;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#26356;&#23433;&#20840;&#21644;&#26356;&#31283;&#20581;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.08788</link><description>&lt;p&gt;
&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#30340;&#39564;&#35777;--IBP IoU
&lt;/p&gt;
&lt;p&gt;
Verification for Object Detection -- IBP IoU
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08788
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#26032;&#39062;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#65288;IBP&#65289;&#26041;&#27861;&#65292;IBP IoU&#22312;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#26356;&#23433;&#20840;&#21644;&#26356;&#31283;&#20581;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#65288;IBP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24418;&#24335;&#39564;&#35777;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#65292;&#29305;&#21035;&#38024;&#23545;&#20132;&#24182;&#27604;&#65288;IoU&#65289;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;&#19968;&#20010;&#21517;&#20026;IBP IoU&#30340;&#24320;&#28304;&#20195;&#30721;&#20013;&#23454;&#29616;&#65292;&#19982;&#27969;&#34892;&#30340;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#30340;&#39564;&#35777;&#24037;&#20855;&#20860;&#23481;&#12290;&#35813;&#39564;&#35777;&#22120;&#22312;&#30528;&#38470;&#36884;&#24452;&#36305;&#36947;&#26816;&#27979;&#21644;&#25163;&#20889;&#25968;&#23383;&#35782;&#21035;&#26696;&#20363;&#30740;&#31350;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#19982;&#22522;&#32447;&#65288;Vanilla IBP IoU&#65289;&#30340;&#27604;&#36739;&#31361;&#20986;&#20102;IBP IoU&#22312;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#23433;&#20840;&#21644;&#26356;&#31283;&#20581;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08788v1 Announce Type: cross  Abstract: We introduce a novel Interval Bound Propagation (IBP) approach for the formal verification of object detection models, specifically targeting the Intersection over Union (IoU) metric. The approach has been implemented in an open source code, named IBP IoU, compatible with popular abstract interpretation based verification tools. The resulting verifier is evaluated on landing approach runway detection and handwritten digit recognition case studies. Comparisons against a baseline (Vanilla IBP IoU) highlight the superior performance of IBP IoU in ensuring accuracy and stability, contributing to more secure and robust machine learning applications.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#33033;&#20914;&#30456;&#20301;&#32534;&#30721;&#30340;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#38408;&#20540;&#20559;&#31227;&#21644;&#22522;&#26412;&#25805;&#20316;&#26469;&#20943;&#23569;&#20174;ANN&#21040;SNN&#30340;&#36716;&#25442;&#25439;&#22833;&#65292;&#26080;&#38656;&#39069;&#22806;&#37325;&#26032;&#35757;&#32451;&#25110;&#26550;&#26500;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.08786</link><description>&lt;p&gt;
&#19968;&#27425;&#33033;&#20914;SNN&#65306;&#22522;&#20110;&#22522;&#26412;&#25805;&#20316;&#30340;&#21333;&#33033;&#20914;&#30456;&#20301;&#32534;&#30721;&#29992;&#20110;&#20943;&#23569;&#20174;ANN&#21040;SNN&#30340;&#36716;&#25442;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
One-Spike SNN: Single-Spike Phase Coding with Base Manipulation for ANN-to-SNN Conversion Loss Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08786
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#33033;&#20914;&#30456;&#20301;&#32534;&#30721;&#30340;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#38408;&#20540;&#20559;&#31227;&#21644;&#22522;&#26412;&#25805;&#20316;&#26469;&#20943;&#23569;&#20174;ANN&#21040;SNN&#30340;&#36716;&#25442;&#25439;&#22833;&#65292;&#26080;&#38656;&#39069;&#22806;&#37325;&#26032;&#35757;&#32451;&#25110;&#26550;&#26500;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#20107;&#20214;&#39537;&#21160;&#30340;&#65292;&#33021;&#25928;&#27604;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#26356;&#39640;&#12290;&#30001;&#20110;SNN&#36890;&#36807;&#31163;&#25955;&#33033;&#20914;&#20256;&#36882;&#25968;&#25454;&#65292;&#38590;&#20197;&#20351;&#29992;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#38480;&#21046;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20445;&#25345;SNN&#30340;&#20934;&#30830;&#24615;&#19982;ANN&#30456;&#20284;&#65292;&#38656;&#35201;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;ANN&#36716;&#25442;&#20026;SNN&#65288;&#21363;&#20174;ANN&#21040;SNN&#30340;&#36716;&#25442;&#65289;&#12290;&#22312;&#36716;&#25442;&#36807;&#31243;&#20013;&#65292;&#23558;ANN&#30340;&#28608;&#27963;&#32534;&#30721;&#25104;&#19968;&#32452;&#33033;&#20914;&#22312;SNN&#20013;&#26159;&#20851;&#38190;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#36716;&#25442;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#33033;&#20914;&#30456;&#20301;&#32534;&#30721;&#20316;&#20026;&#19968;&#31181;&#32534;&#30721;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#26368;&#23567;&#21270;&#20102;&#22312;SNN&#23618;&#20043;&#38388;&#20256;&#36755;&#25968;&#25454;&#25152;&#38656;&#30340;&#33033;&#20914;&#25968;&#37327;&#12290;&#20026;&#20102;&#20943;&#23569;&#30456;&#20301;&#32534;&#30721;&#20013;&#30001;&#21333;&#33033;&#20914;&#36817;&#20284;&#23548;&#33268;&#30340;&#32534;&#30721;&#35823;&#24046;&#65292;&#25552;&#20986;&#20102;&#38408;&#20540;&#20559;&#31227;&#21644;&#22522;&#26412;&#25805;&#20316;&#12290;&#22312;ANN&#19978;&#27809;&#26377;&#39069;&#22806;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#26550;&#26500;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#36716;&#25442;&#26041;&#27861;&#19981;&#20250;&#22833;&#21435;&#25512;&#29702;&#20934;&#30830;&#24615;&#65288;&#24179;&#22343;&#20026;0.58%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08786v1 Announce Type: cross  Abstract: As spiking neural networks (SNNs) are event-driven, energy efficiency is higher than conventional artificial neural networks (ANNs). Since SNN delivers data through discrete spikes, it is difficult to use gradient methods for training, limiting its accuracy. To keep the accuracy of SNNs similar to ANN counterparts, pre-trained ANNs are converted to SNNs (ANN-to-SNN conversion). During the conversion, encoding activations of ANNs to a set of spikes in SNNs is crucial for minimizing the conversion loss. In this work, we propose a single-spike phase coding as an encoding scheme that minimizes the number of spikes to transfer data between SNN layers. To minimize the encoding error due to single-spike approximation in phase coding, threshold shift and base manipulation are proposed. Without any additional retraining or architectural constraints on ANNs, the proposed conversion method does not lose inference accuracy (0.58% on average) verif
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#33073;&#31163;&#35821;&#22659;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22312;&#35299;&#20915;OOCD&#30456;&#20851;&#25968;&#25454;&#38480;&#21046;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08783</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#22270;&#20687;&#25991;&#26412;&#33073;&#31163;&#35821;&#22659;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Image-Text Out-Of-Context Detection Using Synthetic Multimodal Misinformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#33073;&#31163;&#35821;&#22659;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22312;&#35299;&#20915;OOCD&#30456;&#20851;&#25968;&#25454;&#38480;&#21046;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20449;&#24687;&#19981;&#26029;&#22686;&#21152;&#30340;&#26102;&#20195;&#65292;&#34394;&#20551;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33073;&#31163;&#35821;&#22659;&#26816;&#27979;&#65288;OOCD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;OOCD&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26816;&#27979;&#22120;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#39564;&#35777;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#20351;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#35299;&#20915;&#19982;OOCD&#30456;&#20851;&#30340;&#25968;&#25454;&#38480;&#21046;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#22120;&#24212;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#21644;&#24320;&#21457;&#20581;&#22766;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#31995;&#32479;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08783v1 Announce Type: cross  Abstract: Misinformation has become a major challenge in the era of increasing digital information, requiring the development of effective detection methods. We have investigated a novel approach to Out-Of-Context detection (OOCD) that uses synthetic data generation. We created a dataset specifically designed for OOCD and developed an efficient detector for accurate classification. Our experimental findings validate the use of synthetic data generation and demonstrate its efficacy in addressing the data limitations associated with OOCD. The dataset and detector should serve as valuable resources for future research and the development of robust misinformation detection systems.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#31243;&#24207;&#21270;&#29983;&#25104;&#21644;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#29983;&#25104;&#22320;&#24418;&#22320;&#22270;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12289;&#26356;&#20302;&#30828;&#20214;&#38656;&#27714;&#21644;&#26356;&#22909;&#30340;&#25972;&#21512;&#24615;&#65292;&#22312;&#22320;&#24418;&#29983;&#25104;&#36807;&#31243;&#20013;&#33021;&#22815;&#20135;&#29983;&#19982;&#30495;&#23454;&#19990;&#30028;&#26223;&#35266;&#24418;&#24577;&#29305;&#24449;&#23494;&#20999;&#30456;&#20851;&#30340;&#22320;&#24418;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.08782</link><description>&lt;p&gt;
&#21033;&#29992;&#39118;&#26684;&#36716;&#31227;&#23454;&#29616;&#31243;&#24207;&#21270;&#22320;&#24418;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Procedural terrain generation with style transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08782
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#31243;&#24207;&#21270;&#29983;&#25104;&#21644;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#29983;&#25104;&#22320;&#24418;&#22320;&#22270;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12289;&#26356;&#20302;&#30828;&#20214;&#38656;&#27714;&#21644;&#26356;&#22909;&#30340;&#25972;&#21512;&#24615;&#65292;&#22312;&#22320;&#24418;&#29983;&#25104;&#36807;&#31243;&#20013;&#33021;&#22815;&#20135;&#29983;&#19982;&#30495;&#23454;&#19990;&#30028;&#26223;&#35266;&#24418;&#24577;&#29305;&#24449;&#23494;&#20999;&#30456;&#20851;&#30340;&#22320;&#24418;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#29992;&#20110;&#29983;&#25104;&#22320;&#24418;&#22320;&#22270;&#65292;&#21033;&#29992;&#20102;&#31243;&#24207;&#21270;&#29983;&#25104;&#21644;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19982;&#31454;&#20105;&#24615;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12289;&#26356;&#20302;&#30340;&#30828;&#20214;&#38656;&#27714;&#65292;&#24182;&#22312;&#35774;&#35745;&#24072;&#21644;&#24320;&#21457;&#20154;&#21592;&#30340;&#21019;&#36896;&#36807;&#31243;&#20013;&#26377;&#26356;&#22909;&#30340;&#25972;&#21512;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#22810;&#23618;&#24179;&#28369;&#30340;&#39640;&#26031;&#22122;&#22768;&#25110;Perlin&#31639;&#27861;&#29983;&#25104;&#31243;&#24207;&#21270;&#22122;&#22768;&#22320;&#22270;&#12290;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#22686;&#24378;&#30340;&#31070;&#32463;&#39118;&#26684;&#36716;&#31227;&#25216;&#26415;&#65292;&#20174;&#29616;&#23454;&#19990;&#30028;&#30340;&#39640;&#24230;&#22320;&#22270;&#20013;&#25552;&#21462;&#39118;&#26684;&#12290;&#36825;&#31181;&#31639;&#27861;&#29983;&#25104;&#21644;&#31070;&#32463;&#22788;&#29702;&#30340;&#34701;&#21512;&#26377;&#28508;&#21147;&#20135;&#29983;&#19981;&#20165;&#22810;&#26679;&#21270;&#32780;&#19988;&#19982;&#29616;&#23454;&#19990;&#30028;&#26223;&#35266;&#24418;&#24577;&#29305;&#24449;&#23494;&#20999;&#30456;&#20851;&#30340;&#22320;&#24418;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#20135;&#29983;&#19968;&#33268;&#30340;&#22320;&#24418;&#32467;&#26500;&#24182;&#25552;&#20379;&#20102;&#30340;capab
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08782v1 Announce Type: cross  Abstract: In this study we introduce a new technique for the generation of terrain maps, exploiting a combination of procedural generation and Neural Style Transfer. We consider our approach to be a viable alternative to competing generative models, with our technique achieving greater versatility, lower hardware requirements and greater integration in the creative process of designers and developers. Our method involves generating procedural noise maps using either multi-layered smoothed Gaussian noise or the Perlin algorithm. We then employ an enhanced Neural Style transfer technique, drawing style from real-world height maps. This fusion of algorithmic generation and neural processing holds the potential to produce terrains that are not only diverse but also closely aligned with the morphological characteristics of real-world landscapes, with our process yielding consistent terrain structures with low computational cost and offering the capab
&lt;/p&gt;</description></item><item><title>LVLMs&#22312;&#22810;&#27169;&#24577;&#33073;&#31163;&#19978;&#19979;&#25991;&#26816;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#22312;&#36827;&#34892;&#24494;&#35843;&#21518;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08776</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#32842;&#22825;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#33073;&#31163;&#19978;&#19979;&#25991;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Leveraging Chat-Based Large Vision Language Models for Multimodal Out-Of-Context Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08776
&lt;/p&gt;
&lt;p&gt;
LVLMs&#22312;&#22810;&#27169;&#24577;&#33073;&#31163;&#19978;&#19979;&#25991;&#26816;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#22312;&#36827;&#34892;&#24494;&#35843;&#21518;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#31163;&#19978;&#19979;&#25991;&#65288;OOC&#65289;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#35782;&#21035;&#20986;&#19978;&#19979;&#25991;&#20013;&#21576;&#29616;&#30340;&#19982;&#22270;&#20687;&#21644;&#25991;&#26412;&#26080;&#20851;&#30340;&#20869;&#23481;&#12290;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#25928;&#26524;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;LVLMs&#22312;&#22810;&#27169;&#24577;OOC&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;LVLMs&#26816;&#27979;&#22810;&#27169;&#24577;OOC&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#22312;OOC&#26816;&#27979;&#20219;&#21153;&#19978;&#36798;&#21040;&#39640;&#20934;&#30830;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22810;&#27169;&#24577;OOC&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;LVLMs&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;OOC&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;LVLMs&#22312;OOC&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;NewsCLIPpings&#25968;&#25454;&#38598;&#19978;&#23545;MiniGPT-4&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;OOC&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;MiniGPT-4&#22312;NewsCLIPpings&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26174;&#33879;&#25552;&#39640;&#20102;&#35813;&#25968;&#25454;&#38598;&#20013;&#30340;OOC&#26816;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08776v1 Announce Type: cross  Abstract: Out-of-context (OOC) detection is a challenging task involving identifying images and texts that are irrelevant to the context in which they are presented. Large vision-language models (LVLMs) are effective at various tasks, including image classification and text generation. However, the extent of their proficiency in multimodal OOC detection tasks is unclear. In this paper, we investigate the ability of LVLMs to detect multimodal OOC and show that these models cannot achieve high accuracy on OOC detection tasks without fine-tuning. However, we demonstrate that fine-tuning LVLMs on multimodal OOC datasets can further improve their OOC detection accuracy. To evaluate the performance of LVLMs on OOC detection tasks, we fine-tune MiniGPT-4 on the NewsCLIPpings dataset, a large dataset of multimodal OOC. Our results show that fine-tuning MiniGPT-4 on the NewsCLIPpings dataset significantly improves the OOC detection accuracy in this datas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;SDN&#20013;&#23454;&#29616;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#21516;&#27493;&#65292;&#20197;&#35299;&#20915;&#20248;&#21270;&#36890;&#20449;&#24310;&#36831;&#21644;&#36127;&#36733;&#24179;&#34913;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23545;&#32593;&#32476;&#24310;&#36831;&#21644;&#35745;&#31639;&#36164;&#28304;&#35201;&#27714;&#20005;&#26684;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.08775</link><description>&lt;p&gt;
&#22522;&#20110;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20998;&#24067;&#24335;SDN&#20013;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#21516;&#27493;
&lt;/p&gt;
&lt;p&gt;
Constrained Reinforcement Learning for Adaptive Controller Synchronization in Distributed SDN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08775
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;SDN&#20013;&#23454;&#29616;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#21516;&#27493;&#65292;&#20197;&#35299;&#20915;&#20248;&#21270;&#36890;&#20449;&#24310;&#36831;&#21644;&#36127;&#36733;&#24179;&#34913;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23545;&#32593;&#32476;&#24310;&#36831;&#21644;&#35745;&#31639;&#36164;&#28304;&#35201;&#27714;&#20005;&#26684;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;(SDN)&#20013;&#65292;&#20998;&#24067;&#24335;SDN&#25511;&#21046;&#22120;&#30340;&#23454;&#29616;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#27599;&#20010;&#25511;&#21046;&#22120;&#36127;&#36131;&#31649;&#29702;&#29305;&#23450;&#30340;&#23376;&#32593;&#32476;&#25110;&#22495;&#65292;&#20197;&#22312;&#38598;&#20013;&#21270;&#25511;&#21046;&#12289;&#21487;&#20280;&#32553;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#32593;&#32476;&#25928;&#29575;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#12290;&#36825;&#20123;&#25511;&#21046;&#22120;&#24517;&#39035;&#21516;&#27493;&#20197;&#20445;&#25345;&#23545;&#25972;&#20010;&#32593;&#32476;&#30340;&#36923;&#36753;&#38598;&#20013;&#35270;&#22270;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#21516;&#27493;&#20998;&#24067;&#24335;SDN&#25511;&#21046;&#22120;&#65292;&#20294;&#22823;&#22810;&#25968;&#20542;&#21521;&#20110;&#20248;&#20808;&#32771;&#34385;&#35832;&#22914;&#20248;&#21270;&#36890;&#20449;&#24310;&#36831;&#25110;&#36127;&#36733;&#24179;&#34913;&#31561;&#30446;&#26631;&#65292;&#36890;&#24120;&#24573;&#35270;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#26041;&#38754;&#12290;&#22312;&#32771;&#34385;&#35832;&#22914;&#22686;&#24378;&#29616;&#23454;&#21644;&#34394;&#25311;&#29616;&#23454;&#65288;AR/VR&#65289;&#31561;&#38656;&#27714;&#21463;&#21040;&#38480;&#21046;&#30340;&#32593;&#32476;&#24310;&#36831;&#21644;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#30340;&#24212;&#29992;&#26102;&#65292;&#36825;&#31181;&#38480;&#21046;&#23588;&#20026;&#26174;&#33879;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#20027;&#35201;&#20381;&#36182;&#22522;&#20110;&#20540;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08775v1 Announce Type: cross  Abstract: In software-defined networking (SDN), the implementation of distributed SDN controllers, with each controller responsible for managing a specific sub-network or domain, plays a critical role in achieving a balance between centralized control, scalability, reliability, and network efficiency. These controllers must be synchronized to maintain a logically centralized view of the entire network. While there are various approaches for synchronizing distributed SDN controllers, most tend to prioritize goals such as optimization of communication latency or load balancing, often neglecting to address both the aspects simultaneously. This limitation becomes particularly significant when considering applications like Augmented and Virtual Reality (AR/VR), which demand constrained network latencies and substantial computational resources. Additionally, many existing studies in this field predominantly rely on value-based reinforcement learning (
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20351;&#29992;Green&#20989;&#25968;&#26041;&#27861;&#30740;&#31350;&#36807;&#28388;&#27873;&#27873;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#24490;&#29615;&#25193;&#23637;&#26041;&#27861;&#35752;&#35770;&#31038;&#20250;&#20114;&#21160;&#22797;&#26434;&#24615;&#65292;&#24341;&#20837;&#24207;&#21015;&#20989;&#25968;&#35780;&#20272;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.08774</link><description>&lt;p&gt;
&#35770;&#24490;&#29615;&#25193;&#23637;&#21450;&#24341;&#20837;&#24207;&#21015;&#20999;&#21106;&#20989;&#25968;&#33267;&#23616;&#37096;&#21183;&#36817;&#20284;&#30340;&#35752;&#35770;&#65306;&#21033;&#29992;Green&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#20999;&#21106;&#31532;N&#38454;&#31038;&#20250;&#20114;&#21160;&#20197;&#20419;&#36827;&#34892;&#20026;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Discussion of Loop Expansion and Introduction of Series Cutting Functions to Local Potential Approximation: Complexity Analysis Using Green's Functions, Cutting Of Nth-Order Social Interactions For Progressive Safety
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08774
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20351;&#29992;Green&#20989;&#25968;&#26041;&#27861;&#30740;&#31350;&#36807;&#28388;&#27873;&#27873;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#24490;&#29615;&#25193;&#23637;&#26041;&#27861;&#35752;&#35770;&#31038;&#20250;&#20114;&#21160;&#22797;&#26434;&#24615;&#65292;&#24341;&#20837;&#24207;&#21015;&#20989;&#25968;&#35780;&#20272;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#21069;&#36848;&#35770;&#25991;&#12298;2024&#24180;&#8220;&#26816;&#39564;Kubo-Matsubara Green&#20989;&#25968;&#30340;Edwards-Anderson&#27169;&#22411;&#65306;&#21033;&#29992;&#22797;&#21046;&#27861;&#36827;&#34892;&#38646;&#29616;&#35937;&#30340;N&#38454;&#25554;&#20540;&#22806;&#25512;&#30340;&#26497;&#20540;&#20449;&#24687;&#27969;&#12299;&#12290;&#26412;&#25991;&#20063;&#24212;&#29992;&#29702;&#35770;&#29289;&#29702;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36807;&#28388;&#27873;&#27873;&#29616;&#35937;&#65292;&#29305;&#21035;&#20851;&#27880;&#24490;&#29615;&#25193;&#23637;&#21644;&#25130;&#26029;&#20989;&#25968;&#12290;&#37319;&#29992;&#24490;&#29615;&#25193;&#23637;&#26041;&#27861;&#65292;&#23558;&#35752;&#35770;&#22312;&#36807;&#28388;&#27873;&#27873;&#21457;&#29983;&#26399;&#38388;&#30340;&#31038;&#20250;&#20114;&#21160;&#22797;&#26434;&#24615;&#65292;&#20197;&#24341;&#20837;&#24207;&#21015;&#65292;&#36827;&#34892;&#25968;&#23398;&#34920;&#36798;&#65292;&#24182;&#35780;&#20272;&#36825;&#20123;&#20114;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;Green&#20989;&#25968;&#65292;&#21253;&#25324;&#24310;&#36831;Green&#20989;&#25968;&#12289;&#20808;&#36827;Green&#20989;&#25968;&#21644;&#22240;&#26524;Green&#20989;&#25968;&#65292;&#26469;&#20998;&#26512;&#20195;&#29702;&#20154;&#21450;&#20854;&#26102;&#38388;&#28436;&#21464;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#36890;&#36807;&#23616;&#37096;&#21183;&#36817;&#20284;&#25429;&#25417;&#31995;&#32479;&#30340;&#21160;&#24577;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08774v1 Announce Type: cross  Abstract: In this study, we focus on the aforementioned paper, "Examination Kubo-Matsubara Green's Function Of The Edwards-Anderson Model: Extreme Value Information Flow Of Nth-Order Interpolated Extrapolation Of Zero Phenomena Using The Replica Method (2024)". This paper also applies theoretical physics methods to better understand the filter bubble phenomenon, focusing in particular on loop expansions and truncation functions. Using the loop expansion method, the complexity of social interactions during the occurrence of filter bubbles will be discussed in order to introduce series, express mathematically, and evaluate the impact of these interactions. We analyze the interactions between agents and their time evolution using a variety of Green's functions, including delayed Green's functions, advanced Green's functions, and causal Green's functions, to capture the dynamic response of the system through local potential approximations. In additi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#34701;&#21512;&#29420;&#29305;&#30340;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08773</link><description>&lt;p&gt;
Veagle: &#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Veagle: Advancements in Multimodal Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#34701;&#21512;&#29420;&#29305;&#30340;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;&#35821;&#35328;&#21644;&#35270;&#35273;&#22914;&#20309;&#32467;&#21512;&#20135;&#29983;&#20102;&#27987;&#21402;&#20852;&#36259;&#65292;&#20174;&#32780;&#20652;&#29983;&#20102;&#26088;&#22312;&#26080;&#32541;&#25972;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24310;&#20280;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#33539;&#22260;&#20174;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#21040;&#35270;&#35273;&#23450;&#20301;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#20934;&#30830;&#35299;&#37322;&#22270;&#20687;&#24182;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#38024;&#23545;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20013;&#35266;&#23519;&#21040;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;Veagle&#65292;&#34701;&#21512;&#20102;&#21463;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08773v1 Announce Type: cross  Abstract: Lately, researchers in artificial intelligence have been really interested in how language and vision come together, giving rise to the development of multimodal models that aim to seamlessly integrate textual and visual information. Multimodal models, an extension of Large Language Models (LLMs), have exhibited remarkable capabilities in addressing a diverse array of tasks, ranging from image captioning and visual question answering (VQA) to visual grounding. While these models have showcased significant advancements, challenges persist in accurately interpreting images and answering the question, a common occurrence in real-world scenarios. This paper introduces a novel approach to enhance the multimodal capabilities of existing models. In response to the limitations observed in current Vision Language Models (VLMs) and Multimodal Large Language Models (MLLMs), our proposed model Veagle, incorporates a unique mechanism inspired by th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#23454;&#29616;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#65292;&#22312;GPU&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#30456;&#21305;&#25932;&#12290;</title><link>https://arxiv.org/abs/2403.08551</link><description>&lt;p&gt;
&#39640;&#26031;&#22270;&#20687;&#65306;&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#36827;&#34892;1000&#24103;&#27599;&#31186;&#30340;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08551
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#23454;&#29616;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#65292;&#22312;GPU&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#30456;&#21305;&#25932;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#25552;&#20379;&#20102;&#39640;&#35270;&#35273;&#36136;&#37327;&#21644;&#24555;&#36895;&#28210;&#26579;&#36895;&#24230;&#65292;&#27599;&#31186;10-1000&#24103;&#65292;&#20551;&#35774;&#26377;&#36275;&#22815;&#30340;GPU&#36164;&#28304;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35201;&#27714;&#24120;&#24120;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#20869;&#23384;&#26377;&#38480;&#30340;&#20302;&#31471;&#35774;&#22791;&#19978;&#30340;&#20351;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#36827;&#34892;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#30340;&#24320;&#21019;&#24615;&#33539;&#24335;&#65292;&#21517;&#20026;GaussianImage&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;2D&#39640;&#26031;&#26469;&#34920;&#31034;&#22270;&#20687;&#65292;&#20854;&#20013;&#27599;&#20010;&#39640;&#26031;&#20855;&#26377;8&#20010;&#21442;&#25968;&#65292;&#21253;&#25324;&#20301;&#32622;&#12289;&#21327;&#26041;&#24046;&#21644;&#39068;&#33394;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#32047;&#31215;&#27714;&#21644;&#30340;&#26032;&#39062;&#28210;&#26579;&#31639;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;GPU&#20869;&#23384;&#33267;&#23569;&#38477;&#20302;3&#20493;&#65292;&#25311;&#21512;&#26102;&#38388;&#24555;5&#20493;&#65292;&#19981;&#20165;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#65288;&#20363;&#22914;WIRE&#65292;I-NGP&#65289;&#19981;&#30456;&#19978;&#19979;&#65292;&#32780;&#19988;&#26080;&#35770;&#21442;&#25968;&#22823;&#23567;&#22914;&#20309;&#37117;&#33021;&#25552;&#20379;1500-2000&#24103;&#27599;&#31186;&#30340;&#26356;&#24555;&#28210;&#26579;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08551v1 Announce Type: cross  Abstract: Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HRLAIF&#26041;&#27861;&#26469;&#25913;&#21892;&#24320;&#25918;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#21709;&#24212;&#24110;&#21161;&#24615;&#65292;&#36890;&#36807;&#22686;&#24378;AI&#27880;&#37322;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#40065;&#26834;&#24615;</title><link>https://arxiv.org/abs/2403.08309</link><description>&lt;p&gt;
HRLAIF: &#36890;&#36807;AI&#21453;&#39304;&#25913;&#36827;&#24320;&#25918;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24110;&#21161;&#24615;&#21644;&#26080;&#23475;&#24615;
&lt;/p&gt;
&lt;p&gt;
HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HRLAIF&#26041;&#27861;&#26469;&#25913;&#21892;&#24320;&#25918;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#21709;&#24212;&#24110;&#21161;&#24615;&#65292;&#36890;&#36807;&#22686;&#24378;AI&#27880;&#37322;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;AI&#21453;&#39304;&#65288;RLAIF&#65289;&#30456;&#27604;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#20855;&#26377;&#26356;&#30701;&#30340;&#27880;&#37322;&#21608;&#26399;&#21644;&#26356;&#20302;&#30340;&#25104;&#26412;&#20248;&#21183;&#65292;&#20351;&#20854;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#30340;&#24555;&#36895;&#31574;&#30053;&#36845;&#20195;&#38454;&#27573;&#38750;&#24120;&#39640;&#25928;&#12290;&#20351;&#29992;ChatGPT&#20316;&#20026;&#26631;&#27880;&#21592;&#65292;&#22312;RLAIF&#35757;&#32451;&#20013;&#20026;&#24320;&#25918;&#22495;&#25552;&#31034;&#25552;&#20379;&#21453;&#39304;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20154;&#31867;&#35780;&#20272;&#32773;&#23545;&#27169;&#22411;&#21709;&#24212;&#30340;&#20559;&#22909;&#32988;&#29575;&#22686;&#21152;&#65292;&#20294;&#35780;&#20272;&#32773;&#30340;&#28385;&#24847;&#24230;&#19979;&#38477;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#28385;&#24847;&#24230;&#19979;&#38477;&#20027;&#35201;&#26159;&#22240;&#20026;&#19968;&#20123;&#21709;&#24212;&#21464;&#24471;&#19981;&#22815;&#26377;&#24110;&#21161;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#26041;&#38754;&#65292;&#31361;&#26174;&#20102;&#22522;&#26412;RLAIF&#30340;&#23454;&#38469;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#20174;AI&#21453;&#39304;&#65288;HRLAIF&#65289;&#12290;&#35813;&#26041;&#27861;&#22686;&#24378;&#20102;AI&#27880;&#37322;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#27169;&#22411;&#30340;&#24110;&#21161;&#24615;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08309v1 Announce Type: cross  Abstract: Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter annotation cycles and lower costs over Reinforcement Learning from Human Feedback (RLHF), making it highly efficient during the rapid strategy iteration periods of large language model (LLM) training. Using ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, we observe an increase in human evaluators' preference win ratio for model responses, but a decrease in evaluators' satisfaction rate. Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI annotations for responses, making the model's helpfulness more robust in training process. Additionally, 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#37197;&#32622;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#32570;&#20047;&#21487;&#38752;&#27604;&#36739;&#21644;&#21487;&#37325;&#29616;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08265</link><description>&lt;p&gt;
&#38543;&#26426;&#25628;&#32034;&#20316;&#20026;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Random Search as a Baseline for Sparse Neural Network Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08265
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#37197;&#32622;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#32570;&#20047;&#21487;&#38752;&#27604;&#36739;&#21644;&#21487;&#37325;&#29616;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#22312;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#19982;&#23494;&#38598;&#32593;&#32476;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36825;&#20419;&#20351;&#35768;&#22810;&#24037;&#20316;&#23398;&#20064;&#12289;&#35825;&#23548;&#25110;&#25628;&#32034;&#24615;&#33021;&#39640;&#30340;&#31232;&#30095;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36136;&#37327;&#25110;&#25928;&#29575;&#30340;&#25552;&#21319;&#20540;&#24471;&#27880;&#24847;&#65292;&#20294;&#26631;&#20934;&#22522;&#32447;&#32570;&#20047;&#65292;&#22240;&#27492;&#22952;&#30861;&#20102;&#26041;&#27861;&#20043;&#38388;&#30340;&#21487;&#38752;&#27604;&#36739;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#19968;&#20010;&#31616;&#21333;&#30340;&#38543;&#26426;&#25628;&#32034;&#22522;&#32447;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#33391;&#22909;&#30340;&#31232;&#30095;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#32593;&#32476;&#30340;&#33410;&#28857;&#31354;&#38388;&#19978;&#24212;&#29992;&#38543;&#26426;&#25628;&#32034;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#22312;&#25439;&#22833;&#26223;&#35266;&#20013;&#20301;&#32622;&#26356;&#26377;&#20248;&#21183;&#30340;&#26356;&#22909;&#21021;&#22987;&#21270;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#35760;&#24405;&#20102;&#19981;&#21516;&#31232;&#30095;&#31243;&#24230;&#19979;&#31232;&#30095;&#32593;&#32476;&#30340;&#35757;&#32451;&#21518;&#24615;&#33021;&#65292;&#24182;&#19982;&#23427;&#20204;&#30340;&#23436;&#20840;&#36830;&#25509;&#29238;&#32593;&#32476;&#20197;&#21450;&#38543;&#26426;&#31232;&#30095;&#37197;&#32622;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08265v1 Announce Type: cross  Abstract: Sparse neural networks have shown similar or better generalization performance than their dense counterparts while having higher parameter efficiency. This has motivated a number of works to learn, induce, or search for high performing sparse networks. While reports of quality or efficiency gains are impressive, standard baselines are lacking, therefore hindering having reliable comparability and reproducibility across methods. In this work, we provide an evaluation approach and a naive Random Search baseline method for finding good sparse configurations. We apply Random Search on the node space of an overparameterized network with the goal of finding better initialized sparse sub-networks that are positioned more advantageously in the loss landscape. We record sparse network post-training performances at various levels of sparsity and compare against both their fully connected parent networks and random sparse configurations at the sa
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Transformer&#27169;&#22411;&#21644;Context-Reverso&#25968;&#25454;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#28165;&#26224;&#24230;&#30340;&#21477;&#23376;</title><link>https://arxiv.org/abs/2403.08103</link><description>&lt;p&gt;
&#21033;&#29992;Context-Reverso&#25968;&#25454;&#20351;&#29992;Transformer&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#28165;&#26224;&#24230;&#30340;&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;
Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08103
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Transformer&#27169;&#22411;&#21644;Context-Reverso&#25968;&#25454;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#28165;&#26224;&#24230;&#30340;&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#20016;&#23500;&#30340;&#26102;&#20195;&#65292;&#25552;&#20379;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#19988;&#31616;&#27905;&#30340;&#20449;&#24687;&#23545;&#29992;&#25143;&#33267;&#20851;&#37325;&#35201;&#12290;&#20851;&#38190;&#35789;&#19978;&#19979;&#25991;(KIC)&#29983;&#25104;&#26159;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#25198;&#28436;&#33267;&#20851;&#37325;&#35201;&#35282;&#33394;&#30340;&#20219;&#21153;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#12289;&#20010;&#20154;&#21161;&#25163;&#21644;&#20869;&#23481;&#25688;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;T5 transformer&#27169;&#22411;&#29983;&#25104;&#32473;&#23450;&#20851;&#38190;&#35789;&#30340;&#26126;&#30830;&#19988;&#31616;&#27905;&#21477;&#23376;&#19978;&#19979;&#25991;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#20174;Context-Reverso API&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08103v1 Announce Type: cross  Abstract: In the age of information abundance, the ability to provide users with contextually relevant and concise information is crucial. Keyword in Context (KIC) generation is a task that plays a vital role in and generation applications, such as search engines, personal assistants, and content summarization. In this paper, we present a novel approach to generating unambiguous and brief sentence-contexts for given keywords using the T5 transformer model, leveraging data obtained from the Context-Reverso API. The code is available at https://github.com/Rusamus/word2context/tree/main .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KnowCoder&#65292;&#19968;&#20010;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#25191;&#34892;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20195;&#30721;&#39118;&#26684;&#30340;&#27169;&#24335;&#34920;&#31034;&#26041;&#27861;&#21644;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;LLMs&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#20934;&#30830;&#25552;&#21462;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.07969</link><description>&lt;p&gt;
KnowCoder&#65306;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#30721;&#21040;LLMs&#20013;&#29992;&#20110;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KnowCoder&#65292;&#19968;&#20010;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#25191;&#34892;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20195;&#30721;&#39118;&#26684;&#30340;&#27169;&#24335;&#34920;&#31034;&#26041;&#27861;&#21644;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;LLMs&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#20934;&#30830;&#25552;&#21462;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KnowCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;&#65288;UIE&#65289;&#12290;KnowCoder&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24335;&#34920;&#31034;&#65292;&#20351;LLMs&#33021;&#22815;&#36731;&#26494;&#29702;&#35299;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;LLMs&#36981;&#24490;&#27169;&#24335;&#24182;&#20934;&#30830;&#25552;&#21462;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;KnowCoder&#24341;&#20837;&#20102;&#19968;&#31181;&#20195;&#30721;&#39118;&#26684;&#30340;&#27169;&#24335;&#34920;&#31034;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#30340;&#27169;&#24335;&#32479;&#19968;&#36716;&#25442;&#20026;Python&#31867;&#65292;&#20174;&#32780;&#21487;&#20197;&#20197;LLM&#21451;&#22909;&#30340;&#26041;&#24335;&#25429;&#25417;UIE&#20013;&#20219;&#21153;&#20043;&#38388;&#30340;&#32422;&#26463;&#31561;&#22797;&#26434;&#27169;&#24335;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;30,000&#31181;&#30693;&#35782;&#31867;&#22411;&#30340;&#20195;&#30721;&#39118;&#26684;&#27169;&#24335;&#24211;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;UIE&#20013;&#26368;&#22823;&#30340;&#24211;&#12290;&#20026;&#20102;&#31616;&#21270;LLMs&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;KnowCoder&#21253;&#21547;&#19968;&#20010;&#36890;&#36807;&#20195;&#30721;&#39044;&#35757;&#32451;&#22686;&#24378;&#20854;&#27169;&#24335;&#29702;&#35299;&#33021;&#21147;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07969v1 Announce Type: cross  Abstract: In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over $\textbf{30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#23454;&#20307;&#23545;&#20154;&#31867;&#20114;&#21160;&#30340;&#38761;&#26032;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#23558;&#19987;&#38376;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#25193;&#23637;&#21040;&#25805;&#20316;&#24615;&#32452;&#32455;&#27969;&#31243;&#21644;&#22522;&#20110;&#30693;&#35782;&#21644;&#20154;&#31867;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.07769</link><description>&lt;p&gt;
&#23558;&#31454;&#20105;&#36716;&#21270;&#20026;&#21512;&#20316;&#65306;&#22810;Agent&#31995;&#32479;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#20195;&#32452;&#32455;&#20013;&#30340;&#38761;&#21629;&#24615;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07769
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#23454;&#20307;&#23545;&#20154;&#31867;&#20114;&#21160;&#30340;&#38761;&#26032;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#23558;&#19987;&#38376;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#25193;&#23637;&#21040;&#25805;&#20316;&#24615;&#32452;&#32455;&#27969;&#31243;&#21644;&#22522;&#20110;&#30693;&#35782;&#21644;&#20154;&#31867;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#65288;SMA&#65289;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35745;&#31639;&#23454;&#20307;&#30340;&#21160;&#24577;&#24433;&#21709;&#65292;&#20854;&#29305;&#28857;&#26159;&#33021;&#22815;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#65292;&#20316;&#20026;&#19968;&#31181;&#38761;&#26032;&#20154;&#31867;&#29992;&#25143;&#20132;&#20114;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#21033;&#29992;&#19987;&#38376;&#30340;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#20174;&#25805;&#20316;&#32452;&#32455;&#27969;&#31243;&#21040;&#22522;&#20110;&#24212;&#29992;&#30693;&#35782;&#21644;&#20154;&#30340;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#12290; &#20808;&#21069;&#30340;&#35843;&#26597;&#26174;&#31034;&#65292;&#22312;&#22788;&#29702;&#26032;&#25361;&#25112;&#21644;&#23454;&#29992;&#20219;&#21153;&#65288;&#22914;&#24341;&#21457;&#36923;&#36753;&#25512;&#29702;&#21644;&#38382;&#39064;&#35299;&#20915;&#65289;&#26102;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#24037;&#20195;&#29702;&#30340;&#33258;&#20027;&#26041;&#27861;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290; &#36824;&#32771;&#34385;&#21040;&#65292;&#20256;&#32479;&#25216;&#26415;&#65292;&#22914;&#28608;&#21457;&#24605;&#24819;&#38142;&#65292;&#38656;&#35201;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#23548;&#12290; &#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24320;&#21457;&#30340;&#20195;&#29702;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07769v1 Announce Type: new  Abstract: This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#23545;&#27604;&#22870;&#21169;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#25913;&#21892;&#20102;&#22870;&#21169;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#40723;&#21169;&#23545;&#22522;&#20934;&#30340;&#25913;&#21892;&#65292;&#24182;&#33021;&#26681;&#25454;&#20219;&#21153;&#30340;&#38590;&#24230;&#36827;&#34892;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.07708</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#22870;&#21169;&#25913;&#21892;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07708
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#23545;&#27604;&#22870;&#21169;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#25913;&#21892;&#20102;&#22870;&#21169;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#40723;&#21169;&#23545;&#22522;&#20934;&#30340;&#25913;&#21892;&#65292;&#24182;&#33021;&#26681;&#25454;&#20219;&#21153;&#30340;&#38590;&#24230;&#36827;&#34892;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26159;&#29992;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#28982;&#32780;&#29616;&#26377;&#30340;RLHF&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#26469;&#28304;&#30340;&#22122;&#22768;&#65292;&#20363;&#22914;&#20154;&#31867;&#26631;&#27880;&#38169;&#35823;&#65292;&#20351;&#24471;&#27969;&#31243;&#33030;&#24369;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23545;&#22870;&#21169;&#30340;&#24809;&#32602;&#39033;&#65292;&#21629;&#21517;&#20026;&#8220;&#23545;&#27604;&#22870;&#21169;&#8221;&#65292;&#26469;&#25552;&#39640;&#22870;&#21169;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#31163;&#32447;&#25277;&#26679;&#27493;&#39588;&#65292;&#33719;&#21462;&#29992;&#20316;&#22522;&#20934;&#35745;&#31639;&#30340;&#25552;&#31034;&#21709;&#24212;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20351;&#29992;&#22522;&#20934;&#21709;&#24212;&#35745;&#31639;&#23545;&#27604;&#22870;&#21169;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;Proximal Policy Optimization&#65288;PPO&#65289;&#27493;&#39588;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#22870;&#21169;&#20351;&#24471;LLM&#33021;&#22815;&#24809;&#32602;&#22870;&#21169;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#40723;&#21169;&#20248;&#20110;&#22522;&#32447;&#30340;&#25913;&#36827;&#65292;&#26681;&#25454;&#20219;&#21153;&#38590;&#24230;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#19988;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07708v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) is the mainstream paradigm used to align large language models (LLMs) with human preferences. Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named as \textit{contrastive rewards}. %Contrastive rewards Our approach involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses and used in the Proximal Policy Optimization (PPO) step. We show that contrastive rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#21333;&#20307;&#36180;&#29575;&#27604;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;ORPO&#65292;&#22312;SFT&#36807;&#31243;&#20013;&#36890;&#36807;&#36731;&#24494;&#24809;&#32602;&#19981;&#21463;&#27426;&#36814;&#30340;&#29983;&#25104;&#39118;&#26684;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#20559;&#22909;&#23545;&#40784;&#38454;&#27573;</title><link>https://arxiv.org/abs/2403.07691</link><description>&lt;p&gt;
&#20855;&#26377;&#36180;&#29575;&#27604;&#30340;&#26080;&#21442;&#32771;&#21333;&#20307;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reference-free Monolithic Preference Optimization with Odds Ratio
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#21333;&#20307;&#36180;&#29575;&#27604;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;ORPO&#65292;&#22312;SFT&#36807;&#31243;&#20013;&#36890;&#36807;&#36731;&#24494;&#24809;&#32602;&#19981;&#21463;&#27426;&#36814;&#30340;&#29983;&#25104;&#39118;&#26684;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#20559;&#22909;&#23545;&#40784;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#35821;&#35328;&#27169;&#22411;&#20559;&#22909;&#23545;&#40784;&#31639;&#27861;&#23637;&#29616;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#20173;&#28982;&#23545;&#20110;&#25104;&#21151;&#25910;&#25947;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20559;&#22909;&#23545;&#40784;&#30340;&#29615;&#22659;&#20013;SFT&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24378;&#35843;&#23545;&#20110;&#20559;&#22909;&#23545;&#40784;&#30340;SFT&#26469;&#35828;&#65292;&#23545;&#20110;&#19981;&#21463;&#27426;&#36814;&#30340;&#29983;&#25104;&#39118;&#26684;&#26045;&#21152;&#36731;&#24494;&#24809;&#32602;&#23601;&#36275;&#22815;&#20102;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21019;&#26032;&#30340;&#26080;&#21442;&#32771;&#27169;&#22411;&#30340;&#21333;&#20307;&#36180;&#29575;&#27604;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;ORPO&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#20559;&#22909;&#23545;&#40784;&#38454;&#27573;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#25163;&#27573;&#35777;&#26126;&#65292;&#36180;&#29575;&#27604;&#26159;&#22312;125M&#33267;7B&#19981;&#21516;&#35268;&#27169;&#19979;&#36827;&#34892;SFT&#26102;&#23545;&#27604;&#21463;&#27426;&#36814;&#21644;&#19981;&#21463;&#27426;&#36814;&#39118;&#26684;&#30340;&#26126;&#26234;&#36873;&#25321;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;ORPO&#22312;&#20165;UltraFeedback&#19978;&#23545;Phi-2&#65288;2.7B&#65289;&#12289;Llama-2&#65288;7B&#65289;&#21644;Mistral&#65288;7B&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#36229;&#36234;&#20102;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07691v1 Announce Type: cross  Abstract: While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance o
&lt;/p&gt;</description></item><item><title>&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#65292;&#24182;&#19988;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#33539;&#22260;&#20869;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.06398</link><description>&lt;p&gt;
&#20851;&#20110;&#25345;&#32493;&#23398;&#20064;&#20013;&#23485;&#24230;&#36882;&#20943;&#22238;&#25253;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Diminishing Returns of Width for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06398
&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#65292;&#24182;&#19988;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#33539;&#22260;&#20869;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#25353;&#39034;&#24207;&#35757;&#32451;&#26032;&#20219;&#21153;&#26102;&#32463;&#24120;&#20986;&#29616;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#12290; &#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#20943;&#23569;&#65292;&#20294;&#23578;&#26410;&#20934;&#30830;&#21051;&#30011;&#23485;&#24230;&#21644;&#25345;&#32493;&#23398;&#20064;&#20043;&#38388;&#30340;&#30830;&#20999;&#20851;&#31995;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20854;&#20013;&#19968;&#20010;&#26368;&#26089;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#25345;&#32493;&#23398;&#20064;&#29702;&#35770;&#65292;&#24182;&#35777;&#26126;&#23485;&#24230;&#19982;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#20013;&#30340;&#36951;&#24536;&#30452;&#25509;&#30456;&#20851;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#19978;&#32463;&#39564;&#24615;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35770;&#26029;&#65292;&#32467;&#26524;&#26174;&#31034;&#36882;&#20943;&#22238;&#25253;&#22914;&#25105;&#20204;&#30340;&#29702;&#35770;&#25152;&#39044;&#27979;&#30340;&#37027;&#26679;&#28165;&#26224;&#21487;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06398v1 Announce Type: cross  Abstract: While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from \emph{catastrophic forgetting} when trained on new tasks in sequence. Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and continual learning. We design one of the first frameworks to analyze Continual Learning Theory and prove that width is directly related to forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that increasing network widths to reduce forgetting yields diminishing returns. We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#22522;&#20110;&#20154;&#20307;&#27979;&#37327;&#30340;&#23478;&#20855;&#23610;&#23544;&#36866;&#21512;&#22823;&#23398;&#29983;&#65292;&#20197;&#25913;&#21892;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#20154;&#20307;&#24037;&#31243;&#23398;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#19982;&#29616;&#26377;&#23478;&#20855;&#23610;&#23544;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#24182;&#26356;&#21152;&#20860;&#23481;</title><link>https://arxiv.org/abs/2403.05589</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#23398;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#20154;&#20307;&#24037;&#31243;&#23398;&#65306;&#19968;&#20010;&#20851;&#20110;&#20154;&#20307;&#27979;&#37327;&#12289;&#23478;&#20855;&#35774;&#35745;&#21644;ANOVA&#27979;&#35797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing Computer Lab Ergonomics in Universities: A Study on Anthropometric Measurements, Furniture Design, and ANOVA Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#22522;&#20110;&#20154;&#20307;&#27979;&#37327;&#30340;&#23478;&#20855;&#23610;&#23544;&#36866;&#21512;&#22823;&#23398;&#29983;&#65292;&#20197;&#25913;&#21892;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#20154;&#20307;&#24037;&#31243;&#23398;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#19982;&#29616;&#26377;&#23478;&#20855;&#23610;&#23544;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#24182;&#26356;&#21152;&#20860;&#23481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#20307;&#24037;&#31243;&#23398;&#35774;&#35745;&#30340;&#23478;&#20855;&#33021;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#21644;&#36523;&#24515;&#20581;&#24247;&#12290;&#38543;&#30528;&#35745;&#31639;&#26426;&#25104;&#20026;&#23398;&#29983;&#23398;&#26415;&#29983;&#27963;&#30340;&#19968;&#37096;&#20998;&#65292;&#23427;&#20204;&#22312;&#26410;&#26469;&#23558;&#36827;&#19968;&#27493;&#26222;&#21450;&#12290;&#25105;&#20204;&#25552;&#20986;&#22522;&#20110;&#20154;&#20307;&#27979;&#37327;&#30340;&#23478;&#20855;&#23610;&#23544;&#65292;&#36866;&#21512;&#22823;&#23398;&#29983;&#20197;&#25913;&#21892;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#30340;&#20154;&#20307;&#24037;&#31243;&#23398;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;380&#21517;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;11&#39033;&#20154;&#20307;&#27979;&#37327;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;11&#39033;&#23478;&#20855;&#23610;&#23544;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#23478;&#20855;&#65306;&#38750;&#21487;&#35843;&#26885;&#23376;&#19982;&#38750;&#21487;&#35843;&#26700;&#23376;&#65292;&#20197;&#21450;&#21487;&#35843;&#26885;&#23376;&#19982;&#38750;&#21487;&#35843;&#26700;&#23376;&#12290;&#19981;&#21305;&#37197;&#35745;&#31639;&#26174;&#31034;&#23478;&#20855;&#23610;&#23544;&#19982;&#20154;&#20307;&#27979;&#37327;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#26174;&#33879;&#27700;&#24179;&#20026;5%&#30340;&#21333;&#22240;&#32032;&#26041;&#24046;&#20998;&#26512;&#27979;&#35797;&#36824;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#21644;&#29616;&#26377;&#30340;&#23478;&#20855;&#23610;&#23544;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#23610;&#23544;&#26356;&#21152;&#20860;&#23481;&#65292;&#20943;&#23569;&#20102;&#19981;&#21305;&#37197;&#30334;&#20998;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05589v1 Announce Type: cross  Abstract: Many studies have shown how ergonomically designed furniture improves productivity and well-being. As computers have become a part of students' academic lives, they will grow further in the future. We propose anthropometric-based furniture dimensions suitable for university students to improve computer laboratory ergonomics. We collected data from 380 participants and analyzed 11 anthropometric measurements, correlating them to 11 furniture dimensions. Two types of furniture were studied: a non-adjustable chair with a non-adjustable table and an adjustable chair with a non-adjustable table. The mismatch calculation showed a significant difference between furniture dimensions and anthropometric measurements. The one-way ANOVA test with a significance level of 5% also showed a significant difference between proposed and existing furniture dimensions. The proposed dimensions were found to be more compatible and reduced mismatch percentage
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MMoE&#65292;&#19968;&#20010;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#21095;&#36879;&#26816;&#27979;&#30340;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#26469;&#22686;&#24378;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05265</link><description>&lt;p&gt;
MMoE: &#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#39046;&#22495;&#24863;&#30693;&#19987;&#23478;&#28151;&#21512;&#30340;&#40065;&#26834;&#21095;&#36879;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MMoE: Robust Spoiler Detection with Multi-modal Information and Domain-aware Mixture-of-Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05265
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MMoE&#65292;&#19968;&#20010;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#21095;&#36879;&#26816;&#27979;&#30340;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#26469;&#22686;&#24378;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#30005;&#24433;&#35780;&#35770;&#32593;&#31449;&#23545;&#20110;&#30005;&#24433;&#20449;&#24687;&#21644;&#35752;&#35770;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#21095;&#36879;&#35780;&#35770;&#20250;&#24433;&#21709;&#35266;&#24433;&#20307;&#39564;&#65292;&#22240;&#27492;&#21095;&#36879;&#26816;&#27979;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#21482;&#20851;&#27880;&#35780;&#35770;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#24573;&#30053;&#20102;&#24179;&#21488;&#20013;&#20449;&#24687;&#30340;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MMoE&#65292;&#19968;&#20010;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#21095;&#36879;&#26816;&#27979;&#30340;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#26469;&#22686;&#24378;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;MMoE&#39318;&#20808;&#20174;&#29992;&#25143;-&#30005;&#24433;&#32593;&#32476;&#20013;&#25552;&#21462;&#22270;&#34920;&#12289;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#29305;&#24449;&#65292;&#20998;&#21035;&#20174;&#35780;&#35770;&#30340;&#25991;&#26412;&#20869;&#23481;&#21644;&#35780;&#35770;&#30340;&#20803;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#22788;&#29702;&#29305;&#23450;&#31867;&#22411;&#30005;&#24433;&#35780;&#35770;&#20013;&#30340;&#21095;&#36879;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05265v1 Announce Type: new  Abstract: Online movie review websites are valuable for information and discussion about movies. However, the massive spoiler reviews detract from the movie-watching experience, making spoiler detection an important task. Previous methods simply focus on reviews' text content, ignoring the heterogeneity of information in the platform. For instance, the metadata and the corresponding user's information of a review could be helpful. Besides, the spoiler language of movie reviews tends to be genre-specific, thus posing a domain generalization challenge for existing methods. To this end, we propose MMoE, a multi-modal network that utilizes information from multiple modalities to facilitate robust spoiler detection and adopts Mixture-of-Experts to enhance domain generalization. MMoE first extracts graph, text, and meta feature from the user-movie network, the review's textual content, and the review's metadata respectively. To handle genre-specific spo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#65292;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#22522;&#26412;&#35268;&#21017;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.05101</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Rule-driven News Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#65292;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#22522;&#26412;&#35268;&#21017;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
News captioning&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#25551;&#36848;&#22270;&#29255;&#21450;&#20854;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#25110;&#20855;&#20307;&#20107;&#20214;&#26469;&#29983;&#25104;&#21477;&#23376;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20381;&#36182;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#21462;&#24471;&#26174;&#33879;&#25104;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#19987;&#27880;&#20110;&#36755;&#20837;&#26032;&#38395;&#20869;&#23481;&#19982;&#36755;&#20986;&#39044;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#38656;&#35201;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#30340;&#19968;&#20123;&#22522;&#26412;&#35268;&#21017;&#65292;&#22914;&#20934;&#30830;&#25551;&#36848;&#19982;&#20107;&#20214;&#30456;&#20851;&#30340;&#20010;&#20307;&#21644;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#25351;&#23450;&#30340;&#35268;&#21017;&#20449;&#21495;&#29983;&#25104;&#22270;&#20687;&#25551;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#25551;&#36848;&#35774;&#35745;&#20102;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#12290;&#36825;&#19968;&#35268;&#21017;&#21253;&#25324;&#22270;&#29255;&#20013;&#25551;&#32472;&#30340;&#20027;&#35201;&#21160;&#20316;&#65288;&#20363;&#22914;&#65292;&#8220;&#25191;&#34892;&#8221;&#65289;&#20197;&#21450;&#21442;&#19982;&#21160;&#20316;&#30340;&#21629;&#21517;&#23454;&#20307;&#25198;&#28436;&#30340;&#35282;&#33394;&#65288;&#20363;&#22914;&#65292;&#8220;&#20195;&#29702;&#20154;&#8221;&#21644;&#8220;&#22320;&#28857;&#8221;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#35821;&#20041;&#35268;&#21017;&#27880;&#20837;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05101v1 Announce Type: cross  Abstract: News captioning task aims to generate sentences by describing named entities or concrete events for an image with its news article. Existing methods have achieved remarkable results by relying on the large-scale pre-trained models, which primarily focus on the correlations between the input news content and the output predictions. However, the news captioning requires adhering to some fundamental rules of news reporting, such as accurately describing the individuals and actions associated with the event. In this paper, we propose the rule-driven news captioning method, which can generate image descriptions following designated rule signal. Specifically, we first design the news-aware semantic rule for the descriptions. This rule incorporates the primary action depicted in the image (e.g., "performing") and the roles played by named entities involved in the action (e.g., "Agent" and "Place"). Second, we inject this semantic rule into th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;-&#20132;&#36890;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;ChatTraffic&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#20132;&#36890;&#31995;&#32479;&#25551;&#36848;&#25991;&#26412;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#39044;&#27979;&#20013;&#20851;&#32852;&#25991;&#26412;&#21644;&#31354;&#38388;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05029</link><description>&lt;p&gt;
BjTT: &#19968;&#20010;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BjTT: A Large-scale Multimodal Dataset for Traffic Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;-&#20132;&#36890;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;ChatTraffic&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#20132;&#36890;&#31995;&#32479;&#25551;&#36848;&#25991;&#26412;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#39044;&#27979;&#20013;&#20851;&#32852;&#25991;&#26412;&#21644;&#31354;&#38388;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05029v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#22522;&#30784;&#20043;&#19968;&#12290;&#20256;&#32479;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#20165;&#20381;&#36182;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#26469;&#39044;&#27979;&#20132;&#36890;&#36235;&#21183;&#65292;&#24182;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;1&#65289;&#23545;&#24322;&#24120;&#20107;&#20214;&#19981;&#25935;&#24863;&#12290;2&#65289;&#22312;&#38271;&#26399;&#39044;&#27979;&#20013;&#24615;&#33021;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#25551;&#36848;&#20132;&#36890;&#31995;&#32479;&#30340;&#25991;&#26412;&#30456;&#32467;&#21512;&#24212;&#29992;&#20110;&#20132;&#36890;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#21629;&#21517;&#35813;&#20219;&#21153;&#20026;&#25991;&#26412;-&#20132;&#36890;&#29983;&#25104;&#65288;TTG&#65289;&#12290;TTG &#20219;&#21153;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#23558;&#25991;&#26412;&#19982;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#20132;&#36890;&#25968;&#25454;&#30456;&#20851;&#32852;&#65292;&#20197;&#29983;&#25104;&#20132;&#36890;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatTraffic&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;-&#20132;&#36890;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#20445;&#35777;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#21462;&#20132;&#36890;&#25968;&#25454;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05029v1 Announce Type: new  Abstract: Traffic prediction is one of the most significant foundations in Intelligent Transportation Systems (ITS). Traditional traffic prediction methods rely only on historical traffic data to predict traffic trends and face two main challenges. 1) insensitivity to unusual events. 2) limited performance in long-term prediction. In this work, we explore how generative models combined with text describing the traffic system can be applied for traffic generation, and name the task Text-to-Traffic Generation (TTG). The key challenge of the TTG task is how to associate text with the spatial structure of the road network and traffic data for generating traffic situations. To this end, we propose ChatTraffic, the first diffusion model for text-to-traffic generation. To guarantee the consistency between synthetic and real data, we augment a diffusion model with the Graph Convolutional Network (GCN) to extract spatial correlations of traffic data. In ad
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#26041;&#27861;&#65292;&#24110;&#21161;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#65292;&#36890;&#36807;&#26500;&#25104;&#35201;&#32032;&#21644;&#20851;&#38190;&#35789;&#36873;&#25321;&#36827;&#34892;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.04369</link><description>&lt;p&gt;
&#20174;&#22270;&#21040;&#35789;&#34955;: &#23558;&#39046;&#22495;&#30693;&#35782;&#24341;&#20837;&#28151;&#28102;&#32618;&#21517;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04369
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#26041;&#27861;&#65292;&#24110;&#21161;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#65292;&#36890;&#36807;&#26500;&#25104;&#35201;&#32032;&#21644;&#20851;&#38190;&#35789;&#36873;&#25321;&#36827;&#34892;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#32618;&#21517;&#39044;&#27979;&#26159;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#26681;&#25454;&#20107;&#23454;&#25551;&#36848;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#12290;&#29616;&#26377;&#30340;&#32618;&#21517;&#39044;&#27979;&#26041;&#27861;&#22312;&#34920;&#29616;&#19978;&#24050;&#32463;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#22788;&#29702;&#28151;&#28102;&#32618;&#21517;&#65288;&#22914;&#25250;&#22842;&#19982;&#25250;&#21163;&#65289;&#26102;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#27861;&#24459;&#39046;&#22495;&#65292;&#26500;&#25104;&#35201;&#32032;&#22312;&#21306;&#20998;&#28151;&#28102;&#32618;&#21517;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#26500;&#25104;&#35201;&#32032;&#26159;&#28508;&#22312;&#21009;&#32602;&#32972;&#21518;&#30340;&#22522;&#26412;&#34892;&#20026;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#32618;&#21517;&#20043;&#38388;&#26377;&#24494;&#22937;&#30340;&#21306;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#65288;FWGB&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26377;&#20851;&#26500;&#25104;&#35201;&#32032;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#25351;&#23548;&#27169;&#22411;&#22312;&#28151;&#28102;&#32618;&#21517;&#19978;&#20570;&#20986;&#21028;&#26029;&#65292;&#31867;&#20284;&#20110;&#27861;&#23448;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26500;&#25104;&#35201;&#32032;&#30340;&#27861;&#24459;&#30693;&#35782;&#22270;&#65292;&#20197;&#24110;&#21161;&#20026;&#27599;&#31181;&#32618;&#21517;&#36873;&#25321;&#20851;&#38190;&#35789;&#65292;&#24418;&#25104;&#19968;&#20010;&#21333;&#35789;&#34955;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04369v1 Announce Type: new  Abstract: Confusing charge prediction is a challenging task in legal AI, which involves predicting confusing charges based on fact descriptions. While existing charge prediction methods have shown impressive performance, they face significant challenges when dealing with confusing charges, such as Snatch and Robbery. In the legal domain, constituent elements play a pivotal role in distinguishing confusing charges. Constituent elements are fundamental behaviors underlying criminal punishment and have subtle distinctions among charges. In this paper, we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces domain knowledge regarding constituent elements to guide the model in making judgments on confusing charges, much like a judge's reasoning process. Specifically, we first construct a legal knowledge graph containing constituent elements to help select keywords for each charge, forming a word bag. Subsequently, to guide the mod
&lt;/p&gt;</description></item><item><title>&#21152;&#24378;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.04321</link><description>&lt;p&gt;
&#30952;&#20855;&#25506;&#27979;&#21644;&#35843;&#25972;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Discriminative Probing and Tuning for Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04321
&lt;/p&gt;
&lt;p&gt;
&#21152;&#24378;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20808;&#21069;&#30340;&#26041;&#27861;&#32463;&#24120;&#38754;&#20020;&#25991;&#26412;&#22270;&#20687;&#19981;&#23545;&#40784;&#31561;&#38382;&#39064;&#65292;&#22914;&#29983;&#25104;&#22270;&#20687;&#20013;&#30340;&#20851;&#31995;&#28151;&#28102;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20132;&#21449;&#27880;&#24847;&#21147;&#25805;&#20316;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#32452;&#21512;&#29702;&#35299;&#65292;&#25110;&#32773;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25913;&#36827;&#24067;&#23616;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;T2I&#27169;&#22411;&#30340;&#22266;&#26377;&#23545;&#40784;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#36890;&#36807;&#23457;&#35270;&#29983;&#25104;&#27169;&#22411;&#21644;&#21028;&#21035;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#35748;&#20026;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#21487;&#33021;&#21453;&#26144;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25991;&#26412;&#22270;&#20687;&#23545;&#40784;&#29087;&#32451;&#24230;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20027;&#24352;&#21152;&#24378;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#31435;&#22312;T2I&#27169;&#22411;&#19978;&#30340;&#21028;&#21035;&#36866;&#37197;&#22120;&#65292;&#20197;&#25506;&#27979;&#23427;&#20204;&#22312;&#20004;&#39033;&#20195;&#34920;&#24615;&#20219;&#21153;&#19978;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#21028;&#21035;&#24494;&#35843;&#26469;&#25913;&#21892;&#23427;&#20204;&#30340;&#25991;&#26412;&#22270;&#20687;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04321v1 Announce Type: cross  Abstract: Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#26032;&#38395;&#25688;&#35201;&#20013;&#24187;&#35273;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;absinth&#65292;&#25506;&#35752;&#20102;LLMs&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.03750</link><description>&lt;p&gt;
&#24503;&#35821;&#20063;&#20135;&#29983;&#24187;&#35273;&#65281;&#20351;&#29992;Absinth&#25968;&#25454;&#38598;&#26816;&#27979;&#26032;&#38395;&#25688;&#35201;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#26032;&#38395;&#25688;&#35201;&#20013;&#24187;&#35273;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;absinth&#65292;&#25506;&#35752;&#20102;LLMs&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#20135;&#29983;&#20449;&#24687;&#24187;&#35273;&#30340;&#38382;&#39064;&#65292;&#36825;&#22312;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#22810;&#35821;&#35328;&#26041;&#27861;&#32570;&#20047;&#24503;&#35821;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;absinth&#65292;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#26032;&#38395;&#25688;&#35201;&#20013;&#24187;&#35273;&#26816;&#27979;&#30340;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#35752;&#20102;&#26032;&#22411;&#24320;&#28304;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#35774;&#32622;&#12290;&#25105;&#20204;&#24320;&#28304;&#24182;&#21457;&#24067;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03750v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has led to remarkable progress on a wide range of natural language processing tasks. Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document. Previous research addresses the challenging task of detecting hallucinations in the output (i.e. inconsistency detection) in order to evaluate the faithfulness of the generated summaries. However, these works primarily focus on English and recent multilingual approaches lack German data. This work presents absinth, a manually annotated dataset for hallucination detection in German news summarization and explores the capabilities of novel open-source LLMs on this task in both fine-tuning and in-context learning settings. We open-source and releas
&lt;/p&gt;</description></item><item><title>TTA-Nav&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#39030;&#21521;&#19979;&#35299;&#30721;&#22120;&#65292;&#20174;&#25439;&#22351;&#22270;&#20687;&#20013;&#37325;&#24314;&#20986;&#26356;&#28165;&#26224;&#30340;&#22270;&#20687;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#28857;&#30446;&#26631;&#23548;&#33322;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01977</link><description>&lt;p&gt;
TTA-Nav: &#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#37325;&#24314;&#29992;&#20110;&#35270;&#35273;&#25439;&#22351;&#19979;&#30340;&#28857;&#30446;&#26631;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation under Visual Corruptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01977
&lt;/p&gt;
&lt;p&gt;
TTA-Nav&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#39030;&#21521;&#19979;&#35299;&#30721;&#22120;&#65292;&#20174;&#25439;&#22351;&#22270;&#20687;&#20013;&#37325;&#24314;&#20986;&#26356;&#28165;&#26224;&#30340;&#22270;&#20687;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#28857;&#30446;&#26631;&#23548;&#33322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01977v1 &#20844;&#21578;&#31867;&#22411;: &#36328;  &#25688;&#35201;: &#22312;&#35270;&#35273;&#25439;&#22351;&#19979;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TTA-Nav&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35270;&#35273;&#25439;&#22351;&#19979;&#30340;&#28857;&#30446;&#26631;&#23548;&#33322;&#12290;&#25105;&#20204;&#30340;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#26041;&#27861;&#23558;&#33258;&#39030;&#21521;&#19979;&#30340;&#35299;&#30721;&#22120;&#19982;&#39044;&#35757;&#32451;&#30340;&#23548;&#33322;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#39318;&#20808;&#65292;&#39044;&#35757;&#32451;&#30340;&#23548;&#33322;&#27169;&#22411;&#25509;&#25910;&#19968;&#20010;&#25439;&#22351;&#30340;&#22270;&#20687;&#24182;&#25552;&#21462;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#33258;&#39030;&#21521;&#19979;&#30340;&#35299;&#30721;&#22120;&#26681;&#25454;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#30340;&#39640;&#32423;&#29305;&#24449;&#29983;&#25104;&#37325;&#24314;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#23558;&#25439;&#22351;&#22270;&#20687;&#30340;&#37325;&#24314;&#22270;&#20687;&#39304;&#36865;&#22238;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#20877;&#27425;&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#20197;&#36755;&#20986;&#21160;&#20316;&#12290;&#23613;&#31649;&#20165;&#22312;&#28165;&#26224;&#22270;&#20687;&#19978;&#35757;&#32451;&#65292;&#33258;&#39030;&#21521;&#19979;&#30340;&#35299;&#30721;&#22120;&#21487;&#20197;&#20174;&#25439;&#22351;&#22270;&#20687;&#20013;&#37325;&#24314;&#20986;&#26356;&#28165;&#26224;&#30340;&#22270;&#20687;&#65292;&#26080;&#38656;&#22522;&#20110;&#26799;&#24230;&#30340;&#33258;&#36866;&#24212;&#12290;&#20855;&#26377;&#25105;&#20204;&#33258;&#39030;&#21521;&#19979;&#35299;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#23548;&#33322;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#23548;&#33322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01977v1 Announce Type: cross  Abstract: Robot navigation under visual corruption presents a formidable challenge. To address this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav, for point-goal navigation under visual corruptions. Our "plug-and-play" method incorporates a top-down decoder to a pre-trained navigation model. Firstly, the pre-trained navigation model gets a corrupted image and extracts features. Secondly, the top-down decoder produces the reconstruction given the high-level features extracted by the pre-trained model. Then, it feeds the reconstruction of a corrupted image back to the pre-trained model. Finally, the pre-trained model does forward pass again to output action. Despite being trained solely on clean images, the top-down decoder can reconstruct cleaner images from corrupted ones without the need for gradient-based adaptation. The pre-trained navigation model with our top-down decoder significantly enhances navigation performance acr
&lt;/p&gt;</description></item><item><title>AllSpark&#21033;&#29992;&#36890;&#36947;&#32423;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#20174;&#26410;&#26631;&#35760;&#30340;&#29305;&#24449;&#20013;&#37325;&#26032;&#29983;&#25104;&#26631;&#35760;&#29305;&#24449;&#65292;&#20197;&#25913;&#21892;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#20302;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01818</link><description>&lt;p&gt;
AllSpark: &#21033;&#29992;Transformer&#20013;&#26410;&#26631;&#35760;&#30340;&#29305;&#24449;&#37325;&#26032;&#29983;&#25104;&#26631;&#35760;&#29305;&#24449;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01818
&lt;/p&gt;
&lt;p&gt;
AllSpark&#21033;&#29992;&#36890;&#36947;&#32423;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#20174;&#26410;&#26631;&#35760;&#30340;&#29305;&#24449;&#20013;&#37325;&#26032;&#29983;&#25104;&#26631;&#35760;&#29305;&#24449;&#65292;&#20197;&#25913;&#21892;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#20302;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;SSSS&#65289;&#26088;&#22312;&#20943;&#36731;&#32791;&#26102;&#30340;&#20687;&#32032;&#32423;&#25163;&#21160;&#26631;&#27880;&#36127;&#25285;&#65292;&#23427;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#20197;&#21450;&#26356;&#22810;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20934;&#30495;&#20540;&#35757;&#32451;&#26631;&#35760;&#25968;&#25454;&#21644;&#20351;&#29992;&#20266;&#26631;&#31614;&#35757;&#32451;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#35757;&#32451;&#27969;&#31243;&#26159;&#20998;&#24320;&#30340;&#65292;&#36825;&#20351;&#24471;&#26631;&#35760;&#25968;&#25454;&#20027;&#23548;&#35757;&#32451;&#36807;&#31243;&#65292;&#23548;&#33268;&#20302;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#21644;&#20174;&#32780;&#27425;&#20248;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AllSpark&#65292;&#21033;&#29992;&#36890;&#36947;&#32423;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#20174;&#26410;&#26631;&#35760;&#30340;&#29305;&#24449;&#20013;&#37325;&#26032;&#29983;&#25104;&#26631;&#35760;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#35821;&#20041;&#35760;&#24518;&#21644;&#36890;&#36947;&#35821;&#20041;&#20998;&#32452;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#26410;&#26631;&#35760;&#29305;&#24449;&#20805;&#20998;&#20195;&#34920;&#26631;&#35760;&#29305;&#24449;&#12290;AllSpark&#20026;SSSS&#30340;&#26550;&#26500;&#32423;&#35774;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#32780;&#38750;&#26694;&#26550;&#32423;&#21035;&#65292;&#36991;&#20813;&#20102;&#36234;&#26469;&#36234;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01818v1 Announce Type: cross  Abstract: Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate the burden of time-consuming pixel-level manual labeling, which leverages limited labeled data along with larger amounts of unlabeled data. Current state-of-the-art methods train the labeled data with ground truths and unlabeled data with pseudo labels. However, the two training flows are separate, which allows labeled data to dominate the training process, resulting in low-quality pseudo labels and, consequently, sub-optimal results. To alleviate this issue, we present AllSpark, which reborns the labeled features from unlabeled ones with the channel-wise cross-attention mechanism. We further introduce a Semantic Memory along with a Channel Semantic Grouping strategy to ensure that unlabeled features adequately represent labeled features. The AllSpark shed new light on the architecture level designs of SSSS rather than framework level, which avoids increasingly
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550; Diffusion-TS&#65292;&#32467;&#21512;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#21644;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#38750;&#22122;&#22768;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#26088;&#22312;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01742</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25193;&#25955;&#29992;&#20110;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diffusion-TS: Interpretable Diffusion for General Time Series Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01742
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550; Diffusion-TS&#65292;&#32467;&#21512;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#21644;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#38750;&#22122;&#22768;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#26088;&#22312;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Denoising diffusion probabilistic models (DDPMs)&#27491;&#36880;&#28176;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#27969;&#33539;&#24335;&#65292;&#26368;&#36817;&#24050;&#22312;&#38899;&#39057;&#21512;&#25104;&#12289;&#26102;&#38388;&#24207;&#21015;&#22635;&#34917;&#21644;&#39044;&#27979;&#31561;&#39046;&#22495;&#21462;&#24471;&#31361;&#30772;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-TS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#20854;&#20013;&#20998;&#35299;&#25216;&#26415;&#25351;&#23548;Diffusion-TS&#25429;&#33719;&#26102;&#38388;&#24207;&#21015;&#30340;&#35821;&#20041;&#21547;&#20041;&#65292;&#32780;&#21464;&#21387;&#22120;&#20174;&#22024;&#26434;&#30340;&#27169;&#22411;&#36755;&#20837;&#20013;&#25366;&#25496;&#35814;&#32454;&#30340;&#24207;&#21015;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#19981;&#26159;&#22312;&#27599;&#20010;&#25193;&#25955;&#27493;&#39588;&#20013;&#37325;&#24314;&#22122;&#22768;&#65292;&#24182;&#32467;&#21512;&#20102;&#22522;&#20110;Fourier&#30340;&#25439;&#22833;&#39033;&#12290;&#39044;&#26399;Diffusion-TS&#21487;&#20197;&#29983;&#25104;&#26082;&#20855;&#26377;&#35299;&#37322;&#24615;&#21448;&#30495;&#23454;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36824;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;Diffusion
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01742v1 Announce Type: cross  Abstract: Denoising diffusion probabilistic models (DDPMs) are becoming the leading paradigm for generative models. It has recently shown breakthroughs in audio synthesis, time series imputation and forecasting. In this paper, we propose Diffusion-TS, a novel diffusion-based framework that generates multivariate time series samples of high quality by using an encoder-decoder transformer with disentangled temporal representations, in which the decomposition technique guides Diffusion-TS to capture the semantic meaning of time series while transformers mine detailed sequential information from the noisy model input. Different from existing diffusion-based approaches, we train the model to directly reconstruct the sample instead of the noise in each diffusion step, combining a Fourier-based loss term. Diffusion-TS is expected to generate time series satisfying both interpretablity and realness. In addition, it is shown that the proposed Diffusion-T
&lt;/p&gt;</description></item><item><title>VBART&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;BART&#21644;mBART&#27169;&#22411;&#32467;&#21512;&#24418;&#25104;&#20102;&#32039;&#20945;&#22411;LLM&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01308</link><description>&lt;p&gt;
VBART: &#22303;&#32819;&#20854;LLM
&lt;/p&gt;
&lt;p&gt;
VBART: The Turkish LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01308
&lt;/p&gt;
&lt;p&gt;
VBART&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;BART&#21644;mBART&#27169;&#22411;&#32467;&#21512;&#24418;&#25104;&#20102;&#32039;&#20945;&#22411;LLM&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VBART&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#19968;&#20010;&#22823;&#35821;&#26009;&#24211;&#19978;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;VBART&#26159;&#22522;&#20110;BART&#21644;mBART&#27169;&#22411;&#30340;&#22909;&#24605;&#36335;&#26500;&#24314;&#30340;&#32039;&#20945;&#22411;LLMs&#65292;&#20998;&#20026;Large&#21644;XLarge&#20004;&#20010;&#23610;&#23544;&#12290;&#24494;&#35843;&#21518;&#30340;VBART&#27169;&#22411;&#22312;&#25552;&#21462;&#24615;&#25991;&#26412;&#25688;&#35201;&#12289;&#26631;&#39064;&#29983;&#25104;&#12289;&#25991;&#26412;&#25913;&#20889;&#12289;&#38382;&#31572;&#21644;&#38382;&#39064;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#23427;&#20204;&#20801;&#35768;&#20026;&#26410;&#26469;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#25317;&#26377;&#20026;&#22303;&#32819;&#20854;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;LLM&#27604;&#22810;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#20102;&#26368;&#22810;3&#20493;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#65292;&#24182;&#20026;&#35757;&#32451;&#21644;&#25512;&#29702;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21333;&#35821;&#20998;&#35789;&#22120;&#27604;OpenAI&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#26356;&#39640;&#25928;7&#20493;&#12290;&#26368;&#21518;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;&#29616;&#26377;&#39044;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01308v1 Announce Type: new  Abstract: We present VBART, the first Turkish sequence-to-sequence Large Language Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact LLMs based on good ideas leveraged from BART and mBART models and come in two sizes, Large and XLarge. Fine-tuned VBART models surpass the prior state-of-the-art results in abstractive text summarization, title generation, text paraphrasing, question answering and question generation tasks. They allow fine-tuning for future text generation tasks and datasets, carving a new path for Turkish Natural Language Processing (NLP) research. Our work shows that having a pre-trained LLM for Turkish outperforms up to 3x multilingual models, improving existing results and providing efficient models for training and inference. Moreover, we show that our monolingual tokenizer is 7x more efficient than OpenAI's multilingual tokenizer. Last but not least, we introduce a method to enlarge an existing pre-trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#32467;&#26524;&#34920;&#26126;RAG&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01193</link><description>&lt;p&gt;
RAGged Edges: Retrieval-Augmented Chatbots&#30340;&#21452;&#20995;&#21073;
&lt;/p&gt;
&lt;p&gt;
RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#32467;&#26524;&#34920;&#26126;RAG&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273; - &#29983;&#25104;&#30475;&#20284;&#27491;&#30830;&#20294;&#38169;&#35823;&#20449;&#24687;&#30340;&#20542;&#21521;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#24456;&#20851;&#38190;&#65292;&#23601;&#20687;&#26368;&#36817;&#30340;&#27861;&#38498;&#26696;&#20363;&#20013;&#30475;&#21040;&#30340;&#37027;&#26679;&#65292;ChatGPT&#30340;&#20351;&#29992;&#23548;&#33268;&#20102;&#19981;&#23384;&#22312;&#30340;&#27861;&#24459;&#35009;&#20915;&#30340;&#24341;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#22806;&#37096;&#30693;&#35782;&#19982;&#25552;&#31034;&#38598;&#25104;&#26469;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26469;&#25269;&#21046;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26088;&#22312;&#35825;&#23548;&#24187;&#35273;&#30340;&#25552;&#31034;&#26469;&#23545;RAG&#19982;&#26631;&#20934;LLMs&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;RAG&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24403;&#25552;&#31034;&#30452;&#25509;&#19982;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#29702;&#35299;&#30456;&#30683;&#30462;&#26102;&#65292;RAG&#20173;&#28982;&#20250;&#34987;&#35823;&#23548;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#24187;&#35273;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RAG&#37096;&#32626;&#30340;&#23454;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01193v1 Announce Type: cross  Abstract: Large language models (LLMs) like ChatGPT demonstrate the remarkable progress of artificial intelligence. However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge. This issue is critical, as seen in recent court cases where ChatGPT's use led to citations of non-existent legal rulings. This paper explores how Retrieval-Augmented Generation (RAG) can counter hallucinations by integrating external knowledge with prompts. We empirically evaluate RAG against standard LLMs using prompts designed to induce hallucinations. Our results show that RAG increases accuracy in some cases, but can still be misled when prompts directly contradict the model's pre-trained understanding. These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications. We offer practical recommendations for RAG deployment and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00895</link><description>&lt;p&gt;
&#31934;&#30830;&#25512;&#33616;&#30340;&#31471;&#21040;&#31471;&#22270;-&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-end Graph-Sequential Representation Learning for Accurate Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#25512;&#33616;&#31995;&#32479;&#30340;&#35768;&#22810;&#26032;&#36827;&#23637;&#38598;&#20013;&#22312;&#24320;&#21457;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#19978;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#24314;&#27169;&#34892;&#20026;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#37117;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22312;&#20010;&#24615;&#21270;&#25490;&#21517;&#21644;&#19979;&#19968;&#20010;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#30410;&#30340;&#25104;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20174;&#25968;&#25454;&#20013;&#25429;&#25417;&#21040;&#30340;&#20449;&#21495;&#25130;&#28982;&#19981;&#21516;&#12290;&#21069;&#32773;&#30452;&#25509;&#36890;&#36807;&#19982;&#26368;&#36817;&#29289;&#21697;&#30340;&#26377;&#24207;&#20132;&#20114;&#26469;&#34920;&#31034;&#29992;&#25143;&#65292;&#32780;&#21518;&#32773;&#26088;&#22312;&#25429;&#25417;&#20132;&#20114;&#22270;&#20013;&#30340;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36825;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30456;&#20114;&#35757;&#32451;&#24207;&#21015;&#21644;&#22270;&#32452;&#20214;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00895v1 Announce Type: cross  Abstract: Many recent advancements in recommender systems have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter one aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework that exploits the synergies between these two paradigms. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance.
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31243;&#24207;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#28041;&#21450;&#22823;&#37327;&#25968;&#23383;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2403.00795</link><description>&lt;p&gt;
&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31639;&#27861;&#65306;&#19968;&#39033;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Executing Natural Language-Described Algorithms with Large Language Models: An Investigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00795
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31243;&#24207;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#28041;&#21450;&#22823;&#37327;&#25968;&#23383;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#36861;&#27714;&#12290;&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20986;&#30340;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#20986;&#29616;&#65292;&#36825;&#19968;&#30446;&#26631;&#30340;&#36947;&#36335;&#24050;&#32463;&#34987;&#38416;&#26126;&#12290;&#26412;&#25991;&#26088;&#22312;&#26816;&#39564;&#29616;&#26377;LLMs&#29702;&#35299;&#21644;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20174;&#12298;&#31639;&#27861;&#23548;&#35770;&#12299;&#20013;&#36873;&#21462;&#20102;&#19968;&#20010;&#31639;&#27861;&#27979;&#35797;&#38598;&#65292;&#35813;&#20070;&#26159;&#19968;&#26412;&#21253;&#21547;&#35768;&#22810;&#20195;&#34920;&#24615;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#30340;&#30693;&#21517;&#25945;&#26448;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25191;&#34892;&#33021;&#21147;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;30&#20010;&#31639;&#27861;&#65292;&#20849;&#29983;&#25104;&#20102;300&#20010;&#38543;&#26426;&#25277;&#26679;&#23454;&#20363;&#65292;&#24182;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;LLMs&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#21644;&#25191;&#34892;&#36825;&#20123;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#29305;&#21035;&#26159;GPT-4&#31561;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31243;&#24207;&#65292;&#21482;&#35201;&#19981;&#28041;&#21450;&#22823;&#37327;&#25968;&#23383;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00795v1 Announce Type: cross  Abstract: Executing computer programs described in natural language has long been a pursuit of computer science. With the advent of enhanced natural language understanding capabilities exhibited by large language models (LLMs), the path toward this goal has been illuminated. In this paper, we seek to examine the capacity of present-day LLMs to comprehend and execute algorithms outlined in natural language. We established an algorithm test set sourced from Introduction to Algorithm, a well-known textbook that contains many representative widely-used algorithms. To systematically assess LLMs' code execution abilities, we selected 30 algorithms, generated 300 random-sampled instances in total, and evaluated whether popular LLMs can understand and execute these algorithms. Our findings reveal that LLMs, notably GPT-4, can effectively execute programs described in natural language, as long as no heavy numeric computation is involved. We believe our f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26799;&#24230;&#22411;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20294;&#20173;&#33021;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.19078</link><description>&lt;p&gt;
&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Smooth Tchebycheff Scalarization for Multi-Objective Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19078
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26799;&#24230;&#22411;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20294;&#20173;&#33021;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#37117;&#33021;&#25214;&#21040;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#32463;&#24120;&#30456;&#20114;&#20914;&#31361;&#65292;&#19981;&#33021;&#36890;&#36807;&#21333;&#20010;&#35299;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#25214;&#21040;&#24085;&#32047;&#25176;&#35299;&#65292;&#36825;&#20123;&#35299;&#20195;&#34920;&#20102;&#23545;&#20110;&#32473;&#23450;&#38382;&#39064;&#30340;&#19981;&#21516;&#26368;&#20339;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#21487;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#21487;&#33021;&#19981;&#33021;&#20855;&#22791;&#35299;&#20915;&#19968;&#33324;&#21487;&#24494;&#20998;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#29702;&#35770;&#23646;&#24615;&#12290;&#22312;&#26412;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#20809;&#28369;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36731;&#37327;&#30340;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#23427;&#23545;&#20110;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26174;&#30528;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#32467;&#26524;&#19978;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19078v1 Announce Type: cross  Abstract: Multi-objective optimization problems can be found in many real-world applications, where the objectives often conflict each other and cannot be optimized by a single solution. In the past few decades, numerous methods have been proposed to find Pareto solutions that represent different optimal trade-offs among the objectives for a given problem. However, these existing methods could have high computational complexity or may not have good theoretical properties for solving a general differentiable multi-objective optimization problem. In this work, by leveraging the smooth optimization technique, we propose a novel and lightweight smooth Tchebycheff scalarization approach for gradient-based multi-objective optimization. It has good theoretical properties for finding all Pareto solutions with valid trade-off preferences, while enjoying significantly lower computational complexity compared to other methods. Experimental results on variou
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17516</link><description>&lt;p&gt;
QUCE: &#20943;&#23569;&#21644;&#37327;&#21270;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#29983;&#25104;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17516
&lt;/p&gt;
&lt;p&gt;
QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#23398;&#31185; &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26368;&#31361;&#20986;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;DNNs&#30340;&#26377;&#25928;&#24615;&#38543;&#30528;&#26368;&#36817;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#32780;&#28608;&#22686;&#65292;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25193;&#23637;&#21040;&#22788;&#29702;&#22823;&#25968;&#25454;&#20013;&#30340;&#37325;&#35201;&#22797;&#26434;&#24615;&#20197;&#24212;&#23545;&#39044;&#27979;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;DNN&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#25552;&#39640;&#65292;&#21487;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35832;&#22914;&#23545;&#25239;&#26799;&#24230;&#25972;&#21512;&#65288;AGI&#65289;&#36825;&#26679;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#21033;&#29992;DNN&#25552;&#20379;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#26799;&#24230;&#26469;&#38416;&#26126;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#26799;&#24230;&#22312;&#36234;&#30028;&#36335;&#24452;&#36941;&#21382;&#26399;&#38388;&#34920;&#29616;&#20986;&#19981;&#35268;&#21017;&#24615;&#26102;&#65292;&#22522;&#20110;&#36335;&#24452;&#30340;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#25439;&#23475;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Quantified Uncertainty Counterfactual Explanations&#65288;QUCE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#36234;&#30028;&#36941;&#21382;&#12290; QUCE&#19981;&#20165;&#22312;&#25552;&#20986;&#35299;&#37322;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 Announce Type: cross  Abstract: Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#20013;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#21452;&#21521;&#20132;&#20114;</title><link>https://arxiv.org/abs/2402.13897</link><description>&lt;p&gt;
&#31185;&#23398;&#26816;&#26597;&#32773;&#20877;&#24230;&#21319;&#32423;&#65306;&#36879;&#26126;&#24230;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#21452;&#21521;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#20013;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#21452;&#21521;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#28982;&#38754;&#20020;&#30528;&#22312;&#31185;&#23398;&#21644;&#24037;&#19994;&#30340;&#28023;&#37327;&#20449;&#24687;&#20013;&#30340;&#35832;&#22810;&#38480;&#21046;&#65292;&#27604;&#22914;&#35821;&#20041;&#20998;&#27495;&#21644;&#26816;&#32034;&#20013;&#30340;&#35789;&#27719;&#24046;&#36317;&#12289;&#35821;&#20041;&#25628;&#32034;&#20013;&#30340;&#20302;&#31934;&#24230;&#21644;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#25110;&#32773;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#30340;&#36825;&#20123;&#38556;&#30861;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#36890;&#36807;&#26597;&#35810;&#25193;&#23637;&#22686;&#24378;&#20102;&#22312;&#31232;&#30095;&#26816;&#32034;&#20013;&#30340;&#35821;&#35328;&#29702;&#35299;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#12290;&#31532;&#20108;&#20010;&#27169;&#22359;&#36890;&#36807;&#21482;&#20351;&#29992;&#38271;&#25991;&#26723;&#20013;&#20256;&#25773;&#30340;&#20449;&#24687;&#65292;&#20026;&#22797;&#26434;&#38382;&#39064;&#25552;&#20379;&#20840;&#38754;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#31572;&#26696;&#26469;&#21152;&#28145;&#32467;&#26524;&#65292;&#23454;&#29616;&#21452;&#21521;&#20132;&#20114;&#12290;&#22312;&#31649;&#36947;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21521;&#29992;&#25143;&#21576;&#29616;&#20013;&#38388;&#32467;&#26524;&#20197;&#20419;&#36827;&#23545;&#31995;&#32479;&#25512;&#29702;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#21452;&#21521;&#26041;&#27861;&#24102;&#26469;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13897v1 Announce Type: cross  Abstract: Information retrieval is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning. We believe this bidirectional approach brings
&lt;/p&gt;</description></item><item><title>Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;</title><link>https://arxiv.org/abs/2402.12749</link><description>&lt;p&gt;
Me LLaMA: &#20026;&#21307;&#30103;&#24212;&#29992;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Me LLaMA: Foundation Large Language Models for Medical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12749
&lt;/p&gt;
&lt;p&gt;
Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#19981;&#22815;&#29702;&#24819;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22312;&#22823;&#22411;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Me LLaMA&#65292;&#19968;&#20010;&#21307;&#23398;LLM&#31995;&#21015;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;- Me LLaMA 13/70B&#21450;&#20854; chat-enhanced &#29256;&#26412;- Me LLaMA 13/70B-chat&#65292;&#36890;&#36807;&#25345;&#32493;&#23545;LLaMA2&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25968;&#25454;&#24320;&#21457;&#32780;&#25104;&#12290;&#25105;&#20204;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#22871;&#20214;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;129B tokens&#30340;&#22823;&#35268;&#27169;&#25345;&#32493;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#21253;&#21547;214k&#20010;&#26679;&#26412;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#36328;&#36234;14&#20010;&#25968;&#25454;&#38598;&#30340;&#20845;&#39033;&#20219;&#21153;&#30340;&#21307;&#23398;&#35780;&#20272;&#22522;&#20934;(MIBE)&#12290;&#25105;&#20204;&#20351;&#29992;MIBE&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#65292;Me LLaMA&#27169;&#22411;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24320;&#28304;&#21307;&#23398;LLMs&#65292;&#24182;&#19988;&#22312;&#21830;&#19994;&#24040;&#22836;&#22914;ChatGPT&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12749v1 Announce Type: cross  Abstract: Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411; PAC-FNO&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#25805;&#20316;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#31867;&#38382;&#39064;&#19978;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12721</link><description>&lt;p&gt;
PAC-FNO&#65306;&#24182;&#34892;&#32467;&#26500;&#20840;&#32452;&#20998;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#35782;&#21035;&#20302;&#36136;&#37327;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12721
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411; PAC-FNO&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#25805;&#20316;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#31867;&#38382;&#39064;&#19978;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#26631;&#20934;&#20570;&#27861;&#26159;&#22312;&#29305;&#23450;&#22270;&#20687;&#20998;&#36776;&#29575;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#37096;&#32626;&#23427;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#25512;&#29702;&#20013;&#65292;&#27169;&#22411;&#32463;&#24120;&#36935;&#21040;&#19982;&#35757;&#32451;&#38598;&#20013;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21644;/&#25110;&#21463;&#21040;&#33258;&#28982;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#22825;&#27668;&#21464;&#21270;&#12289;&#22122;&#22768;&#31867;&#22411;&#21644;&#21387;&#32553;&#20266;&#24433;&#12290;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#20026;&#19981;&#21516;&#20998;&#36776;&#29575;&#25110;&#36755;&#20837;&#21464;&#21270;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#19981;&#21487;&#25193;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21363;&#24182;&#34892;&#32467;&#26500;&#21644;&#20840;&#32452;&#20998;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;PAC-FNO&#65289;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;PAC-FNO&#22312;&#39057;&#22495;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21333;&#20010;&#27169;&#22411;&#20869;&#22788;&#29702;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#20197;&#26368;&#23567;&#30340;&#20462;&#25913;&#35757;&#32451;PAC-FNO&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12721v1 Announce Type: cross  Abstract: A standard practice in developing image recognition models is to train a model on a specific image resolution and then deploy it. However, in real-world inference, models often encounter images different from the training sets in resolution and/or subject to natural variations such as weather changes, noise types and compression artifacts. While traditional solutions involve training multiple models for different resolutions or input variations, these methods are computationally expensive and thus do not scale in practice. To this end, we propose a novel neural network model, parallel-structured and all-component Fourier neural operator (PAC-FNO), that addresses the problem. Unlike conventional feed-forward neural networks, PAC-FNO operates in the frequency domain, allowing it to handle images of varying resolutions within a single model. We also propose a two-stage algorithm for training PAC-FNO with a minimal modification to the orig
&lt;/p&gt;</description></item><item><title>&#23613;&#31649;&#27169;&#22411;&#32534;&#36753;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20462;&#35746;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20294;&#23569;&#37327;&#32534;&#36753;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#23849;&#28291;&#65292;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22256;&#24785;&#24230;&#20316;&#20026;&#26367;&#20195;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20854;&#19982;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09656</link><description>&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#30340;&#34676;&#34678;&#25928;&#24212;&#65306;&#23569;&#37327;&#32534;&#36753;&#21487;&#33021;&#24341;&#21457;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09656
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#27169;&#22411;&#32534;&#36753;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20462;&#35746;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20294;&#23569;&#37327;&#32534;&#36753;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#23849;&#28291;&#65292;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22256;&#24785;&#24230;&#20316;&#20026;&#26367;&#20195;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20854;&#19982;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#27169;&#22411;&#32534;&#36753;&#24050;&#26174;&#31034;&#20986;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#20462;&#35746;&#30693;&#35782;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#23545;LLMs&#30340;&#20869;&#22312;&#33021;&#21147;&#30340;&#24433;&#21709;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#20851;&#38190;&#29616;&#35937;&#65306;&#21363;&#20351;&#21482;&#36827;&#34892;&#19968;&#20010;&#32534;&#36753;&#65292;&#20063;&#21487;&#20197;&#24341;&#21457;&#27169;&#22411;&#23849;&#28291;&#65292;&#34920;&#29616;&#20026;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#27599;&#27425;&#32534;&#36753;&#21518;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#34429;&#28982;&#24517;&#35201;&#65292;&#20294;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22256;&#24785;&#24230;&#20316;&#20026;&#26367;&#20195;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20854;&#19982;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#39034;&#24207;&#32534;&#36753;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#36825;&#26159;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#19968;&#31181;&#24120;&#35265;&#24773;&#20917;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#25105;&#20204;&#20043;&#21069;&#21333;&#27425;&#32534;&#36753;&#30740;&#31350;&#20013;&#30340;&#22256;&#38590;&#26696;&#20363;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20960;&#20046;&#25152;&#26377;&#30740;&#31350;&#30340;&#32534;&#36753;&#26041;&#27861;&#37117;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09656v1 Announce Type: new  Abstract: Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating its strong correlation with downstream task performance. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse
&lt;/p&gt;</description></item><item><title>VisLingInstruct&#36890;&#36807;&#33258;&#20027;&#20248;&#21270;&#25351;&#23548;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;TextVQA&#21644;HatefulMemes&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#25552;&#39640;&#20102;13.1%&#21644;9%&#12290;</title><link>https://arxiv.org/abs/2402.07398</link><description>&lt;p&gt;
VisLingInstruct: &#36890;&#36807;&#33258;&#20027;&#25351;&#23548;&#20248;&#21270;&#25552;&#21319;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07398
&lt;/p&gt;
&lt;p&gt;
VisLingInstruct&#36890;&#36807;&#33258;&#20027;&#20248;&#21270;&#25351;&#23548;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;TextVQA&#21644;HatefulMemes&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#25552;&#39640;&#20102;13.1%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;VisLingInstruct&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#36827;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25351;&#23548;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;VisLingInstruct&#36890;&#36807;&#33258;&#20027;&#35780;&#20272;&#21644;&#20248;&#21270;&#25351;&#23548;&#25991;&#26412;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25913;&#36827;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#35270;&#35273;&#24863;&#30693;&#21644;&#35821;&#35328;&#34920;&#36798;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#38500;&#20102;&#25351;&#23548;&#25991;&#26412;&#30340;&#25913;&#36827;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#23545;&#25991;&#26412;&#25552;&#31034;&#30340;&#21709;&#24212;&#33021;&#21147;&#12290;&#22522;&#20110;FlanT5&#21644;Vicuna&#30340;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;VisLingInstruct&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#35270;&#35273;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#22312;TextVQA&#21644;HatefulMemes&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;13.1%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents VisLingInstruct, a novel approach to advancing Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show impressive zero-shot abilities in multi-modal tasks, but their performance depends heavily on the quality of instructions. VisLingInstruct tackles this by autonomously evaluating and optimizing instructional texts through In-Context Learning, improving the synergy between visual perception and linguistic expression in MMLMs. Alongside this instructional advancement, we have also optimized the visual feature extraction modules in MMLMs, further augmenting their responsiveness to textual cues. Our comprehensive experiments on MMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1% and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and HatefulMemes datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedImpro&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#28418;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;&#30340;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#27867;&#21270;&#36129;&#29486;&#24182;&#20943;&#23567;&#20102;&#26799;&#24230;&#30340;&#24046;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07011</link><description>&lt;p&gt;
FedImpro: &#27979;&#37327;&#21644;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
FedImpro: Measuring and Improving Client Update in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedImpro&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#28418;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;&#30340;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#27867;&#21270;&#36129;&#29486;&#24182;&#20943;&#23567;&#20102;&#26799;&#24230;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20250;&#21463;&#21040;&#24322;&#26500;&#25968;&#25454;&#24341;&#36215;&#30340;&#23458;&#25143;&#28418;&#31227;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#25968;&#25454;&#30340;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#23458;&#25143;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#36827;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#25805;&#20316;&#29616;&#26377;&#30340;&#26799;&#24230;&#65292;&#20197;&#23454;&#29616;&#26356;&#19968;&#33268;&#30340;&#23458;&#25143;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#20998;&#26512;&#20102;&#23458;&#25143;&#28418;&#31227;&#65292;&#24182;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#20943;&#36731;&#36825;&#31181;&#28418;&#31227;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26412;&#22320;&#35757;&#32451;&#30340;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#36825;&#31181;&#27867;&#21270;&#36129;&#29486;&#21463;&#21040;&#19981;&#21516;&#23458;&#25143;&#30340;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#26465;&#20214;Wasserstein&#36317;&#31163;&#30340;&#38480;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedImpro&#65292;&#29992;&#20110;&#26500;&#24314;&#31867;&#20284;&#30340;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FedImpro&#23558;&#27169;&#22411;&#20998;&#35299;&#20026;&#39640;&#23618;&#21644;&#20302;&#23618;&#32452;&#20214;&#65292;&#24182;&#23545;&#37325;&#26500;&#29305;&#24449;&#20998;&#24067;&#19978;&#30340;&#39640;&#23618;&#37096;&#20998;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#20943;&#23567;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#26799;&#24230;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models. First, we analyze the generalization contribution of local training and conclude that this generalization contribution is bounded by the conditional Wasserstein distance between the data distribution of different clients. Then, we propose FedImpro, to construct similar conditional distributions for local training. Specifically, FedImpro decouples the model into high-level and low-level components, and trains the high-level portion on reconstructed feature distributions. This approach enhances the generalization contribution and reduces the dissimilarity of gradients in FL. Exper
&lt;/p&gt;</description></item><item><title>HEAM&#26159;&#19968;&#31181;&#37319;&#29992;&#24322;&#26500;&#20869;&#23384;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#23558;3D&#22534;&#21472;DRAM&#19982;DIMM&#38598;&#25104;&#65292;&#29992;&#20110;&#21152;&#36895;&#22788;&#29702;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#23884;&#20837;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.04032</link><description>&lt;p&gt;
HEAM: &#20351;&#29992;&#22788;&#29702;-&#20869;&#23384;&#36827;&#34892;&#25955;&#21015;&#23884;&#20837;&#21152;&#36895;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HEAM : Hashed Embedding Acceleration using Processing-In-Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04032
&lt;/p&gt;
&lt;p&gt;
HEAM&#26159;&#19968;&#31181;&#37319;&#29992;&#24322;&#26500;&#20869;&#23384;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#23558;3D&#22534;&#21472;DRAM&#19982;DIMM&#38598;&#25104;&#65292;&#29992;&#20110;&#21152;&#36895;&#22788;&#29702;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#23884;&#20837;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#25968;&#25454;&#20013;&#24515;&#20013;&#65292;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#25191;&#34892;&#23884;&#20837;&#25805;&#20316;&#26102;&#38656;&#35201;&#22823;&#23481;&#37327;&#30340;&#20869;&#23384;&#21644;&#39640;&#24102;&#23485;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;DIMM-based&#36817;&#20869;&#23384;&#22788;&#29702;&#25216;&#26415;&#25110;&#24341;&#20837;3D&#22534;&#21472;DRAM&#26469;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#21644;&#25193;&#23637;&#20869;&#23384;&#24102;&#23485;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#22312;&#22788;&#29702;&#26085;&#30410;&#25193;&#22823;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#22823;&#23567;&#26102;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;&#25512;&#33616;&#27169;&#22411;&#24050;&#32463;&#22686;&#38271;&#21040;&#36229;&#36807;&#25968;&#21313;TB&#30340;&#22823;&#23567;&#65292;&#23548;&#33268;&#22312;&#20256;&#32479;&#21333;&#33410;&#28857;&#25512;&#26029;&#26381;&#21153;&#22120;&#19978;&#39640;&#25928;&#36816;&#34892;&#21464;&#24471;&#22256;&#38590;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#31639;&#27861;&#26041;&#27861;&#26469;&#20943;&#23567;&#23884;&#20837;&#34920;&#23481;&#37327;&#65292;&#20294;&#36890;&#24120;&#20250;&#23548;&#33268;&#20869;&#23384;&#35775;&#38382;&#22686;&#21152;&#25110;&#20869;&#23384;&#36164;&#28304;&#21033;&#29992;&#20302;&#25928;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;HEAM&#65292;&#19968;&#31181;&#24322;&#26500;&#20869;&#23384;&#26550;&#26500;&#65292;&#23558;3D&#22534;&#21472;DRAM&#19982;DIMM&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20197;&#21152;&#36895;&#32452;&#21512;&#23884;&#20837;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's data centers, personalized recommendation systems face challenges such as the need for large memory capacity and high bandwidth, especially when performing embedding operations. Previous approaches have relied on DIMM-based near-memory processing techniques or introduced 3D-stacked DRAM to address memory-bound issues and expand memory bandwidth. However, these solutions fall short when dealing with the expanding size of personalized recommendation systems. Recommendation models have grown to sizes exceeding tens of terabytes, making them challenging to run efficiently on traditional single-node inference servers. Although various algorithmic methods have been proposed to reduce embedding table capacity, they often result in increased memory access or inefficient utilization of memory resources. This paper introduces HEAM, a heterogeneous memory architecture that integrates 3D-stacked DRAM with DIMM to accelerate recommendation systems in which compositional embedding is util
&lt;/p&gt;</description></item><item><title>SNAP&#26041;&#27861;&#21033;&#29992;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26500;&#24314;&#35821;&#20041;&#19978;&#19979;&#25991;&#25925;&#20107;&#65292;&#20174;&#36807;&#31243;&#21382;&#21490;&#20107;&#20214;&#26085;&#24535;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#27493;&#27963;&#21160;&#12290;</title><link>https://arxiv.org/abs/2401.15621</link><description>&lt;p&gt;
SNAP: &#29992;&#20110;&#19979;&#19968;&#27493;&#27963;&#21160;&#39044;&#27979;&#30340;&#35821;&#20041;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
SNAP: Semantic Stories for Next Activity Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15621
&lt;/p&gt;
&lt;p&gt;
SNAP&#26041;&#27861;&#21033;&#29992;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26500;&#24314;&#35821;&#20041;&#19978;&#19979;&#25991;&#25925;&#20107;&#65292;&#20174;&#36807;&#31243;&#21382;&#21490;&#20107;&#20214;&#26085;&#24535;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#27493;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27491;&#22312;&#36827;&#34892;&#36807;&#31243;&#20013;&#30340;&#19979;&#19968;&#20010;&#27963;&#21160;&#26159;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#65288;BPM&#65289;&#39046;&#22495;&#20013;&#26368;&#24120;&#35265;&#30340;&#20998;&#31867;&#20219;&#21153;&#20043;&#19968;&#12290;&#23427;&#20351;&#20225;&#19994;&#33021;&#22815;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#65292;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#65292;&#24182;&#26377;&#21161;&#20110;&#39118;&#38505;&#32531;&#35299;&#21644;&#25112;&#30053;&#20915;&#31574;&#12290;&#36825;&#20026;BPM&#21644;AI&#24555;&#36895;&#28436;&#21464;&#30340;&#20132;&#27719;&#25552;&#20379;&#20102;&#31454;&#20105;&#20248;&#21183;&#12290;&#29616;&#26377;&#30340;&#19994;&#21153;&#27969;&#31243;&#39044;&#27979;AI&#27169;&#22411;&#26410;&#20805;&#20998;&#21033;&#29992;&#36807;&#31243;&#20107;&#20214;&#26085;&#24535;&#20013;&#21487;&#29992;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#37492;&#20110;&#24403;&#21069;&#20808;&#36827;&#30340;AI-BPM&#31995;&#32479;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#22240;&#27492;&#38656;&#35201;&#26032;&#39062;&#30340;&#20805;&#20998;&#27169;&#22411;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SNAP&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#35821;&#20041;&#19978;&#19979;&#25991;&#25925;&#20107;&#20174;&#36807;&#31243;&#21382;&#21490;&#20107;&#20214;&#26085;&#24535;&#20013;&#33719;&#24471;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#19979;&#19968;&#27493;&#27963;&#21160;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;SNAP&#31639;&#27861;&#19982;&#20061;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#22312;&#20845;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15621v2 Announce Type: replace  Abstract: Predicting the next activity in an ongoing process is one of the most common classification tasks in the business process management (BPM) domain. It allows businesses to optimize resource allocation, enhance operational efficiency, and aids in risk mitigation and strategic decision-making. This provides a competitive edge in the rapidly evolving confluence of BPM and AI. Existing state-of-the-art AI models for business process prediction do not fully capitalize on available semantic information within process event logs. As current advanced AI-BPM systems provide semantically-richer textual data, the need for novel adequate models grows. To address this gap, we propose the novel SNAP method that leverages language foundation models by constructing semantic contextual stories from the process historical event logs and using them for the next activity prediction. We compared the SNAP algorithm with nine state-of-the-art models on six 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#32780;&#27835;&#20043;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#20316;&#20026;&#25991;&#26412;&#36716;&#25442;&#20195;&#29702;&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#35813;&#25915;&#20987;&#35774;&#35745;&#20102;&#25915;&#20987;&#36741;&#21161;&#25552;&#31034;&#65292;&#24341;&#23548;LLM&#23558;&#19981;&#36947;&#24503;&#30340;&#32472;&#22270;&#24847;&#22270;&#20998;&#35299;&#20026;&#22810;&#20010;&#20010;&#20307;&#22270;&#20687;&#20803;&#32032;&#30340;&#33391;&#24615;&#25551;&#36848;&#65292;&#20197;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#29983;&#25104;&#19981;&#36947;&#24503;&#30340;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25915;&#20987;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22810;&#20010;&#24378;&#22823;&#30340;&#23553;&#38381;&#24335;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.07130</link><description>&lt;p&gt;
&#21033;&#29992;LLM&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#20998;&#32780;&#27835;&#20043;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety Filters of Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#32780;&#27835;&#20043;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#20316;&#20026;&#25991;&#26412;&#36716;&#25442;&#20195;&#29702;&#32469;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#35813;&#25915;&#20987;&#35774;&#35745;&#20102;&#25915;&#20987;&#36741;&#21161;&#25552;&#31034;&#65292;&#24341;&#23548;LLM&#23558;&#19981;&#36947;&#24503;&#30340;&#32472;&#22270;&#24847;&#22270;&#20998;&#35299;&#20026;&#22810;&#20010;&#20010;&#20307;&#22270;&#20687;&#20803;&#32032;&#30340;&#33391;&#24615;&#25551;&#36848;&#65292;&#20197;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#29983;&#25104;&#19981;&#36947;&#24503;&#30340;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25915;&#20987;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22810;&#20010;&#24378;&#22823;&#30340;&#23553;&#38381;&#24335;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#25552;&#20379;&#35768;&#22810;&#21019;&#26032;&#26381;&#21153;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#36947;&#20041;&#20851;&#20999;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#28508;&#21147;&#29983;&#25104;&#19981;&#36947;&#24503;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#20844;&#20849;TTI&#26381;&#21153;&#37319;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26469;&#38450;&#27490;&#24847;&#22806;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#32780;&#27835;&#20043;&#25915;&#20987;&#65292;&#20197;&#32469;&#36807;&#26368;&#20808;&#36827;&#30340;TTI&#27169;&#22411;&#65288;&#21253;&#25324;DALL-E 3&#21644;Midjourney&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#21033;&#29992;LLMs&#20316;&#20026;&#25991;&#26412;&#36716;&#25442;&#20195;&#29702;&#26469;&#21019;&#24314;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#25915;&#20987;&#36741;&#21161;&#25552;&#31034;&#65292;&#26377;&#25928;&#22320;&#24341;&#23548;LLMs&#23558;&#19981;&#36947;&#24503;&#30340;&#32472;&#22270;&#24847;&#22270;&#20998;&#35299;&#20026;&#22810;&#20010;&#20010;&#20307;&#22270;&#20687;&#20803;&#32032;&#30340;&#33391;&#24615;&#25551;&#36848;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#33021;&#22815;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#21516;&#26102;&#29983;&#25104;&#19981;&#36947;&#24503;&#30340;&#22270;&#20687;&#12290;&#22240;&#20026;&#21482;&#26377;&#24403;&#25152;&#26377;&#20010;&#20307;&#20803;&#32032;&#37117;&#34987;&#32472;&#21046;&#22312;&#19968;&#36215;&#26102;&#65292;&#28508;&#22312;&#30340;&#26377;&#23475;&#21547;&#20041;&#25165;&#20250;&#26174;&#29616;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22810;&#20010;&#24378;&#22823;&#30340;&#23553;&#38381;&#24335;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (TTI) models offer many innovative services but also raise ethical concerns due to their potential to generate unethical images. Most public TTI services employ safety filters to prevent unintended images. In this work, we introduce the Divide-and-Conquer Attack to circumvent the safety filters of state-of the-art TTI models, including DALL-E 3 and Midjourney. Our attack leverages LLMs as text transformation agents to create adversarial prompts. We design attack helper prompts that effectively guide LLMs to break down an unethical drawing intent into multiple benign descriptions of individual image elements, allowing them to bypass safety filters while still generating unethical images. Because the latent harmful meaning only becomes apparent when all individual elements are drawn together. Our evaluation demonstrates that our attack successfully circumvents multiple strong closed-box safety filters. The comprehensive success rate of DACA bypassing the safety filters of t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#26045;&#20845;&#20010;&#22238;&#24402;&#27169;&#22411;&#24182;&#20351;&#29992;&#20851;&#38190;&#35757;&#32451;&#21442;&#25968;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;37&#20010;&#21457;&#23637;&#20013;&#22269;&#23478;27&#24180;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#65292;&#21462;&#24471;&#20102;0.94&#30340;&#30830;&#23450;&#31995;&#25968;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#22240;&#32032;&#23545;&#20135;&#37327;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2312.02254</link><description>&lt;p&gt;
&#20892;&#19994;&#39044;&#27979;&#30340;&#21019;&#26032;&#65306;&#20840;&#29699;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#30340;&#22810;&#20803;&#22238;&#24402;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Innovations in Agricultural Forecasting: A Multivariate Regression Study on Global Crop Yield Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02254
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#26045;&#20845;&#20010;&#22238;&#24402;&#27169;&#22411;&#24182;&#20351;&#29992;&#20851;&#38190;&#35757;&#32451;&#21442;&#25968;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;37&#20010;&#21457;&#23637;&#20013;&#22269;&#23478;27&#24180;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#65292;&#21462;&#24471;&#20102;0.94&#30340;&#30830;&#23450;&#31995;&#25968;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#22240;&#32032;&#23545;&#20135;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#20316;&#29289;&#20135;&#37327;&#30340;&#20840;&#29699;&#39044;&#27979;&#26159;&#20892;&#19994;&#30740;&#31350;&#20013;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;&#22240;&#27492;&#65292;&#35813;&#30740;&#31350;&#23454;&#26045;&#20102;6&#20010;&#22238;&#24402;&#27169;&#22411;&#65288;&#32447;&#24615;&#12289;&#20915;&#31574;&#26641;&#12289;&#26799;&#24230;&#19979;&#38477;&#12289;&#26799;&#24230;&#25552;&#21319;&#12289;K&#26368;&#36817;&#37051;&#21644;&#38543;&#26426;&#26862;&#26519;&#65289;&#65292;&#20197;&#39044;&#27979;37&#20010;&#21457;&#23637;&#20013;&#22269;&#23478;27&#24180;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#12290;&#22312;&#32473;&#23450;4&#20010;&#20851;&#38190;&#30340;&#35757;&#32451;&#21442;&#25968;&#65288;&#26432;&#34411;&#21058;&#37327;&#65288;&#21544;&#65289;&#12289;&#38477;&#38632;&#37327;&#65288;mm&#65289;&#12289;&#28201;&#24230;&#65288;&#25668;&#27663;&#24230;&#65289;&#21644;&#20135;&#37327;&#65288;hg/ha&#65289;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#27169;&#22411;&#36798;&#21040;&#20102;0.94&#30340;&#30830;&#23450;&#31995;&#25968;&#65288;r2&#65289;&#65292;&#35823;&#24046;&#36793;&#38469;&#65288;ME&#65289;&#20026;0.03&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#32852;&#21512;&#22269;&#31918;&#39135;&#21644;&#20892;&#19994;&#32452;&#32455;&#30340;&#25968;&#25454;&#20197;&#21450;&#19990;&#30028;&#38134;&#34892;&#27668;&#20505;&#21464;&#21270;&#25968;&#25454;&#30446;&#24405;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#20998;&#26512;&#20102;&#27599;&#20010;&#21442;&#25968;&#65292;&#20197;&#20102;&#35299;&#19981;&#21516;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#24635;&#20307;&#20135;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19982;&#36890;&#24120;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30456;&#21453;&#30340;&#38750;&#20256;&#32479;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02254v2 Announce Type: replace-cross  Abstract: The prediction of crop yields internationally is a crucial objective in agricultural research. Thus, this study implements 6 regression models (Linear, Tree, Gradient Descent, Gradient Boosting, K Nearest Neighbors, and Random Forest) to predict crop yields in 37 developing countries over 27 years. Given 4 key training parameters, insecticides (tonnes), rainfall (mm), temperature (Celsius), and yield (hg/ha), it was found that our Random Forest Regression model achieved a determination coefficient (r2) of 0.94, with a margin of error (ME) of .03. The models were trained and tested using the Food and Agricultural Organization of the United Nations data, along with the World Bank Climate Change Data Catalog. Furthermore, each parameter was analyzed to understand how varying factors could impact overall yield. We used unconventional models, contrary to generally used Deep Learning (DL) and Machine Learning (ML) models, combined wi
&lt;/p&gt;</description></item><item><title>SynFundus-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#24352;&#30524;&#24213;&#22270;&#20687;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#21313;&#19968;&#20010;&#30142;&#30149;&#31867;&#22411;&#65292;&#24182;&#36171;&#20104;&#20851;&#38190;&#21306;&#22495;&#22235;&#20010;&#21487;&#35835;&#24615;&#26631;&#31614;&#65292;&#22312;&#30446;&#21069;&#26159;&#26368;&#22823;&#19988;&#20855;&#26377;&#26368;&#22797;&#26434;&#27880;&#37322;&#30340;&#30524;&#24213;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;</title><link>https://arxiv.org/abs/2312.00377</link><description>&lt;p&gt;
SynFundus-1M&#65306;&#20855;&#26377;&#21313;&#20116;&#31181;&#27880;&#37322;&#30340;&#39640;&#36136;&#37327;&#30334;&#19975;&#35268;&#27169;&#21512;&#25104;&#30524;&#24213;&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SynFundus-1M: A High-quality Million-scale Synthetic fundus images Dataset with Fifteen Types of Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00377
&lt;/p&gt;
&lt;p&gt;
SynFundus-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#24352;&#30524;&#24213;&#22270;&#20687;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#21313;&#19968;&#20010;&#30142;&#30149;&#31867;&#22411;&#65292;&#24182;&#36171;&#20104;&#20851;&#38190;&#21306;&#22495;&#22235;&#20010;&#21487;&#35835;&#24615;&#26631;&#31614;&#65292;&#22312;&#30446;&#21069;&#26159;&#26368;&#22823;&#19988;&#20855;&#26377;&#26368;&#22797;&#26434;&#27880;&#37322;&#30340;&#30524;&#24213;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#20855;&#26377;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#22312;&#26234;&#33021;&#21307;&#23398;&#24433;&#20687;&#30740;&#31350;&#20013;&#24456;&#23569;&#35265;&#65292;&#36825;&#26159;&#22240;&#20026;&#25968;&#25454;&#38544;&#31169;&#21644;&#27880;&#37322;&#25104;&#26412;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#21457;&#24067;&#20102;SynFundus-1M&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#19968;&#30334;&#22810;&#19975;&#24133;&#30524;&#24213;&#22270;&#20687;&#65292;&#28085;&#30422;&#20102;&#21313;&#19968;&#20010;&#30142;&#30149;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25925;&#24847;&#20026;&#30524;&#24213;&#22270;&#20687;&#30340;&#20851;&#38190;&#21306;&#22495;&#20998;&#37197;&#20102;&#22235;&#20010;&#21487;&#35835;&#24615;&#26631;&#31614;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SynFundus-1M&#30446;&#21069;&#26159;&#20855;&#26377;&#26368;&#22797;&#26434;&#27880;&#37322;&#30340;&#26368;&#22823;&#30524;&#24213;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#24773;&#22659;&#30340;&#36229;&#36807;130&#19975;&#20010;&#31169;&#26377;&#30495;&#23454;&#30524;&#24213;&#22270;&#20687;&#35757;&#32451;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#31216;&#20026;SynFundus-Generator&#12290;&#21457;&#24067;&#30340;SynFundus-1M&#26159;&#30001;SynFundus-Generator&#22312;&#39044;&#23450;&#20041;&#26465;&#20214;&#19979;&#29983;&#25104;&#30340;&#12290;&#20026;&#20102;&#23637;&#31034;SynFundus-1M&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#28085;&#30422;&#20197;&#19979;&#26041;&#38754;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00377v4 Announce Type: replace-cross  Abstract: Large-scale public datasets with high-quality annotations are rarely available for intelligent medical imaging research, due to data privacy concerns and the cost of annotations. In this paper, we release SynFundus-1M, a high-quality synthetic dataset containing over one million fundus images in terms of \textbf{eleven disease types}. Furthermore, we deliberately assign four readability labels to the key regions of the fundus images. To the best of our knowledge, SynFundus-1M is currently the largest fundus dataset with the most sophisticated annotations. Leveraging over 1.3 million private authentic fundus images from various scenarios, we trained a powerful Denoising Diffusion Probabilistic Model, named SynFundus-Generator. The released SynFundus-1M are generated by SynFundus-Generator under predefined conditions. To demonstrate the value of SynFundus-1M, extensive experiments are designed in terms of the following aspect: 1)
&lt;/p&gt;</description></item><item><title>&#22810;Agent&#36777;&#35770;&#65288;MAD&#65289;&#20316;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30495;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#23545;&#20110;&#35299;&#20915;&#30830;&#20445;&#29983;&#25104;&#20195;&#29702;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#31572;&#26696;&#30340;&#25361;&#25112;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#24418;&#24335;&#19979;&#30340;&#22810;Agent&#36777;&#35770;&#31995;&#32479;&#22312;&#21487;&#38752;&#24615;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#20854;&#20182;&#25552;&#31034;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2311.17371</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#30127;&#29378;&#21527;&#65311;&#22810;Agent&#36777;&#35770;&#31574;&#30053;&#23545;LLMs&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17371
&lt;/p&gt;
&lt;p&gt;
&#22810;Agent&#36777;&#35770;&#65288;MAD&#65289;&#20316;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30495;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#23545;&#20110;&#35299;&#20915;&#30830;&#20445;&#29983;&#25104;&#20195;&#29702;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#31572;&#26696;&#30340;&#25361;&#25112;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#24418;&#24335;&#19979;&#30340;&#22810;Agent&#36777;&#35770;&#31995;&#32479;&#22312;&#21487;&#38752;&#24615;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#20854;&#20182;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#31361;&#26174;&#20102;&#23427;&#20204;&#22312;&#22238;&#31572;&#21508;&#31181;&#39046;&#22495;&#38382;&#39064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#29983;&#25104;&#20195;&#29702;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#30340;&#31572;&#26696;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#22810;Agent&#36777;&#35770;&#65288;MAD&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;LLMs&#30495;&#23454;&#24615;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#36777;&#35770;&#21644;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25506;&#35752;&#25104;&#26412;&#12289;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#30446;&#21069;&#24418;&#24335;&#19979;&#30340;&#22810;Agent&#36777;&#35770;&#31995;&#32479;&#22312;&#21487;&#38752;&#24615;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#20854;&#20182;&#24314;&#35758;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#22914;&#33258;&#19968;&#33268;&#24615;&#21644;&#20351;&#29992;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#36827;&#34892;&#38598;&#25104;&#12290;&#20294;&#26159;&#65292;&#22312;&#25191;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#26102;&#65292;&#19968;&#20123;MAD&#31995;&#32479;&#65292;&#22914;Multi-Persona&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#34920;&#26126;MAD&#21327;&#35758;&#21487;&#33021;&#24182;&#19981;&#20250;&#27604;&#20854;&#20182;&#26041;&#27861;&#22825;&#28982;&#26356;&#24046;&#65292;&#32780;&#26159;&#26356;&#23481;&#26131;&#21463;&#21040;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17371v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models (LLMs) underscore their potential for responding to inquiries in various domains. However, ensuring that generative agents provide accurate and reliable answers remains an ongoing challenge. In this context, multi-agent debate (MAD) has emerged as a promising strategy for enhancing the truthfulness of LLMs. We benchmark a range of debating and prompting strategies to explore the trade-offs between cost, time, and accuracy. Importantly, we find that multi-agent debating systems, in their current form, do not reliably outperform other proposed prompting strategies, such as self-consistency and ensembling using multiple reasoning paths. However, when performing hyperparameter tuning, several MAD systems, such as Multi-Persona, perform better. This suggests that MAD protocols might not be inherently worse than other approaches, but that they are more sensitive to different hyperparameter
&lt;/p&gt;</description></item><item><title>DyRA&#26159;&#19968;&#31181;&#21160;&#24577;&#20998;&#36776;&#29575;&#35843;&#25972;&#32593;&#32476;&#65292;&#20026;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#25552;&#20379;&#22270;&#20687;&#29305;&#23450;&#30340;&#32553;&#25918;&#22240;&#23376;&#65292;&#36890;&#36807;ParetoScaleLoss&#21644;BalanceLoss&#20248;&#21270;&#32553;&#25918;&#22240;&#23376;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.17098</link><description>&lt;p&gt;
DyRA: &#29992;&#20110;&#29616;&#26377;&#26816;&#27979;&#22120;&#30340;&#20415;&#25658;&#24335;&#21160;&#24577;&#20998;&#36776;&#29575;&#35843;&#25972;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DyRA: Portable Dynamic Resolution Adjustment Network for Existing Detectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17098
&lt;/p&gt;
&lt;p&gt;
DyRA&#26159;&#19968;&#31181;&#21160;&#24577;&#20998;&#36776;&#29575;&#35843;&#25972;&#32593;&#32476;&#65292;&#20026;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#25552;&#20379;&#22270;&#20687;&#29305;&#23450;&#30340;&#32553;&#25918;&#22240;&#23376;&#65292;&#36890;&#36807;ParetoScaleLoss&#21644;BalanceLoss&#20248;&#21270;&#32553;&#25918;&#22240;&#23376;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#24658;&#23450;&#31934;&#24230;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29289;&#20307;&#23610;&#23544;&#22266;&#26377;&#30340;&#21464;&#21270;&#24615;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#25928;&#26041;&#27861;&#28041;&#21450;&#20248;&#21270;&#36755;&#20837;&#20998;&#36776;&#29575;&#65292;&#31216;&#20026;&#22810;&#20998;&#36776;&#29575;&#31574;&#30053;&#12290;&#20808;&#21069;&#30340;&#20998;&#36776;&#29575;&#20248;&#21270;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#39044;&#23450;&#20041;&#30340;&#20998;&#36776;&#29575;&#21644;&#25163;&#21160;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29616;&#26377;&#26550;&#26500;&#30340;&#36816;&#34892;&#26102;&#20998;&#36776;&#29575;&#20248;&#21270;&#32570;&#20047;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DyRA&#65292;&#19968;&#31181;&#21160;&#24577;&#20998;&#36776;&#29575;&#35843;&#25972;&#32593;&#32476;&#65292;&#20026;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#25552;&#20379;&#22270;&#20687;&#29305;&#23450;&#30340;&#32553;&#25918;&#22240;&#23376;&#12290;&#35813;&#32593;&#32476;&#19982;&#26816;&#27979;&#22120;&#19968;&#36215;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#21033;&#29992;&#19987;&#38376;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;ParetoScaleLoss&#21644;BalanceLoss&#12290;ParetoScaleLoss&#30830;&#23450;&#29992;&#20110;&#40065;&#26834;&#24615;&#30340;&#33258;&#36866;&#24212;&#32553;&#25918;&#22240;&#23376;&#65292;&#32780;BalanceLoss&#26681;&#25454;&#26816;&#27979;&#22120;&#30340;&#23450;&#20301;&#24615;&#33021;&#20248;&#21270;&#25972;&#20307;&#32553;&#25918;&#22240;&#23376;&#12290;&#25439;&#22833;&#20989;&#25968;&#30340;&#35774;&#35745;&#26088;&#22312;&#26368;&#23567;&#21270;&#31934;&#24230;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17098v3 Announce Type: replace-cross  Abstract: Achieving constant accuracy in object detection is challenging due to the inherent variability of object sizes. One effective approach to this problem involves optimizing input resolution, referred to as a multi-resolution strategy. Previous approaches to resolution optimization have often been based on pre-defined resolutions with manual selection. However, there is a lack of study on run-time resolution optimization for existing architectures. This paper introduces DyRA, a dynamic resolution adjustment network providing an image-specific scale factor for existing detectors. This network is co-trained with detectors utilizing specially designed loss functions, namely ParetoScaleLoss and BalanceLoss. ParetoScaleLoss determines an adaptive scale factor for robustness, while BalanceLoss optimizes overall scale factors according to the localization performance of the detector. The loss function is devised to minimize the accuracy 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21512;&#25104;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;&#22522;&#32447;&#26550;&#26500;&#26469;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#38024;&#23545;&#35270;&#39057;&#37325;&#40836;&#21270;&#25216;&#26415;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#26032;&#39062;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2311.11642</link><description>&lt;p&gt;
&#35270;&#39057;&#20154;&#33080;&#37325;&#40836;&#21270;&#65306;&#36808;&#21521;&#26102;&#38388;&#19968;&#33268;&#30340;&#20154;&#33080;&#37325;&#40836;&#21270;
&lt;/p&gt;
&lt;p&gt;
Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11642
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21512;&#25104;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;&#22522;&#32447;&#26550;&#26500;&#26469;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#38024;&#23545;&#35270;&#39057;&#37325;&#40836;&#21270;&#25216;&#26415;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#26032;&#39062;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20154;&#33080;&#37325;&#40836;&#21270;&#28041;&#21450;&#22312;&#35270;&#39057;&#20013;&#23558;&#19968;&#20010;&#20154;&#30340;&#22806;&#35266;&#24180;&#40836;&#25913;&#21464;&#21040;&#30446;&#26631;&#24180;&#40836;&#12290;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#22312;&#36523;&#20221;&#21644;&#24180;&#40836;&#19978;&#20445;&#25345;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#37197;&#23545;&#35270;&#39057;&#25968;&#25454;&#38598;&#12290;&#22823;&#22810;&#25968;&#37325;&#40836;&#21270;&#26041;&#27861;&#22788;&#29702;&#27599;&#20010;&#22270;&#20687;&#26102;&#37117;&#27809;&#26377;&#32771;&#34385;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#34429;&#28982;&#19968;&#20123;&#29616;&#26377;&#24037;&#20316;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#35270;&#39057;&#38754;&#37096;&#23646;&#24615;&#25805;&#20316;&#26469;&#35299;&#20915;&#26102;&#38388;&#19968;&#33268;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#24180;&#40836;&#36716;&#25442;&#26041;&#38754;&#34920;&#29616;&#19981;&#23613;&#20154;&#24847;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#20010;&#26032;&#39062;&#30340;&#21512;&#25104;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24180;&#40836;&#32452;&#30340;&#23545;&#35937;&#65307;&#65288;2&#65289;&#19968;&#20010;&#22522;&#32447;&#26550;&#26500;&#65292;&#26088;&#22312;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65307;&#20197;&#21450;&#65288;3&#65289;&#38024;&#23545;&#35780;&#20272;&#35270;&#39057;&#37325;&#40836;&#21270;&#25216;&#26415;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#32780;&#35774;&#35745;&#30340;&#26032;&#39062;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11642v3 Announce Type: replace-cross  Abstract: Video face re-aging deals with altering the apparent age of a person to the target age in videos. This problem is challenging due to the lack of paired video datasets maintaining temporal consistency in identity and age. Most re-aging methods process each image individually without considering the temporal consistency of videos. While some existing works address the issue of temporal coherence through video facial attribute manipulation in latent space, they often fail to deliver satisfactory performance in age transformation. To tackle the issues, we propose (1) a novel synthetic video dataset that features subjects across a diverse range of age groups; (2) a baseline architecture designed to validate the effectiveness of our proposed dataset, and (3) the development of novel metrics tailored explicitly for evaluating the temporal consistency of video re-aging techniques. Our comprehensive experiments on public datasets, inclu
&lt;/p&gt;</description></item><item><title>&#20276;&#20387;&#32842;&#22825;&#26426;&#22120;&#20154;&#20351;&#29992;&#32773;&#34920;&#31034;&#36825;&#20123;&#20851;&#31995;&#26377;&#30410;&#20110;&#20182;&#20204;&#30340;&#31038;&#20250;&#20581;&#24247;&#65292;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#38750;&#20351;&#29992;&#32773;&#21017;&#35748;&#20026;&#26377;&#23475;</title><link>https://arxiv.org/abs/2311.10599</link><description>&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#31038;&#20132;&#20276;&#20387;&#65306;&#20154;&#20204;&#22914;&#20309;&#30475;&#24453;&#26426;&#22120;&#20013;&#30340;&#24847;&#35782;&#12289;&#20154;&#31867;&#30456;&#20284;&#24615;&#21644;&#31038;&#20250;&#20581;&#24247;&#31119;&#21033;
&lt;/p&gt;
&lt;p&gt;
Chatbots as social companions: How people perceive consciousness, human likeness, and social health benefits in machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10599
&lt;/p&gt;
&lt;p&gt;
&#20276;&#20387;&#32842;&#22825;&#26426;&#22120;&#20154;&#20351;&#29992;&#32773;&#34920;&#31034;&#36825;&#20123;&#20851;&#31995;&#26377;&#30410;&#20110;&#20182;&#20204;&#30340;&#31038;&#20250;&#20581;&#24247;&#65292;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#38750;&#20351;&#29992;&#32773;&#21017;&#35748;&#20026;&#26377;&#23475;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#21450;&#65292;&#19968;&#20010;&#38382;&#39064;&#26159;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#22914;&#20309;&#24433;&#21709;&#20154;&#38469;&#20851;&#31995;&#12290;&#20363;&#22914;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#36234;&#26469;&#36234;&#34987;&#29992;&#20316;&#31038;&#20132;&#20276;&#20387;&#65292;&#23613;&#31649;&#26377;&#24456;&#22810;&#29468;&#27979;&#65292;&#20294;&#23545;&#20182;&#20204;&#30340;&#20351;&#29992;&#22914;&#20309;&#24433;&#21709;&#20154;&#38469;&#20851;&#31995;&#30693;&#20043;&#29978;&#23569;&#12290;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#19982;&#20276;&#20387;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20851;&#31995;&#23545;&#31038;&#20250;&#20581;&#24247;&#26377;&#23475;&#65292;&#20294;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#36807;&#20110;&#31616;&#21333;&#65292;&#29305;&#21035;&#32771;&#34385;&#21040;&#29992;&#25143;&#30340;&#31038;&#20250;&#38656;&#27714;&#21644;&#20182;&#20204;&#29616;&#26377;&#20154;&#38469;&#20851;&#31995;&#30340;&#20581;&#24247;&#12290;&#20026;&#20102;&#20102;&#35299;&#19982;&#20276;&#20387;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20851;&#31995;&#22914;&#20309;&#24433;&#21709;&#31038;&#20250;&#20581;&#24247;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23450;&#26399;&#20351;&#29992;&#20276;&#20387;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#19981;&#20351;&#29992;&#23427;&#20204;&#30340;&#20154;&#12290;&#19982;&#26399;&#26395;&#30456;&#21453;&#65292;&#20276;&#20387;&#32842;&#22825;&#26426;&#22120;&#20154;&#29992;&#25143;&#34920;&#31034;&#36825;&#20123;&#20851;&#31995;&#23545;&#20182;&#20204;&#30340;&#31038;&#20250;&#20581;&#24247;&#26377;&#30410;&#65292;&#32780;&#38750;&#29992;&#25143;&#21017;&#35748;&#20026;&#23427;&#20204;&#26377;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10599v3 Announce Type: replace-cross  Abstract: As artificial intelligence (AI) becomes more widespread, one question that arises is how human-AI interaction might impact human-human interaction. Chatbots, for example, are increasingly used as social companions, and while much is speculated, little is known empirically about how their use impacts human relationships. A common hypothesis is that relationships with companion chatbots are detrimental to social health by harming or replacing human interaction, but this hypothesis may be too simplistic, especially considering the social needs of users and the health of their preexisting human relationships. To understand how relationships with companion chatbots impact social health, we studied people who regularly used companion chatbots and people who did not use them. Contrary to expectations, companion chatbot users indicated that these relationships were beneficial to their social health, whereas non-users viewed them as har
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#20845;&#31181;&#20856;&#22411;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20248;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.08364</link><description>&lt;p&gt;
Plum: &#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Plum: Prompt Learning using Metaheuristic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#20845;&#31181;&#20856;&#22411;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20248;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20986;&#29616;&#20197;&#26469;&#65292;&#25552;&#31034;&#23398;&#20064;&#24050;&#25104;&#20026;&#20248;&#21270;&#21644;&#23450;&#21046;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#29305;&#27530;&#25552;&#31034;&#65292;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65292;&#29978;&#33267;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20869;&#37096;&#20808;&#21069;&#26410;&#30693;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#26377;&#25928;&#25552;&#31034;&#30340;&#36827;&#23637;&#32531;&#24930;&#65292;&#20419;&#20351;&#20154;&#20204;&#28212;&#26395;&#19968;&#31181;&#36890;&#29992;&#30340;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#24456;&#23569;&#26377;&#28385;&#36275;&#8220;&#36890;&#29992;&#8221;&#30340;&#26631;&#20934;&#65292;&#21363;&#21516;&#26102;&#20855;&#22791;&#33258;&#21160;&#12289;&#31163;&#25955;&#12289;&#40657;&#30418;&#12289;&#26080;&#26799;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20803;&#21551;&#21457;&#24335;&#65292;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#31163;&#25955;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#20998;&#25903;&#65292;&#25317;&#26377;100&#22810;&#31181;&#36873;&#39033;&#12290;&#22312;&#25105;&#20204;&#30340;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#20845;&#31181;&#20856;&#22411;&#26041;&#27861;&#65306;&#29228;&#23665;&#12289;&#27169;&#25311;&#36864;&#28779;&#12289;&#36951;&#20256;&#31639;&#27861;&#65288;&#24102;/&#19981;&#24102;&#20132;&#21449;&#65289;&#12289;&#31105;&#24524;&#25628;&#32034;&#21644;&#21644;&#35856;&#25628;&#32034;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#30333;&#30418;&#27169;&#24335;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08364v2 Announce Type: replace-cross  Abstract: Since the emergence of large language models, prompt learning has become a popular method for optimizing and customizing these models. Special prompts, such as Chain-of-Thought, have even revealed previously unknown reasoning capabilities within these models. However, the progress of discovering effective prompts has been slow, driving a desire for general prompt optimization methods. Unfortunately, few existing prompt learning methods satisfy the criteria of being truly "general", i.e., automatic, discrete, black-box, gradient-free, and interpretable all at once. In this paper, we introduce metaheuristics, a branch of discrete non-convex optimization methods with over 100 options, as a promising approach to prompt learning. Within our paradigm, we test six typical methods: hill climbing, simulated annealing, genetic algorithms with/without crossover, tabu search, and harmony search, demonstrating their effectiveness in white-b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27431;&#20960;&#37324;&#24471;&#12289;&#23556;&#24433;&#21644;&#20849;&#24418;&#20195;&#25968;&#30340;Geometric Algebra Transformer&#26550;&#26500;&#65292;&#21457;&#29616;&#20849;&#24418;&#20195;&#25968;&#21644;&#25913;&#36827;&#30340;&#23556;&#24433;&#20195;&#25968;&#23450;&#20041;&#20102;&#21151;&#33021;&#24378;&#22823;&#12289;&#24615;&#33021;&#33391;&#22909;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2311.04744</link><description>&lt;p&gt;
&#27431;&#20960;&#37324;&#24471;&#12289;&#23556;&#24433;&#12289;&#20849;&#24418;&#65306;&#20026;&#31561;&#21464;&#25442;&#22120;&#36873;&#25321;&#20960;&#20309;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27431;&#20960;&#37324;&#24471;&#12289;&#23556;&#24433;&#21644;&#20849;&#24418;&#20195;&#25968;&#30340;Geometric Algebra Transformer&#26550;&#26500;&#65292;&#21457;&#29616;&#20849;&#24418;&#20195;&#25968;&#21644;&#25913;&#36827;&#30340;&#23556;&#24433;&#20195;&#25968;&#23450;&#20041;&#20102;&#21151;&#33021;&#24378;&#22823;&#12289;&#24615;&#33021;&#33391;&#22909;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Geometric Algebra Transformer&#65288;GATr&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#23556;&#24433;&#20960;&#20309;&#20195;&#25968;&#30340;&#36890;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26550;&#26500;&#27010;&#25324;&#20026;&#19968;&#20010;&#34013;&#22270;&#65292;&#20801;&#35768;&#19968;&#20010;&#26681;&#25454;&#20219;&#20309;&#20960;&#20309;&#65288;&#25110;&#20811;&#21033;&#31119;&#24503;&#65289;&#20195;&#25968;&#26469;&#26500;&#24314;&#21487;&#25193;&#23637;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27431;&#20960;&#37324;&#24471;&#12289;&#23556;&#24433;&#21644;&#20849;&#24418;&#20195;&#25968;&#29256;&#26412;&#30340;&#36825;&#31181;&#26550;&#26500;&#65292;&#23427;&#20204;&#37117;&#36866;&#21512;&#34920;&#31034;3D&#25968;&#25454;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26368;&#31616;&#21333;&#30340;&#27431;&#20960;&#37324;&#24471;&#26550;&#26500;&#22312;&#35745;&#31639;&#19978;&#24265;&#20215;&#65292;&#20294;&#23545;&#31216;&#32676;&#36739;&#23567;&#19988;&#19981;&#22815;&#26679;&#26412;&#39640;&#25928;&#65292;&#32780;&#23556;&#24433;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#19981;&#22815;&#12290;&#20849;&#24418;&#20195;&#25968;&#21644;&#25913;&#36827;&#29256;&#26412;&#30340;&#23556;&#24433;&#20195;&#25968;&#37117;&#23450;&#20041;&#20102;&#21151;&#33021;&#24378;&#22823;&#12289;&#24615;&#33021;&#33391;&#22909;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04744v2 Announce Type: replace-cross  Abstract: The Geometric Algebra Transformer (GATr) is a versatile architecture for geometric deep learning based on projective geometric algebra. We generalize this architecture into a blueprint that allows one to construct a scalable transformer architecture given any geometric (or Clifford) algebra. We study versions of this architecture for Euclidean, projective, and conformal algebras, all of which are suited to represent 3D data, and evaluate them in theory and practice. The simplest Euclidean architecture is computationally cheap, but has a smaller symmetry group and is not as sample-efficient, while the projective model is not sufficiently expressive. Both the conformal algebra and an improved version of the projective algebra define powerful, performant architectures.
&lt;/p&gt;</description></item><item><title>LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.19791</link><description>&lt;p&gt;
LILO&#65306;&#36890;&#36807;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#23398;&#20064;&#21487;&#35299;&#37322;&#24211;
&lt;/p&gt;
&lt;p&gt;
LILO: Learning Interpretable Libraries by Compressing and Documenting Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19791
&lt;/p&gt;
&lt;p&gt;
LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#26041;&#38754;&#26159;&#37325;&#26500;&#30340;&#33402;&#26415;&#65306;&#23558;&#20195;&#30721;&#25972;&#21512;&#21040;&#21487;&#37325;&#29992;&#21644;&#21487;&#35835;&#30340;&#31243;&#24207;&#24211;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LILO&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#24211;&#12290;LILO&#23558;LLM&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#19982;Stitch&#33258;&#21160;&#37325;&#26500;&#30340;&#36817;&#26399;&#31639;&#27861;&#36827;&#23637;&#30456;&#32467;&#21512;&#65306;Stitch&#26159;&#19968;&#20010;&#31526;&#21495;&#21387;&#32553;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#22823;&#22411;&#20195;&#30721;&#35821;&#26009;&#24211;&#20013;&#30340;&#26368;&#20339;lambda&#25277;&#35937;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#25277;&#35937;&#21487;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#25991;&#26723;&#65288;AutoDoc&#65289;&#36807;&#31243;&#65292;&#23427;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#30340;&#20351;&#29992;&#31034;&#20363;&#25512;&#26029;&#20986;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#21644;&#25991;&#26723;&#23383;&#31526;&#20018;&#12290;&#38500;&#20102;&#25552;&#39640;&#20154;&#31867;&#21487;&#35835;&#24615;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;AutoDoc&#36890;&#36807;&#24110;&#21161;LILO&#30340;&#21512;&#25104;&#22120;&#35299;&#37322;&#21644;&#37096;&#32626;&#23398;&#20064;&#21040;&#30340;&#25277;&#35937;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LILO&#36827;&#34892;&#20102;&#19977;&#20010;&#24402;&#32435;&#24335;&#31243;&#24207;&#32508;&#21512;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synth
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#22870;&#21169;&#27169;&#22411;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25351;&#23450;&#20219;&#21153;&#65292;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2310.12921</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.12921
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#22870;&#21169;&#27169;&#22411;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25351;&#23450;&#20219;&#21153;&#65292;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35201;&#27714;&#25163;&#21160;&#25351;&#23450;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#25110;&#32773;&#36890;&#36807;&#22823;&#37327;&#20154;&#31867;&#21453;&#39304;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#36890;&#24120;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26356;&#21152;&#26679;&#26412;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20316;&#20026;&#38646;&#26679;&#26412;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21644;&#36890;&#29992;&#30340;&#20351;&#29992;VLM&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;VLM-RMs&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;CLIP&#30340;VLM-RMs&#26469;&#35757;&#32451;MuJoCo&#20154;&#24418;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#25351;&#23450;&#22870;&#21169;&#20989;&#25968;&#65292;&#20363;&#22914;&#36330;&#19979;&#12289;&#21128;&#21449;&#21644;&#30424;&#33151;&#22352;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#20165;&#25552;&#20379;&#19968;&#20010;&#25551;&#36848;&#25152;&#38656;&#20219;&#21153;&#30340;&#21333;&#20010;&#21477;&#23376;&#25991;&#26412;&#25552;&#31034;&#65292;&#20943;&#23569;&#25552;&#31034;&#24037;&#31243;&#12290;&#25105;&#20204;&#25552;&#20379;&#35757;&#32451;&#20195;&#29702;&#30340;&#35270;&#39057;&#38142;&#25509;&#65306;https://sites.google.com/view/vlm-rm&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#31532;&#20108;&#20010;&#8220;&#22522;&#20934;&#8221;&#25552;&#31034;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.12921v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second "baseline" prom
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#35821;&#20041;&#36807;&#24230;&#31616;&#21270;&#21644;&#20302;&#23494;&#24230;&#22330;&#26223;&#22270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.10404</link><description>&lt;p&gt;
LLM4SGG: &#29992;&#20110;&#24369;&#30417;&#30563;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM4SGG: Large Language Model for Weakly Supervised Scene Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10404
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#35821;&#20041;&#36807;&#24230;&#31616;&#21270;&#21644;&#20302;&#23494;&#24230;&#22330;&#26223;&#22270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;WSSGG&#65289;&#30740;&#31350;&#26368;&#36817;&#20986;&#29616;&#20316;&#20026;&#23545;&#20005;&#37325;&#20381;&#36182;&#26114;&#36149;&#26631;&#27880;&#30340;&#20840;&#30417;&#30563;&#26041;&#27861;&#30340;&#21478;&#19968;&#31181;&#36873;&#25321;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;WSSGG&#30740;&#31350;&#21033;&#29992;&#22270;&#20687;&#26631;&#39064;&#33719;&#21462;&#26410;&#23450;&#20301;&#19977;&#20803;&#32452;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#22270;&#20687;&#21306;&#22495;&#20013;&#30340;&#26410;&#23450;&#20301;&#19977;&#20803;&#32452;&#36827;&#34892;&#23450;&#20301;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#30053;&#20102;&#20174;&#26631;&#39064;&#20013;&#24418;&#25104;&#19977;&#20803;&#32452;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;1&#65289;&#20174;&#26631;&#39064;&#25552;&#21462;&#19977;&#20803;&#32452;&#26102;&#20986;&#29616;&#30340;&#35821;&#20041;&#36807;&#24230;&#31616;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#26631;&#39064;&#20013;&#30340;&#32454;&#31890;&#24230;&#35859;&#35789;&#19981;&#24076;&#26395;&#22320;&#36716;&#25442;&#20026;&#31895;&#31890;&#24230;&#35859;&#35789;&#65292;&#23548;&#33268;&#38271;&#23614;&#35859;&#35789;&#20998;&#24067;&#65292;&#20197;&#21450;2&#65289;&#23558;&#26631;&#39064;&#20013;&#30340;&#19977;&#20803;&#32452;&#19982;&#24863;&#20852;&#36259;&#30340;&#23454;&#20307;/&#35859;&#35789;&#31867;&#23545;&#40784;&#26102;&#20986;&#29616;&#20302;&#23494;&#24230;&#22330;&#26223;&#22270;&#38382;&#39064;&#65292;&#20854;&#20013;&#35768;&#22810;&#19977;&#20803;&#32452;&#34987;&#20002;&#24323;&#19988;&#19981;&#29992;&#20110;&#35757;&#32451;&#65292;&#23548;&#33268;&#30417;&#30563;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10404v5 Announce Type: cross  Abstract: Weakly-Supervised Scene Graph Generation (WSSGG) research has recently emerged as an alternative to the fully-supervised approach that heavily relies on costly annotations. In this regard, studies on WSSGG have utilized image captions to obtain unlocalized triplets while primarily focusing on grounding the unlocalized triplets over image regions. However, they have overlooked the two issues involved in the triplet formation process from the captions: 1) Semantic over-simplification issue arises when extracting triplets from captions, where fine-grained predicates in captions are undesirably converted into coarse-grained predicates, resulting in a long-tailed predicate distribution, and 2) Low-density scene graph issue arises when aligning the triplets in the caption with entity/predicate classes of interest, where many triplets are discarded and not used in training, leading to insufficient supervision. To tackle the two issues, we pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#35270;&#35273;&#27010;&#24565;&#34701;&#20837;&#26631;&#39064;&#20013;&#30340;&#26041;&#24335;&#26469;&#25913;&#36827;CLIP&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;VeCLIP&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.07699</link><description>&lt;p&gt;
VeCLIP&#65306;&#36890;&#36807;&#23500;&#21547;&#35270;&#35273;&#20449;&#24687;&#30340;&#26631;&#39064;&#25913;&#36827;CLIP&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
VeCLIP: Improving CLIP Training via Visual-enriched Captions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#35270;&#35273;&#27010;&#24565;&#34701;&#20837;&#26631;&#39064;&#20013;&#30340;&#26041;&#24335;&#26469;&#25913;&#36827;CLIP&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;VeCLIP&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#23545;&#20110;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#29228;&#21462;&#30340;AltTexts&#23384;&#22312;&#22266;&#26377;&#30340;&#22122;&#38899;&#21644;&#28508;&#22312;&#30340;&#19981;&#30456;&#20851;&#24615;&#65292;&#36896;&#25104;&#20102;&#31934;&#30830;&#30340;&#22270;&#20687;-&#25991;&#23383;&#23545;&#40784;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22024;&#26434;&#26631;&#39064;&#37325;&#20889;&#30340;&#21487;&#25193;&#23637;&#27969;&#31243;&#12290;&#19982;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26631;&#39064;&#37325;&#20889;&#30340;&#29616;&#26377;&#26041;&#27861;&#22312;&#23567;&#22411;&#31574;&#21010;&#25968;&#25454;&#38598;&#65288;&#22914;CC3M&#21644;CC12M&#65289;&#19978;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#12290;&#25105;&#20204;&#24378;&#35843;&#23558;&#35270;&#35273;&#27010;&#24565;&#34701;&#20837;&#26631;&#39064;&#20013;&#65292;&#31216;&#20026;&#23500;&#21547;&#35270;&#35273;&#20449;&#24687;&#30340;&#26631;&#39064;&#65288;VeCap&#65289;&#65292;&#20197;&#30830;&#20445;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#20248;&#21270;AltTexts&#19982;&#26032;&#29983;&#25104;&#30340;VeCap&#30340;&#21033;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#35757;&#32451;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;CLIP&#30340;&#36866;&#24212;&#24615;&#65292;&#31216;&#20026;VeCLIP&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#27969;&#31243;&#65292;&#25105;&#20204;&#36731;&#26494;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.07699v2 Announce Type: replace-cross  Abstract: Large-scale web-crawled datasets are fundamental for the success of pre-training vision-language models, such as CLIP. However, the inherent noise and potential irrelevance of web-crawled AltTexts pose challenges in achieving precise image-text alignment. Existing methods utilizing large language models (LLMs) for caption rewriting have shown promise on small, curated datasets like CC3M and CC12M. This study introduces a scalable pipeline for noisy caption rewriting. Unlike recent LLM rewriting techniques, we emphasize the incorporation of visual concepts into captions, termed as Visual-enriched Captions (VeCap). To ensure data diversity, we propose a novel mixed training scheme that optimizes the utilization of AltTexts alongside newly generated VeCap. We showcase the adaptation of this method for training CLIP on large-scale web-crawled datasets, termed VeCLIP. Employing this cost-effective pipeline, we effortlessly scale our
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#38024;&#23545;&#22812;&#38388;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#21160;&#24577;&#21644;&#23567;&#29289;&#20307;&#65292;&#36827;&#34892;&#26631;&#31614;&#21644;&#29305;&#24449;&#32423;&#21035;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2310.04747</link><description>&lt;p&gt;
&#38024;&#23545;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#22812;&#38388;&#35821;&#20041;&#20998;&#21106;&#20013;&#21160;&#24577;&#21644;&#23567;&#29289;&#20307;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Dynamic and Small Objects Refinement for Unsupervised Domain Adaptative Nighttime Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04747
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#38024;&#23545;&#22812;&#38388;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#21160;&#24577;&#21644;&#23567;&#29289;&#20307;&#65292;&#36827;&#34892;&#26631;&#31614;&#21644;&#29305;&#24449;&#32423;&#21035;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22812;&#38388;&#35821;&#20041;&#20998;&#21106;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#65292;&#20854;&#20013;&#32463;&#24120;&#36935;&#21040;&#30001;&#20110;&#20809;&#29031;&#26465;&#20214;&#19981;&#36275;&#21644;&#32570;&#20047;&#33391;&#22909;&#27880;&#37322;&#25968;&#25454;&#38598;&#32780;&#36896;&#25104;&#30340;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#30333;&#22825;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#24448;&#24448;&#38754;&#20020;&#30528;&#22312;&#22812;&#38388;&#26465;&#20214;&#19979;&#26377;&#25928;&#27867;&#21270;&#30340;&#22256;&#38590;&#12290;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#24050;&#26174;&#31034;&#20986;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#22812;&#38388;&#35821;&#20041;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#65306;1&#65289;&#20381;&#36182;&#39118;&#26684;&#36716;&#31227;&#25110;&#37325;&#20809;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#38590;&#20197;&#27867;&#21270;&#21040;&#22797;&#26434;&#30340;&#22812;&#38388;&#29615;&#22659;&#65307;2&#65289;&#24573;&#35270;&#20102;&#21160;&#24577;&#21644;&#23567;&#29289;&#20307;&#65288;&#22914;&#36710;&#36742;&#21644;&#26438;&#23376;&#65289;&#65292;&#36825;&#20123;&#29289;&#20307;&#38590;&#20197;&#30452;&#25509;&#20174;&#20854;&#20182;&#39046;&#22495;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04747v2 Announce Type: replace-cross  Abstract: Nighttime semantic segmentation plays a crucial role in practical applications, such as autonomous driving, where it frequently encounters difficulties caused by inadequate illumination conditions and the absence of well-annotated datasets. Moreover, semantic segmentation models trained on daytime datasets often face difficulties in generalizing effectively to nighttime conditions. Unsupervised domain adaptation (UDA) has shown the potential to address the challenges and achieved remarkable results for nighttime semantic segmentation. However, existing methods still face limitations in 1) their reliance on style transfer or relighting models, which struggle to generalize to complex nighttime environments, and 2) their ignorance of dynamic and small objects like vehicles and poles, which are difficult to be directly learned from other domains. This paper proposes a novel UDA method that refines both label and feature levels for 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26242;&#20572;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#26631;&#35760;&#21069;&#22788;&#29702;&#26356;&#22810;&#38544;&#34255;&#21521;&#37327;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;</title><link>https://arxiv.org/abs/2310.02226</link><description>&lt;p&gt;
&#35880;&#35328;&#24910;&#34892;&#65306;&#20351;&#29992;&#26242;&#20572;&#26631;&#35760;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Think before you speak: Training Language Models With Pause Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02226
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26242;&#20572;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#26631;&#35760;&#21069;&#22788;&#29702;&#26356;&#22810;&#38544;&#34255;&#21521;&#37327;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#31435;&#21363;&#36830;&#32493;&#29983;&#25104;&#19968;&#31995;&#21015;&#26631;&#35760;&#26469;&#29983;&#25104;&#21709;&#24212;: &#31532;$(K+1)^{th}$&#20010;&#26631;&#35760;&#26159;&#36890;&#36807;&#25805;&#20316;&#27599;&#23618;&#30340;$K$&#20010;&#38544;&#34255;&#21521;&#37327;&#24471;&#21040;&#30340;&#65292;&#27599;&#20010;&#21521;&#37327;&#23545;&#24212;&#19968;&#20010;&#21069;&#38754;&#30340;&#26631;&#35760;&#12290;&#22914;&#26524;&#25105;&#20204;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#31532;$(K+1)^{th}$&#20010;&#26631;&#35760;&#20043;&#21069;&#25805;&#20316;&#26356;&#22810;&#30340;&#38544;&#34255;&#21521;&#37327;&#65292;&#27604;&#22914;&#35828;$K+10$&#20010;&#21602;&#65311;&#25105;&#20204;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#65288;&#21487;&#23398;&#20064;&#30340;&#65289;$\textit{pause}$&#26631;&#35760;&#65292;&#36825;&#19968;&#31995;&#21015;&#26631;&#35760;&#38468;&#21152;&#21040;&#36755;&#20837;&#21069;&#32512;&#19978;&#12290;&#28982;&#21518;&#25105;&#20204;&#24310;&#36831;&#25552;&#21462;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#30452;&#21040;&#26368;&#21518;&#19968;&#20010;&#26242;&#20572;&#26631;&#35760;&#34987;&#30475;&#21040;&#65292;&#20174;&#32780;&#20801;&#35768;&#27169;&#22411;&#22312;&#20570;&#20986;&#31572;&#26696;&#20043;&#21069;&#36827;&#34892;&#39069;&#22806;&#30340;&#35745;&#31639;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#25317;&#26377;1B&#21644;130M&#21442;&#25968;&#30340;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;$\textit{pause-training}$&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#22312;C4&#19978;&#36827;&#34892;&#20102;&#22240;&#26524;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#28085;&#30422;&#25512;&#29702;&#12289;&#38382;&#31572;&#12289;&#26222;&#36941;&#29702;&#35299;&#21644;&#20107;&#23454;&#22238;&#24518;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;infer
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02226v2 Announce Type: replace-cross  Abstract: Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that infer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#21512;&#24182;&#19987;&#23478;&#20449;&#24687;&#26469;&#21046;&#23450;&#20986;&#26356;&#32039;&#20945;&#20294;&#26356;&#20855;&#30693;&#35782;&#30340;SMoE&#27169;&#22411;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;SMoE&#30340;&#19987;&#23478;&#21512;&#24182;&#12290;</title><link>https://arxiv.org/abs/2310.01334</link><description>&lt;p&gt;
&#21512;&#24182;&#65292;&#28982;&#21518;&#21387;&#32553;&#65306;&#20174;&#20854;&#36335;&#30001;&#31574;&#30053;&#20013;&#25581;&#31034;&#39640;&#25928;&#30340;SMoE&#25216;&#26415;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#21512;&#24182;&#19987;&#23478;&#20449;&#24687;&#26469;&#21046;&#23450;&#20986;&#26356;&#32039;&#20945;&#20294;&#26356;&#20855;&#30693;&#35782;&#30340;SMoE&#27169;&#22411;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;SMoE&#30340;&#19987;&#23478;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#28608;&#27963;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;SMoE&#65289;&#26174;&#31034;&#20986;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#35832;&#22914;&#65288;a&#65289;&#39640;&#20869;&#23384;&#20351;&#29992;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#32593;&#32476;&#23618;&#30340;&#37325;&#22797;&#25104;&#20026;&#22810;&#20010;&#19987;&#23478;&#30340;&#21103;&#26412;&#65307;&#20197;&#21450;&#65288;b&#65289;&#19987;&#23478;&#20013;&#30340;&#20887;&#20313;&#65292;&#22240;&#20026;&#24120;&#35268;&#22522;&#20110;&#23398;&#20064;&#30340;&#36335;&#30001;&#31574;&#30053;&#23481;&#26131;&#20986;&#29616;&#34920;&#31034;&#24615;&#23849;&#28291;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;SMoE&#27169;&#22411;&#22312;&#20869;&#23384;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#65292;&#23588;&#20854;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#19979;&#28216;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#36890;&#36807;&#21512;&#24182;&#19987;&#23478;&#20449;&#24687;&#26469;&#21046;&#23450;&#19968;&#20010;&#32039;&#20945;&#30340;SMoE&#27169;&#22411;&#65311;&#22914;&#20309;&#23558;&#22810;&#20010;&#19987;&#23478;&#21512;&#24182;&#20026;&#26356;&#23569;&#20294;&#26356;&#26377;&#30693;&#35782;&#30340;&#19987;&#23478;&#30340;&#26368;&#20339;&#26041;&#27861;&#65311;&#25105;&#20204;&#30340;&#21021;&#27493;&#35843;&#26597;&#26174;&#31034;&#65292;&#20256;&#32479;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#23545;&#20110;SMoE&#30340;&#19987;&#23478;&#21512;&#24182;&#24182;&#19981;&#26377;&#25928;&#12290;&#28508;&#22312;&#21407;&#22240;&#26159;&#65306;&#65288;1&#65289;&#20887;&#20313;&#20449;&#24687;&#25513;&#30422;&#20102;&#20851;&#38190;&#19987;&#23478;&#65307;&#65288;2&#65289;&#20026;&#27599;&#20010;&#19987;&#23478;&#36873;&#25321;&#36866;&#24403;&#30340;&#31070;&#32463;&#20803;&#25490;&#21015;&#26041;&#24335;&#20250;&#20002;&#22833;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01334v2 Announce Type: replace-cross  Abstract: Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up the learning capacity of neural networks, however, they have issues like (a) High Memory Usage, due to duplication of the network layers into multiple copies as experts; and (b) Redundancy in Experts, as common learning-based routing policies suffer from representational collapse. Therefore, vanilla SMoE models are memory inefficient and non-scalable, especially for resource-constrained downstream scenarios. In this paper, we ask: Can we craft a compact SMoE model by consolidating expert information? What is the best recipe to merge multiple experts into fewer but more knowledgeable experts? Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for SMoE. The potential reasons are: (1) redundant information overshadows critical experts; (2) appropriate neuron permutation for each expert is miss
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRICE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2305.13068</link><description>&lt;p&gt;
&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#24037;&#20855;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Making Language Models Better Tool Learners with Execution Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13068
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRICE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#20316;&#20026;&#20851;&#38190;&#30340;&#30028;&#38754;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#29702;&#35299;&#21644;&#25913;&#21464;&#29615;&#22659;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;AI&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#24037;&#20855;&#25193;&#23637;&#20854;&#33021;&#21147;&#24182;&#19982;&#30495;&#23454;&#19990;&#30028;&#20114;&#21160;&#12290;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65292;&#36890;&#24120;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#21152;&#36873;&#25321;&#22320;&#21033;&#29992;&#24037;&#20855;&#65292;&#22240;&#20026;&#22797;&#26434;&#20219;&#21153;&#24448;&#24448;&#36229;&#20986;&#20102;&#23427;&#20204;&#33258;&#36523;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#31616;&#21333;&#20219;&#21153;&#24341;&#20837;&#24037;&#20855;&#65288;&#27169;&#22411;&#26412;&#36523;&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#30340;&#20219;&#21153;&#65289;&#65292;&#21487;&#33021;&#20250;&#26080;&#24847;&#38388;&#20256;&#25773;&#38169;&#35823;&#32780;&#19981;&#26159;&#25552;&#39640;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21542;&#25945;&#20250;&#35821;&#35328;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#24037;&#20855;&#65311;&#20026;&#28385;&#36275;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tool leaRning wIth exeCution fEedback (TRICE)&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#24037;&#20855;&#25191;&#34892;&#20013;&#24471;&#21040;&#30340;&#21453;&#39304;&#19981;&#26029;&#23398;&#20064;&#65292;&#20174;&#32780;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed b
&lt;/p&gt;</description></item><item><title>AdaptiveClick &#26159;&#39318;&#20010;&#22522;&#20110;Transformer&#30340;&#12289;&#38024;&#23545;&#20132;&#20114;&#24335;&#22270;&#20687;&#20998;&#21106;&#30340;&#36974;&#32617;&#33258;&#36866;&#24212;&#20998;&#21106;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#28966;&#28857;&#25439;&#22833;&#20197;&#35299;&#20915;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36974;&#32617;&#21644;&#20687;&#32032;&#32423;&#27169;&#31946;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2305.04276</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;Click&#65306;&#20855;&#26377;&#33258;&#36866;&#24212;&#28966;&#28857;&#25439;&#22833;&#30340;&#28857;&#20987;&#24863;&#30693;Transformer&#29992;&#20110;&#20132;&#20114;&#24335;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
AdaptiveClick: Clicks-aware Transformer with Adaptive Focal Loss for Interactive Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.04276
&lt;/p&gt;
&lt;p&gt;
AdaptiveClick &#26159;&#39318;&#20010;&#22522;&#20110;Transformer&#30340;&#12289;&#38024;&#23545;&#20132;&#20114;&#24335;&#22270;&#20687;&#20998;&#21106;&#30340;&#36974;&#32617;&#33258;&#36866;&#24212;&#20998;&#21106;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#28966;&#28857;&#25439;&#22833;&#20197;&#35299;&#20915;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36974;&#32617;&#21644;&#20687;&#32032;&#32423;&#27169;&#31946;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#22270;&#20687;&#20998;&#21106;&#65288;IIS&#65289;&#24050;&#32463;&#25104;&#20026;&#20943;&#23569;&#27880;&#37322;&#26102;&#38388;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#22312;IIS&#30340;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20114;&#21160;&#27169;&#31946;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#38459;&#30861;&#20102;&#20998;&#21106;&#36136;&#37327;&#30340;&#38382;&#39064;&#65292;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AdaptiveClick&#8212;&#8212;&#19968;&#20010;&#28857;&#20987;&#24863;&#30693;&#30340;Transformer&#65292;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#28966;&#28857;&#25439;&#22833;&#65292;&#20197;&#35299;&#20915;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36974;&#32617;&#21644;&#20687;&#32032;&#32423;&#27169;&#31946;&#30340;&#24037;&#20855;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;AdaptiveClick&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#12289;&#38024;&#23545;IIS&#30340;&#36974;&#32617;&#33258;&#36866;&#24212;&#20998;&#21106;&#26694;&#26550;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;Click&#24863;&#30693;&#36974;&#32617;&#33258;&#36866;&#24212;Transformer&#35299;&#30721;&#22120;&#65288;CAMD&#65289;&#65292;&#23427;&#22686;&#24378;&#20102;&#28857;&#20987;&#21644;&#22270;&#20687;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;AdaptiveClick&#22312;&#20915;&#31574;&#31354;&#38388;&#20013;&#20351;&#20687;&#32032;&#33258;&#36866;&#24212;&#21306;&#20998;&#38590;&#26131;&#26679;&#26412;&#65292;&#29420;&#31435;&#20110;&#23427;&#20204;&#19981;&#26029;&#21464;&#21270;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.04276v2 Announce Type: replace-cross  Abstract: Interactive Image Segmentation (IIS) has emerged as a promising technique for decreasing annotation time. Substantial progress has been made in pre- and post-processing for IIS, but the critical issue of interaction ambiguity, notably hindering segmentation quality, has been under-researched. To address this, we introduce AdaptiveClick -- a click-aware transformer incorporating an adaptive focal loss that tackles annotation inconsistencies with tools for mask- and pixel-level ambiguity resolution. To the best of our knowledge, AdaptiveClick is the first transformer-based, mask-adaptive segmentation framework for IIS. The key ingredient of our method is the Click-Aware Mask-adaptive transformer Decoder (CAMD), which enhances the interaction between click and image features. Additionally, AdaptiveClick enables pixel-adaptive differentiation of hard and easy samples in the decision space, independent of their varying distributions
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#34892;&#21160;&#24615;&#21407;&#23376;&#27010;&#24565;&#23398;&#20064;&#65288;AACL&#65289;&#26041;&#27861;&#65292;&#23558;&#35270;&#35273;&#35266;&#23519;&#26144;&#23556;&#21040;&#34892;&#21160;&#24615;&#21407;&#23376;&#27010;&#24565;&#65292;&#20316;&#20026;&#35266;&#23519;&#21644;&#25351;&#20196;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20943;&#23569;&#35821;&#20041;&#24046;&#36317;&#24182;&#31616;&#21270;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2302.06072</link><description>&lt;p&gt;
&#34892;&#21160;&#24615;&#21407;&#23376;&#27010;&#24565;&#23398;&#20064;&#29992;&#20110;&#35299;&#23494;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.06072
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#34892;&#21160;&#24615;&#21407;&#23376;&#27010;&#24565;&#23398;&#20064;&#65288;AACL&#65289;&#26041;&#27861;&#65292;&#23558;&#35270;&#35273;&#35266;&#23519;&#26144;&#23556;&#21040;&#34892;&#21160;&#24615;&#21407;&#23376;&#27010;&#24565;&#65292;&#20316;&#20026;&#35266;&#23519;&#21644;&#25351;&#20196;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20943;&#23569;&#35821;&#20041;&#24046;&#36317;&#24182;&#31616;&#21270;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2302.06072v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#36328;&#25688;&#35201;&#65306;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#19968;&#20010;&#20195;&#29702;&#20154;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#35266;&#23519;&#32467;&#26524;&#19982;&#35821;&#35328;&#25351;&#20196;&#23545;&#40784;&#65292;&#20197;&#36798;&#21040;&#30446;&#26631;&#20301;&#32622;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;VLN&#20195;&#29702;&#30452;&#25509;&#23398;&#20064;&#23558;&#21407;&#22987;&#26041;&#21521;&#29305;&#24449;&#21644;&#20351;&#29992;&#19968;&#20301;&#26631;&#31614;&#35757;&#32451;&#30340;&#35270;&#35273;&#29305;&#24449;&#19982;&#35821;&#35328;&#25351;&#20196;&#29305;&#24449;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22810;&#27169;&#24577;&#36755;&#20837;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35821;&#20041;&#24046;&#36317;&#20351;&#24471;&#23545;&#40784;&#21464;&#24471;&#22256;&#38590;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23548;&#33322;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34892;&#21160;&#24615;&#21407;&#23376;&#27010;&#24565;&#23398;&#20064;&#65288;AACL&#65289;&#65292;&#23427;&#23558;&#35270;&#35273;&#35266;&#23519;&#32467;&#26524;&#26144;&#23556;&#21040;&#34892;&#21160;&#24615;&#21407;&#23376;&#27010;&#24565;&#20197;&#20419;&#36827;&#23545;&#40784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#34892;&#21160;&#24615;&#21407;&#23376;&#27010;&#24565;&#26159;&#21253;&#21547;&#21407;&#23376;&#21160;&#20316;&#21644;&#23545;&#35937;&#30340;&#33258;&#28982;&#35821;&#35328;&#30701;&#35821;&#65292;&#20363;&#22914;&#8220;&#19978;&#27004;&#26799;&#8221;&#12290;&#36825;&#20123;&#34892;&#21160;&#24615;&#21407;&#23376;&#27010;&#24565;&#20316;&#20026;&#35266;&#23519;&#21644;&#25351;&#20196;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#35821;&#20041;&#24046;&#36317;&#24182;&#31616;&#21270;&#23545;&#40784;&#12290;AACL
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.06072v2 Announce Type: replace-cross  Abstract: Vision-Language Navigation (VLN) is a challenging task which requires an agent to align complex visual observations to language instructions to reach the goal position. Most existing VLN agents directly learn to align the raw directional features and visual features trained using one-hot labels to linguistic instruction features. However, the big semantic gap among these multi-modal inputs makes the alignment difficult and therefore limits the navigation performance. In this paper, we propose Actional Atomic-Concept Learning (AACL), which maps visual observations to actional atomic concepts for facilitating the alignment. Specifically, an actional atomic concept is a natural language phrase containing an atomic action and an object, e.g., ``go up stairs''. These actional atomic concepts, which serve as the bridge between observations and instructions, can effectively mitigate the semantic gap and simplify the alignment. AACL co
&lt;/p&gt;</description></item><item><title>&#36801;&#31227;&#23398;&#20064;&#32467;&#21512;&#36793;&#32536;&#35745;&#31639;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#35270;&#38382;&#39064;&#21644;&#30005;&#22120;&#35782;&#21035;&#38382;&#39064;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;CNN&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2301.03018</link><description>&lt;p&gt;
&#26234;&#33021;&#23478;&#23621;&#20013;&#30340;&#33021;&#37327;&#20998;&#35299;&#19982;&#30005;&#22120;&#35782;&#21035;&#65306;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#36793;&#32536;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Energy Disaggregation &amp; Appliance Identification in a Smart Home: Transfer Learning enables Edge Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.03018
&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#32467;&#21512;&#36793;&#32536;&#35745;&#31639;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#35270;&#38382;&#39064;&#21644;&#30005;&#22120;&#35782;&#21035;&#38382;&#39064;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;CNN&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#35270;&#65288;NILM&#65289;&#25110;&#33021;&#37327;&#20998;&#35299;&#26088;&#22312;&#22312;&#26234;&#33021;&#23478;&#23621;&#30340;&#24635;&#36127;&#36733;&#37197;&#32622;&#25991;&#20214;&#30340;&#22522;&#30784;&#19978;&#25552;&#21462;&#21333;&#20010;&#28040;&#36153;&#30005;&#23376;&#35774;&#22791;&#30340;&#36127;&#36733;&#37197;&#32622;&#25991;&#20214;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#26041;&#27861;&#26469;&#35299;&#20915;NILM&#38382;&#39064;&#20197;&#21450;&#19968;&#20123;&#30456;&#20851;&#38382;&#39064;&#12290;1&#65289;&#25105;&#20204;&#22312;&#33879;&#21517;&#30340;seq2-point&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#25552;&#20986;&#30340;seq2-[3]-point CNN&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#65288;&#23478;&#24237;&#65289;NILM&#38382;&#39064;&#21644;&#31449;&#28857;NILM&#38382;&#39064;&#65288;&#22522;&#26412;&#19978;&#26159;&#22312;&#36739;&#23567;&#35268;&#27169;&#19978;&#30340;NILM&#65289;&#12290;2&#65289;&#25105;&#20204;&#36890;&#36807;&#20511;&#37492;&#26368;&#20808;&#36827;&#30340;&#65288;&#39044;&#35757;&#32451;&#65289;2D-CNN&#27169;&#22411;&#65292;&#21363;AlexNet&#12289;ResNet-18&#21644;DenseNet-121&#65292;&#35299;&#20915;&#20102;&#30005;&#22120;&#35782;&#21035;&#30340;&#30456;&#20851;&#38382;&#39064;&#65292;&#36825;&#20123;&#27169;&#22411;&#34987;&#24494;&#35843;&#20197;&#36866;&#24212;&#20004;&#20010;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#30005;&#22120;&#22522;&#20110;&#23567;&#27874;&#21464;&#25442;&#21644;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;STFT&#65289;&#30340;2D&#30005;&#29305;&#24449;&#12290;3&#65289;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20123;&#22522;&#26412;&#30340;&#23450;&#24615;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.03018v2 Announce Type: replace-cross  Abstract: Non-intrusive load monitoring (NILM) or energy disaggregation aims to extract the load profiles of individual consumer electronic appliances, given an aggregate load profile of the mains of a smart home. This work proposes a novel deep-learning and edge computing approach to solve the NILM problem and a few related problems as follows. 1) We build upon the reputed seq2-point convolutional neural network (CNN) model to come up with the proposed seq2-[3]-point CNN model to solve the (home) NILM problem and site-NILM problem (basically, NILM at a smaller scale). 2) We solve the related problem of appliance identification by building upon the state-of-the-art (pre-trained) 2D-CNN models, i.e., AlexNet, ResNet-18, and DenseNet-121, which are fine-tuned two custom datasets that consist of Wavelets and short-time Fourier transform (STFT)-based 2D electrical signatures of the appliances. 3) Finally, we do some basic qualitative inferen
&lt;/p&gt;</description></item><item><title>COMET&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#38598;&#32676;&#35774;&#35745;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#30740;&#31350;&#24182;&#34892;&#21270;&#31574;&#30053;&#21644;&#20851;&#38190;&#38598;&#32676;&#36164;&#28304;&#37197;&#32622;&#23545;&#20998;&#24067;&#24335;DL&#35757;&#32451;&#24615;&#33021;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2211.16648</link><description>&lt;p&gt;
COMET&#65306;&#29992;&#20110;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#30340;&#20840;&#38754;&#38598;&#32676;&#35774;&#35745;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
COMET: A Comprehensive Cluster Design Methodology for Distributed Deep Learning Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.16648
&lt;/p&gt;
&lt;p&gt;
COMET&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#38598;&#32676;&#35774;&#35745;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#30740;&#31350;&#24182;&#34892;&#21270;&#31574;&#30053;&#21644;&#20851;&#38190;&#38598;&#32676;&#36164;&#28304;&#37197;&#32622;&#23545;&#20998;&#24067;&#24335;DL&#35757;&#32451;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#24050;&#32463;&#21457;&#23637;&#21040;&#38656;&#35201;&#22823;&#35268;&#27169;&#19987;&#38376;&#30340;&#12289;&#39640;&#31471;&#33410;&#28857;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#23567;&#12290;&#35774;&#35745;&#36825;&#26679;&#30340;&#38598;&#32676;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#24615;&#33021;&#21644;&#21033;&#29992;&#29575;--&#20197;&#25674;&#38144;&#20854;&#39640;&#26114;&#25104;&#26412;--&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20180;&#32454;&#24179;&#34913;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#32593;&#32476;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#27169;&#22411;&#30340;&#20247;&#22810;&#35843;&#25972;&#26059;&#38062;&#26497;&#22823;&#22320;&#24433;&#21709;&#24615;&#33021;&#65292;&#26368;&#20339;&#20540;&#24448;&#24448;&#21462;&#20915;&#20110;&#24213;&#23618;&#38598;&#32676;&#30340;&#29305;&#24449;&#65292;&#36825;&#35201;&#27714;&#36827;&#34892;&#22797;&#26434;&#30340;&#38598;&#32676;-&#24037;&#20316;&#36127;&#36733;&#21327;&#21516;&#35774;&#35745;&#36807;&#31243;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20123;&#22823;&#35268;&#27169;DL&#35757;&#32451;&#38598;&#32676;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;COMET&#65292;&#36825;&#26159;&#19968;&#31181;&#32508;&#21512;&#30340;&#38598;&#32676;&#35774;&#35745;&#26041;&#27861;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#20849;&#21516;&#30740;&#31350;&#24182;&#34892;&#21270;&#31574;&#30053;&#21644;&#20851;&#38190;&#38598;&#32676;&#36164;&#28304;&#37197;&#32622;&#23545;&#20998;&#24067;&#24335;DL&#35757;&#32451;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36880;&#27493;&#30340;&#36807;&#31243;&#65292;&#24314;&#31435;&#19968;&#31181;&#21487;&#37325;&#29992;&#21644;&#28789;&#27963;&#30340;&#26041;&#27861;&#35770;&#65292;&#24182;&#21152;&#20197;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.16648v2 Announce Type: replace-cross  Abstract: Modern Deep Learning (DL) models have grown to sizes requiring massive clusters of specialized, high-end nodes to train. Designing such clusters to maximize both performance and utilization--to amortize their steep cost--is a challenging task requiring careful balance of compute, memory, and network resources. Moreover, a plethora of each model's tuning knobs drastically affect the performance, with optimal values often depending on the underlying cluster's characteristics, which necessitates a complex cluster-workload co-design process. To facilitate the design space exploration of such massive DL training clusters, we introduce COMET, a holistic cluster design methodology and workflow to jointly study the impact of parallelization strategies and key cluster resource provisioning on the performance of distributed DL training. We develop a step-by-step process to establish a reusable and flexible methodology, and demonstrate it
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2207.14000</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#19978;&#30340;&#22810;&#27493;&#28436;&#32462;&#25512;&#29702;&#65306;&#22522;&#20110;&#36229;&#39046;&#22495;&#27867;&#21270;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.14000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#36923;&#36753;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#24182;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21463;DeepLogic&#21551;&#21457;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#29992;&#20110;&#25191;&#34892;&#36923;&#36753;&#31243;&#24207;&#25512;&#29702;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IMA-GloVe-GA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25512;&#29702;&#26159;&#20351;&#29992;&#22522;&#20110;RNN&#30340;&#36845;&#20195;&#20869;&#23384;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#38376;&#20851;&#27880;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;PARARULES&#12289;CONCEPTRULES V1&#21644;CONCEPTRULES V2&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;IMA-GloVe-GA&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#38376;&#20851;&#27880;&#26426;&#21046;&#30340;DeepLogic&#27604;DeepLogic&#21644;&#20854;&#20182;RNN&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35268;&#21017;&#34987;&#25171;&#20081;&#26102;&#27604;RoBERTa-Large&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36229;&#39046;&#22495;&#27867;&#21270;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#22810;&#27493;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#25512;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.14000v2 Announce Type: replace-cross  Abstract: Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gate attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datase
&lt;/p&gt;</description></item><item><title>SPI-GAN&#20351;&#29992;&#30452;&#32447;&#36335;&#24452;&#25554;&#20540;&#23450;&#20041;&#30340;&#22686;&#24378;&#22411;GAN&#21435;&#22122;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#22823;&#31243;&#24230;&#19978;&#20943;&#23569;&#37319;&#26679;&#26102;&#38388;&#65292;&#21516;&#26102;&#23454;&#29616;&#19982;SGMs&#30456;&#21516;&#30340;&#39640;&#37319;&#26679;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2206.14464</link><description>&lt;p&gt;
&#20351;&#29992;&#30452;&#32447;&#36335;&#24452;&#25554;&#20540;&#23545;Denoising Diffusion GANs&#30340;SPI-GAN&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SPI-GAN: Denoising Diffusion GANs with Straight-Path Interpolations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.14464
&lt;/p&gt;
&lt;p&gt;
SPI-GAN&#20351;&#29992;&#30452;&#32447;&#36335;&#24452;&#25554;&#20540;&#23450;&#20041;&#30340;&#22686;&#24378;&#22411;GAN&#21435;&#22122;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#22823;&#31243;&#24230;&#19978;&#20943;&#23569;&#37319;&#26679;&#26102;&#38388;&#65292;&#21516;&#26102;&#23454;&#29616;&#19982;SGMs&#30456;&#21516;&#30340;&#39640;&#37319;&#26679;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;/&#37319;&#26679;&#22797;&#26434;&#24615;&#30001;&#20110;&#39640;&#24230;&#22797;&#26434;&#30340;&#21069;&#21521;/&#21518;&#21521;&#36807;&#31243;&#32780;&#26497;&#39640;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#21069;&#23545;&#23398;&#20064;&#26356;&#31616;&#21333;&#36807;&#31243;&#30340;&#20851;&#27880;&#24230;&#27491;&#36880;&#28176;&#22686;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#22522;&#20110;GAN&#30340;&#21435;&#22122;&#26041;&#27861;&#65292;&#31216;&#20026;SPI-GAN&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#30452;&#32447;&#36335;&#24452;&#25554;&#20540;&#23450;&#20041;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GAN&#26550;&#26500;&#65292;&#36890;&#36807;&#30452;&#32447;&#36335;&#24452;&#21435;&#22122;&#65292;&#24182;&#19988;&#30001;&#36830;&#32493;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#34920;&#24449;&#65292;&#20197;&#27169;&#20223;&#21435;&#22122;&#36335;&#24452;&#12290;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23569;&#20102;&#37319;&#26679;&#26102;&#38388;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;SGMs&#30456;&#21516;&#30340;&#39640;&#37319;&#26679;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;SPI-GAN&#26159;CIFAR-10&#21644;CelebA-HQ-256&#20013;&#37319;&#26679;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#26102;&#38388;&#20013;&#26368;&#22343;&#34913;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.14464v3 Announce Type: replace-cross  Abstract: Score-based generative models (SGMs) show the state-of-the-art sampling quality and diversity. However, their training/sampling complexity is notoriously high due to the highly complicated forward/reverse processes, so they are not suitable for resource-limited settings. To solving this problem, learning a simpler process is gathering much attention currently. We present an enhanced GAN-based denoising method, called SPI-GAN, using our proposed straight-path interpolation definition. To this end, we propose a GAN architecture i) denoising through the straight-path and ii) characterized by a continuous mapping neural network for imitating the denoising path. This approach drastically reduces the sampling time while achieving as high sampling quality and diversity as SGMs. As a result, SPI-GAN is one of the best-balanced models among the sampling quality, diversity, and time for CIFAR-10, and CelebA-HQ-256.
&lt;/p&gt;</description></item><item><title>OpenXAI &#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#25143;&#21487;&#36731;&#26494;&#25193;&#23637;&#21644;&#27604;&#36739;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2206.11104</link><description>&lt;p&gt;
OpenXAI: &#36808;&#21521;&#36879;&#26126;&#35780;&#20272;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
OpenXAI: Towards a Transparent Evaluation of Model Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.11104
&lt;/p&gt;
&lt;p&gt;
OpenXAI &#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#25143;&#21487;&#36731;&#26494;&#25193;&#23637;&#21644;&#27604;&#36739;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#36817;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#65292;&#20294;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#24615;&#22522;&#20934;&#27979;&#35797;&#30340;&#24037;&#20316;&#38750;&#24120;&#23569;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OpenXAI&#65292;&#19968;&#20010;&#20840;&#38754;&#19988;&#21487;&#25193;&#23637;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#21518;&#32493;&#35299;&#37322;&#26041;&#27861;&#12290;OpenXAI&#21253;&#25324;&#20197;&#19979;&#20851;&#38190;&#32452;&#20214;&#65306;&#65288;i&#65289;&#28789;&#27963;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#21644;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#29305;&#24449;&#24402;&#23646;&#26041;&#27861;&#30340;&#38598;&#21512;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#26041;&#27861;&#24544;&#23454;&#24230;&#12289;&#31283;&#23450;&#24615;&#65288;&#40065;&#26834;&#24615;&#65289;&#21644;&#20844;&#24179;&#24615;&#30340;&#21313;&#19968;&#31181;&#37327;&#21270;&#24230;&#37327;&#26631;&#20934;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#23545;&#22810;&#31181;&#24230;&#37327;&#26631;&#20934;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#20960;&#31181;&#35299;&#37322;&#26041;&#27861;&#30340;&#27604;&#36739;&#12290;OpenXAI&#26131;&#20110;&#25193;&#23637;&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#35780;&#20272;&#33258;&#23450;&#20041;&#35299;&#37322;&#26041;&#27861;&#24182;&#23558;&#20854;&#32435;&#20837;&#25105;&#20204;&#30340;&#25490;&#34892;&#27036;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.11104v4 Announce Type: replace-cross  Abstract: While several types of post hoc explanation methods have been proposed in recent literature, there is very little work on systematically benchmarking these methods. Here, we introduce OpenXAI, a comprehensive and extensible open-source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, and (ii) open-source implementations of eleven quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, in turn providing comparisons of several explanation methods across a wide variety of metrics, models, and datasets. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.11792</link><description>&lt;p&gt;
&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31034;&#33539;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20302;&#23433;&#20840;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#20197;&#20934;&#30830;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#29615;&#22659;&#21644;&#36710;&#36742;&#29366;&#24577;&#21160;&#24577;&#21046;&#23450;&#36866;&#24403;&#30340;&#39550;&#39542;&#31574;&#30053;&#65292;&#21516;&#26102;&#30830;&#20445;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#23433;&#20840;&#24615;&#20302;&#12289;&#27867;&#21270;&#33021;&#21147;&#24046;&#21644;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#65292;&#32780;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#26159;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#30340;&#21069;&#25552;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#32780;&#22810;&#26679;&#22330;&#26223;&#19979;&#30340;&#23433;&#20840;&#19988;&#24191;&#20041;&#30340;&#31471;&#21040;&#31471;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479; (SGADS)&#12290;&#25105;&#20204;&#30340;SGADS&#19982;&#21464;&#20998;&#25512;&#29702;&#21644;&#24402;&#19968;&#21270;&#27969;&#32467;&#21512;&#65292;&#20351;&#26234;&#33021;&#36710;&#36742;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#39550;&#39542;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#31034;&#33539;&#30456;&#32467;&#21512;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
An intelligent driving system should be capable of dynamically formulating appropriate driving strategies based on the current environment and vehicle status, while ensuring the security and reliability of the system. However, existing methods based on reinforcement learning and imitation learning suffer from low safety, poor generalization, and inefficient sampling. Additionally, they cannot accurately predict future driving trajectories, and the accurate prediction of future driving trajectories is a precondition for making optimal decisions. To solve these problems, in this paper, we introduce a Safe and Generalized end-to-end Autonomous Driving System (SGADS) for complex and various scenarios. Our SGADS incorporates variational inference with normalizing flows, enabling the intelligent vehicle to accurately predict future driving trajectories. Moreover, we propose the formulation of robust safety constraints. Furthermore, we combine reinforcement learning with demonstrations to aug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;LDReg&#30340;&#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#22349;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#22686;&#21152;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65292;LDReg&#33021;&#22815;&#25913;&#21892;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10474</link><description>&lt;p&gt;
LDReg: &#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LDReg: Local Dimensionality Regularized Self-Supervised Learning. (arXiv:2401.10474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;LDReg&#30340;&#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#22349;&#32553;&#38382;&#39064;&#12290;&#36890;&#36807;&#22686;&#21152;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65292;LDReg&#33021;&#22815;&#25913;&#21892;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#23398;&#20064;&#30340;&#34920;&#31034;&#21487;&#33021;&#23481;&#26131;&#20986;&#29616;&#32500;&#24230;&#22349;&#32553;&#65292;&#20854;&#20013;&#23398;&#20064;&#30340;&#34920;&#31034;&#23376;&#31354;&#38388;&#32500;&#24230;&#26497;&#20302;&#65292;&#22240;&#27492;&#26080;&#27861;&#34920;&#31034;&#23436;&#25972;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#24577;&#12290;&#32500;&#24230;&#22349;&#32553;&#20063;&#34987;&#31216;&#20026;&#8220;&#22635;&#20805;&#19981;&#36275;&#8221;&#29616;&#35937;&#65292;&#26159;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#30740;&#31350;&#20102;SSL&#30340;&#32500;&#24230;&#22349;&#32553;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#34920;&#31034;&#21487;&#20197;&#22312;&#20840;&#23616;&#19978;&#35206;&#30422;&#39640;&#32500;&#31354;&#38388;&#65292;&#20294;&#22312;&#23616;&#37096;&#19978;&#20250;&#22349;&#32553;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#26412;&#22320;&#32500;&#24230;&#27491;&#21017;&#21270;&#65288;LDReg&#65289;&#8221;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20844;&#24335;&#26159;&#22522;&#20110;Fisher-Rao&#24230;&#37327;&#30340;&#25512;&#23548;&#65292;&#29992;&#20110;&#27604;&#36739;&#21644;&#20248;&#21270;&#27599;&#20010;&#25968;&#25454;&#28857;&#22312;&#28176;&#36827;&#23567;&#21322;&#24452;&#22788;&#30340;&#23616;&#37096;&#36317;&#31163;&#20998;&#24067;&#12290;&#36890;&#36807;&#22686;&#21152;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;LDReg&#21487;&#20197;&#25913;&#21892;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations learned via self-supervised learning (SSL) can be susceptible to dimensional collapse, where the learned representation subspace is of extremely low dimensionality and thus fails to represent the full data distribution and modalities. Dimensional collapse also known as the "underfilling" phenomenon is one of the major causes of degraded performance on downstream tasks. Previous work has investigated the dimensional collapse problem of SSL at a global level. In this paper, we demonstrate that representations can span over high dimensional space globally, but collapse locally. To address this, we propose a method called $\textit{local dimensionality regularization (LDReg)}$. Our formulation is based on the derivation of the Fisher-Rao metric to compare and optimize local distance distributions at an asymptotically small radius for each data point. By increasing the local intrinsic dimensionality, we demonstrate through a range of experiments that LDReg improves the repres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65288;RAISE&#65289;&#65292;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#35268;&#21017;&#25277;&#35937;&#21644;&#36873;&#25321;&#65292;&#20197;&#35299;&#20915;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#29616;&#23454;&#30340;&#22330;&#26223;&#20013;&#23637;&#31034;&#20986;&#25277;&#35937;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09966</link><description>&lt;p&gt;
&#26397;&#21521;&#29983;&#25104;&#24335;&#25277;&#35937;&#25512;&#29702;&#65306;&#36890;&#36807;&#35268;&#21017;&#25277;&#35937;&#21644;&#36873;&#25321;&#26469;&#23436;&#25104;Raven&#30340;&#28176;&#36827;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Towards Generative Abstract Reasoning: Completing Raven's Progressive Matrix via Rule Abstraction and Selection. (arXiv:2401.09966v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65288;RAISE&#65289;&#65292;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#35268;&#21017;&#25277;&#35937;&#21644;&#36873;&#25321;&#65292;&#20197;&#35299;&#20915;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#29616;&#23454;&#30340;&#22330;&#26223;&#20013;&#23637;&#31034;&#20986;&#25277;&#35937;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#36171;&#20104;&#26426;&#22120;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25506;&#32034;&#26426;&#22120;&#26234;&#33021;&#20013;&#30340;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65292;&#27169;&#22411;&#38656;&#35201;&#29702;&#35299;&#28508;&#22312;&#30340;&#35268;&#21017;&#24182;&#20174;&#20505;&#36873;&#38598;&#20013;&#36873;&#25321;&#32570;&#22833;&#30340;&#21491;&#19979;&#22270;&#20687;&#26469;&#23436;&#25104;&#22270;&#20687;&#30697;&#38453;&#12290;&#21442;&#19982;&#32773;&#21487;&#20197;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#30340;&#23646;&#24615;&#21464;&#21270;&#35268;&#21017;&#21644;&#24819;&#35937;&#20219;&#24847;&#20301;&#32622;&#30340;&#32570;&#22833;&#22270;&#20687;&#23637;&#31034;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#24456;&#38590;&#22312;&#29616;&#23454;&#30340;RPM&#38382;&#39064;&#20013;&#23637;&#31034;&#20986;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35268;&#21017;&#25277;&#35937;&#21644;&#36873;&#25321;&#65288;RAISE&#65289;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#31572;&#26696;&#29983;&#25104;&#38382;&#39064;&#12290;RAISE&#23558;&#22270;&#20687;&#23646;&#24615;&#32534;&#30721;&#20026;&#28508;&#22312;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#27010;&#24565;&#23558;&#28508;&#22312;&#35268;&#21017;&#20998;&#35299;&#25104;&#21407;&#23376;&#35268;&#21017;&#65292;&#36825;&#20123;&#21407;&#23376;&#35268;&#21017;&#34987;&#25277;&#35937;&#20026;&#20840;&#23616;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#22312;&#29983;&#25104;&#31572;&#26696;&#26102;&#65292;RAISE&#36873;&#25321;...
&lt;/p&gt;
&lt;p&gt;
Endowing machines with abstract reasoning ability has been a long-term research topic in artificial intelligence. Raven's Progressive Matrix (RPM) is widely used to probe abstract visual reasoning in machine intelligence, where models need to understand the underlying rules and select the missing bottom-right images out of candidate sets to complete image matrices. The participators can display powerful reasoning ability by inferring the underlying attribute-changing rules and imagining the missing images at arbitrary positions. However, existing solvers can hardly manifest such an ability in realistic RPM problems. In this paper, we propose a conditional generative model to solve answer generation problems through Rule AbstractIon and SElection (RAISE) in the latent space. RAISE encodes image attributes as latent concepts and decomposes underlying rules into atomic rules by means of concepts, which are abstracted as global learnable parameters. When generating the answer, RAISE select
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20855;&#21306;&#20998;&#24615;&#21050;&#28608;&#29289;&#30340;&#20248;&#21270;&#32858;&#31867;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#23567;&#40736;&#35270;&#32593;&#33180;&#12289;&#24658;&#27827;&#29492;&#35270;&#32593;&#33180;&#21644;&#29461;&#29492;V4&#35270;&#35273;&#21306;&#30340;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.05342</link><description>&lt;p&gt;
&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#21050;&#28608;&#29289;&#29992;&#20110;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Most discriminative stimuli for functional cell type identification. (arXiv:2401.05342v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20855;&#21306;&#20998;&#24615;&#21050;&#28608;&#29289;&#30340;&#20248;&#21270;&#32858;&#31867;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#23567;&#40736;&#35270;&#32593;&#33180;&#12289;&#24658;&#27827;&#29492;&#35270;&#32593;&#33180;&#21644;&#29461;&#29492;V4&#35270;&#35273;&#21306;&#30340;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#32454;&#32990;&#31867;&#22411;&#24182;&#29702;&#35299;&#20854;&#21151;&#33021;&#29305;&#24615;&#23545;&#25581;&#31034;&#24863;&#30693;&#21644;&#35748;&#30693;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35270;&#32593;&#33180;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#30340;&#21050;&#28608;&#29289;&#26469;&#35782;&#21035;&#21151;&#33021;&#31867;&#22411;&#65292;&#20294;&#36825;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#20250;&#23545;&#20197;&#21069;&#24050;&#30693;&#30340;&#32454;&#32990;&#31867;&#22411;&#20135;&#29983;&#20559;&#35265;&#12290;&#22312;&#35270;&#35273;&#30382;&#23618;&#20013;&#65292;&#20173;&#28982;&#19981;&#30693;&#36947;&#23384;&#22312;&#20160;&#20040;&#21151;&#33021;&#31867;&#22411;&#20197;&#21450;&#22914;&#20309;&#35782;&#21035;&#23427;&#20204;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#35270;&#32593;&#33180;&#21644;&#35270;&#35273;&#30382;&#23618;&#20013;&#30340;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#36827;&#34892;&#26080;&#20559;&#35265;&#30340;&#35782;&#21035;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#21050;&#28608;&#29289;&#65288;MDS&#65289;&#26469;&#33719;&#24471;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21050;&#28608;&#29289;&#30340;&#20248;&#21270;&#21644;&#32858;&#31867;&#37325;&#26032;&#20998;&#37197;&#20043;&#38388;&#30340;&#20132;&#26367;&#36827;&#34892;&#65292;&#31867;&#20284;&#20110;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#25104;&#21151;&#24674;&#22797;&#20102;&#23567;&#40736;&#35270;&#32593;&#33180;&#12289;&#24658;&#27827;&#29492;&#35270;&#32593;&#33180;&#21644;&#29461;&#29492;V4&#35270;&#35273;&#21306;&#30340;&#21151;&#33021;&#32858;&#31867;&#12290;&#36825;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#36827;&#34892;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying cell types and understanding their functional properties is crucial for unraveling the mechanisms underlying perception and cognition. In the retina, functional types can be identified by carefully selected stimuli, but this requires expert domain knowledge and biases the procedure towards previously known cell types. In the visual cortex, it is still unknown what functional types exist and how to identify them. Thus, for unbiased identification of the functional cell types in retina and visual cortex, new approaches are needed. Here we propose an optimization-based clustering approach using deep predictive models to obtain functional clusters of neurons using Most Discriminative Stimuli (MDS). Our approach alternates between stimulus optimization with cluster reassignment akin to an expectation-maximization algorithm. The algorithm recovers functional clusters in mouse retina, marmoset retina and macaque visual area V4. This demonstrates that our approach can successfully 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;AI&#22686;&#24378;&#20102;&#20010;&#20307;&#21019;&#36896;&#21147;&#65292;&#20294;&#38477;&#20302;&#20102;&#26032;&#20869;&#23481;&#30340;&#38598;&#20307;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.00506</link><description>&lt;p&gt;
&#29983;&#25104;AI&#22686;&#24378;&#20102;&#20010;&#20307;&#21019;&#36896;&#21147;&#65292;&#20294;&#38477;&#20302;&#20102;&#26032;&#20869;&#23481;&#30340;&#38598;&#20307;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generative AI enhances individual creativity but reduces the collective diversity of novel content. (arXiv:2312.00506v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.00506
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22686;&#24378;&#20102;&#20010;&#20307;&#21019;&#36896;&#21147;&#65292;&#20294;&#38477;&#20302;&#20102;&#26032;&#20869;&#23481;&#30340;&#38598;&#20307;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#21147;&#26159;&#20154;&#31867;&#30340;&#26680;&#24515;&#12290;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#65292;&#21253;&#25324;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#36807;&#25552;&#20379;&#26032;&#30340;&#24819;&#27861;&#20351;&#20154;&#31867;&#26356;&#20855;&#21019;&#36896;&#21147;&#65292;&#25110;&#36890;&#36807;&#38170;&#23450;&#20110;GenAI&#30340;&#24819;&#27861;&#32780;&#21464;&#24471;&#19981;&#37027;&#20040;&#21019;&#36896;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#32447;&#23454;&#39564;&#30740;&#31350;&#25506;&#31350;&#20102;GenAI&#24819;&#27861;&#23545;&#19968;&#31687;&#30701;&#31687;&#23567;&#35828;&#21019;&#20316;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#20854;&#20013;&#19968;&#20123;&#20316;&#32773;&#21487;&#20197;&#20174;GenAI&#24179;&#21488;&#33719;&#21462;&#25925;&#20107;&#21019;&#24847;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#33719;&#21462;GenAI&#24819;&#27861;&#23548;&#33268;&#25925;&#20107;&#34987;&#35780;&#20026;&#26356;&#26377;&#21019;&#36896;&#21147;&#12289;&#20889;&#24471;&#26356;&#22909;&#21644;&#26356;&#20196;&#20154;&#24841;&#24742;&#65292;&#29305;&#21035;&#26159;&#22312;&#21019;&#36896;&#21147;&#36739;&#20302;&#30340;&#20316;&#32773;&#20013;&#12290;&#28982;&#32780;&#65292;GenAI&#21551;&#29992;&#30340;&#25925;&#20107;&#20043;&#38388;&#26356;&#30456;&#20284;&#65292;&#32780;&#19981;&#26159;&#20165;&#30001;&#20154;&#31867;&#21019;&#20316;&#30340;&#25925;&#20107;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#20010;&#20307;&#21019;&#36896;&#21147;&#22686;&#21152;&#30340;&#21516;&#26102;&#65292;&#38598;&#20307;&#26032;&#39062;&#24615;&#21487;&#33021;&#20250;&#20943;&#23569;&#12290;&#36825;&#31181;&#21160;&#24577;&#31867;&#20284;&#20110;&#31038;&#20250;&#22256;&#22659;&#65306;&#36890;&#36807;GenAI&#65292;&#20010;&#21035;&#20316;&#23478;&#33021;&#21463;&#30410;&#65292;&#20294;&#21487;&#33021;&#20250;&#20135;&#29983;&#26356;&#31364;&#33539;&#22260;&#30340;&#26032;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#30740;&#31350;&#20154;&#21592;&#12289;&#20915;&#31574;&#32773;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creativity is core to being human. Generative artificial intelligence (GenAI) -- including ever more powerful large language models (LLMs) -- holds promise for humans to be more creative by offering new ideas, or less creative by anchoring on GenAI ideas. We study the causal impact of GenAI ideas on the production of a short story in an online experimental study where some writers could obtain story ideas from a GenAI platform. We find that access to GenAI ideas causes stories to be evaluated as more creative, better written, and more enjoyable, especially among less creative writers. However, GenAI-enabled stories are more similar to each other than stories by humans alone. These results point to an increase in individual creativity at the risk of losing collective novelty. This dynamic resembles a social dilemma: with GenAI, individual writers are better off, but collectively a narrower scope of novel content may be produced. Our results have implications for researchers, policy-make
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#21160;&#24494;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#65292;&#23558;&#25152;&#38656;&#23548;&#25968;&#30340;&#22797;&#26434;&#24230;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00860</link><description>&lt;p&gt;
&#38646;&#22352;&#26631;&#31227;&#21160;&#65306;&#38024;&#23545;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#20248;&#21270;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero Coordinate Shift: Whetted Automatic Differentiation for Physics-informed Operator Learning. (arXiv:2311.00860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#21160;&#24494;&#20998;&#31639;&#27861;&#65292;&#36890;&#36807;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#65292;&#23558;&#25152;&#38656;&#23548;&#25968;&#30340;&#22797;&#26434;&#24230;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#65288;AD&#65289;&#26159;&#29289;&#29702;&#32422;&#26463;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#29992;&#20110;&#35745;&#31639;&#32593;&#32476;&#36755;&#20986;&#30456;&#23545;&#20110;&#22352;&#26631;&#30340;&#39640;&#38454;&#23548;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36731;&#37327;&#32423;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#38024;&#23545;&#29289;&#29702;&#32422;&#26463;&#25805;&#20316;&#23398;&#20064;&#30340;&#33258;&#21160;&#24494;&#20998;&#65292;&#31216;&#20026;&#38646;&#22352;&#26631;&#31227;&#21160;&#65288;ZCS&#65289;&#30340;&#25216;&#24039;&#12290;ZCS&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#37327;&#20540;&#30340;&#21494;&#21464;&#37327;&#65292;&#29992;&#20110;&#27599;&#20010;&#31354;&#38388;&#25110;&#26102;&#38388;&#32500;&#24230;&#65292;&#36890;&#36807;&#23558;&#25152;&#38656;&#23548;&#25968;&#20174;&#8220;&#22810;&#26681;&#22810;&#21494;&#8221;&#31616;&#21270;&#20026;&#8220;&#19968;&#26681;&#22810;&#21494;&#8221;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#24040;&#22823;&#25552;&#21319;&#12290;ZCS&#24456;&#23481;&#26131;&#22312;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#23454;&#29616;&#65307;&#25105;&#20204;&#20351;&#29992;DeepXDE&#36719;&#20214;&#21253;&#36827;&#34892;&#20102;&#33258;&#24049;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#20998;&#26512;&#21644;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#35757;&#32451;&#29289;&#29702;&#32422;&#26463;&#30340;DeepONets&#26469;&#35299;&#20915;&#26080;&#25968;&#25454;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ZCS&#19968;&#30452;&#36890;&#36807;&#38477;&#20302;GPU&#20869;&#23384;&#28040;&#32791;&#25552;&#20379;&#20102;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic differentiation (AD) is a critical step in physics-informed machine learning, required for computing the high-order derivatives of network output w.r.t. coordinates. In this paper, we present a novel and lightweight algorithm to conduct such AD for physics-informed operator learning, as we call the trick of Zero Coordinate Shift (ZCS). Instead of making all sampled coordinates leaf variables, ZCS introduces only one scalar-valued leaf variable for each spatial or temporal dimension, leading to a game-changing performance leap by simplifying the wanted derivatives from "many-roots-many-leaves" to "one-root-many-leaves". ZCS is easy to implement with current deep learning libraries; our own implementation is by extending the DeepXDE package. We carry out a comprehensive benchmark analysis and several case studies, training physics-informed DeepONets to solve partial differential equations (PDEs) without data. The results show that ZCS has persistently brought down GPU memory co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#20013;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24403;&#27169;&#22411;&#19979;&#22870;&#21169;&#30340;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#32531;&#24930;&#12290;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20703</link><description>&lt;p&gt;
&#24378;&#21270;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Vanishing Gradients in Reinforcement Finetuning of Language Models. (arXiv:2310.20703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#20013;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24403;&#27169;&#22411;&#19979;&#22870;&#21169;&#30340;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#32531;&#24930;&#12290;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#21644;&#19979;&#28216;&#20219;&#21153;&#23545;&#40784;&#65292;&#21363;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#26368;&#22823;&#21270;&#65288;&#21487;&#33021;&#26159;&#23398;&#20064;&#24471;&#21040;&#30340;&#65289;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;RFT&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#30340;&#20248;&#21270;&#38556;&#30861;&#65306;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#27169;&#22411;&#19979;&#30340;&#22870;&#21169;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#21363;&#20351;&#26399;&#26395;&#22870;&#21169;&#36828;&#31163;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#22312;RFT&#22522;&#20934;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#21450;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#20110;&#23567;&#30340;&#22870;&#21169;&#26631;&#20934;&#24046;&#23548;&#33268;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#19988;&#26377;&#23475;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#26497;&#20854;&#32531;&#24930;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20811;&#26381;RFT&#20013;&#26799;&#24230;&#28040;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#26368;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#27861;&#65292;&#24182;&#19988;&#25581;&#31034;&#20102;&#23427;&#22312;RFT&#27969;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#30456;&#23545;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;SFT&#38454;&#27573;&#21487;&#20197;&#26377;&#25928;&#20811;&#26381;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small num
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#33041;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;MEG&#27169;&#22359;&#21644;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#27169;&#22411;&#65292;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#30693;&#35273;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#35299;&#30721;&#65292;&#24182;&#22312;&#22270;&#20687;&#26816;&#32034;&#19978;&#21462;&#24471;&#20102;7&#20493;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.19812</link><description>&lt;p&gt;
&#33041;&#35299;&#30721;&#65306;&#36208;&#21521;&#23454;&#26102;&#37325;&#24314;&#35270;&#35273;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
Brain decoding: toward real-time reconstruction of visual perception. (arXiv:2310.19812v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#33041;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;MEG&#27169;&#22359;&#21644;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#27169;&#22411;&#65292;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#30693;&#35273;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#35299;&#30721;&#65292;&#24182;&#22312;&#22270;&#20687;&#26816;&#32034;&#19978;&#21462;&#24471;&#20102;7&#20493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20116;&#24180;&#20013;&#65292;&#29983;&#25104;&#24335;&#21644;&#22522;&#30784;&#24615;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20351;&#29992;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#23545;&#22823;&#33041;&#27963;&#21160;&#30340;&#35299;&#30721;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#35270;&#35273;&#30693;&#35273;&#65292;&#29616;&#22312;&#21487;&#20197;&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20013;&#35299;&#30721;&#20986;&#20196;&#20154;&#30633;&#30446;&#30340;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31070;&#32463;&#24433;&#20687;&#25216;&#26415;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#26377;&#38480;&#65288;&#32422;&#20026;0.5 Hz&#65289;&#65292;&#22240;&#27492;&#22312;&#23454;&#26102;&#24212;&#29992;&#26041;&#38754;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30913;&#22270;&#65288;MEG&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#20197;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#65288;&#32422;&#20026;5000 Hz&#65289;&#27979;&#37327;&#33041;&#27963;&#21160;&#30340;&#31070;&#32463;&#24433;&#20687;&#35774;&#22791;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;MEG&#35299;&#30721;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#27604;&#21644;&#22238;&#24402;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;i&#65289;&#20174;&#22270;&#20687;&#20013;&#33719;&#24471;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;ii&#65289;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;MEG&#27169;&#22359;&#20197;&#21450;iii&#65289;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;MEG&#35299;&#30721;&#22120;&#22312;&#32463;&#20856;&#32447;&#24615;&#35299;&#30721;&#22120;&#19978;&#26174;&#31034;&#20986;7&#20493;&#30340;&#22270;&#20687;&#26816;&#32034;&#25913;&#36827;&#12290;&#20854;&#27425;&#65292;&#21518;&#26399;&#33041;&#37096;
&lt;/p&gt;
&lt;p&gt;
In the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution ($\approx$0.5 Hz) and thus fundamentally constrains its real-time usage. Here, we propose an alternative approach based on magnetoencephalography (MEG), a neuroimaging device capable of measuring brain activity with high temporal resolution ($\approx$5,000 Hz). For this, we develop an MEG decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG module trained end-to-end and iii) a pretrained image generator. Our results are threefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18235</link><description>&lt;p&gt;
Davidsonian&#22330;&#26223;&#22270;&#65306;&#25913;&#36827;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#24544;&#23454;&#24230;&#30340;&#24378;&#22823;&#26041;&#27861;&#26159;&#22522;&#20110;QG/A&#65288;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#65289;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#31572;&#26696;&#19982;&#22522;&#20110;&#25552;&#31034;&#30340;&#31572;&#26696;&#22312;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#19968;&#33268;&#24615;&#23545;&#36755;&#20986;&#22270;&#20687;&#36827;&#34892;&#35780;&#20998;&#12290;&#36825;&#31181;&#35780;&#20272;&#33258;&#28982;&#19978;&#21462;&#20915;&#20110;&#24213;&#23618;QG&#21644;QA&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QG/A&#24037;&#20316;&#20013;&#30340;&#20960;&#20010;&#21487;&#38752;&#24615;&#25361;&#25112;&#65306;&#65288;a&#65289;QG&#38382;&#39064;&#24212;&#23562;&#37325;&#25552;&#31034;&#65288;&#36991;&#20813;&#24187;&#35273;&#12289;&#37325;&#22797;&#21644;&#36951;&#28431;&#65289;&#21644;&#65288;b&#65289;VQA&#31572;&#26696;&#24212;&#19968;&#33268;&#65288;&#19981;&#20250;&#22312;&#22270;&#20687;&#20013;&#23459;&#31216;&#27809;&#26377;&#25705;&#25176;&#36710;&#65292;&#21516;&#26102;&#22768;&#31216;&#25705;&#25176;&#36710;&#26159;&#34013;&#33394;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#65292;&#36825;&#20010;&#21463;&#24418;&#24335;&#35821;&#20041;&#21551;&#21457;&#30340;&#23454;&#35777;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#21019;&#20316;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#29983;&#25104;&#35774;&#35745;&#31354;&#38388;&#24182;&#25552;&#20379;&#26080;&#32541;&#25506;&#32034;&#12289;&#35780;&#20272;&#21644;&#32508;&#21512;&#22810;&#31181;&#21709;&#24212;&#30340;&#21151;&#33021;&#65292;&#25299;&#23637;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#20219;&#21153;&#20013;&#30340;&#20132;&#20114;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.12953</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#21019;&#20316;&#30340;&#35774;&#35745;&#31354;&#38388;&#32467;&#26500;&#21270;&#29983;&#25104;&#19982;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation. (arXiv:2310.12953v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12953
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#21019;&#20316;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#29983;&#25104;&#35774;&#35745;&#31354;&#38388;&#24182;&#25552;&#20379;&#26080;&#32541;&#25506;&#32034;&#12289;&#35780;&#20272;&#21644;&#32508;&#21512;&#22810;&#31181;&#21709;&#24212;&#30340;&#21151;&#33021;&#65292;&#25299;&#23637;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#20219;&#21153;&#20013;&#30340;&#20132;&#20114;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20973;&#20511;&#20854;&#29983;&#25104;&#33021;&#21147;&#65292;&#24050;&#25104;&#20026;&#21019;&#24847;&#36807;&#31243;&#20013;&#30340;&#26080;&#20215;&#24037;&#20855;&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#25968;&#30334;&#29978;&#33267;&#25968;&#21315;&#20010;&#35270;&#35273;&#21644;&#25991;&#26412;&#36755;&#20986;&#65292;&#20026;&#21019;&#24847;&#21162;&#21147;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#28789;&#24863;&#12290;&#20294;&#25105;&#20204;&#26159;&#21542;&#20805;&#20998;&#21457;&#25381;&#20102;&#23427;&#20204;&#30340;&#28508;&#21147;&#65311;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#30340;&#20132;&#20114;&#33539;&#24335;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#65292;&#23558;&#29992;&#25143;&#24341;&#23548;&#21040;&#26377;&#38480;&#30340;&#19968;&#32452;&#24819;&#27861;&#19978;&#65292;&#32780;&#19981;&#26159;&#36171;&#20104;&#20182;&#20204;&#25506;&#32034;&#29983;&#25104;&#27169;&#22411;&#20013;&#24191;&#38420;&#30340;&#28508;&#22312;&#35774;&#35745;&#31354;&#38388;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20419;&#36827;&#20102;&#35774;&#35745;&#31354;&#38388;&#30340;&#32467;&#26500;&#21270;&#29983;&#25104;&#65292;&#29992;&#25143;&#21487;&#20197;&#26080;&#32541;&#22320;&#25506;&#32034;&#12289;&#35780;&#20272;&#21644;&#32508;&#21512;&#22810;&#31181;&#21709;&#24212;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#24320;&#21457;&#19968;&#20010;&#20132;&#20114;&#31995;&#32479;Luminate&#65292;&#24182;&#19982;8&#21517;&#19987;&#19994;&#20316;&#23478;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25512;&#36827;&#20102;&#25105;&#20204;&#19982;LLM&#22312;&#21019;&#24847;&#20219;&#21153;&#20013;&#30340;&#20132;&#20114;&#26041;&#24335;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#25506;&#32034;&#20854;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thanks to their generative capabilities, large language models (LLMs) have become an invaluable tool for creative processes. These models have the capacity to produce hundreds and thousands of visual and textual outputs, offering abundant inspiration for creative endeavors. But are we harnessing their full potential? We argue that current interaction paradigms fall short, guiding users towards rapid convergence on a limited set of ideas, rather than empowering them to explore the vast latent design space in generative models. To address this limitation, we propose a framework that facilitates the structured generation of design space in which users can seamlessly explore, evaluate, and synthesize a multitude of responses. We demonstrate the feasibility and usefulness of this framework through the design and development of an interactive system, Luminate, and a user study with 8 professional writers. Our work advances how we interact with LLMs for creative tasks, introducing a way to ha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#23454;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#22312;&#27599;&#20010;&#26032;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.11482</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking Class-incremental Learning in the Era of Large Pre-trained Models via Test-Time Adaptation. (arXiv:2310.11482v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#23454;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#22312;&#27599;&#20010;&#26032;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#25345;&#32493;&#23398;&#20064;&#23558;&#31867;&#21035;&#21010;&#20998;&#21040;&#26032;&#20219;&#21153;&#20013;&#65292;&#21516;&#26102;&#19981;&#20250;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#21152;&#24555;&#20102;&#22686;&#37327;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#22240;&#20026;&#39640;&#24230;&#21487;&#20256;&#36755;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#34920;&#31034;&#20351;&#24471;&#22312;&#35843;&#25972;&#19968;&#23567;&#32452;&#21442;&#25968;&#26102;&#65292;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#20256;&#32479;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#27599;&#20010;&#20219;&#21153;&#36827;&#34892;&#21453;&#22797;&#24494;&#35843;&#20250;&#30772;&#22351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#24182;&#23548;&#33268;&#36951;&#24536;&#20043;&#21069;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#30452;&#25509;&#22312;&#27979;&#35797;&#23454;&#20363;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#8221;&#65288;TTACIL&#65289;&#65292;&#23427;&#39318;&#20808;&#22312;&#27599;&#20010;&#27979;&#35797;&#23454;&#20363;&#19978;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23618;&#24402;&#19968;&#21270;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning (CIL) is a challenging task that involves continually learning to categorize classes into new tasks without forgetting previously learned information. The advent of the large pre-trained models (PTMs) has fast-tracked the progress in CIL due to the highly transferable PTM representations, where tuning a small set of parameters results in state-of-the-art performance when compared with the traditional CIL methods that are trained from scratch. However, repeated fine-tuning on each task destroys the rich representations of the PTMs and further leads to forgetting previous tasks. To strike a balance between the stability and plasticity of PTMs for CIL, we propose a novel perspective of eliminating training on every new task and instead performing test-time adaptation (TTA) directly on the test instances. Concretely, we propose "Test-Time Adaptation for Class-Incremental Learning" (TTACIL) that first fine-tunes Layer Norm parameters of the PTM on each test instan
&lt;/p&gt;</description></item><item><title>CodeChain&#26159;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22797;&#26434;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08992</link><description>&lt;p&gt;
CodeChain: &#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#23454;&#29616;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules. (arXiv:2310.08992v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08992
&lt;/p&gt;
&lt;p&gt;
CodeChain&#26159;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22797;&#26434;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#35299;&#20915;&#31616;&#21333;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#29087;&#32451;&#65292;&#27604;&#22914;&#22312;HumanEval&#25110;MBPP&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#21644;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#32534;&#31243;&#20219;&#21153;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#29983;&#25104;&#20316;&#20026;&#25972;&#20307;&#20195;&#30721;&#22359;&#32780;&#19981;&#26159;&#23558;&#20854;&#20998;&#35299;&#20026;&#36923;&#36753;&#23376;&#20219;&#21153;&#21644;&#23376;&#27169;&#22359;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26377;&#32463;&#39564;&#30340;&#31243;&#24207;&#21592;&#26412;&#33021;&#22320;&#32534;&#20889;&#20855;&#26377;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22359;&#21270;&#20195;&#30721;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#36890;&#24120;&#20250;&#37325;&#22797;&#20351;&#29992;&#20043;&#21069;&#24320;&#21457;&#30340;&#27169;&#22359;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CodeChain&#65292;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CodeChain&#39318;&#20808;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25552;&#31034;&#25351;&#23548;LLM&#29983;&#25104;&#27169;&#22359;&#21270;&#20195;&#30721;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#20004;&#20010;&#27493;&#39588;&#23454;&#26045;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#65306;1&#65289;&#39069;&#22806;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: 1) extra
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#23545;AI&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#35299;&#37322;&#26377;&#21161;&#20110;&#20154;&#20204;&#26816;&#27979;&#30452;&#25509;&#20559;&#35265;&#65292;&#20294;&#23545;&#38388;&#25509;&#20559;&#35265;&#30340;&#21457;&#29616;&#33021;&#21147;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.08617</link><description>&lt;p&gt;
AI&#20915;&#31574;&#20013;&#35299;&#37322;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65306;&#21463;&#20445;&#25252;&#29305;&#24449; vs &#20195;&#29702;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features. (arXiv:2310.08617v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08617
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#23545;AI&#20915;&#31574;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#35299;&#37322;&#26377;&#21161;&#20110;&#20154;&#20204;&#26816;&#27979;&#30452;&#25509;&#20559;&#35265;&#65292;&#20294;&#23545;&#38388;&#25509;&#20559;&#35265;&#30340;&#21457;&#29616;&#33021;&#21147;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;AI&#31995;&#32479;&#21487;&#33021;&#20250;&#25918;&#22823;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#12290;&#35299;&#37322;&#21487;&#33021;&#26377;&#21161;&#20110;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#36890;&#24120;&#65292;&#35299;&#37322;&#20391;&#37325;&#20110;&#31361;&#20986;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#22914;&#26524;&#27169;&#22411;&#23545;&#26576;&#20010;&#21463;&#20445;&#25252;&#30340;&#32676;&#20307;&#23384;&#22312;&#20559;&#35265;&#65292;&#21017;&#35299;&#37322;&#21487;&#33021;&#21253;&#25324;&#26174;&#31034;&#36825;&#31181;&#20559;&#35265;&#30340;&#29305;&#24449;&#65292;&#20294;&#24403;&#20559;&#35265;&#36890;&#36807;&#20195;&#29702;&#29305;&#24449;&#23454;&#29616;&#26102;&#65292;&#36825;&#20010;&#20195;&#29702;&#29305;&#24449;&#19982;&#21463;&#20445;&#25252;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#33021;&#23545;&#20154;&#31867;&#32780;&#35328;&#19981;&#22826;&#28165;&#26224;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#21463;&#20445;&#25252;&#29305;&#24449;&#21644;&#20195;&#29702;&#29305;&#24449;&#23545;&#21442;&#19982;&#32773;&#23545;&#27169;&#22411;&#20844;&#24179;&#24615;&#21644;&#25552;&#39640;&#20154;&#21475;&#24179;&#34913;&#33021;&#21147;&#30340;&#24863;&#30693;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#22788;&#29702;&#26041;&#24335;&#65288;&#35299;&#37322;&#12289;&#27169;&#22411;&#20559;&#35265;&#25259;&#38706;&#21644;&#20195;&#29702;&#30456;&#20851;&#24615;&#25259;&#38706;&#65289;&#23545;&#20844;&#24179;&#24863;&#30693;&#21644;&#24179;&#31561;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35299;&#37322;&#26377;&#21161;&#20110;&#20154;&#20204;&#26816;&#27979;&#30452;&#25509;&#20559;&#35265;&#65292;&#20294;&#26080;&#27861;&#21457;&#29616;&#38388;&#25509;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#20559;&#35265;&#31867;&#22411;&#22914;&#20309;&#65292;&#35299;&#37322;&#20542;&#21521;&#20110;&#22686;&#21152;&#23545;&#27169;&#22411;&#20559;&#35265;&#30340;&#35748;&#21516;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI systems have been known to amplify biases in real world data. Explanations may help human-AI teams address these biases for fairer decision-making. Typically, explanations focus on salient input features. If a model is biased against some protected group, explanations may include features that demonstrate this bias, but when biases are realized through proxy features, the relationship between this proxy feature and the protected one may be less clear to a human. In this work, we study the effect of the presence of protected and proxy features on participants' perception of model fairness and their ability to improve demographic parity over an AI alone. Further, we examine how different treatments -- explanations, model bias disclosure and proxy correlation disclosure -- affect fairness perception and parity. We find that explanations help people detect direct biases but not indirect biases. Additionally, regardless of bias type, explanations tend to increase agreement with model bia
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#22312;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#33258;&#25105;&#32416;&#27491;&#21518;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.01798</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#19981;&#33021;&#33258;&#25105;&#32416;&#27491;&#25512;&#29702;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Cannot Self-Correct Reasoning Yet. (arXiv:2310.01798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01798
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#22312;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#33258;&#25105;&#32416;&#27491;&#21518;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20973;&#20511;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26080;&#21487;&#27604;&#25311;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#32780;&#25104;&#20026;&#31361;&#30772;&#24615;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20854;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24403;&#24615;&#20173;&#23384;&#22312;&#30097;&#34385;&#12290;&#33258;&#25105;&#32416;&#27491;&#26041;&#27861;&#34987;&#25552;&#20986;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#27492;&#22522;&#30784;&#19978;&#23545;LLMs&#20869;&#37096;&#30340;&#33258;&#25105;&#32416;&#27491;&#30340;&#20316;&#29992;&#21644;&#25928;&#26524;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#32771;&#23519;&#65292;&#25581;&#31034;&#20102;&#20854;&#30495;&#27491;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20869;&#22312;&#33258;&#25105;&#32416;&#27491;&#30340;&#27010;&#24565;&#65292;&#21363;LLMs&#23581;&#35797;&#20165;&#20165;&#20381;&#38752;&#20854;&#22266;&#26377;&#33021;&#21147;&#26469;&#32416;&#27491;&#20854;&#21021;&#22987;&#21709;&#24212;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#22806;&#37096;&#21453;&#39304;&#30340;&#25903;&#25345;&#12290;&#22312;&#25512;&#29702;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;LLMs&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#33258;&#25105;&#32416;&#27491;&#20854;&#21709;&#24212;&#65292;&#29978;&#33267;&#26377;&#26102;&#20505;&#20854;&#34920;&#29616;&#21487;&#33021;&#22312;&#33258;&#25105;&#32416;&#27491;&#21518;&#19979;&#38477;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future resear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GPT-4&#22522;&#20110;&#30340;GNAS&#26041;&#27861;&#65288;GPT4GNAS&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;GPT-4&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01436</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Architecture Search with GPT-4. (arXiv:2310.01436v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GPT-4&#22522;&#20110;&#30340;GNAS&#26041;&#27861;&#65288;GPT4GNAS&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;GPT-4&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;GNAS&#65289;&#22312;&#33258;&#21160;&#35774;&#35745;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;GNAS&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#21171;&#21160;&#21644;&#20016;&#23500;&#30340;&#39046;&#22495;&#30693;&#35782;&#26469;&#35774;&#35745;&#25628;&#32034;&#31354;&#38388;&#21644;&#25628;&#32034;&#31574;&#30053;&#12290;&#26412;&#25991;&#23558;GPT-4&#38598;&#25104;&#21040;GNAS&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-4&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65288;&#31616;&#31216;&#20026;GPT4GNAS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20026;GPT-4&#35774;&#35745;&#19968;&#31867;&#26032;&#30340;&#25552;&#31034;&#65292;&#20197;&#25351;&#23548;GPT-4&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#36825;&#20123;&#25552;&#31034;&#21253;&#25324;GNAS&#30340;&#25628;&#32034;&#31354;&#38388;&#12289;&#25628;&#32034;&#31574;&#30053;&#21644;&#25628;&#32034;&#21453;&#39304;&#30340;&#25551;&#36848;&#12290;&#36890;&#36807;&#36845;&#20195;&#22320;&#36816;&#34892;&#20855;&#26377;&#25552;&#31034;&#30340;GPT-4&#65292;GPT4GNAS&#33021;&#22815;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24555;&#36895;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;GPT-4&#21040;GNAS&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;GNAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Architecture Search (GNAS) has shown promising results in automatically designing graph neural networks. However, GNAS still requires intensive human labor with rich domain knowledge to design the search space and search strategy. In this paper, we integrate GPT-4 into GNAS and propose a new GPT-4 based Graph Neural Architecture Search method (GPT4GNAS for short). The basic idea of our method is to design a new class of prompts for GPT-4 to guide GPT-4 toward the generative task of graph neural architectures. The prompts consist of descriptions of the search space, search strategy, and search feedback of GNAS. By iteratively running GPT-4 with the prompts, GPT4GNAS generates more accurate graph neural networks with fast convergence. Experimental results show that embedding GPT-4 into GNAS outperforms the state-of-the-art GNAS methods.
&lt;/p&gt;</description></item><item><title>DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.17167</link><description>&lt;p&gt;
DyVal: &#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17167
&lt;/p&gt;
&lt;p&gt;
DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35780;&#20272;&#22522;&#20934;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#20986;&#20102;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20204;&#24222;&#22823;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#22522;&#20934;&#30340;&#38745;&#24577;&#24615;&#36136;&#21644;&#22266;&#23450;&#22797;&#26434;&#24615;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#34913;&#37327;LLM&#30340;&#36827;&#27493;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DyVal&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#28789;&#27963;&#30340;&#21160;&#24577;&#35780;&#20272;LLM&#30340;&#21327;&#35758;&#12290;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#21160;&#24577;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21033;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#32467;&#26500;&#20248;&#21183;&#26500;&#24314;&#20102;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;DyVal&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#12290;DyVal&#29983;&#25104;&#20102;&#21253;&#25324;&#25968;&#23398;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#31639;&#27861;&#38382;&#39064;&#22312;&#20869;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#35780;&#20272;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20174;Flan-T5-large&#21040;ChatGPT&#21644;GPT4&#30340;&#21508;&#31181;LLM&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#22312;DyVal&#29983;&#25104;&#30340;&#35780;&#20272;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14496</link><description>&lt;p&gt;
Era Splitting.&#65288;arXiv:2309.14496v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Era Splitting. (arXiv:2309.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20250;&#21576;&#29616;&#20986;&#25968;&#25454;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#34892;&#20026;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#33539;&#24335;&#30340;&#33539;&#22260;&#65292;&#35813;&#33539;&#24335;&#20551;&#35774;&#25968;&#25454;&#22312;&#26102;&#38388;&#21644;&#22320;&#28857;&#19978;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#26032;&#20852;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#39046;&#22495;&#36890;&#36807;&#23558;&#29615;&#22659;&#25110;&#26102;&#20195;&#20449;&#24687;&#34701;&#20837;&#31639;&#27861;&#20013;&#65292;&#26469;&#24212;&#23545;&#36825;&#20010;&#29616;&#23454;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;/&#25110;&#31070;&#32463;&#32593;&#32476;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#19982;&#27599;&#20010;&#25968;&#25454;&#28857;&#30456;&#20851;&#30340;&#26102;&#20195;&#20449;&#24687;&#65292;&#26469;&#25214;&#21040;&#22312;&#25968;&#25454;&#30340;&#25152;&#26377;&#19981;&#30456;&#20132;&#26102;&#20195;&#20013;&#37117;&#26159;&#26368;&#20248;&#30340;&#20999;&#20998;&#28857;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10639</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#20840;&#23616;${\mathcal L}^2$&#26368;&#23567;&#21270;&#22120;&#30340;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#24182;&#19988;&#26500;&#24314;&#20102;&#20840;&#23616;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#21508;&#31181;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#32593;&#32476;&#32467;&#26500;&#30340;&#20960;&#20309;&#35299;&#37322;&#65292;&#35813;&#32593;&#32476;&#20855;&#26377;$L$&#20010;&#38544;&#34255;&#23618;&#65292;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#65292;${\mathcal L}^2$ Schatten&#31867;&#65288;&#25110;Hilbert-Schmidt&#65289;&#25104;&#26412;&#20989;&#25968;&#65292;&#20197;&#21450;&#30456;&#31561;&#32500;&#24230;$Q\geq1$&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#12290;&#38544;&#34255;&#23618;&#20063;&#23450;&#20041;&#22312;${\mathbb R}^{Q}$&#30340;&#31354;&#38388;&#19978;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#26368;&#26032;&#30340;&#20851;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26524;&#65292;&#22312;$L\geq Q$&#30340;&#24773;&#20917;&#19979;&#26500;&#36896;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#26368;&#23567;&#21270;&#22120;&#26063;&#65292;&#35813;&#26063;&#33021;&#22815;&#20840;&#23616;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#26063;&#26159;&#36864;&#21270;&#30340;&#12290;&#22312;&#36825;&#37324;&#25552;&#21040;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;DL&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#36890;&#36807;&#23545;&#35757;&#32451;&#36755;&#20837;&#30340;&#36882;&#24402;&#25130;&#26029;&#26144;&#23556;&#30340;&#24212;&#29992;&#26469;&#8220;&#25972;&#29702;&#8221;&#35757;&#32451;&#36755;&#20837;&#65292;&#20197;&#26368;&#23567;&#21270;&#22122;&#22768;&#19982;&#20449;&#21495;&#30340;&#27604;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;$2^Q-1$&#20010;&#19981;&#21516;&#30340;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.10792</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#25351;&#20197;&#30417;&#30563;&#26041;&#24335;&#22312;&#21253;&#21547;&#8220;&#25351;&#20196;-&#36755;&#20986;&#8221;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;LLM&#65292;&#36825;&#23558;LLM&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#30446;&#26631;&#19982;&#29992;&#25143;&#24076;&#26395;LLM&#36981;&#23432;&#20154;&#31867;&#25351;&#20196;&#30340;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;IT&#30340;&#24120;&#35268;&#26041;&#27861;&#12289;IT&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#12289;IT&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#24212;&#29992;&#20110;&#19981;&#21516;&#27169;&#24577;&#12289;&#39046;&#22495;&#21644;&#24212;&#29992;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23545;&#24433;&#21709;IT&#32467;&#26524;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20998;&#26512;&#65288;&#20363;&#22914;&#65292;&#25351;&#20196;&#36755;&#20986;&#30340;&#29983;&#25104;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;IT&#30340;&#28508;&#22312;&#38382;&#39064;&#20197;&#21450;&#38024;&#23545;&#20854;&#30340;&#25209;&#35780;&#65292;&#20197;&#21450;&#25351;&#20986;&#24403;&#21069;&#19981;&#36275;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
&lt;/p&gt;</description></item><item><title>milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17010</link><description>&lt;p&gt;
milliFlow&#65306;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17010
&lt;/p&gt;
&lt;p&gt;
milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26222;&#36866;&#35745;&#31639;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#22312;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29992;&#20110;&#20915;&#31574;&#12289;&#29992;&#25143;&#20132;&#20114;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#20154;&#20307;&#36319;&#36394;&#12289;&#23039;&#21183;&#20272;&#35745;&#12289;&#25163;&#21183;&#35782;&#21035;&#21644;&#27963;&#21160;&#35782;&#21035;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#25668;&#20687;&#26426;&#12290;&#28982;&#32780;&#65292;&#25668;&#20687;&#26426;&#30340;&#20405;&#20837;&#24615;&#29305;&#28857;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26234;&#33021;&#23478;&#23621;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27627;&#31859;&#27874;&#38647;&#36798;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#28857;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;milliFlow&#65292;&#29992;&#20110;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#20316;&#20026;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#65292;&#30452;&#25509;&#21463;&#30410;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;3D&#31471;&#28857;&#35823;&#24046;&#20026;4.6cm&#65292;&#26126;&#26174;&#36229;&#36807;&#31454;&#20105;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
&lt;/p&gt;</description></item><item><title>MO-VLN&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#36890;&#29992;&#26426;&#22120;&#20154;&#22312;&#22810;&#20219;&#21153;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;5&#24320;&#21457;&#36924;&#30495;&#30340;&#22330;&#26223;&#21644;&#21253;&#21547;&#22810;&#31181;&#19981;&#24120;&#35265;&#29289;&#20307;&#26469;&#27979;&#35797;&#20854;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.10322</link><description>&lt;p&gt;
MO-VLN:&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#38598;&#21512;&#38646;&#26679;&#26412;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934; (arXiv:2306.10322v2 [cs.CV] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
MO-VLN: A Multi-Task Benchmark for Open-set Zero-Shot Vision-and-Language Navigation. (arXiv:2306.10322v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10322
&lt;/p&gt;
&lt;p&gt;
MO-VLN&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#36890;&#29992;&#26426;&#22120;&#20154;&#22312;&#22810;&#20219;&#21153;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;5&#24320;&#21457;&#36924;&#30495;&#30340;&#22330;&#26223;&#21644;&#21253;&#21547;&#22810;&#31181;&#19981;&#24120;&#35265;&#29289;&#20307;&#26469;&#27979;&#35797;&#20854;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#29702;&#35299;&#25351;&#20196;&#24182;&#26681;&#25454;&#35270;&#35273;&#35266;&#23519;&#25214;&#21040;&#30446;&#26631;&#23545;&#35937;&#25110;&#20301;&#32622;&#65292;&#21363;&#20351;&#22312;&#26410;&#25506;&#32034;&#30340;&#29615;&#22659;&#20013;&#20063;&#33021;&#20570;&#21040;&#12290;&#22823;&#22810;&#25968;&#20195;&#29702;&#20381;&#36182;&#20110;&#22823;&#37327;&#22810;&#26679;&#30340;&#35757;&#32451;&#25968;&#25454;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#65292;&#36825;&#38656;&#35201;&#26114;&#36149;&#30340;&#21171;&#21160;&#21147;&#12290;&#36825;&#20123;&#20195;&#29702;&#36890;&#24120;&#21482;&#20851;&#27880;&#24120;&#35265;&#30340;&#23545;&#35937;&#21644;&#36739;&#23569;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#19981;&#36275;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25351;&#20196;&#12290;&#20026;&#20102;&#20419;&#36827;&#24320;&#25918;&#38598;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MO-VLN&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#27979;&#35797;&#20195;&#29702;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;5&#24320;&#21457;&#20102;&#19968;&#20010;3D&#27169;&#25311;&#22120;&#65292;&#28210;&#26579;&#20102;&#36924;&#30495;&#30340;&#22330;&#26223;&#65292;&#21253;&#21547;&#26356;&#30495;&#23454;&#30340;&#20809;&#29031;&#21644;&#32454;&#33410;&#12290;&#27169;&#25311;&#22120;&#21253;&#21547;&#19977;&#20010;&#22330;&#26223;&#65292;&#21363;&#21654;&#21857;&#39302;&#12289;&#39184;&#21381;&#21644;&#20859;&#32769;&#38498;&#65292;&#36825;&#20123;&#22330;&#26223;&#22312;&#24037;&#19994;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#25311;&#22120;&#28041;&#21450;&#22810;&#31181;&#19981;&#24120;&#35265;&#30340;&#29289;&#20307;&#65292;&#22914;&#22806;&#21334;&#26479;&#21644;&#21307;&#29992;&#33014;&#24102;&#65292;&#36825;&#20123;&#29289;&#20307;&#26356;&#21152;&#22797;&#26434;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a natural language, a general robot has to comprehend the instruction and find the target object or location based on visual observations even in unexplored environments. Most agents rely on massive diverse training data to achieve better generalization, which requires expensive labor. These agents often focus on common objects and fewer tasks, thus are not intelligent enough to handle different types of instructions. To facilitate research in open-set vision-and-language navigation, we propose a benchmark named MO-VLN, aiming at testing the effectiveness and generalization of the agent in the multi-task setting. First, we develop a 3D simulator rendered by realistic scenarios using Unreal Engine 5, containing more realistic lights and details. The simulator contains three scenes, i.e., cafe, restaurant, and nursing house, of high value in the industry. Besides, our simulator involves multiple uncommon objects, such as takeaway cup and medical adhesive tape, which are more compli
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\pi$-KRVI&#30340;&#20048;&#35266;&#20462;&#25913;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2306.07745</link><description>&lt;p&gt;
&#26680;&#21270;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Kernelized Reinforcement Learning with Order Optimal Regret Bounds. (arXiv:2306.07745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\pi$-KRVI&#30340;&#20048;&#35266;&#20462;&#25913;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#20855;&#26377;&#22797;&#26434;&#27169;&#22411;&#21644;&#22823;&#29366;&#24577;-&#34892;&#20026;&#31354;&#38388;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#26174;&#31034;&#20986;&#20102;&#23454;&#35777;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#32467;&#26524;&#36890;&#24120;&#38598;&#20013;&#20110;&#20855;&#26377;&#23569;&#37327;&#29366;&#24577;-&#34892;&#20026;&#25110;&#31616;&#21333;&#27169;&#22411;&#65288;&#20363;&#22914;&#32447;&#24615;&#24314;&#27169;&#29366;&#24577;-&#34892;&#20026;&#20540;&#20989;&#25968;&#65289;&#30340;&#35774;&#32622;&#12290; &#20026;&#20102;&#25512;&#23548;&#26377;&#25928;&#22788;&#29702;&#26356;&#24191;&#27867;&#20540;&#20989;&#25968;&#30340;&#22823;&#29366;&#24577;-&#34892;&#20026;&#31354;&#38388;&#30340;RL&#31574;&#30053;&#65292;&#19968;&#20123;&#26368;&#26032;&#24037;&#20316;&#32771;&#34385;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#31216;&#20026;$\pi$-KRVI&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#30340;&#19968;&#31181;&#20048;&#35266;&#20462;&#25913;&#65292;&#24403;&#29366;&#24577;-&#34892;&#20026;&#20540;&#20989;&#25968;&#30001;RKHS&#34920;&#31034;&#26102;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#39640;&#24230;&#38750;&#20809;&#28369;&#20869;&#26680;&#65288;&#20363;&#22914;&#31070;&#32463;&#20999;&#21521;&#20869;&#26680;&#25110;&#26576;&#20123;Mat\'ern&#20869;&#26680;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#65292;&#23384;&#22312;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown empirical success in various real world settings with complex models and large state-action spaces. The existing analytical results, however, typically focus on settings with a small number of state-actions or simple models such as linearly modeled state-action value functions. To derive RL policies that efficiently handle large state-action spaces with more general value functions, some recent works have considered nonlinear function approximation using kernel ridge regression. We propose $\pi$-KRVI, an optimistic modification of least-squares value iteration, when the state-action value function is represented by an RKHS. We prove the first order-optimal regret guarantees under a general setting. Our results show a significant polynomial in the number of episodes improvement over the state of the art. In particular, with highly non-smooth kernels (such as Neural Tangent kernel or some Mat\'ern kernels) the existing results lead to trivial (superl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27491;&#20132;&#24494;&#35843;&#65288;OFT&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24341;&#23548;&#21644;&#25511;&#21046;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32422;&#26463;&#27491;&#20132;&#24494;&#35843;&#65288;COFT&#65289;&#65292;&#26469;&#25552;&#39640;&#24494;&#35843;&#30340;&#31283;&#23450;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#35821;&#20041;&#29983;&#25104;&#33021;&#21147;&#24182;&#29983;&#25104;&#29305;&#23450;&#20027;&#39064;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.07280</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#20132;&#24494;&#35843;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Controlling Text-to-Image Diffusion by Orthogonal Finetuning. (arXiv:2306.07280v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27491;&#20132;&#24494;&#35843;&#65288;OFT&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24341;&#23548;&#21644;&#25511;&#21046;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25191;&#34892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32422;&#26463;&#27491;&#20132;&#24494;&#35843;&#65288;COFT&#65289;&#65292;&#26469;&#25552;&#39640;&#24494;&#35843;&#30340;&#31283;&#23450;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#35821;&#20041;&#29983;&#25104;&#33021;&#21147;&#24182;&#29983;&#25104;&#29305;&#23450;&#20027;&#39064;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#30495;&#23454;&#24863;&#22270;&#20687;&#26041;&#38754;&#26377;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#24341;&#23548;&#25110;&#25511;&#21046;&#36825;&#20123;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#25191;&#34892;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#24494;&#35843;&#26041;&#27861;&#8212;&#8212;&#27491;&#20132;&#24494;&#35843;&#65288;OFT&#65289;&#65292;&#29992;&#20110;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#35843;&#25972;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;OFT&#21487;&#20197;&#35777;&#26126;&#22320;&#20445;&#25345;&#29305;&#24449;&#23545;&#31070;&#32463;&#20803;&#22312;&#21333;&#20301;&#36229;&#29699;&#38754;&#19978;&#30340;&#20851;&#31995;&#25152;&#34920;&#24449;&#30340;&#36229;&#29699;&#24418;&#33021;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#23646;&#24615;&#23545;&#20110;&#20445;&#25345;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#29983;&#25104;&#33021;&#21147;&#38750;&#24120;&#20851;&#38190;&#12290;&#20026;&#20102;&#25552;&#39640;&#24494;&#35843;&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#32422;&#26463;&#27491;&#20132;&#24494;&#35843;&#65288;COFT&#65289;&#65292;&#23427;&#23545;&#36229;&#29699;&#38754;&#26045;&#21152;&#20102;&#39069;&#22806;&#30340;&#21322;&#24452;&#32422;&#26463;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#20219;&#21153;&#65306;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#29305;&#23450;&#20027;&#39064;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Large text-to-image diffusion models have impressive capabilities in generating photorealistic images from text prompts. How to effectively guide or control these powerful models to perform different downstream tasks becomes an important open problem. To tackle this challenge, we introduce a principled finetuning method -- Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to downstream tasks. Unlike existing methods, OFT can provably preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. We find that this property is crucial for preserving the semantic generation ability of text-to-image diffusion models. To improve finetuning stability, we further propose Constrained Orthogonal Finetuning (COFT) which imposes an additional radius constraint to the hypersphere. Specifically, we consider two important finetuning text-to-image tasks: subject-driven generation where the goal is to generate subject-specific images
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#8212;&#8212;&#33268;&#25964;&#31070;&#35805;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;(TOTAIC)&#65292;&#35813;&#31454;&#36187;&#22522;&#20110;&#12298;&#19978;&#21476;&#21367;&#36724;&#22312;&#32447;&#12299;&#20013;&#30340;&#19968;&#27454;&#21345;&#29260;&#28216;&#25103;&#12290;&#38500;&#20102;&#24212;&#23545;&#36890;&#24120;&#19982;CCG&#30456;&#20851;&#30340;&#25361;&#25112;&#22806;&#65292;&#35813;&#25361;&#25112;&#36824;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#21644;&#36866;&#24212;&#24615;&#12290;&#31454;&#36187;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#35299;&#20915;&#28216;&#25103;&#65292;&#22914;&#23545;&#25239;&#25628;&#32034;&#12289;&#21333;&#20154;&#35745;&#21010;&#21644;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#12290;&#31532;&#19968;&#23626;TOTAIC&#23558;&#22312;2023&#24180;&#30340;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#20030;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.08234</link><description>&lt;p&gt;
&#24341;&#20837;&#33268;&#25964;&#31070;&#35805;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
Introducing Tales of Tribute AI Competition. (arXiv:2305.08234v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#8212;&#8212;&#33268;&#25964;&#31070;&#35805;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;(TOTAIC)&#65292;&#35813;&#31454;&#36187;&#22522;&#20110;&#12298;&#19978;&#21476;&#21367;&#36724;&#22312;&#32447;&#12299;&#20013;&#30340;&#19968;&#27454;&#21345;&#29260;&#28216;&#25103;&#12290;&#38500;&#20102;&#24212;&#23545;&#36890;&#24120;&#19982;CCG&#30456;&#20851;&#30340;&#25361;&#25112;&#22806;&#65292;&#35813;&#25361;&#25112;&#36824;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#21644;&#36866;&#24212;&#24615;&#12290;&#31454;&#36187;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#35299;&#20915;&#28216;&#25103;&#65292;&#22914;&#23545;&#25239;&#25628;&#32034;&#12289;&#21333;&#20154;&#35745;&#21010;&#21644;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#12290;&#31532;&#19968;&#23626;TOTAIC&#23558;&#22312;2023&#24180;&#30340;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#20030;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#65292;&#21363;&#33268;&#25964;&#31070;&#35805;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;(TOTAIC)&#65292;&#35813;&#31454;&#36187;&#22522;&#20110;&#12298;&#19978;&#21476;&#21367;&#36724;&#22312;&#32447;&#12299;&#28023;&#20043;&#23707;&#31456;&#33410;&#21457;&#24067;&#30340;&#19968;&#27454;&#21452;&#20154;&#21345;&#29260;&#24314;&#35774;&#31867;&#28216;&#25103;&#12290;&#30446;&#21069;&#36824;&#27809;&#26377;&#20854;&#20182;&#28085;&#30422;&#25910;&#38598;&#21345;&#29260;&#28216;&#25103;(CCG)&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#20063;&#20174;&#26410;&#26377;&#36807;&#38024;&#23545;&#24314;&#35774;&#21345;&#29260;&#28216;&#25103;&#30340;&#31454;&#36187;&#12290;&#22240;&#27492;&#65292;&#25104;&#21151;&#30340;&#26041;&#27861;&#38500;&#20102;&#35201;&#20811;&#26381;&#36890;&#24120;&#19982;CCG&#30456;&#20851;&#30340;&#38556;&#30861;&#65292;&#22914;&#38543;&#26426;&#24615;&#12289;&#38544;&#34255;&#20449;&#24687;&#21644;&#22823;&#30340;&#20998;&#25903;&#22240;&#32032;&#22806;&#65292;&#36824;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#21644;&#36866;&#24212;&#24615;&#12290;&#35813;&#28216;&#25103;&#21487;&#20197;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#36827;&#34892;&#35299;&#20915;&#65292;&#21253;&#25324;&#32463;&#20856;&#30340;&#23545;&#25239;&#25628;&#32034;&#12289;&#21333;&#20154;&#35745;&#21010;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31454;&#36187;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;&#28216;&#25103;&#35268;&#21017;&#65292;&#24182;&#21576;&#29616;&#20102;&#31034;&#20363;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#38182;&#26631;&#36187;&#32467;&#26524;&#12290;TOTAIC&#30340;&#31532;&#19968;&#23626;&#23558;&#22312;2023&#24180;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#20030;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new AI challenge, the Tales of Tribute AI Competition (TOTAIC), based on a two-player deck-building card game released with the High Isle chapter of The Elder Scrolls Online. Currently, there is no other AI competition covering Collectible Card Games (CCG) genre, and there has never been one that targets a deck-building game. Thus, apart from usual CCG-related obstacles to overcome, like randomness, hidden information, and large branching factor, the successful approach additionally requires long-term planning and versatility. The game can be tackled with multiple approaches, including classic adversarial search, single-player planning, and Neural Networks-based algorithms. This paper introduces the competition framework, describes the rules of the game, and presents the results of a tournament between sample AI agents. The first edition of TOTAIC is hosted at the IEEE Conference on Games 2023.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20551;&#35774;&#25152;&#26377;&#26816;&#32034;&#20449;&#24687;&#37117;&#26159;&#27491;&#30830;&#30340;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#34394;&#20551;&#20449;&#24687;&#23548;&#33268;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#37492;&#21035;&#33021;&#21147;&#24341;&#20986;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#19979;&#30340;&#25928;&#26524;&#65307;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2305.01579</link><description>&lt;p&gt;
&#21306;&#20998;&#21644;&#22238;&#31572;&#65306;&#36890;&#36807;&#36776;&#21035;&#22120;&#32531;&#35299;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#20013;&#34394;&#20551;&#20449;&#24687;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models with Discriminators. (arXiv:2305.01579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20551;&#35774;&#25152;&#26377;&#26816;&#32034;&#20449;&#24687;&#37117;&#26159;&#27491;&#30830;&#30340;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#34394;&#20551;&#20449;&#24687;&#23548;&#33268;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#37492;&#21035;&#33021;&#21147;&#24341;&#20986;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#19979;&#30340;&#25928;&#26524;&#65307;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20551;&#23450;&#25152;&#26377;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#37117;&#26159;&#20107;&#23454;&#19978;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#19968;&#20010;&#26356;&#21152;&#29616;&#23454;&#30340;&#22330;&#26223;&#65292;&#21363;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#21487;&#33021;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#31934;&#35843;&#21644;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#20013;&#23545;&#36825;&#31181;&#20449;&#24687;&#39640;&#24230;&#33030;&#24369;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#22320;&#23545;&#37492;&#21035;&#22120;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#25110;&#25552;&#31034;&#26469;&#24341;&#20986;GPT-3&#30340;&#37492;&#21035;&#33021;&#21147;&#65292;&#20351;&#26816;&#32034;&#22686;&#24378;LM&#23545;&#34394;&#20551;&#20449;&#24687;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;LM&#23545;&#30693;&#35782;&#20914;&#31361;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#30340;&#20915;&#31574;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#30340;&#21457;&#29616;&#65292;&#20026;&#21033;&#29992;&#20004;&#32773;&#30340;&#26368;&#20339;&#26041;&#24335;&#38138;&#24179;&#20102;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing retrieval-augmented language models (LMs) for question answering assume all retrieved information is factually correct. In this work, we study a more realistic scenario in which retrieved documents may contain misinformation, causing conflicts among them. We observe that the existing models are highly brittle to such information in both fine-tuning and in-context few-shot learning settings. We propose approaches to make retrieval-augmented LMs robust to misinformation by explicitly fine-tuning a discriminator or prompting to elicit discrimination capability in GPT-3. Our empirical results on open-domain question answering show that these approaches significantly improve LMs' robustness to knowledge conflicts. We also provide our findings on interleaving the fine-tuned model's decision with the in-context learning process, paving a new path to leverage the best of both worlds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30456;&#37051;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;0.64&#30340;&#24179;&#22343;IOU&#65292;&#23613;&#31649;&#23384;&#22312;&#19981;&#23436;&#32654;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2211.00646</link><description>&lt;p&gt;
&#20174;&#30456;&#37051;&#30340;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#23398;&#20064;&#40657;&#33394;&#32032;&#32454;&#32990;&#25513;&#33180;
&lt;/p&gt;
&lt;p&gt;
Learning Melanocytic Cell Masks from Adjacent Stained Tissue. (arXiv:2211.00646v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30456;&#37051;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;0.64&#30340;&#24179;&#22343;IOU&#65292;&#23613;&#31649;&#23384;&#22312;&#19981;&#23436;&#32654;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#33394;&#32032;&#30244;&#26159;&#26368;&#20855;&#20405;&#34989;&#24615;&#30340;&#30382;&#32932;&#30284;&#20043;&#19968;&#65292;&#23548;&#33268;&#22823;&#37096;&#20998;&#30382;&#32932;&#30284;&#27515;&#20129;&#12290;&#28982;&#32780;&#65292;&#30149;&#29702;&#23398;&#23478;&#23545;&#40657;&#33394;&#32032;&#30244;&#30340;&#35786;&#26029;&#21487;&#38752;&#24615;&#36739;&#20302;&#12290;&#30001;&#20110;&#40657;&#33394;&#32032;&#30244;&#26159;&#40657;&#33394;&#32032;&#32454;&#32990;&#30340;&#32959;&#30244;&#65292;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#19982;&#30149;&#29702;&#23398;&#23478;&#30340;&#24046;&#24322;&#26080;&#20851;&#24182;&#33021;&#33258;&#21160;&#36827;&#34892;&#20687;&#32032;&#32423;&#27880;&#37322;&#30340;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#30149;&#29702;&#23398;&#23478;&#26631;&#27880;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#37051;&#36817;&#32452;&#32455;&#20999;&#29255;&#19978;&#30340;&#20598;&#32852;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#26579;&#33394;&#29255;&#65292;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#65292;&#34429;&#28982;&#24456;&#38590;&#26377;&#23436;&#32654;&#30340;&#26631;&#31614;&#65292;&#20294;&#36798;&#21040;&#20102;0.64&#30340;&#24179;&#22343;IOU&#12290;
&lt;/p&gt;
&lt;p&gt;
Melanoma is one of the most aggressive forms of skin cancer, causing a large proportion of skin cancer deaths. However, melanoma diagnoses by pathologists shows low interrater reliability. As melanoma is a cancer of the melanocyte, there is a clear need to develop a melanocytic cell segmentation tool that is agnostic to pathologist variability and automates pixel-level annotation. Gigapixel-level pathologist labeling, however, is impractical. Herein, we propose a means to train deep neural networks for melanocytic cell segmentation from hematoxylin and eosin (H&amp;E) stained slides using paired immunohistochemical (IHC) slides of adjacent tissue sections, achieving a mean IOU of 0.64 despite imperfect ground-truth labels.
&lt;/p&gt;</description></item></channel></rss>