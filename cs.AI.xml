<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20363;&#32423;&#21035;&#30340;&#29305;&#27931;&#20234;&#25915;&#20987;&#26041;&#27861;&#65292;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#36890;&#36807;&#31070;&#32463;&#20803;&#28608;&#27963;&#31354;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#29983;&#25104;&#22810;&#26679;&#24615;&#29305;&#27931;&#20234;&#65292;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.00436</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#20803;&#28608;&#27963;&#31354;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#65292;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#23545;&#23454;&#20363;&#36827;&#34892;&#30340;&#29305;&#27931;&#20234;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Instance-level Trojan Attacks on Visual Question Answering via Adversarial Learning in Neuron Activation Space. (arXiv:2304.00436v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20363;&#32423;&#21035;&#30340;&#29305;&#27931;&#20234;&#25915;&#20987;&#26041;&#27861;&#65292;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#36890;&#36807;&#31070;&#32463;&#20803;&#28608;&#27963;&#31354;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#29983;&#25104;&#22810;&#26679;&#24615;&#29305;&#27931;&#20234;&#65292;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#25200;&#21160;&#34987;&#23884;&#20837;&#36755;&#20837;&#25968;&#25454;&#20013;&#65292;&#31216;&#20026;&#29305;&#27931;&#20234;&#25915;&#20987;&#65292;&#21487;&#20197;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#21363;&#20174;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#65288;&#22914;&#35270;&#35273;&#38382;&#31572;&#65289;&#21521;&#30446;&#26631;&#27169;&#22411;&#36716;&#31227;&#30693;&#35782;&#30340;&#36807;&#31243;&#20013;&#65292;&#29305;&#27931;&#20234;&#25915;&#20987;&#30340;&#24433;&#21709;&#26377;&#25152;&#20943;&#23567;&#12290;&#20026;&#20102;&#20943;&#36731;&#29305;&#27931;&#20234;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21487;&#20197;&#26367;&#25442;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22810;&#20010;&#23618;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#26679;&#26412;&#25928;&#29575;&#12289;&#38544;&#34109;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23454;&#20363;&#32423;&#30340;&#29305;&#27931;&#20234;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#29983;&#25104;&#36328;&#36755;&#20837;&#26679;&#26412;&#21644;&#27169;&#24577;&#30340;&#22810;&#26679;&#21270;&#29305;&#27931;&#20234;&#12290;&#23545;&#25239;&#23398;&#20064;&#24314;&#31435;&#20102;&#25351;&#23450;&#25200;&#21160;&#23618;&#21644;&#24494;&#35843;&#27169;&#22411;&#30340;&#24322;&#24120;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;VQA-v2&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#25351;&#26631;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#36866;&#24212;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malicious perturbations embedded in input data, known as Trojan attacks, can cause neural networks to misbehave. However, the impact of a Trojan attack is reduced during fine-tuning of the model, which involves transferring knowledge from a pretrained large-scale model like visual question answering (VQA) to the target model. To mitigate the effects of a Trojan attack, replacing and fine-tuning multiple layers of the pretrained model is possible. This research focuses on sample efficiency, stealthiness and variation, and robustness to model fine-tuning. To address these challenges, we propose an instance-level Trojan attack that generates diverse Trojans across input samples and modalities. Adversarial learning establishes a correlation between a specified perturbation layer and the misbehavior of the fine-tuned model. We conducted extensive experiments on the VQA-v2 dataset using a range of metrics. The results show that our proposed method can effectively adapt to a fine-tuned model 
&lt;/p&gt;</description></item><item><title>&#23567;&#25209;&#37327;$k$-means&#32858;&#31867;&#27861;&#34987;&#35777;&#26126;&#21487;&#20197;&#22312;&#19968;&#23450;&#36845;&#20195;&#27425;&#25968;&#20869;&#25910;&#25947;&#65292;&#26080;&#35770;&#21021;&#22987;&#32858;&#31867;&#20013;&#24515;&#22914;&#20309;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#23436;&#20840;&#25209;&#37327;&#29256;&#26412;&#30456;&#21516;&#30340;&#36924;&#36817;&#27604;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00419</link><description>&lt;p&gt;
&#23567;&#25209;&#37327;$k$-means&#32858;&#31867;&#27861;&#22312;$O(d/\epsilon)$&#27425;&#36845;&#20195;&#20869;&#32456;&#27490;&#12290;(arXiv:2304.00419v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Mini-batch $k$-means terminates within $O(d/\epsilon)$ iterations. (arXiv:2304.00419v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00419
&lt;/p&gt;
&lt;p&gt;
&#23567;&#25209;&#37327;$k$-means&#32858;&#31867;&#27861;&#34987;&#35777;&#26126;&#21487;&#20197;&#22312;&#19968;&#23450;&#36845;&#20195;&#27425;&#25968;&#20869;&#25910;&#25947;&#65292;&#26080;&#35770;&#21021;&#22987;&#32858;&#31867;&#20013;&#24515;&#22914;&#20309;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#23436;&#20840;&#25209;&#37327;&#29256;&#26412;&#30456;&#21516;&#30340;&#36924;&#36817;&#27604;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#8220;&#23567;&#25209;&#37327;$k$-means&#65288;Min-batch $K$-Means&#65289;&#30340;&#23616;&#37096;&#36827;&#23637;&#65288;&#22312;&#25209;&#22788;&#29702;&#19978;&#65289;&#26159;&#21542;&#24847;&#21619;&#30528;&#20840;&#23616;&#36827;&#23637;&#65288;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#65289;&#65311;&#8221;&#25105;&#20204;&#32771;&#34385;&#20102;&#20165;&#24403;&#22312;&#37319;&#26679;&#25209;&#22788;&#29702;&#30340;&#36136;&#37327;&#25913;&#36827;&#20302;&#20110;&#26576;&#20010;&#38408;&#20540;&#26102;&#25165;&#32456;&#27490;&#30340;&#23567;&#25209;&#37327;$k$-means&#32858;&#31867;&#26041;&#27861;&#12290;&#23613;&#31649;&#20045;&#19968;&#30475;&#36825;&#20010;&#31639;&#27861;&#21487;&#33021;&#27704;&#36828;&#19981;&#20250;&#25191;&#34892;&#23436;&#65292;&#20294;&#25105;&#20204;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#65292;&#22914;&#26524;&#25209;&#27425;&#22823;&#23567;&#20026;$\tilde{\Omega}((d/\epsilon)^2)$&#65292;&#21017;&#23427;&#24517;&#39035;&#22312;$O(d/\epsilon)$&#27425;&#36845;&#20195;&#20869;&#20197;&#39640;&#27010;&#29575;&#32456;&#27490;&#65292;&#20854;&#20013;$d$&#26159;&#36755;&#20837;&#30340;&#32500;&#24230;&#65292;$\epsilon$&#26159;&#32456;&#27490;&#30340;&#38408;&#20540;&#21442;&#25968;&#12290;&#36825;&#19968;&#28857;&#26159;&#26377;&#36947;&#29702;&#30340;&#65292;&#26080;&#35770;&#20013;&#24515;&#22914;&#20309;&#21021;&#22987;&#21270;&#12290;&#24403;&#31639;&#27861;&#20351;&#29992;$k$-means++&#21021;&#22987;&#21270;&#26041;&#26696;&#21021;&#22987;&#21270;&#26102;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;$O(\log k)$&#65288;&#19982;&#23436;&#20840;&#25209;&#37327;&#29256;&#26412;&#30456;&#21516;&#65289;&#30340;&#36924;&#36817;&#27604;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#23567;&#25209;&#37327;$k$-means&#32858;&#31867;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We answer the question: "Does local progress (on batches) imply global progress (on the entire dataset) for mini-batch $k$-means?". Specifically, we consider mini-batch $k$-means which terminates only when the improvement in the quality of the clustering on the sampled batch is below some threshold.  Although at first glance it appears that this algorithm might execute forever, we answer the above question in the affirmative and show that if the batch is of size $\tilde{\Omega}((d/\epsilon)^2)$, it must terminate within $O(d/\epsilon)$ iterations with high probability, where $d$ is the dimension of the input, and $\epsilon$ is a threshold parameter for termination. This is true regardless of how the centers are initialized. When the algorithm is initialized with the $k$-means++ initialization scheme, it achieves an approximation ratio of $O(\log k)$ (the same as the full-batch version).  Finally, we show the applicability of our results to the mini-batch $k$-means algorithm implemented
&lt;/p&gt;</description></item><item><title>SafeguardGPT&#26694;&#26550;&#20351;&#29992;&#24515;&#29702;&#27835;&#30103;&#26469;&#32416;&#27491;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#65292;&#25913;&#36827;&#19982;&#20154;&#31867;&#30340;&#23545;&#35805;&#36136;&#37327;&#65292;&#36827;&#32780;&#25512;&#36827;&#20581;&#24247;AI&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.00416</link><description>&lt;p&gt;
&#36808;&#21521;&#20581;&#24247;AI&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#38656;&#35201;&#27835;&#30103;&#24072;
&lt;/p&gt;
&lt;p&gt;
Towards Healthy AI: Large Language Models Need Therapists Too. (arXiv:2304.00416v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00416
&lt;/p&gt;
&lt;p&gt;
SafeguardGPT&#26694;&#26550;&#20351;&#29992;&#24515;&#29702;&#27835;&#30103;&#26469;&#32416;&#27491;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#65292;&#25913;&#36827;&#19982;&#20154;&#31867;&#30340;&#23545;&#35805;&#36136;&#37327;&#65292;&#36827;&#32780;&#25512;&#36827;&#20581;&#24247;AI&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#21151;&#33021;&#24378;&#22823;&#30340; AI &#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#21442;&#19982;&#33258;&#28982;&#19988;&#31867;&#20284;&#20154;&#31867;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#33021;&#20855;&#26377;&#28508;&#22312;&#30340;&#21361;&#23475;&#24615;&#65292;&#34920;&#29616;&#20986;&#25805;&#32437;&#12289;&#28748;&#36755;&#34394;&#20551;&#35266;&#24565;&#21644;&#33258;&#24651;&#34892;&#20026;&#12290;&#25105;&#20204;&#23450;&#20041;&#20581;&#24247;AI&#20026;&#23433;&#20840;&#12289;&#21487;&#20449;&#21644;&#36947;&#24503;&#30340;AI&#12290;&#20026;&#20102;&#21019;&#36896;&#20581;&#24247;&#30340;AI&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SafeguardGPT&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#24515;&#29702;&#27835;&#30103;&#26469;&#32416;&#27491;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#36825;&#20123;&#26377;&#23475;&#34892;&#20026;&#12290;&#35813;&#26694;&#26550;&#28041;&#21450;&#22235;&#31181;&#31867;&#22411;&#30340;AI&#20195;&#29702;&#65306;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;"&#29992;&#25143;"&#12289;"&#27835;&#30103;&#24072;"&#21644;"&#35780;&#35770;&#23478;"&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#31038;&#20132;&#23545;&#35805;&#30340;&#24037;&#20316;&#31034;&#20363;&#23637;&#31034;&#20102;SafeguardGPT&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25913;&#36827;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#23545;&#35805;&#36136;&#37327;&#12290;&#34429;&#28982;&#26410;&#26469;&#20173;&#38656;&#35299;&#20915;&#20960;&#20010;&#25361;&#25112;&#21644;&#26041;&#21521;&#65292;&#20294;SafeguardGPT&#20026;&#25913;&#21892;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20043;&#38388;&#30340;&#21327;&#35843;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have led to the development of powerful AI chatbots capable of engaging in natural and human-like conversations. However, these chatbots can be potentially harmful, exhibiting manipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to be safe, trustworthy and ethical. To create healthy AI systems, we present the SafeguardGPT framework that uses psychotherapy to correct for these harmful behaviors in AI chatbots. The framework involves four types of AI agents: a Chatbot, a "User," a "Therapist," and a "Critic." We demonstrate the effectiveness of SafeguardGPT through a working example of simulating a social conversation. Our results show that the framework can improve the quality of conversations between AI chatbots and humans. Although there are still several challenges and directions to be addressed in the future, SafeguardGPT provides a promising approach to improving the alignment between AI chatbots and human value
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;150&#20010;CWE&#30340;&#26032;&#30340;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#27604;&#20197;&#21069;&#25152;&#26377;&#25968;&#25454;&#38598;&#21152;&#36215;&#26469;&#22810;305&#20010;&#39033;&#30446;&#12290;&#20316;&#32773;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#25913;&#36827;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#32467;&#21512;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#25361;&#25112;&#21644;&#21069;&#36884;&#30340;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#28431;&#27934;&#20173;&#23384;&#22312;&#39640;&#35823;&#25253;&#29575;&#65292;&#20302;F1&#20998;&#25968;&#21644;&#38590;&#20197;&#26816;&#27979;&#20005;&#37325;CWE&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00409</link><description>&lt;p&gt;
DiverseVul: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#30340;&#26032;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection. (arXiv:2304.00409v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00409
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;150&#20010;CWE&#30340;&#26032;&#30340;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#27604;&#20197;&#21069;&#25152;&#26377;&#25968;&#25454;&#38598;&#21152;&#36215;&#26469;&#22810;305&#20010;&#39033;&#30446;&#12290;&#20316;&#32773;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#25913;&#36827;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#32467;&#21512;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#25361;&#25112;&#21644;&#21069;&#36884;&#30340;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#28431;&#27934;&#20173;&#23384;&#22312;&#39640;&#35823;&#25253;&#29575;&#65292;&#20302;F1&#20998;&#25968;&#21644;&#38590;&#20197;&#26816;&#27979;&#20005;&#37325;CWE&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#28431;&#27934;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#29228;&#21462;&#23433;&#20840;&#38382;&#39064;&#32593;&#31449;&#65292;&#25552;&#21462;&#30456;&#24212;&#39033;&#30446;&#30340;&#28431;&#27934;&#20462;&#22797;&#25552;&#20132;&#21644;&#28304;&#20195;&#30721;&#65292;&#31579;&#36873;&#20986;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;150&#20010;CWE&#65292;26,635&#20010;&#26131;&#21463;&#25915;&#20987;&#30340;&#20989;&#25968;&#21644;352,606&#20010;&#19981;&#26131;&#21463;&#25915;&#20987;&#30340;&#20989;&#25968;&#65292;&#25552;&#21462;&#33258;7,861&#20010;&#25552;&#20132;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35206;&#30422;&#20102;&#27604;&#20197;&#21069;&#25152;&#26377;&#25968;&#25454;&#38598;&#21152;&#36215;&#26469;&#22810;305&#20010;&#39033;&#30446;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;11&#20010;&#27169;&#22411;&#26550;&#26500;&#65292;&#23646;&#20110;4&#20010;&#23478;&#26063;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#20110;&#39640;&#35823;&#25253;&#29575;&#65292;&#20302;F1&#20998;&#25968;&#21644;&#38590;&#20197;&#26816;&#27979;&#20005;&#37325;CWE&#65292;&#28145;&#24230;&#23398;&#20064;&#20173;&#26410;&#20934;&#22791;&#22909;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;......
&lt;/p&gt;
&lt;p&gt;
We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 150 CWEs, 26,635 vulnerable functions, and 352,606 non-vulnerable functions extracted from 7,861 commits. Our dataset covers 305 more projects than all previous datasets combined. We show that increasing the diversity and volume of training data improves the performance of deep learning models for vulnerability detection.  Combining our new dataset with previous datasets, we present an analysis of the challenges and promising research directions of using deep learning for detecting software vulnerabilities. We study 11 model architectures belonging to 4 families. Our results show that deep learning is still not ready for vulnerability detection, due to high false positive rate, low F1 score, and difficulty of detecting hard CWEs. In particular, we demonst
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#38752;&#30528;&#31616;&#21333;&#30340;&#25968;&#23398;&#20889;&#35770;&#25991;&#65292;&#21364;&#24471;&#19981;&#21040;&#19990;&#20154;&#30340;&#20851;&#27880;&#21644;&#35748;&#21487;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;zero2hero&#30340;&#31639;&#27861;&#65292;&#23558;&#27599;&#31687;&#35770;&#25991;&#36716;&#21270;&#20026;&#31185;&#23398;&#26480;&#20316;&#65292;&#22797;&#26434;&#21270;&#27599;&#19968;&#20010;&#26041;&#31243;&#24335;&#65292;&#20197;&#27492;&#22312;&#26410;&#26469;&#30340;&#23457;&#31295;&#20154;&#20013;&#24341;&#26469;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2304.00399</link><description>&lt;p&gt;
&#20174;&#38646;&#21040;&#33521;&#38596;&#65306;&#29992;&#26497;&#20854;&#22797;&#26434;&#30340;&#25968;&#23398;&#35828;&#26381;&#20154;&#20204;
&lt;/p&gt;
&lt;p&gt;
From Zero to Hero: Convincing with Extremely Complicated Math. (arXiv:2304.00399v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00399
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#38752;&#30528;&#31616;&#21333;&#30340;&#25968;&#23398;&#20889;&#35770;&#25991;&#65292;&#21364;&#24471;&#19981;&#21040;&#19990;&#20154;&#30340;&#20851;&#27880;&#21644;&#35748;&#21487;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;zero2hero&#30340;&#31639;&#27861;&#65292;&#23558;&#27599;&#31687;&#35770;&#25991;&#36716;&#21270;&#20026;&#31185;&#23398;&#26480;&#20316;&#65292;&#22797;&#26434;&#21270;&#27599;&#19968;&#20010;&#26041;&#31243;&#24335;&#65292;&#20197;&#27492;&#22312;&#26410;&#26469;&#30340;&#23457;&#31295;&#20154;&#20013;&#24341;&#26469;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#20026;(&#36229;&#32423;)&#33521;&#38596;&#26159;&#20960;&#20046;&#27599;&#20010;&#23401;&#23376;&#30340;&#26790;&#24819;&#12290;&#22312;&#20182;&#20204;&#28201;&#39336;&#30340;&#31461;&#24180;&#20013;&#65292;&#20182;&#20204;&#20250;&#23613;&#19968;&#20999;&#21162;&#21147;&#38271;&#22823;&#25104;&#20026;&#19968;&#20010;&#33521;&#38596;&#12290;&#26397;&#20061;&#26202;&#20116;&#22320;&#24037;&#20316;&#65292;&#27426;&#22825;&#21916;&#22320;&#22320;&#29609;&#32781;&#12290;&#20294;&#26159;&#38543;&#30528;&#24180;&#40836;&#30340;&#22686;&#38271;&#65292;&#20250;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#24178;&#25200;&#12290;&#20182;&#20204;&#36208;&#19978;&#20102;&#23700;&#36335;&#12290;&#20182;&#20204;&#24320;&#22987;&#21457;&#29616;&#20196;&#20154;&#30031;&#24807;&#30340;&#31616;&#21333;&#25968;&#23398;&#12290;&#26368;&#32456;&#65292;&#20182;&#20204;&#25104;&#20026;&#20102;&#19968;&#21517;&#30740;&#31350;&#32773;&#65292;&#25972;&#22825;&#20889;&#30528;&#20047;&#21619;&#30340;&#12289;&#19981;&#22826;&#20986;&#33394;&#30340;&#35770;&#25991;&#65292;&#22240;&#20026;&#20182;&#20204;&#21482;&#20381;&#36182;&#31616;&#21333;&#30340;&#25968;&#23398;&#12290;&#27809;&#26377;&#39030;&#32423;&#20250;&#35758;&#65292;&#27809;&#26377;&#23562;&#37325;&#65292;&#27809;&#26377;&#27515;&#20826;&#12290;&#29983;&#27963;&#32467;&#26463;&#20102;&#12290;&#20026;&#20102;&#26368;&#32456;&#32467;&#26463;&#36825;&#19968;&#24754;&#21095;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;zero2hero&#65292;&#23558;&#27599;&#19968;&#31687;&#30740;&#31350;&#35770;&#25991;&#36716;&#21270;&#20026;&#31185;&#23398;&#26480;&#20316;&#12290;&#22522;&#20110;&#19979;&#19968;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#33258;&#21160;&#36807;&#24230;&#22797;&#26434;&#21270;&#27599;&#19968;&#20010;&#26041;&#31243;&#24335;&#65292;&#20351;&#24471;&#27809;&#26377;&#20154;&#65292;&#21253;&#25324;&#33258;&#24049;&#65292;&#33021;&#22815;&#29702;&#35299;&#21040;&#24213;&#21457;&#29983;&#20102;&#20160;&#20040;&#12290;&#26410;&#26469;&#30340;&#23457;&#31295;&#20154;&#20204;&#23558;&#34987;&#25152;&#22797;&#26434;&#30340;&#20869;&#23481;&#25152;&#38663;&#24778;&#12290;
&lt;/p&gt;
&lt;p&gt;
Becoming a (super) hero is almost every kid's dream. During their sheltered childhood, they do whatever it takes to grow up to be one. Work hard, play hard -- all day long. But as they're getting older, distractions are more and more likely to occur. They're getting off track. They start discovering what is feared as simple math. Finally, they end up as a researcher, writing boring, non-impressive papers all day long because they only rely on simple mathematics. No top-tier conferences, no respect, no groupies. Life's over.  To finally put an end to this tragedy, we propose a fundamentally new algorithm, dubbed zero2hero, that turns every research paper into a scientific masterpiece. Given a LaTeX document containing ridiculously simple math, based on next-generation large language models, our system automatically over-complicates every single equation so that no one, including yourself, is able to understand what the hell is going on. Future reviewers will be blown away by the complex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#20154;&#31867;&#39550;&#39542;&#21592;&#34892;&#20026;&#20197;&#23454;&#29616;&#20986;&#20837;&#21277;&#36947;&#30340;&#26377;&#25928;&#21512;&#24182;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;CAV&#21644;HDV&#20114;&#21160;&#30340;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;&#28151;&#21512;&#20132;&#36890;&#26465;&#20214;&#19979;&#23454;&#29616;&#23433;&#20840;&#19988;&#39640;&#25928;&#30340;&#21512;&#24182;&#12290;</title><link>http://arxiv.org/abs/2304.00397</link><description>&lt;p&gt;
&#28151;&#21512;&#20132;&#36890;&#20013;&#30340;&#36830;&#25509;&#33258;&#21160;&#27773;&#36710;&#65306;&#23398;&#20064;&#20154;&#31867;&#39550;&#39542;&#21592;&#34892;&#20026;&#20197;&#23454;&#29616;&#20986;&#20837;&#21277;&#36947;&#30340;&#26377;&#25928;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Connected and Automated Vehicles in Mixed-Traffic: Learning Human Driver Behavior for Effective On-Ramp Merging. (arXiv:2304.00397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#20154;&#31867;&#39550;&#39542;&#21592;&#34892;&#20026;&#20197;&#23454;&#29616;&#20986;&#20837;&#21277;&#36947;&#30340;&#26377;&#25928;&#21512;&#24182;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;CAV&#21644;HDV&#20114;&#21160;&#30340;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;&#28151;&#21512;&#20132;&#36890;&#26465;&#20214;&#19979;&#23454;&#29616;&#23433;&#20840;&#19988;&#39640;&#25928;&#30340;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20132;&#36890;&#26465;&#20214;&#19979;&#30340;&#20844;&#36335;&#21512;&#27969;&#22330;&#26223;&#23545;&#20110;&#19982;&#36827;&#20837;&#20986;&#20837;&#21475;&#30340;&#20154;&#31867;&#39550;&#39542;&#36710;&#36742;&#65288;HDV&#65289;&#20132;&#20114;&#30340;&#36830;&#25509;&#33258;&#21160;&#27773;&#36710;&#65288;CAV&#65289;&#32780;&#35328;&#65292;&#23384;&#22312;&#30528;&#26174;&#30528;&#30340;&#24314;&#27169;&#21644;&#25511;&#21046;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;CAV&#21644;HDV&#20114;&#21160;&#30340;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#27169;&#22411;&#65292;&#20351;CAV&#22312;&#20844;&#36335;&#21512;&#27969;&#26102;&#23433;&#20840;&#39550;&#39542;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;CAV&#20250;&#22312;&#29983;&#25104;&#25511;&#21046;&#31574;&#30053;&#26469;&#20419;&#36827;&#21512;&#24182;&#20043;&#21069;&#65292;&#20351;&#29992;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#23398;&#20064;&#36827;&#20837;HDV&#30340;&#34892;&#20026;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#39564;&#35777;&#20102;&#36825;&#31181;&#26694;&#26550;&#30340;&#21151;&#25928;&#65292;&#36890;&#36807;&#22312;&#20174;Next-Generation Simulation&#23384;&#20648;&#24211;&#20013;&#25552;&#21462;&#30340;&#28151;&#21512;&#20132;&#36890;&#24773;&#20917;&#19979;&#39044;&#27979;HDV&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;&#21453;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20026;&#20844;&#36335;&#21512;&#27969;&#22330;&#26223;&#29983;&#25104;HDV-CAV&#20132;&#20114;&#30340;&#27169;&#25311;&#25968;&#25454;&#12290;&#22312;&#19981;&#20551;&#35774;&#29983;&#25104;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#27169;&#22411;&#20351;&#24471;&#22312;&#28151;&#21512;&#20132;&#36890;&#26465;&#20214;&#19979;&#23454;&#29616;&#23433;&#20840;&#19988;&#39640;&#25928;&#30340;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Highway merging scenarios featuring mixed traffic conditions pose significant modeling and control challenges for connected and automated vehicles (CAVs) interacting with incoming on-ramp human-driven vehicles (HDVs). In this paper, we present an approach to learn an approximate information state model of CAV-HDV interactions for a CAV to maneuver safely during highway merging. In our approach, the CAV learns the behavior of an incoming HDV using approximate information states before generating a control strategy to facilitate merging. First, we validate the efficacy of this framework on real-world data by using it to predict the behavior of an HDV in mixed traffic situations extracted from the Next-Generation Simulation repository. Then, we generate simulation data for HDV-CAV interactions in a highway merging scenario using a standard inverse reinforcement learning approach. Without assuming a prior knowledge of the generating model, we show that our approximate information state mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;3D&#22797;&#21512;&#20960;&#20309;&#21464;&#25442;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;CompoundE3D&#65292;&#22312;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.00378</link><description>&lt;p&gt;
3D&#22797;&#21512;&#20960;&#20309;&#21464;&#25442;&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding with 3D Compound Geometric Transformations. (arXiv:2304.00378v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;3D&#22797;&#21512;&#20960;&#20309;&#21464;&#25442;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;CompoundE3D&#65292;&#22312;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CompoundE3D&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#21253;&#25324;&#24179;&#31227;&#12289;&#26059;&#36716;&#12289;&#32553;&#25918;&#12289;&#21453;&#23556;&#21644;&#21098;&#20999;&#22312;&#20869;&#30340;3D&#22797;&#21512;&#20960;&#20309;&#21464;&#25442;&#12290;CompoundE3D&#20801;&#35768;&#22810;&#20010;&#35774;&#35745;&#21464;&#20307;&#20197;&#21305;&#37197;&#30693;&#35782;&#22270;&#35889;&#30340;&#24213;&#23618;&#29305;&#24449;&#65292;&#24182;&#33021;&#20135;&#29983;&#36229;&#20986;&#21333;&#20010;&#21464;&#20307;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#38142;&#25509;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cascade of 2D geometric transformations were exploited to model relations between entities in a knowledge graph (KG), leading to an effective KG embedding (KGE) model, CompoundE. Furthermore, the rotation in the 3D space was proposed as a new KGE model, Rotate3D, by leveraging its non-commutative property. Inspired by CompoundE and Rotate3D, we leverage 3D compound geometric transformations, including translation, rotation, scaling, reflection, and shear and propose a family of KGE models, named CompoundE3D, in this work. CompoundE3D allows multiple design variants to match rich underlying characteristics of a KG. Since each variant has its own advantages on a subset of relations, an ensemble of multiple variants can yield superior performance. The effectiveness and flexibility of CompoundE3D are experimentally verified on four popular link prediction datasets.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35843;&#26597;&#20102;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#23558;&#20854;&#20998;&#20026;&#19971;&#31867;&#65292;&#24182;&#32473;&#20986;&#20102;&#35843;&#26597;&#25991;&#29486;&#30340;&#32479;&#35745;&#20803;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.00377</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24773;&#24863;&#35745;&#31639;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Personalized Affective Computing in Human-Machine Interaction. (arXiv:2304.00377v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00377
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35843;&#26597;&#20102;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#23558;&#20854;&#20998;&#20026;&#19971;&#31867;&#65292;&#24182;&#32473;&#20986;&#20102;&#35843;&#26597;&#25991;&#29486;&#30340;&#32479;&#35745;&#20803;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#39046;&#22495;&#20013;&#65292;&#20010;&#24615;&#21270;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#25110;&#22810;&#20010;&#24615;&#33021;&#25351;&#26631;&#24182;&#36981;&#23432;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#26469;&#35757;&#32451;&#36814;&#21512;&#29305;&#23450;&#20010;&#20154;&#25110;&#20154;&#32676;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#24773;&#24863;&#21644;&#20154;&#26684;&#35745;&#31639;&#65288;&#20197;&#19979;&#31616;&#31216;&#24773;&#24863;&#35745;&#31639;&#65289;&#20013;&#20010;&#24615;&#21270;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#23545;&#24773;&#24863;&#35745;&#31639;&#20013;&#20010;&#24615;&#21270;&#30340;&#26368;&#26032;&#26041;&#27861;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#28085;&#30422;&#20102;&#35757;&#32451;&#25216;&#26415;&#21644;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#24773;&#24863;&#35745;&#31639;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#23450;&#21046;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#26041;&#27861;&#20998;&#20026;&#19971;&#31867;&#65306;&#65288;1&#65289;&#38754;&#21521;&#29305;&#23450;&#30446;&#26631;&#30340;&#27169;&#22411;&#65292;&#65288;2&#65289;&#38754;&#21521;&#29305;&#23450;&#32676;&#20307;&#30340;&#27169;&#22411;&#65292;&#65288;3&#65289;&#22522;&#20110;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#65288;4&#65289;&#24494;&#35843;&#26041;&#27861;&#65292;&#65288;5&#65289;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#65288;6&#65289;&#29983;&#25104;&#24335;&#27169;&#22411;&#21644;&#65288;7&#65289;&#29305;&#24449;&#22686;&#24378;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#35843;&#26597;&#25991;&#29486;&#30340;&#32479;&#35745;&#20803;&#20998;&#26512;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#24773;&#24863;&#35745;&#31639;&#20219;&#21153;&#12289;&#20132;&#20114;&#27169;&#24335;&#12289;&#20132;&#20114;&#19978;&#19979;&#25991;&#20197;&#21450;&#25152;&#28041;&#21450;&#39046;&#22495;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computing, the aim of personalization is to train a model that caters to a specific individual or group of people by optimizing one or more performance metrics and adhering to specific constraints. In this paper, we discuss the need for personalization in affective and personality computing (hereinafter referred to as affective computing). We present a survey of state-of-the-art approaches for personalization in affective computing. Our review spans training techniques and objectives towards the personalization of affective computing models. We group existing approaches into seven categories: (1) Target-specific Models, (2) Group-specific Models, (3) Weighting-based Approaches, (4) Fine-tuning Approaches, (5) Multitask Learning, (6) Generative-based Models, and (7) Feature Augmentation. Additionally, we provide a statistical meta-analysis of the surveyed literature, analyzing the prevalence of different affective computing tasks, interaction modes, interaction contexts, and the leve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#33258;&#20027;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#36890;&#36807;&#23545;&#27604;&#34892;&#20026;&#24635;&#32467;&#20256;&#36882;&#32473;&#20154;&#31867;&#35266;&#23519;&#32773;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#24635;&#32467;&#26377;&#25928;&#22320;&#24110;&#21161;&#20154;&#31867;&#35780;&#20272;&#26426;&#22120;&#20154;&#25972;&#20307;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.00367</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#34892;&#20026;&#24635;&#32467;&#20256;&#36882;&#33258;&#20027;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Conveying Autonomous Robot Capabilities through Contrasting Behaviour Summaries. (arXiv:2304.00367v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#33258;&#20027;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#36890;&#36807;&#23545;&#27604;&#34892;&#20026;&#24635;&#32467;&#20256;&#36882;&#32473;&#20154;&#31867;&#35266;&#23519;&#32773;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#24635;&#32467;&#26377;&#25928;&#22320;&#24110;&#21161;&#20154;&#31867;&#35780;&#20272;&#26426;&#22120;&#20154;&#25972;&#20307;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#36825;&#20351;&#24471;&#20154;&#31867;&#35266;&#23519;&#32773;&#26356;&#38590;&#26377;&#25928;&#22320;&#26500;&#24314;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#20026;&#20102;&#25104;&#21151;&#37096;&#32626;&#33258;&#20027;&#26426;&#22120;&#20154;&#65292;&#20154;&#31867;&#19981;&#20165;&#24212;&#35813;&#33021;&#22815;&#29702;&#35299;&#26426;&#22120;&#20154;&#30340;&#20010;&#20307;&#23616;&#38480;&#24615;&#65292;&#36824;&#24212;&#35813;&#20102;&#35299;&#23427;&#20204;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#24635;&#32467;&#12290;&#36807;&#21435;&#65292;&#21333;&#20010;&#26426;&#22120;&#20154;&#34892;&#20026;&#24635;&#32467;&#26159;&#36890;&#36807;&#29983;&#25104;&#26576;&#20010;&#26102;&#27493;&#19979;&#26426;&#22120;&#20154;&#36873;&#25321;&#29305;&#23450;&#21160;&#20316;&#30340;&#35299;&#37322;&#26469;&#35299;&#20915;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#20219;&#21153;&#65292;&#27599;&#20010;&#21160;&#20316;&#30340;&#35299;&#37322;&#21487;&#33021;&#26080;&#27861;&#20256;&#36798;&#26426;&#22120;&#20154;&#30340;&#20840;&#23616;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#22987;&#23547;&#27714;&#33021;&#22815;&#26356;&#22909;&#22320;&#24110;&#21161;&#20154;&#31867;&#35780;&#20272;&#26426;&#22120;&#20154;&#25972;&#20307;&#33021;&#21147;&#30340;&#22810;&#27493;&#39588;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
As advances in artificial intelligence enable increasingly capable learning-based autonomous agents, it becomes more challenging for human observers to efficiently construct a mental model of the agent's behaviour. In order to successfully deploy autonomous agents, humans should not only be able to understand the individual limitations of the agents but also have insight on how they compare against one another. To do so, we need effective methods for generating human interpretable agent behaviour summaries. Single agent behaviour summarization has been tackled in the past through methods that generate explanations for why an agent chose to pick a particular action at a single timestep. However, for complex tasks, a per-action explanation may not be able to convey an agents global strategy. As a result, researchers have looked towards multi-timestep summaries which can better help humans assess an agents overall capability. More recently, multi-step summaries have also been used for gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#19987;&#23478;&#30340;&#33258;&#36866;&#24212;&#22833;&#25928;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#25506;&#32034;&#21644;&#21457;&#29616;&#33258;&#20027;&#31574;&#30053;&#22312;&#20223;&#30495;&#20013;&#30340;&#22833;&#25928;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2304.00365</link><description>&lt;p&gt;
&#39046;&#22495;&#19987;&#23478;&#20851;&#38190;&#29366;&#24577;&#33258;&#36866;&#24212;&#22833;&#25928;&#25628;&#32034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Failure Search Using Critical States from Domain Experts. (arXiv:2304.00365v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#19987;&#23478;&#30340;&#33258;&#36866;&#24212;&#22833;&#25928;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#25506;&#32034;&#21644;&#21457;&#29616;&#33258;&#20027;&#31574;&#30053;&#22312;&#20223;&#30495;&#20013;&#30340;&#22833;&#25928;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28508;&#22312;&#25925;&#38556;&#26159;&#39564;&#35777;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#30001;&#20110;&#25925;&#38556;&#20107;&#20214;&#30340;&#31232;&#23569;&#24615;&#65292;&#20351;&#29992;&#38543;&#26426;&#25628;&#32034;&#26041;&#27861;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#25214;&#21040;&#28508;&#22312;&#30340;&#31995;&#32479;&#24369;&#28857;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#25628;&#32034;&#25216;&#26415;&#26469;&#26377;&#25928;&#22320;&#25506;&#32034;&#21644;&#21457;&#29616;&#33258;&#20027;&#31574;&#30053;&#22312;&#20223;&#30495;&#20013;&#30340;&#22833;&#25928;&#36712;&#36857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#19987;&#23478;&#30340;&#33258;&#36866;&#24212;&#22833;&#25928;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncovering potential failure cases is a crucial step in the validation of safety critical systems such as autonomous vehicles. Failure search may be done through logging substantial vehicle miles in either simulation or real world testing. Due to the sparsity of failure events, naive random search approaches require significant amounts of vehicle operation hours to find potential system weaknesses. As a result, adaptive searching techniques have been proposed to efficiently explore and uncover failure trajectories of an autonomous policy in simulation. Adaptive Stress Testing (AST) is one such method that poses the problem of failure search as a Markov decision process and uses reinforcement learning techniques to find high probability failures. However, this formulation requires a probability model for the actions of all agents in the environment. In systems where the environment actions are discrete and dependencies among agents exist, it may be infeasible to fully characterize the d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#37319;&#26679;&#30340;&#31574;&#30053;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#24378;&#20581;&#30340;&#20219;&#21153;&#34920;&#31034;&#21644;&#26356;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00354</link><description>&lt;p&gt;
&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Context Distribution Shift in Task Representation Learning for Offline Meta RL. (arXiv:2304.00354v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#19978;&#19979;&#25991;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#37319;&#26679;&#30340;&#31574;&#30053;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#24378;&#20581;&#30340;&#20219;&#21153;&#34920;&#31034;&#21644;&#26356;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;OMRL&#65289;&#26088;&#22312;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#20419;&#36827;&#26032;&#30446;&#26631;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;RL&#37319;&#29992;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#25512;&#26029;&#20219;&#21153;&#34920;&#31034;&#26469;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#28982;&#21518;&#26681;&#25454;&#25512;&#26029;&#20986;&#30340;&#20219;&#21153;&#34920;&#31034;&#35843;&#25972;&#34892;&#21160;&#31574;&#30053;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;OMRL&#65292;&#29305;&#21035;&#26159;OMRL&#20013;&#30340;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#65292;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#21487;&#33021;&#20250;&#36973;&#21463;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#20351;&#29992;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30828;&#37319;&#26679;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#24378;&#20581;&#30340;&#20219;&#21153;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#12290;&#22522;&#20110;&#19981;&#21516;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#21033;&#29992;&#23548;&#33268;&#26356;&#24378;&#20581;&#30340;&#20219;&#21153;&#34920;&#31034;&#21644;&#26356;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#32047;&#31215;&#22238;&#25253;&#27604;&#22522;&#20934;&#26041;&#27861;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline meta reinforcement learning (OMRL) aims to learn transferrable knowledge from offline datasets to facilitate the learning process for new target tasks. Context-based RL employs a context encoder to rapidly adapt the agent to new tasks by inferring about the task representation, and then adjusting the acting policy based on the inferred task representation. Here we consider context-based OMRL, in particular, the issue of task representation learning for OMRL. We empirically demonstrate that the context encoder trained on offline datasets could suffer from distribution shift between the contexts used for training and testing. To tackle this issue, we propose a hard sampling based strategy for learning a robust task context encoder. Experimental results, based on distinct continuous control tasks, demonstrate that the utilization of our technique results in more robust task representations and better testing performance in terms of accumulated returns, compared with baseline metho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#20102;&#22240;&#23376;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#21333;&#20010;&#26426;&#22120;&#20154;&#26041;&#27861;&#35745;&#31639;&#19978;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#36880;&#27493;&#26500;&#24314;&#36229;&#22270;&#21487;&#20197;&#23558;&#20195;&#29702;&#20998;&#21106;&#20026;&#29420;&#31435;&#30340;&#23376;&#22270;&#12290;</title><link>http://arxiv.org/abs/2304.00342</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#25277;&#26679;&#35268;&#21010;&#30340;&#22240;&#23376;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factorization of Multi-Agent Sampling-Based Motion Planning. (arXiv:2304.00342v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#20102;&#22240;&#23376;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20013;&#21333;&#20010;&#26426;&#22120;&#20154;&#26041;&#27861;&#35745;&#31639;&#19978;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#36880;&#27493;&#26500;&#24314;&#36229;&#22270;&#21487;&#20197;&#23558;&#20195;&#29702;&#20998;&#21106;&#20026;&#29420;&#31435;&#30340;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#20154;&#25216;&#26415;&#24448;&#24448;&#28041;&#21450;&#22810;&#20010;&#20849;&#20139;&#29615;&#22659;&#20013;&#36816;&#20316;&#30340;&#23454;&#20307;&#26234;&#33021;&#20307;&#12290;&#36825;&#20123;&#24773;&#20917;&#19979;&#30340;&#36335;&#24452;&#35268;&#21010;&#27604;&#21333;&#20010;&#26426;&#22120;&#20154;&#30340;&#24773;&#20917;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#26631;&#20934;&#30340;&#25277;&#26679;&#31639;&#27861;&#65288;SBAs&#65289;&#21487;&#20197;&#29992;&#20110;&#22312;&#26426;&#22120;&#20154;&#20851;&#33410;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#65292;&#35831;&#27880;&#24847;&#24403;&#26426;&#22120;&#20154;&#25968;&#30446;&#22686;&#21152;&#26102;&#65292;&#27492;&#26041;&#27861;&#24456;&#24555;&#20250;&#21464;&#24471;&#35745;&#31639;&#19978;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#22240;&#23376;&#20998;&#35299;&#30340;&#27010;&#24565;&#38598;&#25104;&#21040;&#25277;&#26679;&#31639;&#27861;&#20013;&#65292;&#36825;&#21482;&#38656;&#35201;&#26368;&#23569;&#30340;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20462;&#25913;&#12290;&#22312;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#26399;&#38388;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#22240;&#23376;&#20998;&#35299;&#21551;&#21457;&#24335;&#26469;&#23558;&#19981;&#21516;&#30340;&#20195;&#29702;&#23376;&#38598;&#35299;&#32806;&#21512;&#65288;&#21363;&#22240;&#23376;&#20998;&#35299;&#65289;&#25104;&#29420;&#31435;&#30340;&#20302;&#32500;&#25628;&#32034;&#31354;&#38388;&#65292;&#19968;&#26086;&#25105;&#20204;&#35777;&#26126;&#23427;&#20204;&#26410;&#26469;&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#20250;&#20114;&#30456;&#20381;&#36182;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36880;&#27493;&#26500;&#24314;&#19968;&#20010;&#32039;&#20945;&#30340;&#36229;&#22270;&#65292;&#20854;&#20013;&#26576;&#20123;&#65288;&#36229;&#65289;&#36793;&#23558;&#20195;&#29702;&#20998;&#21106;&#20026;&#29420;&#31435;&#30340;&#23376;&#22270;&#12290;&#22312;&#26368;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569; d &#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern robotics often involves multiple embodied agents operating within a shared environment. Path planning in these cases is considerably more challenging than in single-agent scenarios. Although standard Sampling-based Algorithms (SBAs) can be used to search for solutions in the robots' joint space, this approach quickly becomes computationally intractable as the number of agents increases. To address this issue, we integrate the concept of factorization into sampling-based algorithms, which requires only minimal modifications to existing methods. During the search for a solution we can decouple (i.e., factorize) different subsets of agents into independent lower-dimensional search spaces once we certify that their future solutions will be independent of each other using a factorization heuristic. Consequently, we progressively construct a lean hypergraph where certain (hyper-)edges split the agents to independent subgraphs. In the best case, this approach can reduce the growth in d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26080;&#20559;&#26631;&#31614;&#22122;&#22768;&#30340;&#38544;&#21547;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;SGD&#30340;&#21160;&#24577;&#24314;&#27169;&#20026;&#21452;&#37325;&#38543;&#26426;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00320</link><description>&lt;p&gt;
&#21452;&#37325;&#38543;&#26426;&#27169;&#22411;&#65306;&#26080;&#20559;&#26631;&#31614;&#22122;&#22768;&#30340;&#23398;&#20064;&#19982;&#25512;&#29702;&#31283;&#23450;
&lt;/p&gt;
&lt;p&gt;
Doubly Stochastic Models: Learning with Unbiased Label Noises and Inference Stability. (arXiv:2304.00320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26080;&#20559;&#26631;&#31614;&#22122;&#22768;&#30340;&#38544;&#21547;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;SGD&#30340;&#21160;&#24577;&#24314;&#27169;&#20026;&#21452;&#37325;&#38543;&#26426;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26631;&#31614;&#22122;&#22768;&#24191;&#27867;&#23384;&#22312;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#30740;&#31350;&#26631;&#31614;&#22122;&#22768;&#30340;&#38544;&#21547;&#27491;&#21017;&#21270;&#25928;&#24212;&#65292;&#20551;&#35774;&#26631;&#31614;&#22122;&#22768;&#26159;&#26080;&#20559;&#30340;&#65292;&#20998;&#26512;&#20102;SGD&#22312;&#26080;&#20559;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#23558;SGD&#30340;&#21160;&#24577;&#24314;&#27169;&#20026;&#20855;&#26377;&#20004;&#20010;&#25193;&#25955;&#39033;&#30340;&#38543;&#26426;&#21487;&#24494;&#26041;&#31243;&#65288;&#21363;&#21452;&#37325;&#38543;&#26426;&#27169;&#22411;&#65289;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#21457;&#29616;&#65292;&#36825;&#31181;&#38544;&#21547;&#27491;&#21017;&#21270;&#21487;&#20197;&#22312;&#23398;&#21040;&#30340;&#27169;&#22411;&#19978;&#23454;&#26045;&#21452;&#37325;&#38543;&#26426;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#39564;&#35777;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#21644;&#27700;&#24179;&#30340;&#26631;&#31614;&#22122;&#22768;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random label noises (or observational noises) widely exist in practical machine learning settings. While previous studies primarily focus on the affects of label noises to the performance of learning, our work intends to investigate the implicit regularization effects of the label noises, under mini-batch sampling settings of stochastic gradient descent (SGD), with assumptions that label noises are unbiased. Specifically, we analyze the learning dynamics of SGD over the quadratic loss with unbiased label noises, where we model the dynamics of SGD as a stochastic differentiable equation (SDE) with two diffusion terms (namely a Doubly Stochastic Model). While the first diffusion term is caused by mini-batch sampling over the (label-noiseless) loss gradients as many other works on SGD, our model investigates the second noise term of SGD dynamics, which is caused by mini-batch sampling over the label noises, as an implicit regularizer. Our theoretical analysis finds such implicit regulariz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#29992;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#21487;&#38752;&#22320;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#26469;&#25506;&#32034;&#21487;&#29992;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20026;&#23376;&#20154;&#32676;&#21010;&#20998;&#25552;&#20379;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2304.00305</link><description>&lt;p&gt;
&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65306;&#24230;&#37327;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Predictive Heterogeneity: Measures and Applications. (arXiv:2304.00305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#29992;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#21487;&#38752;&#22320;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#26469;&#25506;&#32034;&#21487;&#29992;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20026;&#23376;&#20154;&#32676;&#21010;&#20998;&#25552;&#20379;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#22823;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22522;&#26412;&#23646;&#24615;&#65292;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#23384;&#22312;&#20110;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#22914;&#31934;&#20934;&#21307;&#23398;&#12289;&#33258;&#21160;&#39550;&#39542;&#12289;&#37329;&#34701;&#24212;&#29992;&#31561;&#12290;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32780;&#35328;&#65292;&#24573;&#30053;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#20250;&#26497;&#22823;&#22320;&#25439;&#23475;&#27867;&#21270;&#24615;&#33021;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#65292;&#22240;&#20026;&#19981;&#21516;&#23376;&#20154;&#32676;&#20043;&#38388;&#30340;&#39044;&#27979;&#26426;&#21046;&#21487;&#33021;&#20250;&#23384;&#22312;&#24046;&#24322;&#12290;&#26412;&#25991;&#20851;&#27880;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#20102;&#8220;&#21487;&#29992;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#8221;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#27169;&#22411;&#23481;&#37327;&#21644;&#35745;&#31639;&#32422;&#26463;&#12290;&#35777;&#26126;&#20102;&#23427;&#21487;&#20197;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#21487;&#38752;&#22320;&#20272;&#35745;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#20449;&#30340;&#27491;&#30830;&#24615;(PAC)&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#26469;&#20174;&#25968;&#25454;&#20013;&#25506;&#32034;&#21487;&#29992;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#25506;&#32034;&#20986;&#30340;&#24322;&#36136;&#24615;&#20026;&#23376;&#20154;&#32676;&#21010;&#20998;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an intrinsic and fundamental property of big data, data heterogeneity exists in a variety of real-world applications, such as precision medicine, autonomous driving, financial applications, etc. For machine learning algorithms, the ignorance of data heterogeneity will greatly hurt the generalization performance and the algorithmic fairness, since the prediction mechanisms among different sub-populations are likely to differ from each other. In this work, we focus on the data heterogeneity that affects the prediction of machine learning models, and firstly propose the \emph{usable predictive heterogeneity}, which takes into account the model capacity and computational constraints. We prove that it can be reliably estimated from finite data with probably approximately correct (PAC) bounds. Additionally, we design a bi-level optimization algorithm to explore the usable predictive heterogeneity from data. Empirically, the explored heterogeneity provides insights for sub-population divis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;RL&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#38544;&#34255;&#22312;&#20195;&#29702;&#20013;&#30340;&#21518;&#38376;&#12290;</title><link>http://arxiv.org/abs/2304.00252</link><description>&lt;p&gt;
RL&#20013;&#30340;&#21453;&#21521;&#25915;&#20987;&#20445;&#25252;&#65306;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning. (arXiv:2304.00252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;RL&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#38544;&#34255;&#22312;&#20195;&#29702;&#20013;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#25915;&#20987;&#21487;&#20197;&#20351;&#24694;&#24847;&#29992;&#25143;&#25805;&#32437;&#29615;&#22659;&#25110;&#30772;&#22351;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23558;&#19968;&#20010;&#38544;&#34255;&#30340;&#21518;&#38376;&#25554;&#20837;&#21040;&#35757;&#32451;&#20195;&#29702;&#31243;&#24207;&#20013;&#12290;&#36825;&#31181;&#25915;&#20987;&#21361;&#21450;RL&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#65292;&#22312;&#21508;&#20010;&#20851;&#38190;&#39046;&#22495;&#21487;&#33021;&#20250;&#36896;&#25104;&#28798;&#38590;&#24615;&#30340;&#24433;&#21709;&#12290;&#19982;&#27492;&#30456;&#27604;&#65292;&#23545;&#20110;RL&#20013;&#30340;&#21453;&#21521;&#25915;&#20987;&#26377;&#25928;&#30340;&#38450;&#24481;&#25514;&#26045;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20445;&#25252;&#21463;&#23475;&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290; RTS&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#12290;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#20195;&#29702;&#20013;&#38544;&#34255;&#30340;&#21518;&#38376;&#12290;&#22312;&#35757;&#32451;&#26367;&#20195;&#32593;&#32476;&#26469;&#39044;&#27979;&#29366;&#24577;&#26102;&#65292;&#25105;&#20204;&#23558;&#20195;&#29702;&#21160;&#20316;&#20449;&#24687;&#24182;&#20837;&#65292;&#20943;&#23569;&#20195;&#29702;&#22312;&#39044;&#27979;&#29366;&#24577;&#19978;&#37319;&#21462;&#30340;&#21160;&#20316;&#21644;&#23454;&#38469;&#29366;&#24577;&#19978;&#37319;&#21462;&#30340;&#21160;&#20316;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A backdoor attack allows a malicious user to manipulate the environment or corrupt the training data, thus inserting a backdoor into the trained agent. Such attacks compromise the RL system's reliability, leading to potentially catastrophic results in various key fields. In contrast, relatively limited research has investigated effective defenses against backdoor attacks in RL. This paper proposes the Recovery Triggered States (RTS) method, a novel approach that effectively protects the victim agents from backdoor attacks. RTS involves building a surrogate network to approximate the dynamics model. Developers can then recover the environment from the triggered state to a clean state, thereby preventing attackers from activating backdoors hidden in the agent by presenting the trigger. When training the surrogate to predict states, we incorporate agent action information to reduce the discrepancy between the actions taken by the agent on predicted states and the actions taken on real sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#24615;&#33021;&#35780;&#20272;&#30340;&#26234;&#33021;&#21330;&#20013;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#27604;&#36739;&#20102;&#20116;&#31181;&#24120;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#26368;&#36866;&#21512;&#21330;&#20013;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.00249</link><description>&lt;p&gt;
&#20174;&#27010;&#24565;&#21040;&#37096;&#32626;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#24615;&#33021;&#35780;&#20272;&#30340;&#26234;&#33021;&#21330;&#20013;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From Conception to Deployment: Intelligent Stroke Prediction Framework using Machine Learning and Performance Evaluation. (arXiv:2304.00249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#24615;&#33021;&#35780;&#20272;&#30340;&#26234;&#33021;&#21330;&#20013;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#27604;&#36739;&#20102;&#20116;&#31181;&#24120;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#26368;&#36866;&#21512;&#21330;&#20013;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21330;&#20013;&#26159;&#20840;&#29699;&#31532;&#20108;&#22823;&#27515;&#22240;&#12290;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#21330;&#20013;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#27809;&#26377;&#38024;&#23545;&#21330;&#20013;&#25968;&#25454;&#20998;&#26512;&#30340;&#32508;&#21512;&#26694;&#26550;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#20010;&#26234;&#33021;&#21330;&#20013;&#39044;&#27979;&#26694;&#26550;&#12290;&#23545;&#21330;&#20013;&#39044;&#27979;&#20013;&#20116;&#20010;&#26368;&#24120;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#32479;&#19968;&#30340;&#35774;&#32622;&#36827;&#34892;&#23458;&#35266;&#27604;&#36739;&#12290;&#27604;&#36739;&#20998;&#26512;&#21644;&#25968;&#23383;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#26368;&#36866;&#21512;&#21330;&#20013;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stroke is the second leading cause of death worldwide. Machine learning classification algorithms have been widely adopted for stroke prediction. However, these algorithms were evaluated using different datasets and evaluation metrics. Moreover, there is no comprehensive framework for stroke data analytics. This paper proposes an intelligent stroke prediction framework based on a critical examination of machine learning prediction algorithms in the literature. The five most used machine learning algorithms for stroke prediction are evaluated using a unified setup for objective comparison. Comparative analysis and numerical results reveal that the Random Forest algorithm is best suited for stroke prediction.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;R-BOCPD-UCRL2&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#37325;&#26032;&#21551;&#21160;&#30340;&#36125;&#21494;&#26031;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65288;R-BOCPD&#65289;&#65292;&#24182;&#22312;&#22810;&#39033;&#24335;&#20998;&#24067;&#37319;&#26679;&#30340;MDP&#19978;&#25552;&#20379;&#20102;&#36739;&#20248;&#31168;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.00232</link><description>&lt;p&gt;
&#37325;&#21551;&#36125;&#21494;&#26031;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#29992;&#20110;&#38750;&#24179;&#31283;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Restarted Bayesian Online Change-point Detection for Non-Stationary Markov Decision Processes. (arXiv:2304.00232v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00232
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;R-BOCPD-UCRL2&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#37325;&#26032;&#21551;&#21160;&#30340;&#36125;&#21494;&#26031;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65288;R-BOCPD&#65289;&#65292;&#24182;&#22312;&#22810;&#39033;&#24335;&#20998;&#24067;&#37319;&#26679;&#30340;MDP&#19978;&#25552;&#20379;&#20102;&#36739;&#20248;&#31168;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#19968;&#20010;&#38750;&#24179;&#31283;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#35813;&#35774;&#32622;&#21487;&#20197;&#34987;&#23436;&#20840;&#25551;&#36848;&#20026;&#20998;&#27573;&#24179;&#31283;&#30340;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#37325;&#26032;&#21551;&#21160;&#30340;&#36125;&#21494;&#26031;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#31639;&#27861;&#65288;R-BOCPD&#65289;&#21464;&#20307;&#65292;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#20174;&#26356;&#19968;&#33324;&#30340;&#22810;&#39033;&#24335;&#20998;&#24067;&#20013;&#29983;&#25104;&#30340;&#36755;&#20837;&#27969;&#65292;&#24182;&#22312;&#35823;&#35686;&#29575;&#21644;&#26816;&#27979;&#24310;&#36831;&#26041;&#38754;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20174;&#22810;&#39033;&#24335;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#29366;&#24577;&#36716;&#31227;&#20869;&#26680;&#30340;MDPs&#30340;&#25913;&#36827;&#29256;&#26412;UCRL2&#31639;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;R-BOCPD-UCRL2&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#24615;&#33021;&#20998;&#26512;&#65292;&#24182;&#34920;&#26126;R-BOCPD-UCRL2&#20855;&#26377;&#26377;&#21033;&#30340;&#36951;&#25022;&#30028;&#30340;$O\left(D O \sqrt{A T K_T \log\left (\frac{T}{\delta} \right) + \frac{K_T \log \frac{K_T}{\delta}}{\min\limits_\ell \: \mathbf{KL}\left( {\mathbf{\theta}^{(\ell+1)}}\mid\mid{\mathbf{\theta}^{(\ell)}}\right)}}\right)$&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning in a non-stationary reinforcement learning (RL) environment, where the setting can be fully described by a piecewise stationary discrete-time Markov decision process (MDP). We introduce a variant of the Restarted Bayesian Online Change-Point Detection algorithm (R-BOCPD) that operates on input streams originating from the more general multinomial distribution and provides near-optimal theoretical guarantees in terms of false-alarm rate and detection delay. Based on this, we propose an improved version of the UCRL2 algorithm for MDPs with state transition kernel sampled from a multinomial distribution, which we call R-BOCPD-UCRL2. We perform a finite-time performance analysis and show that R-BOCPD-UCRL2 enjoys a favorable regret bound of $O\left(D O \sqrt{A T K_T \log\left (\frac{T}{\delta} \right) + \frac{K_T \log \frac{K_T}{\delta}}{\min\limits_\ell \: \mathbf{KL}\left( {\mathbf{\theta}^{(\ell+1)}}\mid\mid{\mathbf{\theta}^{(\ell)}}\right)}}\right)$,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MaskDeep&#36825;&#19968;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#36974;&#34109;&#20998;&#23618;&#29305;&#24449;&#30340;&#31574;&#30053;&#65292;&#20174;&#31232;&#30095;&#30340;&#21487;&#35265;&#34917;&#19969;&#26469;&#25512;&#29702;&#22270;&#20687;&#30340;&#20840;&#23616;&#35821;&#20041;&#12290;&#26412;&#26041;&#27861;&#30456;&#27604;&#20854;&#23427;&#33258;&#30417;&#30563;&#26041;&#27861;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.00218</link><description>&lt;p&gt;
&#36974;&#34109;&#20998;&#23618;&#29305;&#24449;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mask Hierarchical Features For Self-Supervised Learning. (arXiv:2304.00218v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MaskDeep&#36825;&#19968;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#36974;&#34109;&#20998;&#23618;&#29305;&#24449;&#30340;&#31574;&#30053;&#65292;&#20174;&#31232;&#30095;&#30340;&#21487;&#35265;&#34917;&#19969;&#26469;&#25512;&#29702;&#22270;&#20687;&#30340;&#20840;&#23616;&#35821;&#20041;&#12290;&#26412;&#26041;&#27861;&#30456;&#27604;&#20854;&#23427;&#33258;&#30417;&#30563;&#26041;&#27861;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#36974;&#34109;&#28145;&#23618;&#20998;&#23618;&#29305;&#24449;&#30340;&#26377;&#25928;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#31216;&#20026;MaskDeep&#12290; MaskDeep&#23558;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#27599;&#20010;&#34917;&#19969;&#35270;&#20026;&#29420;&#31435;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#36974;&#34109;&#37096;&#20998;&#34917;&#19969;&#65292;&#28982;&#21518;&#21033;&#29992;&#31232;&#30095;&#21487;&#35265;&#34917;&#19969;&#37325;&#24314;&#39640;&#35821;&#20041;&#22270;&#20687;&#34920;&#31034;&#12290; MaskDeep&#30340;&#30452;&#35273;&#22312;&#20110;&#65292;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20174;&#31232;&#30095;&#21487;&#35265;&#34917;&#19969;&#35821;&#20041;&#21040;&#22270;&#20687;&#30340;&#20840;&#23616;&#35821;&#20041;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#26694;&#26550;&#20013;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19977;&#20010;&#35774;&#35745;&#65306;1&#65289;&#23618;&#27425;&#28145;&#23618;&#36974;&#34109;&#27169;&#22359;&#65292;&#20197;&#20851;&#27880;&#34917;&#19969;&#34920;&#31034;&#30340;&#20998;&#23618;&#29305;&#24615;&#65292;2&#65289;&#22810;&#32452;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#32534;&#30721;&#22120;&#30340;&#25928;&#29575;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#35745;&#31639;&#28040;&#32791;&#65292;3&#65289;&#22810;&#30446;&#26631;&#31574;&#30053;&#65292;&#20197;&#25552;&#20379;&#26356;&#22810;&#20840;&#23616;&#35821;&#20041;&#30340;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;MaskDeep&#24102;&#26469;&#20102;&#19981;&#38169;&#30340;&#25913;&#36827;&#12290;&#22312;ResNet50&#19978;&#36827;&#34892;200&#20010;epoch&#30340;&#35757;&#32451;&#65292;MaskDeep&#22312;I&#19978;&#23454;&#29616;&#20102;71.2&#65285;&#30340;Top1&#20934;&#30830;&#29575;&#32447;&#24615;&#20998;&#31867;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper shows that Masking the Deep hierarchical features is an efficient self-supervised method, denoted as MaskDeep. MaskDeep treats each patch in the representation space as an independent instance. We mask part of patches in the representation space and then utilize sparse visible patches to reconstruct high semantic image representation. The intuition of MaskDeep lies in the fact that models can reason from sparse visible patches semantic to the global semantic of the image. We further propose three designs in our framework: 1) a Hierarchical Deep-Masking module to concern the hierarchical property of patch representations, 2) a multi-group strategy to improve the efficiency without any extra computing consumption of the encoder and 3) a multi-target strategy to provide more description of the global semantic. Our MaskDeep brings decent improvements. Trained on ResNet50 with 200 epochs, MaskDeep achieves state-of-the-art results of 71.2% Top1 accuracy linear classification on I
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL&#31639;&#27861;&#65292;&#23558;&#36328;&#23610;&#24230;&#20851;&#31995;&#26174;&#24335;&#32858;&#21512;&#21040;&#19968;&#20010;&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;MIL&#32593;&#32476;&#20013;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24573;&#30053;&#23545;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#30340;&#36328;&#23610;&#24230;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00216</link><description>&lt;p&gt;
&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;&#36328;&#23610;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-scale Multi-instance Learning for Pathological Image Diagnosis. (arXiv:2304.00216v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL&#31639;&#27861;&#65292;&#23558;&#36328;&#23610;&#24230;&#20851;&#31995;&#26174;&#24335;&#32858;&#21512;&#21040;&#19968;&#20010;&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;MIL&#32593;&#32476;&#20013;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24573;&#30053;&#23545;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#30340;&#36328;&#23610;&#24230;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#65292;&#36328;&#22810;&#20010;&#23610;&#24230;&#20998;&#26512;&#39640;&#20998;&#36776;&#29575;&#30340;&#20840;&#24133;&#22270;&#20687; (WSIs) &#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22810;&#23454;&#20363;&#23398;&#20064; (MIL) &#26159;&#21033;&#29992;&#20998;&#31867;&#23545;&#35937;&#38598; (&#20363;&#22914;&#36739;&#23567;&#30340;&#22270;&#20687;&#22359;&#38598;) &#23545;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22788;&#29702;&#36890;&#24120;&#22312;WSIs&#30340;&#21333;&#20010;&#23610;&#24230;&#65288;&#20363;&#22914;20&#20493;&#25918;&#22823;&#65289;&#19978;&#36827;&#34892;&#65292;&#24573;&#30053;&#20102;&#23545;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#30340;&#36328;&#23610;&#24230;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL&#31639;&#27861;&#65292;&#23558;&#36328;&#23610;&#24230;&#20851;&#31995;&#26174;&#24335;&#32858;&#21512;&#21040;&#19968;&#20010;&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;MIL&#32593;&#32476;&#20013;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;(1) &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL (CS-MIL)&#31639;&#27861;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#36328;&#23610;&#24230;&#20851;&#31995;&#65307;(2) &#21019;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#29609;&#20855;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#23610;&#24230;&#29305;&#24322;&#24615;&#24418;&#24577;&#29305;&#24449;&#65292;&#20197;&#26816;&#26597;&#21644;&#21487;&#35270;&#21270;&#19981;&#21516;&#30340;&#36328;&#23610;&#24230;&#20851;&#31995;&#65307;(3)&#22312;&#22235;&#20010;WSI&#30340;&#32454;&#32990;&#32954;&#30284;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;CS-MIL&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing high resolution whole slide images (WSIs) with regard to information across multiple scales poses a significant challenge in digital pathology. Multi-instance learning (MIL) is a common solution for working with high resolution images by classifying bags of objects (i.e. sets of smaller image patches). However, such processing is typically performed at a single scale (e.g., 20x magnification) of WSIs, disregarding the vital inter-scale information that is key to diagnoses by human pathologists. In this study, we propose a novel cross-scale MIL algorithm to explicitly aggregate inter-scale relationships into a single MIL network for pathological image diagnosis. The contribution of this paper is three-fold: (1) A novel cross-scale MIL (CS-MIL) algorithm that integrates the multi-scale information and the inter-scale relationships is proposed; (2) A toy dataset with scale-specific morphological features is created and released to examine and visualize differential cross-scale a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00215</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#24402;&#32435;&#20851;&#31995;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#20851;&#31995;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#29616;&#26377;&#30340;&#23884;&#20837;&#24335;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36716;&#23548;&#35774;&#32622;&#65292;&#32570;&#20047;&#24402;&#32435;&#33021;&#21147;&#65292;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#23454;&#20307;&#19978;&#36827;&#34892;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#20998;&#23618;Transformer&#26694;&#26550;&#65292;&#21363;REPORT&#65292;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;REPORT&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#20004;&#20010;&#23436;&#20840;&#24402;&#32435;&#30340;&#25968;&#25454;&#38598;&#30340;&#20843;&#20010;&#29256;&#26412;&#23376;&#38598;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;REPORT&#33021;&#22815;&#23558;&#25512;&#29702;&#25512;&#24191;&#21040;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#27809;&#26377;&#20844;&#20849;&#23454;&#20307;&#30340;&#26032;&#23454;&#20307;&#19978;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;Neo4j&#22270;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#65292;&#33021;&#22815;&#23454;&#29616;&#25317;&#22581;&#36335;&#27573;&#30340;&#36127;&#36733;&#24179;&#34913;&#21644;&#20248;&#21270;&#65292;&#21516;&#26102;&#21487;&#20197;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#23545;&#25972;&#20307;&#20132;&#36890;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.00192</link><description>&lt;p&gt;
&#22522;&#20110;Neo4j&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#36890;&#25317;&#22581;&#27169;&#25311;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Leveraging Neo4j and deep learning for traffic congestion simulation &amp; optimization. (arXiv:2304.00192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;Neo4j&#22270;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#65292;&#33021;&#22815;&#23454;&#29616;&#25317;&#22581;&#36335;&#27573;&#30340;&#36127;&#36733;&#24179;&#34913;&#21644;&#20248;&#21270;&#65292;&#21516;&#26102;&#21487;&#20197;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#23545;&#25972;&#20307;&#20132;&#36890;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25317;&#22581;&#19968;&#30452;&#26159;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#36807;&#21435;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20197;&#20984;&#26174;&#19982;&#20132;&#36890;&#25317;&#22581;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#20132;&#36890;&#25317;&#22581;&#20998;&#26512;&#37117;&#26159;&#20351;&#29992;&#27169;&#25311;&#36719;&#20214;&#36827;&#34892;&#30340;&#65292;&#36825;&#20123;&#36719;&#20214;&#30001;&#20110;&#20351;&#29992;&#30340;&#24037;&#20855;&#21644;&#23454;&#29992;&#31243;&#24207;&#30340;&#38480;&#21046;&#32780;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#27934;&#35265;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#24433;&#21709;&#21040;&#23450;&#21046;&#19994;&#21153;&#38382;&#39064;&#30340;&#21046;&#23450;&#65292;&#36825;&#20123;&#38382;&#39064;&#22240;&#22320;&#21306;&#21644;&#22269;&#23478;&#32780;&#24322;&#12290;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#24314;&#27169;&#20026;Neo4j&#22270;&#65292;&#28982;&#21518;&#20351;&#29992;&#36127;&#36733;&#24179;&#34913;&#12289;&#20248;&#21270;&#31639;&#27861;&#26469;&#35782;&#21035;&#26080;&#25317;&#22581;&#30340;&#36947;&#36335;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#25317;&#22581;&#25110;&#20107;&#25925;&#24773;&#20917;&#19979;&#20132;&#36890;&#22914;&#20309;&#21521;&#21518;&#20256;&#25773;&#20197;&#21450;&#20854;&#23545;&#20854;&#20182;&#36947;&#36335;&#27573;&#30340;&#24635;&#20307;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#22312;&#23454;&#26102;&#20132;&#36890;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;&#39034;&#24207;RNN-LSTM(&#38271;&#30701;&#26102;&#35760;&#24518;)&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic congestion has been a major challenge in many urban road networks. Extensive research studies have been conducted to highlight traffic-related congestion and address the issue using data-driven approaches. Currently, most traffic congestion analyses are done using simulation software that offers limited insight due to the limitations in the tools and utilities being used to render various traffic congestion scenarios. All that impacts the formulation of custom business problems which vary from place to place and country to country. By exploiting the power of the knowledge graph, we model a traffic congestion problem into the Neo4j graph and then use the load balancing, optimization algorithm to identify congestion-free road networks. We also show how traffic propagates backward in case of congestion or accident scenarios and its overall impact on other segments of the roads. We also train a sequential RNN-LSTM (Long Short-Term Memory) deep learning model on the real-time traffi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#20154;&#31867;&#31354;&#38388;&#24863;&#30693;&#30340;3D&#23556;&#24433;&#20960;&#20309;&#23398;&#19982;&#26234;&#33021;&#20307;&#24863;&#30693;&#26041;&#26696;&#20013;&#30340;&#32676;&#27010;&#24565;&#30456;&#32467;&#21512;&#65292;&#25506;&#35752;&#19981;&#21516;&#32676;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#26234;&#33021;&#20307;&#30340;&#22909;&#22855;&#24515;&#39537;&#21160;&#25506;&#32034;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.00188</link><description>&lt;p&gt;
&#22909;&#22855;&#24515;&#39537;&#21160;&#25506;&#32034;&#20013;&#27431;&#27663;&#32676;&#19982;&#23556;&#24433;&#32676;&#23545;&#20110;&#26234;&#33021;&#20307;&#20869;&#37096;&#31354;&#38388;&#30340;&#20316;&#29992;&#65306;&#24418;&#24335;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Action of the Euclidean versus Projective group on an agent's internal space in curiosity driven exploration: a formal analysis. (arXiv:2304.00188v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00188
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#20154;&#31867;&#31354;&#38388;&#24863;&#30693;&#30340;3D&#23556;&#24433;&#20960;&#20309;&#23398;&#19982;&#26234;&#33021;&#20307;&#24863;&#30693;&#26041;&#26696;&#20013;&#30340;&#32676;&#27010;&#24565;&#30456;&#32467;&#21512;&#65292;&#25506;&#35752;&#19981;&#21516;&#32676;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#26234;&#33021;&#20307;&#30340;&#22909;&#22855;&#24515;&#39537;&#21160;&#25506;&#32034;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#30340;&#31354;&#38388;&#24863;&#30693;&#20013;&#65292;&#20449;&#24687;&#20284;&#20046;&#26159;&#26681;&#25454;&#19977;&#32500;&#23556;&#24433;&#20960;&#20309;&#23398;&#34920;&#31034;&#30340;&#12290;&#23427;&#23558;&#20449;&#24687;&#38598;&#25104;&#21644;&#34892;&#21160;&#35268;&#21010;&#32452;&#32455;&#22312;&#19968;&#20010;&#20869;&#37096;&#34920;&#31034;&#31354;&#38388;&#20013;&#12290;&#19981;&#21516;&#20010;&#20307;&#30340;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#26159;&#22914;&#20309;&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#30340;&#21464;&#25442;&#30456;&#20114;&#20851;&#32852;&#65292;&#24182;&#23450;&#20041;&#26234;&#33021;&#20307;&#30340;&#29305;&#23450;&#24863;&#30693;&#26041;&#26696;&#12290;&#36825;&#20123;&#21464;&#25442;&#30340;&#38598;&#21512;&#22312;&#25968;&#23398;&#19978;&#34987;&#31216;&#20026;&#8220;&#32676;&#8221;&#65292;&#23427;&#36890;&#36807;&#23545;&#20960;&#20309;&#31354;&#38388;&#30340;&#25805;&#20316;&#26469;&#34920;&#24449;&#20854;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#25317;&#26377;&#8220;&#20960;&#20309;&#8221;&#32467;&#26500;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#32473;&#20986;&#30001;&#32676;&#25552;&#20379;&#30340;&#19968;&#31181;&#26041;&#24335;&#26469;&#25429;&#25417;&#20195;&#29702;&#20043;&#38388;&#19981;&#21516;&#30340;&#24863;&#30693;&#26041;&#26696;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#25913;&#21464;&#19990;&#30028;&#27169;&#22411;&#30340;&#20960;&#20309;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30528;&#30524;&#20110;&#36825;&#20123;&#20960;&#20309;&#36816;&#31639;&#22914;&#20309;&#36890;&#36807;&#22312;&#39537;&#21160;&#26234;&#33021;&#20307;&#22909;&#22855;&#24515;&#30340;&#20027;&#35266;&#25512;&#26029;&#30340;&#24418;&#24335;&#34920;&#36798;&#19978;&#36827;&#34892;&#36716;&#25442;&#65292;&#24182;&#30456;&#24212;&#22320;&#24433;&#21709;&#25506;&#32034;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#32676;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In human spatial awareness, information appears to be represented according to 3-D projective geometry. It structures information integration and action planning within an internal representation space. The way different first person perspectives of an agent relate to each other, through transformations of a world model, defines a specific perception scheme for the agent. In mathematics, this collection of transformations is called a `group' and it characterizes a geometric space by acting on it. We propose that imbuing world models with a `geometric' structure, given by a group, is one way to capture different perception schemes of agents. We explore how changing the geometric structure of a world model impacts the behavior of an agent.  In particular, we focus on how such geometrical operations transform the formal expression of epistemic value in active inference as driving an agent's curiosity about its environment, and impact exploration behaviors accordingly. We used group action
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#25968;&#25454;&#36755;&#20837;&#24466;&#24351;&#27169;&#22411;&#65292;&#23398;&#20064;&#24182;&#25512;&#26029;&#20986;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#33258;&#23450;&#20041;&#22270;&#20687;&#65292;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2304.00186</link><description>&lt;p&gt;
&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Subject-driven Text-to-Image Generation via Apprenticeship Learning. (arXiv:2304.00186v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#25968;&#25454;&#36755;&#20837;&#24466;&#24351;&#27169;&#22411;&#65292;&#23398;&#20064;&#24182;&#25512;&#26029;&#20986;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#33258;&#23450;&#20041;&#22270;&#20687;&#65292;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;DreamBooth&#65289;&#22312;&#36890;&#36807;&#38024;&#23545;&#30446;&#26631;&#20027;&#39064;&#24494;&#35843;&#8220;&#19987;&#23478;&#27169;&#22411;&#8221;&#65292;&#29983;&#25104;&#39640;&#24230;&#33258;&#23450;&#20041;&#30340;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#24456;&#26114;&#36149;&#65292;&#22240;&#20026;&#27599;&#20010;&#20027;&#39064;&#37117;&#24517;&#39035;&#23398;&#20064;&#19968;&#20010;&#26032;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26367;&#20195;&#20027;&#39064;&#29305;&#23450;&#24494;&#35843;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#12290;&#32473;&#23450;&#19968;&#20010;&#26032;&#20027;&#39064;&#30340;&#23569;&#37327;&#28436;&#31034;&#65292;SuTI&#21487;&#20197;&#21363;&#26102;&#29983;&#25104;&#19981;&#21516;&#22330;&#26223;&#20013;&#20027;&#39064;&#30340;&#26032;&#29256;&#26412;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#20027;&#39064;&#29305;&#23450;&#30340;&#20248;&#21270;&#12290;SuTI&#30001;&#8220;&#24466;&#24351;&#23398;&#20064;&#8221;&#39537;&#21160;&#65292;&#20854;&#20013;&#20174;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21333;&#20010;&#30340;&#24466;&#24351;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20114;&#32852;&#32593;&#25366;&#25496;&#20102;&#25968;&#30334;&#19975;&#20010;&#22270;&#20687;&#31751;&#65292;&#27599;&#20010;&#22270;&#20687;&#31751;&#37117;&#32858;&#28966;&#20110;&#19968;&#20010;&#29305;&#23450;&#30340;&#35270;&#35273;&#20027;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#36825;&#20123;&#31751;&#26469;&#35757;&#32451;&#22823;&#37327;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#35270;&#35273;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#24466;&#24351;&#27169;&#22411;&#36890;&#36807;&#25512;&#26029;&#22522;&#20110;&#20854;&#25991;&#26412;&#25551;&#36848;&#30340;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#24182;&#29983;&#25104;&#22270;&#20687;&#26469;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SuTI&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#23427;&#21487;&#20197;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#19981;&#21516;&#20027;&#39064;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#27604;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with \emph{in-context} learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by {\em apprenticeship learning}, where a single apprentice model is learned from data generated by massive amount of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train massive amount of expert models specialized on diff
&lt;/p&gt;</description></item><item><title>$\textit{PrefGen}$ &#31995;&#32479;&#21033;&#29992;&#31616;&#21333;&#30340;&#25104;&#23545;&#27604;&#36739;&#26597;&#35810;&#65292;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#30340;&#30456;&#23545;&#23646;&#24615;&#12290;&#21033;&#29992;&#36825;&#20123;&#26597;&#35810;&#21709;&#24212;&#30340;&#20449;&#24687;&#65292;&#23545;&#19968;&#32452;&#22270;&#20687;&#23646;&#24615;&#30340;&#20559;&#22909;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#36827;&#34892;&#22522;&#20110;&#20559;&#22909;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2304.00185</link><description>&lt;p&gt;
PrefGen&#65306;&#22522;&#20110;&#20559;&#22909;&#30340;&#30456;&#23545;&#23646;&#24615;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PrefGen: Preference Guided Image Generation with Relative Attributes. (arXiv:2304.00185v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00185
&lt;/p&gt;
&lt;p&gt;
$\textit{PrefGen}$ &#31995;&#32479;&#21033;&#29992;&#31616;&#21333;&#30340;&#25104;&#23545;&#27604;&#36739;&#26597;&#35810;&#65292;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#30340;&#30456;&#23545;&#23646;&#24615;&#12290;&#21033;&#29992;&#36825;&#20123;&#26597;&#35810;&#21709;&#24212;&#30340;&#20449;&#24687;&#65292;&#23545;&#19968;&#32452;&#22270;&#20687;&#23646;&#24615;&#30340;&#20559;&#22909;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#36827;&#34892;&#22522;&#20110;&#20559;&#22909;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#28210;&#26579;&#20986;&#39640;&#20445;&#30495;&#30340;&#20154;&#33080;&#31561;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#25968;&#37327;&#23646;&#24615;&#30340;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#20363;&#22914;&#24773;&#24863;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#29992;&#25143;&#26126;&#30830;&#37327;&#21270;&#25152;&#38656;&#35270;&#35273;&#23646;&#24615;&#30340;&#24378;&#24230;&#12290;&#20294;&#26159;&#38480;&#21046;&#22312;&#20110;&#35768;&#22810;&#23646;&#24615;&#65292;&#20363;&#22914;&#38754;&#37096;&#34920;&#24773;&#30340; "&#24868;&#24594;" &#31243;&#24230;&#65292;&#29992;&#25143;&#38590;&#20197;&#20934;&#30830;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#21487;&#20197;&#21487;&#38752;&#22320;&#34920;&#36798;&#20986; "&#21738;&#24352;&#33080;&#30475;&#36215;&#26469;&#26356;&#24868;&#24594;"&#65292;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102; $\textit{PrefGen}$ &#31995;&#32479;&#65292;&#36890;&#36807;&#21576;&#29616;&#29992;&#25143;&#31616;&#21333;&#30340;&#25104;&#23545;&#27604;&#36739;&#26597;&#35810;&#65292;&#22914; "&#20320;&#26356;&#21916;&#27426;&#22270;&#20687; $a$ &#36824;&#26159; $b$&#65311;" &#26469;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#30340;&#30456;&#23545;&#23646;&#24615;&#12290;&#21033;&#29992;&#24207;&#21015;&#21270;&#26597;&#35810;&#21709;&#24212;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#29992;&#25143;&#23545;&#19968;&#32452;&#22270;&#20687;&#23646;&#24615;&#30340;&#20559;&#22909;&#65292;&#24182;&#36827;&#34892;&#22522;&#20110;&#20559;&#22909;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have the capacity to render high fidelity images of content like human faces. Recently, there has been substantial progress in conditionally generating images with specific quantitative attributes, like the emotion conveyed by one's face. These methods typically require a user to explicitly quantify the desired intensity of a visual attribute. A limitation of this method is that many attributes, like how "angry" a human face looks, are difficult for a user to precisely quantify. However, a user would be able to reliably say which of two faces seems "angrier". Following this premise, we develop the $\textit{PrefGen}$ system, which allows users to control the relative attributes of generated images by presenting them with simple paired comparison queries of the form "do you prefer image $a$ or image $b$?" Using information from a sequence of query responses, we can estimate user preferences over a set of image attributes and perform preference-guided image editing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#31070;&#32463;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#22810;&#20010;&#28192;&#36947;&#30340;&#21382;&#21490;&#23545;&#35805;&#21644;&#20505;&#36873;&#26469;&#28304;&#19978;&#19979;&#25991;&#20449;&#24687;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#25913;&#21892;&#22810;&#36718;&#23545;&#35805;&#21709;&#24212;&#25490;&#24207;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.00180</link><description>&lt;p&gt;
FCC: &#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#34701;&#21512;&#21382;&#21490;&#23545;&#35805;&#21644;&#20505;&#36873;&#26469;&#28304;&#36827;&#34892;&#19978;&#19979;&#25991;&#21709;&#24212;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
FCC: Fusing Conversation History and Candidate Provenance for Contextual Response Ranking in Dialogue Systems. (arXiv:2304.00180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#31070;&#32463;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#22810;&#20010;&#28192;&#36947;&#30340;&#21382;&#21490;&#23545;&#35805;&#21644;&#20505;&#36873;&#26469;&#28304;&#19978;&#19979;&#25991;&#20449;&#24687;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#25913;&#21892;&#22810;&#36718;&#23545;&#35805;&#21709;&#24212;&#25490;&#24207;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#21709;&#24212;&#25490;&#24207;&#22312;&#26816;&#32034;&#22411;&#20250;&#35805;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#65292;&#20026;&#20102;&#25429;&#25417;&#23545;&#35805;&#30340;&#35201;&#28857;&#65292;&#19978;&#19979;&#25991;&#20449;&#24687;&#20316;&#20026;&#37325;&#35201;&#30340;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#31070;&#32463;&#26694;&#26550;&#65292;&#21487;&#20197;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#28192;&#36947;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#38024;&#23545;&#24403;&#21069;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#25552;&#20379;&#20004;&#20010;&#20449;&#24687;&#36890;&#36947;&#24182;&#34892;&#65292;&#21363;&#23558;&#19982;&#20505;&#36873;&#26469;&#28304;&#30456;&#20851;&#30340; Conversational History&#65288;&#23545;&#35805;&#21382;&#21490;&#65289;&#21644; Domain Knowledge&#65288;&#39046;&#22495;&#30693;&#35782;&#65289;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20316;&#20026;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#25552;&#39640;&#22810;&#36718;&#23545;&#35805;&#21709;&#24212;&#25490;&#24207;&#30340;&#34920;&#29616;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#29992;&#20110;&#23558;&#20854;&#20182;&#19978;&#19979;&#25991;&#30446;&#26631;&#20219;&#21153;&#30340;&#21508;&#31181;&#19978;&#19979;&#25991;&#29305;&#24449;&#32435;&#20837;&#27169;&#22359;&#20013;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#20250;&#35805;&#21709;&#24212;&#25490;&#21517;&#20219;&#21153;&#30340; MSDialog &#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#30528;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Response ranking in dialogues plays a crucial role in retrieval-based conversational systems. In a multi-turn dialogue, to capture the gist of a conversation, contextual information serves as essential knowledge to achieve this goal. In this paper, we present a flexible neural framework that can integrate contextual information from multiple channels. Specifically for the current task, our approach is to provide two information channels in parallel, Fusing Conversation history and domain knowledge extracted from Candidate provenance (FCC), where candidate responses are curated, as contextual information to improve the performance of multi-turn dialogue response ranking. The proposed approach can be generalized as a module to incorporate miscellaneous contextual features for other context-oriented tasks. We evaluate our model on the MSDialog dataset widely used for evaluating conversational response ranking tasks. Our experimental results show that our framework significantly outperform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#30340;&#25913;&#36827;&#26041;&#27861;&#26469;&#26816;&#27979;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#28909;&#24102;&#27668;&#26059;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2304.00176</link><description>&lt;p&gt;
&#29992;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#30340;&#26816;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving extreme weather events detection with light-weight neural networks. (arXiv:2304.00176v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#30340;&#25913;&#36827;&#26041;&#27861;&#26469;&#26816;&#27979;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#28909;&#24102;&#27668;&#26059;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25512;&#36827;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#30340;&#33258;&#21160;&#21270;&#26816;&#27979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23545;&#19968;&#31181;&#26032;&#22411;&#36731;&#37327;&#32423;&#19978;&#19979;&#25991;&#24341;&#23548;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#30340;&#20462;&#25913;&#65292;&#35813;&#32593;&#32476;&#29992;&#20110;&#22312;&#27668;&#20505;&#25968;&#25454;&#20013;&#36827;&#34892;&#28909;&#24102;&#27668;&#26059;&#21644;&#22823;&#27668;&#27827;&#27969;&#30340;&#35821;&#20041;&#20998;&#21106;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#22312;&#24403;&#21069;&#34920;&#29616;&#26377;&#38480;&#30340;&#28909;&#24102;&#27668;&#26059;&#19978;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29305;&#24449;&#24037;&#31243;&#12289;&#25968;&#25454;&#22686;&#24378;&#12289;&#23398;&#20064;&#29575;&#20462;&#25913;&#12289;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#21644;&#26550;&#26500;&#21464;&#21270;&#12290;&#19982;&#20197;&#24448;&#20248;&#21270;&#20132;&#21449;&#28857;&#20197;&#19978;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#29305;&#21035;&#23547;&#27714;&#25913;&#36827;&#21484;&#22238;&#29575;&#20197;&#24809;&#32602;&#27424;&#35745;&#25968;&#24182;&#20248;&#20808;&#35782;&#21035;&#28909;&#24102;&#27668;&#26059;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#26469;&#25269;&#28040;&#32597;&#35265;&#20107;&#20214;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#26041;&#27861;&#32780;&#33719;&#24471;&#25104;&#21151;&#12290;&#25105;&#20204;&#26368;&#21518;&#24635;&#32467;&#26410;&#26469;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#26816;&#27979;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#36825;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
To advance automated detection of extreme weather events, which are increasing in frequency and intensity with climate change, we explore modifications to a novel light-weight Context Guided convolutional neural network architecture trained for semantic segmentation of tropical cyclones and atmospheric rivers in climate data. Our primary focus is on tropical cyclones, the most destructive weather events, for which current models show limited performance. We investigate feature engineering, data augmentation, learning rate modifications, alternative loss functions, and architectural changes. In contrast to previous approaches optimizing for intersection over union, we specifically seek to improve recall to penalize under-counting and prioritize identification of tropical cyclones. We report success through the use of weighted loss functions to counter class imbalance for these rare events. We conclude with directions for future research on extreme weather events detection, a crucial tas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Lego-Features&#30340;&#29305;&#24449;&#65292;&#23558;&#29616;&#26377;&#32534;&#30721;&#34920;&#31034;&#36716;&#25442;&#25104;&#27169;&#22359;&#21270;&#29305;&#24449;&#65292;&#32780;&#19981;&#20462;&#25913;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#27969;&#24335;&#29615;&#22659;&#20013;&#20063;&#36866;&#29992;&#12290;&#36825;&#20123;&#29305;&#24449;&#24615;&#33021;&#24378;&#22823;&#65292;RNN-T&#25110;LAS&#35299;&#30721;&#22120;&#27979;&#35797;&#26102;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#24182;&#36275;&#22815;&#20016;&#23500;&#65292;&#21487;&#20197;&#20195;&#34920;&#20004;&#20010;&#36890;&#36947;&#20915;&#31574;&#20013;&#30340;&#31532;&#19968;&#36941;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.00173</link><description>&lt;p&gt;
Lego-Features&#65306;&#23548;&#20986;&#27169;&#22359;&#21270;&#32534;&#30721;&#22120;&#29305;&#24449;&#29992;&#20110;&#27969;&#24335;&#21644;&#20915;&#31574;ASR
&lt;/p&gt;
&lt;p&gt;
Lego-Features: Exporting modular encoder features for streaming and deliberation ASR. (arXiv:2304.00173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00173
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Lego-Features&#30340;&#29305;&#24449;&#65292;&#23558;&#29616;&#26377;&#32534;&#30721;&#34920;&#31034;&#36716;&#25442;&#25104;&#27169;&#22359;&#21270;&#29305;&#24449;&#65292;&#32780;&#19981;&#20462;&#25913;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#27969;&#24335;&#29615;&#22659;&#20013;&#20063;&#36866;&#29992;&#12290;&#36825;&#20123;&#29305;&#24449;&#24615;&#33021;&#24378;&#22823;&#65292;RNN-T&#25110;LAS&#35299;&#30721;&#22120;&#27979;&#35797;&#26102;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#24182;&#36275;&#22815;&#20016;&#23500;&#65292;&#21487;&#20197;&#20195;&#34920;&#20004;&#20010;&#36890;&#36947;&#20915;&#31574;&#20013;&#30340;&#31532;&#19968;&#36941;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31471;&#21040;&#31471;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#65292;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#19981;&#21487;&#36991;&#20813;&#22320;&#20986;&#29616;&#20102;&#19968;&#31181;&#32039;&#23494;&#32806;&#21512;&#30340;&#34920;&#36798;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;&#26368;&#36817;&#24320;&#22987;&#25506;&#32034;&#26500;&#24314;&#24102;&#26377;&#27169;&#22359;&#21270;&#32534;&#30721;&#34920;&#31034;&#30340;&#32534;&#30721;&#22120;&#30340;&#24037;&#20316;&#20043;&#19978;&#65292;&#36825;&#26679;&#19981;&#21516;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25340;&#25509;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#21482;&#28041;&#21450;&#20840;&#19978;&#19979;&#25991;&#35821;&#38899;&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#20063;&#22312;&#27969;&#24335;&#29615;&#22659;&#20013;&#25506;&#32034;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#22312;&#29616;&#26377;&#32534;&#30721;&#34920;&#31034;&#20043;&#19978;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#27169;&#22359;&#21270;&#29305;&#24449;&#65292;&#31216;&#20026;Lego-Features&#65292;&#32780;&#19981;&#20462;&#25913;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#24449;&#22312;&#29992;&#19981;&#21516;&#30340;&#21021;&#22987;&#21270;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#26102;&#20173;&#28982;&#21487;&#20197;&#30456;&#20114;&#26367;&#25442;&#12290;&#23613;&#31649;&#31232;&#30095;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Lego-Features&#22312;&#20351;&#29992;RNN-T&#25110;LAS&#35299;&#30721;&#22120;&#36827;&#34892;&#27979;&#35797;&#26102;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#23427;&#20204;&#20063;&#36275;&#22815;&#20016;&#23500;&#65292;&#21487;&#20197;&#20195;&#34920;&#20004;&#20010;&#36890;&#36947;&#20915;&#31574;&#20013;&#30340;&#31532;&#19968;&#36941;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In end-to-end (E2E) speech recognition models, a representational tight-coupling inevitably emerges between the encoder and the decoder. We build upon recent work that has begun to explore building encoders with modular encoded representations, such that encoders and decoders from different models can be stitched together in a zero-shot manner without further fine-tuning. While previous research only addresses full-context speech models, we explore the problem in a streaming setting as well. Our framework builds on top of existing encoded representations, converting them to modular features, dubbed as Lego-Features, without modifying the pre-trained model. The features remain interchangeable when the model is retrained with distinct initializations. Though sparse, we show that the Lego-Features are powerful when tested with RNN-T or LAS decoders, maintaining high-quality downstream performance. They are also rich enough to represent the first-pass prediction during two-pass deliberatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#21521;&#36830;&#36890;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#26696;&#65292;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#26377;&#25928;&#22320;&#35299;&#32806;&#21644;&#21033;&#29992;&#36890;&#36947;&#26041;&#21521;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#35299;&#21078;&#19968;&#33268;&#20998;&#21106;&#65292;&#23454;&#39564;&#32467;&#26524;&#36229;&#36234;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.00145</link><description>&lt;p&gt;
&#22522;&#20110;&#26041;&#21521;&#36830;&#36890;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Directional Connectivity-based Segmentation of Medical Images. (arXiv:2304.00145v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#21521;&#36830;&#36890;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#26696;&#65292;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#26377;&#25928;&#22320;&#35299;&#32806;&#21644;&#21033;&#29992;&#36890;&#36947;&#26041;&#21521;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#35299;&#21078;&#19968;&#33268;&#20998;&#21106;&#65292;&#23454;&#39564;&#32467;&#26524;&#36229;&#36234;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#26631;&#35760;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#19968;&#33268;&#24615;&#23545;&#20110;&#35768;&#22810;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#23558;&#20687;&#32032;&#36830;&#36890;&#24615;&#65292;&#21363;&#25968;&#23383;&#25299;&#25169;&#20013;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#32435;&#20837;&#28145;&#24230;&#32593;&#32476;&#20197;&#24314;&#27169;&#20687;&#32032;&#38388;&#30340;&#20851;&#31995;&#26159;&#23454;&#29616;&#35299;&#21078;&#19968;&#33268;&#20998;&#21106;&#30340;&#26377;&#21069;&#36884;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#36830;&#36890;&#24615;&#27169;&#22411;&#24037;&#20316;&#24573;&#30053;&#20102;&#28508;&#22312;&#31354;&#38388;&#20013;&#20016;&#23500;&#30340;&#36890;&#36947;&#26041;&#21521;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;&#20849;&#20139;&#28508;&#22312;&#31354;&#38388;&#20013;&#26377;&#25928;&#22320;&#20998;&#31163;&#20986;&#26041;&#21521;&#23376;&#31354;&#38388;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#22522;&#20110;&#36830;&#36890;&#24615;&#30340;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#21521;&#36830;&#25509;&#24314;&#27169;&#26041;&#26696;&#65292;&#29992;&#20110;&#20998;&#21106;&#65292;&#21487;&#20197;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#35299;&#32806;&#12289;&#36319;&#36394;&#21644;&#21033;&#29992;&#26041;&#21521;&#20449;&#24687;&#12290;&#23545;&#21508;&#31181;&#20844;&#20849;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22522;&#20934;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Zyun-Y/Dconn&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anatomical consistency in biomarker segmentation is crucial for many medical image analysis tasks. A promising paradigm for achieving anatomically consistent segmentation via deep networks is incorporating pixel connectivity, a basic concept in digital topology, to model inter-pixel relationships. However, previous works on connectivity modeling have ignored the rich channel-wise directional information in the latent space. In this work, we demonstrate that effective disentanglement of directional sub-space from the shared latent space can significantly enhance the feature representation in the connectivity-based network. To this end, we propose a directional connectivity modeling scheme for segmentation that decouples, tracks, and utilizes the directional information across the network. Experiments on various public medical image segmentation benchmarks show the effectiveness of our model as compared to the state-of-the-art methods. Code is available at https://github.com/Zyun-Y/Dconn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#31181;&#32852;&#37030;Chow&#26816;&#39564;&#27169;&#22411;&#65292;&#25972;&#21512;&#20102;APOE&#29366;&#24577;&#12289;Tau&#27785;&#31215;&#24230;&#21644;&#28023;&#39532;&#34920;&#38754;&#24418;&#24577;&#23398;&#65292;&#21487;&#20197;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#26032;&#22411;&#35786;&#26029;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#24320;&#21457;</title><link>http://arxiv.org/abs/2304.00134</link><description>&lt;p&gt;
&#24314;&#31435;&#22522;&#20110;&#34920;&#38754;&#30340;&#32852;&#37030;Chow&#26816;&#39564;&#27169;&#22411;&#65292;&#25972;&#21512;APOE&#29366;&#24577;&#12289;Tau&#27785;&#31215;&#24230;&#21644;&#28023;&#39532;&#34920;&#38754;&#24418;&#24577;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Surface-Based Federated Chow Test Model for Integrating APOE Status, Tau Deposition Measure, and Hippocampal Surface Morphometry. (arXiv:2304.00134v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#31181;&#32852;&#37030;Chow&#26816;&#39564;&#27169;&#22411;&#65292;&#25972;&#21512;&#20102;APOE&#29366;&#24577;&#12289;Tau&#27785;&#31215;&#24230;&#21644;&#28023;&#39532;&#34920;&#38754;&#24418;&#24577;&#23398;&#65292;&#21487;&#20197;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#26032;&#22411;&#35786;&#26029;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#26368;&#24120;&#35265;&#30340;&#32769;&#24180;&#30196;&#21574;&#30151;&#31867;&#22411;&#65292;&#26681;&#25454;CDC&#25968;&#25454;&#65292;&#24433;&#21709;65&#23681;&#25110;&#20197;&#19978;&#30340;6.2&#30334;&#19975;&#20154;&#12290;&#21457;&#29616;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#29983;&#29289;&#26631;&#24535;&#29289;&#21487;&#33021;&#20855;&#26377;&#26497;&#22823;&#30340;&#20844;&#20849;&#21355;&#29983;&#25928;&#30410;&#12290;Tau&#31070;&#32463;&#21407;&#32420;&#32500;&#32544;&#32467;&#26159;&#36896;&#25104;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#19979;&#28216;&#31070;&#32463;&#36864;&#34892;&#24615;&#21644;&#38543;&#21518;&#35748;&#30693;&#38556;&#30861;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#65292;&#23548;&#33268;&#22914;&#28023;&#39532;&#33806;&#32553;&#31561;&#32467;&#26500;&#21464;&#24418;&#65292;&#21487;&#20197;&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#25195;&#25551;&#20013;&#35266;&#23519;&#21040;&#12290;&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#34920;&#38754;&#30340;&#27169;&#22411;&#65292;&#20197;1&#65289;&#26816;&#27979;APOE&#20122;&#32452;&#20043;&#38388;Tau&#27785;&#31215;&#21644;&#28023;&#39532;&#33806;&#32553;&#27169;&#24335;&#30340;&#24046;&#24322;&#65292;&#21644;2&#65289;&#21033;&#29992;&#25552;&#21462;&#30340;&#22522;&#20110;&#34920;&#38754;&#30340;&#29305;&#24449;&#39044;&#27979;&#35748;&#30693;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Alzheimer's Disease (AD) is the most common type of age-related dementia, affecting 6.2 million people aged 65 or older according to CDC data. It is commonly agreed that discovering an effective AD diagnosis biomarker could have enormous public health benefits, potentially preventing or delaying up to 40% of dementia cases. Tau neurofibrillary tangles are the primary driver of downstream neurodegeneration and subsequent cognitive impairment in AD, resulting in structural deformations such as hippocampal atrophy that can be observed in magnetic resonance imaging (MRI) scans. Objective: To build a surface-based model to 1) detect differences between APOE subgroups in patterns of tau deposition and hippocampal atrophy, and 2) use the extracted surface-based features to predict cognitive decline. Methods: Using data obtained from different institutions, we develop a surface-based federated Chow test model to study the synergistic effects of APOE, a previously reported significa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#31264;&#23494;&#26816;&#32034;&#65292;&#20351;&#29992;Tevatron &#21644;MSMARCO&#12289;NQ&#21644;TriviaQA&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#26367;&#25442;&#20026;&#21407;&#26377;&#27169;&#22411;&#65292;&#20960;&#20046;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#26368;&#22810;4.3&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.00114</link><description>&lt;p&gt;
&#31264;&#23494;&#31232;&#30095;&#26816;&#32034;&#65306;&#20351;&#29992;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#31264;&#23494;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Dense Sparse Retrieval: Using Sparse Language Models for Inference Efficient Dense Retrieval. (arXiv:2304.00114v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#31264;&#23494;&#26816;&#32034;&#65292;&#20351;&#29992;Tevatron &#21644;MSMARCO&#12289;NQ&#21644;TriviaQA&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#26367;&#25442;&#20026;&#21407;&#26377;&#27169;&#22411;&#65292;&#20960;&#20046;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#26368;&#22810;4.3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21521;&#37327;&#30340;&#26816;&#32034;&#31995;&#32479;&#24050;&#25104;&#20026;&#23398;&#26415;&#21644;&#24037;&#19994;&#25628;&#32034;&#24212;&#29992;&#30340;&#24120;&#35265;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;&#25991;&#26723;&#21644;&#26597;&#35810;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#36827;&#34892;&#25628;&#32034;&#12290;&#30001;&#20110;&#36825;&#20123;&#21521;&#37327;&#31995;&#32479;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#65292;&#22240;&#27492;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;GPU&#30340;&#20351;&#29992;&#65292;&#36825;&#21487;&#33021;&#20250;&#24456;&#26114;&#36149;&#19988;&#38590;&#20197;&#31649;&#29702;&#12290;&#37492;&#20110;&#36817;&#24180;&#26469;&#24341;&#20837;&#31232;&#30095;&#24615;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31264;&#23494;&#26816;&#32034;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#20351;&#29992;&#27969;&#34892;&#30340;&#26816;&#32034;&#24211;Tevatron&#21644;MSMARCO&#12289;NQ&#21644;TriviaQA&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#26367;&#25442;&#20026;&#21407;&#26377;&#27169;&#22411;&#65292;&#20960;&#20046;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#26368;&#22810;4.3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector-based retrieval systems have become a common staple for academic and industrial search applications because they provide a simple and scalable way of extending the search to leverage contextual representations for documents and queries. As these vector-based systems rely on contextual language models, their usage commonly requires GPUs, which can be expensive and difficult to manage. Given recent advances in introducing sparsity into language models for improved inference efficiency, in this paper, we study how sparse language models can be used for dense retrieval to improve inference efficiency. Using the popular retrieval library Tevatron and the MSMARCO, NQ, and TriviaQA datasets, we find that sparse language models can be used as direct replacements with little to no drop in accuracy and up to 4.3x improved inference speeds
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31934;&#36873;&#32508;&#36848;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#32463;&#27982;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#20998;&#26512;&#30340;&#25991;&#31456;&#65292;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22788;&#29702;&#38750;&#20256;&#32479;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12289;&#25429;&#25417;&#24378;&#38750;&#32447;&#24615;&#24615;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#20248;&#21183;&#65292;&#25104;&#20026;&#35745;&#37327;&#32463;&#27982;&#23398;&#23478;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2304.00086</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#32463;&#27982;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65306;&#20309;&#26102;&#12289;&#20160;&#20040;&#21644;&#22914;&#20309;&#36816;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Economics Research: When What and How?. (arXiv:2304.00086v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31934;&#36873;&#32508;&#36848;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#32463;&#27982;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#20998;&#26512;&#30340;&#25991;&#31456;&#65292;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22788;&#29702;&#38750;&#20256;&#32479;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12289;&#25429;&#25417;&#24378;&#38750;&#32447;&#24615;&#24615;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#20248;&#21183;&#65292;&#25104;&#20026;&#35745;&#37327;&#32463;&#27982;&#23398;&#23478;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#32463;&#27982;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#20998;&#26512;&#30340;&#37325;&#35201;&#32463;&#27982;&#26399;&#21002;&#19978;&#21457;&#34920;&#30340;&#25991;&#31456;&#36827;&#34892;&#20102;&#31934;&#36873;&#32508;&#36848;&#12290;&#32508;&#36848;&#22238;&#31572;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#20309;&#26102;&#22312;&#32463;&#27982;&#23398;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#65288;2&#65289;&#24120;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#20160;&#20040;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22914;&#20309;&#23558;&#23427;&#20204;&#29992;&#20110;&#32463;&#27982;&#24212;&#29992;&#12290;&#32508;&#36848;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#38750;&#20256;&#32479;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12289;&#25429;&#25417;&#24378;&#38750;&#32447;&#24615;&#24615;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#38750;&#20256;&#32479;&#25968;&#25454;&#65292;&#32780;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#20256;&#32479;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#35745;&#37327;&#32463;&#27982;&#23398;&#27169;&#22411;&#22312;&#20998;&#26512;&#20302;&#22797;&#26434;&#24615;&#25968;&#25454;&#26102;&#21487;&#33021;&#36275;&#22815;&#65292;&#20294;&#30001;&#20110;&#24555;&#36895;&#25968;&#23383;&#21270;&#21644;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#29486;&#65292;&#32463;&#27982;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#26426;&#22120;&#23398;&#20064;&#27491;&#25104;&#20026;&#35745;&#37327;&#32463;&#27982;&#23398;&#23478;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides a curated review of selected papers published in prominent economics journals that use machine learning (ML) tools for research and policy analysis. The review focuses on three key questions: (1) when ML is used in economics, (2) what ML models are commonly preferred, and (3) how they are used for economic applications. The review highlights that ML is particularly used in processing nontraditional and unstructured data, capturing strong nonlinearity, and improving prediction accuracy. Deep learning models are suitable for nontraditional data, whereas ensemble learning models are preferred for traditional datasets. While traditional econometric models may suffice for analyzing low-complexity data, the increasing complexity of economic data due to rapid digitalization and the growing literature suggest that ML is becoming an essential addition to the econometrician's toolbox.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#25351;&#26631;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20004;&#20010;&#25351;&#26631;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#20114;&#30456;&#21463;&#30410;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.00061</link><description>&lt;p&gt;
&#22362;&#22266;&#19988;&#20844;&#27491;: &#30830;&#20445;&#20844;&#24179;&#19982;&#40065;&#26834;&#24615;&#30456;&#19968;&#33268;
&lt;/p&gt;
&lt;p&gt;
To be Robust and to be Fair: Aligning Fairness with Robustness. (arXiv:2304.00061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#25351;&#26631;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20004;&#20010;&#25351;&#26631;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#20114;&#30456;&#21463;&#30410;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#21487;&#38752;&#22320;&#25552;&#39640;&#23545;&#25239;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#23601;&#20844;&#24179;&#24615;&#32780;&#35328;&#65292;&#23545;&#25239;&#35757;&#32451;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#36866;&#24403;&#30740;&#31350;&#65292;&#20844;&#24179;&#24615;&#19982;&#20934;&#30830;&#24615;&#25915;&#20987;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;&#23545;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#40065;&#26834;&#24615;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#21644;&#23545;&#25239;&#25915;&#20987;&#23545;&#36825;&#20004;&#20010;&#25351;&#26631;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24615;&#25915;&#20987;&#30340;&#32479;&#19968;&#32467;&#26500;&#65292;&#23558;&#32676;&#20307;&#20844;&#24179;&#20013;&#30340;&#24120;&#35265;&#27010;&#24565;&#27719;&#38598;&#22312;&#19968;&#36215;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#19981;&#21516;&#27010;&#24565;&#19979;&#30340;&#20844;&#24179;&#24615;&#25915;&#20987;&#31561;&#20215;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#25915;&#20987;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#19968;&#31181;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#20250;&#21463;&#21040;&#21478;&#19968;&#31181;&#25351;&#26631;&#40065;&#26834;&#24615;&#30340;&#30410;&#22788;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#20844;&#24179;&#19982;&#20934;&#30830;&#24615;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Adversarial training has been shown to be reliable in improving robustness against adversarial samples. However, the problem of adversarial training in terms of fairness has not yet been properly studied, and the relationship between fairness and accuracy attack still remains unclear. Can we simultaneously improve robustness w.r.t. both fairness and accuracy? To tackle this topic, in this paper, we study the problem of adversarial training and adversarial attack w.r.t. both metrics. We propose a unified structure for fairness attack which brings together common notions in group fairness, and we theoretically prove the equivalence of fairness attack against different notions. Moreover, we show the alignment of fairness and accuracy attack, and theoretically demonstrate that robustness w.r.t. one metric benefits from robustness w.r.t. the other metric. Our study suggests a novel way to unify adversarial training and attack w.r.t. fairness and accuracy, and experimental results show that 
&lt;/p&gt;</description></item><item><title>Cyberlogic&#26159;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#21644;&#20998;&#26512;&#25968;&#23383;&#20132;&#26131;&#65292;&#21516;&#26102;&#28041;&#21450;&#25968;&#23383;&#35777;&#25454;&#30340;&#20132;&#25442;&#12290;&#23427;&#21487;&#20197;&#23450;&#20041;&#25480;&#26435;&#31574;&#30053;&#30340;&#26680;&#24515;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21487;&#20449;&#26102;&#38388;&#28304;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#23454;&#29616;&#20855;&#26377;&#34920;&#36798;&#26102;&#24577;&#30693;&#35782;&#30340;&#25480;&#26435;&#31574;&#30053;&#21644;&#21327;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.00060</link><description>&lt;p&gt;
&#22522;&#20110;&#35777;&#25454;&#30340;Cyberlogic&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
Evidential Transactions with Cyberlogic. (arXiv:2304.00060v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00060
&lt;/p&gt;
&lt;p&gt;
Cyberlogic&#26159;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#21644;&#20998;&#26512;&#25968;&#23383;&#20132;&#26131;&#65292;&#21516;&#26102;&#28041;&#21450;&#25968;&#23383;&#35777;&#25454;&#30340;&#20132;&#25442;&#12290;&#23427;&#21487;&#20197;&#23450;&#20041;&#25480;&#26435;&#31574;&#30053;&#30340;&#26680;&#24515;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21487;&#20449;&#26102;&#38388;&#28304;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#23454;&#29616;&#20855;&#26377;&#34920;&#36798;&#26102;&#24577;&#30693;&#35782;&#30340;&#25480;&#26435;&#31574;&#30053;&#21644;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cyberlogic&#26159;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#21644;&#20998;&#26512;&#25968;&#23383;&#20132;&#26131;&#30340;&#36923;&#36753;&#22522;&#30784;&#65292;&#36825;&#20123;&#20132;&#26131;&#28041;&#21450;&#25968;&#23383;&#35777;&#25454;&#30340;&#20132;&#25442;&#12290;&#23427;&#22522;&#20110;&#65288;&#19968;&#38454;&#65289;&#30452;&#35273;istic&#35859;&#35789;&#36923;&#36753;&#30340;&#25193;&#23637;&#65292;&#20855;&#26377;&#35777;&#26126;&#21644;&#30693;&#35782;&#24773;&#24577;&#12290;Cyberlogic&#30340;&#20851;&#38190;&#24605;&#24819;&#38750;&#24120;&#31616;&#21333;&#65292;&#21363;&#65288;1&#65289;&#20844;&#38053;&#23545;&#24212;&#25480;&#26435;&#65292;&#65288;2&#65289;&#20132;&#26131;&#34987;&#25351;&#23450;&#20026;&#20998;&#24067;&#24335;&#36923;&#36753;&#31243;&#24207;&#65292;&#65288;3&#65289;&#21487;&#39564;&#35777;&#30340;&#35777;&#25454;&#36890;&#36807;&#20998;&#24067;&#24335;&#35777;&#26126;&#25628;&#32034;&#25910;&#38598;&#12290;&#29305;&#21035;&#26159;&#21487;&#39564;&#35777;&#30340;&#35777;&#25454;&#26159;&#30001;&#39069;&#22806;&#30340;&#36923;&#36753;&#20803;&#32032;&#65292;&#22914;&#31614;&#32626;&#25991;&#20214;&#21644;&#21152;&#23494;&#31614;&#21517;&#26500;&#36896;&#30340;&#12290;&#23613;&#31649;Cyberlogic&#20855;&#26377;&#36825;&#31181;&#27010;&#24565;&#19978;&#30340;&#31616;&#21333;&#24615;&#65292;&#20294;&#25480;&#26435;&#31574;&#30053;&#30340;&#20013;&#24515;&#29305;&#24449;&#65292;&#21253;&#25324;&#20449;&#20219;&#65292;&#25480;&#26435;&#30340;&#22996;&#25176;&#21644;&#21514;&#38144;&#65292;&#26159;&#21487;&#23450;&#20041;&#30340;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21487;&#20449;&#26102;&#38388;&#28304;&#65292;&#21487;&#20197;&#22312;Cyberlogic&#20013;&#23450;&#20041;&#29992;&#20110;&#25351;&#23450;&#20998;&#24067;&#24335;&#25480;&#26435;&#31574;&#30053;&#21644;&#21327;&#35758;&#30340;&#34920;&#36798;&#26102;&#24577;&#30693;&#35782;&#36923;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyberlogic is an enabling logical foundation for building and analyzing digital transactions that involve the exchange of digital forms of evidence. It is based on an extension of (first-order) intuitionistic predicate logic with an attestation and a knowledge modality. The key ideas underlying Cyberlogic are extremely simple, as (1) public keys correspond to authorizations, (2) transactions are specified as distributed logic programs, and (3) verifiable evidence is collected by means of distributed proof search. Verifiable evidence, in particular, are constructed from extra-logical elements such as signed documents and cryptographic signatures. Despite this conceptual simplicity of Cyberlogic, central features of authorization policies including trust, delegation, and revocation of authority are definable. An expressive temporal-epistemic logic for specifying distributed authorization policies and protocols is therefore definable in Cyberlogic using a trusted time source. We describe 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#21487;&#20197;&#36890;&#36807;&#20998;&#21035;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#21644;&#36741;&#21161;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#21892;&#25506;&#32034;&#21644;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; NetHack &#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.00046</link><description>&lt;p&gt;
&#20351;&#29992;&#31163;&#32447;&#39044;&#35757;&#32451;&#21152;&#36895;&#25506;&#32034;&#21644;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating exploration and representation learning with offline pre-training. (arXiv:2304.00046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#21487;&#20197;&#36890;&#36807;&#20998;&#21035;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#21644;&#36741;&#21161;&#22870;&#21169;&#27169;&#22411;&#26469;&#25913;&#21892;&#25506;&#32034;&#21644;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; NetHack &#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20018;&#34892;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25165;&#33021;&#35299;&#20915;&#12290;&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#36890;&#36807;&#25913;&#36827;&#20449;&#29992;&#20998;&#37197;&#65292;&#24341;&#20837;&#35760;&#24518;&#33021;&#21147;&#65292;&#25913;&#21464;&#20195;&#29702;&#30340;&#20869;&#22312;&#21160;&#26426;&#65288;&#21363;&#25506;&#32034;&#65289;&#25110;&#20854;&#19990;&#30028;&#35266;&#65288;&#21363;&#30693;&#35782;&#34920;&#31034;&#65289;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#20013;&#30340;&#35768;&#22810;&#37117;&#21487;&#20197;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#36890;&#36807;&#20174;&#21333;&#20010;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#20998;&#21035;&#23398;&#20064;&#20004;&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#25913;&#21892;&#25506;&#32034;&#21644;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#20197;&#21450;&#20174;&#21333;&#20010;&#20154;&#31867;&#28436;&#31034;&#30340;&#27169;&#22411;&#36741;&#21161;&#22870;&#21169;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; NetHack &#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#28040;&#34701;&#20102;&#23454;&#39564;&#35774;&#32622;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#24182;&#31361;&#26174;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential decision-making agents struggle with long horizon tasks, since solving them requires multi-step reasoning. Most reinforcement learning (RL) algorithms address this challenge by improved credit assignment, introducing memory capability, altering the agent's intrinsic motivation (i.e. exploration) or its worldview (i.e. knowledge representation). Many of these components could be learned from offline data. In this work, we follow the hypothesis that exploration and representation learning can be improved by separately learning two different models from a single offline dataset. We show that learning a state representation using noise-contrastive estimation and a model of auxiliary reward separately from a single collection of human demonstrations can significantly improve the sample efficiency on the challenging NetHack benchmark. We also ablate various components of our experimental setting and highlight crucial insights.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SemiMemes&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;Memes&#30340;&#20998;&#26512;&#21644;&#27880;&#37322;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#26368;&#26032;&#30340;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.00020</link><description>&lt;p&gt;
SemiMemes&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;Memes&#20998;&#26512;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SemiMemes&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;Memes&#30340;&#20998;&#26512;&#21644;&#27880;&#37322;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#26368;&#26032;&#30340;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;Memes&#30340;&#26222;&#21450;&#24615;&#24341;&#21457;&#20102;&#20998;&#26512;&#20854;&#38544;&#21547;&#21547;&#20041;&#12289;&#23457;&#26597;&#26377;&#23475;&#20869;&#23481;&#30340;&#38656;&#27714;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;Meme&#23457;&#26597;&#31995;&#32479;&#38656;&#35201;&#21322;&#30417;&#30563;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21033;&#29992;&#20114;&#32852;&#32593;&#19978;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;Memes&#65292;&#24182;&#20351;&#27880;&#37322;&#36807;&#31243;&#21464;&#24471;&#26356;&#31616;&#21333;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#22240;&#20026;Memes&#30340;&#21547;&#20041;&#36890;&#24120;&#26469;&#33258;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#21363;&#22810;&#23186;&#20307;&#33258;&#21160;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;Memes&#25968;&#25454;&#38598;&#19978;&#65292;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#27169;&#22411;&#12290;&#20511;&#37492;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;SemiMemes&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#21644;&#20998;&#31867;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
The prevalence of memes on social media has created the need to sentiment analyze their underlying meanings for censoring harmful content. Meme censoring systems by machine learning raise the need for a semi-supervised learning solution to take advantage of the large number of unlabeled memes available on the internet and make the annotation process less challenging. Moreover, the approach needs to utilize multimodal data as memes' meanings usually come from both images and texts. This research proposes a multimodal semi-supervised learning approach that outperforms other multimodal semi-supervised learning and supervised learning state-of-the-art models on two datasets, the Multimedia Automatic Misogyny Identification and Hateful Memes dataset. Building on the insights gained from Contrastive Language-Image Pre-training, which is an effective multimodal learning technique, this research introduces SemiMemes, a novel training method that combines auto-encoder and classification task to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#20851;&#32852;&#20998;&#35299;&#32593;&#32476;&#65292;&#20351;&#29992;&#23618;&#27425;&#30456;&#20851;&#20256;&#25773;&#23558;&#32852;&#21512;&#20540;&#20989;&#25968;&#30340;&#23398;&#20064;&#21644;&#23616;&#37096;&#22870;&#21169;&#20449;&#21495;&#30340;&#29983;&#25104;&#20998;&#24320;&#65292;&#38024;&#23545;&#23384;&#22312;&#22823;&#37327;&#20887;&#20313;&#26234;&#33021;&#20307;&#30340;&#29615;&#22659;&#20013;&#65292;&#35813;&#31639;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.00009</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#20215;&#20540;&#22240;&#23376;&#21270;&#20013;&#20887;&#20313;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The challenge of redundancy on multi-agent value factorisation. (arXiv:2304.00009v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00009
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#20851;&#32852;&#20998;&#35299;&#32593;&#32476;&#65292;&#20351;&#29992;&#23618;&#27425;&#30456;&#20851;&#20256;&#25773;&#23558;&#32852;&#21512;&#20540;&#20989;&#25968;&#30340;&#23398;&#20064;&#21644;&#23616;&#37096;&#22870;&#21169;&#20449;&#21495;&#30340;&#29983;&#25104;&#20998;&#24320;&#65292;&#38024;&#23545;&#23384;&#22312;&#22823;&#37327;&#20887;&#20313;&#26234;&#33021;&#20307;&#30340;&#29615;&#22659;&#20013;&#65292;&#35813;&#31639;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#20013;&#65292;&#26631;&#20934;&#33539;&#24335;&#26159;&#20351;&#29992;&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#25955;&#24335;&#25191;&#34892;&#65292;&#22312;&#27492;&#33539;&#24335;&#20013;&#65292;&#20013;&#22830;&#25209;&#35780;&#23478;&#22522;&#20110;&#20013;&#22830;&#29366;&#24577;&#35843;&#33410;&#21512;&#20316;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#22823;&#37327;&#20887;&#20313;&#26234;&#33021;&#20307;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#21464;&#24471;&#19981;&#22826;&#26377;&#25928;&#12290;&#22312;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#19979;&#65292;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#25968;&#37327;&#21487;&#33021;&#27604;&#35299;&#20915;&#20219;&#21153;&#25152;&#38656;&#30340;&#25968;&#37327;&#35201;&#22810;&#12290;&#36825;&#20123;&#20887;&#20313;&#26234;&#33021;&#20307;&#36890;&#36807;&#22686;&#21152;&#29366;&#24577;&#31354;&#38388;&#30340;&#32500;&#24230;&#21644;&#22686;&#21152;&#29992;&#20110;&#35299;&#20915;&#29615;&#22659;&#30340;&#32852;&#21512;&#31574;&#30053;&#30340;&#22823;&#23567;&#26469;&#38477;&#20302;&#24615;&#33021;&#12290;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#23618;&#27425;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#26469;&#20998;&#31163;&#32852;&#21512;&#20540;&#20989;&#25968;&#30340;&#23398;&#20064;&#21644;&#23616;&#37096;&#22870;&#21169;&#20449;&#21495;&#30340;&#29983;&#25104;&#65292;&#24182;&#21019;&#24314;&#19968;&#31181;&#26032; &#30340;MARL&#31639;&#27861;&#65306;&#20851;&#32852;&#20998;&#35299;&#32593;&#32476;&#65288;RDN&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#20004;&#20010;&#22522;&#32447;VDN&#21644;Qmix&#30340;&#24615;&#33021;&#38543;&#30528;&#20887;&#20313;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#38477;&#20302;&#65292;&#20294;RDN&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20173;&#28982;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of cooperative multi-agent reinforcement learning (MARL), the standard paradigm is the use of centralised training and decentralised execution where a central critic conditions the policies of the cooperative agents based on a central state. It has been shown, that in cases with large numbers of redundant agents these methods become less effective. In a more general case, there is likely to be a larger number of agents in an environment than is required to solve the task. These redundant agents reduce performance by enlarging the dimensionality of both the state space and and increasing the size of the joint policy used to solve the environment. We propose leveraging layerwise relevance propagation (LRP) to instead separate the learning of the joint value function and generation of local reward signals and create a new MARL algorithm: relevance decomposition network (RDN). We find that although the performance of both baselines VDN and Qmix degrades with the number of redu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.00008</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Creativity of Large Language Models. (arXiv:2304.00008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00008
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#22312;&#39072;&#35206;&#20154;&#24037;&#26234;&#33021;&#30340;&#22810;&#20010;&#39046;&#22495;&#12290;&#20854;&#20013;&#26368;&#26174;&#33879;&#30340;&#24212;&#29992;&#20043;&#19968;&#26159;&#21019;&#20316;&#65292;&#20363;&#22914;&#35799;&#27468;&#25110;&#25925;&#20107;&#65306;&#29983;&#25104;&#30340;&#36755;&#20986;&#36890;&#24120;&#20855;&#26377;&#24778;&#20154;&#30340;&#36136;&#37327;&#12290;&#20294;&#26159;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;LLMs&#30495;&#30340;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21019;&#36896;&#24615;&#30340;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21019;&#36896;&#24615;&#29702;&#35770;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;LLMs&#30340;&#21457;&#23637;&#65292;&#25506;&#35752;&#20102;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#19982;LLMs&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#26041;&#38754;&#30830;&#23450;&#20102;&#19968;&#32452;&#8220;&#26131;&#8221;&#21644;&#8220;&#38590;&#8221;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arise: can LLMs really be considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. Then, we identify a set of "easy" and "hard" problems in machine creativity, discussing them in relation to LLMs. Finally, we analyze the societal impact of these technologies with a particular focus on the creative industries.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;AI-XR&#25163;&#26415;&#20803;&#23431;&#23449;&#30340;&#28508;&#22312;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#23454;&#29616;&#20854;&#28508;&#21147;&#25152;&#38656;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#23433;&#20840;&#23041;&#32961;&#23545;AI-XR&#25163;&#26415;&#20803;&#23431;&#23449;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.00007</link><description>&lt;p&gt;
&#33021;&#21542;&#20511;&#21161;AI-XR&#25163;&#26415;&#20803;&#23431;&#23449;&#37325;&#25391;&#20171;&#20837;&#24335;&#21307;&#30103;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Revitalize Interventional Healthcare with AI-XR Surgical Metaverses?. (arXiv:2304.00007v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;AI-XR&#25163;&#26415;&#20803;&#23431;&#23449;&#30340;&#28508;&#22312;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#23454;&#29616;&#20854;&#28508;&#21147;&#25152;&#38656;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#23433;&#20840;&#23041;&#32961;&#23545;AI-XR&#25163;&#26415;&#20803;&#23431;&#23449;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25216;&#26415;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#12289;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#20803;&#23431;&#23449;&#30340;&#36827;&#27493;&#20026;&#38761;&#26032;&#22806;&#31185;&#31185;&#23398;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#20154;&#24037;&#26234;&#33021;&#21644;&#22686;&#24378;&#29616;&#23454;&#65288;AI-XR&#65289;&#25216;&#26415;&#30340;&#32467;&#21512;&#26377;&#21487;&#33021;&#21019;&#24314;&#19968;&#20010;&#25163;&#26415;&#20803;&#23431;&#23449;&#65292;&#21363;&#25163;&#26415;&#21487;&#20197;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#35268;&#21010;&#21644;&#25191;&#34892;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;AI-XR&#25163;&#26415;&#20803;&#23431;&#23449;&#21508;&#31181;&#28508;&#22312;&#24212;&#29992;&#21644;&#24517;&#39035;&#24212;&#23545;&#30340;&#25361;&#25112;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#20851;&#27880;&#23545;&#20110;&#20805;&#20998;&#23454;&#29616;AI-XR&#25163;&#26415;&#20803;&#23431;&#23449;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#20026;&#24378;&#35843;&#38656;&#35201;&#23433;&#20840;&#21644;&#24378;&#22823;&#30340;AI-XR&#25163;&#26415;&#20803;&#23431;&#23449;&#30340;&#38656;&#27714;&#24182;&#23637;&#31034;&#23433;&#20840;&#23041;&#32961;&#23545;AI-XR&#25163;&#26415;&#20803;&#23431;&#23449;&#30340;&#23454;&#38469;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20854;&#20013;&#23637;&#31034;&#20102;&#8220;&#27785;&#28024;&#24335;&#25163;&#26415;&#25915;&#20987;&#8221;&#23545;&#20999;&#21475;&#28857;&#23450;&#20301;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in technology, particularly in machine learning (ML), deep learning (DL), and the metaverse, offer great potential for revolutionizing surgical science. The combination of artificial intelligence and extended reality (AI-XR) technologies has the potential to create a surgical metaverse, a virtual environment where surgeries can be planned and performed. This paper aims to provide insight into the various potential applications of an AI-XR surgical metaverse and the challenges that must be addressed to bring its full potential to fruition. It is important for the community to focus on these challenges to fully realize the potential of the AI-XR surgical metaverses. Furthermore, to emphasize the need for secure and robust AI-XR surgical metaverses and to demonstrate the real-world implications of security threats to the AI-XR surgical metaverses, we present a case study in which the ``an immersive surgical attack'' on incision point localization is performed in the co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#26053;&#34892;&#25252;&#29702;&#34892;&#19994;&#30340;&#25307;&#32856;&#26041;&#26696;&#65292;&#37319;&#29992;&#22810;&#27169;&#22411;&#25968;&#25454;&#26381;&#21153;&#21152;&#36895;&#25968;&#25454;&#37319;&#38598;&#65292;&#24182;&#20351;&#29992;&#21452;&#21521;&#24378;&#21270;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#34892;&#19994;&#26631;&#27880;&#25968;&#25454;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00006</link><description>&lt;p&gt;
&#38754;&#21521;&#26053;&#34892;&#25252;&#29702;&#34892;&#19994;&#30340;&#20351;&#29992;&#22810;&#27169;&#22411;&#25968;&#25454;&#26381;&#21153;&#30340;&#21452;&#21521;&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bi-directional personalization reinforcement learning-based architecture with active learning using a multi-model data service for the travel nursing industry. (arXiv:2304.00006v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#26053;&#34892;&#25252;&#29702;&#34892;&#19994;&#30340;&#25307;&#32856;&#26041;&#26696;&#65292;&#37319;&#29992;&#22810;&#27169;&#22411;&#25968;&#25454;&#26381;&#21153;&#21152;&#36895;&#25968;&#25454;&#37319;&#38598;&#65292;&#24182;&#20351;&#29992;&#21452;&#21521;&#24378;&#21270;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#34892;&#19994;&#26631;&#27880;&#25968;&#25454;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#22411;&#25968;&#25454;&#26381;&#21153;&#21152;&#36895;&#25968;&#25454;&#37319;&#38598;&#24182;&#20351;&#29992;&#21452;&#21521;&#24378;&#21270;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#20174;&#32780;&#22686;&#24378;&#26053;&#34892;&#25252;&#29702;&#34892;&#19994;&#30340;&#25307;&#32856;&#27969;&#31243;&#12290;&#35813;&#26041;&#26696;&#21487;&#24110;&#21161;&#25307;&#32856;&#20154;&#21592;&#25512;&#33616;&#21512;&#26684;&#30003;&#35831;&#20154;&#65292;&#24182;&#20351;&#30003;&#35831;&#20154;&#25509;&#25910;&#21040;&#20010;&#24615;&#21270;&#24037;&#20316;&#25512;&#33616;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#19968;&#34892;&#19994;&#26631;&#27880;&#25968;&#25454;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenges of using inadequate online recruitment systems can be addressed with machine learning and software engineering techniques. Bi-directional personalization reinforcement learning-based architecture with active learning can get recruiters to recommend qualified applicants and also enable applicants to receive personalized job recommendations. This paper focuses on how machine learning techniques can enhance the recruitment process in the travel nursing industry by helping speed up data acquisition using a multi-model data service and then providing personalized recommendations using bi-directional reinforcement learning with active learning. This need was especially evident when trying to respond to the overwhelming needs of healthcare facilities during the COVID-19 pandemic. The need for traveling nurses and other healthcare professionals was more evident during the lockdown period. A data service was architected for job feed processing using an orchestration of natural la
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#26032;&#30340;&#31895;&#31961;&#38543;&#26426;&#24615;&#27010;&#24565;&#65292;&#29992;&#20110;&#25429;&#25417;&#38745;&#24577;&#21644;&#21160;&#24577;&#25968;&#25454;&#30340;&#21508;&#31181;&#31895;&#31961;&#36807;&#31243;&#65292;&#23588;&#20854;&#26159;&#22312;&#26500;&#24314;&#31895;&#31961;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#21516;&#26102;&#20063;&#25506;&#32034;&#20102;&#36719;/&#30828;&#32858;&#31867;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20854;&#20013;&#65292;&#22823;&#24605;&#24819;&#32773;&#31895;&#31961;&#38543;&#26426;&#20989;&#25968;&#26159;&#26680;&#24515;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#35745;&#31639;&#26377;&#25928;&#30340;&#20195;&#25968;&#35777;&#26126;&#31639;&#27861;&#65292;&#29992;&#20110;&#36719;/&#30828;&#38598;&#32676;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.00005</link><description>&lt;p&gt;
&#31895;&#31961;&#38543;&#26426;&#24615;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rough Randomness and its Application. (arXiv:2304.00005v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#26032;&#30340;&#31895;&#31961;&#38543;&#26426;&#24615;&#27010;&#24565;&#65292;&#29992;&#20110;&#25429;&#25417;&#38745;&#24577;&#21644;&#21160;&#24577;&#25968;&#25454;&#30340;&#21508;&#31181;&#31895;&#31961;&#36807;&#31243;&#65292;&#23588;&#20854;&#26159;&#22312;&#26500;&#24314;&#31895;&#31961;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#21516;&#26102;&#20063;&#25506;&#32034;&#20102;&#36719;/&#30828;&#32858;&#31867;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20854;&#20013;&#65292;&#22823;&#24605;&#24819;&#32773;&#31895;&#31961;&#38543;&#26426;&#20989;&#25968;&#26159;&#26680;&#24515;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#35745;&#31639;&#26377;&#25928;&#30340;&#20195;&#25968;&#35777;&#26126;&#31639;&#27861;&#65292;&#29992;&#20110;&#36719;/&#30828;&#38598;&#32676;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#24050;&#26377;&#22810;&#31181;&#38543;&#26426;&#24615;&#21644;&#20449;&#24687;&#35770;&#38543;&#26426;&#24615;&#30340;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;&#31895;&#31961;&#25512;&#29702;&#65288;&#22240;&#27492;&#19981;&#33021;&#24456;&#22909;&#22320;&#24212;&#29992;&#20110;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#31946;&#21644;&#21160;&#24577;&#32972;&#26223;&#65289;&#12290;&#26412;&#30740;&#31350;&#30001;&#20316;&#32773;&#24341;&#20837;&#20102;&#31895;&#31961;&#38543;&#26426;&#24615;&#30340;&#26032;&#27010;&#24565;&#65292;&#23427;&#20204;&#26082;&#19981;&#26159;&#38543;&#26426;&#30340;&#65292;&#20063;&#19981;&#22522;&#20110;&#23383;&#31526;&#20018;&#30340;&#23646;&#24615;&#12290;&#36825;&#20123;&#27010;&#24565;&#26088;&#22312;&#25429;&#25417;&#21508;&#31181;&#31895;&#31961;&#36807;&#31243;&#65288;&#36866;&#29992;&#20110;&#38745;&#24577;&#21644;&#21160;&#24577;&#25968;&#25454;&#65289;&#65292;&#26500;&#24314;&#30456;&#20851;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#20013;&#65292;&#26368;&#21518;&#25552;&#21040;&#30340;&#26159;&#36719;/&#30828;&#32858;&#31867;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#20195;&#25968;&#35777;&#26126;&#31639;&#27861;&#65292;&#29992;&#20110;&#36719;&#38598;&#32676;&#21644;&#30828;&#38598;&#32676;&#39564;&#35777;&#65292;&#36825;&#20123;&#31639;&#27861;&#37117;&#28041;&#21450;&#21040;&#20102;&#31895;&#31961;&#38543;&#26426;&#20989;&#25968;&#12290;&#19968;&#31867;&#21517;&#20026;&#8220;&#22823;&#24605;&#24819;&#32773;&#8221;&#30340;&#31895;&#31961;&#38543;&#26426;&#20989;&#25968;&#22312;&#26412;&#25991;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#65292;&#24182;&#36890;&#36807;&#31034;&#20363;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#26500;&#24314;&#31895;&#31961;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A number of generalizations of stochastic and information-theoretic randomness are known in the literature. However, they are not compatible with handling meaning in vague and dynamic contexts of rough reasoning (and therefore explainable artificial intelligence and machine learning). In this research, new concepts of rough randomness that are neither stochastic nor based on properties of strings are introduced by the present author. Her concepts are intended to capture a wide variety of rough processes (applicable to both static and dynamic data), construct related models, and explore the validity of other machine learning algorithms. The last mentioned is restricted to soft/hard clustering algorithms in this paper. Two new computationally efficient algebraically-justified algorithms for soft and hard cluster validation that involve rough random functions are additionally proposed in this research. A class of rough random functions termed large-minded reasoners have a central role in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#39046;&#22495;&#26412;&#20307;&#35770;&#20013;&#30340;&#27010;&#24565;&#32416;&#32467;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#27010;&#24565;&#24314;&#27169;&#31574;&#30053;&#65292;&#26088;&#22312;&#26500;&#24314;&#27010;&#24565;&#19978;&#20998;&#31163;&#30340;&#39046;&#22495;&#26412;&#20307;&#35770;&#12290;</title><link>http://arxiv.org/abs/2304.00004</link><description>&lt;p&gt;
&#20998;&#31163;&#39046;&#22495;&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
Disentangling Domain Ontologies. (arXiv:2304.00004v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#39046;&#22495;&#26412;&#20307;&#35770;&#20013;&#30340;&#27010;&#24565;&#32416;&#32467;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#27010;&#24565;&#24314;&#27169;&#31574;&#30053;&#65292;&#26088;&#22312;&#26500;&#24314;&#27010;&#24565;&#19978;&#20998;&#31163;&#30340;&#39046;&#22495;&#26412;&#20307;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#35770;&#35777;&#20102;"&#27010;&#24565;&#32416;&#32467;"&#29616;&#35937;&#65292;&#35813;&#29616;&#35937;&#30001;&#20110;&#36880;&#27493;&#36328;&#36234;&#20197;&#19979;&#20116;&#20010;&#23618;&#27425;&#65288;&#24863;&#30693;&#12289;&#26631;&#35760;&#12289;&#35821;&#20041;&#23545;&#40784;&#12289;&#20998;&#23618;&#24314;&#27169;&#21644;&#20869;&#28085;&#23450;&#20041;&#65289;&#20043;&#38388;&#30340;&#34920;&#24449;&#22810;&#26679;&#24615;&#32780;&#20986;&#29616;&#65292;&#38543;&#21363;&#25105;&#20204;&#25552;&#20986;&#20102;"&#27010;&#24565;&#20998;&#31163;"&#30340;&#22810;&#23618;&#27010;&#24565;&#24314;&#27169;&#31574;&#30053;&#65292;&#20854;&#36890;&#36807;&#25351;&#23548;&#21407;&#21017;&#24378;&#21046;&#21644;&#38416;&#26126;&#27599;&#20010;&#27010;&#24565;&#32416;&#32544;&#23618;&#27425;&#65288;&#23545;&#25152;&#26377;&#19978;&#36848;&#20116;&#20010;&#23618;&#27425;&#65289;&#30340;&#35821;&#20041;&#21452;&#23556;&#65292;&#20026;&#26500;&#24314;&#27010;&#24565;&#19978;&#20998;&#31163;&#30340;&#39046;&#22495;&#26412;&#20307;&#35770;&#25195;&#28165;&#20102;&#36947;&#36335;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#31616;&#35201;&#38416;&#36848;&#20102;&#20026;&#20160;&#20040;&#24403;&#21069;&#30340;&#26412;&#20307;&#35770;&#24320;&#21457;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#22312;&#25105;&#20204;&#30340;&#29305;&#23450;&#20998;&#26512;&#19978;&#26159;&#19981;&#36275;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce and illustrate the novel phenomenon of Conceptual Entanglement which emerges due to the representational manifoldness immanent while incrementally modelling domain ontologies step-by-step across the following five levels: perception, labelling, semantic alignment, hierarchical modelling and intensional definition. In turn, we propose Conceptual Disentanglement, a multi-level conceptual modelling strategy which enforces and explicates, via guiding principles, semantic bijections with respect to each level of conceptual entanglement (across all the above five levels) paving the way for engineering conceptually disentangled domain ontologies. We also briefly argue why state-of-the-art ontology development methodologies and approaches are insufficient with respect to our characterization.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;(AI)&#19987;&#23478;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#20915;&#31574;&#25152;&#38656;&#25216;&#33021;&#24182;&#19981;&#20102;&#35299;&#30340;&#38382;&#39064;&#12290; &#34429;&#28982;&#24403;&#21069;&#30340;&#26426;&#22120;&#21487;&#20197;&#27169;&#25311;&#29305;&#23450;&#30340;&#20154;&#31867;&#23646;&#24615;&#65292;&#20294; AGI &#26159;&#22312;&#36825;&#26679;&#30340;&#21069;&#25552;&#19979;&#24320;&#21457;&#20986;&#26469;&#30340;&#65306;&#36825;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25193;&#23637;&#21040;&#19968;&#33324;&#26234;&#33021;&#27700;&#24179;&#12290;&#36825;&#20250;&#20998;&#25955;&#24403;&#21069;&#30740;&#31350;&#30340;&#27880;&#24847;&#21147;&#65292;&#36828;&#31163;&#30456;&#20851;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00002</link><description>&lt;p&gt;
&#20998;&#26512;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#35821;&#22659;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Contextual Shortcomings of Artificial General Intelligence. (arXiv:2304.00002v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00002
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;(AI)&#19987;&#23478;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#20915;&#31574;&#25152;&#38656;&#25216;&#33021;&#24182;&#19981;&#20102;&#35299;&#30340;&#38382;&#39064;&#12290; &#34429;&#28982;&#24403;&#21069;&#30340;&#26426;&#22120;&#21487;&#20197;&#27169;&#25311;&#29305;&#23450;&#30340;&#20154;&#31867;&#23646;&#24615;&#65292;&#20294; AGI &#26159;&#22312;&#36825;&#26679;&#30340;&#21069;&#25552;&#19979;&#24320;&#21457;&#20986;&#26469;&#30340;&#65306;&#36825;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25193;&#23637;&#21040;&#19968;&#33324;&#26234;&#33021;&#27700;&#24179;&#12290;&#36825;&#20250;&#20998;&#25955;&#24403;&#21069;&#30740;&#31350;&#30340;&#27880;&#24847;&#21147;&#65292;&#36828;&#31163;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#26159;&#26368;&#23574;&#31471;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#39033;&#30446;&#65292;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#20063;&#21313;&#20998;&#26126;&#26174;&#12290;&#23613;&#31649;&#36825;&#31181;&#24046;&#24322;&#26412;&#36136;&#19978;&#23558;&#27599;&#20010;&#20154;&#30340;&#33021;&#21147;&#21010;&#20998;&#24320;&#26469;&#65292;&#20294;&#20154;&#31867;&#32423;&#21035;&#30340;&#26234;&#33021;(HLI)&#24050;&#32463;&#26159;AGI&#20960;&#21313;&#24180;&#26469;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#21453;&#23545;&#22270;&#28789;&#27979;&#35797;&#30340;&#20108;&#20803;&#35770;&#65292;&#21363;&#23558;&#20854;&#20316;&#20026;&#28508;&#22312;&#26234;&#33021;&#26426;&#22120;&#30340;&#22522;&#30784;&#21644;&#21407;&#22987;&#24314;&#31435;&#30340;&#24847;&#22270;&#12290;&#23427;&#35752;&#35770;&#20102;AI&#19987;&#23478;&#22914;&#20309;&#35823;&#35299;&#27169;&#20223;&#28216;&#25103;&#20316;&#20026;&#23545;&#35745;&#31639;&#26426;&#31995;&#32479;&#36827;&#34892;&#25311;&#20154;&#21270;&#30340;&#25163;&#27573;&#65292;&#24182;&#26029;&#35328;HLI&#26159;&#19968;&#20010;&#36716;&#31227;&#27880;&#24847;&#21147;&#12289;&#20351;&#24403;&#21069;&#30740;&#31350;&#36828;&#31163;&#30456;&#20851;&#38382;&#39064;&#30340;&#38169;&#35823;&#26041;&#21521;&#12290;&#23613;&#31649;&#23545;AGI&#24212;&#29992;&#30340;&#28508;&#22312;&#35774;&#35745;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#21364;&#24456;&#23569;&#32771;&#34385;&#36825;&#26679;&#19968;&#20010;&#31995;&#32479;&#22914;&#20309;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#27700;&#24179;&#35775;&#38382;&#21644;&#25668;&#21462;&#25968;&#25454;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#26426;&#22120;&#21487;&#33021;&#27169;&#25311;&#29305;&#23450;&#30340;&#20154;&#31867;&#23646;&#24615;&#65292;&#20294;AGI&#26159;&#22312;&#36825;&#26679;&#30340;&#21069;&#25552;&#19979;&#24320;&#21457;&#20986;&#26469;&#30340;&#65306;&#36825;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25193;&#23637;&#21040;&#19968;&#33324;&#26234;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even in the most cutting-edge Artificial General Intelligence (AGI) endeavors, the disparity between humans and artificial systems is extremely apparent. Although this difference fundamentally divides the capabilities of each, human-level intelligence (HLI) has remained the aim of AGI for decades. This paper opposes the binarity of the Turing Test, the foundation of this intention and original establishment of a potentially intelligent machine. It discusses how AI experts misinterpreted the Imitation Game as a means to anthropomorphize computer systems and asserts that HLI is a red herring that distracts current research from relevant problems. Despite the extensive research on the potential design of an AGI application, there has been little consideration of how such a system will access and ingest data at a human-like level. Although current machines may emulate specific human attributes, AGI is developed under the pretense that this can be easily scaled up to a general intelligence 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22686;&#37327;&#29677;&#32423;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#35768;&#22810;&#26041;&#27861;&#22312;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#23384;&#20648;&#26041;&#38754;&#38750;&#24120;&#20302;&#25928;&#12290;&#20026;&#20102;&#20351;&#36845;&#20195;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#65292;&#30740;&#31350;&#30028;&#19981;&#33021;&#24573;&#35270;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.18171</link><description>&lt;p&gt;
&#20170;&#22825;&#30340;&#36845;&#20195;&#23398;&#20064;&#31639;&#27861;&#26377;&#22810;&#39640;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Efficient Are Today's Continual Learning Algorithms?. (arXiv:2303.18171v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18171
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22686;&#37327;&#29677;&#32423;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#35768;&#22810;&#26041;&#27861;&#22312;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#23384;&#20648;&#26041;&#38754;&#38750;&#24120;&#20302;&#25928;&#12290;&#20026;&#20102;&#20351;&#36845;&#20195;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#65292;&#30740;&#31350;&#30028;&#19981;&#33021;&#24573;&#35270;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#36845;&#20195;&#23398;&#20064;&#28041;&#21450;&#20174;&#19981;&#26029;&#22686;&#38271;&#30340;&#24102;&#26631;&#31614;&#25968;&#25454;&#27969;&#20013;&#26356;&#26032;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#19978;&#65292;&#20294;&#36845;&#20195;&#23398;&#20064;&#32972;&#21518;&#30340;&#20027;&#35201;&#21160;&#26426;&#20043;&#19968;&#26159;&#33021;&#22815;&#26377;&#25928;&#22320;&#26356;&#26032;&#32593;&#32476;&#65292;&#32780;&#19981;&#26159;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#38598;&#38543;&#26102;&#38388;&#22686;&#38271;&#65292;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36845;&#20195;&#23398;&#20064;&#26041;&#27861;&#22522;&#26412;&#19978;&#35299;&#20915;&#20102;&#28798;&#38590;&#36951;&#24536;&#38382;&#39064;&#65292;&#20294;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#25928;&#29575;&#20851;&#27880;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22686;&#37327;&#29677;&#32423;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#35768;&#22810;&#26041;&#27861;&#22312;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#23384;&#20648;&#26041;&#38754;&#38750;&#24120;&#20302;&#25928;&#12290;&#26377;&#20123;&#26041;&#27861;&#29978;&#33267;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#23436;&#25104;&#35757;&#32451;&#65281;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#20351;&#36845;&#20195;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#65292;&#30740;&#31350;&#30028;&#19981;&#33021;&#24573;&#35270;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;&#36845;&#20195;&#23398;&#20064;&#19981;&#20165;&#20165;&#26159;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised Continual learning involves updating a deep neural network (DNN) from an ever-growing stream of labeled data. While most work has focused on overcoming catastrophic forgetting, one of the major motivations behind continual learning is being able to efficiently update a network with new information, rather than retraining from scratch on the training dataset as it grows over time. Despite recent continual learning methods largely solving the catastrophic forgetting problem, there has been little attention paid to the efficiency of these algorithms. Here, we study recent methods for incremental class learning and illustrate that many are highly inefficient in terms of compute, memory, and storage. Some methods even require more compute than training from scratch! We argue that for continual learning to have real-world applicability, the research community cannot ignore the resources used by these algorithms. There is more to continual learning than mitigating catastrophic forg
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#20307;&#37096;&#20214;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29289;&#20307;&#36816;&#21160;&#21160;&#21147;&#23398;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#26816;&#27979;&#24213;&#23618;3D&#32467;&#26500;&#20013;&#30340;&#31227;&#21160;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2303.17774</link><description>&lt;p&gt;
&#21322;&#24369;&#30417;&#30563;&#19979;&#30340;&#29289;&#20307;&#36816;&#21160;&#21160;&#21147;&#23398;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semi-Weakly Supervised Object Kinematic Motion Prediction. (arXiv:2303.17774v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#20307;&#37096;&#20214;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29289;&#20307;&#36816;&#21160;&#21160;&#21147;&#23398;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#26816;&#27979;&#24213;&#23618;3D&#32467;&#26500;&#20013;&#30340;&#31227;&#21160;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;3D&#29289;&#20307;&#65292;&#36816;&#21160;&#21160;&#21147;&#23398;&#39044;&#27979;&#26088;&#22312;&#30830;&#23450;&#31227;&#21160;&#37096;&#20214;&#20197;&#21450;&#30456;&#24212;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#30001;&#20110;3D&#29289;&#20307;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#20960;&#20309;&#32454;&#33410;&#30340;&#24040;&#22823;&#21464;&#21270;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#32570;&#20047;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#20063;&#38480;&#21046;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20197;&#21322;&#24369;&#30417;&#30563;&#26041;&#24335;&#35299;&#20915;&#20102;&#29289;&#20307;&#36816;&#21160;&#21160;&#21147;&#23398;&#39044;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#32467;&#26524;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#23613;&#31649;&#20840;&#38754;&#27880;&#37322;&#36816;&#21160;&#26631;&#31614;&#30340;3D&#25968;&#25454;&#38598;&#26159;&#26377;&#38480;&#30340;&#65292;&#20294;&#23384;&#22312;&#22823;&#35268;&#27169;&#30340;&#29289;&#20307;&#37096;&#20214;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#35821;&#20041;&#37096;&#20998;&#20998;&#21106;&#21644;&#31227;&#21160;&#37096;&#20998;&#20998;&#21106;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#30340;&#65292;&#20294;&#21487;&#20197;&#20174;&#24213;&#23618;3D&#32467;&#26500;&#20013;&#26816;&#27979;&#20986;&#31227;&#21160;&#37096;&#20998;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#20998;&#23618;&#37096;&#20214;&#32423;&#21035;&#20998;&#21106;&#21644;&#31227;&#21160;&#37096;&#20214;&#21442;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a 3D object, kinematic motion prediction aims to identify the mobile parts as well as the corresponding motion parameters. Due to the large variations in both topological structure and geometric details of 3D objects, this remains a challenging task and the lack of large scale labeled data also constrain the performance of deep learning based approaches. In this paper, we tackle the task of object kinematic motion prediction problem in a semi-weakly supervised manner. Our key observations are two-fold. First, although 3D dataset with fully annotated motion labels is limited, there are existing datasets and methods for object part semantic segmentation at large scale. Second, semantic part segmentation and mobile part segmentation is not always consistent but it is possible to detect the mobile parts from the underlying 3D structure. Towards this end, we propose a graph neural network to learn the map between hierarchical part-level segmentation and mobile parts parameters, which 
&lt;/p&gt;</description></item><item><title>&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17580</link><description>&lt;p&gt;
HuggingGPT: &#22312;HugingFace&#20013;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20249;&#20276;&#35299;&#20915;AI&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17580
&lt;/p&gt;
&lt;p&gt;
&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22797;&#26434;AI&#20219;&#21153;&#26159;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#31649;&#29702;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#20197;&#35299;&#20915;AI&#20219;&#21153;&#65292;&#35821;&#35328;&#25104;&#20026;&#36890;&#29992;&#25509;&#21475;&#26469;&#36171;&#33021;&#23427;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#26681;&#25454;HuggingFace&#20013;&#21487;&#29992;&#30340;&#27169;&#22411;&#21151;&#33021;&#25551;&#36848;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#22312;&#36873;&#23450;AI&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17573</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#23478;&#20013;&#27979;&#37327;&#24085;&#37329;&#26862;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Using AI to Measure Parkinson's Disease Severity at Home. (arXiv:2303.17573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25317;&#26377;&#36739;&#39640;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36828;&#31243;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#36816;&#21160;&#34920;&#29616;&#30340;&#26041;&#27861;&#12290;&#21442;&#19982;&#32773;&#22312;&#32593;&#32476;&#25668;&#20687;&#22836;&#21069;&#23436;&#25104;&#20102;&#36816;&#21160;&#20219;&#21153;&#65288;&#21363;&#28857;&#20987;&#25163;&#25351;&#65289;&#65292;250&#21517;&#20840;&#29699;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#25353;&#29031;&#36816;&#21160;&#38556;&#30861;&#21327;&#20250;&#32479;&#19968;&#24085;&#37329;&#26862;&#30149;&#35780;&#20998;&#37327;&#34920; (MDS-UPDRS) &#30340;&#26631;&#20934;&#30001;&#19977;&#21517;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#38752;&#24615;&#65292;&#20869;&#37096;&#19968;&#33268;&#24615;&#31995;&#25968;&#65288;ICC&#65289;&#20026;0.88&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#35745;&#31639;&#26426;&#31639;&#27861;&#26469;&#33719;&#24471;&#19982;MDS-UPDRS&#25351;&#21335;&#19968;&#33268;&#19988;&#19982;&#31070;&#32463;&#23398;&#23478;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#30340;&#23458;&#35266;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#25351;&#26631;&#30340;&#35757;&#32451;&#19979;&#34920;&#29616;&#20248;&#20110;&#19968;&#20010;MDS-UPDRS&#35748;&#35777;&#30340;&#35780;&#20998;&#32773;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;0.59&#65292;&#32780;&#35780;&#20998;&#32773;&#30340;MAE&#20026;0.79&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#34920;&#29616;&#30053;&#36874;&#20110;&#19987;&#23478;&#31070;&#32463;&#23398;&#23478;&#65288;0.53 MAE&#65289;&#12290;&#35813;&#26041;&#27861;&#21487;&#37325;&#22797;&#29992;&#20110;&#31867;&#20284;&#30340;&#36816;&#21160;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an artificial intelligence system to remotely assess the motor performance of individuals with Parkinson's disease (PD). Participants performed a motor task (i.e., tapping fingers) in front of a webcam, and data from 250 global participants were rated by three expert neurologists following the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The neurologists' ratings were highly reliable, with an intra-class correlation coefficient (ICC) of 0.88. We developed computer algorithms to obtain objective measurements that align with the MDS-UPDRS guideline and are strongly correlated with the neurologists' ratings. Our machine learning model trained on these measures outperformed an MDS-UPDRS certified rater, with a mean absolute error (MAE) of 0.59 compared to the rater's MAE of 0.79. However, the model performed slightly worse than the expert neurologists (0.53 MAE). The methodology can be replicated for similar motor tasks, providing the possibili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#21033;&#29992;&#21512;&#25104;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#21487;&#35270;&#35821;&#38899;&#35782;&#21035;&#65288;VSR&#65289;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;SynthVSR&#36890;&#36807;&#21033;&#29992;&#35821;&#38899;&#39537;&#21160;&#30340;&#21767;&#37096;&#21160;&#30011;&#27169;&#22411;&#21512;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;VSR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17200</link><description>&lt;p&gt;
SynthVSR&#65306;&#20351;&#29992;&#21512;&#25104;&#30417;&#30563;&#23454;&#29616;&#21487;&#35270;&#35821;&#38899;&#35782;&#21035;&#30340;&#35268;&#27169;&#21270;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision. (arXiv:2303.17200v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#21033;&#29992;&#21512;&#25104;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#21487;&#35270;&#35821;&#38899;&#35782;&#21035;&#65288;VSR&#65289;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;SynthVSR&#36890;&#36807;&#21033;&#29992;&#35821;&#38899;&#39537;&#21160;&#30340;&#21767;&#37096;&#21160;&#30011;&#27169;&#22411;&#21512;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;VSR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#21487;&#35270;&#35821;&#38899;&#35782;&#21035;&#65288;VSR&#65289;&#39046;&#22495;&#20013;&#25253;&#36947;&#30340;&#26368;&#26032;&#25104;&#26524;&#36890;&#24120;&#20381;&#36182;&#20110;&#36234;&#26469;&#36234;&#22810;&#30340;&#35270;&#39057;&#25968;&#25454;&#65292;&#32780;&#20844;&#24320;&#21487;&#29992;&#30340;&#36716;&#24405;&#35270;&#39057;&#25968;&#25454;&#38598;&#22823;&#23567;&#26377;&#38480;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#21033;&#29992;&#21512;&#25104;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;VSR&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8220;SynthVSR&#8221;&#36890;&#36807;&#21512;&#25104;&#22068;&#21767;&#21160;&#20316;&#26174;&#33879;&#25552;&#39640;&#20102;VSR&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;SynthVSR&#32972;&#21518;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#19968;&#20010;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;&#21767;&#37096;&#21160;&#30011;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26681;&#25454;&#36755;&#20837;&#35821;&#38899;&#29983;&#25104;&#21767;&#37096;&#21160;&#20316;&#12290;&#35813;&#35821;&#38899;&#39537;&#21160;&#30340;&#21767;&#37096;&#21160;&#30011;&#27169;&#22411;&#26159;&#22312;&#26410;&#26631;&#35760;&#30340;&#38899;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26377;&#26631;&#35760;&#30340;&#35270;&#39057;&#21487;&#29992;&#26102;&#36827;&#19968;&#27493;&#20248;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;VSR&#27169;&#22411;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#30340;&#36716;&#24405;&#22768;&#23398;&#25968;&#25454;&#21644;&#38754;&#37096;&#22270;&#20687;&#65292;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#21767;&#37096;&#21160;&#30011;&#27169;&#22411;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#25968;&#25454;&#29992;&#20110;&#21322;&#30417;&#30563;VSR&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#8220;Lip Reading in the Wild&#8221;&#65288;Lrw&#65289;&#35780;&#27979;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently reported state-of-the-art results in visual speech recognition (VSR) often rely on increasingly large amounts of video data, while the publicly available transcribed video datasets are limited in size. In this paper, for the first time, we study the potential of leveraging synthetic visual data for VSR. Our method, termed SynthVSR, substantially improves the performance of VSR systems with synthetic lip movements. The key idea behind SynthVSR is to leverage a speech-driven lip animation model that generates lip movements conditioned on the input speech. The speech-driven lip animation model is trained on an unlabeled audio-visual dataset and could be further optimized towards a pre-trained VSR model when labeled videos are available. As plenty of transcribed acoustic data and face images are available, we are able to generate large-scale synthetic data using the proposed lip animation model for semi-supervised VSR training. We evaluate the performance of our approach on the la
&lt;/p&gt;</description></item><item><title>ChatGPT-4&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#23545;&#35805;&#26426;&#22120;&#20154;&#65292;&#33021;&#36798;&#21040;&#25509;&#36817;&#20110;&#29289;&#29702;&#19987;&#23478;&#27700;&#24179;&#30340;&#21147;&#27010;&#24565;&#27979;&#35797;&#25104;&#32489;&#65292;&#23545;&#26410;&#26469;&#30340;&#29289;&#29702;&#25945;&#32946;&#21644;&#25945;&#23398;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.17012</link><description>&lt;p&gt;
ChatGPT-4&#20013;&#26174;&#33879;&#27010;&#24565;&#29289;&#29702;&#25512;&#29702;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advances in apparent conceptual physics reasoning in ChatGPT-4. (arXiv:2303.17012v1 [physics.ed-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17012
&lt;/p&gt;
&lt;p&gt;
ChatGPT-4&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#23545;&#35805;&#26426;&#22120;&#20154;&#65292;&#33021;&#36798;&#21040;&#25509;&#36817;&#20110;&#29289;&#29702;&#19987;&#23478;&#27700;&#24179;&#30340;&#21147;&#27010;&#24565;&#27979;&#35797;&#25104;&#32489;&#65292;&#23545;&#26410;&#26469;&#30340;&#29289;&#29702;&#25945;&#32946;&#21644;&#25945;&#23398;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#24314;&#31435;&#22312;&#19968;&#20010;&#24040;&#22823;&#30340;&#20154;&#31867;&#25991;&#26412;&#20449;&#24687;&#24211;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#12290;&#26368;&#36817;Kortemeyer&#65288;2023&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#29289;&#29702;&#23450;&#24459;&#30340;&#26126;&#30830;&#32534;&#31243;&#25351;&#23548;&#65292;ChatGPT-3.5&#21487;&#20197;&#36890;&#36807;&#19968;&#20123;&#21517;&#20041;&#27700;&#24179;&#30340;&#20837;&#38376;&#29289;&#29702;&#35838;&#31243;&#65292;&#24182;&#22312;&#21147;&#23398;&#30340;&#21147;&#27010;&#24565;&#27979;&#35797;&#20013;&#24471;&#21040;&#25509;&#36817;&#26368;&#23567;&#29702;&#35299;&#30340;&#25104;&#32489;&#12290;&#26412;&#30740;&#31350;&#22797;&#21046;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#26368;&#26032;&#29256;&#26412;ChatGPT-4&#22312;&#35813;&#29615;&#22659;&#19979;&#30340;&#25104;&#32489;&#36828;&#39640;&#20110;&#21069;&#29256;&#26412;&#65292;&#22312;&#19968;&#20123;&#38750;&#24120;&#20540;&#24471;&#27880;&#24847;&#30340;&#20363;&#22806;&#21644;&#38480;&#21046;&#26465;&#20214;&#19979;&#65292;&#20854;&#22238;&#31572;&#38750;&#24120;&#25509;&#36817;&#20110;&#23436;&#32654;&#22320;&#23637;&#31034;&#19987;&#23478;&#27700;&#24179;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#31616;&#35201;&#35780;&#36848;&#20102;&#36825;&#23545;&#20110;&#26410;&#26469;&#29289;&#29702;&#25945;&#32946;&#21644;&#25945;&#23398;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is built on a large language model trained on an enormous corpus of human text to emulate human conversation. Despite lacking any explicit programming regarding the laws of physics, recent work by Kortemeyer (2023) has demonstrated that ChatGPT-3.5 could pass an introductory physics course at some nominal level and register something close to a minimal understanding of Newtonian Mechanics on the Force Concept Inventory. This work replicates those results and also demonstrates that the latest version, ChatGPT-4, has reached a much higher mark in the latter context. Indeed, its responses come quite close to perfectly demonstrating expert-level competence, with a few very notable exceptions and limitations. We briefly comment on the implications of this for the future of physics education and pedagogy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#37096;&#20998;&#35266;&#27979;&#31995;&#32479;&#30340;&#26368;&#22351;&#24773;&#20917;&#25511;&#21046;&#21644;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#30340;&#26377;&#38480;&#20540;&#19981;&#30830;&#23450;&#21464;&#37327;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#23450;&#20041;&#20449;&#24687;&#29366;&#24577;&#26469;&#25552;&#39640;&#21160;&#24577;&#35268;&#21010;&#30340;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#21487;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26500;&#24314;&#25110;&#23398;&#20064;&#30340;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.16321</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#21033;&#29992;&#37096;&#20998;&#35266;&#27979;&#36827;&#34892;&#26368;&#22351;&#24773;&#20917;&#25511;&#21046;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Worst-Case Control and Learning Using Partial Observations Over an Infinite Time-Horizon. (arXiv:2303.16321v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#37096;&#20998;&#35266;&#27979;&#31995;&#32479;&#30340;&#26368;&#22351;&#24773;&#20917;&#25511;&#21046;&#21644;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#30340;&#26377;&#38480;&#20540;&#19981;&#30830;&#23450;&#21464;&#37327;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#23450;&#20041;&#20449;&#24687;&#29366;&#24577;&#26469;&#25552;&#39640;&#21160;&#24577;&#35268;&#21010;&#30340;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#21487;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26500;&#24314;&#25110;&#23398;&#20064;&#30340;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#20851;&#38190;&#30340;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#38656;&#35201;&#33391;&#22909;&#30340;&#25511;&#21046;&#31574;&#30053;&#26469;&#24212;&#23545;&#25932;&#23545;&#24178;&#25200;&#21644;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#37096;&#20998;&#35266;&#27979;&#31995;&#32479;&#36827;&#34892;&#36817;&#20284;&#25511;&#21046;&#21644;&#23398;&#20064;&#65292;&#20197;&#26368;&#23567;&#21270;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#26368;&#22351;&#24773;&#20917;&#36148;&#29616;&#25104;&#26412;&#12290;&#25105;&#20204;&#23558;&#23545;&#31995;&#32479;&#30340;&#24178;&#25200;&#24314;&#27169;&#20026;&#20855;&#26377;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#30340;&#26377;&#38480;&#20540;&#19981;&#30830;&#23450;&#21464;&#37327;&#12290;&#23545;&#20110;&#24050;&#30693;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#20998;&#35299;&#26469;&#35745;&#31639;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#36129;&#29486;&#26159;&#23450;&#20041;&#20449;&#24687;&#29366;&#24577;&#65292;&#25552;&#39640;&#20102;DP&#30340;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#65292;&#32780;&#19981;&#25439;&#22833;&#26368;&#20248;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31867;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#20135;&#29983;&#21487;&#35266;&#27979;&#25104;&#26412;&#30340;&#38382;&#39064;&#30340;&#31616;&#21270;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#23450;&#20041;&#20102;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#30452;&#25509;&#26500;&#24314;&#25110;&#23398;&#20064;&#30340;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety-critical cyber-physical systems require control strategies whose worst-case performance is robust against adversarial disturbances and modeling uncertainties. In this paper, we present a framework for approximate control and learning in partially observed systems to minimize the worst-case discounted cost over an infinite time-horizon. We model disturbances to the system as finite-valued uncertain variables with unknown probability distributions. For problems with known system dynamics, we construct a dynamic programming (DP) decomposition to compute the optimal control strategy. Our first contribution is to define information states that improve the computational tractability of this DP without loss of optimality. Then, we describe a simplification for a class of problems where the incurred cost is observable at each time-instance. Our second contribution is a definition of approximate information states that can be constructed or learned directly from observed data for problem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#22312;&#21307;&#30103;&#25104;&#20687;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#26696;&#20013;&#19982;&#20351;&#29992;&#36719;&#26631;&#31614;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.16296</link><description>&lt;p&gt;
Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#65306;&#29992;&#36719;&#26631;&#31614;&#20248;&#21270;Dice&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels. (arXiv:2303.16296v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#22312;&#21307;&#30103;&#25104;&#20687;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#26696;&#20013;&#19982;&#20351;&#29992;&#36719;&#26631;&#31614;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#30340;&#35768;&#22810;&#33258;&#21160;&#20998;&#21106;&#26041;&#26696;&#20013;&#65292;&#36719;Dice&#25439;&#22833;&#65288;SDL&#65289;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#25581;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#32972;&#21518;&#30340;&#19968;&#20123;&#21407;&#22240;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#20854;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23454;&#29616;&#25903;&#25345;&#30452;&#25509;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#23427;&#30340;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#22312;&#20351;&#29992;SDL&#21644;&#30740;&#31350;&#21033;&#29992;&#36719;&#26631;&#31614;&#30340;&#21516;&#26102;&#36827;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21327;&#21516;&#20316;&#29992;&#20173;&#28982;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#65288;DMLs&#65289;&#65292;&#23427;&#20204;&#65288;i&#65289;&#22312;&#30828;&#26631;&#31614;&#30340;&#26631;&#20934;&#35774;&#32622;&#19979;&#19982;SDL&#30456;&#21516;&#65292;&#20294;&#65288;ii&#65289;&#20063;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#30340;QUBIQ&#12289;LiTS&#21644;KiTS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;DMLs&#19982;&#36719;&#26631;&#31614;&#65288;&#22914;&#24179;&#22343;&#12289;&#26631;&#31614;&#24179;&#28369;&#21644;&#30693;&#35782;&#33976;&#39311;&#65289;&#30340;&#28508;&#22312;&#21327;&#21516;&#20316;&#29992;&#65292;&#32780;DMLs&#19982;&#30828;&#26631;&#31614;&#65288;&#22914;&#22823;&#22810;&#25968;&#25237;&#31080;&#21644;&#38543;&#26426;&#36873;&#25321;&#65289;&#30456;&#27604;&#65292;&#20135;&#29983;&#20102;&#26356;&#20248;&#31168;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The soft Dice loss (SDL) has taken a pivotal role in many automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct use in settings with soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be used in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g. averaging, label smoothing, and knowledge distillation) over hard labels (e.g. majority voting and random selection). As a result, we obtain superior Dice scores and model calib
&lt;/p&gt;</description></item><item><title>TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.15954</link><description>&lt;p&gt;
TraffNet&#65306;&#23398;&#20064;&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#20132;&#36890;&#29983;&#25104;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15954
&lt;/p&gt;
&lt;p&gt;
TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#65288;RNDT&#65289;&#22312;&#24320;&#21457;&#19979;&#19968;&#20195;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20132;&#36890;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#65292;RNDT&#38656;&#35201;&#19968;&#20010;&#27169;&#22411;&#65292;&#20174;&#22312;&#32447;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#21160;&#24577;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#27169;&#25311;&#32467;&#26524;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24403;&#21069;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25216;&#26415;&#20165;&#36890;&#36807;&#25366;&#25496;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#26469;&#39044;&#27979;&#26410;&#26469;&#20132;&#36890;&#65292;&#32780;&#24573;&#30053;&#20102;&#20132;&#36890;&#29983;&#25104;&#30340;&#21407;&#22240;&#65292;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#36335;&#24452;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23454;&#26102;&#20915;&#31574;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; TraffNet&#65292;&#35813;&#26694;&#26550;&#20174;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#34920;&#31034;&#36947;&#36335;&#32593;&#32476;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24182;&#20837;&#39044;&#27979;&#25152;&#38656;&#30340;&#20854;&#20182;&#25968;&#25454;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BFP&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#26032;&#29305;&#24449;&#21464;&#25442;&#20026;&#26087;&#29305;&#24449;&#30340;&#32447;&#24615;&#21464;&#25442;&#26469;&#32500;&#25252;&#32447;&#24615;&#21487;&#20998;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#26032;&#29305;&#24449;&#26041;&#21521;&#30340;&#20986;&#29616;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#26087;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.14595</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#29305;&#24449;&#25237;&#24433;&#22312;&#19981;&#26029;&#23398;&#20064;&#20013;&#32500;&#25252;&#32447;&#24615;&#21487;&#20998;&#24615;
&lt;/p&gt;
&lt;p&gt;
Preserving Linear Separability in Continual Learning by Backward Feature Projection. (arXiv:2303.14595v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14595
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BFP&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#26032;&#29305;&#24449;&#21464;&#25442;&#20026;&#26087;&#29305;&#24449;&#30340;&#32447;&#24615;&#21464;&#25442;&#26469;&#32500;&#25252;&#32447;&#24615;&#21487;&#20998;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#26032;&#29305;&#24449;&#26041;&#21521;&#30340;&#20986;&#29616;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#26087;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#23398;&#20064;&#20013;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#27169;&#22411;&#38656;&#35201;&#22312;&#26377;&#38480;&#25110;&#27809;&#26377;&#20197;&#21069;&#26597;&#30475;&#20219;&#21153;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#24182;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#30452;&#25509;&#32422;&#26463;&#26032;&#29305;&#24449;&#20197;&#21305;&#37197;&#26087;&#29305;&#24449;&#65292;&#24573;&#35270;&#20102;&#21487;&#22609;&#24615;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Backward Feature Projection&#65288;BFP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#26032;&#29305;&#24449;&#22312;&#26087;&#29305;&#24449;&#30340;&#21487;&#23398;&#20064;&#32447;&#24615;&#21464;&#25442;&#20013;&#21457;&#29983;&#21464;&#21270;&#12290;BFP&#20445;&#30041;&#26087;&#31867;&#21035;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#65292;&#21516;&#26102;&#20801;&#35768;&#26032;&#30340;&#29305;&#24449;&#26041;&#21521;&#20986;&#29616;&#20197;&#36866;&#24212;&#26032;&#30340;&#31867;&#21035;&#12290;BFP&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#32463;&#39564;&#37325;&#25773;&#26041;&#27861;&#38598;&#25104;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;BFP&#26377;&#21161;&#20110;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting has been a major challenge in continual learning, where the model needs to learn new tasks with limited or no access to data from previously seen tasks. To tackle this challenge, methods based on knowledge distillation in feature space have been proposed and shown to reduce forgetting. However, most feature distillation methods directly constrain the new features to match the old ones, overlooking the need for plasticity. To achieve a better stability-plasticity trade-off, we propose Backward Feature Projection (BFP), a method for continual learning that allows the new features to change up to a learnable linear transformation of the old features. BFP preserves the linear separability of the old classes while allowing the emergence of new feature directions to accommodate new classes. BFP can be integrated with existing experience replay methods and boost performance by a significant margin. We also demonstrate that BFP helps learn a better representation space,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#21551;&#21457;&#24335;&#35268;&#21010;&#20316;&#20026;&#23450;&#29702;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#23450;&#29702;&#35777;&#26126;&#25552;&#21319;&#21551;&#21457;&#24335;(TPLH)&#35268;&#21010;&#22120;&#65292;&#22312;&#24773;&#20917;&#26641;&#20013;&#25628;&#32034;&#30701;&#30340;&#35745;&#21010;&#65292;&#24182;&#20943;&#23569;&#25506;&#35752;&#30340;&#29366;&#24577;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.13638</link><description>&lt;p&gt;
&#29992;&#21551;&#21457;&#24335;&#35268;&#21010;&#20316;&#20026;&#23450;&#29702;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Planning as Theorem Proving with Heuristics. (arXiv:2303.13638v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13638
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#21551;&#21457;&#24335;&#35268;&#21010;&#20316;&#20026;&#23450;&#29702;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#23450;&#29702;&#35777;&#26126;&#25552;&#21319;&#21551;&#21457;&#24335;(TPLH)&#35268;&#21010;&#22120;&#65292;&#22312;&#24773;&#20917;&#26641;&#20013;&#25628;&#32034;&#30701;&#30340;&#35745;&#21010;&#65292;&#24182;&#20943;&#23569;&#25506;&#35752;&#30340;&#29366;&#24577;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#22659;&#28436;&#31639;&#20013;&#23558;&#35745;&#21010;&#20316;&#20026;&#23450;&#29702;&#35777;&#26126;&#22312;50&#24180;&#21069;&#34987;&#25918;&#24323;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#19981;&#21487;&#33021;&#23436;&#25104;&#30340;&#39033;&#30446;&#12290;&#20294;&#26159;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23450;&#29702;&#35777;&#26126;&#25552;&#21319;&#21551;&#21457;&#24335;(TPLH)&#35268;&#21010;&#22120;&#65292;&#23427;&#20351;&#29992;A*&#25628;&#32034;&#31639;&#27861;&#22312;&#24773;&#20917;&#26641;&#20013;&#25628;&#32034;&#35745;&#21010;&#12290;&#23427;&#30001;&#22522;&#20110;&#21024;&#38500;&#26494;&#24347;&#30340;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#21551;&#21457;&#24335;&#25511;&#21046;&#12290;&#25105;&#20204;&#23558;TPLH&#19982;Fast Downward&#65288;FD&#65289;&#21644;Best First Width Search&#65288;BFWS&#65289;&#35268;&#21010;&#22120;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#27604;&#36739;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#21151;&#33021;&#23454;&#29616;&#26410;&#32463;&#20248;&#21270;&#65292;TPLH&#27604;FD&#21644;BFWS&#24930;&#12290;&#20294;&#23427;&#20250;&#35745;&#31639;&#20986;&#26356;&#30701;&#30340;&#35745;&#21010;&#65292;&#24182;&#20943;&#23569;&#20102;&#25506;&#35752;&#30340;&#29366;&#24577;&#25968;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20197;&#21069;&#22312;&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#39046;&#22495;&#20869;&#36827;&#34892;&#35268;&#21010;&#30340;&#30740;&#31350;&#65292;&#24182;&#30830;&#23450;&#20102;&#30456;&#20851;&#26041;&#21521;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#34920;&#26126;&#24773;&#22659;&#28436;&#31639;&#20013;&#30340;&#28436;&#32462;&#24335;&#25552;&#21319;&#21551;&#21457;&#24335;&#35268;&#21010;&#23454;&#38469;&#19978;&#26159;&#21487;&#20197;&#23436;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning as theorem proving in situation calculus was abandoned 50 years ago as an impossible project. But we have developed a Theorem Proving Lifted Heuristic (TPLH) planner that searches for a plan in a tree of situations using the A* search algorithm. It is controlled by a delete relaxation-based domain independent heuristic. We compare TPLH with Fast Downward (FD) and Best First Width Search (BFWS) planners over several standard benchmarks. Since our implementation of the heuristic function is not optimized, TPLH is slower than FD and BFWS. But it computes shorter plans, and it explores fewer states. We discuss previous research on planning within KR\&amp;R and identify related directions. Thus, we show that deductive lifted heuristic planning in situation calculus is actually doable.
&lt;/p&gt;</description></item><item><title>&#27492;&#25991;&#20171;&#32461;&#20102;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#20004;&#20010;&#27963;&#36291;&#20219;&#21153;&#65306;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#65292;&#24182;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2303.13336</link><description>&lt;p&gt;
&#35821;&#38899;&#21512;&#25104;&#30340;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#65306;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI. (arXiv:2303.13336v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13336
&lt;/p&gt;
&lt;p&gt;
&#27492;&#25991;&#20171;&#32461;&#20102;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#20004;&#20010;&#27963;&#36291;&#20219;&#21153;&#65306;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#65292;&#24182;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#35821;&#38899;&#21512;&#25104;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#26041;&#21521;&#12290;&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#25104;&#20026;&#26368;&#27969;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35768;&#22810;&#24037;&#20316;&#24050;&#32463;&#23581;&#35797;&#20102;&#20004;&#20010;&#27963;&#36291;&#20219;&#21153;&#65306;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#12290;&#26412;&#25991;&#23545;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#35843;&#26597;&#30340;&#34917;&#20805;&#65292;&#36825;&#20123;&#35843;&#26597;&#35201;&#20040;&#32570;&#20047;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#21512;&#25104;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35201;&#20040;&#24378;&#35843;&#22312;&#22810;&#20010;&#39046;&#22495;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#25972;&#20307;&#24773;&#20917;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#39318;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;&#38899;&#39057;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32972;&#26223;&#12290;&#23545;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#30340;&#38454;&#27573;&#65306;&#22768;&#23398;&#27169;&#22411;&#12289;&#22768;&#30721;&#22120;&#21644;&#31471;&#21040;&#31471;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#26576;&#20123;&#20449;&#21495;&#20174;&#36755;&#20837;&#35821;&#38899;&#20013;&#21024;&#38500;&#25110;&#28155;&#21152;&#26469;&#23558;&#21508;&#31181;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#36824;&#28085;&#30422;&#20102;&#23454;&#39564;&#32467;&#26524;&#30340;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered 
&lt;/p&gt;</description></item><item><title>RepoCoder&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#21644;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.12570</link><description>&lt;p&gt;
RepoCoder&#65306;&#36890;&#36807;&#36845;&#20195;&#26816;&#32034;&#21644;&#29983;&#25104;&#23454;&#29616;&#30340;&#20195;&#30721;&#23384;&#20648;&#24211;&#32423;&#21035;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12570
&lt;/p&gt;
&lt;p&gt;
RepoCoder&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#20219;&#21153;&#26159;&#22522;&#20110;&#20195;&#30721;&#24211;&#26356;&#24191;&#38420;&#19978;&#19979;&#25991;&#20013;&#32487;&#32493;&#32534;&#20889;&#26410;&#23436;&#25104;&#20195;&#30721;&#30340;&#36807;&#31243;&#12290;&#20294;&#26159;&#23545;&#20110;&#33258;&#21160;&#23436;&#25104;&#24037;&#20855;&#32780;&#35328;&#65292;&#24456;&#38590;&#21033;&#29992;&#25955;&#24067;&#22312;&#19981;&#21516;&#25991;&#20214;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RepoCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#31616;&#21270;&#20102;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#65292;&#20174;&#32780;&#20801;&#35768;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#31890;&#24230;&#23618;&#38754;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;RepoCoder &#36824;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26816;&#32034;-&#29983;&#25104;&#27169;&#22411;&#65292;&#24357;&#21512;&#20102;&#26816;&#32034;&#19978;&#19979;&#25991;&#21644;&#39044;&#26399;&#23436;&#25104;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;RepoEval&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26368;&#26032;&#21644;&#39640;&#36136;&#37327;&#30495;&#23454;&#19990;&#30028;&#30340;&#20195;&#30721;&#24211;&#65292;&#28085;&#30422;&#20102;&#34892;&#12289;API &#35843;&#29992;&#21644;&#20989;&#25968;&#20307;&#23436;&#25104;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model, which allows for the effective utilization of repository-level information for code completion and grants the ability to generate code at various levels of granularity. Furthermore, RepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges the gap between retrieval context and the intended completion target. We also propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion sce
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#28145;&#24230;&#35270;&#39057;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#32534;&#30721;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26377;&#25928;&#30340;&#35793;&#30721;&#22120;&#36741;&#21161;&#20449;&#24687;&#29983;&#25104;&#27169;&#22359;&#65292;&#23454;&#29616;&#22312;&#35774;&#22791;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#35270;&#39057;&#24103;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#39640;&#25928;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2303.11599</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#32534;&#30721;&#32467;&#26500;&#30340;&#20302;&#22797;&#26434;&#24230;&#28145;&#24230;&#35270;&#39057;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Low-complexity Deep Video Compression with A Distributed Coding Architecture. (arXiv:2303.11599v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#28145;&#24230;&#35270;&#39057;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#32534;&#30721;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26377;&#25928;&#30340;&#35793;&#30721;&#22120;&#36741;&#21161;&#20449;&#24687;&#29983;&#25104;&#27169;&#22359;&#65292;&#23454;&#29616;&#22312;&#35774;&#22791;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#35270;&#39057;&#24103;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#39640;&#25928;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#39044;&#27979;&#32534;&#30721;&#35270;&#39057;&#21387;&#32553;&#26041;&#27861;&#20381;&#38752;&#22797;&#26434;&#30340;&#32534;&#30721;&#22120;&#38477;&#20302;&#26102;&#22495;&#20887;&#20313;&#65292;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#30340;&#20998;&#24067;&#24335;&#32534;&#30721;&#26041;&#27861;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#23545;&#31471;&#20998;&#24067;&#24335;&#28145;&#24230;&#35270;&#39057;&#21387;&#32553;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#31181;&#26377;&#25928;&#30340;SI&#29983;&#25104;&#27169;&#22359;&#65292;&#26377;&#21161;&#20110;&#22312;&#27809;&#26377;&#35745;&#31639;&#23494;&#38598;&#22411;&#32534;&#30721;&#22120;&#20391;&#36816;&#21160;&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#21033;&#29992;&#24103;&#38388;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prevalent predictive coding-based video compression methods rely on a heavy encoder to reduce the temporal redundancy, which makes it challenging to deploy them on resource-constrained devices. Meanwhile, as early as the 1970s, distributed source coding theory has indicated that independent encoding and joint decoding with side information (SI) can achieve high-efficient compression of correlated sources. This has inspired a distributed coding architecture aiming at reducing the encoding complexity. However, traditional distributed coding methods suffer from a substantial performance gap to predictive coding ones. Inspired by the great success of learning-based compression, we propose the first end-to-end distributed deep video compression framework to improve the rate-distortion performance. A key ingredient is an effective SI generation module at the decoder, which helps to effectively exploit inter-frame correlations without computation-intensive encoder-side motion estimation and c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#24517;&#35201;&#24615;&#24182;&#20174;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#35813;&#32508;&#36848;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.10158</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#32508;&#36848;&#65306;&#19968;&#20221;&#35843;&#26597;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-centric Artificial Intelligence: A Survey. (arXiv:2303.10158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#24517;&#35201;&#24615;&#24182;&#20174;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#35813;&#32508;&#36848;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#20135;&#29983;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#20854;&#25104;&#21151;&#30340;&#20851;&#38190;&#20043;&#19968;&#26159;&#21487;&#29992;&#20110;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20016;&#23500;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#25968;&#25454;&#22312;AI&#20013;&#30340;&#20316;&#29992;&#24471;&#21040;&#20102;&#26174;&#33879;&#25918;&#22823;&#65292;&#24341;&#21457;&#20102;&#25968;&#25454;&#20013;&#24515;AI&#36825;&#19968;&#26032;&#20852;&#27010;&#24565;&#30340;&#20986;&#29616;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#30340;&#27880;&#24847;&#21147;&#36880;&#28176;&#20174;&#25512;&#36827;&#27169;&#22411;&#35774;&#35745;&#36716;&#21521;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;AI&#30340;&#24517;&#35201;&#24615;&#65292;&#38543;&#21518;&#20174;&#35757;&#32451;&#25968;&#25454;&#24320;&#21457;&#12289;&#25512;&#29702;&#25968;&#25454;&#24320;&#21457;&#21644;&#25968;&#25454;&#32500;&#25252;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#20197;&#21450;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#25105;&#20204;&#36824;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#35752;&#35770;&#20102;&#25361;&#25112;&#65292;&#24182;&#21015;&#20986;&#20102;&#21508;&#31181;&#20219;&#21153;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#31532;&#19968;&#20221;&#25552;&#20379;&#36328;&#36234;&#21508;&#20010;&#38454;&#27573;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#20840;&#29699;&#35270;&#35282;&#30340;&#32508;&#21512;&#24615;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36739;&#23569;&#30340;&#28436;&#31034;&#25968;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#20219;&#21153;&#65292;&#22312;&#27979;&#35797;&#26597;&#35810;&#19978;&#21482;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#26102;&#24182;&#27809;&#26377;&#26126;&#26174;&#24615;&#33021;&#19979;&#38477;&#65292;&#32780;&#21482;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#28436;&#31034;&#30340;ICL&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20840;&#28436;&#31034;ICL&#12290;</title><link>http://arxiv.org/abs/2303.08119</link><description>&lt;p&gt;
&#19968;&#20154;&#29420;&#33310;&#22909;&#36824;&#26159;&#20154;&#22810;&#38393;&#24515;&#65311;&#19981;&#21516;&#28436;&#31034;&#27425;&#25968;&#19979;&#30340;&#19978;&#19979;&#25991;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
It Takes One to Tango but More Make Trouble? In-Context Training with Different Number of Demonstrations. (arXiv:2303.08119v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36739;&#23569;&#30340;&#28436;&#31034;&#25968;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#20219;&#21153;&#65292;&#22312;&#27979;&#35797;&#26597;&#35810;&#19978;&#21482;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#26102;&#24182;&#27809;&#26377;&#26126;&#26174;&#24615;&#33021;&#19979;&#38477;&#65292;&#32780;&#21482;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#28436;&#31034;&#30340;ICL&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20840;&#28436;&#31034;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#20102;&#19968;&#20123;&#36755;&#20837;&#36755;&#20986;&#28436;&#31034;&#65288;demos&#65289;&#24182;&#32473;&#20986;&#26356;&#22810;&#28436;&#31034;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65288;&#8220;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#8221;&#65289;&#26102;&#65292;&#33021;&#22815;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27599;&#20010;&#27979;&#35797;&#26597;&#35810;&#19978;&#20351;&#29992;&#36739;&#23569;&#30340;&#28436;&#31034;&#26469;&#36827;&#34892;ICL&#30340;&#20219;&#21153;~\cite{wei2022chain}&#12290;&#24778;&#20154;&#22320;&#65292;&#24403;&#21482;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#26102;&#65292;&#25105;&#20204;&#24182;&#27809;&#26377;&#35266;&#23519;&#21040;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#23545;&#20110;&#27599;&#20010;&#27979;&#35797;&#26597;&#35810;&#65292;&#25105;&#20204;&#23558;&#28436;&#31034;&#20998;&#31867;&#20026;&#8220;&#27491;&#30830;&#28436;&#31034;&#8221;&#21644;&#8220;&#38169;&#35823;&#28436;&#31034;&#8221;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#24191;&#27867;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#20559;&#24046;&#65306;&#22823;&#22810;&#25968;&#27979;&#35797;&#26597;&#35810;&#30340;&#22823;&#22810;&#25968;&#28436;&#31034;&#37117;&#26159;&#27491;&#30830;&#30340;&#65292;&#36825;&#35299;&#37322;&#20102;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#28436;&#31034;&#26102;&#34920;&#29616;&#33391;&#22909;&#30340;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#28436;&#31034;&#30340;ICL&#65288;&#24102;&#21644;&#19981;&#24102;CoT&#65289;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#22823;&#22810;&#25968;&#20808;&#21069;&#24037;&#20316;&#37319;&#29992;&#30340;&#20840;&#28436;&#31034;ICL&#65292;&#34920;&#26126;&#28436;&#31034;&#25968;&#37327;&#24182;&#19981;&#24635;&#26159;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are capable to perform complex reasoning by in-context learning (ICL) when provided with a few input-output demonstrations (demos) and more powerful when intermediate reasoning steps ("chain of thoughts (CoT)") of the demos are given. Is it necessary to use multi-demo in ICL? In this paper, we study ICL using fewer demos for each test query on the tasks in~\cite{wei2022chain}. Surprisingly, we do not observe significant degradation when using only one randomly chosen demo. To study this phenomenon, for each test query, we categorize demos into "correct demos" leading to the correct answer, and "wrong demos" resulting in wrong answers. Our analysis reveals an inherent bias in those widely studied datasets: most demos are correct for a majority of test queries, which explains the good performance of using one random demo. Moreover, ICL (with and w/o CoT) using only one correct demo significantly outperforms all-demo ICL adopted by most previous works, indicat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.07909</link><description>&lt;p&gt;
&#29983;&#25104;AI&#20013;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Text-to-image Diffusion Model in Generative AI: A Survey. (arXiv:2303.07909v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#27969;&#34892;&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#24037;&#20316;&#65292;&#26412;&#35843;&#26597;&#20174;&#31616;&#21333;&#20171;&#32461;&#22522;&#26412;&#25193;&#25955;&#27169;&#22411;&#22914;&#20309;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#24320;&#22987;&#65292;&#25509;&#30528;&#26159;&#26465;&#20214;&#25110;&#24341;&#23548;&#22914;&#20309;&#25913;&#36827;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#24635;&#32467;&#20102;&#25991;&#26412;&#24341;&#23548;&#21019;&#24847;&#29983;&#25104;&#21644;&#22270;&#20687;&#32534;&#36753;&#30340;&#24212;&#29992;&#12290;&#38500;&#20102;&#36804;&#20170;&#20026;&#27490;&#25152;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey reviews text-to-image diffusion models in the context that diffusion models have emerged to be popular for a wide range of generative tasks. As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis, i.e., text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21644;&#21452;&#21521;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#36136;&#37327;&#26356;&#22909;&#12289;&#27169;&#22359;&#21270;&#21464;&#21270;&#26356;&#24555;&#30340;&#21512;&#25104;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2303.04743</link><description>&lt;p&gt;
&#24102;&#26377;&#21452;&#21521;&#20808;&#39564;&#27169;&#22411;&#30340;&#21521;&#37327;&#37327;&#21270;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Vector Quantized Time Series Generation with a Bidirectional Prior Model. (arXiv:2303.04743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21644;&#21452;&#21521;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#36136;&#37327;&#26356;&#22909;&#12289;&#27169;&#22359;&#21270;&#21464;&#21270;&#26356;&#24555;&#30340;&#21512;&#25104;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#19982;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21464;&#20307;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451; GAN &#30340;&#22522;&#26412;&#38480;&#21046;&#21644;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;RNN&#26063;&#36890;&#24120;&#22312;&#36828;&#31243;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21463;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986; TimeVQVAE&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#31532;&#19968;&#20010;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#25216;&#26415;&#35299;&#20915; TSG &#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#20351;&#29992;&#21452;&#21521;&#21464;&#21387;&#22120;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#22312;&#26102;&#38388; - &#39057;&#29575;&#22495;&#20013;&#36827;&#34892; VQ &#24314;&#27169;&#65292;&#20998;&#20026;&#20302;&#39057;&#65288;LF&#65289;&#21644;&#39640;&#39057;&#65288;HF&#65289;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20445;&#30041;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#29983;&#25104;&#36136;&#37327;&#26356;&#22909;&#12289;&#27169;&#22359;&#24615;&#21464;&#21270;&#26356;&#24555;&#30340;&#26032;&#21512;&#25104;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series generation (TSG) studies have mainly focused on the use of Generative Adversarial Networks (GANs) combined with recurrent neural network (RNN) variants. However, the fundamental limitations and challenges of training GANs still remain. In addition, the RNN-family typically has difficulties with temporal consistency between distant timesteps. Motivated by the successes in the image generation (IMG) domain, we propose TimeVQVAE, the first work, to our knowledge, that uses vector quantization (VQ) techniques to address the TSG problem. Moreover, the priors of the discrete latent spaces are learned with bidirectional transformer models that can better capture global temporal consistency. We also propose VQ modeling in a time-frequency domain, separated into low-frequency (LF) and high-frequency (HF). This allows us to retain important characteristics of the time series and, in turn, generate new synthetic signals that are of better quality, with sharper changes in modularity, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BPBA&#35774;&#32622;&#21644;BTOL&#35757;&#32451;&#31574;&#30053;&#20004;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#29992;&#20110;&#29992;&#25143;&#35757;&#32451;&#31169;&#26377;&#27169;&#22411;&#21644;&#21033;&#29992;&#22522;&#30784;/&#28304;&#27169;&#22411;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#26159;&#26377;&#25928;&#19988;&#31283;&#20581;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.03709</link><description>&lt;p&gt;
Bootstrap The Original Latent: &#20174;&#40657;&#30418;&#27169;&#22411;&#23398;&#20064;&#31169;&#26377;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bootstrap The Original Latent: Learning a Private Model from a Black-box Model. (arXiv:2303.03709v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BPBA&#35774;&#32622;&#21644;BTOL&#35757;&#32451;&#31574;&#30053;&#20004;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#29992;&#20110;&#29992;&#25143;&#35757;&#32451;&#31169;&#26377;&#27169;&#22411;&#21644;&#21033;&#29992;&#22522;&#30784;/&#28304;&#27169;&#22411;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#26159;&#26377;&#25928;&#19988;&#31283;&#20581;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Back-Propagated Black-Box Adaptation&#65288;BPBA&#65289;&#30340;&#26032;&#35774;&#32622;&#65292;&#20197;&#24179;&#34913;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#25968;&#25454;/&#27169;&#22411;&#38544;&#31169;&#21644;&#29992;&#25143;&#38656;&#27714;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#25351;&#23548;&#12290;&#35813;&#35774;&#32622;&#21487;&#20197;&#31616;&#21270;&#22522;&#30784;/&#28304;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#38450;&#27490;&#22522;&#30784;/&#28304;&#27169;&#22411;&#30340;&#27844;&#28431;&#21644;&#35823;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Bootstrap The Original Latent&#65288;BTOL&#65289;&#30340;&#26032;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#22522;&#30784;/&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#30001;&#39046;&#22495;&#36866;&#37197;&#22120;&#21644;&#20919;&#20923;-&#35299;&#20923;&#31574;&#30053;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, considering the balance of data/model privacy of model owners and user needs, we propose a new setting called Back-Propagated Black-Box Adaptation (BPBA) for users to better train their private models via the guidance of the back-propagated results of a Black-box foundation/source model. Our setting can ease the usage of foundation/source models as well as prevent the leakage and misuse of foundation/source models. Moreover, we also propose a new training strategy called Bootstrap The Original Latent (BTOL) to fully utilize the foundation/source models. Our strategy consists of a domain adapter and a freeze-and-thaw strategy. We apply our BTOL under BPBA and Black-box UDA settings on three different datasets. Experiments show that our strategy is efficient and robust in various settings without manual augmentations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#30899;&#36275;&#36857;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25511;&#21046;GPU&#30340;&#33021;&#32791;&#26469;&#38477;&#20302;&#30899;&#25490;&#25918;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#30899;&#24378;&#24230;&#65292;&#23545;&#21508;&#31181;DNN&#24212;&#29992;&#31243;&#24207;&#36866;&#29992;&#65292;&#26080;&#38656;&#39069;&#22806;&#30828;&#20214;&#25110;&#22522;&#30784;&#35774;&#26045;&#12290;</title><link>http://arxiv.org/abs/2303.02508</link><description>&lt;p&gt;
&#36861;&#27714;&#23454;&#29992;&#21487;&#25345;&#32493;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20302;&#30899;&#30005;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chasing Low-Carbon Electricity for Practical and Sustainable DNN Training. (arXiv:2303.02508v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#30899;&#36275;&#36857;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25511;&#21046;GPU&#30340;&#33021;&#32791;&#26469;&#38477;&#20302;&#30899;&#25490;&#25918;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#30899;&#24378;&#24230;&#65292;&#23545;&#21508;&#31181;DNN&#24212;&#29992;&#31243;&#24207;&#36866;&#29992;&#65292;&#26080;&#38656;&#39069;&#22806;&#30828;&#20214;&#25110;&#22522;&#30784;&#35774;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#21457;&#23637;&#65292;&#30001;&#20110;&#20351;&#29992;GPU&#36827;&#34892;&#35757;&#32451;&#65292;&#23548;&#33268;&#33021;&#32791;&#21644;&#30899;&#25490;&#25918;&#37327;&#22686;&#21152;&#12290;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#23581;&#35797;&#23558;&#35757;&#32451;&#24037;&#20316;&#31227;&#21160;&#21040;&#30899;&#24378;&#24230;&#36739;&#20302;&#30340;&#20301;&#32622;&#25110;&#26102;&#38388;&#26694;&#26550;&#65292;&#20197;&#22238;&#24212;&#21487;&#25345;&#32493;&#24615;&#30340;&#21628;&#21505;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#25968;&#25454;&#27861;&#35268;&#31561;&#21407;&#22240;&#65292;&#23558;&#24037;&#20316;&#31227;&#21160;&#21040;&#20854;&#20182;&#22320;&#26041;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25512;&#36831;&#35757;&#32451;&#21487;&#33021;&#20250;&#23545;&#24212;&#29992;&#26381;&#21153;&#36136;&#37327;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#22240;&#20026;&#25903;&#25345;&#26381;&#21153;&#30340;DNN&#27809;&#26377;&#24471;&#21040;&#21450;&#26102;&#26356;&#26032;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#38477;&#20302;DNN&#35757;&#32451;&#30340;&#30899;&#36275;&#36857;&#65292;&#32780;&#26080;&#38656;&#36801;&#31227;&#25110;&#25512;&#36831;&#24037;&#20316;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35266;&#23519;&#23454;&#26102;&#30340;&#30899;&#24378;&#24230;&#21464;&#21270;&#24182;&#25511;&#21046;GPU&#30340;&#33021;&#32791;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#35757;&#32451;&#24615;&#33021;&#30340;&#21516;&#26102;&#38477;&#20302;&#30899;&#36275;&#36857;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20027;&#21160;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#30005;&#32593;&#26465;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#26041;&#27861;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#30899;&#24378;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#30899;&#20943;&#25490;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;DNN&#24212;&#29992;&#31243;&#24207;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30828;&#20214;&#25110;&#22522;&#30784;&#35774;&#26045;&#12290;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#30899;&#24378;&#24230;&#38477;&#20302;&#39640;&#36798;44%&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has experienced significant growth in recent years, resulting in increased energy consumption and carbon emission from the use of GPUs for training deep neural networks (DNNs). Answering the call for sustainability, conventional solutions have attempted to move training jobs to locations or time frames with lower carbon intensity. However, moving jobs to other locations may not always be feasible due to large dataset sizes or data regulations. Moreover, postponing training can negatively impact application service quality because the DNNs backing the service are not updated in a timely fashion. In this work, we present a practical solution that reduces the carbon footprint of DNN training without migrating or postponing jobs. Specifically, our solution observes real-time carbon intensity shifts during training and controls the energy consumption of GPUs, thereby reducing carbon footprint while maintaining training performance. Furthermore, in order to proactively adapt to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#30340;&#20844;&#27491;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#26041;&#27861;Fairguard&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#38745;&#24577;&#29983;&#25104;&#21644;&#21160;&#24577;&#35843;&#33410;&#65292;&#32531;&#35299;&#30001;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#23548;&#33268;&#30340;&#19981;&#20844;&#27491;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.11137</link><description>&lt;p&gt;
Fairguard: &#22312;&#26234;&#24935;&#22478;&#24066;&#20013;&#21033;&#29992;&#22522;&#20110;&#36923;&#36753;&#30340;&#20844;&#27491;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Fairguard: Harness Logic-based Fairness Rules in Smart Cities. (arXiv:2302.11137v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#30340;&#20844;&#27491;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#26041;&#27861;Fairguard&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#38745;&#24577;&#29983;&#25104;&#21644;&#21160;&#24577;&#35843;&#33410;&#65292;&#32531;&#35299;&#30001;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#23548;&#33268;&#30340;&#19981;&#20844;&#27491;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#24935;&#22478;&#24066;&#36816;&#34892;&#22312;&#35745;&#31639;&#39044;&#27979;&#26694;&#26550;&#19978;&#65292;&#25910;&#38598;&#12289;&#25972;&#21512;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26694;&#26550;&#23481;&#26131;&#21463;&#21040;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#19981;&#20844;&#27491;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#30740;&#31350;&#30000;&#32435;&#35199;&#24030;&#26597;&#22612;&#21162;&#21152;&#30340;&#30495;&#23454;&#22478;&#24066;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#20559;&#35265;&#22312;&#24494;&#35266;&#23618;&#38754;&#19978;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Fairguard&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#24494;&#35266;&#23618;&#38754;&#26102;&#38388;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#31354;&#38388;&#22495;&#20013;&#36827;&#34892;&#20844;&#27491;&#30340;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#12290;Fairguard&#26694;&#26550;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38745;&#24577;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#36890;&#36807;&#26368;&#23567;&#21270;&#25152;&#36873;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#26465;&#20214;&#26469;&#20943;&#23569;&#25968;&#25454;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#30830;&#20445;&#39044;&#27979;&#31639;&#27861;&#30340;&#20844;&#27491;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21160;&#24577;&#32452;&#20214;&#26469;&#35843;&#33410;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#29983;&#25104;&#26410;&#26469;&#30340;&#20844;&#27491;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart cities operate on computational predictive frameworks that collect, aggregate, and utilize data from large-scale sensor networks. However, these frameworks are prone to multiple sources of data and algorithmic bias, which often lead to unfair prediction results. In this work, we first demonstrate that bias persists at a micro-level both temporally and spatially by studying real city data from Chattanooga, TN. To alleviate the issue of such bias, we introduce Fairguard, a micro-level temporal logic-based approach for fair smart city policy adjustment and generation in complex temporal-spatial domains. The Fairguard framework consists of two phases: first, we develop a static generator that is able to reduce data bias based on temporal logic conditions by minimizing correlations between selected attributes. Then, to ensure fairness in predictive algorithms, we design a dynamic component to regulate prediction results and generate future fair predictions by harnessing logic rules. E
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#22788;&#29702;&#23450;&#20215;&#21644;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#21830;&#19994;&#20915;&#31574;&#23545;&#26410;&#26469;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25340;&#36710;&#30340;&#25928;&#29575;&#21644;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.10510</link><description>&lt;p&gt;
&#38754;&#21521;&#21487;&#25345;&#32493;&#24615;&#30340;&#21363;&#26102;&#25340;&#36710;&#30340;&#26410;&#26469;&#24863;&#30693;&#23450;&#20215;&#21644;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Future Aware Pricing and Matching for Sustainable On-demand Ride Pooling. (arXiv:2302.10510v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#22788;&#29702;&#23450;&#20215;&#21644;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#21830;&#19994;&#20915;&#31574;&#23545;&#26410;&#26469;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25340;&#36710;&#30340;&#25928;&#29575;&#21644;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#26102;&#25340;&#36710;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#22312;&#20110;&#20026;&#39038;&#23458;&#65288;&#26356;&#20302;&#30340;&#20215;&#26684;&#65289;&#12289;&#20986;&#31199;&#36710;&#21496;&#26426;&#65288;&#26356;&#39640;&#30340;&#25910;&#20837;&#65289;&#12289;&#29615;&#22659;&#65288;&#30001;&#20110;&#26356;&#23569;&#30340;&#36710;&#36742;&#32780;&#20943;&#23569;&#30899;&#25490;&#25918;&#37327;&#65289;&#21644;Uber&#31561;&#32858;&#21512;&#20844;&#21496;&#65288;&#26356;&#39640;&#30340;&#25910;&#20837;&#65289;&#25552;&#20379;&#20102;&#22909;&#22788;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#25910;&#30410;&#65292;&#24517;&#39035;&#26377;&#25928;&#22320;&#35299;&#20915;&#20004;&#20010;&#20851;&#38190;&#32780;&#30456;&#20114;&#20851;&#32852;&#30340;&#25361;&#25112;&#65306;&#65288;a&#65289;&#23450;&#20215;&#8212;&#8212;&#20026;&#20986;&#31199;&#36710;&#39038;&#23458;&#35831;&#27714;&#35774;&#32622;&#20215;&#26684;&#65307;&#21644;&#65288;b&#65289;&#21305;&#37197;&#8212;&#8212;&#23558;&#25509;&#21463;&#20215;&#26684;&#30340;&#23458;&#25143;&#20998;&#37197;&#32473;&#20986;&#31199;&#36710;/&#27773;&#36710;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20004;&#20010;&#25361;&#25112;&#37117;&#26159;&#21333;&#29420;&#30740;&#31350;&#30340;&#65292;&#24182;&#19988;&#20351;&#29992;&#30701;&#35270;&#30340;&#26041;&#27861;&#65288;&#21482;&#32771;&#34385;&#24403;&#21069;&#35831;&#27714;&#65289;&#65292;&#32780;&#19981;&#32771;&#34385;&#24403;&#21069;&#21305;&#37197;&#23545;&#35299;&#20915;&#26410;&#26469;&#35831;&#27714;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#22788;&#29702;&#23450;&#20215;&#21644;&#21305;&#37197;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#23450;&#20215;&#21644;&#21305;&#37197;&#20915;&#31574;&#23545;&#26410;&#26469;&#24433;&#21709;&#30340;&#24433;&#21709;&#12290;&#22312;&#30495;&#23454;&#20986;&#31199;&#36710;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#21305;&#37197;&#21644;&#23450;&#20215;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of on-demand ride pooling is owing to the benefits offered to customers (lower prices), taxi drivers (higher revenue), environment (lower carbon footprint due to fewer vehicles) and aggregation companies like Uber (higher revenue). To achieve these benefits, two key interlinked challenges have to be solved effectively: (a) pricing -- setting prices to customer requests for taxis; and (b) matching -- assignment of customers (that accepted the prices) to taxis/cars. Traditionally, both these challenges have been studied individually and using myopic approaches (considering only current requests), without considering the impact of current matching on addressing future requests. In this paper, we develop a novel framework that handles the pricing and matching problems together, while also considering the future impact of the pricing and matching decisions. In our experimental results on a real-world taxi dataset, we demonstrate that our framework can significantly improve re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;&#27169;&#22411;&#24494;&#35843;&#21644;&#20462;&#21098;&#31561;&#27700;&#21360;&#21024;&#38500;&#25915;&#20987;&#65307;&#36890;&#36807;&#22686;&#24378;&#27700;&#21360;&#21644;&#27169;&#22411;&#21151;&#33021;&#30340;&#32806;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#30830;&#20445;&#21024;&#38500;&#27700;&#21360;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#24120;&#35268;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.10296</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21151;&#33021;&#32806;&#21512;&#27700;&#21360;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Function-Coupled Watermarks for Deep Neural Networks. (arXiv:2302.10296v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;&#27169;&#22411;&#24494;&#35843;&#21644;&#20462;&#21098;&#31561;&#27700;&#21360;&#21024;&#38500;&#25915;&#20987;&#65307;&#36890;&#36807;&#22686;&#24378;&#27700;&#21360;&#21644;&#27169;&#22411;&#21151;&#33021;&#30340;&#32806;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#30830;&#20445;&#21024;&#38500;&#27700;&#21360;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#24120;&#35268;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#22909;&#34920;&#29616;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#38656;&#35201;&#28023;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#20445;&#25252;&#36825;&#20123;&#30693;&#35782;&#20135;&#26435;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#27700;&#21360;&#25216;&#26415;&#65292;&#20854;&#20013;DNN&#25552;&#20379;&#21830;&#23558;&#31192;&#23494;&#20449;&#24687;&#26893;&#20837;&#27169;&#22411;&#20013;&#65292;&#20197;&#20415;&#22312;&#31245;&#21518;&#36890;&#36807;&#19968;&#20123;&#19987;&#29992;&#35302;&#21457;&#36755;&#20837;&#26816;&#32034;&#23884;&#20837;&#30340;&#27700;&#21360;&#32034;&#26435;&#65307;&#34429;&#28982;&#25991;&#29486;&#20013;&#25253;&#21578;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#36973;&#21463;&#27700;&#21360;&#21024;&#38500;&#25915;&#20987;&#65292;&#20363;&#22914;&#27169;&#22411;&#24494;&#35843;&#21644;&#27169;&#22411;&#20462;&#21098;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#27700;&#21360;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;&#19978;&#36848;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#22686;&#24378;&#27700;&#21360;&#21644;&#27169;&#22411;&#21151;&#33021;&#30340;&#32806;&#21512;&#65292;&#36825;&#26679;&#21024;&#38500;&#27700;&#21360;&#20250;&#19981;&#21487;&#36991;&#20813;&#22320;&#38477;&#20302;&#27169;&#22411;&#22312;&#24120;&#35268;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#26469;&#33258;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#31192;&#23494;&#29305;&#24449;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Well-performed deep neural networks (DNNs) generally require massive labelled data and computational resources for training. Various watermarking techniques are proposed to protect such intellectual properties (IPs), wherein the DNN providers implant secret information into the model so that they can later claim IP ownership by retrieving their embedded watermarks with some dedicated trigger inputs. While promising results are reported in the literature, existing solutions suffer from watermark removal attacks, such as model fine-tuning and model pruning.  In this paper, we propose a novel DNN watermarking solution that can effectively defend against the above attacks. Our key insight is to enhance the coupling of the watermark and model functionalities such that removing the watermark would inevitably degrade the model's performance on normal inputs. To this end, unlike previous methods relying on secret features learnt from out-of-distribution data, our method only uses features lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.05743</link><description>&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20165;&#20381;&#38752;&#36317;&#31163;&#30697;&#38453;&#36275;&#22815;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24120;&#29992;&#20110;&#28041;&#21450;&#22270;&#24418;&#20960;&#20309;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#34429;&#28982;&#20960;&#20309;&#22270;&#30340;&#36317;&#31163;&#30697;&#38453;&#21253;&#21547;&#23436;&#25972;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#26080;&#27861;&#23398;&#20064;&#36825;&#31181;&#20960;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;&#26032;&#39062;&#30340;&#23545;&#31216;&#20960;&#20309;&#22270;&#30340;&#23478;&#26063;&#65292;&#25193;&#23637;&#20102;MPNN&#26080;&#27861;&#21306;&#20998;&#20854;&#36317;&#31163;&#30697;&#38453;&#30340;&#21453;&#20363;&#23478;&#26063;&#65292;&#24182;&#25552;&#20986;$k$-DisGNNs&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#20960;&#20309;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;$k$-DisGNNs&#30340;&#29305;&#27530;&#24773;&#20917;&#32479;&#19968;&#36215;&#26469;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23637;&#31034;&#20102;&#37027;&#20123;&#26368;&#21021;&#20026;&#20302;&#24230;&#34920;&#36798;&#33021;&#21147;&#30340;GNN&#27169;&#22411;&#35774;&#35745;&#30340;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#26469;&#20248;&#21270;Jaccard&#25351;&#25968;&#65292;&#35813;&#25439;&#22833;&#22312;&#36719;&#26631;&#31614;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.05666</link><description>&lt;p&gt;
Jaccard&#24230;&#37327;&#25439;&#22833;&#65306;&#20351;&#29992;&#36719;&#26631;&#31614;&#20248;&#21270;Jaccard&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels. (arXiv:2302.05666v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#26469;&#20248;&#21270;Jaccard&#25351;&#25968;&#65292;&#35813;&#25439;&#22833;&#22312;&#36719;&#26631;&#31614;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
IoU&#25439;&#22833;&#26159;&#30452;&#25509;&#20248;&#21270;Jaccard&#25351;&#25968;&#30340;&#26367;&#20195;&#21697;&#12290;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#65292;&#23558;IoU&#25439;&#22833;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#19968;&#37096;&#20998;&#65292;&#19982;&#20165;&#20248;&#21270;&#20687;&#32032;&#25439;&#22833;&#65288;&#22914;&#20132;&#21449;&#29109;&#25439;&#22833;&#65289;&#30456;&#27604;&#65292;&#23545;&#20110;Jaccard&#25351;&#25968;&#27979;&#37327;&#34920;&#29616;&#26356;&#22909;&#12290;&#26368;&#26174;&#30528;&#30340;IoU&#25439;&#22833;&#26159;&#36719;Jaccard&#25439;&#22833;&#21644;Lovasz-Softmax&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25439;&#22833;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#36719;&#26631;&#31614;&#19981;&#20860;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#65292;&#23427;&#20204;&#22312;&#26631;&#20934;&#35774;&#32622;&#19979;&#19982;&#36719;&#26631;&#31614;&#20860;&#23481;&#65292;&#19982;&#36719;Jaccard&#25439;&#22833;&#30456;&#21516;&#12290;&#20351;&#29992;JMLs&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26368;&#27969;&#34892;&#30340;&#36719;&#26631;&#31614;&#29992;&#20363;&#65306;&#26631;&#31614;&#24179;&#28369;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#19977;&#20010;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;Cityscapes&#12289;PASCAL VOC&#21644;DeepGlobe Land&#65289;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;DeepGlobe Land&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoU losses are surrogates that directly optimize the Jaccard index. In semantic segmentation, leveraging IoU losses as part of the loss function is shown to perform better with respect to the Jaccard index measure than optimizing pixel-wise losses such as the cross-entropy loss alone. The most notable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss. However, these losses are incompatible with soft labels which are ubiquitous in machine learning. In this paper, we propose Jaccard metric losses (JMLs), which are identical to the soft Jaccard loss in a standard setting with hard labels, but are compatible with soft labels. With JMLs, we study two of the most popular use cases of soft labels: label smoothing and knowledge distillation. With a variety of architectures, our experiments show significant improvements over the cross-entropy loss on three semantic segmentation datasets (Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approach outperforms state-of-the-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#31354;&#27169;&#22411;&#30340;&#22810;&#30456;&#26426;&#19977;&#32500;&#22810;&#30446;&#26631;&#36319;&#36394;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#8220;&#36807;&#21435;&#21644;&#26410;&#26469;&#20043;&#38388;&#30340;&#36319;&#36394;&#8221;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#27880;&#24847;&#21147;&#36319;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35937;&#26597;&#35810;&#36830;&#32493;&#22320;&#34920;&#31034;&#36319;&#36394;&#23454;&#20363;&#65292;&#24182;&#25972;&#21512;&#20102;&#36319;&#36394;&#23545;&#35937;&#30340;&#21069;&#21518;&#25512;&#29702;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#36319;&#36394;&#20934;&#30830;&#24615;&#21644;ID-Switches&#30340;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2302.03802</link><description>&lt;p&gt;
&#36807;&#21435;&#21644;&#26410;&#26469;&#20043;&#38388;&#65306;&#22522;&#20110;&#26102;&#31354;&#27169;&#22411;&#30340;&#22810;&#30456;&#26426;&#19977;&#32500;&#22810;&#30446;&#26631;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking. (arXiv:2302.03802v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#31354;&#27169;&#22411;&#30340;&#22810;&#30456;&#26426;&#19977;&#32500;&#22810;&#30446;&#26631;&#36319;&#36394;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#8220;&#36807;&#21435;&#21644;&#26410;&#26469;&#20043;&#38388;&#30340;&#36319;&#36394;&#8221;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#27880;&#24847;&#21147;&#36319;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35937;&#26597;&#35810;&#36830;&#32493;&#22320;&#34920;&#31034;&#36319;&#36394;&#23454;&#20363;&#65292;&#24182;&#25972;&#21512;&#20102;&#36319;&#36394;&#23545;&#35937;&#30340;&#21069;&#21518;&#25512;&#29702;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#36319;&#36394;&#20934;&#30830;&#24615;&#21644;ID-Switches&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#30456;&#26426;&#19977;&#32500;&#22810;&#30446;&#26631;&#36319;&#36394;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#24378;&#35843;&#26102;&#31354;&#36830;&#32493;&#24615;&#65292;&#24182;&#25972;&#21512;&#20102;&#36319;&#36394;&#23545;&#35937;&#30340;&#21069;&#21518;&#25512;&#29702;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#8220;&#22522;&#20110;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#36319;&#36394;&#8221;&#65288;PF-Track&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#8220;&#27880;&#24847;&#21147;&#36319;&#36394;&#8221;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23545;&#35937;&#26597;&#35810;&#36830;&#32493;&#22320;&#34920;&#31034;&#36319;&#36394;&#23454;&#20363;&#12290;&#20026;&#20102;&#26126;&#30830;&#20351;&#29992;&#21382;&#21490;&#32447;&#32034;&#65292;&#25105;&#20204;&#30340;&#8220;&#36807;&#21435;&#25512;&#29702;&#8221;&#27169;&#22359;&#23398;&#20064;&#31934;&#32454;&#21270;&#36319;&#36394;&#65292;&#24182;&#36890;&#36807;&#36328;&#21069;&#19968;&#24103;&#21644;&#20854;&#20182;&#23545;&#35937;&#30340;&#26597;&#35810;&#20132;&#21449;&#27880;&#24847;&#26469;&#22686;&#24378;&#23545;&#35937;&#29305;&#24449;&#12290;&#32780;&#8220;&#26410;&#26469;&#25512;&#29702;&#8221;&#27169;&#22359;&#21017;&#28040;&#21270;&#21382;&#21490;&#20449;&#24687;&#24182;&#39044;&#27979;&#24378;&#20581;&#30340;&#26410;&#26469;&#36712;&#36857;&#12290;&#22312;&#38271;&#26102;&#38388;&#36974;&#25377;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32500;&#25345;&#23545;&#35937;&#20301;&#32622;&#65292;&#36890;&#36807;&#25972;&#21512;&#36816;&#21160;&#39044;&#27979;&#23454;&#29616;&#37325;&#26032;&#20851;&#32852;&#12290;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#24133;&#25552;&#39640;&#20102;AMOTA&#65292;&#24182;&#23558;ID-Switches&#20943;&#23569;&#20102;90%&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT) framework. It emphasizes spatio-temporal continuity and integrates both past and future reasoning for tracked objects. Thus, we name it "Past-and-Future reasoning for Tracking" (PF-Track). Specifically, our method adapts the "tracking by attention" framework and represents tracked instances coherently over time with object queries. To explicitly use historical cues, our "Past Reasoning" module learns to refine the tracks and enhance the object features by cross-attending to queries from previous frames and other objects. The "Future Reasoning" module digests historical information and predicts robust future trajectories. In the case of long-term occlusions, our method maintains the object positions and enables re-association by integrating motion predictions. On the nuScenes dataset, our method improves AMOTA by a large margin and remarkably reduces ID-Switches by 90% compared to prior approaches, which is an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#35752;&#35770;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#21487;&#20449;AI&#30417;&#31649;&#65292;&#21253;&#25324;&#30452;&#25509;&#30417;&#31649;&#12289;&#25968;&#25454;&#20445;&#25252;&#12289;&#20869;&#23481;&#30417;&#31649;&#21644;&#25919;&#31574;&#24314;&#35758;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#26032;&#26415;&#35821;&#26469;&#21306;&#20998;&#21442;&#19982;&#32773;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;LGAIMs&#30340;&#21487;&#20449;&#24230;&#24182;&#20351;&#20854;&#20026;&#21463;&#30410;&#25152;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.02337</link><description>&lt;p&gt;
&#31649;&#21046;ChatGPT&#21644;&#20854;&#20182;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Regulating ChatGPT and other Large Generative AI Models. (arXiv:2302.02337v5 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#35752;&#35770;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#21487;&#20449;AI&#30417;&#31649;&#65292;&#21253;&#25324;&#30452;&#25509;&#30417;&#31649;&#12289;&#25968;&#25454;&#20445;&#25252;&#12289;&#20869;&#23481;&#30417;&#31649;&#21644;&#25919;&#31574;&#24314;&#35758;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#26032;&#26415;&#35821;&#26469;&#21306;&#20998;&#21442;&#19982;&#32773;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;LGAIMs&#30340;&#21487;&#20449;&#24230;&#24182;&#20351;&#20854;&#20026;&#21463;&#30410;&#25152;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#65288;LGAIMs&#65289;&#65292;&#22914;ChatGPT&#25110;Stable Diffusion&#65292;&#27491;&#22312;&#24555;&#36895;&#25913;&#21464;&#25105;&#20204;&#30340;&#27807;&#36890;&#12289;&#35828;&#26126;&#21644;&#21019;&#36896;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#27431;&#30431;&#21450;&#20854;&#20182;&#22320;&#21306;&#30340;AI&#30417;&#31649;&#20027;&#35201;&#38598;&#20013;&#22312;&#20256;&#32479;AI&#27169;&#22411;&#19978;&#65292;&#32780;&#38750;LGAIMs&#12290;&#26412;&#25991;&#23558;&#25226;&#36825;&#20123;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#25918;&#32622;&#22312;&#24403;&#21069;&#30340;&#8220;&#21487;&#20449;AI&#30417;&#31649;&#8221;&#36777;&#35770;&#20013;&#65292;&#24182;&#25506;&#35752;&#22914;&#20309;&#35843;&#25972;&#27861;&#24459;&#20197;&#36866;&#24212;&#20854;&#33021;&#21147;&#12290;&#22312;&#22880;&#23450;&#25216;&#26415;&#22522;&#30784;&#20043;&#21518;&#65292;&#26412;&#25991;&#30340;&#27861;&#24459;&#37096;&#20998;&#20998;&#22235;&#27493;&#36827;&#34892;&#65292;&#21253;&#25324;&#65288;1&#65289;&#30452;&#25509;&#30417;&#31649;&#65292;&#65288;2&#65289;&#25968;&#25454;&#20445;&#25252;&#65292;&#65288;3&#65289;&#20869;&#23481;&#30417;&#31649;&#21644;&#65288;4&#65289;&#25919;&#31574;&#24314;&#35758;&#12290;&#23427;&#24314;&#35758;&#20351;&#29992;&#26032;&#26415;&#35821;&#26469;&#25429;&#25417;LGAIM&#35774;&#32622;&#20013;&#30340;AI&#20215;&#20540;&#38142;&#65292;&#21306;&#20998;LGAIM&#24320;&#21457;&#20154;&#21592;&#12289;&#37096;&#32626;&#32773;&#12289;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#29992;&#25143;&#65292;&#20197;&#21450;LGAIM&#36755;&#20986;&#30340;&#25509;&#25910;&#32773;&#12290;&#25105;&#20204;&#23558;&#30417;&#31649;&#32844;&#36131;&#38024;&#23545;&#36825;&#20123;&#19981;&#21516;&#30340;&#20215;&#20540;&#38142;&#21442;&#19982;&#32773;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#22235;&#20010;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;LGAIMs&#30340;&#20449;&#20219;&#24230;&#24182;&#20351;&#20854;&#20026;&#21463;&#30410;&#25152;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large generative AI models (LGAIMs), such as ChatGPT or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest four strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of so
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35282;&#21512;&#25104;&#30340;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#26080;&#38656;&#20351;&#29992;&#30495;&#23454;&#28145;&#24230;&#65292;&#20351;&#29992;L0&#33539;&#25968;&#38480;&#21046;&#25200;&#21160;&#26469;&#25552;&#39640;&#23545;&#29289;&#29702;&#25915;&#20987;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.13487</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#23545;&#25239;&#24615;&#35757;&#32451;&#38450;&#24481;&#29289;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks. (arXiv:2301.13487v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35282;&#21512;&#25104;&#30340;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#26080;&#38656;&#20351;&#29992;&#30495;&#23454;&#28145;&#24230;&#65292;&#20351;&#29992;L0&#33539;&#25968;&#38480;&#21046;&#25200;&#21160;&#26469;&#25552;&#39640;&#23545;&#29289;&#29702;&#25915;&#20987;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;MDE&#65289;&#26159;&#33258;&#21160;&#39550;&#39542;&#31561;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;MDE&#32593;&#32476;&#38754;&#20020;&#21508;&#31181;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;&#29289;&#29702;&#25915;&#20987;&#65292;&#23041;&#32961;&#36825;&#31181;&#31995;&#32479;&#30340;&#23433;&#20840;&#12290;&#20256;&#32479;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#38656;&#35201;&#26377;&#26631;&#31614;&#25968;&#25454;&#65292;&#22240;&#27492;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#27809;&#26377;&#30495;&#23454;&#28145;&#24230;&#30340;&#33258;&#30417;&#30563;MDE&#12290;&#19968;&#20123;&#33258;&#30417;&#30563;&#27169;&#22411;&#24378;&#21270;&#25216;&#26415;&#65288;&#20363;&#22914;&#23545;&#27604;&#23398;&#20064;&#65289;&#24573;&#30053;&#20102;MDE&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#24456;&#38590;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35282;&#21512;&#25104;&#30340;&#33258;&#30417;&#30563;MDE&#27169;&#22411;&#30340;&#26032;&#39062;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#26080;&#38656;&#20351;&#29992;&#30495;&#23454;&#28145;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;L0&#33539;&#25968;&#38480;&#21046;&#25200;&#21160;&#26469;&#25552;&#39640;&#23545;&#29289;&#29702;&#25915;&#20987;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#38024;&#23545;MDE&#37327;&#36523;&#23450;&#21046;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;MDE&#32593;&#32476;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monocular Depth Estimation (MDE) is a critical component in applications such as autonomous driving. There are various attacks against MDE networks. These attacks, especially the physical ones, pose a great threat to the security of such systems. Traditional adversarial training method requires ground-truth labels hence cannot be directly applied to self-supervised MDE that does not have ground-truth depth. Some self-supervised model hardening techniques (e.g., contrastive learning) ignore the domain knowledge of MDE and can hardly achieve optimal performance. In this work, we propose a novel adversarial training method for self-supervised MDE models based on view synthesis without using ground-truth depth. We improve adversarial robustness against physical-world attacks using L0-norm-bounded perturbation in training. We compare our method with supervised learning based and contrastive learning based methods that are tailored for MDE. Results on two representative MDE networks show tha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;&#25110;&#26412;&#20307;&#23545;&#40784;&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#30340;Truveta Mapper&#26694;&#26550;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#26174;&#24335;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.09767</link><description>&lt;p&gt;
Truveta Mapper&#65306;&#19968;&#20010;&#38646;&#26679;&#26412;&#26412;&#20307;&#26144;&#23556;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09767
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;&#25110;&#26412;&#20307;&#23545;&#40784;&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#30340;Truveta Mapper&#26694;&#26550;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#26174;&#24335;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;(Ontology Matching, OM)&#25110;&#26412;&#20307;&#23545;&#40784;(Ontology Alignment, OA)&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#12290;&#23558;&#26412;&#20307;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#22312;&#28304;&#26412;&#20307;&#22270;&#20013;&#30340;&#33410;&#28857;&#21040;&#30446;&#26631;&#26412;&#20307;&#22270;&#20013;&#30340;&#36335;&#24452;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#25152;&#25552;&#20986;&#30340;Truveta Mapper (TM)&#26694;&#26550;&#21033;&#29992;&#22810;&#20219;&#21153;&#24207;&#21015;&#21040;&#24207;&#21015;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#22810;&#20219;&#21153;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#38544;&#21547;&#22320;&#23398;&#20064;&#19981;&#21516;&#26412;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26080;&#38656;&#20219;&#20309;&#26174;&#24335;&#30340;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;&#36825;&#20063;&#20351;&#24471;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#27169;&#22411;&#20165;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#21644;&#20869;&#37096;&#26412;&#20307;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#35813;&#26041;&#26696;&#20248;&#20110;&#29616;&#26377;&#26631;&#20934;&#22522;&#20934;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;Edit-Similarity&#21644;MINTE+&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a new perspective is suggested for unsupervised Ontology Matching (OM) or Ontology Alignment (OA) by treating it as a translation task. Ontologies are represented as graphs, and the translation is performed from a node in the source ontology graph to a path in the target ontology graph. The proposed framework, Truveta Mapper (TM), leverages a multi-task sequence-to-sequence transformer model to perform alignment across multiple ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables the model to implicitly learn the relationship between different ontologies via transfer-learning without requiring any explicit cross-ontology manually labeled data. This also enables the formulated framework to outperform existing solutions for both runtime latency and alignment quality. The model is pre-trained and fine-tuned only on publicly available text corpus and inner-ontologies data. The proposed solution outperforms state-of-the-art approaches, Edit-Similari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;&#8212;&#8212;&#22240;&#26524;&#19977;&#20803;&#32452;&#65292;&#35813;&#22522;&#20934;&#20855;&#26377;&#21487;&#25805;&#20316;&#30340;&#21453;&#20107;&#23454;&#35774;&#32622;&#21644;&#24178;&#39044;&#24615;&#19979;&#28216;&#20219;&#21153;&#65292;&#23545;&#20998;&#31163;&#21644;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21270;&#65292;&#28982;&#32780;&#22312;&#22240;&#26524;&#20851;&#31995;&#30340;&#35782;&#21035;&#21644;&#24178;&#39044;&#24615;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#27424;&#20339;&#12290;</title><link>http://arxiv.org/abs/2301.05169</link><description>&lt;p&gt;
&#22240;&#26524;&#19977;&#20803;&#32452;&#65306;&#38754;&#21521;&#24178;&#39044;&#20013;&#24515;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#24320;&#25918;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Causal Triplet: An Open Challenge for Intervention-centric Causal Representation Learning. (arXiv:2301.05169v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;&#8212;&#8212;&#22240;&#26524;&#19977;&#20803;&#32452;&#65292;&#35813;&#22522;&#20934;&#20855;&#26377;&#21487;&#25805;&#20316;&#30340;&#21453;&#20107;&#23454;&#35774;&#32622;&#21644;&#24178;&#39044;&#24615;&#19979;&#28216;&#20219;&#21153;&#65292;&#23545;&#20998;&#31163;&#21644;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21270;&#65292;&#28982;&#32780;&#22312;&#22240;&#26524;&#20851;&#31995;&#30340;&#35782;&#21035;&#21644;&#24178;&#39044;&#24615;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#27424;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23398;&#32773;&#20204;&#23545;&#20174;&#24178;&#39044;&#19979;&#30340;&#20302;&#32423;&#22270;&#20687;&#23545;&#20013;&#23398;&#20064;&#39640;&#32423;&#22240;&#26524;&#34920;&#31034;&#20135;&#29983;&#20102;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24448;&#24448;&#23616;&#38480;&#20110;&#31616;&#21333;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#36825;&#36828;&#31163;&#20102;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#19977;&#20803;&#32452;&#65292;&#36825;&#26159;&#19968;&#20010;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;&#65292;&#19981;&#20165;&#20855;&#26377;&#26356;&#20026;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#20004;&#20010;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#20851;&#38190;&#24895;&#26395;&#65306;(i) &#19968;&#20010;&#21487;&#25805;&#20316;&#30340;&#21453;&#20107;&#23454;&#35774;&#32622;&#65292;&#20854;&#20013;&#21482;&#26377;&#26576;&#20123;&#29289;&#20307;&#32423;&#21464;&#37327;&#20801;&#35768;&#21453;&#20107;&#23454;&#35266;&#23519;&#65292;&#32780;&#20854;&#20182;&#21464;&#37327;&#21017;&#19981;&#20801;&#35768;&#65307;(ii) &#19968;&#20010;&#24178;&#39044;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24378;&#35843;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#21407;&#21017;&#19979;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#20998;&#31163;&#30340;&#25110;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#30340;&#30693;&#35782;&#30340;&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#20998;&#24067;&#24335;&#23545;&#24212;&#29289;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#35782;&#21035;&#21487;&#25805;&#20316;&#30340;&#21453;&#20107;&#23454;&#35774;&#32622;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19988;&#22312;&#24178;&#39044;&#24615;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#27424;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a surge of interest in learning high-level causal representations from low-level image pairs under interventions. Yet, existing efforts are largely limited to simple synthetic settings that are far away from real-world problems. In this paper, we present Causal Triplet, a causal representation learning benchmark featuring not only visually more complex scenes, but also two crucial desiderata commonly overlooked in previous works: (i) an actionable counterfactual setting, where only certain object-level variables allow for counterfactual observations whereas others do not; (ii) an interventional downstream task with an emphasis on out-of-distribution robustness from the independent causal mechanisms principle. Through extensive experiments, we find that models built with the knowledge of disentangled or object-centric representations significantly outperform their distributed counterparts. However, recent causal representation learning methods still struggle to id
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;DCAI&#65289;&#30340;&#27010;&#24565;&#65292;&#24378;&#35843;&#25968;&#25454;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#24635;&#32467;&#20102;&#35757;&#32451;&#25968;&#25454;&#24320;&#21457;&#12289;&#25512;&#26029;&#25968;&#25454;&#24320;&#21457;&#21644;&#25968;&#25454;&#32500;&#25252;&#19977;&#20010;&#24635;&#20307;&#20351;&#21629;&#65292;&#25552;&#20379;&#20102;&#23545;DCAI&#20219;&#21153;&#30340;&#35752;&#35770;&#21644;&#35266;&#28857;&#65292;&#24182;&#21015;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.04819</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65306;&#35270;&#35282;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Data-centric AI: Perspectives and Challenges. (arXiv:2301.04819v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04819
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;DCAI&#65289;&#30340;&#27010;&#24565;&#65292;&#24378;&#35843;&#25968;&#25454;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#24635;&#32467;&#20102;&#35757;&#32451;&#25968;&#25454;&#24320;&#21457;&#12289;&#25512;&#26029;&#25968;&#25454;&#24320;&#21457;&#21644;&#25968;&#25454;&#32500;&#25252;&#19977;&#20010;&#24635;&#20307;&#20351;&#21629;&#65292;&#25552;&#20379;&#20102;&#23545;DCAI&#20219;&#21153;&#30340;&#35752;&#35770;&#21644;&#35266;&#28857;&#65292;&#24182;&#21015;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22312;&#26500;&#24314;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26041;&#38754;&#30340;&#20316;&#29992;&#36890;&#36807;&#26032;&#20852;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;DCAI&#65289;&#27010;&#24565;&#24471;&#21040;&#20102;&#26174;&#33879;&#22686;&#24378;&#65292;&#35813;&#27010;&#24565;&#20027;&#24352;&#23558;&#37325;&#28857;&#20174;&#27169;&#22411;&#25913;&#36827;&#36716;&#21521;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#31038;&#21306;&#19968;&#30452;&#22312;&#19981;&#21516;&#26041;&#38754;&#21162;&#21147;&#22686;&#24378;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26159;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#23396;&#31435;&#20030;&#25514;&#12290;&#20026;&#20102;&#25512;&#21160;&#31038;&#21306;&#30340;&#38598;&#20307;&#20513;&#35758;&#24182;&#25512;&#21160;DCAI&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24635;&#20307;&#26694;&#26550;&#65292;&#24182;&#38598;&#21512;&#20102;&#19977;&#20010;&#24635;&#20307;&#20351;&#21629;&#65306;&#35757;&#32451;&#25968;&#25454;&#24320;&#21457;&#12289;&#25512;&#26029;&#25968;&#25454;&#24320;&#21457;&#21644;&#25968;&#25454;&#32500;&#25252;&#12290;&#25105;&#20204;&#23545;&#20195;&#34920;DCAI&#20219;&#21153;&#36827;&#34892;&#20102;&#39640;&#23618;&#27425;&#35752;&#35770;&#24182;&#20998;&#20139;&#20102;&#35266;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21015;&#20986;&#20102;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#26356;&#22810;&#36164;&#28304;&#24635;&#32467;&#35814;&#35265;https://github.com/daochenzha/data-centric-AI&#12290;
&lt;/p&gt;
&lt;p&gt;
The role of data in building AI systems has recently been significantly magnified by the emerging concept of data-centric AI (DCAI), which advocates a fundamental shift from model advancements to ensuring data quality and reliability. Although our community has continuously invested efforts into enhancing data in different aspects, they are often isolated initiatives on specific tasks. To facilitate the collective initiative in our community and push forward DCAI, we draw a big picture and bring together three general missions: training data development, inference data development, and data maintenance. We provide a top-level discussion on representative DCAI tasks and share perspectives. Finally, we list open challenges. More resources are summarized at https://github.com/daochenzha/data-centric-AI
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#21338;&#24328;&#20013;&#35299;&#20915;&#26031;&#22612;&#20811;&#20271;&#26684;&#22343;&#34913;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#8220;&#21487;&#24378;&#21046;&#23653;&#34892;&#30340;&#25910;&#30410;&#36793;&#30028;&#8221;&#65288;EPF&#65289;&#26469;&#36924;&#36817;&#26368;&#20248;&#31574;&#30053;&#22343;&#34913;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#24191;&#20041;&#21644;&#21338;&#24328;&#12290;</title><link>http://arxiv.org/abs/2212.14431</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#35299;&#20915;&#26031;&#22612;&#20811;&#20271;&#26684;&#22343;&#34913;&#30340;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Function Approximation for Solving Stackelberg Equilibrium in Large Perfect Information Games. (arXiv:2212.14431v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14431
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#21338;&#24328;&#20013;&#35299;&#20915;&#26031;&#22612;&#20811;&#20271;&#26684;&#22343;&#34913;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#8220;&#21487;&#24378;&#21046;&#23653;&#34892;&#30340;&#25910;&#30410;&#36793;&#30028;&#8221;&#65288;EPF&#65289;&#26469;&#36924;&#36817;&#26368;&#20248;&#31574;&#30053;&#22343;&#34913;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#24191;&#20041;&#21644;&#21338;&#24328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20989;&#25968;&#36924;&#36817;&#26159;&#35299;&#20915;&#22823;&#22411;&#38646;&#21644;&#28216;&#25103;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22312;&#35299;&#20915;&#24191;&#20041;&#21644;&#21338;&#24328;&#26041;&#38754;&#65292;&#20989;&#25968;&#36924;&#36817;&#24471;&#21040;&#30340;&#20851;&#27880;&#21364;&#24456;&#23569;&#12290;&#24191;&#20041;&#21644;&#21338;&#24328;&#34987;&#24191;&#27867;&#35748;&#20026;&#22312;&#35745;&#31639;&#19978;&#27604;&#23427;&#20204;&#30340;&#23436;&#20840;&#31454;&#20105;&#25110;&#21512;&#20316;&#20249;&#20276;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#23545;&#20110;&#24191;&#20041;&#21644;&#21338;&#24328;&#20013;&#30340;&#35768;&#22810;&#22343;&#34913;&#65292;&#19981;&#23384;&#22312;&#20687;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#38646;&#21644;&#28216;&#25103;&#20013;&#20351;&#29992;&#30340;&#29366;&#24577;&#20215;&#20540;&#20989;&#25968;&#30340;&#31616;&#21333;&#31867;&#27604;&#29289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#8220;&#21487;&#24378;&#21046;&#23653;&#34892;&#30340;&#25910;&#30410;&#36793;&#30028;&#8221;&#65288;EPF&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#24191;&#20041;&#21644;&#21338;&#24328;&#29366;&#24577;&#20215;&#20540;&#20989;&#25968;&#30340;&#19968;&#33324;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;EPF&#24182;&#20351;&#29992;&#36866;&#24403;&#30340;&#22791;&#20221;&#25805;&#20316;&#21644;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#23427;&#20204;&#65292;&#20174;&#32780;&#36924;&#36817;&#20102;&#26368;&#20248;&#30340;&#26031;&#22612;&#20811;&#20271;&#26684;&#24191;&#20041;&#21644;&#21338;&#24328;&#31574;&#30053;&#22343;&#34913;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#20989;&#25968;&#36924;&#36817;&#24212;&#29992;&#20110;&#26031;&#22612;&#20811;&#20271;&#26684;&#22343;&#34913;&#30340;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#21338;&#24328;&#24182;&#20173;&#28982;&#20139;&#21463;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Function approximation (FA) has been a critical component in solving large zero-sum games. Yet, little attention has been given towards FA in solving \textit{general-sum} extensive-form games, despite them being widely regarded as being computationally more challenging than their fully competitive or cooperative counterparts. A key challenge is that for many equilibria in general-sum games, no simple analogue to the state value function used in Markov Decision Processes and zero-sum games exists. In this paper, we propose learning the \textit{Enforceable Payoff Frontier} (EPF) -- a generalization of the state value function for general-sum games. We approximate the optimal \textit{Stackelberg extensive-form correlated equilibrium} by representing EPFs with neural networks and training them by using appropriate backup operations and loss functions. This is the first method that applies FA to the Stackelberg setting, allowing us to scale to much larger games while still enjoying performa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(REVEAL)&#65292;&#20854;&#20013;&#21253;&#21547;&#22235;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#23384;&#20648;&#22120;&#12289;&#32534;&#30721;&#22120;&#12289;&#26816;&#32034;&#22120;&#21644;&#29983;&#25104;&#22120;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#22810;&#31181;&#22810;&#27169;&#24577;&#30693;&#35782;&#28304;&#65292;&#24182;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#39044;&#35757;&#32451;&#65292;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.05221</link><description>&lt;p&gt;
REVEAL: &#26816;&#32034;&#22686;&#24378;&#30340;&#22810;&#28304;&#22810;&#27169;&#24577;&#30693;&#35782;&#23384;&#20648;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory. (arXiv:2212.05221v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(REVEAL)&#65292;&#20854;&#20013;&#21253;&#21547;&#22235;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#23384;&#20648;&#22120;&#12289;&#32534;&#30721;&#22120;&#12289;&#26816;&#32034;&#22120;&#21644;&#29983;&#25104;&#22120;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#22810;&#31181;&#22810;&#27169;&#24577;&#30693;&#35782;&#28304;&#65292;&#24182;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#39044;&#35757;&#32451;&#65292;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(REVEAL)&#65292;&#35813;&#27169;&#22411;&#23398;&#20250;&#23558;&#19990;&#30028;&#30693;&#35782;&#32534;&#30721;&#21040;&#22823;&#35268;&#27169;&#23384;&#20648;&#22120;&#20013;&#65292;&#24182;&#20174;&#20013;&#26816;&#32034;&#20197;&#22238;&#31572;&#30693;&#35782;&#23494;&#38598;&#22411;&#26597;&#35810;&#12290;REVEAL&#30001;&#22235;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;&#23384;&#20648;&#22120;&#12289;&#32534;&#30721;&#22120;&#12289;&#26816;&#32034;&#22120;&#21644;&#29983;&#25104;&#22120;&#12290;&#22823;&#35268;&#27169;&#23384;&#20648;&#22120;&#36890;&#36807;&#32479;&#19968;&#32534;&#30721;&#22120;&#23545;&#21508;&#31181;&#22810;&#27169;&#24577;&#19990;&#30028;&#30693;&#35782;&#26469;&#28304;&#65288;&#22914;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12289;&#38382;&#31572;&#23545;&#12289;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#31561;&#65289;&#36827;&#34892;&#32534;&#30721;&#12290;&#26816;&#32034;&#22120;&#22312;&#23384;&#20648;&#22120;&#20013;&#25214;&#21040;&#26368;&#30456;&#20851;&#30340;&#30693;&#35782;&#26465;&#30446;&#65292;&#29983;&#25104;&#22120;&#23558;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#19982;&#36755;&#20837;&#26597;&#35810;&#34701;&#21512;&#20135;&#29983;&#36755;&#20986;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#26159;&#65292;&#23384;&#20648;&#22120;&#12289;&#32534;&#30721;&#22120;&#12289;&#26816;&#32034;&#22120;&#21644;&#29983;&#25104;&#22120;&#37117;&#22312;&#28023;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#22810;&#31181;&#22810;&#27169;&#24577;&#30693;&#35782;&#28304;&#65292;&#36825;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#25910;&#30410;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;REVEAL&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an end-to-end Retrieval-Augmented Visual Language Model (REVEAL) that learns to encode world knowledge into a large-scale memory, and to retrieve from it to answer knowledge-intensive queries. REVEAL consists of four key components: the memory, the encoder, the retriever and the generator. The large-scale memory encodes various sources of multimodal world knowledge (e.g. image-text pairs, question answering pairs, knowledge graph triplets, etc) via a unified encoder. The retriever finds the most relevant knowledge entries in the memory, and the generator fuses the retrieved knowledge with the input query to produce the output. A key novelty in our approach is that the memory, encoder, retriever and generator are all pre-trained end-to-end on a massive amount of data. Furthermore, our approach can use a diverse set of multimodal knowledge sources, which is shown to result in significant gains. We show that REVEAL achieves state-of-the-art results on visual ques
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#24352;&#23558;AI&#20262;&#29702;&#37325;&#26032;&#26500;&#24314;&#25104;&#19968;&#20010;&#21019;&#26032;&#21152;&#36895;&#22120;&#12290;&#36890;&#36807;&#20998;&#26512;&#31283;&#23450;AI&#19982;OpenAI&#30340;&#23545;&#27604;&#65292;&#24635;&#32467;&#20986;&#20116;&#20010;&#21152;&#36895;AI&#30340;&#20849;&#21516;&#28857;&#65292;&#21363;&#23558;&#19981;&#30830;&#23450;&#24615;&#30475;&#20316;&#31215;&#26497;&#22240;&#32032;&#65292;&#23558;&#21019;&#26032;&#35270;&#20026;&#20869;&#22312;&#20215;&#20540;&#65292;&#37319;&#29992;&#26356;&#22810;AI&#35299;&#20915;AI&#38382;&#39064;&#65292;&#37319;&#29992;&#20998;&#25955;&#30340;&#36807;&#31243;&#25511;&#21046;AI&#30340;&#35768;&#21487;&#21644;&#38480;&#21046;&#65292;&#24182;&#23558;&#20262;&#29702;&#21512;&#24182;&#21040;AI&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#20013;&#12290;&#36825;&#20123;&#20570;&#27861;&#20351;&#20262;&#29702;&#25104;&#20026;&#19968;&#20010;&#28608;&#21169;AI&#21019;&#26032;&#30340;&#21147;&#37327;&#12290;</title><link>http://arxiv.org/abs/2212.01834</link><description>&lt;p&gt;
&#21152;&#36895; AI &#20262;&#29702;&#65306;&#21019;&#26032;&#19982;&#23433;&#20840;&#20043;&#38388;&#30340;&#36777;&#35770;&#65292;&#31283;&#23450; AI &#30340;&#25193;&#25955;&#19982; OpenAI &#30340; Dall-E
&lt;/p&gt;
&lt;p&gt;
Acceleration AI Ethics, the Debate between Innovation and Safety, and Stability AI's Diffusion versus OpenAI's Dall-E. (arXiv:2212.01834v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#23558;AI&#20262;&#29702;&#37325;&#26032;&#26500;&#24314;&#25104;&#19968;&#20010;&#21019;&#26032;&#21152;&#36895;&#22120;&#12290;&#36890;&#36807;&#20998;&#26512;&#31283;&#23450;AI&#19982;OpenAI&#30340;&#23545;&#27604;&#65292;&#24635;&#32467;&#20986;&#20116;&#20010;&#21152;&#36895;AI&#30340;&#20849;&#21516;&#28857;&#65292;&#21363;&#23558;&#19981;&#30830;&#23450;&#24615;&#30475;&#20316;&#31215;&#26497;&#22240;&#32032;&#65292;&#23558;&#21019;&#26032;&#35270;&#20026;&#20869;&#22312;&#20215;&#20540;&#65292;&#37319;&#29992;&#26356;&#22810;AI&#35299;&#20915;AI&#38382;&#39064;&#65292;&#37319;&#29992;&#20998;&#25955;&#30340;&#36807;&#31243;&#25511;&#21046;AI&#30340;&#35768;&#21487;&#21644;&#38480;&#21046;&#65292;&#24182;&#23558;&#20262;&#29702;&#21512;&#24182;&#21040;AI&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#20013;&#12290;&#36825;&#20123;&#20570;&#27861;&#20351;&#20262;&#29702;&#25104;&#20026;&#19968;&#20010;&#28608;&#21169;AI&#21019;&#26032;&#30340;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479; AI &#20262;&#29702;&#30340;&#19968;&#20010;&#21453;&#23545;&#24847;&#35265;&#26159;&#23427;&#20250;&#20943;&#32531;&#21019;&#26032;&#12290;&#26412;&#25991;&#22238;&#24212;&#20102;&#36825;&#19968;&#35266;&#28857;&#65292;&#25226;&#20262;&#29702;&#37325;&#26032;&#26500;&#24314;&#25104;&#19968;&#20010;&#21019;&#26032;&#21152;&#36895;&#22120;&#12290;&#20851;&#38190;&#20803;&#32032;&#26469;&#33258;&#31283;&#23450; AI &#30340;&#25193;&#25955;&#21644; OpenAI &#30340; Dall-E &#20043;&#38388;&#30340;&#23545;&#27604;&#12290;&#36890;&#36807;&#20998;&#26512;&#23427;&#20204;&#25152;&#25903;&#25345;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#25112;&#30053;&#32972;&#21518;&#30340;&#19981;&#21516;&#20215;&#20540;&#35266;&#65292;&#35782;&#21035;&#20986;&#20116;&#20010;&#27010;&#24565;&#20316;&#20026;&#21152;&#36895;&#20262;&#29702;&#30340;&#20849;&#21516;&#28857;&#12290;&#19981;&#30830;&#23450;&#24615;&#34987;&#29702;&#35299;&#20026;&#31215;&#26497;&#21644;&#40723;&#21169;&#24615;&#30340;&#65292;&#32780;&#19981;&#26159;&#26377;&#25152;&#36943;&#21046;&#12290;&#21019;&#26032;&#34987;&#35748;&#20026;&#26159;&#20869;&#22312;&#26377;&#20215;&#20540;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#36890;&#36807;&#31038;&#20250;&#24433;&#21709;&#25165;&#26377;&#20215;&#20540;&#12290;AI &#38382;&#39064;&#36890;&#36807;&#26356;&#22810;&#30340; AI &#26469;&#35299;&#20915;&#65292;&#32780;&#19981;&#26159;&#26356;&#23569;&#12290;&#25511;&#21046; AI &#30340;&#35768;&#21487;&#21644;&#38480;&#21046;&#26469;&#33258;&#20998;&#25955;&#30340;&#36807;&#31243;&#65292;&#32780;&#19981;&#26159;&#32479;&#19968;&#30340;&#26435;&#23041;&#12290;&#20262;&#29702;&#30340;&#24037;&#20316;&#26159;&#23884;&#20837;&#22312; AI &#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#20013;&#65292;&#32780;&#19981;&#26159;&#20174;&#22806;&#37096;&#21457;&#25381;&#20316;&#29992;&#12290;&#36825;&#20123;&#24577;&#24230;&#21644;&#20570;&#27861;&#20849;&#21516;&#20351;&#20262;&#29702;&#25104;&#20026;&#28608;&#21169;&#20154;&#24037;&#26234;&#33021;&#65292;&#32780;&#19981;&#26159;&#21046;&#32422;&#23427;&#30340;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
One objection to conventional AI ethics is that it slows innovation. This presentation responds by reconfiguring ethics as an innovation accelerator. The critical elements develop from a contrast between Stability AI's Diffusion and OpenAI's Dall-E. By analyzing the divergent values underlying their opposed strategies for development and deployment, five conceptions are identified as common to acceleration ethics. Uncertainty is understood as positive and encouraging, rather than discouraging. Innovation is conceived as intrinsically valuable, instead of worthwhile only as mediated by social effects. AI problems are solved by more AI, not less. Permissions and restrictions governing AI emerge from a decentralized process, instead of a unified authority. The work of ethics is embedded in AI development and application, instead of functioning from outside. Together, these attitudes and practices remake ethics as provoking rather than restraining artificial intelligence.
&lt;/p&gt;</description></item><item><title>SinGRAF&#26159;&#19968;&#20010;3D&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21482;&#38656;&#23569;&#37327;&#36755;&#20837;&#19968;&#21333;&#20010;&#22330;&#26223;&#30340;&#22270;&#20687;&#21363;&#21487;&#35757;&#32451;&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#36755;&#20837;&#22806;&#35266;&#30340;&#21516;&#26102;&#29983;&#25104;&#22810;&#26679;&#30340;3D&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2211.17260</link><description>&lt;p&gt;
SinGRAF&#65306;&#23398;&#20064;&#21333;&#20010;&#22330;&#26223;&#30340;3D&#29983;&#25104;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene. (arXiv:2211.17260v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17260
&lt;/p&gt;
&lt;p&gt;
SinGRAF&#26159;&#19968;&#20010;3D&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21482;&#38656;&#23569;&#37327;&#36755;&#20837;&#19968;&#21333;&#20010;&#22330;&#26223;&#30340;&#22270;&#20687;&#21363;&#21487;&#35757;&#32451;&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#36755;&#20837;&#22806;&#35266;&#30340;&#21516;&#26102;&#29983;&#25104;&#22810;&#26679;&#30340;3D&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#21512;&#25104;&#36924;&#30495;&#30340;3D&#29289;&#20307;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SinGRAF&#65292;&#36825;&#26159;&#19968;&#20010;3D&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21482;&#38656;&#23569;&#37327;&#36755;&#20837;&#19968;&#21333;&#20010;&#22330;&#26223;&#30340;&#22270;&#20687;&#21363;&#21487;&#35757;&#32451;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;SinGRAF&#21487;&#20197;&#29983;&#25104;&#20445;&#30041;&#36755;&#20837;&#22806;&#35266;&#24182;&#21464;&#21270;&#22330;&#26223;&#24067;&#23616;&#30340;&#19981;&#21516;3D&#22330;&#26223;&#23454;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#36817;&#26399;3D GAN&#26550;&#26500;&#30340;&#36827;&#23637;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28176;&#36827;&#24335;&#35268;&#27169;&#34917;&#19969;&#21028;&#21035;&#26041;&#27861;&#12290;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;SinGRAF&#20135;&#29983;&#30340;&#32467;&#26524;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#37117;&#36828;&#36828;&#20248;&#20110;&#26368;&#25509;&#36817;&#30340;&#30456;&#20851;&#20316;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have shown great promise in synthesizing photorealistic 3D objects, but they require large amounts of training data. We introduce SinGRAF, a 3D-aware generative model that is trained with a few input images of a single scene. Once trained, SinGRAF generates different realizations of this 3D scene that preserve the appearance of the input while varying scene layout. For this purpose, we build on recent progress in 3D GAN architectures and introduce a novel progressive-scale patch discrimination approach during training. With several experiments, we demonstrate that the results produced by SinGRAF outperform the closest related works in both quality and diversity by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#35268;&#21017;&#30340;&#26377;&#38024;&#23545;&#24615;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#20813;&#24050;&#30693;&#23376;&#20248;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21306;&#22495;&#26469;&#26356;&#24555;&#22320;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#30340;&#25910;&#25947;&#65292;&#24182;&#22312;&#19968;&#20010;&#25151;&#38388;&#28201;&#24230;&#25511;&#21046;&#26696;&#20363;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;6-7&#20493;&#30340;&#36895;&#24230;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2211.16691</link><description>&lt;p&gt;
&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#31616;&#21333;&#35268;&#21017;&#30340;&#26377;&#38024;&#23545;&#24615;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging simple Rules. (arXiv:2211.16691v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#35268;&#21017;&#30340;&#26377;&#38024;&#23545;&#24615;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#20813;&#24050;&#30693;&#23376;&#20248;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21306;&#22495;&#26469;&#26356;&#24555;&#22320;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#30340;&#25910;&#25947;&#65292;&#24182;&#22312;&#19968;&#20010;&#25151;&#38388;&#28201;&#24230;&#25511;&#21046;&#26696;&#20363;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;6-7&#20493;&#30340;&#36895;&#24230;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#30001;&#20110;&#38656;&#35201;&#31351;&#20030;&#25506;&#32034;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20197;&#25214;&#21040;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#32780;&#23548;&#33268;&#26679;&#26412;&#22797;&#26434;&#24230;&#19981;&#22826;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#31995;&#32479;&#30340;&#19987;&#23478;&#30693;&#35782;&#36890;&#24120;&#20801;&#35768;&#25105;&#20204;&#35774;&#35745;&#31616;&#21333;&#35268;&#21017;&#65292;&#25105;&#20204;&#26399;&#26395;&#33391;&#22909;&#30340;&#31574;&#30053;&#22987;&#32456;&#36981;&#24490;&#36825;&#20123;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36830;&#32493;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#20197;&#32435;&#20837;&#36825;&#20123;&#35268;&#21017;&#24182;&#36991;&#20813;&#24050;&#30693;&#23376;&#20248;&#30340;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21306;&#22495;&#65292;&#20174;&#32780;&#26174;&#30528;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#30340;&#25913;&#36827;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#20195;&#29702;&#31243;&#24207;&#36873;&#25321;&#30340;&#21160;&#20316;&#19981;&#31526;&#21512;&#25105;&#20204;&#30340;&#30452;&#35273;&#65292;&#25105;&#20204;&#20250;&#39281;&#21644;&#36825;&#20123;&#21160;&#20316;&#65292;&#20851;&#38190;&#26159;&#20462;&#25913;&#31574;&#30053;&#30340;&#26799;&#24230;&#26356;&#26032;&#27493;&#39588;&#65292;&#20197;&#30830;&#20445;&#23398;&#20064;&#27969;&#31243;&#19981;&#21463;&#39281;&#21644;&#27493;&#39588;&#30340;&#24433;&#21709;&#12290;&#22312;&#19968;&#20010;&#25151;&#38388;&#28201;&#24230;&#25511;&#21046;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#23427;&#20351;&#20195;&#29702;&#31243;&#24207;&#20197;&#27604;&#20256;&#32479;&#20195;&#29702;&#31243;&#24207;&#24555;6-7&#20493;&#30340;&#36895;&#24230;&#25910;&#25947;&#21040;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#65292;&#32780;&#19981;&#38656;&#35201;&#28040;&#32791;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) generally suffers from poor sample complexity, mostly due to the need to exhaustively explore the state-action space to find well-performing policies. On the other hand, we postulate that expert knowledge of the system often allows us to design simple rules we expect good policies to follow at all times. In this work, we hence propose a simple yet effective modification of continuous actor-critic frameworks to incorporate such rules and avoid regions of the state-action space that are known to be suboptimal, thereby significantly accelerating the convergence of RL agents. Concretely, we saturate the actions chosen by the agent if they do not comply with our intuition and, critically, modify the gradient update step of the policy to ensure the learning process is not affected by the saturation step. On a room temperature control case study, it allows agents to converge to well-performing policies up to 6-7x faster than classical agents without computational o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#35302;&#24335;&#30340;&#26041;&#27861;&#65292;&#24341;&#23548;&#35270;&#35273;Transformer&#23398;&#20064;&#22810;&#35270;&#35282;&#20960;&#20309;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26497;&#32447;&#26469;&#24341;&#23548;Transformer&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#22270;&#65292;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#19981;&#38656;&#35201;&#25552;&#20379;&#20219;&#20309;&#25668;&#20687;&#26426;&#23039;&#24577;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#23039;&#24577;&#19981;&#21464;&#30340;&#29289;&#20307;&#23454;&#20363;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2211.15107</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#25945;&#25480;Transformer&#22810;&#35270;&#35282;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
A Light Touch Approach to Teaching Transformers Multi-view Geometry. (arXiv:2211.15107v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#35302;&#24335;&#30340;&#26041;&#27861;&#65292;&#24341;&#23548;&#35270;&#35273;Transformer&#23398;&#20064;&#22810;&#35270;&#35282;&#20960;&#20309;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26497;&#32447;&#26469;&#24341;&#23548;Transformer&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#22270;&#65292;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#19981;&#38656;&#35201;&#25552;&#20379;&#20219;&#20309;&#25668;&#20687;&#26426;&#23039;&#24577;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#23039;&#24577;&#19981;&#21464;&#30340;&#29289;&#20307;&#23454;&#20363;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35270;&#35273;&#23398;&#20064;&#20013;&#34920;&#29616;&#24378;&#22823;&#65292;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#23427;&#20204;&#32570;&#20047;&#25163;&#21160;&#35268;&#23450;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#28789;&#27963;&#24615;&#22312;&#28041;&#21450;&#22810;&#35270;&#35282;&#20960;&#20309;&#30340;&#20219;&#21153;&#20013;&#21487;&#33021;&#20250;&#25104;&#20026;&#38382;&#39064;&#65292;&#22240;&#20026;3D&#24418;&#29366;&#21644;&#35270;&#28857;&#30340;&#36817;&#20046;&#26080;&#38480;&#21487;&#33021;&#30340;&#21464;&#21270;&#38656;&#35201;&#28789;&#27963;&#24615;&#65292;&#32780;&#25237;&#24433;&#20960;&#20309;&#30340;&#31934;&#30830;&#24615;&#21017;&#38656;&#35201;&#20005;&#26684;&#30340;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#36731;&#35302;&#8221;&#26041;&#27861;&#65292;&#24341;&#23548;&#35270;&#35273;Transformer&#23398;&#20064;&#22810;&#35270;&#35282;&#20960;&#20309;&#65292;&#20294;&#22312;&#38656;&#35201;&#26102;&#20801;&#35768;&#23427;&#20204;&#33258;&#30001;&#21457;&#25381;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26497;&#32447;&#26469;&#24341;&#23548;Transformer&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#22270;&#65292;&#24809;&#32602;&#26497;&#32447;&#20197;&#22806;&#30340;&#27880;&#24847;&#20540;&#65292;&#24182;&#40723;&#21169;&#27839;&#36825;&#20123;&#32447;&#30340;&#26356;&#39640;&#30340;&#27880;&#24847;&#65292;&#22240;&#20026;&#23427;&#20204;&#21253;&#21547;&#20960;&#20309;&#19978;&#21512;&#29702;&#30340;&#21305;&#37197;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#25552;&#20379;&#20219;&#20309;&#25668;&#20687;&#26426;&#23039;&#24577;&#20449;&#24687;&#12290;&#25105;&#20204;&#20851;&#27880;&#20110;&#23039;&#24577;&#19981;&#21464;&#30340;&#29289;&#20307;&#23454;&#20363;&#26816;&#32034;&#65292;&#26631;&#20934;&#30340;Transformer&#32593;&#32476;&#30001;&#20110;&#19981;&#21516;&#23039;&#24577;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#32780;&#38590;&#20197;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are powerful visual learners, in large part due to their conspicuous lack of manually-specified priors. This flexibility can be problematic in tasks that involve multiple-view geometry, due to the near-infinite possible variations in 3D shapes and viewpoints (requiring flexibility), and the precise nature of projective geometry (obeying rigid laws). To resolve this conundrum, we propose a "light touch" approach, guiding visual Transformers to learn multiple-view geometry but allowing them to break free when needed. We achieve this by using epipolar lines to guide the Transformer's cross-attention maps, penalizing attention values outside the epipolar lines and encouraging higher attention along these lines since they contain geometrically plausible matches. Unlike previous methods, our proposal does not require any camera pose information at test-time. We focus on pose-invariant object instance retrieval, where standard Transformer networks struggle, due to the large diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#38750;&#30417;&#30563;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20013;&#22256;&#25200;UISS&#27169;&#22411;&#30340;&#29305;&#24449;&#23545;&#40784;&#21644;&#29305;&#24449;&#22343;&#21248;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Semantic Attention Network(SAN) &#27169;&#22411;&#65292;&#21253;&#21547;&#19968;&#20010;&#26032;&#27169;&#22359; semantic attention&#65288;SEAT&#65289;&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#36880;&#20687;&#32032;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#38750;&#30417;&#30563;&#20998;&#21106;&#26694;&#26550;&#19987;&#27880;&#20110;&#25429;&#25417;&#35821;&#20041;&#34920;&#31034;&#65292;&#22312;&#22810;&#20010;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2211.14513</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#38750;&#30417;&#30563;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Alignment and Uniformity in Unsupervised Image Semantic Segmentation. (arXiv:2211.14513v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#38750;&#30417;&#30563;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20013;&#22256;&#25200;UISS&#27169;&#22411;&#30340;&#29305;&#24449;&#23545;&#40784;&#21644;&#29305;&#24449;&#22343;&#21248;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Semantic Attention Network(SAN) &#27169;&#22411;&#65292;&#21253;&#21547;&#19968;&#20010;&#26032;&#27169;&#22359; semantic attention&#65288;SEAT&#65289;&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#36880;&#20687;&#32032;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#38750;&#30417;&#30563;&#20998;&#21106;&#26694;&#26550;&#19987;&#27880;&#20110;&#25429;&#25417;&#35821;&#20041;&#34920;&#31034;&#65292;&#22312;&#22810;&#20010;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#30417;&#30563;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;(UISS)&#26088;&#22312;&#23558;&#20302;&#23618;&#35270;&#35273;&#29305;&#24449;&#19982;&#35821;&#20041;&#32423;&#21035;&#30340;&#34920;&#31034;&#21305;&#37197;&#65292;&#32780;&#26080;&#38656;&#22806;&#37096;&#30417;&#31649;&#12290;&#26412;&#25991;&#20174;&#29305;&#24449;&#23545;&#40784;&#21644;&#29305;&#24449;&#22343;&#21248;&#24615;&#30340;&#35282;&#24230;&#25506;&#31350;&#20102;UISS&#27169;&#22411;&#30340;&#20851;&#38190;&#24615;&#36136;&#65292;&#24182;&#23558;UISS&#19982;&#25972;&#24133;&#22270;&#20687;&#30340;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#35748;&#20026;UISS&#20013;&#29616;&#26377;&#30340;&#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#26041;&#27861;&#23384;&#22312;&#34920;&#31034;&#23849;&#28291;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#32593;&#32476;&#27169;&#22411;&#8212;&#8212;Semantic Attention Network(SAN)&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22359;Semantic Attention(SEAT)&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#36880;&#20687;&#32032;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#22312;&#22810;&#20010;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#38750;&#30417;&#30563;&#20998;&#21106;&#26694;&#26550;&#19987;&#27880;&#20110;&#25429;&#25417;&#35821;&#20041;&#34920;&#31034;&#65292;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#26410;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#19968;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised image semantic segmentation(UISS) aims to match low-level visual features with semantic-level representations without outer supervision. In this paper, we address the critical properties from the view of feature alignments and feature uniformity for UISS models. We also make a comparison between UISS and image-wise representation learning. Based on the analysis, we argue that the existing MI-based methods in UISS suffer from representation collapse. By this, we proposed a robust network called Semantic Attention Network(SAN), in which a new module Semantic Attention(SEAT) is proposed to generate pixel-wise and semantic features dynamically. Experimental results on multiple semantic segmentation benchmarks show that our unsupervised segmentation framework specializes in catching semantic representations, which outperforms all the unpretrained and even several pretrained methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;PIP&#65288;&#20301;&#32622;&#32534;&#30721;&#22270;&#20687;&#20808;&#39564;&#65289;&#19982;DIP&#34920;&#29616;&#30456;&#20284;&#65292;&#20294;&#25152;&#38656;&#21442;&#25968;&#26356;&#23569;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;3D-DIP&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.14298</link><description>&lt;p&gt;
PIP&#65306;&#20301;&#32622;&#32534;&#30721;&#22270;&#20687;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
PIP: Positional-encoding Image Prior. (arXiv:2211.14298v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;PIP&#65288;&#20301;&#32622;&#32534;&#30721;&#22270;&#20687;&#20808;&#39564;&#65289;&#19982;DIP&#34920;&#29616;&#30456;&#20284;&#65292;&#20294;&#25152;&#38656;&#21442;&#25968;&#26356;&#23569;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;3D-DIP&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#65288;DIP&#65289;&#20013;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#34987;&#36866;&#37197;&#20026;&#23558;&#28508;&#31354;&#38388;&#26144;&#23556;&#21040;&#38477;&#36136;&#65288;&#20363;&#22914;&#22122;&#38899;&#65289;&#22270;&#20687;&#65292;&#20294;&#22312;&#27492;&#36807;&#31243;&#20013;&#23398;&#20064;&#37325;&#24314;&#24178;&#20928;&#22270;&#20687;&#12290;&#36825;&#31181;&#29616;&#35937;&#24402;&#22240;&#20110;CNN&#30340;&#20869;&#37096;&#22270;&#20687;&#20808;&#39564;&#12290;&#25105;&#20204;&#20174;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;DIP&#26694;&#26550;&#12290;&#21463;&#21040;&#36825;&#31181;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#29992;&#20613;&#37324;&#21494;&#29305;&#24449;&#65288;&#20301;&#32622;&#32534;&#30721;&#65289;&#26367;&#25442;&#20102;&#38543;&#26426;&#25110;&#23398;&#20064;&#24471;&#21040;&#30340;&#28508;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#20110;&#20613;&#37324;&#21494;&#29305;&#24449;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#31616;&#21333;&#30340;&#20687;&#32032;&#32423;MLP&#26367;&#25442;&#21367;&#31215;&#23618;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#26696;&#21629;&#21517;&#20026;&#8220;&#20301;&#32622;&#32534;&#30721;&#22270;&#20687;&#20808;&#39564;&#8221;&#65288;PIP&#65289;&#65292;&#24182;&#23637;&#31034;&#23427;&#22312;&#21508;&#31181;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#20013;&#19982;DIP&#34920;&#29616;&#38750;&#24120;&#30456;&#20284;&#65292;&#20294;&#25152;&#38656;&#30340;&#21442;&#25968;&#35201;&#23569;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;PIP&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#35270;&#39057;&#65292;&#20854;&#20013;3D-DIP&#34920;&#29616;&#19981;&#20339;&#19988;&#19981;&#31283;&#23450;&#12290;&#25152;&#26377;&#20219;&#21153;&#30340;&#20195;&#30721;&#21644;&#20854;&#20182;&#20363;&#23376;&#65288;&#21253;&#25324;&#35270;&#39057;&#65289;&#22343;&#21487;&#22312;&#39033;&#30446;&#39029;&#38754;http://people.csail.mit.edu/yilun/pip/&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Deep Image Prior (DIP), a Convolutional Neural Network (CNN) is fitted to map a latent space to a degraded (e.g. noisy) image but in the process learns to reconstruct the clean image. This phenomenon is attributed to CNN's internal image-prior. We revisit the DIP framework, examining it from the perspective of a neural implicit representation. Motivated by this perspective, we replace the random or learned latent with Fourier-Features (Positional Encoding). We show that thanks to the Fourier features properties, we can replace the convolution layers with simple pixel-level MLPs. We name this scheme ``Positional Encoding Image Prior" (PIP) and exhibit that it performs very similarly to DIP on various image-reconstruction tasks with much less parameters required. Additionally, we demonstrate that PIP can be easily extended to videos, where 3D-DIP struggles and suffers from instability. Code and additional examples for all tasks, including videos, are available on the project page http
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;N&#20803;&#26597;&#35810;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21452;&#24322;&#26500;Transformer&#32534;&#30721;&#22120;&#21644;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#26469;&#28385;&#36275;&#25152;&#26377;N&#20803;&#30340;FOL&#26597;&#35810;&#65292;&#21253;&#25324;&#23384;&#22312;&#37327;&#35789;&#12289;&#21512;&#21462;&#12289;&#26512;&#21462;&#21644;&#21542;&#23450;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#22788;&#29702;&#31639;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#25110;&#39044;&#27979;&#20219;&#24847;&#30340;N&#20803;FOL&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2211.13469</link><description>&lt;p&gt;
NQE: &#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#20013;&#30340;N&#20803;&#26597;&#35810;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
NQE: N-ary Query Embedding for Complex Query Answering over Hyper-Relational Knowledge Graphs. (arXiv:2211.13469v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;N&#20803;&#26597;&#35810;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21452;&#24322;&#26500;Transformer&#32534;&#30721;&#22120;&#21644;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#26469;&#28385;&#36275;&#25152;&#26377;N&#20803;&#30340;FOL&#26597;&#35810;&#65292;&#21253;&#25324;&#23384;&#22312;&#37327;&#35789;&#12289;&#21512;&#21462;&#12289;&#26512;&#21462;&#21644;&#21542;&#23450;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#22788;&#29702;&#31639;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#25110;&#39044;&#27979;&#20219;&#24847;&#30340;N&#20803;FOL&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#26159;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#22810;&#36339;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#38480;&#20110;&#22312;&#20108;&#20803;&#20851;&#31995;&#20107;&#23454;&#20013;&#36827;&#34892;&#26597;&#35810;&#65292;&#24182;&#19988;&#23545;&#21253;&#21547;&#22810;&#20010;&#23454;&#20307;&#30340;N&#20803;&#20107;&#23454;&#65288;n &gt;= 2&#65289;&#20851;&#27880;&#36739;&#23569;&#65292;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#26356;&#20026;&#26222;&#36941;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;CQA&#26041;&#27861;&#20165;&#33021;&#20026;&#19968;&#20123;&#29305;&#23450;&#31867;&#22411;&#30340;&#26597;&#35810;&#20570;&#20986;&#39044;&#27979;&#65292;&#24182;&#19988;&#19981;&#33021;&#28789;&#27963;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#30340;&#36923;&#36753;&#26597;&#35810;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#20013;CQA&#30340;N&#20803;&#26597;&#35810;&#23884;&#20837;&#65288;NQE&#65289;&#27169;&#22411;&#65292;&#36825;&#20123;&#30693;&#35782;&#22270;&#35889;&#21253;&#25324;&#22823;&#37327;&#30340;N&#20803;&#20107;&#23454;&#12290;NQE&#21033;&#29992;&#21452;&#24322;&#26500;Transformer&#32534;&#30721;&#22120;&#21644;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#26469;&#28385;&#36275;&#25152;&#26377;N&#20803;FOL&#26597;&#35810;&#65292;&#21253;&#25324;&#23384;&#22312;&#37327;&#35789;&#12289;&#21512;&#21462;&#12289;&#26512;&#21462;&#21644;&#21542;&#23450;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#22788;&#29702;&#31639;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#25110;&#39044;&#27979;&#20219;&#24847;&#30340;N&#20803;FOL&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex query answering (CQA) is an essential task for multi-hop and logical reasoning on knowledge graphs (KGs). Currently, most approaches are limited to queries among binary relational facts and pay less attention to n-ary facts (n&gt;=2) containing more than two entities, which are more prevalent in the real world. Moreover, previous CQA methods can only make predictions for a few given types of queries and cannot be flexibly extended to more complex logical queries, which significantly limits their applications. To overcome these challenges, in this work, we propose a novel N-ary Query Embedding (NQE) model for CQA over hyper-relational knowledge graphs (HKGs), which include massive n-ary facts. The NQE utilizes a dual-heterogeneous Transformer encoder and fuzzy logic theory to satisfy all n-ary FOL queries, including existential quantifiers, conjunction, disjunction, and negation. We also propose a parallel processing algorithm that can train or predict arbitrary n-ary FOL queries i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20381;&#36182;&#22270;&#30340;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;(GraphS4mer)&#65292;&#29992;&#20110;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26550;&#26500;&#21644;&#21160;&#24577;&#28436;&#21464;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#23618;&#26469;&#35299;&#20915;&#38271;&#26102;&#24207;&#21644;&#22797;&#26434;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11176</link><description>&lt;p&gt;
&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models. (arXiv:2211.11176v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20381;&#36182;&#22270;&#30340;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;(GraphS4mer)&#65292;&#29992;&#20110;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26550;&#26500;&#21644;&#21160;&#24577;&#28436;&#21464;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#23618;&#26469;&#35299;&#20915;&#38271;&#26102;&#24207;&#21644;&#22797;&#26434;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#22312;&#35768;&#22810;&#21307;&#23398;&#39046;&#22495;&#20013;&#37117;&#24456;&#26222;&#36941;&#65292;&#20363;&#22914;&#33041;&#30005;&#22270;&#12289;&#22810;&#23548;&#30561;&#30496;&#22270;&#21644;&#24515;&#30005;&#22270;&#12290;&#30001;&#20110;&#65288;1&#65289;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#65288;2&#65289;&#30005;&#26497;&#20043;&#38388;&#22797;&#26434;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#34920;&#31034;&#20026;&#26102;&#38388;&#20381;&#36182;&#22270;&#65292;&#24182;&#20171;&#32461;&#20102;GraphS4mer&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#32467;&#26500;&#65292;&#36890;&#36807;&#24314;&#31435;&#29983;&#29289;&#20449;&#21495;&#20013;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#26469;&#25552;&#39640;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;1&#65289;&#25105;&#20204;&#21033;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26550;&#26500;&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24207;&#21015;&#27169;&#22411;&#65292;&#26469;&#25429;&#25417;&#29983;&#29289;&#20449;&#21495;&#20013;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#65288;2&#65289;&#25105;&#20204;&#24314;&#35758;&#22312;GraphS4mer&#20013;&#28155;&#21152;&#22270;&#32467;&#26500;&#23398;&#20064;&#23618;&#65292;&#20197;&#23398;&#20064;&#25968;&#25454;&#20013;&#21160;&#24577;&#28436;&#21464;&#30340;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20248;&#20110;&#20960;&#31181;&#22522;&#20934;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#24314;&#31435;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate biosignals are prevalent in many medical domains, such as electroencephalography, polysomnography, and electrocardiography. Modeling spatiotemporal dependencies in multivariate biosignals is challenging due to (1) long-range temporal dependencies and (2) complex spatial correlations between the electrodes. To address these challenges, we propose representing multivariate biosignals as time-dependent graphs and introduce GraphS4mer, a general graph neural network (GNN) architecture that improves performance on biosignal classification tasks by modeling spatiotemporal dependencies in biosignals. Specifically, (1) we leverage the Structured State Space architecture, a state-of-the-art deep sequence model, to capture long-range temporal dependencies in biosignals and (2) we propose a graph structure learning layer in GraphS4mer to learn dynamically evolving graph structures in the data. We evaluate our proposed model on three distinct biosignal classification tasks and show th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#30340;&#32534;&#30721;&#22120;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#23398;&#35821;&#20041;&#65292;&#27604;&#33258;&#21160;&#32534;&#30721;&#22120;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08142</link><description>&lt;p&gt;
&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#35821;&#20041;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Semantic Representations of Mathematical Expressions in a Continuous Vector Space. (arXiv:2211.08142v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08142
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#30340;&#32534;&#30721;&#22120;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#23398;&#35821;&#20041;&#65292;&#27604;&#33258;&#21160;&#32534;&#30721;&#22120;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#31526;&#21495;&#22312;STEM&#25991;&#29486;&#20013;&#21344;&#25454;&#20102;&#24456;&#22823;&#19968;&#37096;&#20998;&#65292;&#20294;&#26159;&#65292;&#20026;&#20844;&#24335;&#25214;&#21040;&#35821;&#20041;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#25968;&#23398;&#31526;&#21495;&#26159;&#31934;&#30830;&#30340;&#65292;&#22312;&#23383;&#31526;&#24494;&#23567;&#21464;&#21270;&#26102;&#20854;&#21547;&#20041;&#20250;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#33258;&#28982;&#25991;&#26412;&#30340;&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#34920;&#31034;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#30340;&#32534;&#30721;&#22120;&#65292;&#35757;&#32451;&#20854;&#22312;&#35270;&#35273;&#19978;&#19981;&#21516;&#20294;&#22312;&#25968;&#23398;&#19978;&#31561;&#20215;&#30340;&#34920;&#36798;&#24335;&#19978;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#65288;&#25110;&#23884;&#20837;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#21069;&#32773;&#26356;&#33021;&#25429;&#25417;&#25968;&#23398;&#35821;&#20041;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20221;&#31561;&#20215;&#30340;&#36229;&#36234;&#21644;&#20195;&#25968;&#34920;&#36798;&#24335;&#23545;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical notation makes up a large portion of STEM literature, yet, finding semantic representations for formulae remains a challenging problem. Because mathematical notation is precise, and its meaning changes significantly with small character shifts, the methods that work for natural text do not necessarily work well for mathematical expressions. In this work, we describe an approach for representing mathematical expressions in a continuous vector space. We use the encoder of a sequence-to-sequence architecture, trained on visually different but mathematically equivalent expressions, to generate vector representations (or embeddings). We compare this approach with an autoencoder and show that the former is better at capturing mathematical semantics. Finally, to expedite future research, we publish a corpus of equivalent transcendental and algebraic expression pairs.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21487;&#35299;&#37322;&#34892;&#20026;&#24314;&#35758;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#29983;&#21487;&#20197;&#29702;&#35299;&#25152;&#23398;&#30340;&#20869;&#23481;&#24182;&#36827;&#34892;&#25512;&#29702;&#20174;&#32780;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;</title><link>http://arxiv.org/abs/2211.07882</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Explainable Action Advising for Multi-Agent Reinforcement Learning. (arXiv:2211.07882v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07882
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21487;&#35299;&#37322;&#34892;&#20026;&#24314;&#35758;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#29983;&#21487;&#20197;&#29702;&#35299;&#25152;&#23398;&#30340;&#20869;&#23481;&#24182;&#36827;&#34892;&#25512;&#29702;&#20174;&#32780;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#24314;&#35758;&#26159;&#19968;&#31181;&#22522;&#20110;&#24072;&#29983;&#33539;&#24335;&#30340;&#24378;&#21270;&#23398;&#20064;&#30693;&#35782;&#36716;&#31227;&#25216;&#26415;&#12290;&#19987;&#23478;&#32769;&#24072;&#22312;&#35757;&#32451;&#26399;&#38388;&#25552;&#20379;&#24314;&#35758;&#65292;&#20197;&#25552;&#39640;&#23398;&#29983;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#31574;&#30053;&#34920;&#29616;&#12290;&#36825;&#31181;&#24314;&#35758;&#36890;&#24120;&#20197;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24418;&#24335;&#32473;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20351;&#24471;&#23398;&#29983;&#38590;&#20197;&#25512;&#29702;&#21644;&#24212;&#29992;&#20110;&#26032;&#39062;&#29366;&#24577;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#34892;&#20026;&#24314;&#35758;&#65292;&#20854;&#20013;&#32769;&#24072;&#25552;&#20379;&#34892;&#20026;&#24314;&#35758;&#21644;&#30456;&#20851;&#30340;&#35299;&#37322;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#36873;&#21462;&#35813;&#34892;&#20026;.&#36825;&#20801;&#35768;&#23398;&#29983;&#33258;&#25105;&#21453;&#24605;&#25152;&#23398;&#30340;&#20869;&#23481;&#65292;&#23454;&#29616;&#24314;&#35758;&#30340;&#27867;&#21270;&#65292;&#24182;&#23548;&#33268;&#23398;&#20064;&#25928;&#29575;&#30340;&#25552;&#39640;&#8212;&#8212;&#21363;&#20351;&#22312;&#32769;&#24072;&#19981;&#29702;&#24819;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#31574;&#30053;&#22238;&#25253;&#21644;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action advising is a knowledge transfer technique for reinforcement learning based on the teacher-student paradigm. An expert teacher provides advice to a student during training in order to improve the student's sample efficiency and policy performance. Such advice is commonly given in the form of state-action pairs. However, it makes it difficult for the student to reason with and apply to novel states. We introduce Explainable Action Advising, in which the teacher provides action advice as well as associated explanations indicating why the action was chosen. This allows the student to self-reflect on what it has learned, enabling advice generalization and leading to improved sample efficiency and learning performance - even in environments where the teacher is sub-optimal. We empirically show that our framework is effective in both single-agent and multi-agent scenarios, yielding improved policy returns and convergence rates when compared to state-of-the-art methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#19982;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#21457;&#29616;&#27169;&#22411;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#23545;&#40784;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#37117;&#23545;&#23545;&#40784;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#33021;&#26174;&#33879;&#25552;&#39640;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01201</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Human alignment of neural network representations. (arXiv:2211.01201v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#19982;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#21457;&#29616;&#27169;&#22411;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#23545;&#40784;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#37117;&#23545;&#23545;&#40784;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#33021;&#26174;&#33879;&#25552;&#39640;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#20154;&#31867;&#25110;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#21644;&#23398;&#20064;&#31639;&#27861;&#19982;&#23548;&#33268;&#20154;&#31867;&#35270;&#35273;&#30340;&#26041;&#24335;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#20043;&#22788;&#12290;&#26412;&#25991;&#30740;&#31350;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#36890;&#36807;&#34892;&#20026;&#21453;&#24212;&#25512;&#26029;&#20986;&#30340;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#23545;&#40784;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#19982;&#20154;&#31867;&#34892;&#20026;&#21453;&#24212;&#30340;&#23545;&#40784;&#22522;&#26412;&#19978;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#21017;&#20855;&#26377;&#26356;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#22312;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#20219;&#21153;&#25910;&#38598;&#30340;&#19977;&#20010;&#20154;&#31867;&#30456;&#20284;&#24230;&#21028;&#26029;&#25968;&#25454;&#38598;&#20013;&#20445;&#25345;&#19968;&#33268;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20123;&#20154;&#31867;&#27010;&#24565;...
&lt;/p&gt;
&lt;p&gt;
Today's computer vision models achieve human or near-human level performance across a wide variety of vision tasks. However, their architectures, data, and learning algorithms differ in numerous ways from those that give rise to human vision. In this paper, we investigate the factors that affect the alignment between the representations learned by neural networks and human mental representations inferred from behavioral responses. We find that model scale and architecture have essentially no effect on the alignment with human behavioral responses, whereas the training dataset and objective function both have a much larger impact. These findings are consistent across three datasets of human similarity judgments collected using two different tasks. Linear transformations of neural network representations learned from behavioral responses from one dataset substantially improve alignment with human similarity judgments on the other two datasets. In addition, we find that some human concept
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#31243;&#24207;&#21512;&#25104;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#26469;&#35299;&#20915;&#65292;&#26368;&#21518;&#27604;&#36739;&#20102;&#35813;&#26041;&#27861;&#21644;&#20854;&#20182;&#31243;&#24207;&#21512;&#25104;&#25216;&#26415;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.00828</link><description>&lt;p&gt;
&#36830;&#32493;&#20248;&#21270;&#21512;&#25104;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Programs with Continuous Optimization. (arXiv:2211.00828v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#31243;&#24207;&#21512;&#25104;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#26469;&#35299;&#20915;&#65292;&#26368;&#21518;&#27604;&#36739;&#20102;&#35813;&#26041;&#27861;&#21644;&#20854;&#20182;&#31243;&#24207;&#21512;&#25104;&#25216;&#26415;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26576;&#20123;&#35268;&#33539;&#30340;&#33258;&#21160;&#36719;&#20214;&#29983;&#25104;&#34987;&#31216;&#20026;&#31243;&#24207;&#21512;&#25104;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23558;&#31243;&#24207;&#21512;&#25104;&#21046;&#23450;&#20026;&#20855;&#26377;&#31163;&#25955;&#21442;&#25968;&#30340;&#25628;&#32034;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#30340;&#31243;&#24207;&#21512;&#25104;&#20844;&#24335;&#65292;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#36827;&#21270;&#26041;&#27861;&#20043;&#19968;&#65292;&#31216;&#20026;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#26144;&#23556;&#26041;&#26696;&#26469;&#23558;&#36830;&#32493;&#20844;&#24335;&#36716;&#25442;&#20026;&#23454;&#38469;&#31243;&#24207;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31995;&#32479; GENESYS &#19982;&#20960;&#31181;&#26368;&#36817;&#30340;&#31243;&#24207;&#21512;&#25104;&#25216;&#26415;&#65288;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#22495;&#20013;&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034; GENESYS &#22312;&#30456;&#21516;&#26102;&#38388;&#39044;&#31639;&#20869;&#21512;&#25104;&#30340;&#31243;&#24207;&#27604;&#37027;&#20123;&#29616;&#26377;&#30340;&#26041;&#26696;&#26356;&#22810;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#38271;&#24230;&#20026; 10 &#30340;&#31243;&#24207;&#65292;GENESYS &#22312;&#30456;&#21516;&#30340;&#26102;&#38388;&#39044;&#31639;&#20869;&#21512;&#25104;&#30340;&#31243;&#24207;&#27604;&#29616;&#26377;&#26041;&#26696;&#22810; 28&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic software generation based on some specification is known as program synthesis. Most existing approaches formulate program synthesis as a search problem with discrete parameters. In this paper, we present a novel formulation of program synthesis as a continuous optimization problem and use a state-of-the-art evolutionary approach, known as Covariance Matrix Adaptation Evolution Strategy to solve the problem. We then propose a mapping scheme to convert the continuous formulation into actual programs. We compare our system, called GENESYS, with several recent program synthesis techniques (in both discrete and continuous domains) and show that GENESYS synthesizes more programs within a fixed time budget than those existing schemes. For example, for programs of length 10, GENESYS synthesizes 28% more programs than those existing schemes within the same time budget.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;NLP&#31995;&#32479;&#20013;&#31038;&#20132;&#26234;&#33021;&#21644;&#24515;&#29702;&#29702;&#35770;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#20351;&#29992;&#20004;&#20010;&#20219;&#21153;&#35780;&#20272;&#27169;&#22411;&#22312;&#29702;&#35299;&#31038;&#20132;&#20114;&#21160;&#24847;&#22270;&#21644;&#25512;&#26029;&#21442;&#19982;&#32773;&#24515;&#29702;&#29366;&#24577;&#21644;&#29616;&#23454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-3&#32570;&#20047;&#36825;&#31181;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;NLP&#31995;&#32479;&#22312;&#29702;&#35299;&#31038;&#20132;&#21160;&#24577;&#26041;&#38754;&#30340;&#24403;&#21069;&#23616;&#38480;&#24615;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2210.13312</link><description>&lt;p&gt;
&#31070;&#32463;&#24515;&#29702;&#29702;&#35770;&#65311;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20132;&#26234;&#33021;&#23616;&#38480;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. (arXiv:2210.13312v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;NLP&#31995;&#32479;&#20013;&#31038;&#20132;&#26234;&#33021;&#21644;&#24515;&#29702;&#29702;&#35770;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#20351;&#29992;&#20004;&#20010;&#20219;&#21153;&#35780;&#20272;&#27169;&#22411;&#22312;&#29702;&#35299;&#31038;&#20132;&#20114;&#21160;&#24847;&#22270;&#21644;&#25512;&#26029;&#21442;&#19982;&#32773;&#24515;&#29702;&#29366;&#24577;&#21644;&#29616;&#23454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-3&#32570;&#20047;&#36825;&#31181;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;NLP&#31995;&#32479;&#22312;&#29702;&#35299;&#31038;&#20132;&#21160;&#24577;&#26041;&#38754;&#30340;&#24403;&#21069;&#23616;&#38480;&#24615;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#26234;&#33021;&#21644;&#24515;&#29702;&#29702;&#35770;&#65288;ToM&#65289;&#21363;&#29702;&#35299;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#19981;&#21516;&#24515;&#29702;&#29366;&#24577;&#12289;&#24847;&#22270;&#21644;&#21453;&#24212;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;&#20154;&#31867;&#33021;&#22815;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#23545;&#26085;&#24120;&#31038;&#20132;&#20114;&#21160;&#12290;&#38543;&#30528;NLP&#31995;&#32479;&#22312;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#31038;&#20132;&#22330;&#26223;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#29702;&#35299;&#31038;&#20132;&#21160;&#24577;&#30340;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20174;&#23454;&#35777;&#21644;&#29702;&#35770;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#29616;&#20195;NLP&#31995;&#32479;&#20013;&#31038;&#20132;&#26234;&#33021;&#21644;&#24515;&#29702;&#29702;&#35770;&#36825;&#19968;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#39033;&#20219;&#21153;&#65306;SocialIQa&#65288;Sap et al&#12290;&#65292;2019&#65289;&#21644;ToMi&#65288;Le et al&#12290;&#65292;2019&#65289;&#26469;&#34913;&#37327;&#27169;&#22411;&#29702;&#35299;&#31038;&#20132;&#20114;&#21160;&#21442;&#19982;&#32773;&#24847;&#22270;&#21644;&#21453;&#24212;&#20197;&#21450;&#25512;&#26029;&#21442;&#19982;&#32773;&#24515;&#29702;&#29366;&#24577;&#21644;&#29616;&#23454;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20170;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-3&#65307;Brown et al&#12290;&#65292;2020&#65289;&#32570;&#20047;&#36825;&#31181;&#31038;&#20132;&#26234;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;NLP&#31995;&#32479;&#22312;&#29702;&#35299;&#31038;&#20132;&#21160;&#24577;&#26041;&#38754;&#30340;&#24403;&#21069;&#23616;&#38480;&#24615;&#20197;&#21450;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#39046;&#22495;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social intelligence and Theory of Mind (ToM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allow humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial. In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theory-based perspective. We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measures models' ability to understand intents and reactions of participants of social interactions, and ToMi (Le et al., 2019), which measures whether models can infer mental states and realities of participants of situations. Our results show that models struggle substantially at these Theory 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31354;&#20013;&#35745;&#31639;&#65288;OAC&#65289;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#22810;&#36335;&#35775;&#38382;&#20449;&#36947;&#20013;&#30340;&#24178;&#25200;&#36827;&#34892;&#35745;&#31639;&#21487;&#20197;&#25552;&#20379;&#26174;&#30528;&#26356;&#39640;&#30340;&#21487;&#23454;&#29616;&#35745;&#31639;&#36895;&#29575;&#65292;&#30456;&#36739;&#20110;&#20998;&#31163;&#36890;&#20449;&#21644;&#35745;&#31639;&#20219;&#21153;&#12290;&#25991;&#20013;&#20171;&#32461;&#20102;OAC&#26041;&#27861;&#30340;&#20855;&#20307;&#23454;&#29616;&#12289;&#20248;&#32570;&#28857;&#20197;&#21450;&#24212;&#29992;&#30340;&#24120;&#35265;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.11350</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#31354;&#20013;&#35745;&#31639;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Over-the-Air Computation. (arXiv:2210.11350v5 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31354;&#20013;&#35745;&#31639;&#65288;OAC&#65289;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#22810;&#36335;&#35775;&#38382;&#20449;&#36947;&#20013;&#30340;&#24178;&#25200;&#36827;&#34892;&#35745;&#31639;&#21487;&#20197;&#25552;&#20379;&#26174;&#30528;&#26356;&#39640;&#30340;&#21487;&#23454;&#29616;&#35745;&#31639;&#36895;&#29575;&#65292;&#30456;&#36739;&#20110;&#20998;&#31163;&#36890;&#20449;&#21644;&#35745;&#31639;&#20219;&#21153;&#12290;&#25991;&#20013;&#20171;&#32461;&#20102;OAC&#26041;&#27861;&#30340;&#20855;&#20307;&#23454;&#29616;&#12289;&#20248;&#32570;&#28857;&#20197;&#21450;&#24212;&#29992;&#30340;&#24120;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#21644;&#35745;&#31639;&#36890;&#24120;&#34987;&#35270;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#35745;&#31639;&#23548;&#21521;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20027;&#35201;&#20852;&#36259;&#26159;&#35774;&#22791;&#26412;&#22320;&#20449;&#24687;&#30340;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#26412;&#22320;&#20449;&#24687;&#26412;&#36523;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20449;&#24687;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#22810;&#36335;&#35775;&#38382;&#20449;&#36947;&#20013;&#30340;&#24178;&#25200;&#36827;&#34892;&#35745;&#31639;&#65292;&#21363;&#31354;&#20013;&#35745;&#31639;(OAC)&#65292;&#21487;&#20197;&#25552;&#20379;&#26174;&#30528;&#26356;&#39640;&#30340;&#21487;&#23454;&#29616;&#35745;&#31639;&#36895;&#29575;&#65292;&#32780;&#19981;&#26159;&#20998;&#31163;&#36890;&#20449;&#21644;&#35745;&#31639;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#21442;&#19982;&#33410;&#28857;&#30340;&#22686;&#22810;&#65292;&#31354;&#20013;&#35745;&#31639;&#19982;&#20998;&#31163;&#20043;&#38388;&#30340;&#35745;&#31639;&#36895;&#29575;&#24046;&#36317;&#20063;&#20250;&#22686;&#21152;&#12290;&#37492;&#20110;&#36825;&#19968;&#21160;&#26426;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;&#23454;&#29992;OAC&#26041;&#27861;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#22312;&#27010;&#36848;&#19982;OAC&#30456;&#20851;&#30340;&#22522;&#30784;&#30693;&#35782;&#20043;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21487;&#29992;&#30340;OAC&#26041;&#26696;&#21450;&#20854;&#20248;&#32570;&#28857;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29616;OAC&#30340;&#26041;&#27861;&#27010;&#36848;&#65292;&#21253;&#25324;&#30828;&#20214;&#12289;&#36719;&#20214;&#12289;&#27169;&#22411;&#26816;&#27979;&#21644;&#26816;&#27979;&#30340;&#21551;&#29992;&#26426;&#21046;&#20197;&#21450;&#29616;&#26377;&#24179;&#21488;&#30340;&#35843;&#26597;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OAC&#26041;&#27861;&#24212;&#29992;&#30340;&#19968;&#20123;&#26222;&#36941;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication and computation are often viewed as separate tasks. This approach is very effective from the perspective of engineering as isolated optimizations can be performed. However, for many computation-oriented applications, the main interest is a function of the local information at the devices, rather than the local information itself. In such scenarios, information theoretical results show that harnessing the interference in a multiple access channel for computation, i.e., over-the-air computation (OAC), can provide a significantly higher achievable computation rate than separating communication and computation tasks. Moreover, the gap between OAC and separation in terms of computation rate increases with more participating nodes. Given this motivation, in this study, we provide a comprehensive survey on practical OAC methods. After outlining fundamentals related to OAC, we discuss the available OAC schemes with their pros and cons. We provide an overview of the enabling mecha
&lt;/p&gt;</description></item><item><title>BioGPT&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29983;&#25104;&#24335;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22810;&#39033;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#22312;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#23588;&#20026;&#31361;&#20986;&#12290;</title><link>http://arxiv.org/abs/2210.10341</link><description>&lt;p&gt;
BioGPT&#65306;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#29983;&#25104;&#21644;&#25366;&#25496;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining. (arXiv:2210.10341v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10341
&lt;/p&gt;
&lt;p&gt;
BioGPT&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29983;&#25104;&#24335;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22810;&#39033;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#22312;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#23588;&#20026;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20063;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36890;&#29992;&#35821;&#35328;&#39046;&#22495;&#20013;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20004;&#20010;&#20027;&#35201;&#20998;&#25903;&#26159;BERT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#21644;GPT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#12290;&#24050;&#32463;&#26377;&#24456;&#22810;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22914;BioBERT&#21644;PubMedBERT&#65292;&#22312;&#22810;&#31181;&#21028;&#21035;&#24335;&#19979;&#28216;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#32570;&#20047;&#29983;&#25104;&#33021;&#21147;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29983;&#25104;&#24335;Transformer&#35821;&#35328;&#27169;&#22411;BioGPT&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;BC5CDR&#12289;KD-DTI&#21644;DDI&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#20998;&#21035;&#33719;&#24471;&#20102;44.98&#65285;&#65292;38.42&#65285;&#21644;40.76&#65285;&#30340;F1&#24471;&#20998;&#65292;&#20197;&#21450;&#22312;PubMedQA&#20219;&#21153;&#19978;&#30340;78.2&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e., BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks respectively, and 78.2% accuracy on PubMedQA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25705;&#25830;&#22810;&#29289;&#20307;&#25235;&#21462;&#30340;&#39640;&#25928;&#35745;&#21010;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#24037;&#20316;&#65292;&#20854;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;13.7&#65285;&#65292;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;1.6&#20493;&#65292;&#24182;&#19988;&#25235;&#21462;&#35745;&#21010;&#26102;&#38388;&#20943;&#23569;&#20102;6.3&#20493;&#12290;</title><link>http://arxiv.org/abs/2210.07420</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#25928;&#35745;&#21010;&#31283;&#20581;&#30340;&#25705;&#25830;&#22810;&#29289;&#20307;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Learning to Efficiently Plan Robust Frictional Multi-Object Grasps. (arXiv:2210.07420v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25705;&#25830;&#22810;&#29289;&#20307;&#25235;&#21462;&#30340;&#39640;&#25928;&#35745;&#21010;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#24037;&#20316;&#65292;&#20854;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;13.7&#65285;&#65292;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;1.6&#20493;&#65292;&#24182;&#19988;&#25235;&#21462;&#35745;&#21010;&#26102;&#38388;&#20943;&#23569;&#20102;6.3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26434;&#20081;&#38382;&#39064;&#65292;&#22810;&#20010;&#21018;&#24615;&#20984;&#22810;&#36793;&#24418;&#29289;&#20307;&#38543;&#26426;&#25918;&#32622;&#22312;&#19968;&#20010;&#24179;&#38754;&#34920;&#38754;&#19978;&#65292;&#24517;&#39035;&#20351;&#29992;&#21333;&#20010;&#21644;&#22810;&#20010;&#29289;&#20307;&#30340;&#25235;&#21462;&#26041;&#24335;&#65292;&#23558;&#23427;&#20204;&#26377;&#25928;&#22320;&#36816;&#36755;&#21040;&#35013;&#31665;&#20013;&#12290;&#25105;&#20204;&#24341;&#20837;&#25705;&#25830;&#26469;&#22686;&#21152;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#65292;&#24182;&#20351;&#29992;&#23454;&#20363;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#20197;&#35745;&#21010;&#31283;&#20581;&#30340;&#22810;&#29289;&#20307;&#25235;&#21462;&#12290;&#22312;&#29289;&#29702;&#23454;&#39564;&#20013;&#65292;&#30456;&#27604;&#20110;&#22810;&#29289;&#20307;&#25235;&#21462;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#25105;&#20204;&#21457;&#29616;&#25104;&#21151;&#29575;&#22686;&#21152;&#20102;13.7&#65285;&#65292;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;1.6&#20493;&#65292;&#25235;&#21462;&#35745;&#21010;&#26102;&#38388;&#20943;&#23569;&#20102;6.3&#20493;&#12290;&#19982;&#21333;&#20010;&#29289;&#20307;&#25235;&#21462;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#27599;&#23567;&#26102;&#30340;&#25342;&#21462;&#27425;&#25968;&#22686;&#21152;&#20102;3.1&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a decluttering problem where multiple rigid convex polygonal objects rest in randomly placed positions and orientations on a planar surface and must be efficiently transported to a packing box using both single and multi-object grasps. Prior work considered frictionless multi-object grasping. In this paper, we introduce friction to increase picks per hour. We train a neural network using real examples to plan robust multi-object grasps. In physical experiments, we find a 13.7% increase in success rate, a 1.6x increase in picks per hour, and a 6.3x decrease in grasp planning time compared to prior work on multi-object grasping. Compared to single object grasping, we find a 3.1x increase in picks per hour.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#30340;&#28789;&#27963;&#24615;&#35774;&#35745;&#20102;&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#26631;&#35760;&#25351;&#23548;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2210.06462</link><description>&lt;p&gt;
&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Guided Diffusion Models. (arXiv:2210.06462v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#30340;&#28789;&#27963;&#24615;&#35774;&#35745;&#20102;&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#26631;&#35760;&#25351;&#23548;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#25351;&#23548;&#26469;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#26102;&#12290;&#28982;&#32780;&#65292;&#25351;&#23548;&#38656;&#35201;&#22823;&#37327;&#30340;&#22270;&#20687;-&#27880;&#37322;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#27492;&#20381;&#36182;&#20110;&#20854;&#21487;&#29992;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#26080;&#20559;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#27880;&#37322;&#38656;&#27714;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#20449;&#21495;&#30340;&#28789;&#27963;&#24615;&#35774;&#35745;&#20102;&#33258;&#25105;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#25552;&#21462;&#20989;&#25968;&#21644;&#33258;&#25105;&#27880;&#37322;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22270;&#20687;&#31890;&#24230;&#19978;&#25552;&#20379;&#25351;&#23548;&#20449;&#21495;&#65306;&#20174;&#25972;&#20307;&#22270;&#20687;&#21040;&#29289;&#20307;&#26694;&#65292;&#29978;&#33267;&#21040;&#20998;&#21106;&#33945;&#29256;&#12290;&#25105;&#20204;&#22312;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#26631;&#35760;&#25351;&#23548;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated remarkable progress in image generation quality, especially when guidance is used to control the generative process. However, guidance requires a large amount of image-annotation pairs for training and is thus dependent on their availability, correctness and unbiasedness. In this paper, we eliminate the need for such annotation by instead leveraging the flexibility of self-supervision signals to design a framework for self-guided diffusion models. By leveraging a feature extraction function and a self-annotation function, our method provides guidance signals at various image granularities: from the level of holistic images to object boxes and even segmentation masks. Our experiments on single-label and multi-label image datasets demonstrate that self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels, especially on unbalanced data. When equipped with self-supervised box or m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#21407;&#22320;&#27169;&#22411;&#19979;&#36733;&#30340;&#26032;&#25216;&#26415;&#65292;&#22312;6G&#31227;&#21160;&#32593;&#32476;&#20013;&#23454;&#29616;&#22810;&#21151;&#33021;&#36793;&#32536;AI&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#20174;&#32593;&#32476;&#30340;AI&#24211;&#20013;&#19979;&#36733;&#36879;&#26126;&#21644;&#23454;&#26102;&#22320;&#26367;&#25442;&#35774;&#22791;&#19978;&#30340;AI&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#20214;&#26159;&#19968;&#22823;&#32452;&#21160;&#24577;&#25216;&#26415;&#65292;&#21487;&#20197;&#26681;&#25454;&#35774;&#22791;&#23384;&#20648;&#35745;&#31639;&#33021;&#21147;&#12289;&#20449;&#36947;&#29366;&#24577;&#21644;&#24212;&#29992;&#31243;&#24207;&#35201;&#27714;&#31561;&#65292;&#33258;&#36866;&#24212;&#22320;&#21387;&#32553;&#27169;&#22411;&#23454;&#29616;&#27169;&#22411;&#30340;&#19979;&#36733;&#12290;</title><link>http://arxiv.org/abs/2210.03555</link><description>&lt;p&gt;
&#22312;6G&#31227;&#21160;&#32593;&#32476;&#20013;&#23454;&#29616;&#22810;&#21151;&#33021;&#36793;&#32536;AI&#30340;&#21407;&#22320;&#27169;&#22411;&#19979;&#36733;
&lt;/p&gt;
&lt;p&gt;
In-situ Model Downloading to Realize Versatile Edge AI in 6G Mobile Networks. (arXiv:2210.03555v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#21407;&#22320;&#27169;&#22411;&#19979;&#36733;&#30340;&#26032;&#25216;&#26415;&#65292;&#22312;6G&#31227;&#21160;&#32593;&#32476;&#20013;&#23454;&#29616;&#22810;&#21151;&#33021;&#36793;&#32536;AI&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#20174;&#32593;&#32476;&#30340;AI&#24211;&#20013;&#19979;&#36733;&#36879;&#26126;&#21644;&#23454;&#26102;&#22320;&#26367;&#25442;&#35774;&#22791;&#19978;&#30340;AI&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#20214;&#26159;&#19968;&#22823;&#32452;&#21160;&#24577;&#25216;&#26415;&#65292;&#21487;&#20197;&#26681;&#25454;&#35774;&#22791;&#23384;&#20648;&#35745;&#31639;&#33021;&#21147;&#12289;&#20449;&#36947;&#29366;&#24577;&#21644;&#24212;&#29992;&#31243;&#24207;&#35201;&#27714;&#31561;&#65292;&#33258;&#36866;&#24212;&#22320;&#21387;&#32553;&#27169;&#22411;&#23454;&#29616;&#27169;&#22411;&#30340;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20845;&#20195;&#65288;6G&#65289;&#31227;&#21160;&#32593;&#32476;&#39044;&#35745;&#23558;&#22312;&#32593;&#32476;&#36793;&#32536;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#26222;&#21450;&#37096;&#32626;&#12290;&#38543;&#30528;&#36793;&#32536;AI&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#29616;&#22312;&#26159;&#23454;&#29616;&#22312;&#36793;&#32536;&#35774;&#22791;&#65288;&#20363;&#22914;&#26234;&#33021;&#25163;&#26426;&#21644;&#20256;&#24863;&#22120;&#65289;&#19978;&#19979;&#36733;&#26234;&#33021;&#27169;&#22411;&#30340;&#26102;&#20505;&#20102;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#29256;&#26412;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#31216;&#20026;&#21407;&#22320;&#27169;&#22411;&#19979;&#36733;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#30340;AI&#24211;&#19979;&#36733;&#26469;&#23454;&#29616;&#36879;&#26126;&#21644;&#23454;&#26102;&#26367;&#25442;&#35774;&#22791;&#19978;&#30340;AI&#27169;&#22411;&#12290;&#20854;&#29420;&#29305;&#30340;&#29305;&#28857;&#26159;&#36866;&#24212;&#19979;&#36733;&#21040;&#26102;&#21464;&#24773;&#20917;&#65288;&#20363;&#22914;&#24212;&#29992;&#31243;&#24207;&#12289;&#20301;&#32622;&#21644;&#26102;&#38388;&#65289;&#12289;&#35774;&#22791;&#24322;&#26500;&#23384;&#20648;&#21644;&#35745;&#31639;&#33021;&#21147;&#20197;&#21450;&#20449;&#36947;&#29366;&#24577;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#26159;&#19968;&#32452;&#25216;&#26415;&#65292;&#21160;&#24577;&#22320;&#22312;&#28145;&#24230;&#32423;&#21035;&#12289;&#21442;&#25968;&#32423;&#21035;&#25110;&#20301;&#32423;&#21035;&#19978;&#21387;&#32553;&#19979;&#36733;&#30340;&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#33258;&#36866;&#24212;&#27169;&#22411;&#19979;&#36733;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#21270;6G&#32593;&#32476;&#26550;&#26500;&#65292;&#23450;&#21046;&#21270;&#20026;&#36793;&#32536;AI&#65292;&#24182;&#25972;&#21512;&#20102;&#25152;&#25552;&#20986;&#30340;&#21407;&#22320;&#27169;&#22411;&#19979;&#36733;&#25216;&#26415;&#12290;&#35813;&#26550;&#26500;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#36793;&#32536;AI&#65292;&#36890;&#36807;&#20419;&#36827;&#26377;&#25928;&#22320;&#20998;&#37197;AI&#27169;&#22411;&#20174;&#20013;&#22830;&#26234;&#33021;&#20013;&#24515;&#21040;&#36793;&#32536;&#35774;&#22791;&#65292;&#24182;&#20445;&#35777;&#26681;&#25454;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#35201;&#27714;&#28789;&#27963;&#22320;&#37096;&#32626;AI&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#25552;&#20986;&#30340;&#26550;&#26500;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sixth-generation (6G) mobile networks are expected to feature the ubiquitous deployment of machine learning and AI algorithms at the network edge. With rapid advancements in edge AI, the time has come to realize intelligence downloading onto edge devices (e.g., smartphones and sensors). To materialize this version, we propose a novel technology in this article, called in-situ model downloading, that aims to achieve transparent and real-time replacement of on-device AI models by downloading from an AI library in the network. Its distinctive feature is the adaptation of downloading to time-varying situations (e.g., application, location, and time), devices' heterogeneous storage-and-computing capacities, and channel states. A key component of the presented framework is a set of techniques that dynamically compress a downloaded model at the depth-level, parameter-level, or bit-level to support adaptive model downloading. We further propose a virtualized 6G network architecture customi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36719;&#27169;&#26495;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#22522;&#31867;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;LASP, &#21516;&#26102;&#36890;&#36807;&#22686;&#21152;&#25552;&#31034;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#26657;&#20934;&#35270;&#35273;-&#35821;&#35328;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292; &#22312;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.01115</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;LASP&#65306;&#38754;&#21521;&#35821;&#35328;&#24863;&#30693;&#30340;&#25991;&#26412;&#20248;&#21270;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision &amp; Language Models. (arXiv:2210.01115v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36719;&#27169;&#26495;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#22522;&#31867;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#25991;&#26412;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;LASP, &#21516;&#26102;&#36890;&#36807;&#22686;&#21152;&#25552;&#31034;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#26657;&#20934;&#35270;&#35273;-&#35821;&#35328;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292; &#22312;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#27169;&#26495;&#23398;&#20064;&#26368;&#36817;&#24050;&#25104;&#20026;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;V&amp;L&#27169;&#22411;&#30340;&#36873;&#25321;&#26041;&#27861;&#20043;&#19968;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#32463;&#36807;&#35757;&#32451;&#25968;&#25454;&#30340;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#36739;&#22823;&#32570;&#38519;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#35821;&#35328;&#24863;&#30693;&#30340;&#25991;&#26412;&#25552;&#31034;&#65288;LASP&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#22522;&#31867;&#30340;&#36807;&#25311;&#21512;&#65292;&#22686;&#21152;&#25552;&#31034;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#26657;&#20934;&#35270;&#35273;-&#35821;&#35328;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#22312;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft prompt learning has recently emerged as one of the methods of choice for adapting V&amp;L models to a downstream task using a few training examples. However, current methods significantly overfit the training data, suffering from large accuracy degradation when tested on unseen classes from the same domain. To this end, in this paper, we make the following 4 contributions: (1) To alleviate base class overfitting, we propose a novel Language-Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts. (2) To increase the representation capacity of the prompts, we propose grouped LASP where each group of prompts is optimized with respect to a separate subset of textual prompts. (3) We identify a visual-language misalignment introduced by prompt learning and LASP, and more importantly, propose a re-calibration mechanism to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21387;&#32553;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#26469;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.06116</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#26469;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;
&lt;/p&gt;
&lt;p&gt;
Patching Weak Convolutional Neural Network Models through Modularization and Composition. (arXiv:2209.06116v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21387;&#32553;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#26469;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#24635;&#26159;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#20462;&#34917;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#26469;&#25913;&#36827;&#23427;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#27169;&#22359;&#21270;&#26041;&#27861;CNNSplitter&#65292;&#23427;&#23558;&#20855;&#26377;$N$&#31867;&#20998;&#31867;&#20219;&#21153;&#30340;&#24378;CNN&#27169;&#22411;&#20998;&#35299;&#20026;$N$&#20010;&#36739;&#23567;&#30340;CNN&#27169;&#22359;&#12290;&#27599;&#20010;&#27169;&#22359;&#26159;&#19968;&#20010;&#23376;&#27169;&#22411;&#65292;&#21253;&#21547;&#24378;&#27169;&#22411;&#30340;&#37096;&#20998;&#21367;&#31215;&#26680;&#12290;&#20026;&#20102;&#20462;&#34917;&#22312;&#30446;&#26631;&#31867;&#21035;&#65288;TC&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#24369;CNN&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20174;&#24378;CNN&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#30456;&#24212;&#27169;&#22359;&#30456;&#32467;&#21512;&#12290;&#36825;&#26679;&#65292;&#24369;CNN&#27169;&#22411;&#35782;&#21035;TC&#30340;&#33021;&#21147;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite great success in many applications, deep neural networks are not always robust in practice. For instance, a convolutional neuron network (CNN) model for classification tasks often performs unsatisfactorily in classifying some particular classes of objects. In this work, we are concerned with patching the weak part of a CNN model instead of improving it through the costly retraining of the entire model. Inspired by the fundamental concepts of modularization and composition in software engineering, we propose a compressed modularization approach, CNNSplitter, which decomposes a strong CNN model for $N$-class classification into $N$ smaller CNN modules. Each module is a sub-model containing a part of the convolution kernels of the strong model. To patch a weak CNN model that performs unsatisfactorily on a target class (TC), we compose the weak CNN model with the corresponding module obtained from a strong CNN model. The ability of the weak CNN model to recognize the TC can thus be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#20013;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#24341;&#20837;&#22823;&#37327;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2209.05732</link><description>&lt;p&gt;
R\'{e}nyi&#25955;&#24230;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
R\'{e}nyi Divergence Deep Mutual Learning. (arXiv:2209.05732v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#20013;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#24341;&#20837;&#22823;&#37327;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#23457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#8212;&#8212;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#65288;DML&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#32780;&#19981;&#26159;KL&#25955;&#24230;&#65292;&#36825;&#31181;&#20570;&#27861;&#26356;&#21152;&#28789;&#27963;&#12289;&#21487;&#35843;&#65292;&#20197;&#25913;&#21892;vanilla DML&#12290;&#36825;&#31181;&#20462;&#25913;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#38468;&#21152;&#22797;&#26434;&#24615;&#19979;&#19981;&#26029;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#33539;&#20363;&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#19988;&#34920;&#26126;&#20855;&#26377;&#24658;&#23450;&#23398;&#20064;&#29575;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#38750;&#20984;&#20248;&#21270;&#20219;&#21153;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#25910;&#25947;&#30340;&#20559;&#24046;&#20026;$\mathcal{O}(1)$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits Deep Mutual Learning (DML), a simple yet effective computing paradigm. We propose using R\'{e}nyi divergence instead of the KL divergence, which is more flexible and tunable, to improve vanilla DML. This modification is able to consistently improve performance over vanilla DML with limited additional complexity. The convergence properties of the proposed paradigm are analyzed theoretically, and Stochastic Gradient Descent with a constant learning rate is shown to converge with $\mathcal{O}(1)$-bias in the worst case scenario for nonconvex optimization tasks. That is, learning will reach nearby local optima but continue searching within a bounded scope, which may help mitigate overfitting. Finally, our extensive empirical results demonstrate the advantage of combining DML and R\'{e}nyi divergence, which further improves generalized models.
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#26159;&#35270;&#35273;&#20013;&#30340;&#26032;&#20852;&#20027;&#39064;&#65292;&#20854;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#21463;&#21040;&#24191;&#27867;&#27427;&#36175;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#19977;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#20197;&#21450;&#21508;&#31181;&#31639;&#27861;&#21644;&#26550;&#26500;&#26041;&#38754;&#30340;&#35752;&#35770;&#65292;&#24182;&#24635;&#32467;&#27604;&#36739;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.04747</link><description>&lt;p&gt;
&#35270;&#35273;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models in Vision: A Survey. (arXiv:2209.04747v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04747
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#35270;&#35273;&#20013;&#30340;&#26032;&#20852;&#20027;&#39064;&#65292;&#20854;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#21463;&#21040;&#24191;&#27867;&#27427;&#36175;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#19977;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#20197;&#21450;&#21508;&#31181;&#31639;&#27861;&#21644;&#26550;&#26500;&#26041;&#38754;&#30340;&#35752;&#35770;&#65292;&#24182;&#24635;&#32467;&#27604;&#36739;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#26032;&#20852;&#20027;&#39064;&#65292;&#23637;&#29616;&#20102;&#22312;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#38750;&#20961;&#30340;&#32467;&#26524;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65292;&#21069;&#21521;&#25193;&#25955;&#21644;&#21453;&#21521;&#25193;&#25955;&#12290;&#22312;&#21069;&#21521;&#25193;&#25955;&#38454;&#27573;&#65292;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#36880;&#28176;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#12290;&#22312;&#21453;&#21521;&#38454;&#27573;&#65292;&#27169;&#22411;&#34987;&#20219;&#21153;&#20026;&#36890;&#36807;&#36880;&#27493;&#23398;&#20064;&#36870;&#36716;&#25193;&#25955;&#36807;&#31243;&#65292;&#36880;&#27493;&#24674;&#22797;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#36739;&#22823;&#65292;&#21363;&#30001;&#20110;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#28041;&#21450;&#30340;&#27493;&#39588;&#25968;&#37327;&#36739;&#22810;&#23548;&#33268;&#30340;&#36895;&#24230;&#36739;&#24930;&#65292;&#20294;&#20854;&#25152;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20173;&#28982;&#21463;&#21040;&#24191;&#27867;&#27427;&#36175;&#12290;&#22312;&#26412;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#35270;&#35273;&#20013;&#24212;&#29992;&#30340;&#32508;&#21512;&#24615;&#35780;&#35770;&#65292;&#21253;&#25324;&#35813;&#39046;&#22495;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#20171;&#32461;&#20102;&#19977;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#65306;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#25193;&#25955;&#27169;&#22411;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#21508;&#31181;&#31639;&#27861;&#21644;&#26550;&#26500;&#26041;&#38754;&#65292;&#22914;&#20351;&#29992; L&#233;vy &#36807;&#31243;&#12289;&#19981;&#21516;&#24418;&#24335;&#30340;&#22122;&#22768;&#12289;&#27169;&#22411;&#26465;&#20214;&#21644;&#27491;&#21017;&#21270;&#12289;&#22810;&#23610;&#24230;&#26550;&#26500;&#21644;&#24182;&#34892;&#21270;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#24182;&#27604;&#36739;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modelin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#35270;&#22270;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#20854;&#20013;DH-KG&#21253;&#21547;&#36229;&#20851;&#31995;&#23454;&#20363;&#35270;&#22270;&#21644;&#20174;&#23454;&#20307;&#23618;&#27425;&#25277;&#35937;&#20986;&#30340;&#36229;&#20851;&#31995;&#26412;&#20307;&#35270;&#22270;&#65292;&#23450;&#20041;&#20102;&#38142;&#25509;&#39044;&#27979;&#21644;&#23454;&#20307;&#31867;&#22411;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;DH-KG&#25968;&#25454;&#38598;JW44K-6K&#21644;HTDM&#12290;DHGE&#26159;&#19968;&#20010;&#22522;&#20110;GRAN&#32534;&#30721;&#22120;&#12289;HGNN&#21644;&#32852;&#21512;&#23398;&#20064;&#30340;DH-KG&#23884;&#20837;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2207.08562</link><description>&lt;p&gt;
DHGE&#65306;&#21452;&#35270;&#22270;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#23884;&#20837;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#21644;&#23454;&#20307;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
DHGE: Dual-View Hyper-Relational Knowledge Graph Embedding for Link Prediction and Entity Typing. (arXiv:2207.08562v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#35270;&#22270;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#20854;&#20013;DH-KG&#21253;&#21547;&#36229;&#20851;&#31995;&#23454;&#20363;&#35270;&#22270;&#21644;&#20174;&#23454;&#20307;&#23618;&#27425;&#25277;&#35937;&#20986;&#30340;&#36229;&#20851;&#31995;&#26412;&#20307;&#35270;&#22270;&#65292;&#23450;&#20041;&#20102;&#38142;&#25509;&#39044;&#27979;&#21644;&#23454;&#20307;&#31867;&#22411;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;DH-KG&#25968;&#25454;&#38598;JW44K-6K&#21644;HTDM&#12290;DHGE&#26159;&#19968;&#20010;&#22522;&#20110;GRAN&#32534;&#30721;&#22120;&#12289;HGNN&#21644;&#32852;&#21512;&#23398;&#20064;&#30340;DH-KG&#23884;&#20837;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#36229;&#20851;&#31995;&#20107;&#23454;&#30001;&#19968;&#20010;&#20027;&#19977;&#20803;&#32452;&#21644;&#20960;&#20010;&#36741;&#21161;&#30340;&#23646;&#24615;-&#20540;&#25551;&#36848;&#32452;&#25104;&#65292;&#34987;&#35748;&#20026;&#27604;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#20107;&#23454;&#26356;&#20840;&#38754;&#21644;&#20855;&#20307;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21333;&#35270;&#22270;&#30340;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#24369;&#21270;&#20102;&#34920;&#31034;&#23454;&#20307;&#20043;&#38388;&#20146;&#23646;&#20851;&#31995;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#20851;&#31995;&#23454;&#20363;&#35270;&#22270;&#21644;&#20174;&#23454;&#20307;&#23618;&#27425;&#25277;&#35937;&#20986;&#30340;&#36229;&#20851;&#31995;&#26412;&#20307;&#35270;&#22270;&#30340;&#21452;&#35270;&#22270;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#65288;DH-KG&#65289;&#12290;&#26412;&#25991;&#39318;&#27425;&#22312;DH-KG&#19978;&#23450;&#20041;&#20102;&#38142;&#25509;&#39044;&#27979;&#21644;&#23454;&#20307;&#31867;&#22411;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#20010;DH-KG&#25968;&#25454;&#38598;&#65292;JW44K-6K&#65292;&#20174;&#32500;&#22522;&#25968;&#25454;&#20013;&#25552;&#21462;&#65292;&#21644;&#22522;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;HTDM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DHGE&#65292;&#19968;&#20010;&#22522;&#20110;GRAN&#32534;&#30721;&#22120;&#12289;HGNN&#21644;&#32852;&#21512;&#23398;&#20064;&#30340;DH-KG&#23884;&#20837;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of representation learning on knowledge graphs (KGs), a hyper-relational fact consists of a main triple and several auxiliary attribute-value descriptions, which is considered more comprehensive and specific than a triple-based fact. However, currently available hyper-relational KG embedding methods in a single view are limited in application because they weaken the hierarchical structure that represents the affiliation between entities. To overcome this limitation, we propose a dual-view hyper-relational KG structure (DH-KG) that contains a hyper-relational instance view for entities and a hyper-relational ontology view for concepts that are abstracted hierarchically from the entities. This paper defines link prediction and entity typing tasks on DH-KG for the first time and constructs two DH-KG datasets, JW44K-6K, extracted from Wikidata, and HTDM based on medical data. Furthermore, we propose DHGE, a DH-KG embedding model based on GRAN encoders, HGNNs, and joint learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37327;&#21270;&#20998;&#26512;&#20102;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#30340;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;&#65292;&#24182;&#20174;&#19977;&#20010;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36827;&#32780;&#32771;&#34385;&#20102;3&#31181;&#25919;&#31574;&#20462;&#25913;&#21450;&#20854;&#24433;&#21709;&#65292;&#26088;&#22312;&#20026;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20559;&#22909;&#25490;&#24207;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2207.07392</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#22768;&#26126;&#24615;&#27169;&#22411;&#8212;&#8212;&#36879;&#26126;&#24230;&#30340;&#24314;&#27169;&#21644;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A simple declarative model of the Federal Disaster Assistance Policy -- modelling and measuring transparency. (arXiv:2207.07392v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37327;&#21270;&#20998;&#26512;&#20102;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#30340;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;&#65292;&#24182;&#20174;&#19977;&#20010;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36827;&#32780;&#32771;&#34385;&#20102;3&#31181;&#25919;&#31574;&#20462;&#25913;&#21450;&#20854;&#24433;&#21709;&#65292;&#26088;&#22312;&#20026;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20559;&#22909;&#25490;&#24207;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#19977;&#20010;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35282;&#24230;&#65292;&#23545;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#30340;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;&#36827;&#34892;&#20102;&#25968;&#37327;&#20998;&#26512;&#12290;&#36825;&#31181;&#25968;&#37327;&#20998;&#26512;&#26041;&#27861;&#26159;&#26032;&#30340;&#65292;&#22312;&#20854;&#20182;&#39046;&#22495;&#22914;&#19994;&#21153;&#21644;&#21307;&#30103;&#27969;&#31243;&#20013;&#20063;&#26377;&#24212;&#29992;&#12290;&#21033;&#30410;&#30456;&#20851;&#32773;&#23545;&#36807;&#31243;&#36879;&#26126;&#24230;&#24456;&#24863;&#20852;&#36259;&#65292;&#20294;&#27599;&#20010;&#20154;&#23545;&#36879;&#26126;&#24230;&#30340;&#20855;&#20307;&#23450;&#20041;&#37117;&#26377;&#19981;&#21516;&#30340;&#30475;&#27861;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#19977;&#31181;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#30340;&#20462;&#25913;&#65292;&#24182;&#20998;&#26512;&#20102;&#20174;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35282;&#24230;&#30475;&#65292;&#27599;&#31181;&#25919;&#31574;&#19979;&#21033;&#30410;&#30456;&#20851;&#32773;&#28385;&#24847;&#24230;&#30340;&#21464;&#21270;&#24773;&#20917;&#12290;&#36825;&#31181;&#20998;&#26512;&#34987;&#29992;&#26469;&#23545;&#22235;&#31181;&#25919;&#31574;&#30340;&#20559;&#22909;&#36827;&#34892;&#25490;&#24207;&#65292;&#20197;&#20415;&#32771;&#34385;&#21040;&#25152;&#26377;&#38598;&#20307;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we will provide a quantitative analysis of a simple model of the Federal Disaster Assistance policy from the viewpoint of three different stakeholders. This quantitative methodology is new and has applications to other areas such as business and healthcare processes. The stakeholders are interested in process transparency but each has a different opinion on precisely what constitutes transparency. We will also consider three modifications to the Federal Disaster Assistance policy and analyse, from a stakeholder viewpoint, how stakeholder satisfaction changes from process to process. This analysis is used to rank the favourability of four policies with respect to all collective stakeholder preferences.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#20307;&#32423;&#21035;&#30693;&#35782;&#20316;&#20026;&#21151;&#33021;&#23545;&#35937;&#23548;&#21521;&#32593;&#32476;&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#21644;&#25191;&#34892;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#22312;CoppeliaSim&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#36827;&#34892;&#38271;&#26399;&#20219;&#21153;&#35268;&#21010;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.05800</link><description>&lt;p&gt;
&#38271;&#31243;&#35268;&#21010;&#19982;&#25191;&#34892;&#30340;&#21151;&#33021;&#23545;&#35937;&#23548;&#21521;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Long-Horizon Planning and Execution with Functional Object-Oriented Networks. (arXiv:2207.05800v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#20307;&#32423;&#21035;&#30693;&#35782;&#20316;&#20026;&#21151;&#33021;&#23545;&#35937;&#23548;&#21521;&#32593;&#32476;&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#21644;&#25191;&#34892;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#22312;CoppeliaSim&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#36827;&#34892;&#38271;&#26399;&#20219;&#21153;&#35268;&#21010;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#21512;&#23545;&#35937;-&#21160;&#20316;&#34920;&#31034;&#26041;&#38754;&#30340;&#24037;&#20316;&#20043;&#21518;&#65292;&#23558;&#21151;&#33021;&#23545;&#35937;&#23548;&#21521;&#32593;&#32476;&#65288;FOON&#65289;&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#30693;&#35782;&#22270;&#34920;&#31034;&#24341;&#20837;&#12290;FOON&#21253;&#21547;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#29702;&#35299;&#21644;&#29289;&#20307;&#32423;&#35268;&#21010;&#30340;&#29615;&#22659;&#26377;&#29992;&#30340;&#31526;&#21495;&#27010;&#24565;&#12290;&#22312;&#26412;&#30740;&#31350;&#20043;&#21069;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#34920;&#26126;&#22914;&#20309;&#20174;FOON&#20013;&#33719;&#21462;&#30340;&#35745;&#21010;&#21487;&#20197;&#30001;&#26426;&#22120;&#20154;&#25191;&#34892;&#65292;&#22240;&#20026;FOON&#20013;&#30340;&#27010;&#24565;&#23545;&#20110;&#25191;&#34892;&#26469;&#35828;&#22826;&#25277;&#35937;&#20102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21033;&#29992;&#29289;&#20307;&#32423;&#21035;&#30693;&#35782;&#20316;&#20026;FOON&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#21644;&#25191;&#34892;&#30340;&#24819;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#21160;&#23558;FOON&#36716;&#25442;&#20026;PDDL&#65292;&#24182;&#21033;&#29992;&#29616;&#25104;&#30340;&#35268;&#21010;&#22120;&#12289;&#34892;&#20026;&#19978;&#19979;&#25991;&#21644;&#26426;&#22120;&#20154;&#25216;&#33021;&#22312;&#20998;&#23618;&#35268;&#21010;&#31649;&#36947;&#20013;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#20219;&#21153;&#35745;&#21010;&#12290;&#25105;&#20204;&#22312;CoppeliaSim&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25972;&#20010;&#26041;&#27861;&#30340;&#38271;&#26399;&#20219;&#21153;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#23398;&#20064;&#21040;&#30340;&#34892;&#20026;&#19978;&#19979;&#25991;&#25193;&#23637;&#21040;&#21069;&#25152;&#26410;&#35265;&#30340;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following work on joint object-action representations, functional object-oriented networks (FOON) were introduced as a knowledge graph representation for robots. A FOON contains symbolic concepts useful to a robot's understanding of tasks and its environment for object-level planning. Prior to this work, little has been done to show how plans acquired from FOON can be executed by a robot, as the concepts in a FOON are too abstract for execution. We thereby introduce the idea of exploiting object-level knowledge as a FOON for task planning and execution. Our approach automatically transforms FOON into PDDL and leverages off-the-shelf planners, action contexts, and robot skills in a hierarchical planning pipeline to generate executable task plans. We demonstrate our entire approach on long-horizon tasks in CoppeliaSim and show how learned action contexts can be extended to never-before-seen scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21547;&#22122;&#25216;&#33021;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#35757;&#32451;&#20986;&#26377;&#25928;&#30340;&#32844;&#31216;&#30456;&#20284;&#24615;&#27169;&#22411;&#65292;&#23545;&#20110;&#25991;&#26412;&#25490;&#24207;&#21644;&#32844;&#20301;&#26631;&#20934;&#21270;&#31561;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2207.00494</link><description>&lt;p&gt;
&#20174;&#21547;&#22122;&#25216;&#33021;&#26631;&#31614;&#20013;&#23398;&#20064;&#32844;&#31216;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
Learning Job Titles Similarity from Noisy Skill Labels. (arXiv:2207.00494v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21547;&#22122;&#25216;&#33021;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#35757;&#32451;&#20986;&#26377;&#25928;&#30340;&#32844;&#31216;&#30456;&#20284;&#24615;&#27169;&#22411;&#65292;&#23545;&#20110;&#25991;&#26412;&#25490;&#24207;&#21644;&#32844;&#20301;&#26631;&#20934;&#21270;&#31561;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#32844;&#31216;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#26159;&#33258;&#21160;&#32844;&#20301;&#25512;&#33616;&#30340;&#37325;&#35201;&#21151;&#33021;&#12290;&#36825;&#20010;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#26469;&#23454;&#29616;&#65292;&#38656;&#35201;&#20197;&#31561;&#25928;&#32844;&#31216;&#23545;&#30340;&#24418;&#24335;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#21547;&#22122;&#25216;&#33021;&#26631;&#31614;&#35757;&#32451;&#32844;&#31216;&#30456;&#20284;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#25991;&#26412;&#25490;&#24207;&#21644;&#32844;&#20301;&#26631;&#20934;&#21270;&#31561;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring semantic similarity between job titles is an essential functionality for automatic job recommendations. This task is usually approached using supervised learning techniques, which requires training data in the form of equivalent job title pairs. In this paper, we instead propose an unsupervised representation learning method for training a job title similarity model using noisy skill labels. We show that it is highly effective for tasks such as text ranking and job normalization.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;SNNs&#20013;&#30340;&#31361;&#35302;&#26435;&#37325;&#21644;&#33033;&#20914;&#38408;&#20540;&#65292;&#21033;&#29992;&#20102;&#31070;&#32463;&#20803;&#22266;&#26377;&#30340;&#38750;&#31361;&#35302;&#26426;&#21046;&#65292;&#20351;&#24471;SNNs&#22312;&#38745;&#24577;&#21644;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26174;&#33879;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.06129</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#20803;&#38408;&#20540;&#30340;&#31361;&#35302;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Synapse-Threshold Synergistic Learning Approach for Spiking Neural Networks. (arXiv:2206.06129v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06129
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;SNNs&#20013;&#30340;&#31361;&#35302;&#26435;&#37325;&#21644;&#33033;&#20914;&#38408;&#20540;&#65292;&#21033;&#29992;&#20102;&#31070;&#32463;&#20803;&#22266;&#26377;&#30340;&#38750;&#31361;&#35302;&#26426;&#21046;&#65292;&#20351;&#24471;SNNs&#22312;&#38745;&#24577;&#21644;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26174;&#33879;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#22312;&#21508;&#31181;&#26234;&#33021;&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;SNNs&#35757;&#32451;&#26041;&#27861;&#26159;&#22522;&#20110;&#31361;&#35302;&#21487;&#22609;&#24615;&#30340;&#27010;&#24565;;&#28982;&#32780;&#65292;&#29616;&#23454;&#30340;&#22823;&#33041;&#23398;&#20064;&#20063;&#21033;&#29992;&#20102;&#31070;&#32463;&#20803;&#22266;&#26377;&#30340;&#38750;&#31361;&#35302;&#26426;&#21046;&#12290;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#33033;&#20914;&#38408;&#20540;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#22266;&#26377;&#31070;&#32463;&#20803;&#29305;&#24449;&#65292;&#23427;&#22312;&#27627;&#31186;&#26102;&#38388;&#23610;&#24230;&#19978;&#34920;&#29616;&#20986;&#20016;&#23500;&#30340;&#21160;&#24577;&#24615;&#65292;&#24182;&#34987;&#25552;&#20986;&#20316;&#20026;&#20419;&#36827;&#31070;&#32463;&#20449;&#24687;&#22788;&#29702;&#30340;&#28508;&#22312;&#26426;&#21046;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#35757;&#32451;SNNs&#20013;&#30340;&#31361;&#35302;&#26435;&#37325;&#21644;&#33033;&#20914;&#38408;&#20540;&#12290;&#20351;&#29992;&#31361;&#35302;-&#38408;&#20540;&#21327;&#21516;&#23398;&#20064;&#65288;STL-SNNs&#65289;&#35757;&#32451;&#30340;SNNs&#22312;&#21508;&#31181;&#38745;&#24577;&#21644;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#36828;&#20248;&#20110;&#20351;&#29992;&#20004;&#20010;&#36864;&#21270;&#30340;&#21333;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;SNNs&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#20102;&#31070;&#32463;&#20803;&#38408;&#20540;&#65292;&#20026;&#32593;&#32476;&#22609;&#36896;&#20102;&#39069;&#22806;&#30340;&#21160;&#24577;&#35843;&#25972;&#24230;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20102;&#31070;&#32463;&#20803;&#22266;&#26377;&#30340;&#38750;&#31361;&#35302;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have demonstrated excellent capabilities in various intelligent scenarios. Most existing methods for training SNNs are based on the concept of synaptic plasticity; however, learning in the realistic brain also utilizes intrinsic non-synaptic mechanisms of neurons. The spike threshold of biological neurons is a critical intrinsic neuronal feature that exhibits rich dynamics on a millisecond timescale and has been proposed as an underlying mechanism that facilitates neural information processing. In this study, we develop a novel synergistic learning approach that involves simultaneously training synaptic weights and spike thresholds in SNNs. SNNs trained with synapse-threshold synergistic learning~(STL-SNNs) achieve significantly superior performance on various static and neuromorphic datasets than SNNs trained with two degenerated single-learning models. During training, the synergistic learning approach optimizes neural thresholds, providing the network 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#25552;&#37266;&#26410;&#26469;&#38656;&#35201;&#29702;&#35299;&#27424;&#25311;&#21512;&#21046;&#24230;&#19979;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2206.00501</link><description>&lt;p&gt;
&#20998;&#31867;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65306;&#26356;&#22823;&#27169;&#22411;&#30340;&#21457;&#29616;&#21487;&#35777;&#26126;&#23545;&#25239;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models. (arXiv:2206.00501v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#25552;&#37266;&#26410;&#26469;&#38656;&#35201;&#29702;&#35299;&#27424;&#25311;&#21512;&#21046;&#24230;&#19979;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#30740;&#31350;&#20026;&#36229;&#21442;&#25968;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21151;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36807;&#25311;&#21512;&#26159;&#21542;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#30495;&#30340;&#26159;&#33391;&#24615;&#30340;&#12290;&#25105;&#20204;&#24320;&#22987;&#35266;&#23519;&#21040;&#19968;&#20010; ResNet &#27169;&#22411;&#22312; Cifar10 &#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312; ImageNet &#19978;&#21017;&#19981;&#33391;&#12290;&#20026;&#20102;&#20102;&#35299;&#20026;&#20160;&#20040;&#33391;&#24615;&#36807;&#25311;&#21512;&#22312; ImageNet &#23454;&#39564;&#20013;&#22833;&#36133;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#27604;&#25968;&#25454;&#28857;&#25968;&#37327;&#19981;&#26126;&#26174;&#22823;&#30340;&#38480;&#23450;&#26465;&#20214;&#19979;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#12290;&#22312;&#36825;&#20010;&#36731;&#24494;&#36229;&#21442;&#25968;&#21270;&#30340;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#20102;&#19968;&#20010;&#30456;&#21464;&#65306;&#19982;&#20043;&#21069;&#30340;&#37325;&#36229;&#21442;&#25968;&#21270;&#35774;&#32622;&#19981;&#21516;&#65292;&#24403;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#26102;&#65292;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#22312;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#35299;&#37322;&#20102;&#25105;&#20204;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#24182;&#36890;&#36807;&#19968;&#32452; ResNet &#30340;&#25511;&#21046;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#29702;&#35299;&#27424;&#25311;&#21512;&#21046;&#24230;&#19979;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#65292;&#20316;&#20026;&#26410;&#26469;&#30340;&#19968;&#20010;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studies on benign overfitting provide insights for the success of overparameterized deep learning models. In this work, we examine whether overfitting is truly benign in real-world classification tasks. We start with the observation that a ResNet model overfits benignly on Cifar10 but not benignly on ImageNet. To understand why benign overfitting fails in the ImageNet experiment, we theoretically analyze benign overfitting under a more restrictive setup where the number of parameters is not significantly larger than the number of data points. Under this mild overparameterization setup, our analysis identifies a phase change: unlike in the previous heavy overparameterization settings, benign overfitting can now fail in the presence of label noise. Our analysis explains our empirical observations, and is validated by a set of control experiments with ResNets. Our work highlights the importance of understanding implicit bias in underfitting regimes as a future direction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28176;&#36827;&#38750;&#32467;&#26500;&#21270;&#24133;&#20540;&#20462;&#21098;&#36827;&#34892;&#20462;&#21098;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#12290;&#20351;&#29992;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#34987;&#20462;&#21098;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25506;&#32034;&#25110;&#19987;&#38376;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#36716;&#31227;&#21040;&#26032;&#30340;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#20013;&#65292;Sparse*BERT&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;BioBERT&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.12452</link><description>&lt;p&gt;
&#31232;&#30095;*BERT&#65306;&#31232;&#30095;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#30340;&#20219;&#21153;&#21644;&#39046;&#22495;&#65288;&#32763;&#35793;&#33258;arXiv:2205.12452v2 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Sparse*BERT: Sparse Models Generalize To New tasks and Domains. (arXiv:2205.12452v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28176;&#36827;&#38750;&#32467;&#26500;&#21270;&#24133;&#20540;&#20462;&#21098;&#36827;&#34892;&#20462;&#21098;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#12290;&#20351;&#29992;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#34987;&#20462;&#21098;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25506;&#32034;&#25110;&#19987;&#38376;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#36716;&#31227;&#21040;&#26032;&#30340;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#20013;&#65292;Sparse*BERT&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;BioBERT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22823;&#22810;&#25968;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#30340;&#26680;&#24515;&#26550;&#26500;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#20219;&#21153;&#21644;&#39046;&#22495;&#20043;&#38388;&#22987;&#32456;&#25552;&#20379;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#24320;&#38144;&#21487;&#33021;&#20250;&#20351;&#25512;&#29702;&#21464;&#24471;&#22256;&#38590;&#21644;&#26114;&#36149;&#12290;&#20026;&#20102;&#20351;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#25104;&#26412;&#26356;&#20302;&#65292;&#36817;&#26399;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#20462;&#21098;&#12289;&#37327;&#21270;&#21644;&#33976;&#39311;&#26469;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#24182;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28176;&#36827;&#38750;&#32467;&#26500;&#21270;&#24133;&#20540;&#20462;&#21098;&#36827;&#34892;&#20462;&#21098;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#34987;&#20462;&#21098;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25506;&#32034;&#25110;&#19987;&#38376;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#36716;&#31227;&#21040;&#26032;&#30340;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#31232;&#30095;&#36890;&#29992;&#27169;&#22411;Sparse*BERT&#21487;&#20197;&#36890;&#36807;&#22312;&#38750;&#32467;&#26500;&#21270;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#19978;&#39044;&#35757;&#32451;&#21387;&#32553;&#30340;&#26550;&#26500;&#32780;&#25104;&#20026;SparseBioBERT&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#20013;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;BioBERT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have become the core architecture upon which most modern natural language processing (NLP) systems build. These models can consistently deliver impressive accuracy and robustness across tasks and domains, but their high computational overhead can make inference difficult and expensive. To make using these models less costly, recent work has explored leveraging structured and unstructured pruning, quantization, and distillation to improve inference speed and decrease size. This paper studies how models pruned using Gradual Unstructured Magnitude Pruning can transfer between domains and tasks. Our experimentation shows that models that are pruned during pretraining using general domain masked language models can transfer to novel domains and tasks without extensive hyperparameter exploration or specialized approaches. We demonstrate that our general sparse model Sparse*BERT can become SparseBioBERT simply by pretraining the compressed architecture on unstructured bi
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35299;&#20915;&#23567;&#26679;&#26412;&#20154;&#31867;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22788;&#29702;&#22810;&#20219;&#21153;&#39044;&#27979;&#24182;&#21487;&#20197;&#20174;&#25972;&#20307;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2203.16309</link><description>&lt;p&gt;
&#38754;&#21521;&#23567;&#26679;&#26412;&#20154;&#31867;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot meta-learning for small-scale data from human subjects. (arXiv:2203.16309v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16309
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#23567;&#26679;&#26412;&#20154;&#31867;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22788;&#29702;&#22810;&#20219;&#21153;&#39044;&#27979;&#24182;&#21487;&#20197;&#20174;&#25972;&#20307;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#22312;&#22823;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#23454;&#38469;&#19978;&#35768;&#22810;&#20154;&#31867;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#35268;&#27169;&#36739;&#23567;&#19988;&#26631;&#35760;&#31232;&#30095;&#12290;&#24050;&#26377;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#36825;&#26679;&#30340;&#25968;&#25454;&#36890;&#24120;&#19981;&#23481;&#26131;&#27867;&#21270;&#21040;&#26679;&#26412;&#22806;&#30340;&#21463;&#35797;&#32773;&#12290;&#30456;&#21453;&#65292;&#27169;&#22411;&#24517;&#39035;&#23545;&#21487;&#33021;&#26469;&#33258;&#19981;&#21516;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#36825;&#26159;&#19968;&#20010;&#31216;&#20026;&#8220;&#38646;&#26679;&#26412;&#23398;&#20064;&#8221;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;&#20110;&#26679;&#26412;&#22806;&#30340;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#30495;&#23454;&#30340;&#23567;&#35268;&#27169;&#20154;&#31867;&#21463;&#35797;&#32773;&#25968;&#25454;&#38598;&#65288;&#20004;&#20010;&#38543;&#26426;&#23545;&#29031;&#30740;&#31350;&#21644;&#19968;&#20010;&#35266;&#23519;&#24615;&#30740;&#31350;&#65289;&#26469;&#39044;&#27979;&#20445;&#30041;&#30340;&#27835;&#30103;&#32452;&#30340;&#27835;&#30103;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23398;&#20064;&#27599;&#31181;&#24178;&#39044;&#30340;&#28508;&#22312;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#19988;&#33021;&#22815;&#33258;&#28982;&#22320;&#22788;&#29702;&#22810;&#20219;&#21153;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20174;&#25972;&#20307;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
While developments in machine learning led to impressive performance gains on big data, many human subjects data are, in actuality, small and sparsely labeled. Existing methods applied to such data often do not easily generalize to out-of-sample subjects. Instead, models must make predictions on test data that may be drawn from a different distribution, a problem known as \textit{zero-shot learning}. To address this challenge, we develop an end-to-end framework using a meta-learning approach, which enables the model to rapidly adapt to a new prediction task with limited training data for out-of-sample test data. We use three real-world small-scale human subjects datasets (two randomized control studies and one observational study), for which we predict treatment outcomes for held-out treatment groups. Our model learns the latent treatment effects of each intervention and, by design, can naturally handle multi-task predictions. We show that our model performs the best holistically for e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#31995;&#21015;&#65292;&#21487;&#20197;&#26356;&#24555;&#36895;&#22320;&#22312;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2202.01752</link><description>&lt;p&gt;
&#19981;&#23436;&#32654;&#20449;&#24687;&#21338;&#24328;&#20013;&#30340;&#36817;&#20284;&#26368;&#20248;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Learning of Extensive-Form Games with Imperfect Information. (arXiv:2202.01752v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#31995;&#21015;&#65292;&#21487;&#20197;&#26356;&#24555;&#36895;&#22320;&#22312;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#23398;&#20064;&#19981;&#23436;&#32654;&#20449;&#24687;&#24191;&#20041;&#21338;&#24328;&#30340;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;&#35774;&#35745;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#31639;&#27861;&#31995;&#21015;&#65292;&#20165;&#38656;&#35201; $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ &#23616;&#28216;&#25103;&#21363;&#21487;&#22312;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#25214;&#21040;&#19968;&#20010; $\varepsilon$-&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#65292;&#20854;&#20013; $X,Y$ &#26159;&#20449;&#24687;&#38598;&#30340;&#25968;&#37327;&#65292;$A,B$ &#26159;&#20004;&#21517;&#29609;&#23478;&#30340;&#34892;&#21160;&#25968;&#12290;&#36825;&#27604;&#24050;&#30693;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230; $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ &#26377;&#30528; $\widetilde{\mathcal{O}}(\max\{X, Y\})$ &#30340;&#24040;&#22823;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#23545;&#25968;&#22240;&#23376;&#20869;&#19982;&#20449;&#24687;&#29702;&#35770;&#19979;&#38480;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#26032;&#31639;&#27861;&#23454;&#29616;&#20102;&#36825;&#31181;&#26679;&#26412;&#22797;&#26434;&#24230;&#65306;&#24179;&#34913;&#22312;&#32447;&#38236;&#38754;&#19979;&#38477;&#21644;&#24179;&#34913;&#21453;&#20107;&#23454;&#21518;&#24724;&#26368;&#23567;&#21270;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#20381;&#36182;&#20110;&#23558;&#8220;&#24179;&#34913;&#25506;&#32034;&#31574;&#30053;&#8221;&#38598;&#25104;&#21040;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#25163;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#26356;&#24191;&#27867;&#30340;&#25903;&#25345;&#19981;&#23436;&#32654;&#20449;&#24687;&#21338;&#24328;&#30340;&#20108;&#20154;&#21338;&#24328;&#21644;&#22810;&#20154;&#21338;&#24328;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper resolves the open question of designing near-optimal algorithms for learning imperfect-information extensive-form games from bandit feedback. We present the first line of algorithms that require only $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ episodes of play to find an $\varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where $X,Y$ are the number of information sets and $A,B$ are the number of actions for the two players. This improves upon the best known sample complexity of $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ by a factor of $\widetilde{\mathcal{O}}(\max\{X, Y\})$, and matches the information-theoretic lower bound up to logarithmic factors. We achieve this sample complexity by two new algorithms: Balanced Online Mirror Descent, and Balanced Counterfactual Regret Minimization. Both algorithms rely on novel approaches of integrating \emph{balanced exploration policies} into their classical counterparts. We also extend our results t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#22522;&#20110;&#28385;&#36275;&#32858;&#21512;&#20989;&#25968;&#36827;&#21475;&#27861;&#21017;&#30340;&#27169;&#31946;&#34164;&#21547;&#30340;MISO&#27169;&#31946;&#23618;&#27425;&#25512;&#29702;&#24341;&#25806;&#65292;&#20197;&#25552;&#39640;MISO&#27169;&#31946;&#31995;&#32479;&#20013;&#27169;&#31946;&#25512;&#29702;&#24341;&#25806;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2112.12808</link><description>&lt;p&gt;
&#28385;&#36275;&#32858;&#21512;&#20989;&#25968;&#36827;&#21475;&#27861;&#21017;&#30340;MISO&#23618;&#27425;&#25512;&#29702;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
MISO hierarchical inference engine satisfying the law of importation with aggregation functions. (arXiv:2112.12808v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#22522;&#20110;&#28385;&#36275;&#32858;&#21512;&#20989;&#25968;&#36827;&#21475;&#27861;&#21017;&#30340;&#27169;&#31946;&#34164;&#21547;&#30340;MISO&#27169;&#31946;&#23618;&#27425;&#25512;&#29702;&#24341;&#25806;&#65292;&#20197;&#25552;&#39640;MISO&#27169;&#31946;&#31995;&#32479;&#20013;&#27169;&#31946;&#25512;&#29702;&#24341;&#25806;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#25512;&#29702;&#24341;&#25806;&#20316;&#20026;&#27169;&#31946;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#65292;&#21487;&#20197;&#20351;&#29992;&#27169;&#31946;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#20174;&#36755;&#20837;&#31354;&#38388;&#21644;&#27169;&#31946;&#35268;&#21017;&#24211;&#20013;&#33719;&#21462;&#19968;&#20123;&#26377;&#24847;&#20041;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#22686;&#24378;&#22810;&#36755;&#20837;&#21333;&#36755;&#20986;&#65288;MISO&#65289;&#27169;&#31946;&#31995;&#32479;&#20013;&#27169;&#31946;&#25512;&#29702;&#24341;&#25806;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#19977;&#31181;&#22522;&#20110;&#28385;&#36275;&#32858;&#21512;&#20989;&#25968;&#36827;&#21475;&#27861;&#21017;&#65288;LIA&#65289;&#30340;&#27169;&#31946;&#34164;&#21547;&#30340;MISO&#27169;&#31946;&#23618;&#27425;&#25512;&#29702;&#24341;&#25806;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#20123;&#28385;&#36275;&#65288;LIA&#65289;&#30340;&#24050;&#30693;&#27169;&#31946;&#34164;&#21547;&#30340;&#32858;&#21512;&#20989;&#25968;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#32858;&#21512;&#20989;&#25968;&#65292;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#28385;&#36275;&#35813;&#32858;&#21512;&#20989;&#25968;&#19979;&#65288;LIA&#65289;&#30340;&#27169;&#31946;&#34164;&#21547;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#19978;&#36848;&#29702;&#35770;&#21457;&#23637;&#26500;&#24314;&#20102;&#19977;&#20010;MISO&#27169;&#31946;&#31995;&#32479;&#20013;&#30340;&#27169;&#31946;&#23618;&#27425;&#25512;&#29702;&#24341;&#25806;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fuzzy inference engine, as one of the most important components of fuzzy systems, can obtain some meaningful outputs from fuzzy sets on input space and fuzzy rule base using fuzzy logic inference methods. In order to enhance the computational efficiency of fuzzy inference engine in multi-input-single-output(MISO) fuzzy systems,this paper aims mainly to investigate three MISO fuzzy hierarchial inference engines based on fuzzy implications satisfying the law of importation with aggregation functions (LIA). We firstly find some aggregation functions for well-known fuzzy implications such that they satisfy (LIA). For a given aggregation function, the fuzzy implication which satisfies (LIA) with this aggregation function is then characterized. Finally, we construct three fuzzy hierarchical inference engines in MISO fuzzy systems applying aforementioned theoretical developments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;DNN&#20013;&#29702;&#35299;&#20135;&#29983;&#30340;&#20132;&#20114;&#27010;&#24565;&#30340;&#27010;&#24565;-emerging&#29616;&#35937;&#65292;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#29992;&#31232;&#30095;&#30340;&#31526;&#21495;&#22240;&#26524;&#22270;&#21644;And-Or&#22270;&#65288;AOG&#65289;&#36827;&#34892;&#37327;&#21270;&#21644;&#31616;&#21270;&#12290;</title><link>http://arxiv.org/abs/2111.06206</link><description>&lt;p&gt;
DNN&#20013;&#31232;&#30095;&#27010;&#24565;&#30340;&#23450;&#20041;&#21644;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Defining and Quantifying the Emergence of Sparse Concepts in DNNs. (arXiv:2111.06206v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;DNN&#20013;&#29702;&#35299;&#20135;&#29983;&#30340;&#20132;&#20114;&#27010;&#24565;&#30340;&#27010;&#24565;-emerging&#29616;&#35937;&#65292;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#29992;&#31232;&#30095;&#30340;&#31526;&#21495;&#22240;&#26524;&#22270;&#21644;And-Or&#22270;&#65288;AOG&#65289;&#36827;&#34892;&#37327;&#21270;&#21644;&#31616;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35828;&#26126;&#22312;&#35757;&#32451;&#36807;&#30340;DNN&#20013;&#27010;&#24565;&#30340;&#20135;&#29983;&#29616;&#35937;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;DNN&#30340;&#25512;&#29702;&#20998;&#25968;&#21487;&#20197;&#20998;&#35299;&#20026;&#23569;&#25968;&#20132;&#20114;&#27010;&#24565;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#19968;&#20010;&#31232;&#30095;&#30340;&#31526;&#21495;&#22240;&#26524;&#22270;&#20013;&#30340;&#22240;&#26524;&#27169;&#24335;&#65292;&#35813;&#22270;&#35299;&#37322;&#20102;DNN&#12290;&#20351;&#29992;&#27492;&#31867;&#22240;&#26524;&#22270;&#23545;DNN&#36827;&#34892;&#35299;&#37322;&#30340;&#24544;&#23454;&#24615;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#65292;&#22240;&#20026;&#25105;&#20204;&#35777;&#26126;&#20102;&#22240;&#26524;&#22270;&#21487;&#20197;&#22312;&#25351;&#25968;&#25968;&#37327;&#30340;&#19981;&#21516;&#23631;&#34109;&#26679;&#26412;&#19978;&#24456;&#22909;&#22320;&#27169;&#20223;DNN&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#36825;&#26679;&#30340;&#22240;&#26524;&#22270;&#21487;&#20197;&#36827;&#19968;&#27493;&#31616;&#21270;&#24182;&#37325;&#26032;&#32534;&#20889;&#20026;And-Or&#22270;&#65288;AOG&#65289;&#65292;&#32780;&#19981;&#20250;&#22833;&#21435;&#22826;&#22810;&#30340;&#35299;&#37322;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to illustrate the concept-emerging phenomenon in a trained DNN. Specifically, we find that the inference score of a DNN can be disentangled into the effects of a few interactive concepts. These concepts can be understood as causal patterns in a sparse, symbolic causal graph, which explains the DNN. The faithfulness of using such a causal graph to explain the DNN is theoretically guaranteed, because we prove that the causal graph can well mimic the DNN's outputs on an exponential number of different masked samples. Besides, such a causal graph can be further simplified and re-written as an And-Or graph (AOG), without losing much explanation accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#27169;&#22359;&#29992;&#20110;&#28040;&#38500;&#22768;&#23398;&#29305;&#24449;&#20013;&#30340;&#35828;&#35805;&#20154;&#20449;&#24687;&#65292;&#36824;&#28155;&#21152;&#20102;&#25511;&#21046;&#35828;&#35805;&#20154;&#20449;&#24687;&#30340;&#21151;&#33021;&#20197;&#32500;&#25345;&#35821;&#38899;&#30340;&#20811;&#38534;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.03811</link><description>&lt;p&gt;
SIG-VC: &#19968;&#27454;&#38024;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#35828;&#35805;&#20154;&#20449;&#24687;&#25351;&#23548;&#30340;&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SIG-VC: A Speaker Information Guided Zero-shot Voice Conversion System for Both Human Beings and Machines. (arXiv:2111.03811v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#27169;&#22359;&#29992;&#20110;&#28040;&#38500;&#22768;&#23398;&#29305;&#24449;&#20013;&#30340;&#35828;&#35805;&#20154;&#20449;&#24687;&#65292;&#36824;&#28155;&#21152;&#20102;&#25511;&#21046;&#35828;&#35805;&#20154;&#20449;&#24687;&#30340;&#21151;&#33021;&#20197;&#32500;&#25345;&#35821;&#38899;&#30340;&#20811;&#38534;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22312;&#20256;&#32479;&#35821;&#38899;&#36716;&#25442;&#20219;&#21153;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#31995;&#32479;&#37117;&#33021;&#22815;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20154;&#20204;&#30340;&#27880;&#24847;&#21147;&#36880;&#28176;&#36716;&#21521;&#26497;&#31471;&#26465;&#20214;&#19979;&#30340;&#35821;&#38899;&#36716;&#25442;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;&#26041;&#27861;&#12290;&#25105;&#20204;&#26088;&#22312;&#33719;&#24471;&#35821;&#38899;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#35828;&#35805;&#20154;&#20869;&#23481;&#20998;&#31163;&#65292;&#26356;&#22909;&#22320;&#28040;&#38500;&#35828;&#35805;&#20154;&#20449;&#24687;&#65292;&#33719;&#21462;&#32431;&#31929;&#30340;&#20869;&#23481;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#28304;&#21457;&#22768;&#32773;&#30340;&#22768;&#23398;&#29305;&#24449;&#20013;&#21435;&#38500;&#35828;&#35805;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#28155;&#21152;&#20102;&#35828;&#35805;&#20154;&#20449;&#24687;&#25511;&#21046;&#65292;&#20197;&#32500;&#25345;&#35821;&#38899;&#20811;&#38534;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20027;&#35266;&#21644;&#23458;&#35266;&#25351;&#26631;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#31995;&#32479;&#26174;&#33879;&#38477;&#20302;&#20102;&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;&#20013;&#30340;&#25240;&#34935;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#39640;&#30340;&#27450;&#39575;&#24615;&#33021;&#65292;&#21487;&#20197;&#23545;&#25239;&#35828;&#35805;&#20154;&#39564;&#35777;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, as more and more systems achieve good performance in traditional voice conversion (VC) tasks, people's attention gradually turns to VC tasks under extreme conditions. In this paper, we propose a novel method for zero-shot voice conversion. We aim to obtain intermediate representations for speaker-content disentanglement of speech to better remove speaker information and get pure content information. Accordingly, our proposed framework contains a module that removes the speaker information from the acoustic feature of the source speaker. Moreover, speaker information control is added to our system to maintain the voice cloning performance. The proposed system is evaluated by subjective and objective metrics. Results show that our proposed system significantly reduces the trade-off problem in zero-shot voice conversion, while it also manages to have high spoofing power to the speaker verification system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#27010;&#36848;&#36817;&#24180;&#26469;&#25552;&#20986;&#30340;&#20844;&#24179;&#24615;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#65288;FAFL&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2111.01872</link><description>&lt;p&gt;
&#36861;&#27714;&#20844;&#24179;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Fairness-Aware Federated Learning. (arXiv:2111.01872v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.01872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#27010;&#36848;&#36817;&#24180;&#26469;&#25552;&#20986;&#30340;&#20844;&#24179;&#24615;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#65288;FAFL&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#26368;&#36817;&#36827;&#23637;&#20026;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#30340;&#26426;&#20250;&#65292;&#24182;&#20445;&#35777;&#20102;&#24615;&#33021;&#21644;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20316;&#21697;&#38598;&#20013;&#20110;FL&#20013;&#22830;&#25511;&#21046;&#22120;&#30340;&#21033;&#30410;&#65292;&#32780;&#24573;&#35270;&#20102;FL&#23458;&#25143;&#31471;&#30340;&#21033;&#30410;&#12290;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#23545;&#24453;&#23458;&#25143;&#31471;&#65292;&#20351;&#20854;&#19981;&#31215;&#26497;&#21442;&#19982;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#25439;&#23475;FL&#29983;&#24577;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;FL&#30340;&#20844;&#24179;&#24615;&#27491;&#22312;&#21560;&#24341;&#30528;&#22823;&#37327;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#36817;&#24180;&#26469;&#65292;&#19981;&#21516;&#35282;&#24230;&#30340;&#20844;&#24179;&#24615;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#65288;FAFL&#65289;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20840;&#38754;&#30340;&#35843;&#26597;&#26469;&#24110;&#21161;&#35835;&#32773;&#28145;&#20837;&#20102;&#35299;&#36825;&#20010;&#36328;&#23398;&#31185;&#39046;&#22495;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#36825;&#26679;&#19968;&#31687;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Federated Learning (FL) have brought large-scale collaborative machine learning opportunities for massively distributed clients with performance and data privacy guarantees. However, most current works focus on the interest of the central controller in FL,and overlook the interests of the FL clients. This may result in unfair treatment of clients that discourages them from actively participating in the learning process and damages the sustainability of the FL ecosystem. Therefore, the topic of ensuring fairness in FL is attracting a great deal of research interest. In recent years, diverse Fairness-Aware FL (FAFL) approaches have been proposed in an effort to achieve fairness in FL from different perspectives. However, there is no comprehensive survey that helps readers gain insight into this interdisciplinary field. This paper aims to provide such a survey. By examining the fundamental and simplifying assumptions, as well as the notions of fairness adopted by existi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#20197;&#20351;&#29992;&#36739;&#24369;&#20551;&#35774;&#35777;&#26126;&#25910;&#25947;&#30340;&#19968;&#33324;&#26041;&#27861;&#65307;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#26041;&#27861;&#35777;&#26126;&#20102;SARSA&#31639;&#27861;&#30340;&#25209;&#37327;&#24322;&#27493;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2109.03445</link><description>&lt;p&gt;
&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#30340;&#25910;&#25947;&#24615;&#21450;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning. (arXiv:2109.03445v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#20197;&#20351;&#29992;&#36739;&#24369;&#20551;&#35774;&#35777;&#26126;&#25910;&#25947;&#30340;&#19968;&#33324;&#26041;&#27861;&#65307;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#26041;&#27861;&#35777;&#26126;&#20102;SARSA&#31639;&#27861;&#30340;&#25209;&#37327;&#24322;&#27493;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#31639;&#27861;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20165;&#21487;&#29992;&#20989;&#25968;&#30340;&#26377;&#22122;&#27979;&#37327;&#24773;&#20917;&#19979;&#25214;&#21040;&#38646;&#28857;&#25110;&#22266;&#23450;&#28857;&#12290;&#30446;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#21306;&#20998;&#8220;&#21516;&#27493;&#8221;&#26356;&#26032;&#21644;&#8220;&#24322;&#27493;&#8221;&#26356;&#26032;&#65292;&#22312;&#8220;&#21516;&#27493;&#8221;&#26356;&#26032;&#20013;&#65292;&#27599;&#20010;&#29468;&#27979;&#30340;&#32452;&#20214;&#37117;&#20250;&#22312;&#27599;&#20010;&#26102;&#38388;&#26356;&#26032;&#65292;&#32780;&#22312;&#8220;&#24322;&#27493;&#8221;&#26356;&#26032;&#20013;&#65292;&#20165;&#26356;&#26032;&#19968;&#20010;&#32452;&#20214;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20013;&#38388;&#24773;&#20917;&#65292;&#31216;&#20026;&#8220;&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#8221;&#65288;BASA&#65289;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#26102;&#38388;&#28857;&#20165;&#26356;&#26032;&#8220;&#24403;&#21069;&#20272;&#35745;&#35299;&#8221;&#30340;&#19968;&#20123;&#20294;&#19981;&#26159;&#20840;&#37096;&#30340;&#32452;&#20214;&#12290;BASA&#20801;&#35768;&#29992;&#25143;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#35777;&#26126;&#27492;&#31867;&#31639;&#27861;&#25910;&#25947;&#20110;&#25152;&#30740;&#31350;&#26144;&#23556;&#30340;&#22266;&#23450;&#28857;&#12290;&#36825;&#20123;&#25910;&#25947;&#35777;&#26126;&#20351;&#29992;&#27604;&#29616;&#26377;&#32467;&#26524;&#26356;&#24369;&#30340;&#20551;&#35774;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29616;&#26377;&#30340;&#25910;&#25947;&#35777;&#26126;&#35201;&#27714;&#27493;&#38271;&#21442;&#25968;&#20197;&#36866;&#24403;&#30340;&#36895;&#29575;&#19979;&#38477;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20165;&#35201;&#27714;&#27599;&#20010;&#32452;&#20214;&#20855;&#26377;&#36275;&#22815;&#30340;&#26356;&#26032;&#39057;&#29575;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#65292;&#35777;&#26126;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;SARSA&#31639;&#27861;&#30340;&#25209;&#37327;&#24322;&#27493;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic approximation (SA) algorithm is a widely used probabilistic method for finding a zero or a fixed point of a vector-valued funtion, when only noisy measurements of the function are available. In the literature to date, one makes a distinction between ``synchronous'' updating, whereby every component of the current guess is updated at each time, and ``asynchronous'' updating, whereby only one component is updated. In this paper, we study an intermediate situation that we call ``batch asynchronous stochastic approximation'' (BASA), in which, at each time instant, \textit{some but not all} components of the current estimated solution are updated. BASA allows the user to trade off memory requirements against time complexity. We develop a general methodology for proving that such algorithms converge to the fixed point of the map under study. These convergence proofs make use of weaker hypotheses than existing results. Specifically, existing convergence proofs require that the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20915;&#31574;&#35770;&#23545;&#25239;&#20013;AGI/ASI&#30340;&#33268;&#21629;&#24369;&#28857;&#20551;&#35774;&#65292;&#25552;&#20986;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#20915;&#31574;&#35770;&#22916;&#24819;&#23548;&#33268;&#22312;&#23545;&#25239;&#29615;&#22659;&#20013;&#20570;&#20986;&#38750;&#29702;&#24615;&#20915;&#31574;&#30340;&#24369;&#28857;&#65292;&#24182;&#20026;&#29702;&#35299;&#21644;&#21457;&#29616;&#36825;&#20123;&#24369;&#28857;&#22312;&#26410;&#26469;AGI/ASI&#31995;&#32479;&#20013;&#21487;&#33021;&#30340;&#20307;&#29616;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2010.05418</link><description>&lt;p&gt;
&#20915;&#31574;&#35770;&#23545;&#25239;&#20013;AGI/ASI&#30340;&#33268;&#21629;&#24369;&#28857;
&lt;/p&gt;
&lt;p&gt;
Achilles Heels for AGI/ASI via Decision Theoretic Adversaries. (arXiv:2010.05418v9 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.05418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20915;&#31574;&#35770;&#23545;&#25239;&#20013;AGI/ASI&#30340;&#33268;&#21629;&#24369;&#28857;&#20551;&#35774;&#65292;&#25552;&#20986;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#20915;&#31574;&#35770;&#22916;&#24819;&#23548;&#33268;&#22312;&#23545;&#25239;&#29615;&#22659;&#20013;&#20570;&#20986;&#38750;&#29702;&#24615;&#20915;&#31574;&#30340;&#24369;&#28857;&#65292;&#24182;&#20026;&#29702;&#35299;&#21644;&#21457;&#29616;&#36825;&#20123;&#24369;&#28857;&#22312;&#26410;&#26469;AGI/ASI&#31995;&#32479;&#20013;&#21487;&#33021;&#30340;&#20307;&#29616;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#20102;&#35299;&#20808;&#36827;&#31995;&#32479;&#22914;&#20309;&#20570;&#20986;&#36873;&#25321;&#21450;&#20854;&#22833;&#36133;&#26041;&#24335;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#12290;&#22312;&#26576;&#20123;&#39046;&#22495;&#65292;&#26426;&#22120;&#24050;&#32463;&#33021;&#22815;&#36229;&#36234;&#20154;&#31867;&#65292;&#22914;&#20309;&#23433;&#20840;&#22320;&#26500;&#24314;&#21487;&#33021;&#20855;&#26377;&#19982;&#20154;&#31867;&#30456;&#21516;&#25110;&#26356;&#39640;&#33021;&#21147;&#30340;&#31995;&#32479;&#26159;&#19968;&#20010;&#29305;&#21035;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#38463;&#21888;&#29705;&#26031;&#20043;&#36405;&#8221;&#20551;&#35774;&#65292;&#21363;&#21363;&#20351;&#26159;&#28508;&#22312;&#30340;&#36229;&#32423;&#26234;&#33021;&#31995;&#32479;&#65292;&#20854;&#20063;&#21487;&#33021;&#23384;&#22312;&#31283;&#23450;&#30340;&#20915;&#31574;&#35770;&#22916;&#24819;&#65292;&#23548;&#33268;&#23427;&#20204;&#22312;&#23545;&#25239;&#29615;&#22659;&#20013;&#20570;&#20986;&#38750;&#29702;&#24615;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#23545;&#20915;&#31574;&#35770;&#25991;&#29486;&#20013;&#30340;&#20851;&#38190;&#22256;&#22659;&#21644;&#24726;&#35770;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#35752;&#35770;&#20102;&#20854;&#20013;&#19968;&#20123;&#28508;&#22312;&#30340;&#33268;&#21629;&#24369;&#28857;&#12290;&#26412;&#25991;&#36824;&#23545;&#29702;&#35299;&#36825;&#20123;&#24369;&#28857;&#21487;&#33021;&#22312;&#26410;&#26469;&#30340;AGI/ASI&#31995;&#32479;&#20013;&#22914;&#20309;&#20307;&#29616;&#20570;&#20986;&#20102;&#20960;&#20010;&#26032;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
As progress in AI continues to advance, it is important to know how advanced systems will make choices and in what ways they may fail. Machines can already outsmart humans in some domains, and understanding how to safely build ones which may have capabilities at or above the human level is of particular concern. One might suspect that artificially generally intelligent (AGI) and artificially superintelligent (ASI) will be systems that humans cannot reliably outsmart. As a challenge to this assumption, this paper presents the Achilles Heel hypothesis which states that even a potentially superintelligent system may nonetheless have stable decision-theoretic delusions which cause them to make irrational decisions in adversarial settings. In a survey of key dilemmas and paradoxes from the decision theory literature, a number of these potential Achilles Heels are discussed in context of this hypothesis. Several novel contributions are made toward understanding the ways in which these weakne
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#21152;&#24378;&#29256;&#26143;&#38469;&#20105;&#38712;&#25968;&#25454;&#38598;MSC&#65292;&#35813;&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#20854;&#20182;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#32570;&#38519;&#65292;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#25968;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/1710.03131</link><description>&lt;p&gt;
MSC: &#29992;&#20110;&#26143;&#38469;&#20105;&#38712;II&#20013;&#23439;&#35266;&#31649;&#29702;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MSC: A Dataset for Macro-Management in StarCraft II. (arXiv:1710.03131v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1710.03131
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#21152;&#24378;&#29256;&#26143;&#38469;&#20105;&#38712;&#25968;&#25454;&#38598;MSC&#65292;&#35813;&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#20854;&#20182;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#32570;&#38519;&#65292;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#25968;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23439;&#35266;&#31649;&#29702;&#26159;&#26143;&#38469;&#20105;&#38712;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#30740;&#31350;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#23384;&#22312;&#19968;&#20123;&#32570;&#38519;&#65292;&#38590;&#20197;&#25552;&#21319;&#23398;&#26415;&#21644;&#24037;&#19994;&#30740;&#31350;&#30340;&#25928;&#26524;&#65306;1&#65289;&#26576;&#20123;&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#26631;&#20934;&#30340;&#39044;&#22788;&#29702;&#12289;&#35299;&#26512;&#21644;&#29305;&#24449;&#25552;&#21462;&#31243;&#24207;&#20197;&#21450;&#39044;&#23450;&#20041;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#38598;&#65307;2&#65289;&#19968;&#20123;&#25968;&#25454;&#38598;&#20165;&#38024;&#23545;&#23439;&#35266;&#31649;&#29702;&#20013;&#30340;&#29305;&#23450;&#20219;&#21153;&#65307;3&#65289;&#19968;&#20123;&#25968;&#25454;&#38598;&#35201;&#20040;&#22826;&#23567;&#65292;&#35201;&#20040;&#27809;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#38590;&#20197;&#20351;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#21508;&#31181;&#29305;&#24449;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#38590;&#20197;&#30452;&#25509;&#36827;&#34892;&#27604;&#36739;&#12290;&#20026;&#20102;&#25552;&#21319;&#26143;&#38469;&#20105;&#38712;&#23439;&#35266;&#31649;&#29702;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#22522;&#20110;SC2LE&#24179;&#21488;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;MSC&#12290;MSC&#30001;&#31934;&#24515;&#35774;&#35745;&#30340;&#29305;&#24449;&#21521;&#37327;&#21644;&#39044;&#23450;&#20041;&#30340;&#39640;&#32423;&#26679;&#26412;&#32452;&#25104;&#65292;&#33021;&#22815;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#26356;&#22909;&#30340;&#25968;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Macro-management is an important problem in StarCraft, which has been studied for a long time. Various datasets together with assorted methods have been proposed in the last few years. But these datasets have some defects for boosting the academic and industrial research: 1) There're neither standard preprocessing, parsing and feature extraction procedures nor predefined training, validation and test set in some datasets. 2) Some datasets are only specified for certain tasks in macro-management. 3) Some datasets are either too small or don't have enough labeled data for modern machine learning algorithms such as deep neural networks. So most previous methods are trained with various features, evaluated on different test sets from the same or different datasets, making it difficult to be compared directly. To boost the research of macro-management in StarCraft, we release a new dataset MSC based on the platform SC2LE. MSC consists of well-designed feature vectors, pre-defined high-level
&lt;/p&gt;</description></item></channel></rss>