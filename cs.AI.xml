<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2311.01248</link><description>&lt;p&gt;
&#23558;&#20854;&#25512;&#21521;&#23637;&#31034;&#26497;&#38480;&#65306;&#22810;&#27169;&#24577;&#35270;&#35273;&#35302;&#35273;&#27169;&#20223;&#23398;&#20064;&#19982;&#21147;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching. (arXiv:2311.01248v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#36807;&#31243;&#20013;&#33719;&#21462;&#23494;&#38598;&#25509;&#35302;&#20449;&#24687;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#8220;&#36879;&#35270;&#20320;&#30340;&#30382;&#32932;&#8221;&#65288;STS&#65289;&#22411;&#20256;&#24863;&#22120;&#20855;&#26377;&#35270;&#35273;&#21644;&#35302;&#35273;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#21322;&#36879;&#26126;&#34920;&#38754;&#21644;&#21487;&#25511;&#29031;&#26126;&#23454;&#29616;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#19982;&#27169;&#20223;&#23398;&#20064;&#22312;&#23500;&#26377;&#25509;&#35302;&#30340;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#35302;&#35273;&#21147;&#27979;&#37327;&#21644;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#36816;&#21160;&#31034;&#33539;&#20013;&#20135;&#29983;&#26356;&#22909;&#21305;&#37197;&#20154;&#20307;&#31034;&#33539;&#32773;&#30340;&#21147;&#26354;&#32447;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#35270;&#35273;/&#35302;&#35273;STS&#27169;&#24335;&#20999;&#25442;&#20316;&#20026;&#25511;&#21046;&#31574;&#30053;&#36755;&#20986;&#65292;&#31616;&#21270;&#20256;&#24863;&#22120;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#35266;&#23519;&#37197;&#32622;&#65292;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#35270;&#35273;/&#35302;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#27169;&#24335;&#20999;&#25442;&#21644;&#19981;&#20999;&#25442;&#65289;&#19982;&#25163;&#33109;&#25346;&#36733;&#30340;&#30524;&#22312;&#25163;&#25668;&#20687;&#26426;&#30340;&#35270;&#35273;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24191;&#27867;&#30340;&#23454;&#39564;&#31995;&#21015;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#20013;&#23398;&#20064;&#26679;&#26412;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#19981;&#21516;&#35270;&#35282;&#23545;&#26679;&#26412;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#25913;&#36827;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01212</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#20851;&#31995;&#23398;&#20064;&#29992;&#20110;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-view Relation Learning for Cross-domain Few-shot Hyperspectral Image Classification. (arXiv:2311.01212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#20013;&#23398;&#20064;&#26679;&#26412;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#19981;&#21516;&#35270;&#35282;&#23545;&#26679;&#26412;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#25913;&#36827;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#20391;&#37325;&#20110;&#20174;&#28304;&#39046;&#22495;&#20013;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#20013;&#23398;&#20064;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#20013;&#20165;&#21253;&#21547;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#20219;&#21153;&#20013;&#12290;&#22312;&#24230;&#37327;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#35768;&#22810;&#24403;&#21069;&#30340;&#26041;&#27861;&#39318;&#20808;&#25552;&#21462;&#26597;&#35810;&#26679;&#26412;&#21644;&#25903;&#25345;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#26681;&#25454;&#26597;&#35810;&#26679;&#26412;&#21040;&#25903;&#25345;&#26679;&#26412;&#25110;&#21407;&#22411;&#30340;&#36317;&#31163;&#30452;&#25509;&#39044;&#27979;&#26597;&#35810;&#26679;&#26412;&#30340;&#31867;&#21035;&#12290;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#19982;&#24403;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#19981;&#21516;&#35270;&#35282;&#23398;&#20064;&#26679;&#26412;&#20851;&#31995;&#24182;&#23558;&#20854;&#32435;&#20837;&#27169;&#22411;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#20197;&#25552;&#39640;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#22312;&#24403;&#21069;DCFSL&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#23398;&#20064;&#31867;&#21035;&#32423;&#21035;&#30340;&#26679;&#26412;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain few-shot hyperspectral image classification focuses on learning prior knowledge from a large number of labeled samples from source domain and then transferring the knowledge to the tasks which contain only few labeled samples in target domains. Following the metric-based manner, many current methods first extract the features of the query and support samples, and then directly predict the classes of query samples according to their distance to the support samples or prototypes. The relations between samples have not been fully explored and utilized. Different from current works, this paper proposes to learn sample relations from different views and take them into the model learning process, to improve the cross-domain few-shot hyperspectral image classification. Building on current DCFSL method which adopts a domain discriminator to deal with domain-level distribution difference, the proposed method applys contrastive learning to learn the class-level sample relations to o
&lt;/p&gt;</description></item><item><title>DeTiME&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#30340;LLMs&#22686;&#24378;&#25193;&#25955;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#32858;&#31867;&#30340;&#23884;&#20837;&#21644;&#20855;&#26377;&#22686;&#24378;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#33021;&#29983;&#25104;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2310.15296</link><description>&lt;p&gt;
DeTiME: &#20351;&#29992;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#30340;LLM&#22686;&#24378;&#25193;&#25955;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM. (arXiv:2310.15296v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15296
&lt;/p&gt;
&lt;p&gt;
DeTiME&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#30340;LLMs&#22686;&#24378;&#25193;&#25955;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#32858;&#31867;&#30340;&#23884;&#20837;&#21644;&#20855;&#26377;&#22686;&#24378;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#33021;&#29983;&#25104;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20805;&#28385;&#27963;&#21147;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTMs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;NTMs&#20027;&#35201;&#20351;&#29992;&#26469;&#33258;LLMs&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#36825;&#23545;&#20110;&#32858;&#31867;&#25110;&#20027;&#39064;&#29983;&#25104;&#26469;&#35828;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21517;&#20026;DeTiME&#30340;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;DeTiME&#21033;&#29992;&#32534;&#30721;-&#35299;&#30721;&#30340;LLMs&#20135;&#29983;&#39640;&#24230;&#21487;&#32858;&#31867;&#30340;&#23884;&#20837;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#33021;&#22815;&#29983;&#25104;&#26082;&#20855;&#26377;&#20248;&#36234;&#30340;&#32858;&#31867;&#24615;&#21448;&#20855;&#26377;&#22686;&#24378;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#19982;&#24050;&#35782;&#21035;&#20027;&#39064;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#21452;&#37325;&#21151;&#33021;&#20351;&#29992;&#25143;&#33021;&#22815;&#21516;&#26102;&#39640;&#25928;&#20135;&#29983;&#39640;&#24230;&#32858;&#31867;&#30340;&#20027;&#39064;&#21644;&#30456;&#20851;&#20869;&#23481;&#12290;DeTiME&#30340;&#28508;&#21147;&#36824;&#21253;&#25324;&#29983;&#25104;&#38598;&#32676;&#21270;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the burgeoning field of natural language processing, Neural Topic Models (NTMs) and Large Language Models (LLMs) have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic generation. Our study addresses this gap by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages ncoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion, our framework also provides the capability to generate content relevant to the identified topics. This dual functionality allows users to efficiently produce highly clustered topics and related content simultaneously. DeTiME's potential extends to generating clustered embeddi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;AI&#21161;&#25163;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.11971</link><description>&lt;p&gt;
&#36890;&#36807;&#32676;&#20307;&#19981;&#21464;&#24615;&#23398;&#20064;&#25552;&#39640;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization of Alignment with Human Preferences through Group Invariant Learning. (arXiv:2310.11971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;AI&#21161;&#25163;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;AI&#21161;&#25163;&#30340;&#25104;&#21151;&#22312;&#20110;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;, &#20351;&#29983;&#25104;&#30340;&#22238;&#31572;&#26356;&#21152;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;. &#20316;&#20026;&#36890;&#29992;AI&#21161;&#25163;, &#20154;&#20204;&#36234;&#26469;&#36234;&#26399;&#26395;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#34920;&#29616;&#19968;&#33268;. &#28982;&#32780;, &#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;,&#24378;&#21270;&#23398;&#20064;(RL)&#32463;&#24120;&#21033;&#29992;&#25463;&#24452;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#22870;&#21169;, &#24573;&#30053;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;. &#36825;&#31181;&#23545;&#24555;&#36895;&#22870;&#21169;&#25910;&#30410;&#30340;&#20851;&#27880;&#19981;&#20165;&#21066;&#24369;&#20102;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;, &#20063;&#21066;&#24369;&#20102;&#27169;&#22411;&#23545;&#26032;&#30340;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;. &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;, &#21487;&#20197;&#36890;&#36807;RL&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#30340;&#31574;&#30053;. &#37492;&#20110;&#33719;&#24471;&#32676;&#20307;&#26631;&#27880;&#30340;&#25361;&#25112;, &#25105;&#20204;&#30340;&#26041;&#27861;&#20250;&#33258;&#21160;&#23558;&#25968;&#25454;&#20998;&#31867;&#21040;&#19981;&#21516;&#30340;&#32452;&#20013;, &#26377;&#24847;&#22320;&#26368;&#22823;&#21270;&#24615;&#33021;&#24046;&#24322;. &#28982;&#21518;, &#25105;&#20204;&#20248;&#21270;&#31574;&#30053;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#20013;&#34920;&#29616;&#33391;&#22909;. &#26368;&#21518;, &#21033;&#29992;&#24050;&#24314;&#31435;&#30340;
&lt;/p&gt;
&lt;p&gt;
The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the estab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.08475</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#19982;&#32534;&#36753;&#21333;&#27169;&#24335;LLMs&#30456;&#27604;&#65292;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#32534;&#36753;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#23457;&#26597;&#21644;&#24910;&#37325;&#32771;&#34385;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;MMEdit&#65292;&#29992;&#20110;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21253;&#25324;&#21508;&#31181;&#27169;&#22411;&#32534;&#36753;&#22522;&#32447;&#30340;&#32508;&#21512;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#30340;&#19981;&#21516;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#22522;&#32447;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#23454;&#29616;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#20294;&#25928;&#26524;&#20173;&#28982;&#19981;&#29702;&#24819;&#65292;&#34920;&#26126;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#23384;&#22312;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#35265;&#35299;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/EasyEdit&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;</description></item><item><title>Co-NavGPT&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20840;&#23616;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#30340;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#36229;&#36234;&#29616;&#26377;&#27169;&#22411;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07937</link><description>&lt;p&gt;
Co-NavGPT: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#35270;&#35273;&#35821;&#20041;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models. (arXiv:2310.07937v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07937
&lt;/p&gt;
&lt;p&gt;
Co-NavGPT&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20840;&#23616;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#30340;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#36229;&#36234;&#29616;&#26377;&#27169;&#22411;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32423;&#20154;&#26426;&#20132;&#20114;&#20219;&#21153;&#20013;&#65292;&#23545;&#20110;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36807;&#21435;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#26159;&#35774;&#35745;&#29992;&#20110;&#21333;&#19968;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#36825;&#24448;&#24448;&#30001;&#20110;&#29615;&#22659;&#22797;&#26434;&#24615;&#32780;&#23548;&#33268;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#30340;&#31574;&#30053;&#38656;&#35201;&#36164;&#28304;&#23494;&#38598;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Co-NavGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#30340;&#20840;&#23616;&#35268;&#21010;&#22120;&#12290;Co-NavGPT&#23558;&#25506;&#32034;&#30340;&#29615;&#22659;&#25968;&#25454;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#22686;&#24378;LLMs&#23545;&#22330;&#26223;&#30340;&#29702;&#35299;&#12290;&#28982;&#21518;&#65292;&#23427;&#20026;&#27599;&#20010;&#26426;&#22120;&#20154;&#20998;&#37197;&#25506;&#32034;&#21069;&#27839;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#30446;&#26631;&#25628;&#32034;&#12290;&#22312;Habitat-Matterport 3D (HM3D)&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Co-NavGPT&#22312;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#23398;&#20064;&#36807;&#31243;&#65292;&#23637;&#31034;&#20102;LLMs&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In advanced human-robot interaction tasks, visual target navigation is crucial for autonomous robots navigating unknown environments. While numerous approaches have been developed in the past, most are designed for single-robot operations, which often suffer from reduced efficiency and robustness due to environmental complexities. Furthermore, learning policies for multi-robot collaboration are resource-intensive. To address these challenges, we propose Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs) as a global planner for multi-robot cooperative visual target navigation. Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs' scene comprehension. It then assigns exploration frontiers to each robot for efficient target search. Experimental results on Habitat-Matterport 3D (HM3D) demonstrate that Co-NavGPT surpasses existing models in success rates and efficiency without any learning process, demonstrating the vast potential of LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#24335;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#23616;&#37096;&#24378;&#31574;&#30053;&#26469;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#65292;&#20174;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#20013;&#24212;&#29992;&#20110;&#22797;&#26434;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.01569</link><description>&lt;p&gt;
&#36845;&#20195;&#24335;&#35268;&#21010;&#20013;&#30340;&#36873;&#39033;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Iterative Option Discovery for Planning, by Planning. (arXiv:2310.01569v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#24335;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#23616;&#37096;&#24378;&#31574;&#30053;&#26469;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#65292;&#20174;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#20013;&#24212;&#29992;&#20110;&#22797;&#26434;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#26377;&#29992;&#30340;&#26102;&#38388;&#25277;&#35937;&#65292;&#20063;&#23601;&#26159;&#36873;&#39033;&#65292;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#24212;&#29992;&#20110;&#26085;&#30410;&#22797;&#26434;&#30340;&#39046;&#22495;&#30340;&#20851;&#38190;&#12290;&#22312;AlphaZero&#20013;&#20351;&#29992;&#30340;Expert Iteration&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#30340;&#32463;&#39564;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;Option Iteration&#65292;&#19968;&#31181;&#31867;&#20284;&#30340;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#12290;Option Iteration&#19981;&#26159;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#30340;&#24378;&#31574;&#30053;&#65292;&#32780;&#26159;&#23398;&#20064;&#19968;&#32452;&#36873;&#39033;&#31574;&#30053;&#65292;&#23545;&#20110;&#36935;&#21040;&#30340;&#27599;&#20010;&#29366;&#24577;&#65292;&#33267;&#23569;&#26377;&#19968;&#31181;&#31574;&#30053;&#22312;&#26576;&#20010;&#26410;&#26469;&#30340;&#26102;&#38388;&#28857;&#19982;&#25628;&#32034;&#32467;&#26524;&#21563;&#21512;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#36825;&#21487;&#33021;&#26356;&#23481;&#26131;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#31639;&#27861;&#26681;&#25454;&#24773;&#20917;&#28789;&#27963;&#35843;&#25972;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#19968;&#20010;&#22312;&#24403;&#21069;&#29366;&#24577;&#30340;&#32454;&#33410;&#19978;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#24615;&#30340;&#20840;&#23616;&#31574;&#30053;&#12290;&#36890;&#36807;&#23398;&#20064;&#36825;&#26679;&#19968;&#32452;&#23616;&#37096;&#24378;&#31574;&#30053;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#26469;&#25351;&#23548;&#25628;&#32034;&#31639;&#27861;&#65292;&#20174;&#32780;&#24418;&#25104;&#33391;&#24615;&#24490;&#29615;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering useful temporal abstractions, in the form of options, is widely thought to be key to applying reinforcement learning and planning to increasingly complex domains. Building on the empirical success of the Expert Iteration approach to policy learning used in AlphaZero, we propose Option Iteration, an analogous approach to option discovery. Rather than learning a single strong policy that is trained to match the search results everywhere, Option Iteration learns a set of option policies trained such that for each state encountered, at least one policy in the set matches the search results for some horizon into the future. Intuitively, this may be significantly easier as it allows the algorithm to hedge its bets compared to learning a single globally strong policy, which may have complex dependencies on the details of the current state. Having learned such a set of locally strong policies, we can use them to guide the search algorithm resulting in a virtuous cycle where better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#65292;&#36890;&#36807;&#19982;LLMs&#20132;&#20114;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.15074</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#24314;&#27169;&#19982;&#25512;&#29702;&#65306;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial. (arXiv:2309.15074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#65292;&#36890;&#36807;&#19982;LLMs&#20132;&#20114;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;2018&#24180;&#20197;&#26469;&#24613;&#21095;&#22686;&#38271;&#65292;&#33258;&#24341;&#20837;&#19978;&#19979;&#25991;&#24863;&#30693;&#35745;&#31639;&#31995;&#32479;20&#24180;&#21518;&#12290;&#19978;&#19979;&#25991;&#24863;&#30693;&#35745;&#31639;&#36890;&#36807;&#32771;&#34385;&#26222;&#36866;&#35774;&#22791;&#12289;&#29992;&#25143;&#21644;&#31038;&#20250;&#30340;&#24773;&#20917;&#65292;&#23454;&#29616;&#20102;&#24191;&#27867;&#30340;&#21019;&#26032;&#24212;&#29992;&#65292;&#22914;&#36741;&#21161;&#29983;&#27963;&#12289;&#22522;&#20110;&#20301;&#32622;&#30340;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#31561;&#12290;&#20026;&#20102;&#35782;&#21035;&#19978;&#19979;&#25991;&#24182;&#30456;&#24212;&#22320;&#20570;&#20986;&#20915;&#31574;&#65292;&#37319;&#29992;&#20102;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65288;&#22914;&#26412;&#20307;&#35770;&#21644;OWL&#65289;&#20316;&#20026;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#25512;&#29702;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;LLMs&#30340;&#23835;&#36215;&#21644;&#23427;&#20204;&#25913;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#24314;&#27169;&#19978;&#19979;&#25991;&#24182;&#36890;&#36807;&#19982;ChatGPT&#21644;GPT-4&#31561;LLMs&#20132;&#20114;&#36827;&#34892;&#19978;&#19979;&#25991;&#25512;&#29702;&#21464;&#24471;&#21487;&#34892;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#25991;&#26412;&#12289;&#25552;&#31034;&#21644;&#33258;&#20027;&#20195;&#29702;&#65288;AutoAgents&#65289;&#20351;LLMs&#33021;&#22815;&#25191;&#34892;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have become phenomenally surging, since 2018--two decades after introducing context-awareness into computing systems. Through taking into account the situations of ubiquitous devices, users and the societies, context-aware computing has enabled a wide spectrum of innovative applications, such as assisted living, location-based social network services and so on. To recognize contexts and make decisions for actions accordingly, various artificial intelligence technologies, such as Ontology and OWL, have been adopted as representations for context modeling and reasoning. Recently, with the rise of LLMs and their improved natural language understanding and reasoning capabilities, it has become feasible to model contexts using natural language and perform context reasoning by interacting with LLMs such as ChatGPT and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and autonomous agents (AutoAgents) that enable LLMs to perform context modeling 
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.14970</link><description>&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recurrent Hypernetworks are Surprisingly Strong in Meta-RL. (arXiv:2309.14970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14970
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#36229;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#26356;&#20026;&#31616;&#21333;&#20294;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#26102;&#22240;&#26679;&#26412;&#25928;&#29575;&#20302;&#32780;&#19981;&#26131;&#37096;&#32626;&#12290;&#20803;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#22312;&#20803;&#35757;&#32451;&#26102;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#30452;&#25509;&#35299;&#20915;&#20102;&#36825;&#20010;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#19987;&#38376;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#19982;&#19968;&#20010;&#36890;&#29992;&#30340;&#24207;&#21015;&#27169;&#22411;&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26159;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#24378;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35266;&#28857;&#30001;&#20110;&#26377;&#38480;&#30340;&#25903;&#25345;&#35777;&#25454;&#32780;&#24341;&#36215;&#20102;&#20105;&#35758;&#65292;&#29305;&#21035;&#26159;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#30830;&#31435;&#20102;&#23436;&#20840;&#30456;&#21453;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#34429;&#28982;&#25105;&#20204;&#21516;&#26679;&#21457;&#29616;&#24490;&#29615;&#32593;&#32476;&#21487;&#20197;&#36798;&#21040;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#36229;&#32593;&#32476;&#30340;&#20351;&#29992;&#23545;&#20110;&#21457;&#25381;&#24490;&#29615;&#22522;&#32447;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19982;&#36229;&#32593;&#32476;&#30456;&#32467;&#21512;&#26102;&#65292;&#36825;&#31181;&#36828;&#27604;&#29616;&#26377;&#19987;&#38376;&#26041;&#27861;&#31616;&#21333;&#30340;&#24490;&#29615;&#22522;&#20934;&#23454;&#38469;&#19978;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) is notoriously impractical to deploy due to sample inefficiency. Meta-RL directly addresses this sample inefficiency by learning to perform few-shot learning when a distribution of related tasks is available for meta-training. While many specialized meta-RL methods have been proposed, recent work suggests that end-to-end learning in conjunction with an off-the-shelf sequential model, such as a recurrent network, is a surprisingly strong baseline. However, such claims have been controversial due to limited supporting evidence, particularly in the face of prior work establishing precisely the opposite. In this paper, we conduct an empirical investigation. While we likewise find that a recurrent network can achieve strong performance, we demonstrate that the use of hypernetworks is crucial to maximizing their potential. Surprisingly, when combined with hypernetworks, the recurrent baselines that are far simpler than existing specialized methods actually ac
&lt;/p&gt;</description></item><item><title>beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.07867</link><description>&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07867
&lt;/p&gt;
&lt;p&gt;
beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;beta&#25193;&#25955;&#65292;&#19968;&#31181;&#23558;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#38598;&#25104;&#21040;&#19968;&#36215;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#20351;&#29992;&#20102;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#65292;beta&#25193;&#25955;&#21033;&#29992;&#20102;&#38543;&#26102;&#38388;&#30340;&#20056;&#27861;&#36716;&#25442;&#26469;&#21019;&#24314;&#27491;&#21521;&#21644;&#21453;&#21521;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#21516;&#26102;&#32500;&#25345;&#30528;&#27491;&#21521;&#36793;&#32536;&#20998;&#24067;&#21644;&#21453;&#21521;&#26465;&#20214;&#20998;&#24067;&#65292;&#32473;&#23450;&#20219;&#24847;&#26102;&#38388;&#28857;&#30340;&#25968;&#25454;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;&#20256;&#32479;&#27169;&#22411;&#20381;&#36182;&#20110;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#21644;&#37325;&#26032;&#21152;&#26435;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#65292;beta&#25193;&#25955;&#26159;&#20056;&#27861;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#20174;KL&#25955;&#24230;&#30340;&#20984;&#24615;&#25512;&#23548;&#20986;&#26469;&#30340;KL&#25955;&#24230;&#19978;&#30028;&#65288;KLUB&#65289;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;KLUB&#30456;&#23545;&#20110;&#36127;ELBO&#26469;&#35828;&#23545;&#20110;&#20248;&#21270;beta&#25193;&#25955;&#26356;&#21152;&#26377;&#25928;&#65292;&#36127;ELBO&#20063;&#21487;&#20197;&#20316;&#20026;&#30456;&#21516;KL&#25955;&#24230;&#30340;KLUB&#65292;&#21482;&#26159;&#20854;&#20004;&#20010;&#21442;&#25968;&#20132;&#25442;&#20102;&#20301;&#32622;&#12290;beta&#25193;&#25955;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;Bregman&#25955;&#24230;&#20026;&#25351;&#26631;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, furt
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.11730</link><description>&lt;p&gt;
&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#8220;&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#12289;&#39044;&#27979;&#8221;&#33539;&#24335;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;OD-QA&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MD-QA&#65289;&#22330;&#26223;&#19979;&#25506;&#32034;&#36825;&#20010;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#20010;&#35201;&#27714;&#23545;&#19981;&#21516;&#25991;&#26723;&#30340;&#20869;&#23481;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#32852;&#26377;&#28145;&#20837;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#65288;KGP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;MD-QA&#20013;&#20026;LLMs&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#22270;&#26500;&#24314;&#27169;&#22359;&#21644;&#22270;&#36941;&#21382;&#27169;&#22359;&#12290;&#23545;&#20110;&#22270;&#26500;&#24314;&#65292;&#25105;&#20204;&#20351;&#29992;&#33410;&#28857;&#26469;&#34920;&#31034;&#25991;&#27573;&#25110;&#25991;&#26723;&#32467;&#26500;&#65288;&#20363;&#22914;&#65292;&#39029;&#38754;/&#34920;&#26684;&#65289;&#65292;&#32780;&#20351;&#29992;&#36793;&#26469;&#34920;&#31034;&#25991;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;/&#35789;&#27719;&#30456;&#20284;&#24615;&#25110;&#32773;&#25991;&#26723;&#20869;&#30340;&#32467;&#26500;&#20851;&#31995;&#12290;&#23545;&#20110;&#22270;&#36941;&#21382;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#23427;&#22312;&#33410;&#28857;&#20043;&#38388;&#23548;&#33322;&#24182;&#25910;&#38598;&#25903;&#25345;&#24615;&#30340;&#25991;&#27573;&#65292;&#20197;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24110;&#21161;&#29992;&#25143;&#21028;&#26029;&#26631;&#39064;&#20934;&#30830;&#24615;&#21644;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#26041;&#38754;&#24433;&#21709;&#19981;&#22823;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23427;&#20250;&#35823;&#23548;&#29992;&#25143;&#23545;&#30495;&#23454;&#26631;&#39064;&#30340;&#20449;&#20208;&#65292;&#24182;&#22686;&#21152;&#23545;&#26410;&#30830;&#23450;&#34394;&#20551;&#26631;&#39064;&#30340;&#20449;&#20208;&#12290;</title><link>http://arxiv.org/abs/2308.10800</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#26080;&#25928;&#19988;&#20855;&#26377;&#28508;&#22312;&#21361;&#23475;&#24615;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is ineffective and potentially harmful for fact checking. (arXiv:2308.10800v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10800
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24110;&#21161;&#29992;&#25143;&#21028;&#26029;&#26631;&#39064;&#20934;&#30830;&#24615;&#21644;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#26041;&#38754;&#24433;&#21709;&#19981;&#22823;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23427;&#20250;&#35823;&#23548;&#29992;&#25143;&#23545;&#30495;&#23454;&#26631;&#39064;&#30340;&#20449;&#20208;&#65292;&#24182;&#22686;&#21152;&#23545;&#26410;&#30830;&#23450;&#34394;&#20551;&#26631;&#39064;&#30340;&#20449;&#20208;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#26680;&#26597;&#26159;&#23545;&#25239;&#38169;&#35823;&#20449;&#24687;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#20294;&#26159;&#23427;&#22312;&#35268;&#27169;&#19978;&#30340;&#23454;&#26045;&#21463;&#21040;&#20102;&#32593;&#32476;&#19978;&#20449;&#24687;&#36807;&#20110;&#24222;&#22823;&#30340;&#38459;&#30861;&#12290;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#20204;&#22312;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#30340;&#20107;&#23454;&#26680;&#26597;&#20449;&#24687;&#26102;&#30340;&#20316;&#29992;&#26426;&#21046;&#24182;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#39044;&#20808;&#30331;&#35760;&#30340;&#38543;&#26426;&#23545;&#29031;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19968;&#27454;&#28909;&#38376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#20107;&#23454;&#26680;&#26597;&#23545;&#25919;&#27835;&#26032;&#38395;&#20449;&#20208;&#21644;&#20998;&#20139;&#24847;&#22270;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#35813;&#20154;&#24037;&#26234;&#33021;&#22312;&#25581;&#31359;&#34394;&#20551;&#26631;&#39064;&#26041;&#38754;&#34920;&#29616;&#24471;&#30456;&#24403;&#19981;&#38169;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#24182;&#27809;&#26377;&#23545;&#21442;&#19982;&#32773;&#35782;&#21035;&#26631;&#39064;&#20934;&#30830;&#24615;&#25110;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#30340;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#20107;&#23454;&#26680;&#26597;&#22120;&#20855;&#26377;&#21361;&#23475;&#24615;&#65306;&#23558;&#19968;&#20123;&#30495;&#23454;&#26631;&#39064;&#35823;&#26631;&#20026;&#34394;&#20551;&#20250;&#38477;&#20302;&#23545;&#20854;&#30340;&#20449;&#20208;&#65292;&#32780;&#23545;&#20854;&#26410;&#30830;&#23450;&#30340;&#34394;&#20551;&#26631;&#39064;&#21017;&#20250;&#22686;&#21152;&#23545;&#20854;&#30340;&#20449;&#20208;&#12290;&#22312;&#31215;&#26497;&#26041;&#38754;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#25552;&#39640;&#20102;&#27491;&#30830;&#26631;&#23450;&#26631;&#39064;&#30340;&#20998;&#20139;&#24847;&#24895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online. Recent artificial intelligence (AI) language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear. Here we investigate the impact of fact checks generated by a popular AI model on belief in, and sharing intent of, political news in a preregistered randomized control experiment. Although the AI performs reasonably well in debunking false headlines, we find that it does not significantly affect participants' ability to discern headline accuracy or share accurate news. However, the AI fact-checker is harmful in specific cases: it decreases beliefs in true headlines that it mislabels as false and increases beliefs for false headlines that it is unsure about. On the positive side, the AI increases sharing intents for correctly labeled t
&lt;/p&gt;</description></item><item><title>SwinLSTM&#26159;&#19968;&#31181;&#23558;Swin Transformer&#21644;LSTM&#32467;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26032;&#24490;&#29615;&#21333;&#20803;&#65292;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2308.09891</link><description>&lt;p&gt;
SwinLSTM&#65306;&#20351;&#29992;Swin Transformer&#21644;LSTM&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
SwinLSTM:Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM. (arXiv:2308.09891v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09891
&lt;/p&gt;
&lt;p&gt;
SwinLSTM&#26159;&#19968;&#31181;&#23558;Swin Transformer&#21644;LSTM&#32467;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26032;&#24490;&#29615;&#21333;&#20803;&#65292;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;CNN&#21644;RNN&#38598;&#25104;&#20197;&#25429;&#25417;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#26159;&#26102;&#31354;&#39044;&#27979;&#20219;&#21153;&#20013;&#24120;&#29992;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;CNN&#23398;&#20064;&#23616;&#37096;&#31354;&#38388;&#20449;&#24687;&#30340;&#23646;&#24615;&#38477;&#20302;&#20102;&#23427;&#20204;&#22312;&#25429;&#25417;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#26041;&#38754;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24490;&#29615;&#21333;&#20803;SwinLSTM&#65292;&#23427;&#23558;Swin Transformer&#22359;&#21644;&#31616;&#21270;&#30340;LSTM&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#23558;ConvLSTM&#20013;&#30340;&#21367;&#31215;&#32467;&#26500;&#26367;&#25442;&#20026;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20197;SwinLSTM&#21333;&#20803;&#20026;&#26680;&#24515;&#30340;&#26102;&#31354;&#39044;&#27979;&#32593;&#32476;&#12290;SwinLSTM&#22312;Moving MNIST&#65292;Human3.6m&#65292;TaxiBJ&#21644;KTH&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#31454;&#20105;&#24615;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#20840;&#23616;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#23545;&#20110;&#27169;&#22411;&#25429;&#25417;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#26356;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating CNNs and RNNs to capture spatiotemporal dependencies is a prevalent strategy for spatiotemporal prediction tasks. However, the property of CNNs to learn local spatial information decreases their efficiency in capturing spatiotemporal dependencies, thereby limiting their prediction accuracy. In this paper, we propose a new recurrent cell, SwinLSTM, which integrates Swin Transformer blocks and the simplified LSTM, an extension that replaces the convolutional structure in ConvLSTM with the self-attention mechanism. Furthermore, we construct a network with SwinLSTM cell as the core for spatiotemporal prediction. Without using unique tricks, SwinLSTM outperforms state-of-the-art methods on Moving MNIST, Human3.6m, TaxiBJ, and KTH datasets. In particular, it exhibits a significant improvement in prediction accuracy compared to ConvLSTM. Our competitive experimental results demonstrate that learning global spatial dependencies is more advantageous for models to capture spatiotempo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#21697;&#25644;&#36816;&#20219;&#21153;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31561;&#21464;Transporter Net&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31561;&#21464;&#31070;&#32463;&#27169;&#22411;&#25429;&#25417;&#20102;&#25152;&#26377;&#23545;&#31216;&#24615;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25644;&#36816;&#20301;&#32622;&#25512;&#24191;&#25644;&#36816;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2308.07948</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#31216;&#24615;&#22312;&#29289;&#21697;&#25644;&#36816;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging Symmetries in Pick and Place. (arXiv:2308.07948v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07948
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#21697;&#25644;&#36816;&#20219;&#21153;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31561;&#21464;Transporter Net&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31561;&#21464;&#31070;&#32463;&#27169;&#22411;&#25429;&#25417;&#20102;&#25152;&#26377;&#23545;&#31216;&#24615;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25644;&#36816;&#20301;&#32622;&#25512;&#24191;&#25644;&#36816;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#30340;&#29289;&#21697;&#25644;&#36816;&#20219;&#21153;&#22312;&#29289;&#21697;&#21644;&#30446;&#26631;&#25918;&#32622;&#20301;&#32622;&#30340;&#24179;&#31227;&#21644;&#26059;&#36716;&#19979;&#20855;&#26377;&#23545;&#31216;&#24615;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#29289;&#21697;&#34987;&#26059;&#36716;&#25110;&#24179;&#31227;&#65292;&#26368;&#20339;&#25644;&#36816;&#21160;&#20316;&#20063;&#24212;&#35813;&#26059;&#36716;&#25110;&#24179;&#31227;&#12290;&#23545;&#20110;&#25918;&#32622;&#20301;&#32622;&#20063;&#26159;&#22914;&#27492;&#65292;&#22914;&#26524;&#30446;&#26631;&#25918;&#32622;&#20301;&#32622;&#21457;&#29983;&#21464;&#21270;&#65292;&#25918;&#32622;&#21160;&#20316;&#20063;&#24212;&#30456;&#24212;&#25913;&#21464;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#25644;&#36816;&#26694;&#26550;Transporter Net&#25429;&#25417;&#20102;&#37096;&#20998;&#36825;&#20123;&#23545;&#31216;&#24615;&#65292;&#20294;&#19981;&#23436;&#20840;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#24179;&#38754;&#26426;&#22120;&#20154;&#29289;&#21697;&#25644;&#36816;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31561;&#21464;&#31070;&#32463;&#27169;&#22411;&#25972;&#21512;&#21040;Transporter Net&#20013;&#20197;&#25429;&#25417;&#25152;&#26377;&#23545;&#31216;&#24615;&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#26032;&#27169;&#22411;&#34987;&#31216;&#20026;&#31561;&#21464;Transporter Net&#65292;&#23545;&#20110;&#29289;&#21697;&#30340;&#25644;&#36816;&#21644;&#25918;&#32622;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#21487;&#20197;&#31435;&#21363;&#23558;&#25644;&#36816;&#30693;&#35782;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#25644;&#36816;&#20301;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#26032;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#26174;&#31034;&#23427;&#27604;&#20256;&#32479;&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic pick and place tasks are symmetric under translations and rotations of both the object to be picked and the desired place pose. For example, if the pick object is rotated or translated, then the optimal pick action should also rotate or translate. The same is true for the place pose; if the desired place pose changes, then the place action should also transform accordingly. A recently proposed pick and place framework known as Transporter Net captures some of these symmetries, but not all. This paper analytically studies the symmetries present in planar robotic pick and place and proposes a method of incorporating equivariant neural models into Transporter Net in a way that captures all symmetries. The new model, which we call Equivariant Transporter Net, is equivariant to both pick and place symmetries and can immediately generalize pick and place knowledge to different pick and place poses. We evaluate the new model empirically and show that it is much more sample efficient t
&lt;/p&gt;</description></item><item><title>VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.06595</link><description>&lt;p&gt;
VisIT-Bench: &#19968;&#20010;&#21463;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#21551;&#21457;&#30340;&#35270;&#35273;&#35821;&#35328;&#25351;&#31034;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06595
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;VisIT-Bench&#65288;Visual InsTruction Benchmark&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20215;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#31034;&#36981;&#24490;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#36215;&#28857;&#26159;&#31574;&#21010;&#20102;70&#20010;&#8220;&#25351;&#31034;&#23478;&#26063;&#8221;&#65292;&#25105;&#20204;&#35748;&#20026;&#25351;&#31034;&#35843;&#20248;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#23478;&#26063;&#12290;&#20219;&#21153;&#19981;&#20165;&#38480;&#20110;VQAv2&#21644;COCO&#31561;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20174;&#22522;&#26412;&#35782;&#21035;&#21040;&#28216;&#25103;&#29609;&#27861;&#21644;&#21019;&#36896;&#24615;&#29983;&#25104;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#22312;&#31574;&#21010;&#20043;&#21518;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;592&#20010;&#27979;&#35797;&#26597;&#35810;&#65292;&#27599;&#20010;&#26597;&#35810;&#37117;&#24102;&#26377;&#19968;&#20010;&#20154;&#24037;&#32534;&#20889;&#30340;&#25351;&#31034;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#12290;&#36825;&#20123;&#25551;&#36848;&#23637;&#29616;&#20102;&#29305;&#23450;&#25351;&#31034;&#22240;&#32032;&#65292;&#20363;&#22914;&#23545;&#20110;&#35810;&#38382;&#24215;&#38754;&#23545;&#20110;&#36718;&#26885;&#29992;&#25143;&#30340;&#26131;&#35775;&#38382;&#24615;&#30340;&#25351;&#31034;&#65292;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#25551;&#36848;&#20102;&#26012;&#22369;/&#28508;&#22312;&#38556;&#30861;&#29289;&#12290;&#36825;&#20123;&#25551;&#36848;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#65306;1&#65289;&#25910;&#38598;&#27599;&#20010;&#23454;&#20363;&#30340;&#20154;&#24037;&#39564;&#35777;&#30340;&#21442;&#32771;&#36755;&#20986;&#65307;2&#65289;&#20351;&#29992;&#20165;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20505;&#36873;&#22810;&#27169;&#24577;&#29983;&#25104;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16082</link><description>&lt;p&gt;
EnrichEvent: &#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#20026;&#26032;&#20986;&#29616;&#30340;&#20107;&#20214;&#25552;&#20379;&#20016;&#23500;&#30340;&#31038;&#20132;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#24179;&#21488;&#24050;&#25104;&#20026;&#20256;&#25773;&#21644;&#35752;&#35770;&#30495;&#23454;&#20107;&#20214;&#20449;&#24687;&#30340;&#20851;&#38190;&#24179;&#21488;&#65292;&#20026;&#21450;&#26089;&#21457;&#29616;&#26377;&#26032;&#38395;&#20215;&#20540;&#30340;&#20107;&#20214;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#20165;&#21033;&#29992;&#20851;&#38190;&#35789;&#31361;&#21457;&#24615;&#25110;&#32593;&#32476;&#32467;&#26500;&#26469;&#26816;&#27979;&#28909;&#28857;&#20107;&#20214;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20107;&#20214;&#21644;&#31038;&#20132;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32780;&#35328;&#65292;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#22312;&#36798;&#21040;&#36235;&#21183;&#29366;&#24577;&#20043;&#21069;&#35782;&#21035;&#20986;&#26032;&#20986;&#29616;&#30340;&#31038;&#20132;&#20107;&#20214;&#12290;&#31038;&#20132;&#25968;&#25454;&#65292;&#20363;&#22914;&#25512;&#25991;&#65292;&#20855;&#26377;&#25340;&#20889;&#38169;&#35823;&#12289;&#19981;&#23436;&#25972;&#24615;&#12289;&#27495;&#20041;&#24615;&#21644;&#35821;&#35328;&#19981;&#35268;&#33539;&#24615;&#65292;&#20197;&#21450;&#24847;&#35265;&#26041;&#38754;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#26469;&#23398;&#20064;&#20107;&#20214;&#30340;&#28436;&#21464;&#29305;&#24449;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20960;&#20046;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#27969;&#24335;&#31038;&#20132;&#25968;&#25454;&#30340;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social platforms have emerged as a crucial platform for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. However, most existing approaches for event detection solely exploit keyword burstiness or network structures to detect hot events. Thus, they often fail to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, ambiguity, and irregular language, as well as variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, in this paper, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data. In particular, we leverage contextual knowledge to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.12267</link><description>&lt;p&gt;
&#38754;&#21521;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#35770;&#25991;&#30340;&#33258;&#21160;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#33021;&#22815;&#22312;&#25552;&#20379;&#20855;&#20307;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#27969;&#30021;&#22238;&#31572;&#12290;&#23613;&#31649;&#25215;&#35748;&#25216;&#26415;&#36827;&#27493;&#24102;&#26469;&#30340;&#20415;&#21033;&#65292;&#25945;&#32946;&#32773;&#20063;&#25285;&#24515;&#23398;&#29983;&#21487;&#33021;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#24182;&#23558;&#20854;&#20551;&#20882;&#20026;&#33258;&#24049;&#30340;&#21407;&#21019;&#20316;&#21697;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;AI&#20869;&#23481;&#26816;&#27979;&#30740;&#31350;&#26159;&#22522;&#20110;&#36825;&#20123;&#25285;&#24551;&#36827;&#34892;&#30340;&#65292;&#20294;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;AI&#20869;&#23481;&#26816;&#27979;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#20551;&#35774;&#19968;&#20010;&#25991;&#26412;&#35201;&#20040;&#23436;&#20840;&#30001;&#20154;&#31867;&#32534;&#20889;&#65292;&#35201;&#20040;&#23436;&#20840;&#30001;AI&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;AI&#20869;&#23481;&#26816;&#27979;&#22312;&#19968;&#20010;&#23569;&#26377;&#25506;&#32034;&#20294;&#21364;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#26816;&#27979;&#30340;&#25991;&#26412;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#28151;&#21512;&#25991;&#26412;&#65289;&#21327;&#20316;&#32534;&#20889;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#26816;&#27979;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#20174;&#32473;&#23450;&#30340;&#28151;&#21512;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#31867;&#32534;&#20889;&#20869;&#23481;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#36716;&#25442;&#28857;&#65288;&#36793;&#30028;&#26816;&#27979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary det
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#65292;&#21457;&#29616;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.05722</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations. (arXiv:2307.05722v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#65292;&#21457;&#29616;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#34892;&#20026;&#22270;&#30340;&#29702;&#35299;&#28508;&#21147;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34892;&#20026;&#22270;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#29702;&#35299;&#26469;&#25552;&#21319;&#22312;&#32447;&#25307;&#32856;&#20013;&#30340;&#25512;&#33616;&#65292;&#21253;&#25324;&#20419;&#36827;&#38750;&#20998;&#24067;&#24335;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#35821;&#20041;&#34920;&#31034;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#24182;&#25581;&#31034;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#36335;&#24452;&#25552;&#31034;&#26500;&#36896;&#22120;&#65292;&#21033;&#29992;LLM&#25512;&#33616;&#22120;&#39318;&#27425;&#29702;&#35299;&#34892;&#20026;&#22270;&#65292;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#36335;&#24452;&#22686;&#24378;&#27169;&#22359;&#26469;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#24207;&#21015;&#36755;&#20837;&#24341;&#20837;&#30340;&#25552;&#31034;&#20559;&#24046;&#12290;&#36890;&#36807;&#21033;&#29992;&#23558;LM&#30340;&#29305;&#28857;&#24341;&#20837;&#21040;&#34892;&#20026;&#22270;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing tasks, demonstrating their exceptional capabilities in various domains. However, their potential for behavior graph understanding in job recommendations remains largely unexplored. This paper focuses on unveiling the capability of large language models in understanding behavior graphs and leveraging this understanding to enhance recommendations in online recruitment, including the promotion of out-of-distribution (OOD) application. We present a novel framework that harnesses the rich contextual information and semantic representations provided by large language models to analyze behavior graphs and uncover underlying patterns and relationships. Specifically, we propose a meta-path prompt constructor that leverages LLM recommender to understand behavior graphs for the first time and design a corresponding path augmentation module to alleviate the prompt bias introduced by path-based sequence input. By leveragin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05385</link><description>&lt;p&gt;
&#23398;&#20064;&#26680;&#25216;&#26415;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation. (arXiv:2307.05385v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#25239;(PPG)&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#26041;&#27861;&#26469;&#25345;&#32493;&#30417;&#27979;&#21508;&#31181;&#24515;&#34880;&#31649;&#21442;&#25968;&#12290;PPG&#20449;&#21495;&#30001;&#21487;&#31359;&#25140;&#35774;&#22791;&#20135;&#29983;&#65292;&#24120;&#24120;&#21253;&#21547;&#30001;&#22806;&#37096;&#22240;&#32032;(&#22914;&#20154;&#20307;&#36816;&#21160;)&#24341;&#36215;&#30340;&#22823;&#22411;&#20266;&#24433;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#29983;&#29702;&#21442;&#25968;&#36827;&#34892;&#31283;&#20581;&#21644;&#20934;&#30830;&#30340;&#25552;&#21462;&#65292;&#20449;&#21495;&#30340;&#25439;&#22351;&#21306;&#22495;&#38656;&#35201;&#34987;&#27491;&#30830;&#22320;&#35782;&#21035;&#21644;&#22788;&#29702;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20381;&#38752;&#25163;&#24037;&#29305;&#24449;&#26816;&#27979;&#22120;&#25110;&#20449;&#21495;&#24230;&#37327;&#65292;&#32467;&#26524;&#24615;&#33021;&#19981;&#20339;&#65292;&#25110;&#20381;&#38752;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#23494;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#23567;&#32452;&#21487;&#35299;&#37322;&#30340;&#21367;&#31215;&#26680;&#65292;&#20854;&#24615;&#33021;&#19982;&#29616;&#26377;&#25216;&#26415;DNN&#26041;&#27861;&#30456;&#20284;&#65292;&#29978;&#33267;&#26356;&#22909;&#65292;&#32780;&#21442;&#25968;&#25968;&#37327;&#27604;DNN&#26041;&#27861;&#23569;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#39033;&#24037;&#20316;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) provides a low-cost, non-invasive method to continuously monitor various cardiovascular parameters. PPG signals are generated by wearable devices and frequently contain large artifacts caused by external factors, such as motion of the human subject. In order to ensure robust and accurate extraction of physiological parameters, corrupted areas of the signal need to be identified and handled appropriately. Previous methodology relied either on handcrafted feature detectors or signal metrics which yield sub-optimal performance, or relied on machine learning techniques such as deep neural networks (DNN) which lack interpretability and are computationally and memory intensive. In this work, we present a novel method to learn a small set of interpretable convolutional kernels that has performance similar to -- and often better than -- the state-of-the-art DNN approach with several orders of magnitude fewer parameters. This work allows for efficient, robust, and int
&lt;/p&gt;</description></item><item><title>TransformerG2G&#26159;&#19968;&#31181;&#20351;&#29992;Transformer&#36827;&#34892;&#33258;&#36866;&#24212;&#26102;&#38388;&#27493;&#38271;&#30340;&#22270;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#19978;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20934;&#30830;&#22320;&#25429;&#25417;&#26102;&#24577;&#22270;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.02588</link><description>&lt;p&gt;
TransformerG2G&#65306;&#20351;&#29992;Transformer&#36827;&#34892;&#33258;&#36866;&#24212;&#26102;&#38388;&#27493;&#38271;&#30340;&#23398;&#20064;&#26102;&#24577;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers. (arXiv:2307.02588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02588
&lt;/p&gt;
&lt;p&gt;
TransformerG2G&#26159;&#19968;&#31181;&#20351;&#29992;Transformer&#36827;&#34892;&#33258;&#36866;&#24212;&#26102;&#38388;&#27493;&#38271;&#30340;&#22270;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#19978;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20934;&#30830;&#22320;&#25429;&#25417;&#26102;&#24577;&#22270;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#23884;&#20837;&#24050;&#25104;&#20026;&#22788;&#29702;&#19981;&#21516;&#26102;&#38388;&#22270;&#20998;&#26512;&#20219;&#21153;&#65288;&#22914;&#38142;&#36335;&#39044;&#27979;&#12289;&#33410;&#28857;&#20998;&#31867;&#12289;&#25512;&#33616;&#31995;&#32479;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#22270;&#29983;&#25104;&#65289;&#30340;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#12290;&#36825;&#20123;&#26102;&#24577;&#22270;&#23637;&#29616;&#20102;&#24322;&#36136;&#30340;&#30636;&#26102;&#21160;&#24577;&#12289;&#19981;&#21516;&#30340;&#26102;&#38388;&#38388;&#38548;&#20197;&#21450;&#22312;&#28436;&#21270;&#36807;&#31243;&#20013;&#39640;&#24230;&#21464;&#21270;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#23558;&#21382;&#21490;&#22270;&#19978;&#30340;&#38271;&#31243;&#20381;&#36182;&#34701;&#20837;&#21040;&#23398;&#20064;&#26102;&#24577;&#21160;&#24577;&#30340;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#22270;&#23884;&#20837;&#27169;&#22411;TransformerG2G&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;Transformer&#32534;&#30721;&#22120;&#20174;&#24403;&#21069;&#29366;&#24577;&#65288;$t$&#65289;&#21644;&#20043;&#21069;&#30340;&#19978;&#19979;&#25991;&#65288;&#26102;&#38388;&#25139;[$t-1, t-l$]&#65292;$l$&#26159;&#19978;&#19979;&#25991;&#30340;&#38271;&#24230;&#65289;&#20013;&#39318;&#20808;&#23398;&#20064;&#20013;&#38388;&#33410;&#28857;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20004;&#20010;&#25237;&#24433;&#23618;&#26469;&#29983;&#25104;&#27599;&#20010;&#33410;&#28857;&#30340;&#20302;&#32500;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#65292;&#20316;&#20026;&#20854;&#28508;&#22312;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph embedding has emerged as a very effective technique for addressing diverse temporal graph analytic tasks (i.e., link prediction, node classification, recommender systems, anomaly detection, and graph generation) in various applications. Such temporal graphs exhibit heterogeneous transient dynamics, varying time intervals, and highly evolving node features throughout their evolution. Hence, incorporating long-range dependencies from the historical graph context plays a crucial role in accurately learning their temporal dynamics. In this paper, we develop a graph embedding model with uncertainty quantification, TransformerG2G, by exploiting the advanced transformer encoder to first learn intermediate node representations from its current state ($t$) and previous context (over timestamps [$t-1, t-l$], $l$ is the length of context). Moreover, we employ two projection layers to generate lower-dimensional multivariate Gaussian distributions as each node's latent embedding at ti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#21644; Sobolev &#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#32771;&#34385;&#20013;&#24515;&#38544;&#31169;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#30452;&#26041;&#22270;&#20272;&#35745;&#22120;&#22312; L2 &#39118;&#38505;&#19979;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#20998;&#24067;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#22312;&#27491;&#24120;&#24046;&#20998;&#38544;&#31169;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#65307;&#21516;&#26102;&#21457;&#29616;&#65292;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#26045;&#21152;&#38544;&#31169;&#20250;&#38477;&#20302;&#23545;&#20110; Sobolev &#23494;&#24230;&#30340;&#27491;&#21017;&#26497;&#23567;&#39118;&#38505;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#21457;&#29616;&#22312;&#32431;&#25237;&#24433;&#20272;&#35745;&#35774;&#23450;&#19979;&#65292;&#25152;&#35859;&#30340;&#25237;&#24433;&#20272;&#35745;&#22120;&#23545;&#20110;&#30456;&#21516;&#31867;&#23494;&#24230;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.14535</link><description>&lt;p&gt;
&#20851;&#20110;&#20013;&#24515;&#38544;&#31169;&#22312;&#23494;&#24230;&#20272;&#35745;&#20013;&#30340;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
About the Cost of Central Privacy in Density Estimation. (arXiv:2306.14535v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#21644; Sobolev &#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#32771;&#34385;&#20013;&#24515;&#38544;&#31169;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#30452;&#26041;&#22270;&#20272;&#35745;&#22120;&#22312; L2 &#39118;&#38505;&#19979;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#20998;&#24067;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#22312;&#27491;&#24120;&#24046;&#20998;&#38544;&#31169;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#65307;&#21516;&#26102;&#21457;&#29616;&#65292;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#26045;&#21152;&#38544;&#31169;&#20250;&#38477;&#20302;&#23545;&#20110; Sobolev &#23494;&#24230;&#30340;&#27491;&#21017;&#26497;&#23567;&#39118;&#38505;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#21457;&#29616;&#22312;&#32431;&#25237;&#24433;&#20272;&#35745;&#35774;&#23450;&#19979;&#65292;&#25152;&#35859;&#30340;&#25237;&#24433;&#20272;&#35745;&#22120;&#23545;&#20110;&#30456;&#21516;&#31867;&#23494;&#24230;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#21033;&#26222;&#24076;&#33576;&#21644; Sobolev &#31354;&#38388;&#20013;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#65292;&#22312;&#20013;&#24515;&#38544;&#31169;&#26465;&#20214;&#19979;&#36827;&#34892;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#38544;&#31169;&#39044;&#31639;&#19981;&#26159;&#24120;&#25968;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;&#20013;&#24515;&#24046;&#20998;&#38544;&#31169;&#23450;&#20041;&#65292;&#20197;&#21450;&#36739;&#26032;&#30340;&#20013;&#24515;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#27010;&#24565;&#12290;&#25105;&#20204;&#35777;&#23454;&#20102; Barber &amp; Duchi (2014) &#30340;&#32467;&#26524;&#65292;&#21363;&#30452;&#26041;&#22270;&#20272;&#35745;&#22120;&#22312;&#23545;&#20110; L2 &#39118;&#38505;&#19979;&#23545;&#20110;&#21033;&#26222;&#24076;&#33576;&#20998;&#24067;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#22312;&#27491;&#24120;&#24046;&#20998;&#38544;&#31169;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#65292;&#25105;&#20204;&#23558;&#20854;&#25193;&#23637;&#21040;&#20854;&#20182;&#33539;&#25968;&#21644;&#38544;&#31169;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#26356;&#39640;&#31243;&#24230;&#30340;&#20809;&#28369;&#24615;&#65292;&#24471;&#20986;&#20004;&#20010;&#32467;&#35770;&#65306;&#39318;&#20808;&#65292;&#19982;&#24120;&#25968;&#38544;&#31169;&#39044;&#31639;&#38656;&#35201;&#30340;&#24773;&#20917;&#30456;&#21453;&#65288;Wasserman &amp;amp; Zhou, 2010&#65289;&#65292;&#22312; Sobolev &#23494;&#24230;&#19978;&#26045;&#21152;&#38544;&#31169;&#20250;&#38477;&#20302;&#27491;&#21017;&#26497;&#23567;&#39118;&#38505;&#20272;&#35745;&#12290;&#20854;&#27425;&#65292;&#22312;&#36825;&#31181;&#26032;&#30340;&#32431;&#25237;&#24433;&#20272;&#35745;&#35774;&#23450;&#19979;&#65292;&#25152;&#35859;&#30340;&#25237;&#24433;&#20272;&#35745;&#22120;&#23545;&#20110;&#30456;&#21516;&#31867;&#23494;&#24230;&#26159;&#20960;&#20046;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study non-parametric density estimation for densities in Lipschitz and Sobolev spaces, and under central privacy. In particular, we investigate regimes where the privacy budget is not supposed to be constant. We consider the classical definition of central differential privacy, but also the more recent notion of central concentrated differential privacy. We recover the result of Barber \&amp; Duchi (2014) stating that histogram estimators are optimal against Lipschitz distributions for the L2 risk, and under regular differential privacy, and we extend it to other norms and notions of privacy. Then, we investigate higher degrees of smoothness, drawing two conclusions: First, and contrary to what happens with constant privacy budget (Wasserman \&amp; Zhou, 2010), there are regimes where imposing privacy degrades the regular minimax risk of estimation on Sobolev densities. Second, so-called projection estimators are near-optimal against the same classes of densities in this new setup with pure
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05685</link><description>&lt;p&gt;
&#29992;MT-Bench&#21644;Chatbot Arena&#35780;&#20272;&#20197;LLM&#20026;&#22522;&#30784;&#30340;&#32842;&#22825;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05685
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32842;&#22825;&#21161;&#25163;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#32780;&#29616;&#26377;&#30340;&#22522;&#20934;&#26080;&#27861;&#34913;&#37327;&#20154;&#31867;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#22312;&#26356;&#21152;&#24320;&#25918;&#30340;&#38382;&#39064;&#19978;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#30340;&#20351;&#29992;&#21644;&#23616;&#38480;&#24615;&#65292;&#22914;&#20301;&#32622;&#21644;&#20887;&#20313;&#20559;&#35265;&#20197;&#21450;&#26377;&#38480;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#26469;&#36801;&#31227;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#65288;&#19968;&#20010;&#22810;&#36718;&#38382;&#31572;&#38598;&#21644;&#19968;&#20010;&#20247;&#21253;&#31454;&#25216;&#24179;&#21488;&#65289;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#24378;&#22823;LLM&#35780;&#21028;&#21592;&#21487;&#20197;&#24456;&#22909;&#22320;&#21305;&#37197;&#21463;&#25511;&#21644;&#20247;&#21253;&#20154;&#31867;&#20559;&#22909;&#65292;&#36798;&#21040;&#20102;80&#65285;&#20197;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#19982;&#20154;&#31867;&#19968;&#33268;&#24615;&#27700;&#24179;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#36924;&#36817;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#24335;&#65292;&#32780;&#36825;&#20123;&#20559;&#22909;&#26159;&#38750;&#24120;&#26114;&#36149;&#33719;&#21462;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, such as position and verbosity biases and limited reasoning ability, and propose solutions to migrate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;RLCF&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20195;&#30721;&#32534;&#35793;&#22120;&#21453;&#39304;&#36827;&#19968;&#27493;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#20998;&#24067;&#30340;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#25152;&#26377;&#38745;&#24577;&#27491;&#30830;&#24615;&#26816;&#26597;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18341</link><description>&lt;p&gt;
&#20351;&#29992;&#32534;&#35793;&#22120;&#29983;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#35843;&#25972;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tuning Models of Code with Compiler-Generated Reinforcement Learning Feedback. (arXiv:2305.18341v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;RLCF&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20195;&#30721;&#32534;&#35793;&#22120;&#21453;&#39304;&#36827;&#19968;&#27493;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#20998;&#24067;&#30340;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#25152;&#26377;&#38745;&#24577;&#27491;&#30830;&#24615;&#26816;&#26597;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20195;&#30721;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#31243;&#24207;&#21512;&#25104;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#21487;&#33021;&#36829;&#21453;&#22522;&#26412;&#30340;&#35821;&#35328;&#32423;&#21035;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RLCF&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20195;&#30721;&#32534;&#35793;&#22120;&#30340;&#21453;&#39304;&#36827;&#19968;&#27493;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290; RLCF&#23558;LLM&#35270;&#20026;&#36890;&#36807;RL&#20195;&#29702;&#36880;&#27493;&#29983;&#25104;&#20195;&#30721;&#65292;&#24182;&#25509;&#25910;&#20197;&#19979;&#21453;&#39304;&#65306;&#65288;i&#65289;&#32534;&#35793;&#22120;&#27966;&#29983;&#30340;&#21453;&#39304;&#19982;&#25152;&#29983;&#25104;&#30340;&#20195;&#30721;&#26159;&#21542;&#36890;&#36807;&#19968;&#32452;&#27491;&#30830;&#24615;&#26816;&#26597;&#26377;&#20851;; &#65288;ii&#65289;&#19981;&#21516;LLM&#30340;&#21453;&#39304;&#65292;&#19982;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#19968;&#32452;&#21442;&#32771;&#31243;&#24207;&#30456;&#20284;&#12290;&#36825;&#20123;&#21453;&#39304;&#26426;&#21046;&#24110;&#21161;&#25152;&#29983;&#25104;&#30340;&#20195;&#30721;&#22312;&#36890;&#36807;&#25152;&#26377;&#38745;&#24577;&#27491;&#30830;&#24615;&#26816;&#26597;&#30340;&#21516;&#26102;&#20445;&#25345;&#22312;&#30446;&#26631;&#20998;&#24067;&#20013;&#12290;RLCF&#26159;&#27169;&#22411;&#21644;&#35821;&#35328;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#22312;Java&#30340;MBJP&#21644;MathQA&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RLCF&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) pre-trained on code have recently emerged as the dominant approach to program synthesis. However, the code that these models produce can violate basic language-level invariants, leading to lower performance in downstream tasks. We address this issue through an approach, called RLCF, that further trains a pre-trained LLM using feedback from a code compiler. RLCF views the LLM as an RL agent that generates code step by step and receives: (i) compiler-derived feedback on whether the code it generates passes a set of correctness checks; and (ii) feedback from a different LLM on whether the generated code is similar to a set of reference programs in the training corpus. Together, these feedback mechanisms help the generated code remain within the target distribution while passing all static correctness checks. RLCF is model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA tasks for Java. Our experiments show that RLCF significantly raise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;Selfmem&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#33258;&#25105;&#35760;&#24518;&#27744;&#24182;&#37319;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#65292;&#20351;&#26816;&#32034;&#26356;&#21152;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02437</link><description>&lt;p&gt;
&#36816;&#29992;&#33258;&#25105;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory. (arXiv:2305.02437v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;Selfmem&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#33258;&#25105;&#35760;&#24518;&#27744;&#24182;&#37319;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#65292;&#20351;&#26816;&#32034;&#26356;&#21152;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#36739;&#20110;&#20256;&#32479;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#36845;&#20195;&#20154;&#31867;&#32534;&#20889;&#30340;&#21442;&#32771;&#24211;&#65292;&#24182;&#20174;&#20013;&#26816;&#32034;&#20986;&#30456;&#24212;&#30340;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#26356;&#20248;&#36136;&#30340;&#25991;&#26412;&#12290;&#20294;&#24403;&#21069;&#25991;&#29486;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26816;&#32034;&#21040;&#30340;&#35760;&#24518;&#26469;&#33258;&#20110;&#22266;&#23450;&#30340;&#35821;&#26009;&#24211;&#65292;&#20854;&#36136;&#37327;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#21487;&#33021;&#20250;&#38480;&#21046;&#35760;&#24518;&#22686;&#24378;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Selfmem&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36845;&#20195;&#22320;&#37319;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#22120;&#33258;&#36523;&#20197;&#29983;&#25104;&#26080;&#38480;&#21046;&#30340;&#33258;&#25105;&#35760;&#24518;&#27744;&#65292;&#24182;&#20351;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#20026;&#19979;&#19968;&#36718;&#29983;&#25104;&#36873;&#25321;&#19968;&#20010;&#29983;&#25104;&#30340;&#35760;&#24518;&#12290;&#30456;&#32467;&#21512;&#65292;&#36825;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#25552;&#20986;&#20102;&#36816;&#29992;&#33258;&#25105;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation~(we define this as primal problem), previous works mainly focus on how to retrieve better memory. However, one fundamental limitation exists for current literature: the memory is retrieved from a fixed corpus and is bounded by the quality of the corpus. Due to the finite retrieval space, bounded memory would greatly limit the potential of the memory-augmented generation model. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a framework called Selfmem, which iteratively adopts a retrieval-augmented generator itself to generate an unbounded memory pool and uses a memory selector to pick one generated memory for the next generation round. By combining the primal and dual problem, a retrieval-augmented ge
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#25972;&#25968;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#21516;&#26102;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#25511;&#21046;&#21464;&#37327;&#65292; &#24182;&#22312;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#33021;&#28304;&#31649;&#29702;&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01461</link><description>&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#26368;&#20248;&#25511;&#21046;&#22312;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#33021;&#37327;&#31649;&#29702;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Mixed-Integer Optimal Control via Reinforcement Learning: A Case Study on Hybrid Vehicle Energy Management. (arXiv:2305.01461v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#25972;&#25968;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#21516;&#26102;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#25511;&#21046;&#21464;&#37327;&#65292; &#24182;&#22312;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#33021;&#28304;&#31649;&#29702;&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#38656;&#35201;&#21516;&#26102;&#36755;&#20986;&#36830;&#32493;&#21644;&#31163;&#25955;&#25511;&#21046;&#21464;&#37327;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#34987;&#21046;&#23450;&#20026;&#28151;&#21512;&#25972;&#25968;&#26368;&#20248;&#25511;&#21046;(MIOC)&#38382;&#39064;&#65292;&#30001;&#20110;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#30340;&#22797;&#26434;&#24615;&#32780;&#38590;&#20197;&#35299;&#20915;&#12290;&#25968;&#20540;&#26041;&#27861;&#22914;&#20998;&#25903;&#23450;&#30028;&#27861;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#19981;&#36866;&#21512;&#23454;&#26102;&#25511;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#31163;&#25955;&#24378;&#21270;&#23398;&#20064;(CDRL)&#31639;&#27861;&#65292;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#28436;&#21592;- Q(TD3AQ)&#65292;&#29992;&#20110;MIOC&#38382;&#39064;&#12290;TD3AQ&#32467;&#21512;&#20102;&#28436;&#21592;-&#25209;&#35780;&#23478;&#21644;Q&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#21487;&#20197;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#35813;&#31639;&#27861;&#22312;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;(HEV)&#33021;&#37327;&#31649;&#29702;&#38382;&#39064;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#20854;&#20013;&#36830;&#32493;&#21464;&#37327;&#21457;&#21160;&#26426;&#36716;&#30697;&#21644;&#31163;&#25955;&#21464;&#37327;&#40831;&#36718;&#27604;&#30340;&#23454;&#26102;&#25511;&#21046;&#23545;&#20110;&#26368;&#22823;&#21270;&#29123;&#27833;&#32463;&#27982;&#24615;&#24182;&#28385;&#36275;&#39550;&#39542;&#32422;&#26463;&#26465;&#20214;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#21516;&#39537;&#21160;&#24490;&#29615;&#30340;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;CDRL&#31639;&#27861;&#22312;&#35299;&#20915;&#36136;&#37327;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25968;&#20540;&#26041;&#27861;&#21644;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many optimal control problems require the simultaneous output of continuous and discrete control variables. Such problems are usually formulated as mixed-integer optimal control (MIOC) problems, which are challenging to solve due to the complexity of the solution space. Numerical methods such as branch-and-bound are computationally expensive and unsuitable for real-time control. This paper proposes a novel continuous-discrete reinforcement learning (CDRL) algorithm, twin delayed deep deterministic actor-Q (TD3AQ), for MIOC problems. TD3AQ combines the advantages of both actor-critic and Q-learning methods, and can handle the continuous and discrete action spaces simultaneously. The proposed algorithm is evaluated on a hybrid electric vehicle (HEV) energy management problem, where real-time control of the continuous variable engine torque and discrete variable gear ratio is essential to maximize fuel economy while satisfying driving constraints. Simulation results on different drive cyc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#26102;&#38388;&#21644;&#21487;&#36798;&#24615;&#32422;&#26463;&#30340;&#38750;&#30830;&#23450;&#24615;&#28151;&#21512;&#31995;&#32479;&#12290;&#31639;&#27861;&#22522;&#20110;&#22312;&#28151;&#21512;&#31354;&#38388;&#20013;&#29983;&#38271;&#65288;&#25628;&#32034;&#65289;&#28216;&#25103;&#26641;&#65292;&#20197;&#21512;&#25104;&#19968;&#31181;&#21453;&#24212;&#65288;&#40065;&#26834;&#65289;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#30446;&#26631;&#24182;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.06876</link><description>&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#21453;&#24212;&#32508;&#21512;&#31639;&#27861;&#24212;&#29992;&#20110;&#38750;&#30830;&#23450;&#24615;&#28151;&#21512;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Sampling-based Reactive Synthesis for Nondeterministic Hybrid Systems. (arXiv:2304.06876v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#26102;&#38388;&#21644;&#21487;&#36798;&#24615;&#32422;&#26463;&#30340;&#38750;&#30830;&#23450;&#24615;&#28151;&#21512;&#31995;&#32479;&#12290;&#31639;&#27861;&#22522;&#20110;&#22312;&#28151;&#21512;&#31354;&#38388;&#20013;&#29983;&#38271;&#65288;&#25628;&#32034;&#65289;&#28216;&#25103;&#26641;&#65292;&#20197;&#21512;&#25104;&#19968;&#31181;&#21453;&#24212;&#65288;&#40065;&#26834;&#65289;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#30446;&#26631;&#24182;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#36830;&#32493;&#21160;&#21147;&#23398;&#21644;&#26102;&#38388;&#21644;&#21487;&#36798;&#24615;&#32422;&#26463;&#30340;&#38750;&#30830;&#23450;&#24615;&#28151;&#21512;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#28151;&#21512;&#31995;&#32479;&#30340;&#28436;&#21270;&#35270;&#20026;&#19968;&#20010;&#21452;&#20154;&#28216;&#25103;&#65292;&#20854;&#20013;&#38750;&#30830;&#23450;&#24615;&#26159;&#19968;&#20010;&#23545;&#25163;&#29609;&#23478;&#65292;&#20854;&#30446;&#26631;&#26159;&#38459;&#27490;&#23454;&#29616;&#26102;&#38388;&#21644;&#21487;&#36798;&#24615;&#30446;&#26631;&#12290;&#26088;&#22312;&#21512;&#25104;&#19968;&#31181;&#33719;&#32988;&#31574;&#30053;&#8212;&#8212;&#19968;&#31181;&#21453;&#24212;&#65288;&#40065;&#26834;&#65289;&#31574;&#30053;&#65292;&#23427;&#20445;&#35777;&#22312;&#23545;&#25163;&#29609;&#23478;&#30340;&#25152;&#26377;&#21487;&#33021;&#31227;&#21160;&#19979;&#28385;&#36275;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#22312;&#28151;&#21512;&#31354;&#38388;&#20013;&#29983;&#38271;&#65288;&#25628;&#32034;&#65289;&#28216;&#25103;&#26641;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#26041;&#27861;&#19982;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#21644;&#25913;&#36827;&#37096;&#20998;&#31574;&#30053;&#30340;&#26032;&#22411;&#20056;&#23458;&#33329;&#26426;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#30340;&#26465;&#20214;&#19979;&#65292;&#31639;&#27861;&#26159;&#27010;&#29575;&#19978;&#23436;&#22791;&#30340;&#65292;&#21363;&#65292;&#22914;&#26524;&#23384;&#22312;&#33719;&#32988;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#20960;&#20046;&#32943;&#23450;&#20250;&#25214;&#21040;&#23427;&#12290;&#26696;&#20363;&#30740;&#31350;&#21644;&#22522;&#20934;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a sampling-based strategy synthesis algorithm for nondeterministic hybrid systems with complex continuous dynamics under temporal and reachability constraints. We view the evolution of the hybrid system as a two-player game, where the nondeterminism is an adversarial player whose objective is to prevent achieving temporal and reachability goals. The aim is to synthesize a winning strategy -- a reactive (robust) strategy that guarantees the satisfaction of the goals under all possible moves of the adversarial player. The approach is based on growing a (search) game-tree in the hybrid space by combining a sampling-based planning method with a novel bandit-based technique to select and improve on partial strategies. We provide conditions under which the algorithm is probabilistically complete, i.e., if a winning strategy exists, the algorithm will almost surely find it. The case studies and benchmark results show that the algorithm is general and consistently outperf
&lt;/p&gt;</description></item><item><title>SPIRES&#26159;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#36890;&#29992;&#26597;&#35810;&#22238;&#31572;&#65292;&#33021;&#22815;&#22635;&#20805;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#32780;&#26080;&#38656;&#26174;&#24335;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.02711</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#25552;&#31034;&#35810;&#38382;&#19982;&#36882;&#24402;&#35821;&#20041;&#25552;&#21462;&#65288;SPIRES&#65289;&#65306;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#22635;&#20805;&#30693;&#35782;&#24211;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning. (arXiv:2304.02711v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02711
&lt;/p&gt;
&lt;p&gt;
SPIRES&#26159;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#36890;&#29992;&#26597;&#35810;&#22238;&#31572;&#65292;&#33021;&#22815;&#22635;&#20805;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#32780;&#26080;&#38656;&#26174;&#24335;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#30693;&#35782;&#24211;&#21644;&#26412;&#20307;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#20381;&#36182;&#20110;&#25163;&#21160;&#31649;&#29702;&#12290;AI / NLP&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#19987;&#19994;&#31574;&#23637;&#20154;&#22635;&#20805;&#36825;&#20123;&#30693;&#35782;&#24211;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#33021;&#22635;&#20805;&#20219;&#24847;&#22797;&#26434;&#30340;&#23884;&#22871;&#30693;&#35782;&#27169;&#24335;&#12290;&#22312;&#36825;&#37324;&#25105;&#20204;&#25552;&#20986;&#20102;Structured Prompt Interrogation and Recursive Extraction of Semantics&#65288;SPIRES&#65289;&#65292;&#19968;&#31181;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25191;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;ZSL&#65289;&#21644;&#36890;&#29992;&#26597;&#35810;&#22238;&#31572;&#65292;&#20197;&#21450;&#20174;&#28789;&#27963;&#25552;&#31034;&#36820;&#22238;&#31526;&#21512;&#25351;&#23450;&#27169;&#24335;&#30340;&#20449;&#24687;&#12290; SPIRES&#38024;&#23545;&#32473;&#23450;&#30340;&#35814;&#32454;&#29992;&#25143;&#23450;&#20041;&#30340;&#30693;&#35782;&#27169;&#24335;&#21644;&#36755;&#20837;&#25991;&#26412;&#65292;&#23545;GPT-3+&#25191;&#34892;&#36882;&#24402;&#25552;&#31034;&#35810;&#38382;&#65292;&#20197;&#33719;&#24471;&#19982;&#25552;&#20379;&#30340;&#27169;&#24335;&#21305;&#37197;&#30340;&#19968;&#32452;&#21709;&#24212;&#12290; SPIRES&#20351;&#29992;&#29616;&#26377;&#30340;&#26412;&#20307;&#21644;&#35789;&#27719;&#34920;&#20026;&#25152;&#26377;&#21305;&#37197;&#20803;&#32032;&#25552;&#20379;&#26631;&#35782;&#31526;&#12290; &#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#65288;&#21253;&#25324;&#38899;&#20048;&#65292;&#20307;&#32946;&#21644;&#25919;&#27835;&#65289;&#20013;&#20351;&#29992;SPIRES&#30340;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#20854;&#33021;&#22815;&#22635;&#20805;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#32780;&#26080;&#38656;&#26174;&#24335;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating knowledge bases and ontologies is a time consuming task that relies on a manual curation. AI/NLP approaches can assist expert curators in populating these knowledge bases, but current approaches rely on extensive training data, and are not able to populate arbitrary complex nested knowledge schemas.  Here we present Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES), a Knowledge Extraction approach that relies on the ability of Large Language Models (LLMs) to perform zero-shot learning (ZSL) and general-purpose query answering from flexible prompts and return information conforming to a specified schema. Given a detailed, user-defined knowledge schema and an input text, SPIRES recursively performs prompt interrogation against GPT-3+ to obtain a set of responses matching the provided schema. SPIRES uses existing ontologies and vocabularies to provide identifiers for all matched elements.  We present examples of use of SPIRES in different domains, inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.01046</link><description>&lt;p&gt;
&#8220;Polytuplet Loss: &#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#8221;
&lt;/p&gt;
&lt;p&gt;
Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#23398;&#26657;&#25945;&#32946;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#20204;&#23558;&#21463;&#21040;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32771;&#39564;&#12290;&#23398;&#29983;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#23436;&#25104;&#27492;&#31867;&#32771;&#35797;&#65292;&#20854;&#20013;&#26377;&#20123;&#34987;&#35748;&#20026;&#26159;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#30340;&#12290;&#36825;&#26679;&#19968;&#31181;&#31574;&#30053;&#28041;&#21450;&#24378;&#35843;&#30456;&#23545;&#20934;&#30830;&#24615;&#32780;&#38750;&#32477;&#23545;&#20934;&#30830;&#24615;&#65292;&#29702;&#35770;&#19978;&#21487;&#20197;&#22312;&#19981;&#23436;&#20840;&#25484;&#25569;&#35299;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#35757;&#32451;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20197;&#35299;&#20915;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;ReClor&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36923;&#36753;&#25512;&#29702;&#25216;&#33021;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Polytuplet Loss&#20989;&#25968;&#65292;&#26159;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#25193;&#23637;&#65292;&#20197;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#32780;&#38750;&#23398;&#20064;&#32477;&#23545;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CRN&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#30456;&#26426;&#38647;&#36798;&#34701;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#22270;&#29305;&#24449;&#36716;&#25442;&#20026;&#40479;&#30640;&#29305;&#24449;&#22270;&#21644;&#20351;&#29992;&#22810;&#27169;&#24577;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#12289;&#31283;&#20581;&#12289;&#39640;&#25928;&#30340;3D&#24863;&#30693;&#20219;&#21153;</title><link>http://arxiv.org/abs/2304.00670</link><description>&lt;p&gt;
CRN&#65306;&#29992;&#20110;&#20934;&#30830;&#12289;&#31283;&#20581;&#12289;&#39640;&#25928;&#30340;3D&#24863;&#30693;&#30340;&#30456;&#26426;&#38647;&#36798;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception. (arXiv:2304.00670v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CRN&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#30456;&#26426;&#38647;&#36798;&#34701;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#22270;&#29305;&#24449;&#36716;&#25442;&#20026;&#40479;&#30640;&#29305;&#24449;&#22270;&#21644;&#20351;&#29992;&#22810;&#27169;&#24577;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#12289;&#31283;&#20581;&#12289;&#39640;&#25928;&#30340;3D&#24863;&#30693;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#38656;&#35201;&#19968;&#20010;&#20934;&#30830;&#24555;&#36895;&#30340;3D&#24863;&#30693;&#31995;&#32479;&#65292;&#21253;&#25324;3D&#29289;&#20307;&#26816;&#27979;&#12289;&#36319;&#36394;&#21644;&#20998;&#21106;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#20302;&#25104;&#26412;&#22522;&#20110;&#30456;&#26426;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#31967;&#31957;&#30340;&#20809;&#29031;&#25110;&#24694;&#21155;&#30340;&#22825;&#27668;&#26465;&#20214;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#22823;&#30340;&#23450;&#20301;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#23558;&#30456;&#26426;&#19982;&#20302;&#25104;&#26412;&#38647;&#36798;&#30456;&#32467;&#21512;&#65292;&#21518;&#32773;&#21487;&#20197;&#22312;&#25152;&#26377;&#29615;&#22659;&#20013;&#25552;&#20379;&#31934;&#30830;&#30340;&#36828;&#31243;&#27979;&#37327;&#24182;&#21487;&#38752;&#36816;&#34892;&#65292;&#26159;&#26377;&#24076;&#26395;&#30340;&#65292;&#20294;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Camera Radar Net&#65288;CRN&#65289;&#30340;&#26032;&#39062;&#30340;&#30456;&#26426;&#38647;&#36798;&#34701;&#21512;&#26694;&#26550;&#65292;&#20026;&#21508;&#31181;&#20219;&#21153;&#29983;&#25104;&#19968;&#20010;&#35821;&#20041;&#20016;&#23500;&#12289;&#31354;&#38388;&#31934;&#30830;&#30340;&#40479;&#30640;&#29305;&#24449;&#22270;&#65288;BEV&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#22270;&#20687;&#20013;&#32570;&#20047;&#31354;&#38388;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#31232;&#30095;&#20294;&#20934;&#30830;&#30340;&#38647;&#36798;&#28857;&#23558;&#36879;&#35270;&#35270;&#22270;&#22270;&#20687;&#29305;&#24449;&#36716;&#25442;&#20026;BEV&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#22810;&#27169;&#24577;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#22312;BEV&#20013;&#32858;&#21512;&#22270;&#20687;&#21644;&#38647;&#36798;&#29305;&#24449;&#22270;&#65292;&#20197;&#35299;&#20915;&#31354;&#38388;&#23545;&#40784;&#38169;&#35823;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving requires an accurate and fast 3D perception system that includes 3D object detection, tracking, and segmentation. Although recent low-cost camera-based approaches have shown promising results, they are susceptible to poor illumination or bad weather conditions and have a large localization error. Hence, fusing camera with low-cost radar, which provides precise long-range measurement and operates reliably in all environments, is promising but has not yet been thoroughly investigated. In this paper, we propose Camera Radar Net (CRN), a novel camera-radar fusion framework that generates a semantically rich and spatially accurate bird's-eye-view (BEV) feature map for various tasks. To overcome the lack of spatial information in an image, we transform perspective view image features to BEV with the help of sparse but accurate radar points. We further aggregate image and radar feature maps in BEV using multi-modal deformable attention designed to tackle the spatial misalig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;EPAC&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26102;&#27169;&#22411;&#30340;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36817;&#20284;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.14496</link><description>&lt;p&gt;
&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Explanation Constraints. (arXiv:2303.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;EPAC&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26102;&#27169;&#22411;&#30340;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36817;&#20284;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#30417;&#30563;&#23398;&#20064;&#20551;&#35774;&#23384;&#22312;&#26631;&#27880;&#25968;&#25454;&#65292;&#20294;&#25105;&#20204;&#21487;&#33021;&#26377;&#20851;&#20110;&#27169;&#22411;&#24212;&#22914;&#20309;&#36816;&#34892;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#26412;&#25991;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#20174;&#35299;&#37322;&#32422;&#26463;&#20013;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23398;&#20064;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#35299;&#37322;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#26412;&#25991;&#30340;&#31532;&#19968;&#39033;&#20851;&#38190;&#36129;&#29486;&#26159;&#36890;&#36807;&#23450;&#20041;&#25105;&#20204;&#31216;&#20043;&#20026;EPAC&#27169;&#22411;&#65288;&#22312;&#26032;&#25968;&#25454;&#26399;&#26395;&#20013;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#30340;&#27169;&#22411;&#65289;&#26469;&#22238;&#31572;&#21738;&#20123;&#27169;&#22411;&#20250;&#21463;&#30410;&#20110;&#35299;&#37322;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;&#23398;&#20064;&#29702;&#35770;&#24037;&#20855;&#20998;&#26512;&#20102;&#36825;&#31867;&#27169;&#22411;&#12290;&#31532;&#20108;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#23545;&#20110;&#30001;&#32447;&#24615;&#27169;&#22411;&#21644;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#20449;&#24687;&#32473;&#20986;&#30340;&#35268;&#33539;&#35299;&#37322;&#30340;&#38480;&#21046;&#65288;&#20197;&#20854;Rademacher&#22797;&#26434;&#24230;&#20026;&#34913;&#37327;&#26631;&#20934;&#65289;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#21464;&#20998;&#36817;&#20284;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
While supervised learning assumes the presence of labeled data, we may have prior information about how models should behave. In this paper, we formalize this notion as learning from explanation constraints and provide a learning theoretic framework to analyze how such explanations can improve the learning of our models. For what models would explanations be helpful? Our first key contribution addresses this question via the definition of what we call EPAC models (models that satisfy these constraints in expectation over new data), and we analyze this class of models using standard learning theoretic tools. Our second key contribution is to characterize these restrictions (in terms of their Rademacher complexities) for a canonical class of explanations given by gradient information for linear models and two layer neural networks. Finally, we provide an algorithmic solution for our framework, via a variational approximation that achieves better performance and satisfies these constraint
&lt;/p&gt;</description></item><item><title>DualStreamFoveaNet&#26159;&#19968;&#31181;&#20855;&#26377;&#35299;&#21078;&#24847;&#35782;&#30340;&#21452;&#27969;&#34701;&#21512;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#32593;&#33180;&#21644;&#34880;&#31649;&#20998;&#24067;&#36827;&#34892;&#22810;&#32447;&#32034;&#34701;&#21512;&#65292;&#23454;&#29616;&#23545;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26550;&#26500;&#22312;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06961</link><description>&lt;p&gt;
DualStreamFoveaNet: &#19968;&#31181;&#20855;&#26377;&#35299;&#21078;&#24847;&#35782;&#30340;&#21452;&#27969;&#34701;&#21512;&#26550;&#26500;&#29992;&#20110;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization. (arXiv:2302.06961v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06961
&lt;/p&gt;
&lt;p&gt;
DualStreamFoveaNet&#26159;&#19968;&#31181;&#20855;&#26377;&#35299;&#21078;&#24847;&#35782;&#30340;&#21452;&#27969;&#34701;&#21512;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#32593;&#33180;&#21644;&#34880;&#31649;&#20998;&#24067;&#36827;&#34892;&#22810;&#32447;&#32034;&#34701;&#21512;&#65292;&#23454;&#29616;&#23545;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26550;&#26500;&#22312;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#23545;&#20110;&#20998;&#26512;&#35270;&#32593;&#33180;&#30142;&#30149;&#20197;&#39044;&#38450;&#19981;&#21487;&#36870;&#35270;&#21147;&#25439;&#22833;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#34429;&#28982;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20294;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#20013;&#22830;&#20985;&#28857;&#21608;&#22260;&#23616;&#37096;&#35299;&#21078;&#26631;&#35760;&#30340;&#32570;&#22833;&#12289;&#19981;&#33021;&#40065;&#26834;&#22320;&#22788;&#29702;&#30149;&#21464;&#35270;&#32593;&#33180;&#22270;&#20687;&#21644;&#22270;&#20687;&#26465;&#20214;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#31216;&#20026;DualStreamFoveaNet (DSFN)&#29992;&#20110;&#22810;&#32447;&#32034;&#34701;&#21512;&#12290;&#35813;&#26550;&#26500;&#26126;&#30830;&#22320;&#21033;&#29992;&#35270;&#32593;&#33180;&#21644;&#34880;&#31649;&#20998;&#24067;&#26469;&#23454;&#29616;&#38271;&#31243;&#36830;&#25509;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#34701;&#21512;&#65292;&#23454;&#29616;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#12290;&#25105;&#20204;&#22312;&#21452;&#27969;&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#34701;&#21512;&#33258;&#23398;&#20064;&#30340;&#35299;&#21078;&#20449;&#24687;&#65292;&#26356;&#27880;&#37325;&#20998;&#24067;&#22312;&#34880;&#31649;&#27839;&#32447;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#20196;&#29260;&#25968;&#37327;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate fovea localization is essential for analyzing retinal diseases to prevent irreversible vision loss. While current deep learning-based methods outperform traditional ones, they still face challenges such as the lack of local anatomical landmarks around the fovea, the inability to robustly handle diseased retinal images, and the variations in image conditions. In this paper, we propose a novel transformer-based architecture called DualStreamFoveaNet (DSFN) for multi-cue fusion. This architecture explicitly incorporates long-range connections and global features using retina and vessel distributions for robust fovea localization. We introduce a spatial attention mechanism in the dual-stream encoder to extract and fuse self-learned anatomical information, focusing more on features distributed along blood vessels and significantly reducing computational costs by decreasing token numbers. Our extensive experiments show that the proposed architecture achieves state-of-the-art perform
&lt;/p&gt;</description></item><item><title>DynGFN&#26159;&#19968;&#31181;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.04178</link><description>&lt;p&gt;
DynGFN: &#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#36125;&#21494;&#26031;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;GFlowNets&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets. (arXiv:2302.04178v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04178
&lt;/p&gt;
&lt;p&gt;
DynGFN&#26159;&#19968;&#31181;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#29983;&#29289;&#23398;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#25512;&#26029;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65288;GRN&#65289;&#65292;&#35813;&#32593;&#32476;&#25551;&#36848;&#20102;&#25511;&#21046;&#22522;&#22240;&#34920;&#36798;&#21644;&#32454;&#32990;&#21151;&#33021;&#30340;&#22522;&#22240;&#21450;&#20854;&#20135;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;DynGFN&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#29983;&#25104;&#27969;&#32593;&#32476;&#65292;&#20351;&#29992;RNA&#36895;&#24230;&#25968;&#25454;&#25191;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#24182;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the grand challenges of cell biology is inferring the gene regulatory network (GRN) which describes interactions between genes and their products that control gene expression and cellular function. We can treat this as a causal discovery problem but with two non-standard challenges: (1) regulatory networks are inherently cyclic so we should not model a GRN as a directed acyclic graph (DAG), and (2) observations have significant measurement noise, so for typical sample sizes there will always be a large equivalence class of graphs that are likely given the data, and we want methods that capture this uncertainty. Existing methods either focus on challenge (1), identifying cyclic structure from dynamics, or on challenge (2) learning complex Bayesian posteriors over DAGs, but not both. In this paper we leverage the fact that it is possible to estimate the "velocity" of gene expression with RNA velocity techniques to develop an approach that addresses both challenges. Because we have
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#30340;&#25991;&#26412;&#36716;SQL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#19968;&#31995;&#21015;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;SQL&#26597;&#35810;&#12289;&#29702;&#35299;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#21450;&#21306;&#20998;&#26377;&#26080;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.07695</link><description>&lt;p&gt;
EHRSQL&#65306;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#30340;&#23454;&#29992;&#25991;&#26412;&#36716;SQL&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#30340;&#25991;&#26412;&#36716;SQL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#19968;&#31995;&#21015;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;SQL&#26597;&#35810;&#12289;&#29702;&#35299;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#21450;&#21306;&#20998;&#26377;&#26080;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#12290;&#23545;&#35805;&#26159;&#30001;222&#20010;&#21307;&#38498;&#24037;&#20316;&#20154;&#21592;&#21253;&#25324;&#21307;&#29983;&#12289;&#25252;&#22763;&#12289;&#20445;&#38505;&#23457;&#26597;&#21644;&#20581;&#24247;&#26723;&#26696;&#22242;&#38431;&#31561;&#25163;&#26426;&#32780;&#26469;&#12290;&#20026;&#20102;&#26500;&#24314;&#20851;&#20110;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#30340;QA&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;&#19968;&#25152;&#22823;&#23398;&#21307;&#38498;&#36827;&#34892;&#20102;&#19968;&#27425;&#27665;&#35843;&#24182;&#21046;&#20316;&#20102;&#27169;&#26495;&#35805;&#26415;&#20197;&#21019;&#24314;&#31181;&#23376;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25163;&#21160;&#23558;&#23427;&#20204;&#38142;&#25509;&#21040;&#20004;&#20010;&#24320;&#28304;&#30340;EHR&#25968;&#25454;&#24211;&#65288;MIMIC-III&#21644;eICU&#65289;&#20013;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20102;&#26469;&#33258;&#27665;&#24847;&#35843;&#26597;&#30340;&#21508;&#31181;&#26102;&#38388;&#34920;&#36798;&#24335;&#21644;&#26410;&#33021;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#27169;&#22411;&#38656;&#35201; 1&#65289;&#29983;&#25104;&#21453;&#26144;&#21307;&#38498;&#20013;&#21508;&#31181;&#38656;&#27714;&#30340;SQL&#26597;&#35810;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#26816;&#32034;&#21644;&#22797;&#26434;&#30340;&#25805;&#20316;&#65292;&#22914;&#35745;&#31639;&#29983;&#23384;&#29575;&#65292;2&#65289;&#29702;&#35299;&#21508;&#31181;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#22238;&#31572;&#19982;&#26102;&#38388;&#25935;&#24863;&#30340;&#21307;&#30103;&#38382;&#39064;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;3&#65289;&#26681;&#25454;&#39044;&#27979;&#21306;&#20998;&#32473;&#23450;&#38382;&#39064;&#26159;&#21487;&#22238;&#31572;&#36824;&#26159;&#19981;&#21487;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new text-to-SQL dataset for electronic health records (EHRs). The utterances were collected from 222 hospital staff, including physicians, nurses, insurance review and health records teams, and more. To construct the QA dataset on structured EHR data, we conducted a poll at a university hospital and templatized the responses to create seed questions. Then, we manually linked them to two open-source EHR databases, MIMIC-III and eICU, and included them with various time expressions and held-out unanswerable questions in the dataset, which were all collected from the poll. Our dataset poses a unique set of challenges: the model needs to 1) generate SQL queries that reflect a wide range of needs in the hospital, including simple retrieval and complex operations such as calculating survival rate, 2) understand various time expressions to answer time-sensitive questions in healthcare, and 3) distinguish whether a given question is answerable or unanswerable based on the predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#21253;&#25324;&#37319;&#26679;&#21152;&#36895;&#12289;&#26032;&#30340;&#25193;&#25955;&#36807;&#31243;&#35774;&#35745;&#20197;&#21450;&#22312;&#19981;&#21516;&#31354;&#38388;&#20013;&#23454;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#21019;&#26032;&#21162;&#21147;&#26088;&#22312;&#25552;&#39640;&#25193;&#25955;&#27169;&#22411;&#30340;&#21151;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2209.02646</link><description>&lt;p&gt;
&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Generative Diffusion Model. (arXiv:2209.02646v9 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#21253;&#25324;&#37319;&#26679;&#21152;&#36895;&#12289;&#26032;&#30340;&#25193;&#25955;&#36807;&#31243;&#35774;&#35745;&#20197;&#21450;&#22312;&#19981;&#21516;&#31354;&#38388;&#20013;&#23454;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#21019;&#26032;&#21162;&#21147;&#26088;&#22312;&#25552;&#39640;&#25193;&#25955;&#27169;&#22411;&#30340;&#21151;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#29983;&#25104;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#34987;&#29992;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#29983;&#25104;&#36136;&#37327;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20855;&#26377;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#32791;&#26102;&#30340;&#36845;&#20195;&#29983;&#25104;&#36807;&#31243;&#21644;&#38480;&#21046;&#22312;&#39640;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22823;&#37327;&#26088;&#22312;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#21253;&#25324;&#37319;&#26679;&#21152;&#36895;&#21644;&#26032;&#30340;&#25193;&#25955;&#36807;&#31243;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#27969;&#24418;&#21644;&#31163;&#25955;&#31354;&#38388;&#20013;&#23454;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#21450;&#21019;&#24314;&#20004;&#20010;&#20219;&#24847;&#20998;&#24067;&#20043;&#38388;&#26725;&#26753;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#30340;&#21019;&#26032;&#20195;&#34920;&#20102;&#36817;&#24180;&#26469;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;&#21151;&#33021;&#21644;&#25928;&#29575;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models are a prominent approach for data generation, and have been used to produce high quality samples in various domains. Diffusion models, an emerging class of deep generative models, have attracted considerable attention owing to their exceptional generative quality. Despite this, they have certain limitations, including a time-consuming iterative generation process and confinement to high-dimensional Euclidean space. This survey presents a plethora of advanced techniques aimed at enhancing diffusion models, including sampling acceleration and the design of new diffusion processes. In addition, we delve into strategies for implementing diffusion models in manifold and discrete spaces, maximum likelihood training for diffusion models, and methods for creating bridges between two arbitrary distributions. The innovations we discuss represent the efforts for improving the functionality and efficiency of diffusion models in recent years. To examine the efficacy of existi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#22791;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#29615;&#22659;&#22240;&#32032;&#23545;&#20934;&#30830;&#24615;&#36896;&#25104;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#65292;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2203.01077</link><description>&lt;p&gt;
&#35774;&#22791;&#19978;&#23398;&#20064;&#65306;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35757;&#32451;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
On-Device Learning: A Neural Network Based Field-Trainable Edge AI. (arXiv:2203.01077v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#22791;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#29615;&#22659;&#22240;&#32032;&#23545;&#20934;&#30830;&#24615;&#36896;&#25104;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#65292;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#65292;&#20854;&#20934;&#30830;&#24615;&#32463;&#24120;&#21463;&#21040;&#21508;&#31181;&#29615;&#22659;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#22914;&#22122;&#22768;&#12289;&#20256;&#24863;&#22120;&#30340;&#20301;&#32622;/&#26657;&#20934;&#21644;&#26102;&#38388;&#30456;&#20851;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#22791;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#32780;&#19981;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20107;&#23454;&#19978;&#30340;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#26377;&#24456;&#22823;&#21306;&#21035;&#65292;&#32780;&#26159;&#19987;&#20026;&#20302;&#31471;&#36793;&#32536;&#35774;&#22791;&#37327;&#36523;&#23450;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20854;&#31639;&#27861;&#21644;&#22312;&#30001;&#26641;&#33683;&#27966;Pico&#21644;&#20302;&#21151;&#32791;&#26080;&#32447;&#27169;&#22359;&#32452;&#25104;&#30340;&#26080;&#32447;&#20256;&#24863;&#22120;&#33410;&#28857;&#19978;&#30340;&#23454;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#26059;&#36716;&#26426;&#22120;&#30340;&#25391;&#21160;&#27169;&#24335;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#35774;&#22791;&#19978;&#23398;&#20064;&#30340;&#37325;&#26032;&#35757;&#32451;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world edge AI applications, their accuracy is often affected by various environmental factors, such as noises, location/calibration of sensors, and time-related changes. This article introduces a neural network based on-device learning approach to address this issue without going deep. Our approach is quite different from de facto backpropagation based training but tailored for low-end edge devices. This article introduces its algorithm and implementation on a wireless sensor node consisting of Raspberry Pi Pico and low-power wireless module. Experiments using vibration patterns of rotating machines demonstrate that retraining by the on-device learning significantly improves an anomaly detection accuracy at a noisy environment while saving computation and communication costs for low power.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30196;&#21574;&#30417;&#27979;&#21644;&#35786;&#26029;&#12290;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#12289;&#35328;&#35821;&#21644;&#35821;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#21306;&#20998;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#24739;&#32773;&#21644;&#23545;&#29031;&#32452;&#65292;&#20174;&#32780;&#20026;&#30196;&#21574;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2109.01537</link><description>&lt;p&gt;
&#29992;&#20110;&#30196;&#21574;&#30417;&#27979;&#21644;&#35786;&#26029;&#30340;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis. (arXiv:2109.01537v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30196;&#21574;&#30417;&#27979;&#21644;&#35786;&#26029;&#12290;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#12289;&#35328;&#35821;&#21644;&#35821;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#21306;&#20998;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#24739;&#32773;&#21644;&#23545;&#29031;&#32452;&#65292;&#20174;&#32780;&#20026;&#30196;&#21574;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30196;&#21574;&#26159;&#19968;&#31995;&#21015;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#24433;&#21709;&#36234;&#26469;&#36234;&#22810;&#30340;&#20840;&#29699;&#32769;&#40836;&#20154;&#21475;&#30340;&#35760;&#24518;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;&#33258;&#21160;&#21270;&#20998;&#26512;&#35821;&#35328;&#12289;&#35328;&#35821;&#21644;&#35821;&#29992;&#25351;&#26631;&#20316;&#20026;&#35748;&#30693;&#34928;&#36864;&#30340;&#28508;&#22312;&#25351;&#26631;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32437;&#21521;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#33258;&#28982;&#29615;&#22659;&#19979;&#25910;&#38598;&#20102;&#36731;&#24230;&#30196;&#21574;&#24739;&#32773;&#21644;&#37197;&#23545;&#30340;&#24180;&#40836;&#21305;&#37197;&#23545;&#29031;&#32452;&#30340;&#25968;&#25454;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;&#20960;&#20010;&#26376;&#12290;&#22810;&#27169;&#24577;&#25968;&#25454;&#21253;&#25324;&#21475;&#22836;&#20250;&#35805;&#65292;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#34987;&#36716;&#24405;&#65292;&#20197;&#21450;&#36755;&#20837;&#21644;&#20070;&#20889;&#30340;&#24605;&#32771;&#20869;&#23481;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#38750;&#35821;&#35328;&#20449;&#24687;&#65292;&#22914;&#31508;&#30011;&#21644;&#25353;&#38190;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#35813;&#25968;&#25454;&#38598;&#65292;&#24182;&#30528;&#37325;&#35752;&#35770;&#20102;&#20351;&#29992;&#35821;&#38899;&#27169;&#24577;&#30340;&#20219;&#21153;&#12290;&#21518;&#32773;&#28041;&#21450;&#21033;&#29992;&#25968;&#25454;&#30340;&#32437;&#21521;&#29305;&#24615;&#26469;&#21306;&#20998;&#23545;&#29031;&#32452;&#21644;&#30196;&#21574;&#24739;&#32773;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#20250;&#35805;&#38388;&#35821;&#38899;&#30340;&#21464;&#21270;&#22312;&#19981;&#21516;&#30340;&#20250;&#35805;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dementia is a family of neurogenerative conditions affecting memory and cognition in an increasing number of individuals in our globally aging population. Automated analysis of language, speech and paralinguistic indicators have been gaining popularity as potential indicators of cognitive decline. Here we propose a novel longitudinal multi-modal dataset collected from people with mild dementia and age matched controls over a period of several months in a natural setting. The multi-modal data consists of spoken conversations, a subset of which are transcribed, as well as typed and written thoughts and associated extra-linguistic information such as pen strokes and keystrokes. We describe the dataset in detail and proceed to focus on a task using the speech modality. The latter involves distinguishing controls from people with dementia by exploiting the longitudinal nature of the data. Our experiments showed significant differences in how the speech varied from session to session in the 
&lt;/p&gt;</description></item></channel></rss>