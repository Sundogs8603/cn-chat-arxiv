<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#30740;&#31350;&#23545;&#22320;&#19979;CO2&#27844;&#28431;&#36827;&#34892;&#30417;&#27979;&#21644;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25551;&#36848;CO2&#27969;&#21160;&#27169;&#24335;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00290</link><description>&lt;p&gt;
CO2&#27969;&#21160;&#27169;&#24335;&#30340;&#25512;&#26029;--&#21487;&#34892;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Inference of CO2 flow patterns -- a feasibility study. (arXiv:2311.00290v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00290
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#22320;&#19979;CO2&#27844;&#28431;&#36827;&#34892;&#30417;&#27979;&#21644;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25551;&#36848;CO2&#27969;&#21160;&#27169;&#24335;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#30899;&#25429;&#33719;&#21644;&#23553;&#23384;&#65288;CCS&#65289;&#25216;&#26415;&#22312;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#26007;&#20105;&#20013;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#65292;&#24314;&#31435;&#31283;&#20581;&#30340;&#30417;&#27979;&#21644;&#26816;&#27979;&#26426;&#21046;&#65292;&#20197;&#26816;&#27979;&#28508;&#22312;&#30340;&#22320;&#19979;CO2&#27844;&#28431;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#23384;&#20648;&#24211;&#23553;&#22581;&#30340;&#39044;&#20808;&#23384;&#22312;&#25110;&#35825;&#23548;&#30340;&#26029;&#23618;&#65292;&#21464;&#24471;&#36234;&#26469;&#36234;&#36843;&#20999;&#12290;&#34429;&#28982;&#35832;&#22914;&#21382;&#21490;&#21305;&#37197;&#21644;CO2&#20648;&#23384;&#30340;&#26102;&#38388;&#24207;&#21015;&#22320;&#38663;&#30417;&#27979;&#31561;&#25216;&#26415;&#24050;&#25104;&#21151;&#29992;&#20110;&#36319;&#36394;&#22320;&#19979;CO2&#28183;&#28431;&#30340;&#28436;&#21270;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#23545;CO2&#27874;&#21160;&#34892;&#20026;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#22522;&#26412;&#26041;&#27861;&#12290;&#31995;&#32479;&#35780;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#32435;&#20837;&#23545;&#20110;&#39118;&#38505;&#32531;&#35299;&#33267;&#20851;&#37325;&#35201;&#65292;&#21407;&#22240;&#22914;&#19979;&#65306;&#65288;i&#65289;CO2&#27874;&#21160;&#35825;&#21457;&#30340;&#21464;&#21270;&#24456;&#23567;&#19988;&#22320;&#38663;&#25968;&#25454;&#22122;&#22768;&#24456;&#22823;&#65307;&#65288;ii&#65289;&#27491;&#24120;&#21644;&#19981;&#35268;&#21017;&#65288;&#20363;&#22914;&#27844;&#28431;&#24341;&#36215;&#30340;&#65289;&#27969;&#21160;&#27169;&#24335;&#20043;&#38388;&#30340;&#21464;&#21270;&#24456;&#23567;&#65307;&#65288;iii&#65289;&#25511;&#21046;&#27969;&#21160;&#30340;&#20648;&#23618;&#29305;&#24615;&#24378;&#28872;&#24322;&#36136;&#19988;&#36890;&#24120;
&lt;/p&gt;
&lt;p&gt;
As the global deployment of carbon capture and sequestration (CCS) technology intensifies in the fight against climate change, it becomes increasingly imperative to establish robust monitoring and detection mechanisms for potential underground CO2 leakage, particularly through pre-existing or induced faults in the storage reservoir's seals. While techniques such as history matching and time-lapse seismic monitoring of CO2 storage have been used successfully in tracking the evolution of CO2 plumes in the subsurface, these methods lack principled approaches to characterize uncertainties related to the CO2 plumes' behavior. Inclusion of systematic assessment of uncertainties is essential for risk mitigation for the following reasons: (i) CO2 plume-induced changes are small and seismic data is noisy; (ii) changes between regular and irregular (e.g., caused by leakage) flow patterns are small; and (iii) the reservoir properties that control the flow are strongly heterogeneous and typically 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#23454;&#29616;&#39640;&#25928;&#32534;&#36753;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#38271;&#35270;&#39057;&#37319;&#26679;&#26657;&#27491;&#30830;&#20445;&#19968;&#33268;&#24615;&#12290;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2311.00213</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#23454;&#29616;&#19968;&#33268;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Consistent Video-to-Video Transfer Using Synthetic Dataset. (arXiv:2311.00213v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#23454;&#29616;&#39640;&#25928;&#32534;&#36753;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#38271;&#35270;&#39057;&#37319;&#26679;&#26657;&#27491;&#30830;&#20445;&#19968;&#33268;&#24615;&#12290;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#27599;&#20010;&#35270;&#39057;&#27599;&#20010;&#27169;&#22411;&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#24494;&#35843;&#38656;&#27714;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#20026;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#21512;&#25104;&#37197;&#23545;&#35270;&#39057;&#25968;&#25454;&#38598;&#12290;&#21463;&#21040;Instruct Pix2Pix&#30340;&#22270;&#20687;&#36890;&#36807;&#32534;&#36753;&#25351;&#20196;&#36827;&#34892;&#36716;&#25442;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#33539;&#24335;&#24212;&#29992;&#20110;&#35270;&#39057;&#39046;&#22495;&#12290;&#25105;&#20204;&#23545;Prompt-to-Prompt&#36827;&#34892;&#20102;&#25299;&#23637;&#65292;&#39640;&#25928;&#29983;&#25104;&#37197;&#23545;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#37117;&#21253;&#21547;&#19968;&#20010;&#36755;&#20837;&#35270;&#39057;&#21644;&#20854;&#32534;&#36753;&#21518;&#30340;&#23545;&#24212;&#35270;&#39057;&#12290;&#21516;&#26102;&#65292;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#38271;&#35270;&#39057;&#37319;&#26679;&#26657;&#27491;&#65292;&#30830;&#20445;&#25209;&#27425;&#20043;&#38388;&#30340;&#38271;&#35270;&#39057;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#24403;&#21069;&#30340;Tune-A-Video&#31561;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel and efficient approach for text-based video-to-video editing that eliminates the need for resource-intensive per-video-per-model finetuning. At the core of our approach is a synthetic paired video dataset tailored for video-to-video transfer tasks. Inspired by Instruct Pix2Pix's image transfer via editing instruction, we adapt this paradigm to the video domain. Extending the Prompt-to-Prompt to videos, we efficiently generate paired samples, each with an input video and its edited counterpart. Alongside this, we introduce the Long Video Sampling Correction during sampling, ensuring consistent long videos across batches. Our method surpasses current methods like Tune-A-Video, heralding substantial progress in text-based video-to-video editing and suggesting exciting avenues for further exploration and deployment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#38750;&#23545;&#31216;&#20851;&#31995;&#65292;&#19988;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18348</link><description>&lt;p&gt;
&#24847;&#20041;&#34920;&#24449;&#26469;&#33258;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#38750;&#23545;&#31216;&#20851;&#31995;&#65292;&#19988;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#32771;&#34385;&#25193;&#23637;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#26469;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#12290;&#36825;&#31181;&#31574;&#30053;&#26159;&#26080;&#25552;&#31034;&#30340;&#65292;&#19981;&#38656;&#35201;&#24494;&#35843;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#21521;&#37327;&#30340;&#34920;&#24449;&#19981;&#21516;&#65292;&#22522;&#20110;&#20998;&#24067;&#30340;&#34920;&#24449;&#36824;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20284;&#28982;&#20989;&#25968;&#20043;&#38388;&#30340;&#20195;&#25968;&#36816;&#31639;&#26469;&#24314;&#27169;&#38750;&#23545;&#31216;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#36923;&#36753;&#34164;&#28085;&#30340;&#26041;&#21521;&#65292;&#19978;&#20301;&#35789;/&#19979;&#20301;&#35789;&#20851;&#31995;&#65289;&#12290;&#36825;&#20123;&#24819;&#27861;&#22522;&#20110;&#35821;&#20041;&#30340;&#20998;&#24067;&#35270;&#35282;&#65292;&#24182;&#19982;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#26631;&#20934;&#26500;&#36896;&#30456;&#36830;&#25509;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20204;&#23578;&#26410;&#24212;&#29992;&#20110;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20174;&#22823;&#22411;&#27169;&#22411;&#33719;&#24471;&#30340;&#34920;&#24449;&#19982;&#20154;&#31867;&#27880;&#37322;&#24456;&#22909;&#22320;&#19968;&#33268;&#65292;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#38646;&#26679;&#26412;&#21644;&#26080;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#34164;&#28085;&#21644;&#21253;&#21547;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to extract meaning representations from autoregressive language models by considering the distribution of all possible trajectories extending an input text. This strategy is prompt-free, does not require fine-tuning, and is applicable to any pre-trained autoregressive model. Moreover, unlike vector-based representations, distribution-based representations can also model asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym relations) by using algebraic operations between likelihood functions. These ideas are grounded in distributional perspectives on semantics and are connected to standard constructions in automata theory, but to our knowledge they have not been applied to modern language models. We empirically show that the representations obtained from large models align well with human annotations, outperform other zero-shot and prompt-free methods on semantic similarity tasks, and can be used to solve more complex entailment and containment tasks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;&#25143;&#25351;&#23450;&#30340;&#25991;&#26412;&#26631;&#20934;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#32858;&#31867;&#32467;&#26524;&#30340;&#30452;&#25509;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#65292;&#24182;&#33021;&#22312;&#21508;&#31181;&#26631;&#20934;&#19979;&#26377;&#25928;&#22320;&#32858;&#31867;&#22270;&#20687;&#65292;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18297</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Image Clustering Conditioned on Text Criteria. (arXiv:2310.18297v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;&#25143;&#25351;&#23450;&#30340;&#25991;&#26412;&#26631;&#20934;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#32858;&#31867;&#32467;&#26524;&#30340;&#30452;&#25509;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#65292;&#24182;&#33021;&#22312;&#21508;&#31181;&#26631;&#20934;&#19979;&#26377;&#25928;&#22320;&#32858;&#31867;&#22270;&#20687;&#65292;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32858;&#31867;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#28385;&#36275;&#29992;&#25143;&#23545;&#32858;&#31867;&#32467;&#26524;&#30340;&#25511;&#21046;&#38656;&#27714;&#65292;&#32780;&#19988;&#32858;&#31867;&#32467;&#26524;&#21487;&#33021;&#19982;&#29992;&#25143;&#24515;&#20013;&#30456;&#20851;&#30340;&#26631;&#20934;&#19981;&#19968;&#33268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#32858;&#31867;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;&#25143;&#25351;&#23450;&#30340;&#25991;&#26412;&#26631;&#20934;&#65292;&#21033;&#29992;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#22522;&#20110;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#32858;&#31867;&#65288;IC|TC&#65289;&#65292;&#23427;&#20195;&#34920;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#22270;&#20687;&#32858;&#31867;&#33539;&#24335;&#12290;IC|TC&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#65292;&#24182;&#20351;&#29992;&#25143;&#33021;&#22815;&#26377;&#36739;&#22823;&#30340;&#25511;&#21046;&#26435;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IC|TC&#33021;&#22815;&#26377;&#25928;&#22320;&#25353;&#29031;&#21508;&#31181;&#26631;&#20934;&#65288;&#22914;&#20154;&#31867;&#34892;&#20026;&#12289;&#29289;&#29702;&#20301;&#32622;&#25110;&#20154;&#30340;&#24515;&#24773;&#65289;&#23545;&#22270;&#20687;&#36827;&#34892;&#32858;&#31867;&#65292;&#32780;&#19988;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified text criteria by leveraging modern vision-language models and large language models. We call our method Image Clustering Conditioned on Text Criteria (IC$|$TC), and it represents a different paradigm of image clustering. IC$|$TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that IC$|$TC can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, while significantly outperforming baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#20307;&#36816;&#21160;&#30340;&#29289;&#29702;&#30693;&#35782;&#21644;&#31616;&#21333;&#30340;2D&#26631;&#27880;&#65292;&#20174;&#21333;&#30446;&#22270;&#20687;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;3D&#29289;&#20307;&#23450;&#20301;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;3D&#26631;&#27880;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#20165;&#20026;6&#21400;&#31859;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#27861;&#25910;&#38598;3D&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17462</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#36816;&#21160;&#23450;&#24459;&#20174;2D&#26631;&#27880;&#20013;&#23398;&#20064;&#21333;&#30446;3D&#29289;&#20307;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion. (arXiv:2310.17462v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#20307;&#36816;&#21160;&#30340;&#29289;&#29702;&#30693;&#35782;&#21644;&#31616;&#21333;&#30340;2D&#26631;&#27880;&#65292;&#20174;&#21333;&#30446;&#22270;&#20687;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;3D&#29289;&#20307;&#23450;&#20301;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;3D&#26631;&#27880;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#20165;&#20026;6&#21400;&#31859;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#27861;&#25910;&#38598;3D&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#20307;&#30340;&#36816;&#21160;&#29289;&#29702;&#30693;&#35782;&#21644;&#31616;&#21333;&#30340;2D&#26631;&#27880;&#65292;&#20174;&#21333;&#20010;&#26657;&#20934;&#30456;&#26426;&#30340;&#21333;&#20010;&#22270;&#20687;&#20013;&#31934;&#30830;&#23450;&#20301;3D&#29289;&#20307;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;3D&#26631;&#27880;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20986;&#38544;&#21547;&#30340;&#31532;&#19977;&#20010;&#32500;&#24230;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#27492;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#23454;&#29616;&#20102;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#20165;&#20026;6&#21400;&#31859;&#12290;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#27861;&#25910;&#38598;3D&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20316;&#20026;&#23398;&#20064;3D&#29289;&#20307;&#23450;&#20301;&#20272;&#35745;&#30340;&#19968;&#27493;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method for precise 3D object localization in single images from a single calibrated camera using only 2D labels. No expensive 3D labels are needed. Thus, instead of using 3D labels, our model is trained with easy-to-annotate 2D labels along with the physical knowledge of the object's motion. Given this information, the model can infer the latent third dimension, even though it has never seen this information during training. Our method is evaluated on both synthetic and real-world datasets, and we are able to achieve a mean distance error of just 6 cm in our experiments on real data. The results indicate the method's potential as a step towards learning 3D object location estimation, where collecting 3D data for training is not feasible.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#25928;&#35268;&#21010;&#65292;&#24182;&#20445;&#35777;&#20102;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#22312;&#39640;&#32423;&#25628;&#32034;&#20013;&#28155;&#21152;&#20302;&#32423;&#21160;&#20316;&#65292;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#39640;&#32423;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21448;&#20855;&#26377;&#20302;&#32423;&#25628;&#32034;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12819</link><description>&lt;p&gt;
&#24102;&#26377;&#23436;&#25972;&#24615;&#20445;&#35777;&#30340;&#39640;&#25928;&#35268;&#21010;&#30340;&#28151;&#21512;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Hybrid Search for Efficient Planning with Completeness Guarantees. (arXiv:2310.12819v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#25928;&#35268;&#21010;&#65292;&#24182;&#20445;&#35777;&#20102;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#22312;&#39640;&#32423;&#25628;&#32034;&#20013;&#28155;&#21152;&#20302;&#32423;&#21160;&#20316;&#65292;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#39640;&#32423;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21448;&#20855;&#26377;&#20302;&#32423;&#25628;&#32034;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#35268;&#21010;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#23398;&#20064;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#32570;&#20047;&#23436;&#25972;&#24615;&#20445;&#35777;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#23384;&#22312;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#23376;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#23436;&#25972;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#39640;&#32423;&#25628;&#32034;&#20013;&#28155;&#21152;&#20302;&#32423;&#21160;&#20316;&#26469;&#25191;&#34892;&#22810;&#23618;&#27425;&#65288;&#28151;&#21512;&#65289;&#25628;&#32034;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#23436;&#25972;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#12290;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#23454;&#29616;&#20102;&#39640;&#32423;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29575;&#21644;&#20302;&#32423;&#25628;&#32034;&#30340;&#23436;&#25972;&#24615;&#30340;&#26368;&#20339;&#32467;&#21512;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#25628;&#32034;&#26041;&#27861;&#24212;&#29992;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#35780;&#20272;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#31639;&#27861;&#22312;&#22797;&#26434;&#30340;&#35268;&#21010;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#23436;&#25972;&#23376;&#30446;&#26631;&#25628;&#32034;&#19981;&#20165;&#20445;&#35777;&#20102;&#23436;&#25972;&#24615;&#65292;&#36824;&#21487;&#20197;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complex planning problems has been a long-standing challenge in computer science. Learning-based subgoal search methods have shown promise in tackling these problems, but they often suffer from a lack of completeness guarantees, meaning that they may fail to find a solution even if one exists. In this paper, we propose an efficient approach to augment a subgoal search method to achieve completeness in discrete action spaces. Specifically, we augment the high-level search with low-level actions to execute a multi-level (hybrid) search, which we call complete subgoal search. This solution achieves the best of both worlds: the practical efficiency of high-level search and the completeness of low-level search. We apply the proposed search method to a recently proposed subgoal search algorithm and evaluate the algorithm trained on offline data on complex planning problems. We demonstrate that our complete subgoal search not only guarantees completeness but can even improve performan
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.11518</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#22312;&#22810;&#20154;&#28216;&#25103;&#20013;&#23545;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability. (arXiv:2310.11518v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11518
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#23545;&#25239;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#21103;&#26412;&#20132;&#20114;&#26469;&#23398;&#20064;&#12290;&#33258;&#25105;&#23545;&#25239;&#23545;&#20110;&#29983;&#25104;&#22823;&#37327;&#30340;&#23398;&#20064;&#25968;&#25454;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#30340;&#32570;&#28857;&#26159;&#35757;&#32451;&#21518;&#23398;&#20064;&#32773;&#23558;&#38754;&#23545;&#30340;&#26234;&#33021;&#20307;&#21487;&#33021;&#19982;&#36890;&#36807;&#19982;&#33258;&#36523;&#20132;&#20114;&#26102;&#25152;&#26399;&#26395;&#30340;&#26234;&#33021;&#20307;&#34892;&#20026;&#25130;&#28982;&#19981;&#21516;&#12290;&#23545;&#20110;&#20004;&#20154;&#24120;&#21644;&#28216;&#25103;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#36798;&#21040;&#32435;&#20160;&#22343;&#34913;&#30340;&#33258;&#25105;&#23545;&#25239;&#33021;&#22815;&#20445;&#35777;&#20135;&#29983;&#23545;&#20219;&#20309;&#35757;&#32451;&#21518;&#23545;&#25163;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#20154;&#28216;&#25103;&#26469;&#35828;&#27809;&#26377;&#36825;&#26679;&#30340;&#20445;&#35777;&#23384;&#22312;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36817;&#20284;&#20998;&#35299;&#20026;&#19968;&#32452;&#20004;&#20154;&#24120;&#21644;&#28216;&#25103;&#65288;&#31216;&#20026;&#22810;&#30697;&#38453;&#28216;&#25103;&#65289;&#30340;&#28216;&#25103;&#20013;&#65292;&#20854;&#20013;&#20840;&#23616; $\epsilon$-&#32435;&#20160;&#22343;&#34913;&#22312;&#27599;&#20010;&#23376;&#28216;&#25103;&#20013;&#37117;&#19982;&#32435;&#20160;&#22343;&#34913;&#26377;&#26377;&#30028;&#36317;&#31163;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#23558;&#20135;&#29983;&#19968;&#20010;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39318;&#27425;&#30830;&#23450;&#20102;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Self-play is a technique for machine learning in multi-agent systems where a learning algorithm learns by interacting with copies of itself. Self-play is useful for generating large quantities of data for learning, but has the drawback that the agents the learner will face post-training may have dramatically different behavior than the learner came to expect by interacting with itself. For the special case of two-player constant-sum games, self-play that reaches Nash equilibrium is guaranteed to produce strategies that perform well against any post-training opponent; however, no such guarantee exists for multi-player games. We show that in games that approximately decompose into a set of two-player constant-sum games (called polymatrix games) where global $\epsilon$-Nash equilibria are boundedly far from Nash-equilibria in each subgame, any no-external-regret algorithm that learns by self-play will produce a strategy with bounded vulnerability. For the first time, our results identify 
&lt;/p&gt;</description></item><item><title>GROOT&#26159;&#19968;&#20010;&#36890;&#36807;&#35266;&#30475;&#28216;&#25103;&#35270;&#39057;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;&#30340;&#25511;&#21046;&#22120;&#65292;&#23427;&#36890;&#36807;&#20135;&#29983;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#30446;&#26631;&#31354;&#38388;&#26469;&#28040;&#38500;&#26114;&#36149;&#30340;&#25991;&#26412;-&#28216;&#25103;&#27880;&#37322;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;Minecraft SkillForge&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#29609;&#23478;&#30456;&#24403;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.08235</link><description>&lt;p&gt;
GROOT: &#36890;&#36807;&#35266;&#30475;&#28216;&#25103;&#35270;&#39057;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
GROOT: Learning to Follow Instructions by Watching Gameplay Videos. (arXiv:2310.08235v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08235
&lt;/p&gt;
&lt;p&gt;
GROOT&#26159;&#19968;&#20010;&#36890;&#36807;&#35266;&#30475;&#28216;&#25103;&#35270;&#39057;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;&#30340;&#25511;&#21046;&#22120;&#65292;&#23427;&#36890;&#36807;&#20135;&#29983;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#30446;&#26631;&#31354;&#38388;&#26469;&#28040;&#38500;&#26114;&#36149;&#30340;&#25991;&#26412;-&#28216;&#25103;&#27880;&#37322;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;Minecraft SkillForge&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#29609;&#23478;&#30456;&#24403;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#36981;&#24490;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#25511;&#21046;&#22120;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35266;&#30475;&#35270;&#39057;&#20316;&#20026;&#25351;&#20196;&#30340;&#26041;&#24335;&#65292;&#36825;&#31181;&#26041;&#24335;&#25552;&#20379;&#20102;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#30446;&#26631;&#35268;&#33539;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#25991;&#26412;-&#28216;&#25103;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28216;&#25103;&#35270;&#39057;&#20013;&#23398;&#20064;&#36825;&#31181;&#25351;&#20196;&#36981;&#24490;&#25511;&#21046;&#22120;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#33021;&#20135;&#29983;&#32467;&#26500;&#21270;&#30446;&#26631;&#31354;&#38388;&#30340;&#35270;&#39057;&#25351;&#20196;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#22240;&#26524;&#21464;&#21387;&#22120;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#31243;&#24207;GROOT&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#25552;&#20986;&#30340;Minecraft SkillForge&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;GROOT&#36827;&#34892;&#20102;&#19982;&#24320;&#25918;&#19990;&#30028;&#23545;&#25163;&#21644;&#20154;&#31867;&#29609;&#23478;&#30340;&#35780;&#20272;&#12290;Elo&#35780;&#32423;&#28165;&#26970;&#22320;&#26174;&#31034;GROOT&#27491;&#22312;&#32553;&#23567;&#20154;&#26426;&#24046;&#36317;&#65292;&#24182;&#19988;&#23545;&#26368;&#22909;&#30340;&#36890;&#29992;&#20195;&#29702;&#31243;&#24207;&#22522;&#32447;&#20855;&#26377;70%&#30340;&#32988;&#29575;&#12290;&#23545;&#25152;&#20135;&#29983;&#30340;&#30446;&#26631;&#31354;&#38388;&#30340;&#23450;&#24615;&#20998;&#26512;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#26032;&#39062;&#24615;&#36136;&#65292;&#21253;&#25324;&#30446;&#26631;&#30340;&#32452;&#25104;&#21644;...
&lt;/p&gt;
&lt;p&gt;
We study the problem of building a controller that can follow open-ended instructions in open-world environments. We propose to follow reference videos as instructions, which offer expressive goal specifications while eliminating the need for expensive text-gameplay annotations. A new learning framework is derived to allow learning such instruction-following controllers from gameplay videos while producing a video instruction encoder that induces a structured goal space. We implement our agent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers. We evaluate GROOT against open-world counterparts and human players on a proposed Minecraft SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap as well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis of the induced goal space further demonstrates some interesting emergent properties, including the goal composition and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#37322;&#21477;&#23376;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#37325;&#35201;&#30340;&#39044;&#27979;&#20196;&#29260;&#23545;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;</title><link>http://arxiv.org/abs/2310.05703</link><description>&lt;p&gt;
Siamese&#32534;&#30721;&#22120;&#30340;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Attribution Method for Siamese Encoders. (arXiv:2310.05703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#37322;&#21477;&#23376;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#37325;&#35201;&#30340;&#39044;&#27979;&#20196;&#29260;&#23545;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21477;&#23376;&#36716;&#25442;&#22120;&#31561;Siamese&#32534;&#30721;&#22120;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20154;&#20204;&#23545;&#23427;&#20204;&#20851;&#27880;&#30340;&#36755;&#20837;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#12290;&#19968;&#20010;&#38556;&#30861;&#26159;&#23427;&#20204;&#30340;&#39044;&#27979;&#19981;&#33021;&#24402;&#22240;&#20110;&#20010;&#21035;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#20204;&#27604;&#36739;&#30340;&#26159;&#20004;&#20010;&#36755;&#20837;&#32780;&#19981;&#26159;&#19968;&#20010;&#36755;&#20837;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#29305;&#24449;&#23545;&#24402;&#22240;&#30340;&#24418;&#24335;&#65292;&#24182;&#21487;&#23558;&#20854;&#31616;&#21270;&#20026;&#21477;&#23376;&#36716;&#25442;&#22120;&#30340;&#20196;&#29260;-&#20196;&#29260;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24341;&#20837;&#38598;&#25104;&#38597;&#21487;&#27604;&#30697;&#38453;&#65292;&#24182;&#32487;&#25215;&#20102;&#38598;&#25104;&#26799;&#24230;&#30340;&#20248;&#21183;&#24418;&#24335;&#29305;&#24615;&#65306;&#23427;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#23436;&#25972;&#35745;&#31639;&#22270;&#65292;&#24182;&#30830;&#20445;&#25910;&#25947;&#21040;&#23454;&#38469;&#39044;&#27979;&#32467;&#26524;&#12290;&#19968;&#39033;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21477;&#23376;&#36716;&#25442;&#22120;&#20013;&#65292;&#24456;&#23569;&#30340;&#20196;&#29260;&#23545;&#24448;&#24448;&#21487;&#20197;&#35299;&#37322;&#22823;&#37096;&#20998;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23427;&#38656;&#35201;&#20851;&#27880;&#22823;&#22810;&#25968;&#30340;&#20196;&#29260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Siamese encoder models such as sentence transformers (ST), little is known about the aspects of inputs they pay attention to. A barrier is that their predictions cannot be attributed to individual features, as they compare two inputs rather than processing a single one. This paper derives a local attribution method for Siamese encoders by generalizing the principle of integrated gradients to models with multiple inputs. The solution takes the form of feature-pair attributions, and can be reduced to a token-token matrix for STs. Our method involves the introduction of integrated Jacobians and inherits the advantageous formal properties of integrated gradients: it accounts for the model's full computation graph and is guaranteed to converge to the actual prediction. A pilot study shows that in an ST few token-pairs can often explain large fractions of predictions, and it focuses on nouns and verbs. For accurate predictions, it however needs to attend to the majorit
&lt;/p&gt;</description></item><item><title>SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.03684</link><description>&lt;p&gt;
SmoothLLM&#65306;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03684
&lt;/p&gt;
&lt;p&gt;
SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21162;&#21147;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#65288;&#22914;GPT&#12289;Llama&#12289;Claude&#21644;PaLM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#65292;&#21363;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#27450;&#39575;&#65292;&#20197;&#29983;&#25104;&#19981;&#21512;&#36866;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SmoothLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20943;&#36731;LLM&#19978;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#23545;&#25239;&#24615;&#29983;&#25104;&#30340;&#25552;&#31034;&#23545;&#23383;&#31526;&#32423;&#21035;&#30340;&#25913;&#21464;&#24456;&#33030;&#24369;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#39318;&#20808;&#38543;&#26426;&#25200;&#21160;&#32473;&#23450;&#36755;&#20837;&#25552;&#31034;&#30340;&#22810;&#20010;&#21103;&#26412;&#65292;&#28982;&#21518;&#27719;&#24635;&#30456;&#24212;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;SmoothLLM&#23558;&#20247;&#22810;&#28909;&#38376;LLM&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#20445;&#23432;&#24615;&#65292;&#24182;&#23545;&#25915;&#20987;&#32531;&#35299;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#20351;&#29992;&#30340;&#26597;&#35810;&#25968;&#37327;&#27604;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#23569;&#24471;&#22810;&#65292;&#24182;&#19988;&#19982;&#20219;&#20309;LLM&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36328;&#26550;&#26500;&#22320;&#35782;&#21035;&#20989;&#25968;&#65292;&#24182;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.03605</link><description>&lt;p&gt;
FASER: &#36890;&#36807;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
FASER: Binary Code Similarity Search through the use of Intermediate Representations. (arXiv:2310.03605v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#34920;&#31034;&#36827;&#34892;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36328;&#26550;&#26500;&#22320;&#35782;&#21035;&#20989;&#25968;&#65292;&#24182;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#35782;&#21035;&#36328;&#26550;&#26500;&#36719;&#20214;&#20013;&#24863;&#20852;&#36259;&#30340;&#20989;&#25968;&#23545;&#20110;&#20998;&#26512;&#24694;&#24847;&#36719;&#20214;&#12289;&#20445;&#25252;&#36719;&#20214;&#20379;&#24212;&#38142;&#25110;&#36827;&#34892;&#28431;&#27934;&#30740;&#31350;&#37117;&#26159;&#26377;&#29992;&#30340;&#12290;&#36328;&#26550;&#26500;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#25628;&#32034;&#24050;&#22312;&#35768;&#22810;&#30740;&#31350;&#20013;&#25506;&#32034;&#65292;&#24182;&#20351;&#29992;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#26469;&#28304;&#26469;&#23454;&#29616;&#20854;&#30446;&#26631;&#12290;&#36890;&#24120;&#20351;&#29992;&#30340;&#25968;&#25454;&#26469;&#28304;&#21253;&#25324;&#20174;&#20108;&#36827;&#21046;&#25991;&#20214;&#20013;&#25552;&#21462;&#30340;&#24120;&#35265;&#32467;&#26500;&#65292;&#22914;&#20989;&#25968;&#25511;&#21046;&#27969;&#22270;&#25110;&#20108;&#36827;&#21046;&#32423;&#35843;&#29992;&#22270;&#65292;&#21453;&#27719;&#32534;&#36807;&#31243;&#30340;&#36755;&#20986;&#25110;&#21160;&#24577;&#20998;&#26512;&#26041;&#27861;&#30340;&#36755;&#20986;&#12290;&#20854;&#20013;&#19968;&#31181;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#30340;&#25968;&#25454;&#26469;&#28304;&#26159;&#20108;&#36827;&#21046;&#20013;&#38388;&#34920;&#31034;&#12290;&#20108;&#36827;&#21046;&#20013;&#38388;&#34920;&#31034;&#20855;&#26377;&#20004;&#20010;&#26377;&#36259;&#30340;&#23646;&#24615;&#65306;&#23427;&#20204;&#30340;&#36328;&#26550;&#26500;&#24615;&#36136;&#20197;&#21450;&#26126;&#30830;&#32534;&#30721;&#20989;&#25968;&#30340;&#35821;&#20041;&#20197;&#25903;&#25345;&#19979;&#28216;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FASER&#30340;&#20989;&#25968;&#23383;&#31526;&#20018;&#32534;&#30721;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#38271;&#25991;&#26723;&#36716;&#25442;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to identify functions of interest in cross-architecture software is useful whether you are analysing for malware, securing the software supply chain or conducting vulnerability research. Cross-Architecture Binary Code Similarity Search has been explored in numerous studies and has used a wide range of different data sources to achieve its goals. The data sources typically used draw on common structures derived from binaries such as function control flow graphs or binary level call graphs, the output of the disassembly process or the outputs of a dynamic analysis approach. One data source which has received less attention is binary intermediate representations. Binary Intermediate representations possess two interesting properties: they are cross architecture by their very nature and encode the semantics of a function explicitly to support downstream usage. Within this paper we propose Function as a String Encoded Representation (FASER) which combines long document transforme
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01929</link><description>&lt;p&gt;
&#31359;&#36234;&#25991;&#21270;&#40511;&#27807;&#65306;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#65292;&#20363;&#22914;DALL-E&#21644;StableDiffusion&#65292;&#22312;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#38646;&#23556;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#20316;&#20026;&#25991;&#21270;&#30340;&#23186;&#20171;&#65292;&#35821;&#35328;&#22312;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32780;&#22609;&#36896;&#20102;&#23427;&#20204;&#30340;&#25991;&#21270;&#26426;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25551;&#36848;&#25991;&#21270;&#32500;&#24230;&#65292;&#25991;&#21270;&#39046;&#22495;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#19977;&#20010;&#23618;&#27425;&#26469;&#25506;&#32034;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#25216;&#26415;&#65292;&#21253;&#25324;&#20351;&#29992;CLIP&#31354;&#38388;&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36827;&#34892;&#22806;&#22312;&#35780;&#20272;&#20197;&#21450;&#20154;&#31867;&#35780;&#20272;&#65292;&#20197;&#35782;&#21035;TTI&#25991;&#21270;&#24863;&#30693;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CulText2I&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#30340;TTI&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21313;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;
&lt;/p&gt;
&lt;p&gt;
Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have recently gained prominence for their remarkable zero-shot capabilities in generating images guided by textual prompts. Language, as a conduit of culture, plays a pivotal role in these models' multilingual capabilities, which in turn shape their cultural agency. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three hierarchical tiers: cultural dimensions, cultural domains, and cultural concepts. We propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA) model, and human assessments, to discern TTI cultural perceptions. To facilitate our research, we introduce the CulText2I dataset, derived from four diverse TTI models and spanning ten languages. Our experiments reveal insights into these models' cultural awareness, cultural distinctions, and the
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#65292;&#36890;&#36807;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12931</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#30340;&#20998;&#21035;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Separate Normalization in Self-supervised Transformers. (arXiv:2309.12931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12931
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#65292;&#36890;&#36807;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#30340;&#35757;&#32451;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#20197;&#24448;&#30340;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65288;&#22914;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65289;&#36890;&#24120;&#20250;&#20026;[CLS]&#31526;&#21495;&#21644;&#26631;&#35760;&#20351;&#29992;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#32531;&#35299;&#23558;&#30456;&#21516;&#30340;&#24402;&#19968;&#21270;&#32479;&#35745;&#25968;&#25454;&#24212;&#29992;&#20110;&#20004;&#31181;&#26631;&#35760;&#31867;&#22411;&#21487;&#33021;&#24102;&#26469;&#30340;&#36127;&#38754;&#25928;&#26524;&#65292;&#36825;&#20123;&#32479;&#35745;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#19982;&#23427;&#20204;&#21508;&#33258;&#30340;&#35282;&#33394;&#26368;&#20339;&#21305;&#37197;&#12290;&#36890;&#36807;&#20351;&#29992;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;[CLS]&#23884;&#20837;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#22659;&#20449;&#24687;&#65292;&#24182;&#22312;&#20854;&#38750;&#21508;&#21521;&#21516;&#24615;&#31354;&#38388;&#20013;&#20998;&#24067;&#26356;&#22343;&#21248;&#12290;&#24403;&#29992;&#36825;&#20004;&#20010;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#26367;&#25442;&#24120;&#35268;&#30340;&#24402;&#19968;&#21270;&#23618;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24179;&#22343;&#24615;&#33021;&#25552;&#21319;&#20102;2.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised training methods for transformers have demonstrated remarkable performance across various domains. Previous transformer-based models, such as masked autoencoders (MAE), typically utilize a single normalization layer for both the [CLS] symbol and the tokens. We propose in this paper a simple modification that employs separate normalization layers for the tokens and the [CLS] symbol to better capture their distinct characteristics and enhance downstream task performance. Our method aims to alleviate the potential negative effects of using the same normalization statistics for both token types, which may not be optimally aligned with their individual roles. We empirically show that by utilizing a separate normalization layer, the [CLS] embeddings can better encode the global contextual information and are distributed more uniformly in its anisotropic space. When replacing the conventional normalization layer with the two separate layers, we observe an average 2.7% performa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06800</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#65292;&#22240;&#20026;&#23427;&#22312;&#20132;&#36890;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36817;&#26399;&#65292;&#35768;&#22810;&#30740;&#31350;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#39044;&#27979;&#20301;&#32622;&#26377;&#23436;&#25972;&#25110;&#33267;&#23569;&#37096;&#20998;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#19981;&#33021;&#25193;&#23637;&#21040;&#26080;&#21382;&#21490;&#35760;&#24405;&#30340;&#20301;&#32622;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#39044;&#31639;&#38480;&#21046;&#21644;&#23433;&#35013;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#21487;&#33021;&#21463;&#38480;&#65292;&#36825;&#20351;&#24471;&#22823;&#22810;&#25968;&#24403;&#21069;&#27169;&#22411;&#19981;&#36866;&#29992;&#12290;&#34429;&#28982;&#23569;&#25968;&#25991;&#29486;&#23581;&#35797;&#22312;&#32570;&#22833;&#20301;&#32622;&#19978;&#25554;&#34917;&#20132;&#36890;&#29366;&#24577;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19982;&#20256;&#24863;&#22120;&#20301;&#32622;&#21516;&#26102;&#35266;&#27979;&#30340;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#39044;&#27979;&#20219;&#21153;&#12290;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#32570;&#20047;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#24471;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#20219;&#21153;&#25110;&#28041;&#21450;&#20915;&#31574;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#21463;&#21040;&#20808;&#21069;&#30340;&#24402;&#32435;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-awa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DTW+S&#30340;&#26032;&#22411;&#27979;&#37327;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21019;&#24314;&#23616;&#37096;&#36235;&#21183;&#30340;&#30697;&#38453;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#23616;&#37096;&#36235;&#21183;&#30456;&#20284;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03579</link><description>&lt;p&gt;
DTW+S: &#20351;&#29992;&#26377;&#24207;&#23616;&#37096;&#36235;&#21183;&#36827;&#34892;&#22522;&#20110;&#24418;&#29366;&#30340;&#26102;&#38388;&#24207;&#21015;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend. (arXiv:2309.03579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03579
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DTW+S&#30340;&#26032;&#22411;&#27979;&#37327;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21019;&#24314;&#23616;&#37096;&#36235;&#21183;&#30340;&#30697;&#38453;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#23616;&#37096;&#36235;&#21183;&#30456;&#20284;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36317;&#31163;&#25110;&#30456;&#20284;&#24230;&#30340;&#27979;&#37327;&#26159;&#35768;&#22810;&#24212;&#29992;&#21253;&#25324;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#22522;&#26412;&#26041;&#38754;&#12290;&#29616;&#26377;&#30340;&#27979;&#37327;&#26041;&#27861;&#21487;&#33021;&#30001;&#20110;&#23616;&#37096;&#36235;&#21183;&#65288;&#24418;&#29366;&#65289;&#32780;&#26080;&#27861;&#25429;&#25417;&#21040;&#30456;&#20284;&#20043;&#22788;&#65292;&#29978;&#33267;&#21487;&#33021;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#23547;&#25214;&#22312;&#30456;&#20284;&#26102;&#38388;&#21608;&#22260;&#21457;&#29983;&#30340;&#30456;&#20284;&#36235;&#21183;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#24212;&#29992;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26131;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#26377;&#24207;&#30340;&#26377;&#24847;&#20041;&#30340;&#23616;&#37096;&#36235;&#21183;&#24207;&#21015;&#30340;&#24212;&#29992;&#29305;&#21035;&#26377;&#29992;&#65292;&#20363;&#22914;&#22312;&#27969;&#34892;&#30149;&#20013;&#65288;&#20174;&#22686;&#38271;&#21040;&#23792;&#20540;&#20877;&#21040;&#20943;&#23569;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;DTW+S&#65292;&#23427;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#8220;&#20445;&#25345;&#25509;&#36817;&#24615;&#8221;&#30340;&#30697;&#38453;&#34920;&#31034;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#20013;&#27599;&#19968;&#21015;&#20195;&#34920;&#23616;&#37096;&#36235;&#21183;&#65292;&#28982;&#21518;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36825;&#20123;&#30697;&#38453;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25903;&#25345;&#36825;&#31181;&#34920;&#31034;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DTW+S&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring distance or similarity between time-series data is a fundamental aspect of many applications including classification and clustering. Existing measures may fail to capture similarities due to local trends (shapes) and may even produce misleading results. Our goal is to develop a measure that looks for similar trends occurring around similar times and is easily interpretable for researchers in applied domains. This is particularly useful for applications where time-series have a sequence of meaningful local trends that are ordered, such as in epidemics (a surge to an increase to a peak to a decrease). We propose a novel measure, DTW+S, which creates an interpretable "closeness-preserving" matrix representation of the time-series, where each column represents local trends, and then it applies Dynamic Time Warping to compute distances between these matrices. We present a theoretical analysis that supports the choice of this representation. We demonstrate the utility of DTW+S in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#38598;&#25104;SE(3)&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.02685</link><description>&lt;p&gt;
Diffusion-EDFs: &#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation. (arXiv:2309.02685v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#38598;&#25104;SE(3)&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#39564;&#35777;&#20102;&#31561;&#21464;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#26368;&#36817;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#38543;&#26426;&#34892;&#20026;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#23558;&#31354;&#38388;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#21363;SE(3)&#31561;&#21464;&#24615;&#24341;&#20837;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;SE(3)&#31561;&#21464;&#24615;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#26550;&#26500;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#22312;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#26102;&#21482;&#38656;5&#21040;10&#20010;&#20219;&#21153;&#28436;&#31034;&#21363;&#21487;&#12290;&#27492;&#22806;&#65292;&#19982;&#20043;&#21069;&#22522;&#20110;&#25193;&#25955;&#30340;&#25805;&#20316;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have verified that equivariant methods can significantly improve the data efficiency, generalizability, and robustness in robot learning. Meanwhile, denoising diffusion-based generative modeling has recently gained significant attention as a promising approach for robotic manipulation learning from demonstrations with stochastic behaviors. In this paper, we present Diffusion-EDFs, a novel approach that incorporates spatial roto-translation equivariance, i.e., SE(3)-equivariance to diffusion generative modeling. By integrating SE(3)-equivariance into our model architectures, we demonstrate that our proposed method exhibits remarkable data efficiency, requiring only 5 to 10 task demonstrations for effective end-to-end training. Furthermore, our approach showcases superior generalizability compared to previous diffusion-based manipulation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01029</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Explainability for Large Language Models: A Survey. (arXiv:2309.01029v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20869;&#37096;&#26426;&#21046;&#20173;&#28982;&#19981;&#26126;&#30830;&#65292;&#36825;&#31181;&#32570;&#20047;&#36879;&#26126;&#24230;&#20026;&#19979;&#28216;&#24212;&#29992;&#24102;&#26469;&#20102;&#19981;&#24517;&#35201;&#30340;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#21644;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#38416;&#26126;&#23427;&#20204;&#30340;&#34892;&#20026;&#12289;&#38480;&#21046;&#21644;&#31038;&#20250;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#27010;&#36848;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#26681;&#25454;LLMs&#30340;&#35757;&#32451;&#33539;&#24335;&#23558;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#65306;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#21644;&#25552;&#31034;&#33539;&#24335;&#12290;&#23545;&#20110;&#27599;&#20010;&#33539;&#24335;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#29983;&#25104;&#20010;&#20307;&#39044;&#27979;&#30340;&#23616;&#37096;&#35299;&#37322;&#21644;&#25972;&#20307;&#27169;&#22411;&#30693;&#35782;&#30340;&#20840;&#23616;&#35299;&#37322;&#30340;&#30446;&#26631;&#21644;&#20027;&#35201;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#65292;&#20998;&#26512;&#24182;&#21387;&#32553;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15068</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Augmentation Framework for Anomaly Detection. (arXiv:2308.15068v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#65292;&#20998;&#26512;&#24182;&#21387;&#32553;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36890;&#24120;&#34987;&#25972;&#21512;&#21040;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22797;&#21046;&#30495;&#23454;&#19990;&#30028;&#30340;&#24322;&#24120;&#25110;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#24322;&#24120;&#30340;&#26631;&#20934;&#22312;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#20998;&#24067;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#37325;&#26500;&#32593;&#32476;&#35757;&#32451;&#26377;&#36129;&#29486;&#30340;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#21387;&#32553;&#25104;&#20960;&#31181;&#26041;&#27861;&#65292;&#20174;&#32780;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#23558;&#36825;&#20010;&#26694;&#26550;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#26082;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#21448;&#36991;&#20813;&#23545;&#37325;&#26500;&#36807;&#31243;&#24341;&#20837;&#24178;&#25200;&#12290;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#30446;&#26631;&#30456;&#20851;&#25351;&#26631;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation methods are commonly integrated into the training of anomaly detection models. Previous approaches have primarily focused on replicating real-world anomalies or enhancing diversity, without considering that the standard of anomaly varies across different classes, potentially leading to a biased training distribution.This paper analyzes crucial traits of simulated anomalies that contribute to the training of reconstructive networks and condenses them into several methods, thus creating a comprehensive framework by selectively utilizing appropriate combinations.Furthermore, we integrate this framework with a reconstruction-based approach and concurrently propose a split training strategy that alleviates the issue of overfitting while avoiding introducing interference to the reconstruction process. The evaluations conducted on the MVTec anomaly detection dataset demonstrate that our method outperforms the previous state-of-the-art approach, particularly in terms of objec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.01050</link><description>&lt;p&gt;
&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#39118;&#38505;&#35780;&#20272;&#30340;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness. (arXiv:2308.01050v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#21453;&#20107;&#23454;&#27169;&#25311;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#19981;&#21516;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#20013;&#34892;&#20026;&#39118;&#38505;&#12290;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#24182;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21363;&#20351;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#31574;&#30053;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#26377;&#28508;&#21147;&#25552;&#20379;&#35832;&#22810;&#31038;&#20250;&#25928;&#30410;&#65292;&#22914;&#20943;&#23569;&#36947;&#36335;&#20107;&#25925;&#21644;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21382;&#21490;&#25968;&#25454;&#21644;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#37327;&#21270;AVs&#30340;&#39118;&#38505;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;AVs&#22312;&#21508;&#31181;&#25805;&#20316;&#35774;&#35745;&#39046;&#22495;&#65288;ODDs&#65289;&#20013;&#34892;&#20026;&#30340;&#39118;&#38505;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#23545;&#8220;&#19981;&#33391;&#8221;&#36947;&#36335;&#29992;&#25143;&#36827;&#34892;&#21453;&#20107;&#23454;&#27169;&#25311;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#23433;&#20840;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#21487;&#33021;&#23548;&#33268;&#30896;&#25758;&#30340;&#26368;&#23567;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#37327;&#12290;&#35813;&#27010;&#24565;&#26377;&#21161;&#20110;&#25214;&#21040;&#26368;&#20851;&#38190;&#30340;&#24773;&#26223;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#35780;&#20272;AVs&#30340;&#39118;&#38505;&#39057;&#29575;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;AV&#30340;&#34892;&#20026;&#31574;&#30053;&#26159;&#26410;&#30693;&#30340;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#20173;&#28982;&#36866;&#29992;&#20110;&#26368;&#22351;&#21644;&#26368;&#20339;&#24773;&#20917;&#20998;&#26512;&#65292;&#20351;&#35813;&#26041;&#27861;&#23545;&#22806;&#37096;&#31532;&#19977;&#26041;&#39118;&#38505;&#35780;&#20272;&#26426;&#26500;&#20063;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicles (AVs) have the potential to provide numerous societal benefits, such as decreased road accidents and increased overall transportation efficiency. However, quantifying the risk associated with AVs is challenging due to the lack of historical data and the rapidly evolving technology. This paper presents a data-driven framework for comparing the risk of different AVs' behaviors in various operational design domains (ODDs), based on counterfactual simulations of "misbehaving" road users. We introduce the concept of counterfactual safety margin, which represents the minimum deviation from normal behavior that could lead to a collision. This concept helps to find the most critical scenarios but also to assess the frequency and severity of risk of AVs. We show that the proposed methodology is applicable even when the AV's behavioral policy is unknown -- through worst- and best-case analyses -- making the method useful also to external third-party risk assessors. Our experi
&lt;/p&gt;</description></item><item><title>AnyLoc&#23454;&#29616;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;(VPR)&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#21644;&#26080;&#30417;&#30563;&#29305;&#24449;&#32858;&#21512;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.00688</link><description>&lt;p&gt;
AnyLoc:&#26397;&#30528;&#36890;&#29992;&#30340;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
AnyLoc: Towards Universal Visual Place Recognition. (arXiv:2308.00688v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00688
&lt;/p&gt;
&lt;p&gt;
AnyLoc&#23454;&#29616;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;(VPR)&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#21644;&#26080;&#30417;&#30563;&#29305;&#24449;&#32858;&#21512;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#22320;&#28857;&#35782;&#21035;(VPR)&#23545;&#20110;&#26426;&#22120;&#20154;&#23450;&#20301;&#33267;&#20851;&#37325;&#35201;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#26368;&#26377;&#25928;&#30340;VPR&#26041;&#27861;&#26159;&#38024;&#23545;&#29305;&#23450;&#29615;&#22659;&#21644;&#20219;&#21153;&#35774;&#35745;&#30340;&#65306;&#34429;&#28982;&#23427;&#20204;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#65288;&#20027;&#35201;&#26159;&#22478;&#24066;&#39550;&#39542;&#65289;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#65292;&#20351;&#24471;&#22823;&#22810;&#25968;&#26041;&#27861;&#26080;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#31283;&#20581;&#37096;&#32626;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;VPR&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#19968;&#31181;&#22312;&#21508;&#31181;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#65288;&#22478;&#24066;&#12289;&#25143;&#22806;&#12289;&#23460;&#20869;&#12289;&#31354;&#20013;&#12289;&#27700;&#19979;&#21644;&#22320;&#19979;&#29615;&#22659;&#65289;&#20013;&#22343;&#21487;&#36816;&#34892;&#30340;&#25216;&#26415;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;&#29616;&#25104;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#26159;&#26500;&#24314;&#36825;&#31181;&#36890;&#29992;VPR&#35299;&#20915;&#26041;&#26696;&#30340;&#29702;&#24819;&#22522;&#30784;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#27966;&#29983;&#29305;&#24449;&#19982;&#26080;&#30417;&#30563;&#29305;&#24449;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22871;&#20214;AnyLoc&#33021;&#22815;&#23454;&#29616;&#39640;&#36798;4&#20493;&#30340;&#26174;&#33879;&#25552;&#39640;
&lt;/p&gt;
&lt;p&gt;
Visual Place Recognition (VPR) is vital for robot localization. To date, the most performant VPR approaches are environment- and task-specific: while they exhibit strong performance in structured environments (predominantly urban driving), their performance degrades severely in unstructured environments, rendering most approaches brittle to robust real-world deployment. In this work, we develop a universal solution to VPR -- a technique that works across a broad range of structured and unstructured environments (urban, outdoors, indoors, aerial, underwater, and subterranean environments) without any re-training or fine-tuning. We demonstrate that general-purpose feature representations derived from off-the-shelf self-supervised models with no VPR-specific training are the right substrate upon which to build such a universal VPR solution. Combining these derived features with unsupervised feature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X significantly higher 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.05052</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. (arXiv:2307.05052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#24615;&#33021;&#20013;&#65292;&#21508;&#31181;&#28436;&#31034;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26631;&#31614;&#12289;&#36755;&#20837;&#20998;&#24067;&#21644;&#34917;&#20805;&#35299;&#37322;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#36825;&#20123;&#22240;&#32032;&#34987;&#20462;&#25913;&#25110;&#25200;&#21160;&#26102;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22522;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#24037;&#20316;&#23545;&#20110;&#36825;&#20123;&#20803;&#32032;&#22914;&#20309;&#24433;&#21709;ICL&#32473;&#20986;&#20102;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21487;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(XNLP)&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#23545;&#27604;&#28436;&#31034;&#30340;&#26174;&#33879;&#24615;&#22270;&#36827;&#34892;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#23545;&#36755;&#20837;&#20998;&#24067;&#36827;&#34892;&#20102;&#31890;&#24230;&#32423;&#21035;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;ICL&#26041;&#38754;&#30340;&#25928;&#26524;&#26159;&#23384;&#22312;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the role of various demonstration components in the in-context learning (ICL) performance of large language models (LLMs). Specifically, we explore the impacts of ground-truth labels, input distribution, and complementary explanations, particularly when these are altered or perturbed. We build on previous work, which offers mixed findings on how these elements influence ICL. To probe these questions, we employ explainable NLP (XNLP) methods and utilize saliency maps of contrastive demonstrations for both qualitative and quantitative analysis. Our findings reveal that flipping ground-truth labels significantly affects the saliency, though it's more noticeable in larger LLMs. Our analysis of the input distribution at a granular level reveals that changing sentiment-indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground-truth labels. Finally, we find that the effectiveness of complementary explanations in boos
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#26426;&#22120;&#20154;&#36229;&#22768;&#21307;&#29983;&#65292;&#36890;&#36807;&#23398;&#20064;&#19987;&#23478;&#30340;&#32463;&#39564;&#65292;&#33258;&#20027;&#22320;&#25511;&#21046;&#36229;&#22768;&#25506;&#22836;&#26469;&#36827;&#34892;&#29983;&#29289;&#27979;&#37327;&#21644;&#20869;&#37096;&#22120;&#23448;&#35786;&#26029;&#12290;&#20351;&#29992;&#20114;&#20449;&#24687;&#26469;&#25552;&#21462;&#20219;&#21153;&#30456;&#20851;&#21644;&#39046;&#22495;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#22870;&#21169;&#20989;&#25968;&#25512;&#26029;&#20986;&#19987;&#23478;&#30340;&#29983;&#29702;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.03705</link><description>&lt;p&gt;
&#26234;&#33021;&#26426;&#22120;&#20154;&#36229;&#22768;&#21307;&#29983;&#65306;&#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#23569;&#31034;&#33539;&#35299;&#32806;&#22870;&#21169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Intelligent Robotic Sonographer: Mutual Information-based Disentangled Reward Learning from Few Demonstrations. (arXiv:2307.03705v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#26426;&#22120;&#20154;&#36229;&#22768;&#21307;&#29983;&#65292;&#36890;&#36807;&#23398;&#20064;&#19987;&#23478;&#30340;&#32463;&#39564;&#65292;&#33258;&#20027;&#22320;&#25511;&#21046;&#36229;&#22768;&#25506;&#22836;&#26469;&#36827;&#34892;&#29983;&#29289;&#27979;&#37327;&#21644;&#20869;&#37096;&#22120;&#23448;&#35786;&#26029;&#12290;&#20351;&#29992;&#20114;&#20449;&#24687;&#26469;&#25552;&#21462;&#20219;&#21153;&#30456;&#20851;&#21644;&#39046;&#22495;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#22870;&#21169;&#20989;&#25968;&#25512;&#26029;&#20986;&#19987;&#23478;&#30340;&#29983;&#29702;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#65288;US&#65289;&#25104;&#20687;&#30001;&#20110;&#23454;&#26102;&#24615;&#21644;&#26080;&#36752;&#23556;&#30340;&#20248;&#21183;&#32780;&#24191;&#27867;&#29992;&#20110;&#29983;&#29289;&#27979;&#37327;&#21644;&#20869;&#37096;&#22120;&#23448;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#27700;&#24179;&#30340;&#25805;&#20316;&#32773;&#21487;&#21464;&#24615;&#65292;&#32467;&#26524;&#22270;&#20687;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25805;&#20316;&#32773;&#30340;&#32463;&#39564;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#26426;&#22120;&#20154;&#36229;&#22768;&#21307;&#29983;&#65292;&#36890;&#36807;&#20174;&#19987;&#23478;&#37027;&#37324;&#23398;&#20064;&#65292;&#33258;&#20027;&#22320;&#8220;&#25506;&#32034;&#8221;&#30446;&#26631;&#35299;&#21078;&#23398;&#24182;&#23558;&#36229;&#22768;&#25506;&#22836;&#23548;&#33322;&#21040;&#30456;&#20851;&#30340;2D&#24179;&#38754;&#12290;&#19987;&#23478;&#30340;&#24213;&#23618;&#39640;&#32423;&#29983;&#29702;&#30693;&#35782;&#36890;&#36807;&#31070;&#32463;&#22870;&#21169;&#20989;&#25968;&#25512;&#26029;&#20986;&#26469;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#25490;&#21517;&#23545;&#27604;&#22270;&#20687;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#31216;&#20026;&#29702;&#35299; &#8220;&#22768;&#23398;&#35821;&#35328;&#8221;&#12290;&#32771;&#34385;&#21040;&#20811;&#26381;&#24739;&#32773;&#38388;&#21464;&#24322;&#24615;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32593;&#32476;&#36890;&#36807;&#20272;&#35745;&#20114;&#20449;&#24687;&#26469;&#26174;&#24335;&#25552;&#21462;&#28508;&#22312;&#31354;&#38388;&#20013;&#19982;&#20219;&#21153;&#30456;&#20851;&#21644;&#39046;&#22495;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#39640;&#26031;&#20998;&#24067;&#30340;&#28388;&#27874;&#22120;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#21644;&#37319;&#21462;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultrasound (US) imaging is widely used for biometric measurement and diagnosis of internal organs due to the advantages of being real-time and radiation-free. However, due to high inter-operator variability, resulting images highly depend on operators' experience. In this work, an intelligent robotic sonographer is proposed to autonomously "explore" target anatomies and navigate a US probe to a relevant 2D plane by learning from expert. The underlying high-level physiological knowledge from experts is inferred by a neural reward function, using a ranked pairwise image comparisons approach in a self-supervised fashion. This process can be referred to as understanding the "language of sonography". Considering the generalization capability to overcome inter-patient variations, mutual information is estimated by a network to explicitly extract the task-related and domain features in latent space. Besides, a Gaussian distribution-based filter is developed to automatically evaluate and take 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2306.16772</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#30340;&#20154;&#31867;&#22242;&#38431;&#27963;&#21160;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Synthetic Human Group Activities. (arXiv:2306.16772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16772
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#23545;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#30340;&#29702;&#35299;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#20219;&#21153;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#33719;&#21462;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;M3Act&#37319;&#29992;Unity&#24341;&#25806;&#39537;&#21160;&#65292;&#21253;&#21547;&#21487;&#20379;&#20223;&#30495;&#20351;&#29992;&#30340;&#19977;&#32500;&#22330;&#26223;&#21644;&#20154;&#29289;&#36164;&#28304;&#65292;&#21487;&#37197;&#32622;&#30340;&#29031;&#26126;&#21644;&#25668;&#20687;&#31995;&#32479;&#65292;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22359;&#21270;&#22242;&#38431;&#27963;&#21160;&#65292;&#20197;&#21450;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20855;&#26377;&#22823;&#37327;&#39046;&#22495;&#38543;&#26426;&#21270;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#35270;&#22270;&#12289;&#27169;&#24577;&#65288;RGB&#22270;&#20687;&#12289;2D&#23039;&#21183;&#12289;3D&#21160;&#20316;&#65289;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65288;2D&#36793;&#30028;&#26694;&#12289;&#23454;&#20363;&#20998;&#21106;&#25513;&#27169;&#12289;&#20010;&#20307;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#31867;&#21035;&#65289;&#12290;&#21033;&#29992;M3Act&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#32531;&#35299;&#39068;&#33394;&#20559;&#31227;&#30340;&#35745;&#31639;&#24265;&#20215;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#32469;&#36807;&#36830;&#25509;&#26469;&#25913;&#21892;&#29983;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#22343;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.15832</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#32531;&#35299;&#39068;&#33394;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Easing Color Shifts in Score-Based Diffusion Models. (arXiv:2306.15832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#32531;&#35299;&#39068;&#33394;&#20559;&#31227;&#30340;&#35745;&#31639;&#24265;&#20215;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#32469;&#36807;&#36830;&#25509;&#26469;&#25913;&#21892;&#29983;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24471;&#20998;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#21487;&#33021;&#20250;&#22240;&#31354;&#38388;&#22343;&#20540;&#30340;&#38169;&#35823;&#32780;&#20986;&#29616;&#39068;&#33394;&#20559;&#31227;&#65292;&#36825;&#31181;&#25928;&#24212;&#22312;&#36739;&#22823;&#30340;&#22270;&#20687;&#20013;&#20250;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#24265;&#20215;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20943;&#36731;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#39068;&#33394;&#20559;&#31227;&#12290;&#25105;&#20204;&#22312;&#24471;&#20998;&#32593;&#32476;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#32469;&#36807;&#36830;&#25509;&#65292;&#29992;&#20110;&#22788;&#29702;&#36755;&#20837;&#30340;&#31354;&#38388;&#22343;&#20540;&#65292;&#24182;&#39044;&#27979;&#24471;&#20998;&#20989;&#25968;&#30340;&#22343;&#20540;&#12290;&#36825;&#31181;&#32593;&#32476;&#26550;&#26500;&#26174;&#33879;&#25913;&#21892;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#31354;&#38388;&#22343;&#20540;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25913;&#36827;&#19982;&#29983;&#25104;&#22270;&#20687;&#22823;&#23567;&#30340;&#20851;&#31995;&#36817;&#20284;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20026;&#36328;&#22270;&#20687;&#23610;&#23544;&#30340;&#39068;&#33394;&#20559;&#31227;&#38382;&#39064;&#25552;&#20379;&#20102;&#30456;&#23545;&#24265;&#20215;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#29702;&#24819;&#21270;&#24773;&#20917;&#19979;&#39068;&#33394;&#20559;&#31227;&#30340;&#36215;&#28304;&#65292;&#20197;&#25512;&#21160;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25552;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generated images of score-based models can suffer from errors in their spatial means, an effect, referred to as a color shift, which grows for larger images. This paper introduces a computationally inexpensive solution to mitigate color shifts in score-based diffusion models. We propose a simple nonlinear bypass connection in the score network, designed to process the spatial mean of the input and to predict the mean of the score function. This network architecture substantially improves the resulting spatial means of the generated images, and we show that the improvement is approximately independent of the size of the generated images. As a result, our solution offers a comparatively inexpensive solution for the color shift problem across image sizes. Lastly, we discuss the origin of color shifts in an idealized setting in order to motivate our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;SLDAS&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.19891</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces. (arXiv:2305.19891v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;SLDAS&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;LDAS&#65289;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22788;&#29702;&#22810;&#36798;&#20960;&#30334;&#19975;&#20010;&#21160;&#20316;&#30340;&#38750;&#32467;&#26500;&#21270;LDAS&#12290;&#28982;&#32780;&#65292;&#22312;&#29289;&#27969;&#12289;&#29983;&#20135;&#21644;&#36816;&#36755;&#31995;&#32479;&#31561;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#21160;&#20316;&#31354;&#38388;&#20855;&#26377;&#32452;&#21512;&#32467;&#26500;&#65292;&#20854;&#35268;&#27169;&#29978;&#33267;&#22312;&#23567;&#35268;&#27169;&#23454;&#20363;&#19978;&#20063;&#36229;&#36807;&#20102;&#25968;&#30334;&#19975;&#20010;&#21160;&#20316;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#21160;&#20316;&#31354;&#38388;&#21576;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#20363;&#22914;&#31561;&#38388;&#36317;&#30340;&#31163;&#25955;&#36164;&#28304;&#21333;&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22788;&#29702;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;LDAS&#65288;SLDAS&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#29992;&#20110;SLDAS&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#22312;&#20855;&#26377;&#39640;&#36798;$10^{73}$&#20010;&#21160;&#20316;&#30340;&#32467;&#26500;&#21270;&#21160;&#20316;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#19977;&#20010;&#26631;&#26438;&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large discrete action spaces (LDAS) remain a central challenge in reinforcement learning. Existing solution approaches can handle unstructured LDAS with up to a few million actions. However, many real-world applications in logistics, production, and transportation systems have combinatorial action spaces, whose size grows well beyond millions of actions, even on small instances. Fortunately, such action spaces exhibit structure, e.g., equally spaced discrete resource units. With this work, we focus on handling structured LDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we propose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm for SLDAS. We present a scalable neighborhood exploration heuristic that utilizes this paradigm and efficiently explores the discrete neighborhood around the continuous proxy action in structured action spaces with up to $10^{73}$ actions. We demonstrate the performance of our method by benchmarking it against three sta
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18381</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#30719;&#30707;&#20013;&#25552;&#28860;&#40644;&#37329;: &#22522;&#20110;&#20851;&#38190;&#26679;&#26412;&#36873;&#25321;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18381
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25928;&#29575;&#23398;&#20064;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25317;&#26377;&#22823;&#37327;&#22810;&#27169;&#22411;&#30340;&#29616;&#22312;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#25104;&#20026;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#36807;&#31243;&#26412;&#36523;&#20173;&#28982;&#38750;&#24120;&#20302;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#29992;&#20449;&#24687;&#29702;&#35770;&#26469;&#24314;&#27169;&#33976;&#39311;&#38382;&#39064;&#65292;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#25454;&#20887;&#20313;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20197;&#20415;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#26679;&#26412;&#36873;&#25321;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#20248;&#21270;&#36807;&#31243;&#12290;&#36825;&#31181;&#26032;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#25193;&#22823;&#29616;&#26377;&#31639;&#27861;&#33539;&#22260;&#20197;&#23545;&#26356;&#24222;&#22823;&#21644;&#22810;&#20803;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#20363;&#22914;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;0.04&#65285;&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#36275;&#20197;&#20445;&#25345;&#21487;&#27604;&#30340;&#33976;&#39311;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20854;&#36129;&#29486;&#21487;&#33021;&#20026;&#33976;&#39311;&#36807;&#31243;&#30340;&#21160;&#21147;&#23398;&#24320;&#36767;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-efficient learning has drawn significant attention, especially given the current trend of large multi-modal models, where dataset distillation can be an effective solution. However, the dataset distillation process itself is still very inefficient. In this work, we model the distillation problem with reference to information theory. Observing that severe data redundancy exists in dataset distillation, we argue to put more emphasis on the utility of the training samples. We propose a family of methods to exploit the most valuable samples, which is validated by our comprehensive analysis of the optimal data selection. The new strategy significantly reduces the training cost and extends a variety of existing distillation algorithms to larger and more diversified datasets, e.g. in some cases only 0.04% training data is sufficient for comparable distillation performance. Moreover, our strategy consistently enhances the performance, which may open up new analyses on the dynamics of dist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#21512;&#39640;&#26031;&#20808;&#39564;&#20998;&#24067;&#30340;&#36845;&#20195;&#31639;&#27861;&#21644;&#23545;&#24212;&#20110;&#20854;&#36845;&#20195;&#36807;&#31243;&#8220;&#23637;&#24320;&#8221;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11120</link><description>&lt;p&gt;
&#35299;&#20915;&#32447;&#24615;&#21453;&#38382;&#39064;&#30340;&#22797;&#21512;&#39640;&#26031;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Compound Gaussian Network for Solving Linear Inverse Problems. (arXiv:2305.11120v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#21512;&#39640;&#26031;&#20808;&#39564;&#20998;&#24067;&#30340;&#36845;&#20195;&#31639;&#27861;&#21644;&#23545;&#24212;&#20110;&#20854;&#36845;&#20195;&#36807;&#31243;&#8220;&#23637;&#24320;&#8221;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#20687;&#37325;&#24314;&#21644;&#21387;&#32553;&#24863;&#30693;&#20013;&#20986;&#29616;&#30340;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#36845;&#20195;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22522;&#20110;&#22797;&#21512;&#39640;&#26031;&#20808;&#39564;&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#30446;&#26631;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#22797;&#21512;&#39640;&#26031;&#20808;&#39564;&#21253;&#21547;&#20102;&#35768;&#22810;&#24120;&#29992;&#30340;&#22270;&#20687;&#37325;&#24314;&#20808;&#39564;&#65292;&#20363;&#22914;&#31232;&#30095;&#24615;&#26041;&#27861;&#31561;&#12290;&#36825;&#20010;&#36845;&#20195;&#31639;&#27861;&#20026;&#26412;&#25991;&#30340;&#31532;&#20108;&#31181;&#26032;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#21363;&#19968;&#20010;&#23545;&#24212;&#20110;&#36845;&#20195;&#36807;&#31243;&#8220;&#23637;&#24320;&#8221;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#23637;&#24320;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#21487;&#35299;&#37322;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#19988;&#20248;&#20110;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#21253;&#25324;&#35814;&#32454;&#30340;&#35745;&#31639;&#29702;&#35770;&#65292;&#28145;&#20837;&#25506;&#31350;&#20102;&#20004;&#31181;&#31639;&#27861;&#30340;&#26500;&#36896;&#21644;&#24615;&#33021;&#12290;&#26368;&#32456;&#32467;&#35770;&#26159;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#32447;&#24615;&#21453;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For solving linear inverse problems, particularly of the type that appear in tomographic imaging and compressive sensing, this paper develops two new approaches. The first approach is an iterative algorithm that minimizers a regularized least squares objective function where the regularization is based on a compound Gaussian prior distribution. The Compound Gaussian prior subsumes many of the commonly used priors in image reconstruction, including those of sparsity-based approaches. The developed iterative algorithm gives rise to the paper's second new approach, which is a deep neural network that corresponds to an "unrolling" or "unfolding" of the iterative algorithm. Unrolled deep neural networks have interpretable layers and outperform standard deep learning methods. This paper includes a detailed computational theory that provides insight into the construction and performance of both algorithms. The conclusion is that both algorithms outperform other state-of-the-art approaches to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20869;&#23481;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#21487;&#29983;&#25104;&#39640;&#21487;&#36716;&#31227;&#30340;&#35270;&#35273;&#36924;&#30495;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#33021;&#26377;&#25928;&#25915;&#20987;&#21508;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2305.10665</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Content-based Unrestricted Adversarial Attack. (arXiv:2305.10665v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10665
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20869;&#23481;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#21487;&#29983;&#25104;&#39640;&#21487;&#36716;&#31227;&#30340;&#35270;&#35273;&#36924;&#30495;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#33021;&#26377;&#25928;&#25915;&#20987;&#21508;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#25915;&#20987;&#20250;&#25805;&#32437;&#22270;&#20687;&#30340;&#35821;&#20041;&#20869;&#23481;&#65288;&#22914;&#39068;&#33394;&#25110;&#32441;&#29702;&#65289;&#26469;&#21019;&#24314;&#26082;&#26377;&#25928;&#21448;&#36924;&#30495;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#20855;&#26377;&#27450;&#39575;&#20154;&#31867;&#24863;&#30693;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34109;&#24615;&#21644;&#25104;&#21151;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20316;&#21697;&#36890;&#24120;&#20250;&#29306;&#29298;&#19981;&#21463;&#38480;&#21046;&#30340;&#31243;&#24230;&#24182;&#20027;&#35266;&#22320;&#36873;&#25321;&#19968;&#20123;&#22270;&#20687;&#20869;&#23481;&#26469;&#20445;&#35777;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#36924;&#30495;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;&#20026;&#20445;&#35777;&#23545;&#25239;&#26679;&#26412;&#30340;&#36924;&#30495;&#24615;&#21644;&#25552;&#39640;&#25915;&#20987;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#21463;&#38480;&#21046;&#25915;&#20987;&#26694;&#26550;&#65292;&#31216;&#20026;&#22522;&#20110;&#20869;&#23481;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#34920;&#31034;&#33258;&#28982;&#22270;&#20687;&#30340;&#20302;&#32500;&#27969;&#24418;&#23558;&#22270;&#20687;&#26144;&#23556;&#21040;&#27969;&#24418;&#19978;&#65292;&#24182;&#27839;&#30528;&#20854;&#23545;&#25239;&#26041;&#21521;&#36827;&#34892;&#20248;&#21270;&#12290;&#22240;&#27492;&#65292;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#23545;&#25239;&#20869;&#23481;&#25915;&#20987;&#65292;&#24182;&#29983;&#25104;&#20102;&#39640;&#21487;&#36716;&#31227;&#30340;&#36924;&#30495;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#21487;&#20197;&#26377;&#25928;&#25915;&#20987;&#21508;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unrestricted adversarial attacks typically manipulate the semantic content of an image (e.g., color or texture) to create adversarial examples that are both effective and photorealistic, demonstrating their ability to deceive human perception and deep neural networks with stealth and success. However, current works usually sacrifice unrestricted degrees and subjectively select some image content to guarantee the photorealism of unrestricted adversarial examples, which limits its attack performance. To ensure the photorealism of adversarial examples and boost attack performance, we propose a novel unrestricted attack framework called Content-based Unrestricted Adversarial Attack. By leveraging a low-dimensional manifold that represents natural images, we map the images onto the manifold and optimize them along its adversarial direction. Therefore, within this framework, we implement Adversarial Content Attack based on Stable Diffusion and can generate high transferable unrestricted adve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10361</link><description>&lt;p&gt;
&#38750;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#65306;&#22522;&#20110;&#27169;&#25311;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Human Choice Prediction in Non-Cooperative Games: Simulation-based Off-Policy Evaluation. (arXiv:2305.10361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#26381;&#28216;&#25103;&#22312;&#32463;&#27982;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#35821;&#35328;&#30340;&#35828;&#26381;&#28216;&#25103;&#20013;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#20154;&#31867; - &#26426;&#22120;&#20154;&#20132;&#20114;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#30495;&#23454;&#20132;&#20114;&#21644;&#27169;&#25311;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persuasion games have been fundamental in economics and AI research, and have significant practical applications. Recent works in this area have started to incorporate natural language, moving beyond the traditional stylized message setting. However, previous research has focused on on-policy prediction, where the train and test data have the same distribution, which is not representative of real-life scenarios. In this paper, we tackle the challenging problem of off-policy evaluation (OPE) in language-based persuasion games. To address the inherent difficulty of human data collection in this setup, we propose a novel approach which combines real and simulated human-bot interaction data. Our simulated data is created by an exogenous model assuming decision makers (DMs) start with a mixture of random and decision-theoretic based behaviors and improve over time. We present a deep learning training algorithm that effectively integrates real interaction and simulated data, substantially im
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#12289;&#20302;&#25104;&#26412;&#12289;&#21487;&#25193;&#23637;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#36793;&#32536;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23460;&#20869;&#31354;&#38388;&#20013;&#20272;&#35745;&#22810;&#20154;&#30340;&#20301;&#32622;&#12289;&#26397;&#21521;&#21644;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2305.05062</link><description>&lt;p&gt;
&#19981;&#20844;&#24320;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#30456;&#26426;&#32593;&#32476;&#21644;&#36793;&#32536;&#35745;&#31639;&#22312;&#23460;&#20869;&#23450;&#20301;&#21644;&#22810;&#20154;&#36319;&#36394;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Indoor Localization and Multi-person Tracking Using Privacy Preserving Distributed Camera Network with Edge Computing. (arXiv:2305.05062v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#12289;&#20302;&#25104;&#26412;&#12289;&#21487;&#25193;&#23637;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#36793;&#32536;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23460;&#20869;&#31354;&#38388;&#20013;&#20272;&#35745;&#22810;&#20154;&#30340;&#20301;&#32622;&#12289;&#26397;&#21521;&#21644;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24314;&#31569;&#29615;&#22659;&#20013;&#23450;&#20301;&#20010;&#20307;&#26159;&#19968;&#20010;&#26085;&#30410;&#25104;&#38271;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#36890;&#36807;&#20272;&#35745;&#20154;&#20204;&#22312;&#31354;&#38388;&#20013;&#30340;&#20301;&#32622;&#12289;&#38754;&#37096;&#26397;&#21521;&#65288;&#25110;&#20957;&#35270;&#26041;&#21521;&#65289;&#21644;&#36712;&#36857;&#65292;&#21487;&#20197;&#26377;&#35768;&#22810;&#29992;&#36884;&#65292;&#20363;&#22914;&#22312;&#20154;&#32676;&#31649;&#29702;&#12289;&#23433;&#20840;&#21644;&#21307;&#30103;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#12289;&#20302;&#25104;&#26412;&#12289;&#21487;&#25193;&#23637;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#36793;&#32536;&#35745;&#31639;&#26694;&#26550;&#29992;&#20110;&#22810;&#20154;&#23450;&#20301;&#65292;&#21363;&#22312;&#23460;&#20869;&#31354;&#38388;&#20013;&#20272;&#35745;&#22810;&#20154;&#30340;&#20301;&#32622;&#12289;&#26397;&#21521;&#21644;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Localization of individuals in a built environment is a growing research topic. Estimating the positions, face orientation (or gaze direction) and trajectories of people through space has many uses, such as in crowd management, security, and healthcare. In this work, we present an open-source, low-cost, scalable and privacy-preserving edge computing framework for multi-person localization, i.e. estimating the positions, orientations, and trajectories of multiple people in an indoor space. Our computing framework consists of 38 Tensor Processing Unit (TPU)-enabled edge computing camera systems placed in the ceiling of the indoor therapeutic space. The edge compute systems are connected to an on-premise fog server through a secure and private network. A multi-person detection algorithm and a pose estimation model run on the edge TPU in real-time to collect features which are used, instead of raw images, for downstream computations. This ensures the privacy of individuals in the space, re
&lt;/p&gt;</description></item><item><title>NeuroBack&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65292;&#20351;&#24471;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2110.14053</link><description>&lt;p&gt;
NeuroBack: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks. (arXiv:2110.14053v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.14053
&lt;/p&gt;
&lt;p&gt;
NeuroBack&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;CDCL SAT&#27714;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65292;&#20351;&#24471;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#26159;&#19968;&#20010;&#24433;&#21709;&#21040;&#35268;&#21010;&#12289;&#39564;&#35777;&#21644;&#23433;&#20840;&#31561;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#30340;NP&#23436;&#20840;&#38382;&#39064;&#12290;&#20027;&#27969;&#30340;&#29616;&#20195;SAT&#27714;&#35299;&#22120;&#22522;&#20110;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#65288;CDCL&#65289;&#31639;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22686;&#24378;CDCL SAT&#27714;&#35299;&#22120;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#20040;&#27809;&#26377;&#20351;&#27714;&#35299;&#26356;&#21152;&#26377;&#25928;&#65292;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#36164;&#28304;&#36827;&#34892;&#39057;&#32321;&#30340;&#22312;&#32447;&#27169;&#22411;&#25512;&#26029;&#12290;&#20026;&#20102;&#20351;GNN&#30340;&#25913;&#36827;&#21464;&#24471;&#23454;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroBack&#30340;&#26041;&#27861;&#65292;&#23427;&#24314;&#31435;&#22312;&#20004;&#20010;&#27934;&#23519;&#19978;&#65306;&#65288;1&#65289;&#39044;&#27979;&#20986;&#29616;&#22312;&#22823;&#22810;&#25968;&#65288;&#29978;&#33267;&#20840;&#37096;&#65289;&#28385;&#36275;&#36171;&#20540;&#20013;&#30340;&#21464;&#37327;&#30340;&#38454;&#27573;&#65288;&#21363;&#20540;&#65289;&#23545;&#20110;CDCL SAT&#27714;&#35299;&#33267;&#20851;&#37325;&#35201;&#65292;&#65288;2&#65289;&#22312;SAT&#27714;&#35299;&#24320;&#22987;&#20043;&#21069;&#65292;&#21482;&#38656;&#26597;&#35810;&#19968;&#27425;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21363;&#21487;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#31163;&#32447;&#27169;&#22411;&#25512;&#26029;&#20351;NeuroBack&#33021;&#22815;&#20165;&#22312;CPU&#19978;&#25191;&#34892;&#65292;&#28040;&#38500;&#20102;&#23545;GPU&#36164;&#28304;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Propositional satisfiability (SAT) is an NP-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural Networks (GNNs). However, so far this approach either has not made solving more effective, or required substantial GPU resources for frequent online model inferences. Aiming to make GNN improvements practical, this paper proposes an approach called NeuroBack, which builds on two insights: (1) predicting phases (i.e., values) of variables appearing in the majority (or even all) of the satisfying assignments are essential for CDCL SAT solving, and (2) it is sufficient to query the neural model only once for the predictions before the SAT solving starts. Once trained, the offline model inference allows NeuroBack to execute exclusively on the CPU, removing its reliance on GPU resources. To t
&lt;/p&gt;</description></item></channel></rss>