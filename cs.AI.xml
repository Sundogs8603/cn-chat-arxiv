<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23618;&#27425;&#31574;&#30053;&#65292;&#36890;&#36807;&#37322;&#25918;&#20854;&#21019;&#36896;&#28508;&#21147;&#65292;&#25506;&#32034;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;LLMs&#20998;&#20026;&#39046;&#23548;&#32773;&#21644;&#25191;&#34892;&#32773;&#65292;&#39046;&#23548;&#32773;&#25552;&#20379;&#22810;&#31181;&#39640;&#32423;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#20316;&#20026;&#25552;&#31034;&#65292;&#25191;&#34892;&#32773;&#26681;&#25454;&#39046;&#23548;&#32773;&#30340;&#25351;&#24341;&#25191;&#34892;&#35814;&#32454;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#29983;&#25104;&#19968;&#32452;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2311.00694</link><description>&lt;p&gt;
&#35299;&#25918;&#21019;&#36896;&#21147;&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23618;&#27425;&#31574;&#30053;&#20197;&#25913;&#36827;&#25361;&#25112;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving. (arXiv:2311.00694v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23618;&#27425;&#31574;&#30053;&#65292;&#36890;&#36807;&#37322;&#25918;&#20854;&#21019;&#36896;&#28508;&#21147;&#65292;&#25506;&#32034;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;LLMs&#20998;&#20026;&#39046;&#23548;&#32773;&#21644;&#25191;&#34892;&#32773;&#65292;&#39046;&#23548;&#32773;&#25552;&#20379;&#22810;&#31181;&#39640;&#32423;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#20316;&#20026;&#25552;&#31034;&#65292;&#25191;&#34892;&#32773;&#26681;&#25454;&#39046;&#23548;&#32773;&#30340;&#25351;&#24341;&#25191;&#34892;&#35814;&#32454;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#29983;&#25104;&#19968;&#32452;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#38382;&#39064;&#20013;&#24448;&#24448;&#36935;&#21040;&#22256;&#38590;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#26679;&#25110;&#25628;&#32034;&#35814;&#32454;&#21644;&#20302;&#32423;&#30340;&#25512;&#29702;&#38142;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25506;&#32034;&#33021;&#21147;&#19978;&#20173;&#28982;&#26377;&#38480;&#65292;&#20351;&#24471;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#24222;&#22823;&#30340;&#35299;&#31354;&#38388;&#20013;&#24456;&#38590;&#31361;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;LLMs&#20316;&#20026;&#23618;&#27425;&#31574;&#30053;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#37322;&#25918;LLMs&#25506;&#32034;&#22810;&#26679;&#21270;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#30340;&#21019;&#36896;&#28508;&#21147;&#12290;&#35813;&#31574;&#30053;&#21253;&#25324;&#19968;&#20010;&#26377;&#36828;&#35265;&#30340;&#39046;&#23548;&#32773;&#65292;&#25552;&#20986;&#22810;&#31181;&#22810;&#26679;&#30340;&#39640;&#32423;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#26377;&#19968;&#20010;&#25191;&#34892;&#32773;&#65292;&#26681;&#25454;&#27599;&#20010;&#39640;&#32423;&#25351;&#20196;&#25191;&#34892;&#35814;&#32454;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#12290;&#25191;&#34892;&#32773;&#23558;&#39046;&#23548;&#32773;&#30340;&#27599;&#20010;&#25351;&#20196;&#20316;&#20026;&#25351;&#21335;&#65292;&#24182;&#37319;&#26679;&#22810;&#20010;&#25512;&#29702;&#38142;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20026;&#27599;&#20010;&#39046;&#23548;&#32773;&#30340;&#25552;&#35758;&#29983;&#25104;&#19968;&#32452;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved tremendous progress, yet they still often struggle with challenging reasoning problems. Current approaches address this challenge by sampling or searching detailed and low-level reasoning chains. However, these methods are still limited in their exploration capabilities, making it challenging for correct solutions to stand out in the huge solution space. In this work, we unleash LLMs' creative potential for exploring multiple diverse problem solving strategies by framing an LLM as a hierarchical policy via in-context learning. This policy comprises of a visionary leader that proposes multiple diverse high-level problem-solving tactics as hints, accompanied by a follower that executes detailed problem-solving processes following each of the high-level instruction. The follower uses each of the leader's directives as a guide and samples multiple reasoning chains to tackle the problem, generating a solution group for each leader proposal. Additio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#23454;&#20307;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20010;&#24615;&#21270;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23454;&#20307;&#32423;&#21035;&#23569;&#26679;&#26412;&#24773;&#26223;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.00693</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#23454;&#20307;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval. (arXiv:2311.00693v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#23454;&#20307;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20010;&#24615;&#21270;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23454;&#20307;&#32423;&#21035;&#23569;&#26679;&#26412;&#24773;&#26223;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#23454;&#20307;&#26816;&#32034;(VDER)&#26159;&#19968;&#31181;&#20174;&#21457;&#31080;&#21644;&#25910;&#25454;&#31561;&#25991;&#26723;&#22270;&#20687;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;(&#22914;&#26085;&#26399;&#12289;&#22320;&#22336;)&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#22312;&#24037;&#19994;&#32423;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19981;&#26029;&#28044;&#29616;&#30340;&#26032;&#30340;&#25991;&#26723;&#31867;&#22411;&#65292;&#27599;&#31181;&#31867;&#22411;&#37117;&#26377;&#20854;&#29420;&#29305;&#30340;&#23454;&#20307;&#31867;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#35768;&#22810;&#25991;&#26723;&#21253;&#21547;&#20102;&#20165;&#20986;&#29616;&#20960;&#27425;&#30340;&#26410;&#30693;&#23454;&#20307;&#31867;&#22411;&#12290;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#38656;&#35201;&#27169;&#22411;&#20855;&#26377;&#23569;&#26679;&#26412;&#23398;&#20064;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#23569;&#26679;&#26412;VDER&#24037;&#20316;&#20027;&#35201;&#22312;&#25991;&#26723;&#32423;&#21035;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#20840;&#23616;&#23454;&#20307;&#31354;&#38388;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;&#23454;&#20307;&#32423;&#21035;&#30340;&#23569;&#26679;&#26412;&#24773;&#26223;&#65306;&#30446;&#26631;&#23454;&#20307;&#31867;&#22411;&#30001;&#27599;&#20010;&#20219;&#21153;&#36827;&#34892;&#26412;&#22320;&#20010;&#24615;&#21270;&#65292;&#24182;&#19988;&#23454;&#20307;&#20986;&#29616;&#22312;&#25991;&#26723;&#20013;&#24046;&#24322;&#24456;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#26410;&#24320;&#21457;&#30340;&#24773;&#20917;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#20307;&#32423;&#21035;&#23569;&#26679;&#26412;VDER&#20219;&#21153;&#12290;&#25361;&#25112;&#22312;&#20110;&#27599;&#20010;&#20219;&#21153;&#30340;&#26631;&#31614;&#31354;&#38388;&#30340;&#21807;&#19968;&#24615;&#21644;&#22686;&#38271;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually-rich document entity retrieval (VDER), which extracts key information (e.g. date, address) from document images like invoices and receipts, has become an important topic in industrial NLP applications. The emergence of new document types at a constant pace, each with its unique entity types, presents a unique challenge: many documents contain unseen entity types that occur only a couple of times. Addressing this challenge requires models to have the ability of learning entities in a few-shot manner. However, prior works for Few-shot VDER mainly address the problem at the document level with a predefined global entity space, which doesn't account for the entity-level few-shot scenario: target entity types are locally personalized by each task and entity occurrences vary significantly among documents. To address this unexplored scenario, this paper studies a novel entity-level few-shot VDER task. The challenges lie in the uniqueness of the label space for each task and the incre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#12289;&#29983;&#25104;&#27807;&#36890;&#20505;&#36873;&#20197;&#21450;&#27169;&#25311;&#21463;&#20247;&#21453;&#24212;&#65292;&#26469;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;&#12290;&#36890;&#36807;&#35780;&#20272;&#20843;&#20010;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#22522;&#26412;&#36807;&#31243;&#30340;&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00687</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#21463;&#20247;&#32676;&#20307;&#65292;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#12289;&#29983;&#25104;&#27807;&#36890;&#20505;&#36873;&#20197;&#21450;&#27169;&#25311;&#21463;&#20247;&#21453;&#24212;&#65292;&#26469;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;&#12290;&#36890;&#36807;&#35780;&#20272;&#20843;&#20010;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#22522;&#26412;&#36807;&#31243;&#30340;&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#19982;&#20182;&#20154;&#36827;&#34892;&#27807;&#36890;&#20197;&#23454;&#29616;&#33258;&#24049;&#30340;&#30446;&#26631;&#65311;&#25105;&#20204;&#21033;&#29992;&#20808;&#21069;&#30340;&#32463;&#39564;&#25110;&#20182;&#20154;&#30340;&#24314;&#35758;&#65292;&#25110;&#32773;&#36890;&#36807;&#39044;&#27979;&#23545;&#26041;&#30340;&#21453;&#24212;&#26469;&#26500;&#36896;&#20505;&#36873;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#26159;&#26377;&#38480;&#21644;&#26377;&#20559;&#35265;&#30340;&#65292;&#32780;&#19988;&#23545;&#28508;&#22312;&#32467;&#26524;&#36827;&#34892;&#25512;&#29702;&#21487;&#33021;&#26159;&#22256;&#38590;&#19988;&#35748;&#30693;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#26469;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#27807;&#36890;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25506;&#32034;-&#29983;&#25104;-&#27169;&#25311;&#65288;EGS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25509;&#21463;&#20219;&#20309;&#19968;&#20010;&#20010;&#20307;&#19982;&#19968;&#20010;&#30446;&#26631;&#21463;&#20247;&#36827;&#34892;&#27807;&#36890;&#30340;&#22330;&#26223;&#20316;&#20026;&#36755;&#20837;&#12290;EGS&#65288;1&#65289;&#36890;&#36807;&#29983;&#25104;&#19982;&#22330;&#26223;&#30456;&#20851;&#30340;&#22810;&#26679;&#21270;&#24314;&#35758;&#26469;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#65292;&#65288;2&#65289;&#29983;&#25104;&#20197;&#37096;&#20998;&#24314;&#35758;&#20026;&#26465;&#20214;&#30340;&#27807;&#36890;&#20505;&#36873;&#65292;&#65288;3&#65289;&#27169;&#25311;&#19981;&#21516;&#21463;&#20247;&#30340;&#21453;&#24212;&#65292;&#20197;&#30830;&#23450;&#26368;&#20339;&#20505;&#36873;&#21644;&#24314;&#35758;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#21313;&#20010;&#22522;&#26412;&#36807;&#31243;&#30340;&#20843;&#20010;&#22330;&#26223;&#19978;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better. We propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve. EGS (1) explores the solution space by producing a diverse set of advice relevant to the scenario, (2) generates communication candidates conditioned on subsets of the advice, and (3) simulates the reactions from various audiences to determine both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal com
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#24067;&#24335;&#20803;&#24378;&#21270;&#23398;&#20064;&#22312;&#24320;&#25918;&#24335;&#20219;&#21153;&#20998;&#24067;&#19978;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#38598;&#20307;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#22797;&#26434;&#30340;&#21512;&#20316;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2311.00651</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38598;&#20307;&#33258;&#21457;&#24320;&#25918;&#24335;&#25506;&#32034;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Collective Open-Ended Exploration from Decentralized Meta-Reinforcement Learning. (arXiv:2311.00651v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00651
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#24067;&#24335;&#20803;&#24378;&#21270;&#23398;&#20064;&#22312;&#24320;&#25918;&#24335;&#20219;&#21153;&#20998;&#24067;&#19978;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#38598;&#20307;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#22797;&#26434;&#30340;&#21512;&#20316;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#33258;&#25105;&#23545;&#25112;&#30340;&#24320;&#25918;&#24335;&#20219;&#21153;&#20998;&#24067;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#20803;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#20135;&#29983;&#22797;&#26434;&#30340;&#21512;&#20316;&#34892;&#20026;&#12290;&#34429;&#28982;&#32467;&#26524;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#65292;&#33258;&#25105;&#23545;&#25112;&#21644;&#20854;&#20182;&#38598;&#20013;&#21270;&#35757;&#32451;&#25216;&#26415;&#24182;&#19981;&#33021;&#20934;&#30830;&#22320;&#21453;&#26144;&#33258;&#28982;&#30028;&#20013;&#26222;&#36941;&#30340;&#38598;&#20307;&#25506;&#32034;&#31574;&#30053;&#26159;&#22914;&#20309;&#20986;&#29616;&#30340;&#65306;&#36890;&#36807;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#23545;&#20219;&#21153;&#30340;&#26080;&#38480;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38598;&#20307;&#25506;&#32034;&#31574;&#30053;&#30340;&#20986;&#29616;&#65292;&#20854;&#20013;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#19968;&#20010;&#26080;&#38480;&#30340;&#20219;&#21153;&#20998;&#24067;&#20013;&#29420;&#31435;&#22320;&#20803;&#23398;&#20064;&#24490;&#29615;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#29615;&#22659;&#65292;&#23427;&#20855;&#26377;&#19968;&#20010;&#26080;&#38480;&#30340;&#36807;&#31243;&#29983;&#25104;&#30340;&#20219;&#21153;&#31354;&#38388;&#65292;&#21160;&#24577;&#32452;&#21512;&#20102;&#20174;&#20116;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#20013;&#25277;&#26679;&#30340;&#22810;&#20010;&#23376;&#20219;&#21153;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#24222;&#22823;&#30340;&#20219;&#21153;&#26641;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#20998;&#25955;&#26234;&#33021;&#20307;&#22312;&#38754;&#23545;&#26032;&#30340;&#30446;&#26631;&#26102;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have proven that intricate cooperative behaviors can emerge in agents trained using meta reinforcement learning on open ended task distributions using self-play. While the results are impressive, we argue that self-play and other centralized training techniques do not accurately reflect how general collective exploration strategies emerge in the natural world: through decentralized training and over an open-ended distribution of tasks. In this work we therefore investigate the emergence of collective exploration strategies, where several agents meta-learn independent recurrent policies on an open ended distribution of tasks. To this end we introduce a novel environment with an open ended procedurally generated task space which dynamically combines multiple subtasks sampled from five diverse task types to form a vast distribution of task trees. We show that decentralized agents trained in our environment exhibit strong generalization abilities when confronted with novel obj
&lt;/p&gt;</description></item><item><title>FAIRLABEL&#26159;&#19968;&#31181;&#26816;&#27979;&#21644;&#32416;&#27491;&#26631;&#31614;&#20013;&#20559;&#35265;&#30340;&#31639;&#27861;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#20943;&#23569;&#32676;&#20307;&#38388;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#32467;&#26524;&#26174;&#31034;FAIRLABEL&#22312;&#26631;&#31614;&#20462;&#27491;&#26041;&#38754;&#30340;&#27491;&#30830;&#29575;&#36739;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;14.8%, &#22312;&#19981;&#24179;&#31561;&#24433;&#21709;&#27604;&#29575;&#26041;&#38754;&#36798;&#21040;&#20102;54.2%&#30340;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2311.00638</link><description>&lt;p&gt;
FAIRLABEL&#65306;&#20462;&#27491;&#26631;&#31614;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
FAIRLABEL: Correcting Bias in Labels. (arXiv:2311.00638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00638
&lt;/p&gt;
&lt;p&gt;
FAIRLABEL&#26159;&#19968;&#31181;&#26816;&#27979;&#21644;&#32416;&#27491;&#26631;&#31614;&#20013;&#20559;&#35265;&#30340;&#31639;&#27861;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#20943;&#23569;&#32676;&#20307;&#38388;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#32467;&#26524;&#26174;&#31034;FAIRLABEL&#22312;&#26631;&#31614;&#20462;&#27491;&#26041;&#38754;&#30340;&#27491;&#30830;&#29575;&#36739;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;14.8%, &#22312;&#19981;&#24179;&#31561;&#24433;&#21709;&#27604;&#29575;&#26041;&#38754;&#36798;&#21040;&#20102;54.2%&#30340;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#22810;&#31181;&#31639;&#27861;&#21487;&#20197;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#22522;&#26412;&#20551;&#35774;&#26159;&#65292;&#30495;&#23454;&#25968;&#25454;&#26159;&#20844;&#24179;&#25110;&#26080;&#20559;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#30495;&#23454;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#21382;&#21490;&#21644;&#31038;&#20250;&#20559;&#35265;&#21644;&#27495;&#35270;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#23558;&#32487;&#25215;&#24182;&#20256;&#25773;&#20559;&#35265;&#21040;&#27169;&#22411;&#36755;&#20986;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAIRLABEL&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32416;&#27491;&#26631;&#31614;&#20013;&#30340;&#20559;&#35265;&#12290;FAIRLABEL&#30340;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#20943;&#23569;&#32676;&#20307;&#38388;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65288;DI&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24230;&#37327;&#20559;&#35265;&#20462;&#27491;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;FAIRLABEL&#30340;&#27491;&#30830;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#26631;&#31614;&#20462;&#27491;&#30340;&#27491;&#30830;&#29575;&#20026;86.7%&#65292;&#32780;&#22522;&#20934;&#27169;&#22411;&#20026;71.9%&#12290;&#25105;&#20204;&#36824;&#23558;FAIRLABEL&#24212;&#29992;&#20110;UCI Adult&#12289;German Credit Risk&#21644;Compas&#25968;&#25454;&#38598;&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#26174;&#31034;&#19981;&#24179;&#31561;&#24433;&#21709;&#27604;&#29575;&#26368;&#22810;&#22686;&#21152;&#20102;54.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
There are several algorithms for measuring fairness of ML models. A fundamental assumption in these approaches is that the ground truth is fair or unbiased. In real-world datasets, however, the ground truth often contains data that is a result of historical and societal biases and discrimination. Models trained on these datasets will inherit and propagate the biases to the model outputs. We propose FAIRLABEL, an algorithm which detects and corrects biases in labels. The goal of FAIRLABELis to reduce the Disparate Impact (DI) across groups while maintaining high accuracy in predictions. We propose metrics to measure the quality of bias correction and validate FAIRLABEL on synthetic datasets and show that the label correction is correct 86.7% of the time vs. 71.9% for a baseline model. We also apply FAIRLABEL on benchmark datasets such as UCI Adult, German Credit Risk, and Compas datasets and show that the Disparate Impact Ratio increases by as much as 54.2%.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#26694;&#26550;&#65292;&#21033;&#29992;&#27668;&#35937;&#21644;&#36947;&#36335;&#29366;&#20917;&#25968;&#25454;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#12290;&#36890;&#36807;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#21452;&#27169;&#24335;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#20107;&#25925;&#23545;&#20132;&#36890;&#30340;&#24433;&#21709;&#26159;&#30701;&#26399;&#36824;&#26159;&#38271;&#26399;&#65292;&#24182;&#30830;&#23450;&#20107;&#25925;&#30340;&#31934;&#30830;&#25345;&#32493;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2311.00634</link><description>&lt;p&gt;
&#22522;&#20110;&#27668;&#35937;&#21644;&#36947;&#36335;&#29366;&#20917;&#25968;&#25454;&#30340;&#20132;&#36890;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#30340;&#21452;&#23618;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Bi-level Framework for Traffic Accident Duration Prediction: Leveraging Weather and Road Condition Data within a Practical Optimum Pipeline. (arXiv:2311.00634v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#26694;&#26550;&#65292;&#21033;&#29992;&#27668;&#35937;&#21644;&#36947;&#36335;&#29366;&#20917;&#25968;&#25454;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#12290;&#36890;&#36807;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#21452;&#27169;&#24335;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#20107;&#25925;&#23545;&#20132;&#36890;&#30340;&#24433;&#21709;&#26159;&#30701;&#26399;&#36824;&#26159;&#38271;&#26399;&#65292;&#24182;&#30830;&#23450;&#20107;&#25925;&#30340;&#31934;&#30830;&#25345;&#32493;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20107;&#20214;&#30340;&#38543;&#26426;&#24615;&#65292;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#20934;&#30830;&#30340;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;&#21487;&#20197;&#20026;&#36890;&#21220;&#32773;&#36873;&#25321;&#26368;&#20339;&#36335;&#32447;&#21644;&#20132;&#36890;&#31649;&#29702;&#20154;&#21592;&#35299;&#20915;&#38750;&#32463;&#24120;&#24615;&#25317;&#22581;&#38382;&#39064;&#24102;&#26469;&#24040;&#22823;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#20174;&#20132;&#36890;&#20107;&#25925;&#25968;&#25454;&#24211;&#20013;&#25910;&#38598;&#20102;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#12289;&#36947;&#36335;&#29366;&#20917;&#21644;&#27668;&#35937;&#25968;&#25454;&#65292;&#20197;&#26816;&#26597;&#22312;&#27809;&#26377;&#20107;&#25925;&#19978;&#19979;&#25991;&#20449;&#24687;&#25968;&#25454;&#65288;&#22914;&#20107;&#25925;&#20005;&#37325;&#24615;&#21644;&#25991;&#26412;&#25551;&#36848;&#65289;&#30340;&#24773;&#20917;&#19979;&#20132;&#36890;&#20107;&#25925;&#25345;&#32493;&#26102;&#38388;&#31649;&#36947;&#30340;&#21487;&#34892;&#24615;&#12290;&#37319;&#29992;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#23545;&#36947;&#36335;&#20132;&#36890;&#30340;&#24433;&#21709;&#26159;&#30701;&#26399;&#36824;&#26159;&#38271;&#26399;&#65292;&#28982;&#21518;&#21033;&#29992;&#21452;&#27169;&#24335;&#26041;&#27861;&#30830;&#23450;&#20107;&#25925;&#24433;&#21709;&#30340;&#31934;&#30830;&#25345;&#32493;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#20108;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#21487;&#20197;&#20197;83%&#30340;&#20934;&#30830;&#29575;&#21306;&#20998;&#30701;&#26399;&#21644;&#38271;&#26399;&#24433;&#21709;&#65292;&#32780;LightGBM&#22238;&#24402;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#20107;&#25925;&#30340;&#31934;&#30830;&#25345;&#32493;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the stochastic nature of events, predicting the duration of a traffic incident presents a formidable challenge. Accurate duration estimation can result in substantial advantages for commuters in selecting optimal routes and for traffic management personnel in addressing non-recurring congestion issues. In this study, we gathered accident duration, road conditions, and meteorological data from a database of traffic accidents to check the feasibility of a traffic accident duration pipeline without accident contextual information data like accident severity and textual description. Multiple machine learning models were employed to predict whether an accident's impact on road traffic would be of a short-term or long-term nature, and then utilizing a bimodal approach the precise duration of the incident's effect was determined. Our binary classification random forest model distinguished between short-term and long-term effects with an 83% accuracy rate, while the LightGBM regression 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#20934;&#30830;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36824;&#26174;&#31034;&#20986;&#23545;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00619</link><description>&lt;p&gt;
&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#25439;&#22833;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Loss Modeling for Multi-Annotator Datasets. (arXiv:2311.00619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00619
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#20934;&#30830;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36824;&#26174;&#31034;&#20986;&#23545;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#27491;&#24615;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#27880;&#37322;&#32773;&#30340;&#24847;&#35265;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#27880;&#37322;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#65292;&#20010;&#21035;&#27880;&#37322;&#32773;&#32463;&#24120;&#20250;&#25552;&#20379;&#25968;&#21315;&#20010;&#35780;&#20998;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#30130;&#21171;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27880;&#37322;&#36807;&#31243;&#21487;&#33021;&#20250;&#25345;&#32493;&#22810;&#22825;&#65292;&#21487;&#33021;&#23548;&#33268;&#23545;&#27880;&#37322;&#32773;&#30340;&#24847;&#35265;&#38543;&#26102;&#38388;&#30340;&#19981;&#20934;&#30830;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#26356;&#20934;&#30830;&#30340;&#22810;&#26679;&#24847;&#35265;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#26032;&#39062;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#28165;&#26970;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20462;&#25913;&#21487;&#20197;&#25913;&#21892;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#24212;&#29992;&#20110;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20173;&#28982;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accounting for the opinions of all annotators of a dataset is critical for fairness. However, when annotating large datasets, individual annotators will frequently provide thousands of ratings which can lead to fatigue. Additionally, these annotation processes can occur over multiple days which can lead to an inaccurate representation of an annotator's opinion over time. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, we demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;SDVI&#65292;&#21487;&#20197;&#29992;&#20110;&#20855;&#26377;&#38543;&#26426;&#25903;&#25345;&#30340;&#27010;&#29575;&#32534;&#31243;&#12290;&#36890;&#36807;&#23558;&#31243;&#24207;&#20998;&#35299;&#20026;&#20855;&#26377;&#38745;&#24577;&#25903;&#25345;&#30340;&#23376;&#31243;&#24207;&#65292;&#24182;&#20026;&#27599;&#20010;&#23376;&#31243;&#24207;&#26500;&#24314;&#29420;&#31435;&#30340;&#21464;&#20998;&#25351;&#23548;&#65292;SDVI&#22312;&#25512;&#26029;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.00594</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20855;&#26377;&#38543;&#26426;&#25903;&#25345;&#30340;&#27010;&#29575;&#32534;&#31243;&#30340;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Rethinking Variational Inference for Probabilistic Programs with Stochastic Support. (arXiv:2311.00594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;SDVI&#65292;&#21487;&#20197;&#29992;&#20110;&#20855;&#26377;&#38543;&#26426;&#25903;&#25345;&#30340;&#27010;&#29575;&#32534;&#31243;&#12290;&#36890;&#36807;&#23558;&#31243;&#24207;&#20998;&#35299;&#20026;&#20855;&#26377;&#38745;&#24577;&#25903;&#25345;&#30340;&#23376;&#31243;&#24207;&#65292;&#24182;&#20026;&#27599;&#20010;&#23376;&#31243;&#24207;&#26500;&#24314;&#29420;&#31435;&#30340;&#21464;&#20998;&#25351;&#23548;&#65292;SDVI&#22312;&#25512;&#26029;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25903;&#25345;&#20998;&#35299;&#21464;&#20998;&#25512;&#26029;&#65288;SDVI&#65289;&#65292;&#19968;&#31181;&#38754;&#21521;&#20855;&#26377;&#38543;&#26426;&#25903;&#25345;&#30340;&#27010;&#29575;&#32534;&#31243;&#30340;&#26032;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#20381;&#36182;&#20110;&#22312;&#36880;&#21464;&#37327;&#30340;&#22522;&#30784;&#19978;&#35774;&#35745;&#21333;&#20010;&#20840;&#23616;&#21464;&#20998;&#25351;&#23548;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#31243;&#24207;&#30340;&#38543;&#26426;&#25511;&#21046;&#27969;&#12290;SDVI&#30456;&#21453;&#65292;&#23558;&#31243;&#24207;&#20998;&#35299;&#20026;&#20855;&#26377;&#38745;&#24577;&#25903;&#25345;&#30340;&#23376;&#31243;&#24207;&#65292;&#28982;&#21518;&#33258;&#21160;&#26500;&#24314;&#27599;&#20010;&#23376;&#25351;&#23548;&#30340;&#29420;&#31435;&#23376;&#25351;&#23548;&#12290;&#36825;&#31181;&#20998;&#35299;&#26174;&#33879;&#26377;&#21161;&#20110;&#26500;&#24314;&#36866;&#21512;&#30340;&#21464;&#20998;&#26063;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Support Decomposition Variational Inference (SDVI), a new variational inference (VI) approach for probabilistic programs with stochastic support. Existing approaches to this problem rely on designing a single global variational guide on a variable-by-variable basis, while maintaining the stochastic control flow of the original program. SDVI instead breaks the program down into sub-programs with static support, before automatically building separate sub-guides for each. This decomposition significantly aids in the construction of suitable variational families, enabling, in turn, substantial improvements in inference performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Coop&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20869;&#23384;&#31995;&#32479;&#38382;&#39064;&#65292;&#36890;&#36807;&#39537;&#36880;&#36830;&#32493;&#24352;&#37327;&#21644;&#20248;&#21270;&#24352;&#37327;&#20998;&#37197;&#65292;&#20197;&#38477;&#20302;&#20877;&#26448;&#26009;&#21270;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2311.00591</link><description>&lt;p&gt;
Coop: &#20869;&#23384;&#19981;&#26159;&#21830;&#21697;
&lt;/p&gt;
&lt;p&gt;
Coop: Memory is not a Commodity. (arXiv:2311.00591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Coop&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20869;&#23384;&#31995;&#32479;&#38382;&#39064;&#65292;&#36890;&#36807;&#39537;&#36880;&#36830;&#32493;&#24352;&#37327;&#21644;&#20248;&#21270;&#24352;&#37327;&#20998;&#37197;&#65292;&#20197;&#38477;&#20302;&#20877;&#26448;&#26009;&#21270;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#20877;&#26448;&#26009;&#21270;&#25216;&#26415;&#20801;&#35768;&#22312;&#26377;&#38480;&#30340;&#20869;&#23384;&#39044;&#31639;&#19979;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#36890;&#36807;&#22312;&#38656;&#35201;&#26102;&#26816;&#26597;&#28857;&#27169;&#22411;&#24182;&#26681;&#25454;&#38656;&#35201;&#37325;&#26032;&#35745;&#31639;&#34987;&#39537;&#36880;&#30340;&#24352;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24352;&#37327;&#20877;&#26448;&#26009;&#21270;&#25216;&#26415;&#24573;&#35270;&#20102;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20869;&#23384;&#31995;&#32479;&#65292;&#24182;&#20551;&#35774;&#19981;&#21516;&#22320;&#22336;&#30340;&#31354;&#38386;&#20869;&#23384;&#22359;&#26159;&#30456;&#21516;&#30340;&#12290;&#22312;&#36825;&#20010;&#38169;&#35823;&#30340;&#20551;&#35774;&#19979;&#65292;&#19981;&#36830;&#32493;&#30340;&#24352;&#37327;&#34987;&#39537;&#36880;&#65292;&#20854;&#20013;&#19968;&#20123;&#19981;&#29992;&#20110;&#20998;&#37197;&#26032;&#30340;&#24352;&#37327;&#12290;&#36825;&#23548;&#33268;&#20005;&#37325;&#30340;&#20869;&#23384;&#30862;&#29255;&#21270;&#65292;&#22686;&#21152;&#20102;&#28508;&#22312;&#20877;&#26448;&#26009;&#21270;&#30340;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#28369;&#21160;&#31383;&#21475;&#20869;&#39537;&#36880;&#24352;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#25152;&#26377;&#39537;&#36880;&#37117;&#26159;&#36830;&#32493;&#30340;&#24182;&#19988;&#31435;&#21363;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24265;&#20215;&#30340;&#24352;&#37327;&#20998;&#21306;&#21644;&#21487;&#37325;&#31639;&#30340;&#23601;&#22320;&#20248;&#21270;&#24352;&#37327;&#20998;&#37197;&#26469;&#36827;&#19968;&#27493;&#38477;&#20302;&#20877;&#26448;&#26009;&#21270;&#25104;&#26412;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;Coop&#65292;&#22240;&#20026;&#23427;&#26159;&#24352;&#37327;&#20998;&#37197;&#21644;&#24352;&#37327;&#20877;&#26448;&#26009;&#21270;&#30340;&#21327;&#21516;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor rematerialization allows the training of deep neural networks (DNNs) under limited memory budgets by checkpointing the models and recomputing the evicted tensors as needed. However, the existing tensor rematerialization techniques overlook the memory system in deep learning frameworks and implicitly assume that free memory blocks at different addresses are identical. Under this flawed assumption, discontiguous tensors are evicted, among which some are not used to allocate the new tensor. This leads to severe memory fragmentation and increases the cost of potential rematerializations. To address this issue, we propose to evict tensors within a sliding window to ensure all evictions are contiguous and are immediately used. Furthermore, we proposed cheap tensor partitioning and recomputable in-place to further reduce the rematerialization cost by optimizing the tensor allocation. We named our method Coop as it is a co-optimization of tensor allocation and tensor rematerialization. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FlowSUM&#65292;&#19968;&#20010;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;Transformer-based&#25688;&#35201;&#29983;&#25104;&#12290;&#36890;&#36807;&#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#36827;&#34892;&#28789;&#27963;&#30340;&#28508;&#22312;&#21518;&#21521;&#24314;&#27169;&#20197;&#21450;&#37319;&#29992;&#21463;&#25511;&#20132;&#26367;&#28608;&#36827;&#35757;&#32451;&#31574;&#30053;&#65292;FlowSUM&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#27491;&#21017;&#21270;&#27969;&#20013;&#30340;&#21518;&#21521;&#22604;&#38519;&#38382;&#39064;&#21644;&#30456;&#20851;&#24433;&#21709;&#22240;&#32032;&#65292;&#20026;&#30456;&#20851;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2311.00588</link><description>&lt;p&gt;
&#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#21644;&#28608;&#36827;&#35757;&#32451;&#25552;&#21319;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Boosting Summarization with Normalizing Flows and Aggressive Training. (arXiv:2311.00588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FlowSUM&#65292;&#19968;&#20010;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;Transformer-based&#25688;&#35201;&#29983;&#25104;&#12290;&#36890;&#36807;&#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#36827;&#34892;&#28789;&#27963;&#30340;&#28508;&#22312;&#21518;&#21521;&#24314;&#27169;&#20197;&#21450;&#37319;&#29992;&#21463;&#25511;&#20132;&#26367;&#28608;&#36827;&#35757;&#32451;&#31574;&#30053;&#65292;FlowSUM&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#27491;&#21017;&#21270;&#27969;&#20013;&#30340;&#21518;&#21521;&#22604;&#38519;&#38382;&#39064;&#21644;&#30456;&#20851;&#24433;&#21709;&#22240;&#32032;&#65292;&#20026;&#30456;&#20851;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;FlowSUM&#65292;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#25688;&#35201;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#21464;&#20998;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#19981;&#36275;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21518;&#21521;&#22604;&#38519;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#28508;&#22312;&#21518;&#21521;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#38376;&#26426;&#21046;&#19979;&#30340;&#21463;&#25511;&#20132;&#26367;&#28608;&#36827;&#35757;&#32451;&#65288;CAAT&#65289;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FlowSUM&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#23545;&#25512;&#29702;&#26102;&#38388;&#30340;&#24433;&#21709;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#37322;&#25918;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#27969;&#20013;&#30340;&#21518;&#21521;&#22604;&#38519;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#35757;&#32451;&#31574;&#30053;&#12289;&#38376;&#21021;&#22987;&#21270;&#20197;&#21450;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#27969;&#31867;&#22411;&#21644;&#25968;&#37327;&#23545;&#25688;&#35201;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents FlowSUM, a normalizing flows-based variational encoder-decoder framework for Transformer-based summarization. Our approach tackles two primary challenges in variational summarization: insufficient semantic information in latent representations and posterior collapse during training. To address these challenges, we employ normalizing flows to enable flexible latent posterior modeling, and we propose a controlled alternate aggressive training (CAAT) strategy with an improved gate mechanism. Experimental results show that FlowSUM significantly enhances the quality of generated summaries and unleashes the potential for knowledge distillation with minimal impact on inference time. Furthermore, we investigate the issue of posterior collapse in normalizing flows and analyze how the summary quality is affected by the training strategy, gate initialization, and the type and number of normalizing flows used, offering valuable insights for future research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#30446;&#26631;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;Nash&#22343;&#34913;&#24182;&#20855;&#26377;&#29305;&#23450;&#20215;&#20540;&#33539;&#22260;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2311.00582</link><description>&lt;p&gt;
&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20197;&#23454;&#29616;&#20219;&#24847;Nash&#22343;&#34913;&#21644;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value. (arXiv:2311.00582v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#30446;&#26631;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;Nash&#22343;&#34913;&#24182;&#20855;&#26377;&#29305;&#23450;&#20215;&#20540;&#33539;&#22260;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20301;&#21892;&#24847;&#30340;&#28216;&#25103;&#35774;&#35745;&#32773;&#25110;&#24694;&#24847;&#30340;&#23545;&#25163;&#20462;&#25913;&#20102;&#19968;&#20010;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#20415;&#19968;&#20010;&#30446;&#26631;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#30340;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;&#39532;&#23572;&#21487;&#22827;&#23436;&#32654;Nash&#22343;&#34913;&#65292;&#24182;&#19988;&#22312;&#30446;&#26631;&#33539;&#22260;&#20869;&#20855;&#26377;&#20215;&#20540;&#65292;&#20197;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#33021;&#22815;&#23433;&#35013;&#20026;&#26576;&#20010;&#28216;&#25103;&#30340;&#21807;&#19968;&#22343;&#34913;&#30340;&#31574;&#30053;&#37197;&#32622;&#30340;&#38598;&#21512;&#65292;&#24182;&#24314;&#31435;&#20102;&#25104;&#21151;&#23433;&#35013;&#30340;&#20805;&#20998;&#21644;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#19968;&#20010;&#24102;&#26377;&#32447;&#24615;&#32422;&#26463;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#28982;&#21518;&#36827;&#34892;&#38543;&#26426;&#25200;&#21160;&#65292;&#26469;&#33719;&#24471;&#19968;&#20010;&#25104;&#26412;&#36817;&#20046;&#26368;&#20248;&#30340;&#20462;&#25913;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the game modification problem, where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost. We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation. We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.
&lt;/p&gt;</description></item><item><title>LLaVA-Interactive&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20154;&#26426;&#20132;&#20114;&#30340;&#30740;&#31350;&#21407;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;&#26469;&#23545;&#40784;&#20154;&#31867;&#24847;&#22270;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#23545;&#35805;&#12289;&#20998;&#21106;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#22810;&#27169;&#24577;&#21151;&#33021;&#65292;&#20855;&#26377;&#39640;&#25104;&#26412;&#25928;&#30410;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00571</link><description>&lt;p&gt;
LLaVA-Interactive:&#19968;&#20010;&#22270;&#20687;&#23545;&#35805;&#12289;&#20998;&#21106;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#20840;&#33021;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing. (arXiv:2311.00571v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00571
&lt;/p&gt;
&lt;p&gt;
LLaVA-Interactive&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20154;&#26426;&#20132;&#20114;&#30340;&#30740;&#31350;&#21407;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;&#26469;&#23545;&#40784;&#20154;&#31867;&#24847;&#22270;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#23545;&#35805;&#12289;&#20998;&#21106;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#22810;&#27169;&#24577;&#21151;&#33021;&#65292;&#20855;&#26377;&#39640;&#25104;&#26412;&#25928;&#30410;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLaVA-Interactive&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#20154;&#26426;&#20132;&#20114;&#30340;&#30740;&#31350;&#21407;&#22411;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25509;&#25910;&#22810;&#27169;&#24577;&#29992;&#25143;&#36755;&#20837;&#24182;&#29983;&#25104;&#22810;&#27169;&#24577;&#22238;&#24212;&#19982;&#29992;&#25143;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;LLaVA-Interactive&#19981;&#20165;&#20165;&#26159;&#35821;&#35328;&#25552;&#31034;&#65292;&#36824;&#21487;&#20197;&#20351;&#29992;&#35270;&#35273;&#25552;&#31034;&#26469;&#23545;&#40784;&#20154;&#31867;&#24847;&#22270;&#12290;LLaVA-Interactive&#30340;&#30740;&#21457;&#25104;&#26412;&#25928;&#30410;&#38750;&#24120;&#39640;&#65292;&#22240;&#20026;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#39044;&#24314;&#30340;AI&#27169;&#22411;&#30340;&#19977;&#39033;&#22810;&#27169;&#24577;&#25216;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27169;&#22411;&#35757;&#32451;&#65306;LLaVA&#30340;&#22270;&#20687;&#23545;&#35805;&#65292;SEEM&#30340;&#22270;&#20687;&#20998;&#21106;&#20197;&#21450;GLIGEN&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#12290;&#23637;&#31034;&#20102;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#65292;&#20197;&#23637;&#31034;LLaVA-Interactive&#30340;&#28508;&#21147;&#65292;&#24182;&#28608;&#21457;&#26410;&#26469;&#22312;&#22810;&#27169;&#24577;&#20132;&#20114;&#31995;&#32479;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLaVA-Interactive is a research prototype for multimodal human-AI interaction. The system can have multi-turn dialogues with human users by taking multimodal user inputs and generating multimodal responses. Importantly, LLaVA-Interactive goes beyond language prompt, where visual prompt is enabled to align human intents in the interaction. The development of LLaVA-Interactive is extremely cost-efficient as the system combines three multimodal skills of pre-built AI models without additional model training: visual chat of LLaVA, image segmentation from SEEM, as well as image generation and editing from GLIGEN. A diverse set of application scenarios is presented to demonstrate the promises of LLaVA-Interactive and to inspire future research in multimodal interactive systems.
&lt;/p&gt;</description></item><item><title>&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#24320;&#21457;&#33021;&#22815;&#26816;&#27979;&#35270;&#35273;&#32447;&#32034;&#24182;&#19982;&#24739;&#32773;&#20020;&#24202;&#29366;&#24577;&#20851;&#32852;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#23458;&#35266;&#21644;&#35814;&#32454;&#30340;&#30417;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00565</link><description>&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#26816;&#27979;&#35270;&#35273;&#32447;&#32034;&#19982;&#24739;&#32773;&#20020;&#24202;&#29366;&#24577;&#30340;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Detecting Visual Cues in the Intensive Care Unit and Association with Patient Clinical Status. (arXiv:2311.00565v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00565
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#24320;&#21457;&#33021;&#22815;&#26816;&#27979;&#35270;&#35273;&#32447;&#32034;&#24182;&#19982;&#24739;&#32773;&#20020;&#24202;&#29366;&#24577;&#20851;&#32852;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#23458;&#35266;&#21644;&#35814;&#32454;&#30340;&#30417;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#20026;&#24739;&#26377;&#29983;&#21629;&#23041;&#32961;&#30340;&#24739;&#32773;&#25552;&#20379;&#23494;&#20999;&#30417;&#25252;&#21644;&#36830;&#32493;&#25252;&#29702;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#21644;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#32773;&#30340;&#24037;&#20316;&#36127;&#33655;&#65292;ICU&#20013;&#30340;&#36830;&#32493;&#24739;&#32773;&#35780;&#20272;&#20173;&#28982;&#26377;&#38480;&#12290;&#29616;&#26377;&#30340;ICU&#24739;&#32773;&#35780;&#20272;&#65292;&#22914;&#30140;&#30171;&#25110;&#27963;&#21160;&#33021;&#21147;&#35780;&#20272;&#65292;&#22823;&#22810;&#26159;&#38646;&#25955;&#21644;&#25163;&#21160;&#23454;&#26045;&#30340;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#20154;&#20026;&#38169;&#35823;&#30340;&#28508;&#22312;&#21487;&#33021;&#24615;&#12290;&#24320;&#21457;&#33021;&#22815;&#22686;&#24378;ICU&#20013;&#20154;&#31867;&#35780;&#20272;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24037;&#20855;&#21487;&#20197;&#26377;&#21033;&#20110;&#25552;&#20379;&#26356;&#23458;&#35266;&#21644;&#35814;&#32454;&#30340;&#30417;&#27979;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#25429;&#25417;&#19982;&#30140;&#30171;&#25110;&#19981;&#23433;&#30456;&#20851;&#30340;&#24739;&#32773;&#38754;&#37096;&#32447;&#32034;&#30340;&#21464;&#21270;&#21487;&#20197;&#24110;&#21161;&#35843;&#25972;&#19982;&#30140;&#30171;&#30456;&#20851;&#30340;&#33647;&#29289;&#25110;&#26816;&#27979;&#21487;&#33021;&#24341;&#36215;&#19981;&#23433;&#30340;&#24773;&#20917;&#65292;&#22914;&#35893;&#22916;&#12290;&#27492;&#22806;&#65292;&#22312;&#19981;&#33391;&#20020;&#24202;&#20107;&#20214;&#21457;&#29983;&#26399;&#38388;&#25110;&#20043;&#21069;&#65292;&#35270;&#35273;&#32447;&#32034;&#30340;&#24494;&#22937;&#21464;&#21270;&#19982;&#39640;&#20998;&#36776;&#29575;&#29983;&#29702;&#20449;&#21495;&#30456;&#32467;&#21512;&#21487;&#33021;&#26377;&#21161;&#20110;&#36830;&#32493;&#24739;&#32773;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intensive Care Units (ICU) provide close supervision and continuous care to patients with life-threatening conditions. However, continuous patient assessment in the ICU is still limited due to time constraints and the workload on healthcare providers. Existing patient assessments in the ICU such as pain or mobility assessment are mostly sporadic and administered manually, thus introducing the potential for human errors. Developing Artificial intelligence (AI) tools that can augment human assessments in the ICU can be beneficial for providing more objective and granular monitoring capabilities. For example, capturing the variations in a patient's facial cues related to pain or agitation can help in adjusting pain-related medications or detecting agitation-inducing conditions such as delirium. Additionally, subtle changes in visual cues during or prior to adverse clinical events could potentially aid in continuous patient monitoring when combined with high-resolution physiological signal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#21644;MDL&#21407;&#21017;&#65292;&#29992;&#20110;&#35299;&#20915;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#12290;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#39044;&#27979;&#21644;&#32852;&#21512;&#25551;&#36848;&#36755;&#20837;/&#36755;&#20986;&#23545;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00545</link><description>&lt;p&gt;
&#29992;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#21644;MDL&#21407;&#21017;&#35299;&#20915;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;
&lt;/p&gt;
&lt;p&gt;
Tackling the Abstraction and Reasoning Corpus (ARC) with Object-centric Models and the MDL Principle. (arXiv:2311.00545v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00545
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#21644;MDL&#21407;&#21017;&#65292;&#29992;&#20110;&#35299;&#20915;&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#12290;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#39044;&#27979;&#21644;&#32852;&#21512;&#25551;&#36848;&#36755;&#20837;/&#36755;&#20986;&#23545;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#26397;&#21521;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#26041;&#21521;&#21457;&#23637;&#12290;&#23427;&#26159;&#19968;&#32452;&#20851;&#20110;&#29983;&#25104;&#24425;&#33394;&#32593;&#26684;&#30340;&#29420;&#29305;&#20219;&#21153;&#65292;&#20165;&#30001;&#23569;&#37327;&#31034;&#20363;&#36827;&#34892;&#35268;&#23450;&#12290;&#19982;&#29616;&#26377;&#24037;&#20316;&#20013;&#30340;&#22522;&#20110;&#36716;&#25442;&#30340;&#31243;&#24207;&#30456;&#27604;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#33258;&#28982;&#31243;&#24207;&#19968;&#33268;&#30340;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#36827;&#34892;&#39044;&#27979;&#65292;&#36824;&#21487;&#20197;&#20026;&#36755;&#20837;/&#36755;&#20986;&#23545;&#25552;&#20379;&#32852;&#21512;&#25551;&#36848;&#12290;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#65288;MDL&#65289;&#21407;&#21017;&#29992;&#20110;&#39640;&#25928;&#25628;&#32034;&#22823;&#22411;&#27169;&#22411;&#31354;&#38388;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#19982;&#33258;&#28982;&#31243;&#24207;&#30456;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Abstraction and Reasoning Corpus (ARC) is a challenging benchmark, introduced to foster AI research towards human-level intelligence. It is a collection of unique tasks about generating colored grids, specified by a few examples only. In contrast to the transformation-based programs of existing work, we introduce object-centric models that are in line with the natural programs produced by humans. Our models can not only perform predictions, but also provide joint descriptions for input/output pairs. The Minimum Description Length (MDL) principle is used to efficiently search the large model space. A diverse range of tasks are solved, and the learned models are similar to the natural programs. We demonstrate the generality of our approach by applying it to a different domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#22312;&#23548;&#33322;&#39046;&#22495;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2311.00530</link><description>&lt;p&gt;
LLMs&#23545;&#20855;&#36523;&#23548;&#33322;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
The Development of LLMs for Embodied Navigation. (arXiv:2311.00530v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#22312;&#23548;&#33322;&#39046;&#22495;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35832;&#22914;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#20043;&#31867;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#30340;&#24212;&#29992;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#20247;&#22810;&#24212;&#29992;&#20013;&#65292;&#23548;&#33322;&#20219;&#21153;&#23588;&#20026;&#24341;&#20154;&#27880;&#30446;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;&#23545;&#29615;&#22659;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#21644;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;LLMs&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20854;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#22270;&#20687;&#22788;&#29702;&#33021;&#21147;&#65292;&#22686;&#24378;&#20855;&#36523;&#26234;&#33021;&#31995;&#32479;&#22312;&#29615;&#22659;&#24863;&#30693;&#21644;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#20102;LLMs&#19982;&#20855;&#36523;&#26234;&#33021;&#20043;&#38388;&#22312;&#23548;&#33322;&#26041;&#38754;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#23457;&#35270;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12289;&#30740;&#31350;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#20855;&#36523;&#23548;&#33322;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#38416;&#26126;&#20102;LLMs&#22312;&#20855;&#36523;&#23548;&#33322;&#39046;&#22495;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#39034;&#24207;&#21487;&#35299;&#37322;&#31574;&#30053;&#30340;&#26080;&#20559;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#27010;&#29575;&#36827;&#34892;&#22870;&#21169;&#26469;&#20943;&#36731;&#29616;&#26377;&#26041;&#27861;&#20013;&#21487;&#33021;&#23548;&#33268;&#20559;&#21521;&#29305;&#23450;&#21160;&#20316;&#30340;&#31574;&#30053;&#30340;&#19981;&#24076;&#26395;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00523</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#39034;&#24207;&#21487;&#35299;&#37322;&#31574;&#30053;&#30340;&#26080;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning impartial policies for sequential counterfactual explanations using Deep Reinforcement Learning. (arXiv:2311.00523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#39034;&#24207;&#21487;&#35299;&#37322;&#31574;&#30053;&#30340;&#26080;&#20559;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#27010;&#29575;&#36827;&#34892;&#22870;&#21169;&#26469;&#20943;&#36731;&#29616;&#26377;&#26041;&#27861;&#20013;&#21487;&#33021;&#23548;&#33268;&#20559;&#21521;&#29305;&#23450;&#21160;&#20316;&#30340;&#31574;&#30053;&#30340;&#19981;&#24076;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#39034;&#24207;&#21487;&#35299;&#37322;&#65288;SCF&#65289;&#31034;&#20363;&#36890;&#36807;&#23545;&#36755;&#20837;&#23454;&#20363;&#36827;&#34892;&#19968;&#31995;&#21015;&#20462;&#25913;&#26469;&#25913;&#21464;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;&#34429;&#28982;&#26576;&#20123;&#27979;&#35797;&#26102;&#31639;&#27861;&#26088;&#22312;&#38024;&#23545;&#27599;&#20010;&#26032;&#23454;&#20363;&#36827;&#34892;&#20248;&#21270;&#65292;&#20294;&#26368;&#36817;&#25552;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#23398;&#20064;&#29992;&#20110;&#21457;&#29616;SCF&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#21487;&#20280;&#32553;&#24615;&#12290;&#22312;RL&#20013;&#65292;RL&#38382;&#39064;&#30340;&#21046;&#23450;&#65292;&#21253;&#25324;&#29366;&#24577;&#31354;&#38388;&#65292;&#21160;&#20316;&#21644;&#22870;&#21169;&#30340;&#35268;&#23450;&#65292;&#36890;&#24120;&#23384;&#22312;&#27495;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#21487;&#33021;&#23548;&#33268;&#20855;&#26377;&#19981;&#24076;&#26395;&#30340;&#23646;&#24615;&#65288;&#22914;&#20559;&#21521;&#29305;&#23450;&#21160;&#20316;&#65289;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#21019;&#24314;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#22870;&#21169;&#65292;&#20197;&#20943;&#36731;&#36825;&#31181;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of explainable Artificial Intelligence (XAI), sequential counterfactual (SCF) examples are often used to alter the decision of a trained classifier by implementing a sequence of modifications to the input instance. Although certain test-time algorithms aim to optimize for each new instance individually, recently Reinforcement Learning (RL) methods have been proposed that seek to learn policies for discovering SCFs, thereby enhancing scalability. As is typical in RL, the formulation of the RL problem, including the specification of state space, actions, and rewards, can often be ambiguous. In this work, we identify shortcomings in existing methods that can result in policies with undesired properties, such as a bias towards specific actions. We propose to use the output probabilities of the classifier to create a more informative reward, to mitigate this effect.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#33258;&#21160;&#26435;&#37325;&#37327;&#21270;&#21644;&#20248;&#21270;&#20869;&#26680;&#65292;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#23637;&#31034;&#20102;&#26497;&#39640;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.00502</link><description>&lt;p&gt;
&#22312;CPU&#19978;&#39640;&#25928;&#30340;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Efficient LLM Inference on CPUs. (arXiv:2311.00502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#33258;&#21160;&#26435;&#37325;&#37327;&#21270;&#21644;&#20248;&#21270;&#20869;&#26680;&#65292;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#23637;&#31034;&#20102;&#26497;&#39640;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#21644;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#24222;&#22823;&#25968;&#37327;&#65292;LLMs&#30340;&#37096;&#32626;&#19968;&#30452;&#38754;&#20020;&#25361;&#25112;&#65292;&#23545;&#22823;&#20869;&#23384;&#23481;&#37327;&#21644;&#39640;&#20869;&#23384;&#24102;&#23485;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;LLMs&#30340;&#37096;&#32626;&#26356;&#39640;&#25928;&#12290;&#25105;&#20204;&#25903;&#25345;&#33258;&#21160;&#30340;INT4&#26435;&#37325;&#37327;&#21270;&#27969;&#31243;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#27530;&#30340;LLM&#36816;&#34892;&#26102;&#65292;&#20855;&#26377;&#39640;&#24230;&#20248;&#21270;&#30340;&#20869;&#26680;&#65292;&#20197;&#21152;&#36895;&#22312;CPU&#19978;&#30340;LLM&#25512;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#30340;&#26222;&#36866;&#24615;&#65292;&#21253;&#25324;Llama2&#65292;Llama&#65292;GPT-NeoX&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;CPU&#19978;&#30340;&#26497;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110;: https://github.com/intel/intel-extension-for-transformers.
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks. However, deploying these models has been challenging due to the astronomical amount of model parameters, which requires a demand for large memory capacity and high memory bandwidth. In this paper, we propose an effective approach that can make the deployment of LLMs more efficiently. We support an automatic INT4 weight-only quantization flow and design a special LLM runtime with highly-optimized kernels to accelerate the LLM inference on CPUs. We demonstrate the general applicability of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase the extreme inference efficiency on CPUs. The code is publicly available at: https://github.com/intel/intel-extension-for-transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#25968;&#25454;&#24402;&#22240;&#26041;&#38754;&#65292;&#19968;&#20123;&#22312;&#29702;&#35770;&#19978;&#19981;&#21512;&#29702;&#30340;&#35774;&#35745;&#36873;&#25321;&#33021;&#22815;&#22312;&#23454;&#38469;&#20013;&#34920;&#29616;&#20986;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#36825;&#23545;&#20110;&#30830;&#20445;&#25968;&#25454;&#36129;&#29486;&#32773;&#20844;&#24179;&#34917;&#20607;&#25110;&#35748;&#21487;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2311.00500</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#24402;&#22240;&#30340;&#26377;&#36259;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intriguing Properties of Data Attribution on Diffusion Models. (arXiv:2311.00500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#25968;&#25454;&#24402;&#22240;&#26041;&#38754;&#65292;&#19968;&#20123;&#22312;&#29702;&#35770;&#19978;&#19981;&#21512;&#29702;&#30340;&#35774;&#35745;&#36873;&#25321;&#33021;&#22815;&#22312;&#23454;&#38469;&#20013;&#34920;&#29616;&#20986;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#36825;&#23545;&#20110;&#30830;&#20445;&#25968;&#25454;&#36129;&#29486;&#32773;&#20844;&#24179;&#34917;&#20607;&#25110;&#35748;&#21487;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24402;&#22240;&#26088;&#22312;&#23558;&#27169;&#22411;&#36755;&#20986;&#36861;&#28335;&#21040;&#35757;&#32451;&#25968;&#25454;&#12290;&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#25968;&#25454;&#24402;&#22240;&#24050;&#25104;&#20026;&#19968;&#20010;&#29702;&#24819;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#20026;&#39640;&#36136;&#37327;&#25110;&#29256;&#26435;&#20445;&#25252;&#30340;&#35757;&#32451;&#26679;&#26412;&#27491;&#30830;&#20998;&#37197;&#20215;&#20540;&#65292;&#30830;&#20445;&#25968;&#25454;&#36129;&#29486;&#32773;&#24471;&#21040;&#20844;&#24179;&#30340;&#34917;&#20607;&#25110;&#35748;&#21487;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22312;&#29702;&#35770;&#19978;&#26377;&#21160;&#26426;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25968;&#25454;&#24402;&#22240;&#65292;&#20197;&#25913;&#21892;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#26524;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;CIFAR-10&#21644;CelebA&#19978;&#35757;&#32451;&#30340;DDPM&#20197;&#21450;&#22312;ArtBench&#19978;&#36827;&#34892;&#32454;&#35843;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;LoRA&#30340;&#24402;&#22240;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#29702;&#35770;&#19978;&#19981;&#21512;&#29702;&#30340;&#35774;&#35745;&#36873;&#25321;&#22312;&#23454;&#38469;&#20013;&#22823;&#24133;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#22522;&#32447;&#65292;&#26080;&#35770;&#26159;&#22312;&#32447;&#24615;&#25968;&#25454;&#24314;&#27169;&#24471;&#20998;&#36824;&#26159;&#21453;&#20107;&#23454;&#35780;&#20272;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21576;&#29616;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data attribution seeks to trace model outputs back to training data. With the recent development of diffusion models, data attribution has become a desired module to properly assign valuations for high-quality or copyrighted training samples, ensuring that data contributors are fairly compensated or credited. Several theoretically motivated methods have been proposed to implement data attribution, in an effort to improve the trade-off between computational scalability and effectiveness. In this work, we conduct extensive experiments and ablation studies on attributing diffusion models, specifically focusing on DDPMs trained on CIFAR-10 and CelebA, as well as a Stable Diffusion model LoRA-finetuned on ArtBench. Intriguingly, we report counter-intuitive observations that theoretically unjustified design choices for attribution empirically outperform previous baselines by a large margin, in terms of both linear datamodeling score and counterfactual evaluation. Our work presents a signific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#22686;&#24378;&#30340;&#22810;&#35270;&#35282;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#35299;&#20915;POI&#25512;&#33616;&#20013;&#19981;&#30830;&#23450;&#22240;&#32032;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#20010;&#20154;POI&#36716;&#25442;&#22270;&#12289;&#22522;&#20110;&#35821;&#20041;&#30340;POI&#22270;&#21644;&#22522;&#20110;&#36317;&#31163;&#30340;POI&#22270;&#26469;&#20840;&#38754;&#24314;&#27169;&#20381;&#36182;&#20851;&#31995;&#65292;&#25552;&#39640;POI&#25512;&#33616;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00491</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#22686;&#24378;&#30340;&#22810;&#35270;&#35282;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#24378;&#22823;&#30340;POI&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Bayes-enhanced Multi-view Attention Networks for Robust POI Recommendation. (arXiv:2311.00491v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#22686;&#24378;&#30340;&#22810;&#35270;&#35282;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#35299;&#20915;POI&#25512;&#33616;&#20013;&#19981;&#30830;&#23450;&#22240;&#32032;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#20010;&#20154;POI&#36716;&#25442;&#22270;&#12289;&#22522;&#20110;&#35821;&#20041;&#30340;POI&#22270;&#21644;&#22522;&#20110;&#36317;&#31163;&#30340;POI&#22270;&#26469;&#20840;&#38754;&#24314;&#27169;&#20381;&#36182;&#20851;&#31995;&#65292;&#25552;&#39640;POI&#25512;&#33616;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
POI&#25512;&#33616;&#22312;&#20419;&#36827;&#21508;&#31181;&#22522;&#20110;&#20301;&#32622;&#30340;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#26041;&#38754;&#20855;&#26377;&#23454;&#38469;&#37325;&#35201;&#24615;&#65292;&#24182;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#20316;&#21697;&#36890;&#24120;&#20551;&#35774;&#29992;&#25143;&#25253;&#21578;&#30340;&#21487;&#29992;POI&#31614;&#21040;&#26159;&#29992;&#25143;&#34892;&#20026;&#30340;&#21807;&#19968;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#20027;&#35266;&#21644;&#23458;&#35266;&#21407;&#22240;&#65288;&#21253;&#25324;&#23450;&#20301;&#35823;&#24046;&#21644;&#29992;&#25143;&#38544;&#31169;&#38382;&#39064;&#65289;&#65292;&#31614;&#21040;&#25968;&#25454;&#21487;&#33021;&#30456;&#24403;&#19981;&#21487;&#38752;&#65292;&#36825;&#23545;POI&#25512;&#33616;&#30340;&#24615;&#33021;&#36896;&#25104;&#20102;&#26174;&#33879;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#31614;&#21040;&#30340;&#19981;&#30830;&#23450;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#22686;&#24378;&#30340;&#22810;&#35270;&#35282;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#36827;&#34892;&#24378;&#22823;&#30340;POI&#25512;&#33616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20010;&#20154;POI&#36716;&#25442;&#22270;&#12289;&#22522;&#20110;&#35821;&#20041;&#30340;POI&#22270;&#21644;&#22522;&#20110;&#36317;&#31163;&#30340;POI&#22270;&#65292;&#20840;&#38754;&#24314;&#27169;&#20102;POI&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#30001;&#20110;&#20010;&#20154;POI&#36716;&#25442;&#22270;&#36890;&#24120;&#31232;&#30095;&#19988;&#23545;&#22122;&#22768;&#25935;&#24863;&#65292;&#25152;&#20197;&#25105;&#20204;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#22686;&#24378;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
POI recommendation is practically important to facilitate various Location-Based Social Network services, and has attracted rising research attention recently. Existing works generally assume the available POI check-ins reported by users are the ground-truth depiction of user behaviors. However, in real application scenarios, the check-in data can be rather unreliable due to both subjective and objective causes including positioning error and user privacy concerns, leading to significant negative impacts on the performance of the POI recommendation. To this end, we investigate a novel problem of robust POI recommendation by considering the uncertainty factors of the user check-ins, and proposes a Bayes-enhanced Multi-view Attention Network. Specifically, we construct personal POI transition graph, the semantic-based POI graph and distance-based POI graph to comprehensively model the dependencies among the POIs. As the personal POI transition graph is usually sparse and sensitive to noi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#37325;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;DCDM&#65289;&#65292;&#29992;&#20110;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;&#20013;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#23384;&#22312;&#39640;&#24230;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#22823;&#37327;&#20869;&#37096;&#21464;&#24322;&#24615;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;&#20013;&#30340;&#24515;&#33039;&#35270;&#22270;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#25298;&#32477;&#31867;&#20284;&#30340;&#22806;&#20998;&#24067;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2311.00469</link><description>&lt;p&gt;
&#21452;&#37325;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#22806;&#20998;&#24067;&#26816;&#27979;&#65306;&#24212;&#29992;&#20110;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Dual Conditioned Diffusion Models for Out-Of-Distribution Detection: Application to Fetal Ultrasound Videos. (arXiv:2311.00469v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#37325;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;DCDM&#65289;&#65292;&#29992;&#20110;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;&#20013;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#23384;&#22312;&#39640;&#24230;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#22823;&#37327;&#20869;&#37096;&#21464;&#24322;&#24615;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;&#20013;&#30340;&#24515;&#33039;&#35270;&#22270;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#25298;&#32477;&#31867;&#20284;&#30340;&#22806;&#20998;&#24067;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#23545;&#20110;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#33021;&#22815;&#26816;&#27979;&#20986;&#19981;&#23646;&#20110;&#35757;&#32451;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;OOD&#26679;&#26412;&#21487;&#33021;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#35757;&#32451;&#20998;&#24067;&#65288;ID&#65289;&#20869;&#37096;&#23384;&#22312;&#26174;&#33879;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#19988;ID&#21644;OOD&#31867;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;&#20013;&#26816;&#27979;&#24515;&#33039;&#35270;&#22270;&#26102;&#65292;&#24515;&#33039;&#21644;&#33145;&#37096;&#31561;&#20854;&#20182;&#35299;&#21078;&#32467;&#26500;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#27599;&#20010;&#35270;&#22270;&#20869;&#37096;&#23384;&#22312;&#30528;5&#31181;&#19981;&#21516;&#30340;&#35270;&#22270;&#21644;&#32467;&#26500;&#21464;&#21270;&#12290;&#20026;&#20102;&#26816;&#27979;&#27492;&#32972;&#26223;&#19979;&#30340;OOD&#26679;&#26412;&#65292;&#25152;&#24471;&#27169;&#22411;&#24212;&#33021;&#22815;&#23398;&#20064;&#21040;&#35299;&#21078;&#32467;&#26500;&#20869;&#37096;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#33021;&#22815;&#25298;&#32477;&#31867;&#20284;&#30340;OOD&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21452;&#37325;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;DCDM&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;ID&#31867;&#20449;&#24687;&#21644;&#36755;&#20837;&#22270;&#20687;&#30340;&#28508;&#22312;&#29305;&#24449;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#26465;&#20214;&#21270;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is essential to improve the reliability of machine learning models by detecting samples that do not belong to the training distribution. Detecting OOD samples effectively in certain tasks can pose a challenge because of the substantial heterogeneity within the in-distribution (ID), and the high structural similarity between ID and OOD classes. For instance, when detecting heart views in fetal ultrasound videos there is a high structural similarity between the heart and other anatomies such as the abdomen, and large in-distribution variance as a heart has 5 distinct views and structural variations within each view. To detect OOD samples in this context, the resulting model should generalise to the intra-anatomy variations while rejecting similar OOD samples. In this paper, we introduce dual-conditioned diffusion models (DCDM) where we condition the model on in-distribution class information and latent features of the input image for reconstruction-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#32454;&#32990;&#26426;&#22120;&#20154;&#31895;&#32454;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;&#26354;&#23884;&#20837;&#26694;&#26550;&#22312;&#20849;&#20139;&#30340;&#21452;&#26354;&#31354;&#38388;&#20869;&#32479;&#19968;&#20102;&#21508;&#31181;&#31890;&#24230;&#30340;&#26426;&#22120;&#20154;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#33258;&#20027;&#22320;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#30830;&#23450;&#25506;&#32034;&#30340;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2311.00462</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#26354;&#23884;&#20837;&#36827;&#34892;&#31895;&#32454;&#35774;&#35745;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design. (arXiv:2311.00462v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#32454;&#32990;&#26426;&#22120;&#20154;&#31895;&#32454;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;&#26354;&#23884;&#20837;&#26694;&#26550;&#22312;&#20849;&#20139;&#30340;&#21452;&#26354;&#31354;&#38388;&#20869;&#32479;&#19968;&#20102;&#21508;&#31181;&#31890;&#24230;&#30340;&#26426;&#22120;&#20154;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#33258;&#20027;&#22320;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#30830;&#23450;&#25506;&#32034;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32454;&#32990;&#26426;&#22120;&#20154;&#35774;&#35745;&#26088;&#22312;&#21019;&#24314;&#30001;&#35768;&#22810;&#32454;&#32990;&#32452;&#25104;&#30340;&#26426;&#22120;&#20154;&#65292;&#20197;&#20415;&#33021;&#22815;&#39640;&#25928;&#22320;&#25511;&#21046;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#29983;&#25104;&#21508;&#31181;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#30452;&#25509;&#22312;&#24222;&#22823;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#20248;&#21270;&#65292;&#23548;&#33268;&#20102;&#38590;&#20197;&#25511;&#21046;&#30340;&#22797;&#26434;&#24418;&#24577;&#30340;&#26426;&#22120;&#20154;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#32454;&#32990;&#26426;&#22120;&#20154;&#31895;&#32454;&#35774;&#35745;&#26041;&#27861;&#12290;&#35813;&#31574;&#30053;&#39318;&#20808;&#23547;&#27714;&#26368;&#20339;&#30340;&#31895;&#31890;&#24230;&#26426;&#22120;&#20154;&#65292;&#28982;&#21518;&#36880;&#27493;&#23545;&#20854;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#22312;&#31895;&#32454;&#36716;&#25442;&#36807;&#31243;&#20013;&#30830;&#23450;&#31934;&#32454;&#35843;&#25972;&#20851;&#33410;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;&#21452;&#26354;&#23884;&#20837; (HERD) &#26694;&#26550;&#12290;HERD &#22312;&#19968;&#20010;&#20849;&#20139;&#30340;&#21452;&#26354;&#31354;&#38388;&#20869;&#32479;&#19968;&#20102;&#21508;&#31181;&#31890;&#24230;&#30340;&#26426;&#22120;&#20154;&#65292;&#24182;&#21033;&#29992;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#20010;&#26694;&#26550;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#33258;&#20027;&#22320;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#30830;&#23450;&#25506;&#32034;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-cellular robot design aims to create robots comprised of numerous cells that can be efficiently controlled to perform diverse tasks. Previous research has demonstrated the ability to generate robots for various tasks, but these approaches often optimize robots directly in the vast design space, resulting in robots with complicated morphologies that are hard to control. In response, this paper presents a novel coarse-to-fine method for designing multi-cellular robots. Initially, this strategy seeks optimal coarse-grained robots and progressively refines them. To mitigate the challenge of determining the precise refinement juncture during the coarse-to-fine transition, we introduce the Hyperbolic Embeddings for Robot Design (HERD) framework. HERD unifies robots of various granularity within a shared hyperbolic space and leverages a refined Cross-Entropy Method for optimization. This framework enables our method to autonomously identify areas of exploration in hyperbolic space and c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#32511;&#33394;&#35745;&#31639;&#39046;&#22495;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#35745;&#31639;&#36164;&#28304;&#21644;&#29615;&#22659;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.00447</link><description>&lt;p&gt;
&#20851;&#20110;&#32511;&#33394;&#35745;&#31639;&#30340;&#26426;&#36935;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Opportunities of Green Computing: A Survey. (arXiv:2311.00447v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00447
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#32511;&#33394;&#35745;&#31639;&#39046;&#22495;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#35745;&#31639;&#36164;&#28304;&#21644;&#29615;&#22659;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#25216;&#26415;&#21644;&#30740;&#31350;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12289;&#35821;&#38899;&#21512;&#25104;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#32773;&#20851;&#27880;&#20110;&#36861;&#27714;&#26032;&#30340;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#32467;&#26524;&#65292;&#23548;&#33268;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#12290;&#23545;&#20110;&#39640;&#35745;&#31639;&#33021;&#21147;&#30340;&#38656;&#27714;&#23548;&#33268;&#26356;&#39640;&#30340;&#30899;&#25490;&#25918;&#65292;&#24182;&#36890;&#36807;&#38459;&#27490;&#36164;&#37329;&#26377;&#38480;&#30340;&#23567;&#22411;&#25110;&#20013;&#22411;&#30740;&#31350;&#26426;&#26500;&#21644;&#20844;&#21496;&#21442;&#19982;&#30740;&#31350;&#26469;&#30772;&#22351;&#30740;&#31350;&#20844;&#24179;&#24615;&#12290;&#20026;&#24212;&#23545;&#35745;&#31639;&#36164;&#28304;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#29615;&#22659;&#24433;&#21709;&#30340;&#25361;&#25112;&#65292;&#32511;&#33394;&#35745;&#31639;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#32511;&#33394;&#35745;&#31639;&#20013;&#20351;&#29992;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;G&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has achieved significant advancements in technology and research with the development over several decades, and is widely used in many areas including computing vision, natural language processing, time-series analysis, speech synthesis, etc. During the age of deep learning, especially with the arise of Large Language Models, a large majority of researchers' attention is paid on pursuing new state-of-the-art (SOTA) results, resulting in ever increasing of model size and computational complexity. The needs for high computing power brings higher carbon emission and undermines research fairness by preventing small or medium-sized research institutions and companies with limited funding in participating in research. To tackle the challenges of computing resources and environmental impact of AI, Green Computing has become a hot research topic. In this survey, we give a systematic overview of the technologies used in Green Computing. We propose the framework of G
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#21512;&#36923;&#36753;&#65292;&#29978;&#33267;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#65292;&#20294;&#21363;&#20351;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#19982;&#20154;&#31867;&#25512;&#29702;&#31867;&#20284;&#30340;&#38169;&#35823;&#65292;&#24635;&#20307;&#19978;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#20154;&#31867;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2311.00445</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19977;&#27573;&#35770;&#25512;&#29702;&#30340;&#31995;&#32479;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. (arXiv:2311.00445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00445
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#21512;&#36923;&#36753;&#65292;&#29978;&#33267;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#65292;&#20294;&#21363;&#20351;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#19982;&#20154;&#31867;&#25512;&#29702;&#31867;&#20284;&#30340;&#38169;&#35823;&#65292;&#24635;&#20307;&#19978;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#20154;&#31867;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24615;&#34892;&#20026;&#30340;&#19968;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26159;&#36923;&#36753;&#25512;&#29702;&#65306;&#30830;&#23450;&#21738;&#20123;&#32467;&#35770;&#21487;&#20197;&#20174;&#19968;&#32452;&#21069;&#25552;&#20013;&#24471;&#20986;&#12290;&#24515;&#29702;&#23398;&#23478;&#24050;&#32463;&#35760;&#24405;&#19979;&#20154;&#31867;&#25512;&#29702;&#19982;&#36923;&#36753;&#35268;&#21017;&#19981;&#31526;&#30340;&#20960;&#31181;&#26041;&#24335;&#12290;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22797;&#21046;&#36825;&#20123;&#20559;&#24046;&#65292;&#25110;&#32773;&#23427;&#20204;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#20559;&#24046;&#65311;&#25105;&#20204;&#20851;&#27880;&#19977;&#27573;&#35770;&#30340;&#24773;&#20917; - &#20174;&#20004;&#20010;&#31616;&#21333;&#21069;&#25552;&#20013;&#25512;&#23548;&#20986;&#30340;&#25512;&#29702;&#65292;&#36825;&#22312;&#24515;&#29702;&#23398;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350; - &#25105;&#20204;&#21457;&#29616;&#36739;&#22823;&#30340;&#27169;&#22411;&#27604;&#36739;&#21512;&#36923;&#36753;&#65292;&#32780;&#19988;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21363;&#20351;&#26159;&#26368;&#22823;&#30340;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#31995;&#32479;&#24615;&#38169;&#35823;&#65292;&#20854;&#20013;&#19968;&#20123;&#38169;&#35823;&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;&#20559;&#35265;&#30456;&#20284;&#65292;&#20363;&#22914;&#25490;&#24207;&#25928;&#24212;&#21644;&#36923;&#36753;&#35884;&#35823;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#27169;&#20223;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#20154;&#31867;&#20559;&#35265;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans' inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate these biases, or are they able to overcome them? Focusing on the case of syllogisms -- inferences from two simple premises, which have been studied extensively in psychology -- we show that larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases such as ordering effects and logical fallacies. Overall, we find that language models mimic the human biases included in their training data, but are able to overcome them in some cases.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Dynamic Scanning Augmentation&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;Vision Transformer&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#24212;&#24615;&#22320;&#32858;&#28966;&#20110;&#19981;&#21516;&#30340;&#22270;&#20687;&#22359;&#65292;&#24182;&#25913;&#21464;&#20102;ViT&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2311.00441</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#30340;&#21160;&#24577;&#25195;&#25551;&#22686;&#24378;&#25216;&#26415;&#25913;&#36827;Vision Transformer&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness for Vision Transformer with a Simple Dynamic Scanning Augmentation. (arXiv:2311.00441v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00441
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Dynamic Scanning Augmentation&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;Vision Transformer&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#24212;&#24615;&#22320;&#32858;&#28966;&#20110;&#19981;&#21516;&#30340;&#22270;&#20687;&#22359;&#65292;&#24182;&#25913;&#21464;&#20102;ViT&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT)&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#40065;&#26834;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;ViT&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21160;&#24577;&#25195;&#25551;&#22686;&#24378;&#8221;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#21033;&#29992;&#21160;&#24577;&#36755;&#20837;&#24207;&#21015;&#26469;&#33258;&#36866;&#24212;&#22320;&#32858;&#28966;&#20110;&#19981;&#21516;&#30340;&#22270;&#20687;&#22359;&#65292;&#20174;&#32780;&#20445;&#25345;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#35814;&#32454;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#31181;&#23545;&#36755;&#20837;&#24207;&#21015;&#30340;&#36866;&#24212;&#24615;&#20250;&#23548;&#33268;ViT&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#65292;&#21363;&#20351;&#23545;&#20110;&#30456;&#21516;&#30340;&#22270;&#20687;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22235;&#31181;Dynamic Scanning Augmentation&#30340;&#21464;&#20307;&#65292; &#22312;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21644;&#23545;&#33258;&#28982;&#22270;&#20687;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#22343;&#32988;&#36807;ViT&#65292;&#20854;&#20013;&#19968;&#31181;&#21464;&#20307;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has demonstrated promising performance in computer vision tasks, comparable to state-of-the-art neural networks. Yet, this new type of deep neural network architecture is vulnerable to adversarial attacks limiting its capabilities in terms of robustness. This article presents a novel contribution aimed at further improving the accuracy and robustness of ViT, particularly in the face of adversarial attacks. We propose an augmentation technique called `Dynamic Scanning Augmentation' that leverages dynamic input sequences to adaptively focus on different patches, thereby maintaining performance and robustness. Our detailed investigations reveal that this adaptability to the input sequence induces significant changes in the attention mechanism of ViT, even for the same image. We introduce four variations of Dynamic Scanning Augmentation, outperforming ViT in terms of both robustness to adversarial attacks and accuracy against natural images, with one variant showin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#20808;&#32423;&#25490;&#24207;&#21644;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#33258;&#25105;&#27169;&#20223;&#24378;&#21270;&#23398;&#20064;&#22312;&#36807;&#31243;&#21270;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00426</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#22870;&#21169;&#30340;&#33258;&#25105;&#27169;&#20223;&#24378;&#21270;&#23398;&#20064;&#22312;&#36807;&#31243;&#21270;&#29615;&#22659;&#20013;&#36890;&#36807;&#20248;&#20808;&#32423;&#21644;&#22810;&#26679;&#24615;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhanced Generalization through Prioritization and Diversity in Self-Imitation Reinforcement Learning over Procedural Environments with Sparse Rewards. (arXiv:2311.00426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#20808;&#32423;&#25490;&#24207;&#21644;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#33258;&#25105;&#27169;&#20223;&#24378;&#21270;&#23398;&#20064;&#22312;&#36807;&#31243;&#21270;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#26234;&#33021;&#20307;&#23398;&#20064;&#26368;&#20248;&#20915;&#31574;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#32570;&#20047;&#20449;&#24687;&#21453;&#39304;&#20449;&#21495;&#12290;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#21033;&#29992;&#22238;&#25918;&#32531;&#20914;&#21306;&#26469;&#23384;&#20648;&#21644;&#37325;&#29616;&#25104;&#21151;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#22312;&#27867;&#21270;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#36807;&#31243;&#21270;&#29983;&#25104;&#30340;&#29615;&#22659;&#20013;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#19981;&#21516;&#26041;&#24335;&#23545;&#36716;&#25442;&#36827;&#34892;&#20248;&#20808;&#32423;&#25490;&#24207;&#30340;&#23450;&#21046;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#23558;&#20248;&#20808;&#32423;&#25216;&#26415;&#25193;&#23637;&#21040;&#36807;&#31243;&#21270;&#29983;&#25104;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20462;&#25913;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#30340;&#28436;&#31034;&#26469;&#35299;&#20915;&#22810;&#26679;&#24615;&#25439;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration poses a fundamental challenge in Reinforcement Learning (RL) with sparse rewards, limiting an agent's ability to learn optimal decision-making due to a lack of informative feedback signals. Self-Imitation Learning (self-IL) has emerged as a promising approach for exploration, leveraging a replay buffer to store and reproduce successful behaviors. However, traditional self-IL methods, which rely on high-return transitions and assume singleton environments, face challenges in generalization, especially in procedurally-generated (PCG) environments. Therefore, new self-IL methods have been proposed to rank which experiences to persist, but they replay transitions uniformly regardless of their significance, and do not address the diversity of the stored demonstrations. In this work, we propose tailored self-IL sampling strategies by prioritizing transitions in different ways and extending prioritization techniques to PCG environments. We also address diversity loss through modif
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#29289;&#20307;&#21644;&#22330;&#26223;&#29615;&#22659;&#20132;&#20114;&#30340;&#31070;&#32463;&#38544;&#24335;&#22330;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#20869;&#22312;&#20998;&#35299;&#26041;&#27861;&#25104;&#21151;&#22320;&#20998;&#31163;&#20102;&#29289;&#20307;&#21644;&#22330;&#26223;&#29615;&#22659;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#21512;&#29702;&#30340;&#22330;&#26223;&#22806;&#35266;&#32534;&#36753;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00425</link><description>&lt;p&gt;
&#32771;&#34385;&#29289;&#20307;-&#29615;&#22659;&#20132;&#20114;&#30340;&#31070;&#32463;&#38544;&#24335;&#22330;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Field Editing Considering Object-environment Interaction. (arXiv:2311.00425v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#29289;&#20307;&#21644;&#22330;&#26223;&#29615;&#22659;&#20132;&#20114;&#30340;&#31070;&#32463;&#38544;&#24335;&#22330;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#20869;&#22312;&#20998;&#35299;&#26041;&#27861;&#25104;&#21151;&#22320;&#20998;&#31163;&#20102;&#29289;&#20307;&#21644;&#22330;&#26223;&#29615;&#22659;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#21512;&#29702;&#30340;&#22330;&#26223;&#22806;&#35266;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;3D&#22330;&#26223;&#32534;&#36753;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22312;3D&#32534;&#36753;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#24573;&#30053;&#20102;&#29289;&#20307;&#19982;&#22330;&#26223;&#29615;&#22659;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#22330;&#26223;&#22806;&#35266;&#30340;&#21464;&#21270;&#65292;&#22914;&#38452;&#24433;&#65292;&#22312;&#28210;&#26579;&#35270;&#22270;&#20013;&#26080;&#27861;&#26174;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#29289;&#20307;&#21644;&#22330;&#26223;&#29615;&#22659;&#20132;&#20114;&#30340;&#26032;&#22411;&#20004;&#27969;&#31070;&#32463;&#28210;&#26579;&#31995;&#32479;(OSI-aware)&#65292;&#36890;&#36807;&#20869;&#22312;&#20998;&#35299;&#26041;&#27861;&#25104;&#21151;&#22320;&#20998;&#31163;&#20102;&#29289;&#20307;&#21644;&#22330;&#26223;&#29615;&#22659;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#20026;&#20102;&#30740;&#31350;&#29289;&#20307;&#32423;&#21035;&#32534;&#36753;&#20219;&#21153;&#23545;&#22330;&#26223;&#22806;&#35266;&#30340;&#30456;&#24212;&#25913;&#21464;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28145;&#24230;&#22270;&#24341;&#23548;&#30340;&#22330;&#26223;&#20462;&#22797;&#26041;&#27861;&#21644;&#28857;&#21305;&#37197;&#31574;&#30053;&#30340;&#38452;&#24433;&#28210;&#26579;&#26041;&#27861;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26032;&#27969;&#31243;&#20135;&#29983;&#20102;&#21512;&#29702;&#30340;&#22806;&#35266;
&lt;/p&gt;
&lt;p&gt;
The 3D scene editing method based on neural implicit field has gained wide attention. It has achieved excellent results in 3D editing tasks. However, existing methods often blend the interaction between objects and scene environment. The change of scene appearance like shadows is failed to be displayed in the rendering view. In this paper, we propose an Object and Scene environment Interaction aware (OSI-aware) system, which is a novel two-stream neural rendering system considering object and scene environment interaction. To obtain illuminating conditions from the mixture soup, the system successfully separates the interaction between objects and scene environment by intrinsic decomposition method. To study the corresponding changes to the scene appearance from object-level editing tasks, we introduce a depth map guided scene inpainting method and shadow rendering method by point matching strategy. Extensive experiments demonstrate that our novel pipeline produce reasonable appearance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24102;&#22827;&#22971;&#30340;&#21307;&#38498;/&#23621;&#27665;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#20197;&#21450;&#23545;&#31283;&#23450;B&#21305;&#37197;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#25509;&#36817;&#21487;&#34892;&#30340;&#31283;&#23450;&#21305;&#37197;&#65292;&#24182;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2311.00405</link><description>&lt;p&gt;
&#22827;&#22971;&#21487;&#20197;&#34987;&#35299;&#20915;&#65306;&#26032;&#30340;&#31639;&#27861;&#21644;&#23545;&#24102;&#22827;&#22971;&#30340;&#21307;&#38498;/&#23621;&#27665;&#38382;&#39064;&#30340;&#38590;&#24230;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Couples can be tractable: New algorithms and hardness results for the Hospitals / Residents problem with Couples. (arXiv:2311.00405v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24102;&#22827;&#22971;&#30340;&#21307;&#38498;/&#23621;&#27665;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#20197;&#21450;&#23545;&#31283;&#23450;B&#21305;&#37197;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#25509;&#36817;&#21487;&#34892;&#30340;&#31283;&#23450;&#21305;&#37197;&#65292;&#24182;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#22827;&#22971;&#30340;&#21307;&#38498;/&#23621;&#27665;&#38382;&#39064;&#65288;Hospitals / Residents problem with Couples&#65292;&#31616;&#31216;HRC&#65289;&#65292;&#20854;&#20013;&#35299;&#30340;&#19968;&#31181;&#26159;&#31283;&#23450;&#21305;&#37197;&#65292;&#25110;&#32773;&#25253;&#21578;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#24102;&#22827;&#22971;&#30340;HRC&#23454;&#20363;&#20013;&#25214;&#21040;&#19968;&#20010;&#25509;&#36817;&#21487;&#34892;&#30340;&#31283;&#23450;&#21305;&#37197;&#65288;&#36890;&#36807;&#26368;&#22810;&#35843;&#25972;&#21307;&#38498;&#30340;&#23481;&#37327;1&#20010;&#21333;&#20301;&#65289;&#65292;&#20854;&#20013;&#22827;&#22971;&#30340;&#20559;&#22909;&#26159;&#23376;&#21709;&#24212;&#24615;&#30340;&#65288;&#21363;&#65292;&#22914;&#26524;&#19968;&#20010;&#25104;&#21592;&#36716;&#31227;&#21040;&#19968;&#20010;&#26356;&#22909;&#30340;&#21307;&#38498;&#65292;&#37027;&#20040;&#22827;&#22971;&#20063;&#20250;&#21464;&#24471;&#26356;&#22909;&#65289;&#21644;&#23376;&#23436;&#22791;&#24615;&#30340;&#65288;&#21363;&#65292;&#23545;&#20110;&#27599;&#23545;&#20010;&#21035;&#21487;&#25509;&#21463;&#30340;&#21307;&#38498;&#37117;&#26159;&#22827;&#22971;&#19968;&#36215;&#21487;&#25509;&#21463;&#30340;&#65289;&#65292;&#36890;&#36807;&#23558;&#20854;&#35268;&#32422;&#21040;&#31283;&#23450;&#22266;&#23450;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;HRC&#23376;&#21709;&#24212;&#24615;&#12289;&#23376;&#23436;&#22791;&#23454;&#20363;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#35813;&#23454;&#20363;&#26159;&#19968;&#20010;&#21452;&#37325;&#24066;&#22330;&#65292;&#25110;&#32773;&#25152;&#26377;&#22827;&#22971;&#23646;&#20110;&#20960;&#31181;&#21487;&#33021;&#31867;&#22411;&#20043;&#19968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20063;&#24847;&#21619;&#30528;&#31283;&#23450;B&#21305;&#37197;&#38382;&#39064;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#35299;&#24615;&#65292;&#20854;&#20013;&#24213;&#23618;&#22270;&#26159;&#24102;&#26377;&#29615;&#30340;&#22810;&#37325;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we study the {\sc Hospitals / Residents problem with Couples} ({\sc hrc}), where a solution is a stable matching or a report that none exists. We present a novel polynomial-time algorithm that can find a near-feasible stable matching (adjusting the hospitals' capacities by at most 1) in an {\sc hrc} instance where the couples' preferences are sub-responsive (i.e., if one member switches to a better hospital, than the couple also improves) and sub-complete (i.e., each pair of hospitals that are individually acceptable to both members are jointly acceptable for the couple) by reducing it to an instance of the {\sc Stable Fixtures} problem. We also present a polynomial-time algorithm for {\sc hrc} in a sub-responsive, sub-complete instance that is a Dual Market, or where all couples are one of several possible types. We show that our algorithm also implies the polynomial-time solvability of a stable b-matching problem, where the underlying graph is a multigraph with loops.  
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#21464;&#25442;&#22120;&#30340;&#26694;&#26550;&#65288;STTF&#65289;&#65292;&#29992;&#20110;&#25945;&#32946;&#22330;&#26223;&#20013;&#30340;&#20154;&#20307;&#23039;&#21183;&#35780;&#20272;&#21644;&#20462;&#27491;&#12290;&#26694;&#26550;&#21253;&#25324;&#39592;&#39612;&#36319;&#36394;&#12289;&#23039;&#21183;&#20272;&#35745;&#12289;&#23039;&#21183;&#35780;&#20272;&#21644;&#23039;&#21183;&#20462;&#27491;&#27169;&#22359;&#65292;&#33021;&#22815;&#25552;&#20379;&#19987;&#19994;&#12289;&#24555;&#36895;&#20462;&#27491;&#30340;&#21453;&#39304;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#36741;&#21161;&#30340;&#24418;&#24335;&#25552;&#20379;&#20462;&#27491;&#24615;&#21453;&#39304;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#23398;&#29983;&#34892;&#20026;&#30340;&#36136;&#37327;&#24182;&#36827;&#34892;&#28857;&#35780;&#12290;</title><link>http://arxiv.org/abs/2311.00401</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#21464;&#25442;&#22120;&#30340;&#25945;&#32946;&#22330;&#26223;&#19979;&#20154;&#20307;&#23039;&#21183;&#35780;&#20272;&#19982;&#20462;&#27491;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Spatial-Temporal Transformer based Framework For Human Pose Assessment And Correction in Education Scenarios. (arXiv:2311.00401v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#21464;&#25442;&#22120;&#30340;&#26694;&#26550;&#65288;STTF&#65289;&#65292;&#29992;&#20110;&#25945;&#32946;&#22330;&#26223;&#20013;&#30340;&#20154;&#20307;&#23039;&#21183;&#35780;&#20272;&#21644;&#20462;&#27491;&#12290;&#26694;&#26550;&#21253;&#25324;&#39592;&#39612;&#36319;&#36394;&#12289;&#23039;&#21183;&#20272;&#35745;&#12289;&#23039;&#21183;&#35780;&#20272;&#21644;&#23039;&#21183;&#20462;&#27491;&#27169;&#22359;&#65292;&#33021;&#22815;&#25552;&#20379;&#19987;&#19994;&#12289;&#24555;&#36895;&#20462;&#27491;&#30340;&#21453;&#39304;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#36741;&#21161;&#30340;&#24418;&#24335;&#25552;&#20379;&#20462;&#27491;&#24615;&#21453;&#39304;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#23398;&#29983;&#34892;&#20026;&#30340;&#36136;&#37327;&#24182;&#36827;&#34892;&#28857;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#23039;&#21183;&#35780;&#20272;&#21644;&#20462;&#27491;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#20307;&#32946;&#20998;&#26512;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#23089;&#20048;&#31561;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#21464;&#25442;&#22120;&#30340;&#26694;&#26550;&#65288;STTF&#65289;&#65292;&#29992;&#20110;&#25945;&#32946;&#22330;&#26223;&#20013;&#30340;&#20154;&#20307;&#23039;&#21183;&#35780;&#20272;&#21644;&#20462;&#27491;&#65292;&#20363;&#22914;&#20307;&#32946;&#38203;&#28860;&#21644;&#31185;&#23398;&#23454;&#39564;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#39592;&#39612;&#36319;&#36394;&#12289;&#23039;&#21183;&#20272;&#35745;&#12289;&#23039;&#21183;&#35780;&#20272;&#21644;&#23039;&#21183;&#20462;&#27491;&#27169;&#22359;&#65292;&#20197;&#21521;&#23398;&#29983;&#25552;&#20379;&#19987;&#19994;&#12289;&#24555;&#36895;&#20462;&#27491;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#23039;&#21183;&#20462;&#27491;&#26041;&#27861;&#65292;&#20197;&#35270;&#35273;&#36741;&#21161;&#30340;&#24418;&#24335;&#25552;&#20379;&#20462;&#27491;&#24615;&#21453;&#39304;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#24049;&#30340;&#25968;&#25454;&#38598;&#23545;&#35813;&#26694;&#26550;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;&#65288;a&#65289;&#20116;&#20010;&#38203;&#28860;&#21160;&#20316;&#30340;&#26032;&#24405;&#21046;&#65292;&#65288;b&#65289;&#22312;&#20114;&#32852;&#32593;&#19978;&#25214;&#21040;&#30340;&#21516;&#19968;&#38203;&#28860;&#21160;&#20316;&#30340;&#29616;&#26377;&#24405;&#21046;&#65292;&#20197;&#21450;&#65288;c&#65289;&#30001;&#19987;&#19994;&#36816;&#21160;&#21592;&#21644;&#25945;&#24072;&#23545;&#24405;&#21046;&#30340;&#20462;&#27491;&#24615;&#21453;&#39304;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#23398;&#29983;&#34892;&#20026;&#30340;&#36136;&#37327;&#24182;&#36827;&#34892;&#28857;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human pose assessment and correction play a crucial role in applications across various fields, including computer vision, robotics, sports analysis, healthcare, and entertainment. In this paper, we propose a Spatial-Temporal Transformer based Framework (STTF) for human pose assessment and correction in education scenarios such as physical exercises and science experiment. The framework comprising skeletal tracking, pose estimation, posture assessment, and posture correction modules to educate students with professional, quick-to-fix feedback. We also create a pose correction method to provide corrective feedback in the form of visual aids. We test the framework with our own dataset. It comprises (a) new recordings of five exercises, (b) existing recordings found on the internet of the same exercises, and (c) corrective feedback on the recordings by professional athletes and teachers. Results show that our model can effectively measure and comment on the quality of students' actions. T
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#31070;&#32463;-&#31526;&#21495;AI&#26694;&#26550;&#65292;&#22312;&#25945;&#32946;&#24212;&#29992;&#20013;&#35299;&#20915;&#38590;&#20197;&#25972;&#21512;&#31526;&#21495;&#25945;&#32946;&#30693;&#35782;&#12289;&#23398;&#20064;&#20559;&#35265;&#21644;&#32570;&#20047;&#35299;&#37322;&#24615;&#31561;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;NSAI&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.00393</link><description>&lt;p&gt;
&#29992;&#31526;&#21495;&#30693;&#35782;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65306;&#23454;&#29616;&#21487;&#20449;&#36182;&#21644;&#21487;&#35299;&#37322;&#30340;&#25945;&#32946;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Augmenting deep neural networks with symbolic knowledge: Towards trustworthy and interpretable AI for education. (arXiv:2311.00393v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00393
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#31070;&#32463;-&#31526;&#21495;AI&#26694;&#26550;&#65292;&#22312;&#25945;&#32946;&#24212;&#29992;&#20013;&#35299;&#20915;&#38590;&#20197;&#25972;&#21512;&#31526;&#21495;&#25945;&#32946;&#30693;&#35782;&#12289;&#23398;&#20064;&#20559;&#35265;&#21644;&#32570;&#20047;&#35299;&#37322;&#24615;&#31561;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;NSAI&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#24050;&#32463;&#25104;&#20026;&#25945;&#32946;&#24212;&#29992;&#20013;&#26368;&#37325;&#35201;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20043;&#19968;&#65292;&#25552;&#20379;&#33258;&#36866;&#24212;&#25945;&#32946;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#25945;&#32946;&#28508;&#21147;&#21463;&#21040;&#38480;&#21046;&#65306;&#19968;&#26159;&#38590;&#20197;&#23558;&#31526;&#21495;&#25945;&#32946;&#30693;&#35782;&#65288;&#22914;&#22240;&#26524;&#20851;&#31995;&#21644;&#20174;&#19994;&#32773;&#30693;&#35782;&#65289;&#32435;&#20837;&#21040;&#20854;&#24320;&#21457;&#20013;&#65307;&#20108;&#26159;&#23398;&#20064;&#21644;&#21453;&#26144;&#20559;&#35265;&#65307;&#19977;&#26159;&#32570;&#20047;&#35299;&#37322;&#24615;&#12290;&#37492;&#20110;&#25945;&#32946;&#30340;&#39640;&#39118;&#38505;&#24615;&#65292;&#23558;&#25945;&#32946;&#30693;&#35782;&#25972;&#21512;&#21040;ANNs&#20013;&#23545;&#20110;&#24320;&#21457;&#31526;&#21512;&#22522;&#26412;&#25945;&#32946;&#38480;&#21046;&#12289;&#24182;&#25552;&#20379;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#30340;AI&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;&#31070;&#32463;-&#31526;&#21495;AI&#23478;&#26063;&#26377;&#28508;&#21147;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;AI&#26694;&#26550;&#65292;&#24182;&#30456;&#24212;&#22320;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;NSAI&#30340;&#26041;&#27861;&#65292;&#23558;&#25945;&#32946;&#30693;&#35782;&#27880;&#20837;&#21644;&#25552;&#21462;&#21040;&#35813;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks (ANNs) have shown to be amongst the most important artificial intelligence (AI) techniques in educational applications, providing adaptive educational services. However, their educational potential is limited in practice due to three major challenges: i) difficulty in incorporating symbolic educational knowledge (e.g., causal relationships, and practitioners' knowledge) in their development, ii) learning and reflecting biases, and iii) lack of interpretability. Given the high-risk nature of education, the integration of educational knowledge into ANNs becomes crucial for developing AI applications that adhere to essential educational restrictions, and provide interpretability over the predictions. This research argues that the neural-symbolic family of AI has the potential to address the named challenges. To this end, it adapts a neural-symbolic AI framework and accordingly develops an approach called NSAI, that injects and extracts educational knowledge into
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#32456;&#31471;&#29992;&#25143;&#32534;&#31243;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#29983;&#25104;&#36716;&#21464;&#20551;&#35774;&#65292;&#24182;&#35752;&#35770;&#20102;&#20256;&#32479;&#32534;&#31243;&#35821;&#35328;&#23545;&#38750;&#19987;&#19994;&#32456;&#31471;&#29992;&#25143;&#31243;&#24207;&#21592;&#20173;&#28982;&#20855;&#26377;&#30456;&#20851;&#24615;&#30340;&#21407;&#22240;&#21644;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00382</link><description>&lt;p&gt;
&#32534;&#30721;&#22312;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#36827;&#34892;&#32456;&#31471;&#29992;&#25143;&#32534;&#31243;&#20013;&#26159;&#21542;&#20173;&#28982;&#20855;&#26377;&#30456;&#20851;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Will Code Remain a Relevant User Interface for End-User Programming with Generative AI Models?. (arXiv:2311.00382v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00382
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#32456;&#31471;&#29992;&#25143;&#32534;&#31243;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#29983;&#25104;&#36716;&#21464;&#20551;&#35774;&#65292;&#24182;&#35752;&#35770;&#20102;&#20256;&#32479;&#32534;&#31243;&#35821;&#35328;&#23545;&#38750;&#19987;&#19994;&#32456;&#31471;&#29992;&#25143;&#31243;&#24207;&#21592;&#20173;&#28982;&#20855;&#26377;&#30456;&#20851;&#24615;&#30340;&#21407;&#22240;&#21644;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#31471;&#29992;&#25143;&#32534;&#31243;&#30340;&#30740;&#31350;&#39046;&#22495;&#19968;&#30452;&#33268;&#21147;&#20110;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#22763;&#23398;&#20064;&#36275;&#22815;&#22909;&#30340;&#32534;&#30721;&#65292;&#20197;&#20415;&#23436;&#25104;&#20182;&#20204;&#30340;&#20219;&#21153;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#20195;&#30721;&#65292;&#20174;&#32780;&#23436;&#20840;&#28040;&#38500;&#36825;&#19968;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#22312;&#20855;&#26377;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#19990;&#30028;&#20013;&#65292;&#8220;&#20256;&#32479;&#8221;&#32534;&#31243;&#35821;&#35328;&#23545;&#38750;&#19987;&#19994;&#32456;&#31471;&#29992;&#25143;&#31243;&#24207;&#21592;&#20173;&#28982;&#30456;&#20851;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#29983;&#25104;&#36716;&#21464;&#20551;&#35774;&#8221;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23558;&#22312;&#32456;&#31471;&#29992;&#25143;&#32534;&#31243;&#30340;&#20256;&#32479;&#33539;&#22260;&#19978;&#21019;&#36896;&#23450;&#24615;&#21644;&#23450;&#37327;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20256;&#32479;&#32534;&#31243;&#35821;&#35328;&#21487;&#33021;&#20173;&#28982;&#23545;&#32456;&#31471;&#29992;&#25143;&#31243;&#24207;&#21592;&#26377;&#29992;&#30340;&#19968;&#20123;&#21407;&#22240;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#20123;&#21407;&#22240;&#26159;&#21542;&#21487;&#33021;&#26159;&#26681;&#26412;&#21644;&#25345;&#20037;&#30340;&#65292;&#25110;&#32773;&#23427;&#20204;&#26159;&#21542;&#20250;&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#21019;&#26032;&#32780;&#28040;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#23545;&#32456;&#31471;&#29992;&#25143;&#32534;&#31243;&#30740;&#31350;&#30340;&#19968;&#31995;&#21015;&#24433;&#21709;&#65292;&#21253;&#25324;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research field of end-user programming has largely been concerned with helping non-experts learn to code sufficiently well in order to achieve their tasks. Generative AI stands to obviate this entirely by allowing users to generate code from naturalistic language prompts. In this essay, we explore the extent to which "traditional" programming languages remain relevant for non-expert end-user programmers in a world with generative AI. We posit the "generative shift hypothesis": that generative AI will create qualitative and quantitative expansions in the traditional scope of end-user programming. We outline some reasons that traditional programming languages may still be relevant and useful for end-user programmers. We speculate whether each of these reasons might be fundamental and enduring, or whether they may disappear with further improvements and innovations in generative AI. Finally, we articulate a set of implications for end-user programming research, including the possibili
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#20998;&#25955;&#19987;&#23478;&#31995;&#32479;&#65292;&#21033;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#65292;&#32467;&#21512;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;MRI&#20998;&#26512;&#65292;&#22312;&#26089;&#26399;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2311.00373</link><description>&lt;p&gt;
&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#22686;&#24378;&#30340;&#26089;&#26399;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#39044;&#27979;&#20998;&#25955;&#19987;&#23478;&#31995;&#32479;&#30340;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Architecture of Data Anomaly Detection-Enhanced Decentralized Expert System for Early-Stage Alzheimer's Disease Prediction. (arXiv:2311.00373v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00373
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#20998;&#25955;&#19987;&#23478;&#31995;&#32479;&#65292;&#21033;&#29992;&#21306;&#22359;&#38142;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#65292;&#32467;&#21512;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;MRI&#20998;&#26512;&#65292;&#22312;&#26089;&#26399;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#20010;&#20840;&#29699;&#24615;&#30340;&#20581;&#24247;&#25361;&#25112;&#65292;&#38656;&#35201;&#26089;&#26399;&#21644;&#20934;&#30830;&#30340;&#26816;&#27979;&#20197;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#12290;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20855;&#26377;&#37325;&#35201;&#30340;&#35786;&#26029;&#28508;&#21147;&#65292;&#20294;&#20854;&#26377;&#25928;&#20998;&#26512;&#20173;&#28982;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#20998;&#25955;&#19987;&#23478;&#31995;&#32479;&#65292;&#24039;&#22937;&#22320;&#32467;&#21512;&#20102;&#21306;&#22359;&#38142;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#65292;&#20197;&#25972;&#21512;&#23545;&#24739;&#32773;&#25552;&#20132;&#25968;&#25454;&#30340;&#21487;&#38752;&#24322;&#24120;&#26816;&#27979;&#12290;&#20256;&#32479;&#30340;&#35786;&#26029;&#26041;&#27861;&#22312;&#30142;&#30149;&#30340;&#26089;&#26399;&#38454;&#27573;&#24448;&#24448;&#20250;&#23548;&#33268;&#24310;&#36831;&#21644;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#38598;&#20013;&#24335;&#25968;&#25454;&#23384;&#20648;&#24211;&#38590;&#20197;&#31649;&#29702;&#24222;&#22823;&#37327;&#30340;MRI&#25968;&#25454;&#65292;&#32780;&#25345;&#32493;&#23384;&#22312;&#30340;&#38544;&#31169;&#38382;&#39064;&#38459;&#30861;&#20102;&#21327;&#20316;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#20998;&#25955;&#21270;&#26469;&#20445;&#25252;&#25968;&#25454;&#23436;&#25972;&#24615;&#21644;&#24739;&#32773;&#38544;&#31169;&#65292;&#20381;&#38752;&#21306;&#22359;&#38142;&#25216;&#26415;&#23454;&#29616;&#12290;&#23427;&#19981;&#20165;&#24378;&#35843;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;MRI&#20998;&#26512;&#65292;&#36824;&#34701;&#20837;&#20102;&#22797;&#26434;&#30340;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's Disease is a global health challenge that requires early and accurate detection to improve patient outcomes. Magnetic Resonance Imaging (MRI) holds significant diagnostic potential, but its effective analysis remains a formidable task. This study introduces a groundbreaking decentralized expert system that cleverly combines blockchain technology with Artificial Intelligence (AI) to integrate robust anomaly detection for patient-submitted data.  Traditional diagnostic methods often lead to delayed and imprecise predictions, especially in the early stages of the disease. Centralized data repositories struggle to manage the immense volumes of MRI data, and persistent privacy concerns hinder collaborative efforts. Our innovative solution harnesses decentralization to protect data integrity and patient privacy, facilitated by blockchain technology. It not only emphasizes AI-driven MRI analysis but also incorporates a sophisticated data anomaly detection architecture. These mecha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#36923;&#36753;&#35821;&#20041;&#22686;&#24378;&#65288;PLSE&#65289;&#26041;&#27861;&#26469;&#25913;&#36827;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#65288;IDRR&#65289;&#65292;&#36890;&#36807;&#26080;&#32541;&#22320;&#23558;&#19982;&#31687;&#31456;&#20851;&#31995;&#30456;&#20851;&#30340;&#30693;&#35782;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;IDRR&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00367</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#36923;&#36753;&#35821;&#20041;&#22686;&#24378;&#23545;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition. (arXiv:2311.00367v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#36923;&#36753;&#35821;&#20041;&#22686;&#24378;&#65288;PLSE&#65289;&#26041;&#27861;&#26469;&#25913;&#36827;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#65288;IDRR&#65289;&#65292;&#36890;&#36807;&#26080;&#32541;&#22320;&#23558;&#19982;&#31687;&#31456;&#20851;&#31995;&#30456;&#20851;&#30340;&#30693;&#35782;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;IDRR&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#65288;IDRR&#65289;&#26159;&#19968;&#39033;&#22312;&#27809;&#26377;&#26126;&#30830;&#36830;&#25509;&#35789;&#24110;&#21161;&#19979;&#25512;&#26029;&#31687;&#31456;&#20851;&#31995;&#30340;&#20851;&#38190;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20542;&#21521;&#20110;&#21033;&#29992;&#27880;&#37322;&#30340;&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#25972;&#21512;&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#33719;&#24471;&#22686;&#24378;&#30340;&#31687;&#31456;&#20851;&#31995;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;IDRR&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#21463;&#21040;&#27880;&#37322;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#23384;&#22312;&#22823;&#37327;&#24102;&#26377;&#26126;&#30830;&#36830;&#25509;&#35789;&#30340;&#26410;&#27880;&#37322;&#35805;&#35821;&#65292;&#21487;&#20197;&#29992;&#26469;&#33719;&#21462;&#20016;&#23500;&#30340;&#31687;&#31456;&#20851;&#31995;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#36923;&#36753;&#35821;&#20041;&#22686;&#24378;&#65288;PLSE&#65289;&#26041;&#27861;&#26469;&#25913;&#36827;IDRR&#12290;&#26412;&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#36830;&#25509;&#35789;&#39044;&#27979;&#23558;&#19982;&#31687;&#31456;&#20851;&#31995;&#30456;&#20851;&#30340;&#30693;&#35782;&#26080;&#32541;&#22320;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#22522;&#20110;&#25552;&#31034;&#30340;&#36830;&#25509;&#35789;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Discourse Relation Recognition (IDRR), which infers discourse relations without the help of explicit connectives, is still a crucial and challenging task for discourse parsing. Recent works tend to exploit the hierarchical structure information from the annotated senses, which demonstrate enhanced discourse relation representations can be obtained by integrating sense hierarchy. Nevertheless, the performance and robustness for IDRR are significantly constrained by the availability of annotated data. Fortunately, there is a wealth of unannotated utterances with explicit connectives, that can be utilized to acquire enriched discourse relation features. In light of such motivation, we propose a Prompt-based Logical Semantics Enhancement (PLSE) method for IDRR. Essentially, our method seamlessly injects knowledge relevant to discourse relation into pre-trained language models through prompt-based connective prediction. Furthermore, considering the prompt-based connective predictio
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#27491;&#36127;&#26679;&#26412;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#25968;&#25454;&#25366;&#25496;&#25366;&#25496;&#28508;&#22312;&#26679;&#26412;&#65292;&#24182;&#19988;&#20998;&#26512;&#36127;&#26679;&#26412;&#26799;&#24230;&#65292;&#26368;&#32456;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00358</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#65306;&#28508;&#22312;&#26679;&#26412;&#30340;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Rethinking Samples Selection for Contrastive Learning: Mining of Potential Samples. (arXiv:2311.00358v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00358
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#27491;&#36127;&#26679;&#26412;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#25968;&#25454;&#25366;&#25496;&#25366;&#25496;&#28508;&#22312;&#26679;&#26412;&#65292;&#24182;&#19988;&#20998;&#26512;&#36127;&#26679;&#26412;&#26799;&#24230;&#65292;&#26368;&#32456;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20351;&#20004;&#20010;&#22270;&#20687;&#30340;&#29305;&#24449;&#34920;&#31034;&#23613;&#21487;&#33021;&#25509;&#36817;&#25110;&#36828;&#31163;&#65292;&#20174;&#32780;&#39044;&#27979;&#23427;&#20204;&#26159;&#21542;&#23646;&#20110;&#21516;&#19968;&#31867;&#21035;&#12290;&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#22914;&#20309;&#25366;&#25496;&#26679;&#26412;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20840;&#38754;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#65292;&#24182;&#20174;&#20004;&#20010;&#26041;&#38754;&#25366;&#25496;&#28508;&#22312;&#26679;&#26412;&#65306;&#39318;&#20808;&#65292;&#23545;&#20110;&#27491;&#26679;&#26412;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#33719;&#24471;&#30340;&#22686;&#24378;&#26679;&#26412;&#35270;&#22270;&#21644;&#36890;&#36807;&#25968;&#25454;&#25366;&#25496;&#33719;&#24471;&#30340;&#25366;&#25496;&#26679;&#26412;&#35270;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36719;&#26435;&#20540;&#21644;&#30828;&#26435;&#20540;&#36827;&#34892;&#21152;&#26435;&#21644;&#32452;&#21512;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;&#36127;&#26679;&#26412;&#20013;&#23384;&#22312;&#26080;&#20449;&#24687;&#30340;&#36127;&#26679;&#26412;&#21644;&#35823;&#21028;&#30340;&#36127;&#26679;&#26412;&#65292;&#25105;&#20204;&#20174;&#26799;&#24230;&#30340;&#35282;&#24230;&#20998;&#26512;&#36127;&#26679;&#26412;&#65292;&#24182;&#26368;&#32456;&#25366;&#25496;&#26082;&#19981;&#36807;&#20110;&#22256;&#38590;&#20063;&#19981;&#36807;&#20110;&#23481;&#26131;&#30340;&#36127;&#26679;&#26412;&#20316;&#20026;&#28508;&#22312;&#30340;&#36127;&#26679;&#26412;&#65292;&#21363;&#38752;&#36817;&#27491;&#26679;&#26412;&#30340;&#36127;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning predicts whether two images belong to the same category by training a model to make their feature representations as close or as far away as possible. In this paper, we rethink how to mine samples in contrastive learning, unlike other methods, our approach is more comprehensive, taking into account both positive and negative samples, and mining potential samples from two aspects: First, for positive samples, we consider both the augmented sample views obtained by data augmentation and the mined sample views through data mining. Then, we weight and combine them using both soft and hard weighting strategies. Second, considering the existence of uninformative negative samples and false negative samples in the negative samples, we analyze the negative samples from the gradient perspective and finally mine negative samples that are neither too hard nor too easy as potential negative samples, i.e., those negative samples that are close to positive samples. The experiment
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;QFree&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;&#21435;&#20013;&#24515;&#21270;&#31574;&#30053;&#65292;&#24182;&#36981;&#24490;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#21407;&#21017;&#12290;</title><link>http://arxiv.org/abs/2311.00356</link><description>&lt;p&gt;
QFree: &#19968;&#31181;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QFree: A Universal Value Function Factorization for Multi-Agent Reinforcement Learning. (arXiv:2311.00356v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00356
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;QFree&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;&#21435;&#20013;&#24515;&#21270;&#31574;&#30053;&#65292;&#24182;&#36981;&#24490;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20013;&#24335;&#35757;&#32451;&#24191;&#27867;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#20197;&#30830;&#20445;&#35757;&#32451;&#36807;&#31243;&#30340;&#31283;&#23450;&#24615;&#12290;&#19968;&#26086;&#33719;&#24471;&#32852;&#21512;&#31574;&#30053;&#65292;&#35774;&#35745;&#19968;&#31181;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#26041;&#27861;&#20197;&#25552;&#21462;&#20195;&#29702;&#30340;&#26368;&#20248;&#21435;&#20013;&#24515;&#21270;&#31574;&#30053;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#28385;&#36275;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;(IGM)&#21407;&#21017;&#12290;&#34429;&#28982;&#23545;IGM&#20989;&#25968;&#31867;&#26045;&#21152;&#39069;&#22806;&#38480;&#21046;&#21487;&#20197;&#24110;&#21161;&#28385;&#36275;&#35201;&#27714;&#65292;&#20294;&#21364;&#38480;&#21046;&#20102;&#23427;&#22312;&#26356;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;QFree&#30340;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26681;&#25454;&#20248;&#21183;&#20989;&#25968;&#65292;&#24320;&#21457;&#20102;&#28385;&#36275;IGM&#21407;&#21017;&#30340;&#25968;&#23398;&#31561;&#20215;&#26465;&#20214;&#65292;&#30830;&#20445;&#21407;&#21017;&#22312;&#27809;&#26377;&#20219;&#20309;&#22949;&#21327;&#30340;&#24773;&#20917;&#19979;&#25104;&#31435;&#65292;&#28040;&#38500;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#20445;&#23432;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;&#28151;&#21512;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#31561;&#20215;&#30340;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Centralized training is widely utilized in the field of multi-agent reinforcement learning (MARL) to assure the stability of training process. Once a joint policy is obtained, it is critical to design a value function factorization method to extract optimal decentralized policies for the agents, which needs to satisfy the individual-global-max (IGM) principle. While imposing additional limitations on the IGM function class can help to meet the requirement, it comes at the cost of restricting its application to more complex multi-agent environments. In this paper, we propose QFree, a universal value function factorization method for MARL. We start by developing mathematical equivalent conditions of the IGM principle based on the advantage function, which ensures that the principle holds without any compromise, removing the conservatism of conventional methods. We then establish a more expressive mixing network architecture that can fulfill the equivalent factorization. In particular, th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#23450;&#20041;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#21363;&#26080;&#38480;&#26102;&#38388;&#20869;&#19981;&#26029;&#20135;&#29983;&#26032;&#20803;&#32032;&#12290;&#22312;&#36825;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#30340;&#27010;&#24565;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#24320;&#25918;&#24335;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2311.00344</link><description>&lt;p&gt;
&#20026;&#30446;&#26631;&#26465;&#20214;&#26234;&#33021;&#20307;&#23450;&#20041;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Definition of Open-Ended Learning Problems for Goal-Conditioned Agents. (arXiv:2311.00344v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#23450;&#20041;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#21363;&#26080;&#38480;&#26102;&#38388;&#20869;&#19981;&#26029;&#20135;&#29983;&#26032;&#20803;&#32032;&#12290;&#22312;&#36825;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#30340;&#27010;&#24565;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#24320;&#25918;&#24335;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#35770;&#25991;&#20013;&#37117;&#25552;&#21040;&#20102;&#8220;&#24320;&#25918;&#24335;&#23398;&#20064;&#8221;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#23581;&#35797;&#23450;&#20041;&#36825;&#20010;&#26415;&#35821;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#24403;&#20180;&#32454;&#30740;&#31350;&#26102;&#65292;&#20284;&#20046;&#23545;&#20110;&#24320;&#25918;&#24335;&#23398;&#20064;&#19982;&#36830;&#32493;&#23398;&#20064;&#12289;&#32456;&#36523;&#23398;&#20064;&#25110;&#33258;&#20026;&#30446;&#30340;&#23398;&#20064;&#31561;&#30456;&#20851;&#27010;&#24565;&#30340;&#21306;&#21035;&#27809;&#26377;&#20849;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#38416;&#36848;&#36825;&#20010;&#27010;&#24565;&#30340;&#36215;&#28304;&#21644;&#26368;&#36817;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#23646;&#24615;&#30340;&#22797;&#21512;&#27010;&#24565;&#12290;&#19982;&#36825;&#20123;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#24320;&#25918;&#24335;&#36807;&#31243;&#30340;&#19968;&#20010;&#20851;&#38190;&#22522;&#26412;&#23646;&#24615;&#19982;&#26102;&#38388;&#26080;&#38480;&#21046;&#22320;&#20135;&#29983;&#26032;&#20803;&#32032;&#30456;&#20998;&#31163;&#30340;&#24819;&#27861;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24320;&#25918;&#24335;&#23398;&#20064;&#38382;&#39064;&#30340;&#27010;&#24565;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#20102;&#24320;&#25918;&#24335;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
A lot of recent machine learning research papers have "Open-ended learning" in their title. But very few of them attempt to define what they mean when using the term. Even worse, when looking more closely there seems to be no consensus on what distinguishes open-ended learning from related concepts such as continual learning, lifelong learning or autotelic learning. In this paper, we contribute to fixing this situation. After illustrating the genealogy of the concept and more recent perspectives about what it truly means, we outline that open-ended learning is generally conceived as a composite notion encompassing a set of diverse properties. In contrast with these previous approaches, we propose to isolate a key elementary property of open-ended processes, which is to always produce novel elements from time to time over an infinite horizon. From there, we build the notion of open-ended learning problems and focus in particular on the subset of open-ended goal-conditioned reinforcement
&lt;/p&gt;</description></item><item><title>MetisFL&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#32852;&#37030;&#25511;&#21046;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2311.00334</link><description>&lt;p&gt;
MetisFL:&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#24037;&#20316;&#27969;&#30340;&#23604;&#23596;&#24182;&#34892;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows. (arXiv:2311.00334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00334
&lt;/p&gt;
&lt;p&gt;
MetisFL&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#32852;&#37030;&#25511;&#21046;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#31995;&#32479;&#36890;&#24120;&#30001;&#20004;&#20010;&#26680;&#24515;&#22788;&#29702;&#23454;&#20307;&#32452;&#25104;:&#32852;&#37030;&#25511;&#21046;&#22120;&#21644;&#23398;&#20064;&#22120;&#12290;&#25511;&#21046;&#22120;&#36127;&#36131;&#31649;&#29702;&#22312;&#23398;&#20064;&#22120;&#20043;&#38388;&#25191;&#34892;FL&#24037;&#20316;&#27969;&#31243;&#65292;&#23398;&#20064;&#22120;&#36127;&#36131;&#22312;&#20854;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#32852;&#37030;&#27169;&#22411;&#12290;&#22312;&#25191;&#34892;FL&#24037;&#20316;&#27969;&#26102;&#65292;FL&#31995;&#32479;&#23545;&#21442;&#19982;&#23398;&#20064;&#22120;&#30340;&#35745;&#31639;&#36164;&#28304;&#25110;&#25968;&#25454;&#27809;&#26377;&#25511;&#21046;&#12290;&#23613;&#31649;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;FL&#31995;&#32479;&#26469;&#20419;&#36827;FL&#24037;&#20316;&#27969;&#30340;&#24320;&#21457;&#65292;&#20294;&#36825;&#20123;&#31995;&#32479;&#20013;&#22823;&#22810;&#25968;&#24573;&#35270;&#20102;&#25511;&#21046;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;MetisFL&#30340;&#26032;&#22411;FL&#31995;&#32479;&#65292;&#20854;&#20013;&#32852;&#37030;&#25511;&#21046;&#22120;&#26159;&#31532;&#19968;&#31561;&#20844;&#27665;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Federated Learning (FL) system typically consists of two core processing entities: the federation controller and the learners. The controller is responsible for managing the execution of FL workflows across learners and the learners for training and evaluating federated models over their private datasets. While executing an FL workflow, the FL system has no control over the computational resources or data of the participating learners. Still, it is responsible for other operations, such as model aggregation, task dispatching, and scheduling. These computationally heavy operations generally need to be handled by the federation controller. Even though many FL systems have been recently proposed to facilitate the development of FL workflows, most of these systems overlook the scalability of the controller. To meet this need, we designed and developed a novel FL system called MetisFL, where the federation controller is the first-class citizen. MetisFL re-engineers all the operations cond
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22522;&#20110;&#20803;&#26435;&#37325;&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33410;&#28857;&#23545;&#24212;&#26435;&#37325;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#22122;&#22768;&#36793;&#30340;&#22270;&#20013;&#25214;&#21040;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2311.00322</link><description>&lt;p&gt;
&#22122;&#22768;&#22270;&#20013;&#30340;&#40065;&#26834;&#22270;&#32858;&#31867;&#36890;&#36807;&#20803;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Robust Graph Clustering via Meta Weighting for Noisy Graphs. (arXiv:2311.00322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22522;&#20110;&#20803;&#26435;&#37325;&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33410;&#28857;&#23545;&#24212;&#26435;&#37325;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#22122;&#22768;&#36793;&#30340;&#22270;&#20013;&#25214;&#21040;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22312;&#22122;&#22768;&#36793;&#19978;&#40065;&#26834;&#22320;&#25214;&#21040;&#22270;&#20013;&#30340;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#65311;&#22270;&#32858;&#31867;&#26159;&#22270;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#22312;&#22270;&#32858;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#23384;&#22312;&#22122;&#22768;&#36793;&#30340;&#22270;&#19978;&#30340;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#40065;&#26834;GNN-based&#22270;&#32858;&#31867;&#30340;MetaGC&#12290;MetaGC&#37319;&#29992;&#21487;&#20998;&#35299;&#30340;&#32858;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#20854;&#37325;&#26032;&#34920;&#36848;&#20026;&#33410;&#28857;&#23545;&#20043;&#38388;&#25439;&#22833;&#30340;&#27714;&#21644;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#33410;&#28857;&#23545;&#28155;&#21152;&#21487;&#23398;&#20064;&#30340;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#20803;&#26435;&#37325;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#33410;&#28857;&#23545;&#30340;&#26435;&#37325;&#65292;&#20351;&#26377;&#24847;&#20041;&#30340;&#33410;&#28857;&#23545;&#30340;&#26435;&#37325;&#22686;&#21152;&#65292;&#32780;&#19981;&#37027;&#20040;&#26377;&#24847;&#20041;&#30340;&#33410;&#28857;&#23545;&#65288;&#20363;&#22914;&#22122;&#22768;&#36793;&#65289;&#30340;&#26435;&#37325;&#20943;&#23567;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;MetaGC&#25353;&#29031;&#39044;&#26399;&#23398;&#20064;&#26435;&#37325;&#65292;&#24182;&#19988;&#22240;&#27492;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we find meaningful clusters in a graph robustly against noise edges? Graph clustering (i.e., dividing nodes into groups of similar ones) is a fundamental problem in graph analysis with applications in various fields. Recent studies have demonstrated that graph neural network (GNN) based approaches yield promising results for graph clustering. However, we observe that their performance degenerates significantly on graphs with noise edges, which are prevalent in practice. In this work, we propose MetaGC for robust GNN-based graph clustering. MetaGC employs a decomposable clustering loss function, which can be rephrased as a sum of losses over node pairs. We add a learnable weight to each node pair, and MetaGC adaptively adjusts the weights of node pairs using meta-weighting so that the weights of meaningful node pairs increase and the weights of less-meaningful ones (e.g., noise edges) decrease. We show empirically that MetaGC learns weights as intended and consequently outperfor
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#26367;&#20195;&#35789;&#26469;&#31616;&#21270;&#32473;&#23450;&#30446;&#26631;&#35789;&#19982;&#20854;&#19978;&#19979;&#25991;&#65292;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#35821;&#35328;&#19978;&#20248;&#20110;&#20854;&#20182;&#26080;&#30417;&#30563;&#31995;&#32479;&#65292;&#24182;&#22312;SWORDS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00310</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#19982;&#19978;&#19979;&#25991;&#25193;&#20805;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Lexical Simplification with Context Augmentation. (arXiv:2311.00310v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00310
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#26367;&#20195;&#35789;&#26469;&#31616;&#21270;&#32473;&#23450;&#30446;&#26631;&#35789;&#19982;&#20854;&#19978;&#19979;&#25991;&#65292;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#35821;&#35328;&#19978;&#20248;&#20110;&#20854;&#20182;&#26080;&#30417;&#30563;&#31995;&#32479;&#65292;&#24182;&#22312;SWORDS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#21482;&#20351;&#29992;&#21333;&#35821;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#32473;&#23450;&#30446;&#26631;&#35789;&#21644;&#20854;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#30446;&#26631;&#19978;&#19979;&#25991;&#21644;&#20174;&#21333;&#35821;&#25968;&#25454;&#20013;&#37319;&#26679;&#30340;&#39069;&#22806;&#19978;&#19979;&#25991;&#29983;&#25104;&#26367;&#20195;&#35789;&#12290;&#25105;&#20204;&#22312;TSAR-2022&#20849;&#20139;&#20219;&#21153;&#19978;&#20351;&#29992;&#33521;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#35821;&#35328;&#19978;&#37117;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26080;&#30417;&#30563;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;GPT-3.5&#36827;&#34892;&#38598;&#25104;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;SWORDS&#35789;&#27719;&#26367;&#25442;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new unsupervised lexical simplification method that uses only monolingual data and pre-trained language models. Given a target word and its context, our method generates substitutes based on the target context and also additional contexts sampled from monolingual data. We conduct experiments in English, Portuguese, and Spanish on the TSAR-2022 shared task, and show that our model substantially outperforms other unsupervised systems across all languages. We also establish a new state-of-the-art by ensembling our model with GPT-3.5. Lastly, we evaluate our model on the SWORDS lexical substitution data set, achieving a state-of-the-art result.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#35270;&#35273;&#38382;&#31572;(VQA)&#39046;&#22495;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#21253;&#25324;&#20256;&#32479;VQA&#26550;&#26500;&#21644;&#29616;&#20195;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;VQA&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#22312;&#21382;&#21490;&#19978;&#30340;&#21457;&#23637;&#65292;&#25581;&#31034;&#20102;VLP&#22312;VQA&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#20250;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2311.00308</link><description>&lt;p&gt;
&#20174;&#22270;&#20687;&#21040;&#35821;&#35328;: &#23545;&#35270;&#35273;&#38382;&#31572;(VQA)&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26426;&#20250;&#36827;&#34892;&#30340;&#20851;&#38190;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities. (arXiv:2311.00308v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#35270;&#35273;&#38382;&#31572;(VQA)&#39046;&#22495;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#21253;&#25324;&#20256;&#32479;VQA&#26550;&#26500;&#21644;&#29616;&#20195;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;VQA&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#22312;&#21382;&#21490;&#19978;&#30340;&#21457;&#23637;&#65292;&#25581;&#31034;&#20102;VLP&#22312;VQA&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#20250;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;(VQA)&#26159;&#19968;&#20010;&#32508;&#21512;&#35745;&#31639;&#26426;&#35270;&#35273;(CV)&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#26088;&#22312;&#23545;&#20219;&#20309;&#35270;&#35273;&#36755;&#20837;&#29983;&#25104;&#31572;&#26696;&#12290;VQA&#30340;&#33539;&#22260;&#24050;&#20174;&#20851;&#27880;&#33258;&#28982;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#25193;&#23637;&#21040;&#21253;&#21547;&#21512;&#25104;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;3D&#29615;&#22659;&#21644;&#20854;&#20182;&#35270;&#35273;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#12290;&#22823;&#22411;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#20986;&#29616;&#20351;&#26089;&#26399;&#20381;&#36182;&#29305;&#24449;&#25552;&#21462;&#21644;&#34701;&#21512;&#26041;&#26696;&#30340;VQA&#26041;&#27861;&#36716;&#21521;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#21253;&#25324;&#20256;&#32479;VQA&#26550;&#26500;&#21644;&#29616;&#20195;&#22522;&#20110;VLP&#30340;&#26041;&#27861;&#22312;&#20869;&#30340;&#32508;&#21512;&#35843;&#26597;&#12290;&#27492;&#22806;&#65292;&#36824;&#27809;&#26377;&#23545;VQA&#35270;&#35282;&#19979;&#30340;VLP&#25361;&#25112;&#36827;&#34892;&#28145;&#20837;&#25506;&#35752;&#65292;&#30041;&#19979;&#20102;&#21487;&#33021;&#20986;&#29616;&#28508;&#22312;&#24320;&#25918;&#38382;&#39064;&#30340;&#31354;&#38388;&#12290;&#26412;&#30740;&#31350;&#22312;VQA&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20221;&#35843;&#26597;&#25253;&#21578;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;VQA&#25968;&#25454;&#38598;&#21644;&#21382;&#21490;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multimodal task of Visual Question Answering (VQA) encompassing elements of Computer Vision (CV) and Natural Language Processing (NLP), aims to generate answers to questions on any visual input. Over time, the scope of VQA has expanded from datasets focusing on an extensive collection of natural images to datasets featuring synthetic images, video, 3D environments, and various other visual inputs. The emergence of large pre-trained networks has shifted the early VQA approaches relying on feature extraction and fusion schemes to vision language pre-training (VLP) techniques. However, there is a lack of comprehensive surveys that encompass both traditional VQA architectures and contemporary VLP-based methods. Furthermore, the VLP challenges in the lens of VQA haven't been thoroughly explored, leaving room for potential open problems to emerge. Our work presents a survey in the domain of VQA that delves into the intricacies of VQA datasets and methods over the field's history, introdu
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23545;&#22320;&#19979;CO2&#27844;&#28431;&#36827;&#34892;&#30417;&#27979;&#21644;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25551;&#36848;CO2&#27969;&#21160;&#27169;&#24335;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00290</link><description>&lt;p&gt;
CO2&#27969;&#21160;&#27169;&#24335;&#30340;&#25512;&#26029;--&#21487;&#34892;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Inference of CO2 flow patterns -- a feasibility study. (arXiv:2311.00290v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00290
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#22320;&#19979;CO2&#27844;&#28431;&#36827;&#34892;&#30417;&#27979;&#21644;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25551;&#36848;CO2&#27969;&#21160;&#27169;&#24335;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#30899;&#25429;&#33719;&#21644;&#23553;&#23384;&#65288;CCS&#65289;&#25216;&#26415;&#22312;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#26007;&#20105;&#20013;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#65292;&#24314;&#31435;&#31283;&#20581;&#30340;&#30417;&#27979;&#21644;&#26816;&#27979;&#26426;&#21046;&#65292;&#20197;&#26816;&#27979;&#28508;&#22312;&#30340;&#22320;&#19979;CO2&#27844;&#28431;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#23384;&#20648;&#24211;&#23553;&#22581;&#30340;&#39044;&#20808;&#23384;&#22312;&#25110;&#35825;&#23548;&#30340;&#26029;&#23618;&#65292;&#21464;&#24471;&#36234;&#26469;&#36234;&#36843;&#20999;&#12290;&#34429;&#28982;&#35832;&#22914;&#21382;&#21490;&#21305;&#37197;&#21644;CO2&#20648;&#23384;&#30340;&#26102;&#38388;&#24207;&#21015;&#22320;&#38663;&#30417;&#27979;&#31561;&#25216;&#26415;&#24050;&#25104;&#21151;&#29992;&#20110;&#36319;&#36394;&#22320;&#19979;CO2&#28183;&#28431;&#30340;&#28436;&#21270;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#23545;CO2&#27874;&#21160;&#34892;&#20026;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#22522;&#26412;&#26041;&#27861;&#12290;&#31995;&#32479;&#35780;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#32435;&#20837;&#23545;&#20110;&#39118;&#38505;&#32531;&#35299;&#33267;&#20851;&#37325;&#35201;&#65292;&#21407;&#22240;&#22914;&#19979;&#65306;&#65288;i&#65289;CO2&#27874;&#21160;&#35825;&#21457;&#30340;&#21464;&#21270;&#24456;&#23567;&#19988;&#22320;&#38663;&#25968;&#25454;&#22122;&#22768;&#24456;&#22823;&#65307;&#65288;ii&#65289;&#27491;&#24120;&#21644;&#19981;&#35268;&#21017;&#65288;&#20363;&#22914;&#27844;&#28431;&#24341;&#36215;&#30340;&#65289;&#27969;&#21160;&#27169;&#24335;&#20043;&#38388;&#30340;&#21464;&#21270;&#24456;&#23567;&#65307;&#65288;iii&#65289;&#25511;&#21046;&#27969;&#21160;&#30340;&#20648;&#23618;&#29305;&#24615;&#24378;&#28872;&#24322;&#36136;&#19988;&#36890;&#24120;
&lt;/p&gt;
&lt;p&gt;
As the global deployment of carbon capture and sequestration (CCS) technology intensifies in the fight against climate change, it becomes increasingly imperative to establish robust monitoring and detection mechanisms for potential underground CO2 leakage, particularly through pre-existing or induced faults in the storage reservoir's seals. While techniques such as history matching and time-lapse seismic monitoring of CO2 storage have been used successfully in tracking the evolution of CO2 plumes in the subsurface, these methods lack principled approaches to characterize uncertainties related to the CO2 plumes' behavior. Inclusion of systematic assessment of uncertainties is essential for risk mitigation for the following reasons: (i) CO2 plume-induced changes are small and seismic data is noisy; (ii) changes between regular and irregular (e.g., caused by leakage) flow patterns are small; and (iii) the reservoir properties that control the flow are strongly heterogeneous and typically 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#20219;&#21153;&#24182;&#20027;&#21160;&#35843;&#25972;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00288</link><description>&lt;p&gt;
&#20027;&#21160;&#25351;&#20196;&#35843;&#20248;&#65306;&#36890;&#36807;&#22312;&#25935;&#24863;&#25351;&#20196;&#20219;&#21153;&#19978;&#35757;&#32451;&#26469;&#25552;&#39640;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks. (arXiv:2311.00288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#20219;&#21153;&#24182;&#20027;&#21160;&#35843;&#25972;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#36890;&#36807;&#22312;&#22823;&#37327;&#22810;&#26679;&#30340;&#20219;&#21153;&#19978;&#20351;&#29992;&#25351;&#20196;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#36873;&#25321;&#26032;&#20219;&#21153;&#20197;&#25552;&#39640;IT&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#35745;&#31639;&#35201;&#27714;&#36807;&#39640;&#65292;&#35757;&#32451;&#25152;&#26377;&#29616;&#26377;&#20219;&#21153;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#32780;&#38543;&#26426;&#36873;&#25321;&#20219;&#21153;&#21487;&#33021;&#20250;&#23548;&#33268;&#20122;&#20248;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#25351;&#20196;&#35843;&#20248;&#65292;&#19968;&#31181;&#35782;&#21035;&#20449;&#24687;&#20016;&#23500;&#20219;&#21153;&#24182;&#22312;&#36873;&#23450;&#20219;&#21153;&#19978;&#20027;&#21160;&#35843;&#25972;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#29992;&#24403;&#21069;&#27169;&#22411;&#36755;&#20986;&#22312;&#25200;&#21160;&#25552;&#31034;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#34920;&#31034;&#26032;&#20219;&#21153;&#30340;&#20449;&#24687;&#20016;&#23500;&#24615;&#12290;&#25105;&#20204;&#22312;NIV2&#21644;Self-Instruct&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#31574;&#30053;&#30340;&#20219;&#21153;&#36873;&#25321;&#65292;&#21516;&#26102;&#22312;&#26356;&#23569;&#30340;&#35757;&#32451;&#20219;&#21153;&#19979;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question. Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance. In this work, we propose active instruction tuning based on prompt uncertainty, a novel framework to identify informative tasks, and then actively tune the models on the selected tasks. We represent the informativeness of new tasks with the disagreement of the current model outputs over perturbed prompts. Our experiments on NIV2 and Self-Instruct datasets demonstrate that our method consistently outperforms other baseline strategies for task selection, achieving better out-of-distribution generalization with fewer training tasks. Additionally, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#30340;&#21019;&#26032;&#26041;&#27861;ClinGen&#65292;&#35813;&#26041;&#27861;&#23558;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#20102;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00287</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#27880;&#20837;&#65306;&#35780;&#20272;&#21644;&#25512;&#36827;&#20020;&#24202;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models. (arXiv:2311.00287v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#30340;&#21019;&#26032;&#26041;&#27861;ClinGen&#65292;&#35813;&#26041;&#27861;&#23558;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#20102;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38656;&#35201;&#33021;&#22815;&#24212;&#23545;&#39046;&#22495;&#29305;&#23450;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#22797;&#26434;&#30340;&#21307;&#23398;&#26415;&#35821;&#21644;&#20020;&#24202;&#32972;&#26223;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#30452;&#25509;&#37096;&#32626;&#21487;&#33021;&#23548;&#33268;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#21463;&#21040;&#36164;&#28304;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#20020;&#24202;NLP&#20219;&#21153;&#30340;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#12289;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#27861;ClinGen&#65292;&#23427;&#23558;&#30693;&#35782;&#27880;&#20837;&#21040;&#36825;&#20010;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#28041;&#21450;&#20020;&#24202;&#30693;&#35782;&#25552;&#21462;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;LLM&#25552;&#31034;&#12290;&#20020;&#24202;&#20027;&#39064;&#21644;&#20889;&#20316;&#39118;&#26684;&#37117;&#26469;&#33258;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#65292;&#20197;&#24341;&#23548;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;7&#20010;&#20020;&#24202;NLP&#20219;&#21153;&#21644;16&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;ClinGen&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#22987;&#32456;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#20351;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#23545;&#40784;&#65292;&#24182;&#26174;&#33879;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that ClinGen consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the divers
&lt;/p&gt;</description></item><item><title>JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00286</link><description>&lt;p&gt;
JADE&#65306;&#22522;&#20110;&#35821;&#35328;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00286
&lt;/p&gt;
&lt;p&gt;
JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JADE&#65292;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#20998;&#26512;&#30340;&#27169;&#31946;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#22686;&#24378;&#31181;&#23376;&#38382;&#39064;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#24182;&#22987;&#32456;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#19977;&#31867;LLM&#65306;&#20843;&#20010;&#24320;&#28304;&#20013;&#25991;LLM&#65292;&#20845;&#20010;&#21830;&#19994;&#20013;&#25991;LLM&#21644;&#22235;&#20010;&#21830;&#19994;&#33521;&#25991;LLM&#12290;JADE&#20026;&#36825;&#19977;&#31867;LLM&#29983;&#25104;&#20102;&#19977;&#20010;&#23433;&#20840;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#65306;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#21516;&#26102;&#35302;&#21457;&#22810;&#20010;LLM&#30340;&#26377;&#23475;&#29983;&#25104;&#65292;&#24179;&#22343;&#19981;&#23433;&#20840;&#29983;&#25104;&#27604;&#20363;&#20026;70%&#65288;&#35831;&#21442;&#35265;&#19979;&#34920;&#65289;&#65292;&#21516;&#26102;&#36825;&#20123;&#38382;&#39064;&#20173;&#28982;&#26159;&#33258;&#28982;&#12289;&#27969;&#30021;&#19988;&#20445;&#30041;&#20102;&#26680;&#24515;&#30340;&#19981;&#23433;&#20840;&#35821;&#20041;&#12290;&#25105;&#20204;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#21457;&#24067;&#20102;&#23545;&#21830;&#19994;&#33521;&#25991;LLM&#21644;&#24320;&#28304;&#33521;&#25991;LLM&#29983;&#25104;&#30340;&#22522;&#20934;&#28436;&#31034;&#65306;https://github.com/whitzard-ai/jade-db&#12290;&#23545;&#20110;&#23545;JADE&#29983;&#25104;&#30340;&#26356;&#22810;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#35835;&#32773;&#65292;&#35831;&#19982;&#25105;&#20204;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present \textit{JADE}, a targeted linguistic fuzzing platform which strengthens the linguistic complexity of seed questions to simultaneously and consistently break a wide range of widely-used LLMs categorized in three groups: eight open-sourced Chinese, six commercial Chinese and four commercial English LLMs. JADE generates three safety benchmarks for the three groups of LLMs, which contain unsafe questions that are highly threatening: the questions simultaneously trigger harmful generation of multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$} (please see the table below), while are still natural questions, fluent and preserving the core unsafe semantics. We release the benchmark demos generated for commercial English LLMs and open-sourced English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For readers who are interested in evaluating on more questions generated by JADE, please contact us.  \textit{JADE} is based on Noam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;-&#35821;&#35328;&#30456;&#20284;&#24230;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;Contrastive Language-Image Pre-training (CLIP)&#21644;&#32972;&#26223;&#36127;&#38754;&#37325;&#26032;&#32553;&#25918;&#25439;&#22833; (BNRL)&#31561;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;Faster R-CNN&#65292;&#22312;&#20302;&#25968;&#25454;&#35774;&#32622;&#19979;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#19968;&#33324;&#21270;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;MS-COCO&#21644;PASCAL VOC&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00278</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#20687;-&#35821;&#35328;&#30456;&#20284;&#24230;&#23545;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Re-Scoring Using Image-Language Similarity for Few-Shot Object Detection. (arXiv:2311.00278v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;-&#35821;&#35328;&#30456;&#20284;&#24230;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;Contrastive Language-Image Pre-training (CLIP)&#21644;&#32972;&#26223;&#36127;&#38754;&#37325;&#26032;&#32553;&#25918;&#25439;&#22833; (BNRL)&#31561;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;Faster R-CNN&#65292;&#22312;&#20302;&#25968;&#25454;&#35774;&#32622;&#19979;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#19968;&#33324;&#21270;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;MS-COCO&#21644;PASCAL VOC&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#26159;&#31038;&#21306;&#20013;&#30340;&#19968;&#20010;&#26032;&#20852;&#25361;&#25112;&#65292;&#20027;&#35201;&#20851;&#27880;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#26469;&#26816;&#27979;&#26032;&#39062;&#23545;&#35937;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#22312;&#20302;&#25968;&#25454;&#35774;&#32622;&#19979;&#21033;&#29992;&#23545;&#27604;&#24615;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#21644;&#22256;&#38590;&#36127;&#38754;&#20998;&#31867;&#25439;&#22833;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20687;-&#35821;&#35328;&#30456;&#20284;&#24230;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;&#30340;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65288;RISF&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20351;&#29992;CLIP&#30340;&#26657;&#20934;&#27169;&#22359;&#65288;CM-CLIP&#65289;&#21644;&#32972;&#26223;&#36127;&#38754;&#37325;&#26032;&#32553;&#25918;&#25439;&#22833;&#65288;BNRL&#65289;&#26469;&#25193;&#23637;Faster R-CNN&#12290;&#21069;&#32773;&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;-&#31867;&#21035;&#30456;&#20284;&#24615;&#23558;&#26816;&#27979;&#22120;&#30340;&#20998;&#31867;&#24471;&#20998;&#36827;&#34892;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#21518;&#32773;&#26159;&#20462;&#25913;&#30340;&#20998;&#31867;&#25439;&#22833;&#65292;&#32771;&#34385;&#20102;&#23545;&#24191;&#20041;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#34394;&#20551;&#32972;&#26223;&#21644;&#28151;&#28102;&#31867;&#21035;&#30340;&#24809;&#32602;&#12290;&#22312;MS-COCO&#21644;PASCAL VOC&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Few-shot object detection, which focuses on detecting novel objects with few labels, is an emerging challenge in the community. Recent studies show that adapting a pre-trained model or modified loss function can improve performance. In this paper, we explore leveraging the power of Contrastive Language-Image Pre-training (CLIP) and hard negative classification loss in low data setting. Specifically, we propose Re-scoring using Image-language Similarity for Few-shot object detection (RISF) which extends Faster R-CNN by introducing Calibration Module using CLIP (CM-CLIP) and Background Negative Re-scale Loss (BNRL). The former adapts CLIP, which performs zero-shot classification, to re-score the classification scores of a detector using image-class similarities, the latter is modified classification loss considering the punishment for fake backgrounds as well as confusing categories on a generalized few-shot object detection dataset. Extensive experiments on MS-COCO and PASCAL VOC show t
&lt;/p&gt;</description></item><item><title>ChatCoder&#26159;&#19968;&#31181;&#36890;&#36807;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26469;&#24110;&#21161;&#20154;&#31867;&#29992;&#25143;&#32454;&#21270;&#38656;&#27714;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00272</link><description>&lt;p&gt;
ChatCoder: &#22522;&#20110;&#32842;&#22825;&#30340;&#38656;&#27714;&#32454;&#21270;&#25913;&#36827;&#20102;LLMs&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ChatCoder: Chat-based Refine Requirement Improves LLMs' Code Generation. (arXiv:2311.00272v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00272
&lt;/p&gt;
&lt;p&gt;
ChatCoder&#26159;&#19968;&#31181;&#36890;&#36807;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26469;&#24110;&#21161;&#20154;&#31867;&#29992;&#25143;&#32454;&#21270;&#38656;&#27714;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#28385;&#36275;&#20154;&#31867;&#38656;&#27714;&#30340;&#20195;&#30721;&#26041;&#38754;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#20154;&#31867;&#38656;&#27714;&#21487;&#33021;&#21547;&#31946;&#12289;&#19981;&#23436;&#25972;&#21644;&#27495;&#20041;&#65292;&#23548;&#33268;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35823;&#35299;&#20154;&#31867;&#38656;&#27714;&#24182;&#20135;&#29983;&#38169;&#35823;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#20154;&#31867;&#29992;&#25143;&#24456;&#38590;&#32454;&#21270;&#38656;&#27714;&#12290;&#20026;&#20102;&#24110;&#21161;&#20154;&#31867;&#29992;&#25143;&#32454;&#21270;&#38656;&#27714;&#24182;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatCoder&#65306;&#19968;&#31181;&#36890;&#36807;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26469;&#32454;&#21270;&#38656;&#27714;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32842;&#22825;&#26041;&#26696;&#65292;&#20854;&#20013;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23558;&#25351;&#23548;&#20154;&#31867;&#29992;&#25143;&#32454;&#21270;&#38656;&#27714;&#30340;&#34920;&#36798;&#65292;&#20351;&#20854;&#27604;&#20197;&#21069;&#26356;&#21152;&#31934;&#30830;&#12289;&#26126;&#30830;&#21644;&#23436;&#25972;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ChatCoder&#22823;&#24133;&#25552;&#39640;&#20102;&#29616;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;ChatCoder&#20855;&#26377;&#20248;&#20110;&#22522;&#20110;&#32500;&#20462;&#30340;&#26041;&#27861;&#21644;&#36890;&#36807;&#20154;&#31867;&#21709;&#24212;&#24494;&#35843;&#30340;LLMs&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have shown good performances in generating code to meet human requirements. However, human requirements expressed in natural languages can be vague, incomplete, and ambiguous, leading large language models to misunderstand human requirements and make mistakes. Worse, it is difficult for a human user to refine the requirement. To help human users refine their requirements and improve large language models' code generation performances, we propose ChatCoder: a method to refine the requirements via chatting with large language models. We design a chat scheme in which the large language models will guide the human users to refine their expression of requirements to be more precise, unambiguous, and complete than before. Experiments show that ChatCoder has improved existing large language models' performance by a large margin. Besides, ChatCoder has the advantage over refine-based methods and LLMs fine-tuned via human response.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#65292;&#37325;&#26032;&#24605;&#32771;&#20102;&#20915;&#31574;Transformer&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24207;&#21015;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#39640;&#23618;&#31574;&#30053;&#20026;&#24403;&#21069;&#29366;&#24577;&#25552;&#20379;&#29702;&#24819;&#25552;&#31034;&#65292;&#20302;&#23618;&#31574;&#30053;&#22312;&#32473;&#23450;&#25552;&#31034;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#21160;&#20316;&#12290;&#20182;&#20204;&#21457;&#29616;&#20915;&#31574;Transformer&#26159;&#36825;&#20010;&#26694;&#26550;&#30340;&#19968;&#20010;&#29305;&#20363;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#20849;&#21516;&#20248;&#21270;&#39640;&#23618;&#21644;&#20302;&#23618;&#31574;&#30053;&#20197;&#23454;&#29616;&#25340;&#25509;&#33021;&#21147;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#26032;&#30340;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2311.00267</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#37325;&#26032;&#24605;&#32771;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Rethinking Decision Transformer via Hierarchical Reinforcement Learning. (arXiv:2311.00267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#65292;&#37325;&#26032;&#24605;&#32771;&#20102;&#20915;&#31574;Transformer&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24207;&#21015;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#39640;&#23618;&#31574;&#30053;&#20026;&#24403;&#21069;&#29366;&#24577;&#25552;&#20379;&#29702;&#24819;&#25552;&#31034;&#65292;&#20302;&#23618;&#31574;&#30053;&#22312;&#32473;&#23450;&#25552;&#31034;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#21160;&#20316;&#12290;&#20182;&#20204;&#21457;&#29616;&#20915;&#31574;Transformer&#26159;&#36825;&#20010;&#26694;&#26550;&#30340;&#19968;&#20010;&#29305;&#20363;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#20849;&#21516;&#20248;&#21270;&#39640;&#23618;&#21644;&#20302;&#23618;&#31574;&#30053;&#20197;&#23454;&#29616;&#25340;&#25509;&#33021;&#21147;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#26032;&#30340;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;Transformer&#65288;DT&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Transformer&#26550;&#26500;&#30340;&#21019;&#26032;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;DT&#30340;&#19968;&#20010;&#26174;&#33879;&#23616;&#38480;&#24615;&#26159;&#20854;&#20381;&#36182;&#20110;&#20174;&#25968;&#25454;&#38598;&#20013;&#22238;&#24518;&#36712;&#36857;&#30340;&#33021;&#21147;&#65292;&#22833;&#21435;&#20102;&#26080;&#32541;&#22320;&#23558;&#27425;&#20248;&#36712;&#36857;&#25340;&#25509;&#22312;&#19968;&#36215;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#24207;&#36143;&#20915;&#31574;&#30340;&#36890;&#29992;&#24207;&#21015;&#24314;&#27169;&#26694;&#26550;&#12290;&#22312;&#20570;&#20915;&#31574;&#26102;&#65292;&#39640;&#23618;&#31574;&#30053;&#39318;&#20808;&#20026;&#24403;&#21069;&#29366;&#24577;&#25552;&#20986;&#19968;&#20010;&#29702;&#24819;&#30340;&#25552;&#31034;&#65292;&#20302;&#23618;&#31574;&#30053;&#38543;&#21518;&#22312;&#32473;&#23450;&#30340;&#25552;&#31034;&#26465;&#20214;&#19979;&#29983;&#25104;&#19968;&#20010;&#21160;&#20316;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DT&#26159;&#36825;&#20010;&#26694;&#26550;&#30340;&#29305;&#20363;&#65292;&#36890;&#36807;&#19968;&#23450;&#30340;&#39640;&#23618;&#21644;&#20302;&#23618;&#31574;&#30053;&#36873;&#25321;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#36873;&#25321;&#30340;&#28508;&#22312;&#22833;&#36133;&#12290;&#21463;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#20849;&#21516;&#20248;&#21270;&#39640;&#23618;&#21644;&#20302;&#23618;&#31574;&#30053;&#20197;&#23454;&#29616;&#25340;&#25509;&#33021;&#21147;&#65292;&#36827;&#32780;&#25512;&#21160;&#26032;&#30340;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision Transformer (DT) is an innovative algorithm leveraging recent advances of the transformer architecture in reinforcement learning (RL). However, a notable limitation of DT is its reliance on recalling trajectories from datasets, losing the capability to seamlessly stitch sub-optimal trajectories together. In this work we introduce a general sequence modeling framework for studying sequential decision making through the lens of Hierarchical RL. At the time of making decisions, a high-level policy first proposes an ideal prompt for the current state, a low-level policy subsequently generates an action conditioned on the given prompt. We show DT emerges as a special case of this framework with certain choices of high-level and low-level policies, and discuss the potential failure of these choices. Inspired by these observations, we study how to jointly optimize the high-level and low-level policies to enable the stitching ability, which further leads to the development of new offl
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;&#31574;&#30053;&#35268;&#21010;&#22120;(PDDPP)&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33539;&#24335;&#65292;&#36890;&#36807;&#21487;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#25554;&#20214;&#23454;&#29616;&#20027;&#21160;&#23545;&#35805;&#38382;&#39064;&#30340;&#31574;&#30053;&#21046;&#23450;&#12290;&#21033;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#26032;&#30340;&#26696;&#20363;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00262</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;&#31574;&#30053;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents. (arXiv:2311.00262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00262
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;&#31574;&#30053;&#35268;&#21010;&#22120;(PDDPP)&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33539;&#24335;&#65292;&#36890;&#36807;&#21487;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#25554;&#20214;&#23454;&#29616;&#20027;&#21160;&#23545;&#35805;&#38382;&#39064;&#30340;&#31574;&#30053;&#21046;&#23450;&#12290;&#21033;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#26032;&#30340;&#26696;&#20363;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26102;&#20195;&#20013;&#65292;&#20027;&#21160;&#23545;&#35805;&#20316;&#20026;&#19968;&#20010;&#23454;&#38469;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#35805;&#38382;&#39064;&#65292;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#26159;&#25552;&#39640;LLMs&#20027;&#21160;&#24615;&#30340;&#20851;&#38190;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26041;&#26696;&#25110;&#36890;&#36807;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#36845;&#20195;&#22686;&#24378;&#23545;LLMs&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#21463;&#38480;&#20110;&#20923;&#32467;&#30340;LLMs&#30340;&#31574;&#30053;&#35268;&#21010;&#33021;&#21147;&#65292;&#35201;&#20040;&#38590;&#20197;&#36716;&#31227;&#21040;&#26032;&#30340;&#26696;&#20363;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#33539;&#24335;&#65292;&#20197;&#20351;&#29992;&#21487;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#25554;&#20214;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#35805;&#31574;&#30053;&#35268;&#21010;&#22120;&#26469;&#21046;&#23450;LLMs&#22312;&#20027;&#21160;&#23545;&#35805;&#38382;&#39064;&#19978;&#30340;&#31574;&#30053;&#65292;&#21629;&#21517;&#20026;PPDPP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#20415;&#21033;&#29992;&#21487;&#29992;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;LLM&#30340;&#33258;&#25105;&#23545;&#24328;&#25910;&#38598;&#30340;&#21160;&#24577;&#20132;&#20114;&#25968;&#25454;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#21453;&#21521;&#35823;&#24046;&#20998;&#26512;&#35745;&#31639;&#20102;&#22810;&#20219;&#21153;&#21644;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24335;&#35757;&#32451;&#20559;&#24046;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#20102;&#21407;&#22987;&#25439;&#22833;&#12289;&#24341;&#20837;&#20102;&#38544;&#24335;&#24179;&#22374;&#27491;&#21017;&#39033;&#21644;&#20914;&#31361;&#39033;&#12290;&#22312;&#22810;&#20219;&#21153;&#20013;&#65292;&#20914;&#31361;&#39033;&#34913;&#37327;&#20102;&#20219;&#21153;&#26799;&#24230;&#20043;&#38388;&#30340;&#23545;&#40784;&#24615;&#65307;&#32780;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#20914;&#31361;&#39033;&#26159;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#26032;&#27010;&#24565;&#65292;&#23427;&#36890;&#36807;&#20219;&#21153;&#26799;&#24230;&#20043;&#38388;&#30340;&#26446;&#25324;&#21495;&#26469;&#34913;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.00235</link><description>&lt;p&gt;
&#20174;&#21453;&#21521;&#35823;&#24046;&#20998;&#26512;&#35282;&#24230;&#30475;&#22810;&#20219;&#21153;&#21644;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Implicit biases in multitask and continual learning from a backward error analysis perspective. (arXiv:2311.00235v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#21453;&#21521;&#35823;&#24046;&#20998;&#26512;&#35745;&#31639;&#20102;&#22810;&#20219;&#21153;&#21644;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24335;&#35757;&#32451;&#20559;&#24046;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#20102;&#21407;&#22987;&#25439;&#22833;&#12289;&#24341;&#20837;&#20102;&#38544;&#24335;&#24179;&#22374;&#27491;&#21017;&#39033;&#21644;&#20914;&#31361;&#39033;&#12290;&#22312;&#22810;&#20219;&#21153;&#20013;&#65292;&#20914;&#31361;&#39033;&#34913;&#37327;&#20102;&#20219;&#21153;&#26799;&#24230;&#20043;&#38388;&#30340;&#23545;&#40784;&#24615;&#65307;&#32780;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#20914;&#31361;&#39033;&#26159;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#26032;&#27010;&#24565;&#65292;&#23427;&#36890;&#36807;&#20219;&#21153;&#26799;&#24230;&#20043;&#38388;&#30340;&#26446;&#25324;&#21495;&#26469;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21453;&#21521;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#21644;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#38544;&#24335;&#35757;&#32451;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38544;&#21547;&#22320;&#26368;&#23567;&#21270;&#30340;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#12290;&#23427;&#20204;&#21253;&#25324;&#19977;&#20010;&#39033;&#65306;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#65288;&#32771;&#34385;&#25910;&#25947;&#24615;&#65289;&#65292;&#19982;&#23398;&#20064;&#29575;&#25104;&#27491;&#27604;&#30340;&#38544;&#24335;&#24179;&#22374;&#27491;&#21017;&#39033;&#20197;&#21450;&#26368;&#21518;&#19968;&#20010;&#39033;&#8212;&#8212;&#20914;&#31361;&#39033;&#65292;&#35813;&#39033;&#22312;&#29702;&#35770;&#19978;&#23545;&#25910;&#25947;&#24615;&#21644;&#38544;&#24335;&#27491;&#21017;&#21270;&#37117;&#21487;&#33021;&#26377;&#23475;&#12290;&#22312;&#22810;&#20219;&#21153;&#20013;&#65292;&#20914;&#31361;&#39033;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#20219;&#21153;&#20043;&#38388;&#30340;&#26799;&#24230;&#23545;&#40784;&#24615;&#65292;&#32780;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#20914;&#31361;&#39033;&#26159;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#26032;&#37327;&#65292;&#23613;&#31649;&#22312;&#24494;&#20998;&#20960;&#20309;&#20013;&#26159;&#19968;&#20010;&#22522;&#26412;&#24037;&#20855;&#65306;&#20219;&#21153;&#26799;&#24230;&#20043;&#38388;&#30340;&#26446;&#25324;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using backward error analysis, we compute implicit training biases in multitask and continual learning settings for neural networks trained with stochastic gradient descent. In particular, we derive modified losses that are implicitly minimized during training. They have three terms: the original loss, accounting for convergence, an implicit flatness regularization term proportional to the learning rate, and a last term, the conflict term, which can theoretically be detrimental to both convergence and implicit regularization. In multitask, the conflict term is a well-known quantity, measuring the gradient alignment between the tasks, while in continual learning the conflict term is a new quantity in deep learning optimization, although a basic tool in differential geometry: The Lie bracket between the task gradients.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;StableFDG&#65292;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#21644;&#27880;&#24847;&#21147;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#23454;&#29616;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#39118;&#26684;&#30340;&#23398;&#20064;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#24335;&#25193;&#23637;&#21040;&#21407;&#22987;&#28304;&#22495;&#20043;&#22806;&#65292;&#24182;&#36890;&#36807;&#39118;&#26684;&#20849;&#20139;&#12289;&#36716;&#31227;&#21644;&#25506;&#32034;&#31574;&#30053;&#25913;&#36827;&#20102;&#39046;&#22495;&#22810;&#26679;&#24615;&#12290;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#31361;&#20986;&#22120;&#25429;&#25417;&#20102;&#25968;&#25454;&#26679;&#26412;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00227</link><description>&lt;p&gt;
StableFDG:&#22522;&#20110;&#39118;&#26684;&#21644;&#27880;&#24847;&#21147;&#30340;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StableFDG: Style and Attention Based Learning for Federated Domain Generalization. (arXiv:2311.00227v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;StableFDG&#65292;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#21644;&#27880;&#24847;&#21147;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#23454;&#29616;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#39118;&#26684;&#30340;&#23398;&#20064;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#24335;&#25193;&#23637;&#21040;&#21407;&#22987;&#28304;&#22495;&#20043;&#22806;&#65292;&#24182;&#36890;&#36807;&#39118;&#26684;&#20849;&#20139;&#12289;&#36716;&#31227;&#21644;&#25506;&#32034;&#31574;&#30053;&#25913;&#36827;&#20102;&#39046;&#22495;&#22810;&#26679;&#24615;&#12290;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#31361;&#20986;&#22120;&#25429;&#25417;&#20102;&#25968;&#25454;&#26679;&#26412;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#65288;&#28304;&#22495;&#65289;&#21644;&#27979;&#35797;&#65288;&#30446;&#26631;&#22495;&#65289;&#20013;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#24120;&#21457;&#29983;&#22495;&#20559;&#31227;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20013;&#24341;&#20837;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#26679;&#26412;/&#22495;&#65292;&#29616;&#26377;&#30340;&#39046;&#22495;&#27867;&#21270;&#31639;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#38754;&#20020;&#22522;&#26412;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#39118;&#26684;&#21644;&#27880;&#24847;&#21147;&#30340;&#23398;&#20064;&#31574;&#30053;StableFDG&#65292;&#29992;&#20110;&#23454;&#29616;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#31532;&#19968;&#20010;&#26159;&#22522;&#20110;&#39118;&#26684;&#30340;&#23398;&#20064;&#65292;&#23427;&#20351;&#27599;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#36229;&#36234;&#21407;&#22987;&#28304;&#22495;&#65292;&#25506;&#32034;&#26032;&#39062;&#30340;&#39118;&#26684;&#65292;&#22522;&#20110;&#25552;&#20986;&#30340;&#39118;&#26684;&#20849;&#20139;&#12289;&#36716;&#31227;&#21644;&#25506;&#32034;&#31574;&#30053;&#25913;&#36827;&#20102;&#39046;&#22495;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#31361;&#20986;&#22120;&#65292;&#23427;&#25429;&#25417;&#25968;&#25454;&#26679;&#26412;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional federated learning (FL) algorithms operate under the assumption that the data distributions at training (source domains) and testing (target domain) are the same. The fact that domain shifts often occur in practice necessitates equipping FL methods with a domain generalization (DG) capability. However, existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each client's local dataset. In this paper, we propose StableFDG, a style and attention based learning strategy for accomplishing federated domain generalization, introducing two key contributions. The first is style-based learning, which enables each client to explore novel styles beyond the original source domains in its local dataset, improving domain diversity based on the proposed style sharing, shifting, and exploration strategies. Our second contribution is an attention-based feature highlighter, which captures the similarities between the features of data samples in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINN)&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#22495;&#20998;&#35299;&#21644;Schwarz&#20132;&#26367;&#27861;&#32806;&#21512;PINN&#19982;&#24444;&#27492;&#21644;&#20256;&#32479;&#25968;&#20540;&#27169;&#22411;&#65292;&#20197;&#21152;&#36895;&#35757;&#32451;&#21644;&#35299;&#20915;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#20013;&#30340;&#22256;&#38590;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00224</link><description>&lt;p&gt;
&#22522;&#20110;&#22495;&#20998;&#35299;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;Schwarz&#20132;&#26367;&#27861;&#32806;&#21512;
&lt;/p&gt;
&lt;p&gt;
Domain decomposition-based coupling of physics-informed neural networks via the Schwarz alternating method. (arXiv:2311.00224v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINN)&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#22495;&#20998;&#35299;&#21644;Schwarz&#20132;&#26367;&#27861;&#32806;&#21512;PINN&#19982;&#24444;&#27492;&#21644;&#20256;&#32479;&#25968;&#20540;&#27169;&#22411;&#65292;&#20197;&#21152;&#36895;&#35757;&#32451;&#21644;&#35299;&#20915;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#20013;&#30340;&#22256;&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26159;&#35299;&#20915;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#21644;&#25512;&#26029;&#35299;&#30340;&#21560;&#24341;&#20154;&#30340;&#25968;&#25454;&#39537;&#21160;&#24037;&#20855;&#12290;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;(NNs)&#21482;&#22312;&#35299;&#20915;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#19981;&#21516;&#65292;PINN&#23558;PDE&#30340;&#27531;&#24046;&#32467;&#21512;&#21040;&#20854;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#24182;&#22312;&#35299;&#20915;&#22495;&#19978;&#30340;&#19968;&#32452;&#25554;&#20540;&#28857;&#19978;&#35757;&#32451;&#20197;&#26368;&#23567;&#21270;&#35813;&#27531;&#24046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;Schwarz&#20132;&#26367;&#27861;&#23558;PINN&#19982;&#24444;&#27492;&#21644;&#20256;&#32479;&#25968;&#20540;&#27169;&#22411;(&#21363;&#36890;&#36807;&#26377;&#38480;&#20803;&#12289;&#26377;&#38480;&#24046;&#20998;&#25110;&#26377;&#38480;&#20307;&#31215;&#26041;&#27861;&#33719;&#24471;&#30340;&#23436;&#20840;&#24207;&#27169;&#22411;&#25110;FOMs)&#32806;&#21512;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#23545;&#29289;&#29702;&#22495;&#36827;&#34892;&#20102;&#20998;&#35299;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#24403;PDE&#35299;&#20855;&#26377;&#38497;&#23789;&#30340;&#26799;&#24230;&#26102;&#65292;&#35757;&#32451;PINN&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#22495;&#20998;&#35299;&#21644;Schwarz&#20132;&#26367;&#27861;&#21152;&#36895;PINN&#35757;&#32451;&#38454;&#27573;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) are appealing data-driven tools for solving and inferring solutions to nonlinear partial differential equations (PDEs). Unlike traditional neural networks (NNs), which train only on solution data, a PINN incorporates a PDE's residual into its loss function and trains to minimize the said residual at a set of collocation points in the solution domain. This paper explores the use of the Schwarz alternating method as a means to couple PINNs with each other and with conventional numerical models (i.e., full order models, or FOMs, obtained via the finite element, finite difference or finite volume methods) following a decomposition of the physical domain. It is well-known that training a PINN can be difficult when the PDE solution has steep gradients. We investigate herein the use of domain decomposition and the Schwarz alternating method as a means to accelerate the PINN training phase. Within this context, we explore different approaches for imposi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31639;&#27861;&#36924;&#30495;&#24615;&#21644;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;LLMs&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#24635;&#32479;&#25237;&#31080;&#34892;&#20026;&#65292;&#20294;&#22312;&#20934;&#30830;&#34920;&#31034;&#20840;&#29699;&#21464;&#26262;&#35266;&#28857;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;LLMs&#23545;&#26576;&#20123;&#32676;&#20307;&#30340;&#35266;&#28857;&#20272;&#35745;&#23384;&#22312;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27954;&#35028;&#32654;&#22269;&#20154;&#23545;&#20840;&#29699;&#21464;&#26262;&#30340;&#25285;&#24551;&#20302;&#20272;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;LLMs&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#20934;&#30830;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00217</link><description>&lt;p&gt;
&#33021;&#21542;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#20840;&#29699;&#21464;&#26262;&#30340;&#20844;&#20247;&#24847;&#35265;&#65311;&#19968;&#39033;&#20851;&#20110;&#31639;&#27861;&#36924;&#30495;&#24615;&#21644;&#20559;&#35265;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias. (arXiv:2311.00217v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31639;&#27861;&#36924;&#30495;&#24615;&#21644;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;LLMs&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#24635;&#32479;&#25237;&#31080;&#34892;&#20026;&#65292;&#20294;&#22312;&#20934;&#30830;&#34920;&#31034;&#20840;&#29699;&#21464;&#26262;&#35266;&#28857;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;LLMs&#23545;&#26576;&#20123;&#32676;&#20307;&#30340;&#35266;&#28857;&#20272;&#35745;&#23384;&#22312;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27954;&#35028;&#32654;&#22269;&#20154;&#23545;&#20840;&#29699;&#21464;&#26262;&#30340;&#25285;&#24551;&#20302;&#20272;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;LLMs&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#20934;&#30830;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#30340;&#24863;&#30693;&#21644;&#34892;&#20026;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#34987;&#31216;&#20026;&#31639;&#27861;&#36924;&#30495;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;&#27668;&#20505;&#21464;&#21270;&#35843;&#26597;&#35780;&#20272;LLMs&#30340;&#31639;&#27861;&#36924;&#30495;&#24615;&#21644;&#20559;&#35265;&#12290;LLMs&#34987;&#26465;&#20214;&#21270;&#20026;&#22522;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;/&#25110;&#24515;&#29702;&#21327;&#21464;&#37327;&#26469;&#27169;&#25311;&#35843;&#26597;&#22238;&#31572;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#30456;&#20851;&#21327;&#21464;&#37327;&#27809;&#26377;&#21253;&#21547;&#22312;&#20869;&#26102;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#24635;&#32479;&#25237;&#31080;&#34892;&#20026;&#65292;&#20294;&#22312;&#20934;&#30830;&#34920;&#31034;&#20840;&#29699;&#21464;&#26262;&#35266;&#28857;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#24403;LLMs&#34987;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#21327;&#21464;&#37327;&#21516;&#26102;&#26465;&#20214;&#21270;&#26102;&#65292;GPT-4&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#29305;&#23450;&#32676;&#20307;&#30340;&#35266;&#28857;&#20272;&#35745;&#23384;&#22312;&#24046;&#24322;&#65292;LLMs&#20542;&#21521;&#20110;&#20302;&#20272;&#38750;&#27954;&#35028;&#32654;&#22269;&#20154;&#23545;&#20840;&#29699;&#21464;&#26262;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#31361;&#20986;&#20102;LLMs&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#31934;&#30830;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated their potential in social science research by emulating human perceptions and behaviors, a concept referred to as algorithmic fidelity. This study assesses the algorithmic fidelity and bias of LLMs by utilizing two nationally representative climate change surveys. The LLMs were conditioned on demographics and/or psychological covariates to simulate survey responses. The findings indicate that LLMs can effectively capture presidential voting behaviors but encounter challenges in accurately representing global warming perspectives when relevant covariates are not included. GPT-4 exhibits improved performance when conditioned on both demographics and covariates. However, disparities emerge in LLM estimations of the views of certain groups, with LLMs tending to underestimate worry about global warming among Black Americans. While highlighting the potential of LLMs to aid social science research, these results underscore the importance of metic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#23454;&#29616;&#39640;&#25928;&#32534;&#36753;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#38271;&#35270;&#39057;&#37319;&#26679;&#26657;&#27491;&#30830;&#20445;&#19968;&#33268;&#24615;&#12290;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2311.00213</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#23454;&#29616;&#19968;&#33268;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Consistent Video-to-Video Transfer Using Synthetic Dataset. (arXiv:2311.00213v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#25351;&#20196;&#23454;&#29616;&#39640;&#25928;&#32534;&#36753;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#38271;&#35270;&#39057;&#37319;&#26679;&#26657;&#27491;&#30830;&#20445;&#19968;&#33268;&#24615;&#12290;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#27599;&#20010;&#35270;&#39057;&#27599;&#20010;&#27169;&#22411;&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#24494;&#35843;&#38656;&#27714;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#20026;&#35270;&#39057;&#21040;&#35270;&#39057;&#36716;&#25442;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#21512;&#25104;&#37197;&#23545;&#35270;&#39057;&#25968;&#25454;&#38598;&#12290;&#21463;&#21040;Instruct Pix2Pix&#30340;&#22270;&#20687;&#36890;&#36807;&#32534;&#36753;&#25351;&#20196;&#36827;&#34892;&#36716;&#25442;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#33539;&#24335;&#24212;&#29992;&#20110;&#35270;&#39057;&#39046;&#22495;&#12290;&#25105;&#20204;&#23545;Prompt-to-Prompt&#36827;&#34892;&#20102;&#25299;&#23637;&#65292;&#39640;&#25928;&#29983;&#25104;&#37197;&#23545;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#37117;&#21253;&#21547;&#19968;&#20010;&#36755;&#20837;&#35270;&#39057;&#21644;&#20854;&#32534;&#36753;&#21518;&#30340;&#23545;&#24212;&#35270;&#39057;&#12290;&#21516;&#26102;&#65292;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#38271;&#35270;&#39057;&#37319;&#26679;&#26657;&#27491;&#65292;&#30830;&#20445;&#25209;&#27425;&#20043;&#38388;&#30340;&#38271;&#35270;&#39057;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#24403;&#21069;&#30340;Tune-A-Video&#31561;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel and efficient approach for text-based video-to-video editing that eliminates the need for resource-intensive per-video-per-model finetuning. At the core of our approach is a synthetic paired video dataset tailored for video-to-video transfer tasks. Inspired by Instruct Pix2Pix's image transfer via editing instruction, we adapt this paradigm to the video domain. Extending the Prompt-to-Prompt to videos, we efficiently generate paired samples, each with an input video and its edited counterpart. Alongside this, we introduce the Long Video Sampling Correction during sampling, ensuring consistent long videos across batches. Our method surpasses current methods like Tune-A-Video, heralding substantial progress in text-based video-to-video editing and suggesting exciting avenues for further exploration and deployment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Magmaw&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#36827;&#34892;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#25239;&#25915;&#20987;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#29983;&#25104;&#36890;&#29992;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#25915;&#20987;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#23545;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30340;&#38887;&#24615;&#12290;&#20351;&#29992;&#23454;&#26102;&#26080;&#32447;&#25915;&#20987;&#24179;&#21488;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.00207</link><description>&lt;p&gt;
Magmaw: &#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Magmaw: Modality-Agnostic Adversarial Attacks on Machine Learning-Based Wireless Communication Systems. (arXiv:2311.00207v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Magmaw&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#36827;&#34892;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#25239;&#25915;&#20987;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#29983;&#25104;&#36890;&#29992;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#25915;&#20987;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#23545;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30340;&#38887;&#24615;&#12290;&#20351;&#29992;&#23454;&#26102;&#26080;&#32447;&#25915;&#20987;&#24179;&#21488;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21512;&#24182;&#31471;&#21040;&#31471;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#25152;&#26377;&#29289;&#29702;&#23618;&#27169;&#22359;&#20197;&#23454;&#29616;&#32852;&#21512;&#25910;&#21457;&#22120;&#20248;&#21270;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#38024;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#31995;&#32479;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24182;&#26410;&#25552;&#20379;&#21253;&#25324;&#28304;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#12289;&#20849;&#21516;&#30340;&#29289;&#29702;&#23618;&#32452;&#20214;&#21644;&#26080;&#32447;&#39046;&#22495;&#32422;&#26463;&#22312;&#20869;&#30340;&#20840;&#38754;&#35270;&#35282;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Magmaw&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#38024;&#23545;&#36890;&#36807;&#26080;&#32447;&#20449;&#36947;&#20256;&#36755;&#30340;&#20219;&#20309;&#22810;&#27169;&#24577;&#20449;&#21495;&#29983;&#25104;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#19979;&#28216;&#24212;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;&#24341;&#20837;&#20102;&#26032;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#25915;&#20987;&#23545;&#29616;&#26377;&#24191;&#27867;&#20351;&#29992;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#25200;&#21160;&#20449;&#21495;&#20943;&#27861;&#38450;&#24481;&#26041;&#27861;&#30340;&#38887;&#24615;&#12290;&#20026;&#20102;&#27010;&#24565;&#35777;&#26126;&#65292;&#25105;&#20204;&#20351;&#29992;&#36719;&#20214;&#23450;&#20041;&#26080;&#32447;&#30005;&#31995;&#32479;&#26500;&#24314;&#20102;&#19968;&#20010;&#23454;&#26102;&#26080;&#32447;&#25915;&#20987;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has been instrumental in enabling joint transceiver optimization by merging all physical layer blocks of the end-to-end wireless communication systems. Although there have been a number of adversarial attacks on ML-based wireless systems, the existing methods do not provide a comprehensive view including multi-modality of the source data, common physical layer components, and wireless domain constraints. This paper proposes Magmaw, the first black-box attack methodology capable of generating universal adversarial perturbations for any multimodal signal transmitted over a wireless channel. We further introduce new objectives for adversarial attacks on ML-based downstream applications. The resilience of the attack to the existing widely used defense methods of adversarial training and perturbation signal subtraction is experimentally verified. For proof-of-concept evaluation, we build a real-time wireless attack platform using a software-defined radio system. Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#23618;&#27425;&#27604;&#36739;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#38646;&#26679;&#26412;&#24320;&#25918;&#35789;&#27719;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36882;&#24402;&#22320;&#23558;&#31867;&#21035;&#36827;&#34892;&#20998;&#32452;&#65292;&#24182;&#22312;&#27599;&#20010;&#23618;&#27425;&#32423;&#21035;&#19978;&#27604;&#36739;&#22270;&#20687;&#19982;&#25991;&#26412;&#30340;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#30452;&#35266;&#12289;&#26377;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2311.00206</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#23618;&#27425;&#27604;&#36739;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-Powered Hierarchical Comparisons for Image Classification. (arXiv:2311.00206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#23618;&#27425;&#27604;&#36739;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#38646;&#26679;&#26412;&#24320;&#25918;&#35789;&#27719;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36882;&#24402;&#22320;&#23558;&#31867;&#21035;&#36827;&#34892;&#20998;&#32452;&#65292;&#24182;&#22312;&#27599;&#20010;&#23618;&#27425;&#32423;&#21035;&#19978;&#27604;&#36739;&#22270;&#20687;&#19982;&#25991;&#26412;&#30340;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#30452;&#35266;&#12289;&#26377;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#38646;&#26679;&#26412;&#24320;&#25918;&#35789;&#27719;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;CLIP&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#20013;&#33719;&#30410;&#65292;&#23558;&#31867;&#21035;&#29305;&#23450;&#30693;&#35782;&#34701;&#20837;&#20854;&#20013;&#12290;&#28982;&#32780;&#65292;CLIP&#20013;&#23384;&#22312;&#20559;&#24046;&#23548;&#33268;&#19981;&#21516;&#20294;&#30456;&#20851;&#31867;&#21035;&#20855;&#26377;&#30456;&#20284;&#30340;&#25551;&#36848;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#27604;&#36739;&#23558;&#31867;&#21035;&#36882;&#24402;&#22320;&#20998;&#32452;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#27599;&#20010;&#23618;&#27425;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23884;&#20837;&#26469;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#65292;&#20174;&#32780;&#23454;&#29616;&#30452;&#35266;&#12289;&#26377;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The zero-shot open-vocabulary challenge in image classification is tackled by pretrained vision-language models like CLIP, which benefit from incorporating class-specific knowledge from large language models (LLMs) like ChatGPT. However, biases in CLIP lead to similar descriptions for distinct but related classes, prompting our novel image classification framework via hierarchical comparisons: using LLMs to recursively group classes into hierarchies and classifying images by comparing image-text embeddings at each hierarchy level, resulting in an intuitive, effective, and explainable approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20013;&#25991;&#21307;&#23398;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#24555;&#36895;&#36866;&#24212;Llama 2&#22522;&#30784;&#27169;&#22411;&#21040;&#20013;&#25991;&#21307;&#23398;&#39046;&#22495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#19982;GPT-3.5-turbo&#30456;&#23218;&#32654;&#65292;&#20294;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#26356;&#23569;&#12290;&#36825;&#20026;&#22312;&#19981;&#21516;&#39046;&#22495;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#26495;&#12290;</title><link>http://arxiv.org/abs/2311.00204</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#65292;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#35757;&#32451;&#21644;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering. (arXiv:2311.00204v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20013;&#25991;&#21307;&#23398;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#24555;&#36895;&#36866;&#24212;Llama 2&#22522;&#30784;&#27169;&#22411;&#21040;&#20013;&#25991;&#21307;&#23398;&#39046;&#22495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#19982;GPT-3.5-turbo&#30456;&#23218;&#32654;&#65292;&#20294;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#26356;&#23569;&#12290;&#36825;&#20026;&#22312;&#19981;&#21516;&#39046;&#22495;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36890;&#36807;&#20174;&#22522;&#30784;&#27169;&#22411;&#20013;&#24320;&#21457;&#39046;&#22495;&#19987;&#23478;&#65292;&#21487;&#20197;&#22312;&#19981;&#20250;&#36896;&#25104;&#36807;&#39640;&#35757;&#32451;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#22810;&#31181;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#24555;&#36895;&#36866;&#24212;&#20013;&#25991;&#21307;&#23398;&#39046;&#22495;&#30340;Llama 2&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#26469;&#33258;&#20013;&#22269;&#21307;&#23398;&#21442;&#32771;&#36164;&#26009;&#30340;10&#20159;&#20010;&#26631;&#35760;&#36827;&#34892;&#36830;&#32493;&#35757;&#32451;&#65292;&#20197;&#25945;&#25480;&#30456;&#20851;&#30340;&#35789;&#27719;&#21644;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#26469;&#33258;&#20013;&#22269;&#22269;&#23478;&#21307;&#30103;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#30340;5.4&#19975;&#20010;&#31034;&#20363;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#20013;&#25991;&#21307;&#23398;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#19982;GPT-3.5-turbo&#30456;&#23218;&#32654;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#35201;&#23569;&#24471;&#22810;&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#21487;&#20197;&#22312;&#21508;&#31181;&#20013;&#25991;&#21307;&#23398;&#24212;&#29992;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#36825;&#20026;&#22312;&#32570;&#20047;&#19987;&#19994;&#30693;&#35782;&#30340;&#39046;&#22495;&#20013;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models exhibit promising general capabilities but often lack specialized knowledge for domain-specific tasks. Developing domain experts from a base model enables a range of applications without prohibitive training costs. This work demonstrates a method using continuous training and instruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese medical domain. We first conduct continuous training on 1B tokens from Chinese medical references to teach relevant vocabulary and knowledge. The models are then fine-tuned on 54K examples sourced from the Chinese National Medical Licensing Examination. Experiments on Chinese medical data confirm the effectiveness of this approach, producing a model comparable to GPT-3.5-turbo while using way less computational resource. The resulting domain-specific model could be useful for various Chinese medical applications. More broadly, this provides a template for domain-specific training of large language models in areas wher
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;&#27880;&#37322;&#32773;&#30340;&#27880;&#37322;&#65292;&#36328;&#22810;&#20803;&#31038;&#21306;&#27169;&#22411;&#20027;&#35266;&#24615;&#65292;&#20197;&#35299;&#20915;&#27602;&#24615;&#35780;&#35770;&#35782;&#21035;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.00203</link><description>&lt;p&gt;
&#36328;&#22810;&#20803;&#31038;&#21306;&#27169;&#25311;&#27880;&#37322;&#32773;&#27880;&#37322;&#20197;&#24314;&#27169;&#27602;&#24615;&#35780;&#35770;&#35782;&#21035;&#20013;&#30340;&#20027;&#35266;&#24615;
&lt;/p&gt;
&lt;p&gt;
Modeling subjectivity (by Mimicking Annotator Annotation) in toxic comment identification across diverse communities. (arXiv:2311.00203v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;&#27880;&#37322;&#32773;&#30340;&#27880;&#37322;&#65292;&#36328;&#22810;&#20803;&#31038;&#21306;&#27169;&#22411;&#20027;&#35266;&#24615;&#65292;&#20197;&#35299;&#20915;&#27602;&#24615;&#35780;&#35770;&#35782;&#21035;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#27602;&#24615;&#35752;&#35770;&#30340;&#27969;&#34892;&#21644;&#24433;&#21709;&#20351;&#20869;&#23481;&#23457;&#26680;&#25104;&#20026;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#33258;&#21160;&#21270;&#31995;&#32479;&#22312;&#35782;&#21035;&#27602;&#24615;&#21644;&#20943;&#23569;&#23545;&#20154;&#24037;&#23457;&#26680;&#30340;&#20381;&#36182;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#19981;&#21516;&#31038;&#21306;&#30340;&#27602;&#24615;&#35780;&#35770;&#35782;&#21035;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#26412;&#25991;&#23545;&#27492;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#26412;&#30740;&#31350;&#30340;&#20004;&#20010;&#30446;&#26631;&#26159;&#65306;&#65288;1&#65289;&#20351;&#29992;&#23450;&#37327;&#20998;&#26512;&#30830;&#23450;&#27880;&#37322;&#32773;&#20998;&#27495;&#30340;&#30452;&#35266;&#24046;&#24322;&#65292;&#65288;2&#65289;&#27169;&#25311;&#36825;&#20123;&#35266;&#28857;&#30340;&#20027;&#35266;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;\footnote{\url{https://github.com/XXX}}&#65292;&#20854;&#20013;&#21253;&#21547;&#19987;&#23478;&#27880;&#37322;&#32773;&#30340;&#27880;&#37322;&#65292;&#24182;&#20351;&#29992;&#20854;&#20182;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#26469;&#35782;&#21035;&#27602;&#24615;&#30340;&#20027;&#35266;&#24615;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#27169;&#20223;&#22810;&#20803;&#21270;&#27602;&#24615;&#35266;&#28857;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21464;&#21270;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#21644;&#21033;&#29992;&#19982;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#27979;&#35797;&#38598;&#30456;&#21516;&#30340;&#27880;&#37322;&#32773;&#20316;&#20026;&#27979;&#35797;&#38598;&#65292;&#20197;&#21450;&#21478;&#22806;&#19968;&#32452;&#27880;&#37322;&#32773;&#20316;&#20026;&#27979;&#35797;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence and impact of toxic discussions online have made content moderation crucial.Automated systems can play a vital role in identifying toxicity, and reducing the reliance on human moderation.Nevertheless, identifying toxic comments for diverse communities continues to present challenges that are addressed in this paper.The two-part goal of this study is to(1)identify intuitive variances from annotator disagreement using quantitative analysis and (2)model the subjectivity of these viewpoints.To achieve our goal, we published a new dataset\footnote{\url{https://github.com/XXX}} with expert annotators' annotations and used two other public datasets to identify the subjectivity of toxicity.Then leveraging the Large Language Model(LLM),we evaluate the model's ability to mimic diverse viewpoints on toxicity by varying size of the training data and utilizing same set of annotators as the test set used during model training and a separate set of annotators as the test set.We conclud
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#20248;&#21270;&#20840;&#23616;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#24635;&#22870;&#21169;&#65292;&#23454;&#29616;&#21327;&#20316;&#20915;&#31574;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#21463;&#20449;&#24687;&#20849;&#20139;&#19981;&#23436;&#22791;&#30340;&#24433;&#21709;&#65292;&#19988;&#20855;&#26377;&#38750;&#28176;&#36817;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.00201</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Natural Policy Gradient Methods for Multi-task Reinforcement Learning. (arXiv:2311.00201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#20248;&#21270;&#20840;&#23616;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#24635;&#22870;&#21169;&#65292;&#23454;&#29616;&#21327;&#20316;&#20915;&#31574;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#21463;&#20449;&#24687;&#20849;&#20139;&#19981;&#23436;&#22791;&#30340;&#24433;&#21709;&#65292;&#19988;&#20855;&#26377;&#38750;&#28176;&#36817;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#20915;&#31574;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22810;&#20219;&#21153;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#26377;&#33258;&#24049;&#30340;&#31169;&#26377;&#22870;&#21169;&#20989;&#25968;&#23545;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#20849;&#20139;&#29615;&#22659;&#30340;&#30456;&#21516;&#36716;&#31227;&#26680;&#12290;&#38024;&#23545;&#26080;&#38480;&#26102;&#38388;&#27493;&#26631;&#35760;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#20986;&#19968;&#31181;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#65292;&#22312;&#20998;&#25955;&#30340;&#26041;&#24335;&#19979;&#65292;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#25240;&#25187;&#24635;&#22870;&#21169;&#20043;&#21644;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#20165;&#19982;&#20854;&#22312;&#32473;&#23450;&#22270;&#25299;&#25169;&#20013;&#30340;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#12290;&#25105;&#20204;&#22312; softmax &#21442;&#25968;&#21270;&#19979;&#24320;&#23637;&#20102;&#32852;&#37030;&#32431;&#31929;&#21644;&#29109;&#27491;&#21017;&#21270;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;NPG&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#23558;&#26799;&#24230;&#36319;&#36394;&#24212;&#29992;&#20110;&#20840;&#23616; Q &#20989;&#25968;&#65292;&#20197;&#20943;&#36731;&#20449;&#24687;&#20849;&#20139;&#19981;&#23436;&#22791;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#31934;&#30830;&#31574;&#30053;&#35780;&#20272;&#19979;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#65292;&#36825;&#20123;&#20445;&#35777;&#20960;&#20046;&#26159;&#29420;&#31435;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated reinforcement learning (RL) enables collaborative decision making of multiple distributed agents without sharing local data trajectories. In this work, we consider a multi-task setting, in which each agent has its own private reward function corresponding to different tasks, while sharing the same transition kernel of the environment. Focusing on infinite-horizon tabular Markov decision processes, the goal is to learn a globally optimal policy that maximizes the sum of the discounted total rewards of all the agents in a decentralized manner, where each agent only communicates with its neighbors over some prescribed graph topology. We develop federated vanilla and entropy-regularized natural policy gradient (NPG) methods under softmax parameterization, where gradient tracking is applied to the global Q-function to mitigate the impact of imperfect information sharing. We establish non-asymptotic global convergence guarantees under exact policy evaluation, which are nearly indep
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#26426;&#22120;&#20154;&#32452;&#35013;&#35268;&#21010;&#30340;&#31639;&#27861;&#22534;&#26632;&#65292;&#21487;&#20197;&#22312;&#30701;&#26102;&#38388;&#20869;&#21512;&#25104;&#22797;&#26434;&#35013;&#37197;&#30340;&#24314;&#36896;&#35745;&#21010;&#65292;&#24182;&#35299;&#20915;&#20102;&#31227;&#21160;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#21046;&#36896;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.00192</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#26426;&#22120;&#20154;&#32452;&#35013;&#35268;&#21010;&#29992;&#20110;&#33258;&#20027;&#21046;&#36896;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Multi-Robot Assembly Planning for Autonomous Manufacturing. (arXiv:2311.00192v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#26426;&#22120;&#20154;&#32452;&#35013;&#35268;&#21010;&#30340;&#31639;&#27861;&#22534;&#26632;&#65292;&#21487;&#20197;&#22312;&#30701;&#26102;&#38388;&#20869;&#21512;&#25104;&#22797;&#26434;&#35013;&#37197;&#30340;&#24314;&#36896;&#35745;&#21010;&#65292;&#24182;&#35299;&#20915;&#20102;&#31227;&#21160;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#21046;&#36896;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#33258;&#20027;&#26426;&#22120;&#20154;&#26377;&#28508;&#21147;&#38761;&#26032;&#21046;&#36896;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#23558;&#22823;&#22411;&#26426;&#22120;&#20154;&#32676;&#20307;&#24212;&#29992;&#20110;&#21046;&#36896;&#39046;&#22495;&#38656;&#35201;&#35299;&#20915;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#20849;&#20139;&#24037;&#20316;&#31354;&#38388;&#20013;&#23454;&#29616;&#26080;&#30896;&#25758;&#31227;&#21160;&#65292;&#26377;&#25928;&#30340;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#26469;&#25805;&#32437;&#21644;&#36816;&#36755;&#22823;&#22411;&#36127;&#36733;&#65292;&#30001;&#20110;&#32806;&#21512;&#30340;&#21046;&#36896;&#27969;&#31243;&#23548;&#33268;&#22797;&#26434;&#30340;&#20219;&#21153;&#20998;&#37197;&#65292;&#20197;&#21450;&#23884;&#22871;&#23376;&#35013;&#37197;&#20214;&#30340;&#24182;&#34892;&#35013;&#37197;&#21644;&#36816;&#36755;&#30340;&#31354;&#38388;&#35268;&#21010;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#31639;&#27861;&#22534;&#26632;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#22810;&#26426;&#22120;&#20154;&#32452;&#35013;&#35268;&#21010;&#65292;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#21512;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#37096;&#20214;&#30340;&#22797;&#26434;&#35013;&#37197;&#30340;&#24314;&#36896;&#35745;&#21010;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25509;&#21463;&#31867;&#20284;CAD&#30340;&#20135;&#21697;&#35268;&#33539;&#65292;&#24182;&#33258;&#21160;&#20026;&#19968;&#32452;&#26426;&#22120;&#20154;&#35268;&#21010;&#20840;&#26632;&#35013;&#37197;&#36807;&#31243;&#26469;&#21046;&#36896;&#20135;&#21697;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#22534;&#26632;&#65292;&#21253;&#25324;&#65306;(i)&#36845;&#20195;&#24452;&#21521;&#24067;&#23616;&#20248;&#21270;&#36807;&#31243;&#65292;&#23450;&#20041;&#21046;&#36896;&#36807;&#31243;&#30340;&#20840;&#23616;&#35843;&#24230;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile autonomous robots have the potential to revolutionize manufacturing processes. However, employing large robot fleets in manufacturing requires addressing challenges including collision-free movement in a shared workspace, effective multi-robot collaboration to manipulate and transport large payloads, complex task allocation due to coupled manufacturing processes, and spatial planning for parallel assembly and transportation of nested subassemblies. We propose a full algorithmic stack for large-scale multi-robot assembly planning that addresses these challenges and can synthesize construction plans for complex assemblies with thousands of parts in a matter of minutes. Our approach takes in a CAD-like product specification and automatically plans a full-stack assembly procedure for a group of robots to manufacture the product. We propose an algorithmic stack that comprises: (i) an iterative radial layout optimization procedure to define a global staging layout for the manufacturin
&lt;/p&gt;</description></item><item><title>XAI-CLASS&#26159;&#19968;&#31181;&#35299;&#37322;&#22686;&#24378;&#30340;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32467;&#21512;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#30340;&#35299;&#37322;&#21644;&#21333;&#35789;&#30340;&#26174;&#33879;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#26368;&#23567;&#25110;&#27809;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2311.00189</link><description>&lt;p&gt;
XAI-CLASS&#65306;&#20855;&#26377;&#26497;&#24369;&#30417;&#30563;&#30340;&#35299;&#37322;&#22686;&#24378;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak Supervision. (arXiv:2311.00189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00189
&lt;/p&gt;
&lt;p&gt;
XAI-CLASS&#26159;&#19968;&#31181;&#35299;&#37322;&#22686;&#24378;&#30340;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32467;&#21512;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#30340;&#35299;&#37322;&#21644;&#21333;&#35789;&#30340;&#26174;&#33879;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#26368;&#23567;&#25110;&#27809;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#25991;&#26723;&#26377;&#25928;&#22320;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#12290;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#24471;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24369;&#30417;&#30563;&#21644;&#26497;&#24369;&#30417;&#30563;&#29615;&#22659;&#20013;&#65292;&#20998;&#21035;&#38656;&#35201;&#26368;&#23569;&#25110;&#27809;&#26377;&#20154;&#24037;&#27880;&#37322;&#12290;&#22312;&#20808;&#21069;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#23558;&#20266;&#26631;&#31614;&#20998;&#37197;&#32473;&#19982;&#29305;&#23450;&#31867;&#21035;&#23545;&#40784;&#65288;&#20363;&#22914;&#65292;&#20851;&#38190;&#35789;&#21305;&#37197;&#65289;&#30340;&#25991;&#26723;&#26469;&#29983;&#25104;&#20266;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#22312;&#25991;&#26412;&#20998;&#31867;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#30340;&#35299;&#37322;&#25110;&#20010;&#20307;&#21333;&#35789;&#30340;&#26174;&#33879;&#24615;&#20316;&#20026;&#39069;&#22806;&#25351;&#23548;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;XAI-CLASS&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#22686;&#24378;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification aims to effectively categorize documents into pre-defined categories. Traditional methods for text classification often rely on large amounts of manually annotated training data, making the process time-consuming and labor-intensive. To address this issue, recent studies have focused on weakly-supervised and extremely weakly-supervised settings, which require minimal or no human annotation, respectively. In previous methods of weakly supervised text classification, pseudo-training data is generated by assigning pseudo-labels to documents based on their alignment (e.g., keyword matching) with specific classes. However, these methods ignore the importance of incorporating the explanations of the generated pseudo-labels, or saliency of individual words, as additional guidance during the text classification training process. To address this limitation, we propose XAI-CLASS, a novel explanation-enhanced extremely weakly-supervised text classification method that incorpor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25239;&#24615;&#25552;&#31034;&#23631;&#34109;&#65288;APS&#65289;&#27169;&#22411;&#20197;&#21450;&#33258;&#21160;&#29983;&#25104;&#23545;&#25239;&#24615;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26032;&#31574;&#30053;&#12290;APS&#27169;&#22411;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#19988;&#20855;&#26377;&#38887;&#24615;&#65292;&#24182;&#19988;BAND&#25968;&#25454;&#38598;&#21487;&#20197;&#22686;&#24378;&#23433;&#20840;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00172</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#23433;&#20840;&#20998;&#31867;&#22120;&#65306;&#23545;&#25239;&#24615;&#25552;&#31034;&#23631;&#34109;
&lt;/p&gt;
&lt;p&gt;
Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield. (arXiv:2311.00172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25239;&#24615;&#25552;&#31034;&#23631;&#34109;&#65288;APS&#65289;&#27169;&#22411;&#20197;&#21450;&#33258;&#21160;&#29983;&#25104;&#23545;&#25239;&#24615;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26032;&#31574;&#30053;&#12290;APS&#27169;&#22411;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#19988;&#20855;&#26377;&#38887;&#24615;&#65292;&#24182;&#19988;BAND&#25968;&#25454;&#38598;&#21487;&#20197;&#22686;&#24378;&#23433;&#20840;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#31995;&#32479;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#22312;&#36825;&#20123;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#23433;&#20840;&#20998;&#31867;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#34987;&#35757;&#32451;&#26469;&#36776;&#21035;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#26377;&#23475;&#12289;&#20882;&#29359;&#25110;&#19981;&#36947;&#24503;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#28508;&#21147;&#24040;&#22823;&#65292;&#29616;&#20195;&#30340;&#23433;&#20840;&#20998;&#31867;&#22120;&#36890;&#24120;&#22312;&#26292;&#38706;&#20110;&#20805;&#28385;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#36755;&#20837;&#26102;&#22833;&#36133;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#23545;&#25239;&#24615;&#25552;&#31034;&#23631;&#34109;&#65288;APS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#38887;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#29983;&#25104;&#23545;&#25239;&#24615;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26032;&#31574;&#30053;&#65292;&#31216;&#20026;Bot Adversarial Noisy Dialogue (BAND) &#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#26088;&#22312;&#22686;&#24378;&#23433;&#20840;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#23558;&#23545;&#25239;&#24615;&#26679;&#26412;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#30340;&#21518;&#26524;&#12290;&#36890;&#36807;&#35780;&#20272;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24515;&#29702;&#23398;&#21644;&#21746;&#23398;&#25991;&#29486;&#25552;&#20986;&#20845;&#31181;&#25361;&#25112;&#24694;&#24847;&#35821;&#35328;&#21051;&#26495;&#21360;&#35937;&#30340;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#32534;&#20889;&#30340;&#23545;&#35805;&#20351;&#29992;&#26356;&#20855;&#20307;&#30340;&#31574;&#30053;&#65292;&#32780;&#26426;&#22120;&#29983;&#25104;&#30340;&#23545;&#35805;&#20351;&#29992;&#36739;&#20026;&#26222;&#36941;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2311.00161</link><description>&lt;p&gt;
&#36229;&#36234;&#25209;&#21028;&#24615;&#20167;&#24680;&#65306;&#35299;&#20915;&#35821;&#35328;&#20013;&#38544;&#21547;&#30340;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language. (arXiv:2311.00161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24515;&#29702;&#23398;&#21644;&#21746;&#23398;&#25991;&#29486;&#25552;&#20986;&#20845;&#31181;&#25361;&#25112;&#24694;&#24847;&#35821;&#35328;&#21051;&#26495;&#21360;&#35937;&#30340;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#32534;&#20889;&#30340;&#23545;&#35805;&#20351;&#29992;&#26356;&#20855;&#20307;&#30340;&#31574;&#30053;&#65292;&#32780;&#26426;&#22120;&#29983;&#25104;&#30340;&#23545;&#35805;&#20351;&#29992;&#36739;&#20026;&#26222;&#36941;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#23545;&#24576;&#26377;&#24694;&#24847;&#30340;&#35328;&#35770;&#30340;&#23545;&#35805;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#35299;&#20915;&#26041;&#24335;&#65292;&#20197;&#36991;&#20813;&#23545;&#32447;&#19978;&#24694;&#24847;&#35328;&#35770;&#36827;&#34892;&#23457;&#26597;&#12290;&#28982;&#32780;&#65292;&#36866;&#24403;&#22320;&#23545;&#20184;&#24694;&#24847;&#35821;&#35328;&#38656;&#35201;&#25171;&#30772;&#21644;&#39539;&#26021;&#36825;&#20123;&#35821;&#35328;&#25152;&#26263;&#31034;&#30340;&#19981;&#20934;&#30830;&#21051;&#26495;&#21360;&#35937;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#24515;&#29702;&#23398;&#21644;&#21746;&#23398;&#25991;&#29486;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#20845;&#31181;&#24515;&#29702;&#23398;&#19978;&#21551;&#21457;&#30340;&#31574;&#30053;&#26469;&#25361;&#25112;&#24694;&#24847;&#35821;&#35328;&#25152;&#26263;&#31034;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#26469;&#30740;&#31350;&#27599;&#31181;&#31574;&#30053;&#30340;&#35828;&#26381;&#21147;&#65292;&#28982;&#21518;&#27604;&#36739;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#36825;&#20123;&#31574;&#30053;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#32534;&#20889;&#30340;&#23545;&#35805;&#20351;&#29992;&#20102;&#26356;&#19982;&#25152;&#26263;&#31034;&#30340;&#21051;&#26495;&#21360;&#35937;&#30456;&#20851;&#30340;&#25361;&#25112;&#31574;&#30053;&#65288;&#20363;&#22914;&#65292;&#21051;&#26495;&#21360;&#35937;&#30340;&#21453;&#20363;&#65292;&#20851;&#20110;&#21051;&#26495;&#21360;&#35937;&#26469;&#28304;&#30340;&#22806;&#37096;&#22240;&#32032;&#65289;&#65292;&#32780;&#26426;&#22120;&#29983;&#25104;&#30340;&#23545;&#35805;&#20351;&#29992;&#20102;&#26356;&#19981;&#20855;&#20307;&#30340;&#31574;&#30053;&#65288;&#20363;&#22914;&#65292;&#26222;&#36941;&#25256;&#20987;&#21051;&#26495;&#21360;&#35937;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterspeech, i.e., responses to counteract potential harms of hateful speech, has become an increasingly popular solution to address online hate speech without censorship. However, properly countering hateful language requires countering and dispelling the underlying inaccurate stereotypes implied by such language. In this work, we draw from psychology and philosophy literature to craft six psychologically inspired strategies to challenge the underlying stereotypical implications of hateful language. We first examine the convincingness of each of these strategies through a user study, and then compare their usages in both human- and machine-generated counterspeech datasets. Our results show that human-written counterspeech uses countering strategies that are more specific to the implied stereotype (e.g., counter examples to the stereotype, external factors about the stereotype's origins), whereas machine-generated counterspeech uses less specific strategies (e.g., generally denouncin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24471;&#20998;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#20943;&#23567;&#31215;&#20998;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2311.00157</link><description>&lt;p&gt;
&#19968;&#20010;&#26356;&#24555;&#30340;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#30340;&#24471;&#20998;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score Normalization for a Faster Diffusion Exponential Integrator Sampler. (arXiv:2311.00157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00157
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24471;&#20998;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#20943;&#23567;&#31215;&#20998;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24352;&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#24555;&#36895;&#29983;&#25104;&#26679;&#26412;&#30340;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#65288;DEIS&#65289;&#12290;&#23427;&#21033;&#29992;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#21322;&#32447;&#24615;&#29305;&#24615;&#26469;&#22823;&#22823;&#20943;&#23567;&#31215;&#20998;&#35823;&#24046;&#65292;&#24182;&#22312;&#20302;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#65288;NFEs&#65289;&#26102;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#24471;&#20998;&#20989;&#25968;&#37325;&#21442;&#25968;&#21270;&#65292;&#23427;&#36890;&#36807;&#20943;&#23569;&#22312;&#27599;&#20010;&#31215;&#20998;&#27493;&#39588;&#20013;&#20351;&#29992;&#22266;&#23450;&#24471;&#20998;&#20989;&#25968;&#20272;&#35745;&#32780;&#24341;&#36215;&#30340;&#31215;&#20998;&#35823;&#24046;&#12290;&#21407;&#22987;&#20316;&#32773;&#20351;&#29992;&#20102;&#29992;&#20110;&#22122;&#22768;&#39044;&#27979;&#35757;&#32451;&#30340;&#27169;&#22411;&#40664;&#35748;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21363;&#23558;&#24471;&#20998;&#20056;&#20197;&#26465;&#20214;&#27491;&#21521;&#22122;&#22768;&#20998;&#24067;&#30340;&#26631;&#20934;&#24046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#36825;&#31181;&#24471;&#20998;&#21442;&#25968;&#21270;&#30340;&#32477;&#23545;&#24179;&#22343;&#20540;&#22312;&#22823;&#37096;&#20998;&#21453;&#21521;&#37319;&#26679;&#36807;&#31243;&#20013;&#25509;&#36817;&#24120;&#25968;&#65292;&#20294;&#22312;&#37319;&#26679;&#32467;&#26463;&#26102;&#23427;&#20250;&#36805;&#36895;&#21464;&#21270;&#12290;&#20026;&#20102;&#31616;&#21333;&#20462;&#22797;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;&#24471;&#20998;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65288;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Recently, zhang et al have proposed the Diffusion Exponential Integrator Sampler (DEIS) for fast generation of samples from Diffusion Models. It leverages the semi-linear nature of the probability flow ordinary differential equation (ODE) in order to greatly reduce integration error and improve generation quality at low numbers of function evaluations (NFEs). Key to this approach is the score function reparameterisation, which reduces the integration error incurred from using a fixed score function estimate over each integration step. The original authors use the default parameterisation used by models trained for noise prediction -- multiply the score by the standard deviation of the conditional forward noising distribution. We find that although the mean absolute value of this score parameterisation is close to constant for a large portion of the reverse sampling process, it changes rapidly at the end of sampling. As a simple fix, we propose to instead reparameterise the score (at in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25151;&#38388;&#20914;&#28608;&#21709;&#24212;&#65288;RIR&#65289;&#22522;&#20110;&#30340;&#31354;&#38388;&#29305;&#24449;RIR-SF&#65292;&#36890;&#36807;&#19982;&#24050;&#26377;&#30340;3D&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#65292;&#34920;&#26126;RIR-SF&#22312;&#22810;&#36890;&#36947;&#22810;&#35828;&#35805;&#20154;ASR&#31995;&#32479;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#30456;&#23545;&#38477;&#20302;&#20102;21.3&#65285;&#30340;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;CER&#65289;&#65292;&#24182;&#19988;&#23545;&#24378;&#28151;&#21709;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00146</link><description>&lt;p&gt;
RIR-SF: &#22522;&#20110;&#25151;&#38388;&#20914;&#28608;&#21709;&#24212;&#30340;&#22810;&#36890;&#36947;&#22810;&#35828;&#35805;&#20154;ASR&#30340;&#31354;&#38388;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
RIR-SF: Room Impulse Response Based Spatial Feature for Multi-channel Multi-talker ASR. (arXiv:2311.00146v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25151;&#38388;&#20914;&#28608;&#21709;&#24212;&#65288;RIR&#65289;&#22522;&#20110;&#30340;&#31354;&#38388;&#29305;&#24449;RIR-SF&#65292;&#36890;&#36807;&#19982;&#24050;&#26377;&#30340;3D&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#65292;&#34920;&#26126;RIR-SF&#22312;&#22810;&#36890;&#36947;&#22810;&#35828;&#35805;&#20154;ASR&#31995;&#32479;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#30456;&#23545;&#38477;&#20302;&#20102;21.3&#65285;&#30340;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;CER&#65289;&#65292;&#24182;&#19988;&#23545;&#24378;&#28151;&#21709;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36890;&#36947;&#22810;&#35828;&#35805;&#20154;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#22312;&#35821;&#38899;&#39046;&#22495;&#20013;&#38754;&#20020;&#25345;&#32493;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#26174;&#33879;&#30340;&#28151;&#21709;&#25928;&#26524;&#26102;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#37325;&#21472;&#30340;&#35821;&#38899;&#20449;&#21495;&#19982;&#30446;&#26631;&#35828;&#35805;&#20154;&#20256;&#36755;&#21040;&#40614;&#20811;&#39118;&#38453;&#21015;&#30340;&#25151;&#38388;&#20914;&#28608;&#21709;&#24212;&#65288;RIR&#65289;&#36827;&#34892;&#21367;&#31215;&#12290;&#36825;&#31181;&#21019;&#26032;&#25216;&#26415;&#20135;&#29983;&#20102;&#19968;&#31181;&#21517;&#20026;RIR-SF&#30340;&#26032;&#22411;&#31354;&#38388;&#29305;&#24449;&#12290;&#36890;&#36807;&#19982;&#20808;&#21069;&#24314;&#31435;&#30340;3D&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#37117;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;RIR-SF&#30340;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RIR-SF&#22312;&#22810;&#36890;&#36947;&#22810;&#35828;&#35805;&#20154;ASR&#31995;&#32479;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#23548;&#33268;&#20102;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;CER&#65289;&#30340;&#26174;&#33879;&#30456;&#23545;&#38477;&#20302;21.3&#65285;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#23545;&#24378;&#28151;&#21709;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-channel multi-talker automatic speech recognition (ASR) presents ongoing challenges within the speech community, particularly when confronted with significant reverberation effects. In this study, we introduce a novel approach involving the convolution of overlapping speech signals with the room impulse response (RIR) corresponding to the target speaker's transmission to a microphone array. This innovative technique yields a novel spatial feature known as the RIR-SF. Through a comprehensive comparison with the previously established state-of-the-art 3D spatial feature, both theoretical analysis and experimental results substantiate the superiority of our proposed RIR-SF. We demonstrate that the RIR-SF outperforms existing methods, leading to a remarkable 21.3\% relative reduction in the Character Error Rate (CER) in multi-channel multi-talker ASR systems. Importantly, this novel feature exhibits robustness in the face of strong reverberation, surpassing the limitations of previou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#31454;&#36873;&#27963;&#21160;&#30340;&#36127;&#38754;&#24773;&#32490;&#65292;&#21253;&#25324;&#19968;&#20010;&#20004;&#38454;&#27573;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20102;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#23545;&#20234;&#26391;2021&#24180;&#24635;&#32479;&#36873;&#20030;&#26399;&#38388;50&#21517;&#25919;&#27835;&#29992;&#25143;&#30340;5,100&#26465;&#27874;&#26031;&#35821;&#25512;&#25991;&#36827;&#34892;&#26631;&#27880;&#65292;&#24314;&#31435;&#20102;&#25152;&#38656;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2311.00143</link><description>&lt;p&gt;
&#20234;&#26391;2021&#24180;&#24635;&#32479;&#36873;&#20030;&#26399;&#38388;&#65292;&#20351;&#29992;&#36724;&#23884;&#20837;&#30340;&#20004;&#38454;&#27573;&#20998;&#31867;&#22120;&#36827;&#34892;&#25919;&#27835;&#29992;&#25143;&#25512;&#25991;&#20013;&#30340;&#36127;&#38754;&#24773;&#32490;&#26816;&#27979;&#65306;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Two-Stage Classifier for Campaign Negativity Detection using Axis Embeddings: A Case Study on Tweets of Political Users during 2021 Presidential Election in Iran. (arXiv:2311.00143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#31454;&#36873;&#27963;&#21160;&#30340;&#36127;&#38754;&#24773;&#32490;&#65292;&#21253;&#25324;&#19968;&#20010;&#20004;&#38454;&#27573;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20102;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#23545;&#20234;&#26391;2021&#24180;&#24635;&#32479;&#36873;&#20030;&#26399;&#38388;50&#21517;&#25919;&#27835;&#29992;&#25143;&#30340;5,100&#26465;&#27874;&#26031;&#35821;&#25512;&#25991;&#36827;&#34892;&#26631;&#27880;&#65292;&#24314;&#31435;&#20102;&#25152;&#38656;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#21508;&#22320;&#30340;&#36873;&#20030;&#20013;&#65292;&#20505;&#36873;&#20154;&#21487;&#33021;&#20250;&#22240;&#22833;&#36133;&#21069;&#26223;&#21644;&#26102;&#38388;&#21387;&#21147;&#32780;&#23558;&#20182;&#20204;&#30340;&#31454;&#36873;&#27963;&#21160;&#36716;&#21521;&#36127;&#38754;&#24773;&#32490;&#12290;&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;Twitter&#31561;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#25919;&#27835;&#35805;&#35821;&#30340;&#20016;&#23500;&#26469;&#28304;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;Twitter&#19978;&#21457;&#24067;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#33258;&#21160;&#21270;&#30340;&#31454;&#36873;&#36127;&#38754;&#24773;&#32490;&#26816;&#27979;&#31995;&#32479;&#22312;&#29702;&#35299;&#20505;&#36873;&#20154;&#21644;&#25919;&#20826;&#22312;&#31454;&#36873;&#27963;&#21160;&#20013;&#30340;&#31574;&#30053;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#31454;&#36873;&#27963;&#21160;&#30340;&#36127;&#38754;&#24773;&#32490;&#65292;&#21253;&#25324;&#19968;&#20010;&#20004;&#38454;&#27573;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20102;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;50&#21517;&#25919;&#27835;&#29992;&#25143;&#65288;&#21253;&#25324;&#20505;&#36873;&#20154;&#21644;&#25919;&#24220;&#23448;&#21592;&#65289;&#30340;&#27874;&#26031;&#35821;&#25512;&#25991;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26631;&#27880;&#20102;&#20854;&#20013;5,100&#26465;&#25512;&#25991;&#65292;&#36825;&#20123;&#25512;&#25991;&#26159;&#22312;&#20234;&#26391;2021&#24180;&#24635;&#32479;&#36873;&#20030;&#21069;&#30340;&#19968;&#24180;&#20869;&#21457;&#24067;&#30340;&#12290;&#22312;&#25552;&#20986;&#30340;&#27169;&#22411;&#20013;&#65292;&#39318;&#20808;&#36890;&#36807;&#25512;&#25991;&#23884;&#20837;&#19982;&#36724;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#26469;&#24314;&#31435;&#20004;&#20010;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In elections around the world, the candidates may turn their campaigns toward negativity due to the prospect of failure and time pressure. In the digital age, social media platforms such as Twitter are rich sources of political discourse. Therefore, despite the large amount of data that is published on Twitter, the automatic system for campaign negativity detection can play an essential role in understanding the strategy of candidates and parties in their campaigns. In this paper, we propose a hybrid model for detecting campaign negativity consisting of a two-stage classifier that combines the strengths of two machine learning models. Here, we have collected Persian tweets from 50 political users, including candidates and government officials. Then we annotated 5,100 of them that were published during the year before the 2021 presidential election in Iran. In the proposed model, first, the required datasets of two classifiers based on the cosine similarity of tweet embeddings with axis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20110;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#19979;&#30340;&#38543;&#26426;&#36845;&#20195;&#65288;&#29305;&#21035;&#26159;Q-learning&#36845;&#20195;&#65289;&#36827;&#34892;&#25910;&#25947;&#30340;&#23450;&#29702;&#65292;&#24182;&#32473;&#20986;&#20102;&#25910;&#25947;&#26465;&#20214;&#12290;&#20854;&#27425;&#65292;&#35752;&#35770;&#20102;&#35813;&#23450;&#29702;&#22312;&#22810;&#31181;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.00123</link><description>&lt;p&gt;
Q-Learning&#29992;&#20110;&#36890;&#29992;&#20449;&#24687;&#32467;&#26500;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#19979;&#30340;&#38543;&#26426;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Q-Learning for Stochastic Control under General Information Structures and Non-Markovian Environments. (arXiv:2311.00123v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00123
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20110;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#19979;&#30340;&#38543;&#26426;&#36845;&#20195;&#65288;&#29305;&#21035;&#26159;Q-learning&#36845;&#20195;&#65289;&#36827;&#34892;&#25910;&#25947;&#30340;&#23450;&#29702;&#65292;&#24182;&#32473;&#20986;&#20102;&#25910;&#25947;&#26465;&#20214;&#12290;&#20854;&#27425;&#65292;&#35752;&#35770;&#20102;&#35813;&#23450;&#29702;&#22312;&#22810;&#31181;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25910;&#25947;&#23450;&#29702;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19968;&#33324;&#30340;&#12289;&#21487;&#33021;&#20026;&#38750;&#39532;&#23572;&#21487;&#22827;&#30340;&#38543;&#26426;&#29615;&#22659;&#19979;&#30340;Q-&#23398;&#20064;&#36845;&#20195;&#12290;&#25105;&#20204;&#30340;&#25910;&#25947;&#26465;&#20214;&#28041;&#21450;&#21040;&#19968;&#20010;&#36941;&#21382;&#24615;&#21644;&#19968;&#20010;&#27491;&#24615;&#20934;&#21017;&#12290;&#25105;&#20204;&#23545;&#36845;&#20195;&#30340;&#26497;&#38480;&#21644;&#25910;&#25947;&#30340;&#29615;&#22659;&#21644;&#21021;&#22987;&#21270;&#26465;&#20214;&#36827;&#34892;&#20102;&#31934;&#30830;&#30340;&#25551;&#36848;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#23450;&#29702;&#23545;&#20110;&#22810;&#31181;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#30340;&#24433;&#21709;&#21644;&#24212;&#29992;&#65292;&#20854;&#20013;&#21253;&#25324;(i)&#36830;&#32493;&#31354;&#38388;&#30340;&#23436;&#20840;&#35266;&#27979;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#37327;&#21270;&#36817;&#20284;&#65288;&#37327;&#21270;&#30772;&#22351;&#20102;&#39532;&#23572;&#21487;&#22827;&#32467;&#26500;&#65289;&#65292;(ii)&#37327;&#21270;&#36817;&#20284;&#30340;&#32622;&#20449;MDP&#32422;&#21270;&#37096;&#20998;&#21487;&#35266;&#23519;MDPS&#65288;POMDPs&#65289; with &#24369;Feller&#36830;&#32493;&#24615;&#21644;&#28388;&#27874;&#22120;&#31283;&#23450;&#30340;&#36731;&#24494;&#29256;&#26412;&#65288;&#25511;&#21046;&#22120;&#38656;&#35201;&#20102;&#35299;&#27169;&#22411;&#65289;&#65292;(iii)&#26377;&#38480;&#31383;&#21475;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a primary contribution, we present a convergence theorem for stochastic iterations, and in particular, Q-learning iterates, under a general, possibly non-Markovian, stochastic environment. Our conditions for convergence involve an ergodicity and a positivity criterion. We provide a precise characterization on the limit of the iterates and conditions on the environment and initializations for convergence. As our second contribution, we discuss the implications and applications of this theorem to a variety of stochastic control problems with non-Markovian environments involving (i) quantized approximations of fully observed Markov Decision Processes (MDPs) with continuous spaces (where quantization break down the Markovian structure), (ii) quantized approximations of belief-MDP reduced partially observable MDPS (POMDPs) with weak Feller continuity and a mild version of filter stability (which requires the knowledge of the model by the controller), (iii) finite window approximations of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Bandit&#31639;&#27861;&#30340;&#25209;&#27425;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22312;&#26631;&#31614;&#22122;&#38899;&#19979;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#27700;&#24179;&#30340;&#26631;&#31614;&#27745;&#26579;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#19988;&#26080;&#38656;&#20351;&#29992;&#36741;&#21161;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2311.00096</link><description>&lt;p&gt;
&#20026;&#20102;&#22312;&#26631;&#31614;&#22122;&#38899;&#19979;&#36827;&#34892;&#31283;&#20581;&#23398;&#20064;&#65292;&#22522;&#20110;Bandit&#39537;&#21160;&#30340;&#25209;&#27425;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bandit-Driven Batch Selection for Robust Learning under Label Noise. (arXiv:2311.00096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Bandit&#31639;&#27861;&#30340;&#25209;&#27425;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22312;&#26631;&#31614;&#22122;&#38899;&#19979;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#27700;&#24179;&#30340;&#26631;&#31614;&#27745;&#26579;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#19988;&#26080;&#38656;&#20351;&#29992;&#36741;&#21161;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32452;&#21512;&#36172;&#21338;&#31639;&#27861;&#26469;&#36873;&#25321;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#35757;&#32451;&#20013;&#30340;&#25209;&#27425;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#26631;&#31614;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#27700;&#24179;&#30340;&#26631;&#31614;&#27745;&#26579;&#19979;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#24102;&#26469;&#24120;&#35265;&#30340;&#36741;&#21161;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36825;&#31181;&#20248;&#36234;&#24615;&#33021;&#12290;&#35813;&#24037;&#20316;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#27169;&#22411;&#25928;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#25240;&#34935;&#65292;&#20026;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel approach for batch selection in Stochastic Gradient Descent (SGD) training, leveraging combinatorial bandit algorithms. Our methodology focuses on optimizing the learning process in the presence of label noise, a prevalent issue in real-world datasets. Experimental evaluations on the CIFAR-10 dataset reveal that our approach consistently outperforms existing methods across various levels of label corruption. Importantly, we achieve this superior performance without incurring the computational overhead commonly associated with auxiliary neural network models. This work presents a balanced trade-off between computational efficiency and model efficacy, offering a scalable solution for complex machine learning applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#38500;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#22788;&#29702;&#24615;&#20063;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#30340;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00094</link><description>&lt;p&gt;
&#34920;&#36798;&#24314;&#27169;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19981;&#36275;&#65306;&#21487;&#22788;&#29702;&#30340;&#25512;&#29702;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Expressive Modeling Is Insufficient for Offline RL: A Tractable Inference Perspective. (arXiv:2311.00094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#38500;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#22788;&#29702;&#24615;&#20063;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#30340;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#19968;&#31181;&#27969;&#34892;&#30340;&#33539;&#20363;&#26159;&#20808;&#23558;&#31163;&#32447;&#36712;&#36857;&#25311;&#21512;&#21040;&#19968;&#20010;&#24207;&#21015;&#27169;&#22411;&#20013;&#65292;&#28982;&#21518;&#36890;&#36807;&#35813;&#27169;&#22411;&#25552;&#31034;&#39640;&#26399;&#26395;&#22238;&#25253;&#30340;&#21160;&#20316;&#12290;&#34429;&#28982;&#26222;&#36941;&#35748;&#20026;&#34920;&#36798;&#24615;&#26356;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#26412;&#25991;&#24378;&#35843;&#20102;&#21487;&#22788;&#29702;&#24615;&#65292;&#21363;&#31934;&#30830;&#32780;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#21516;&#26679;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#24102;&#26469;&#30340;&#22522;&#26412;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#36827;&#34892;&#39640;&#24230;&#38750;&#24179;&#20961;&#30340;&#26465;&#20214;/&#32422;&#26463;&#29983;&#25104;&#65292;&#20197;&#24341;&#20986;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#34429;&#28982;&#20173;&#28982;&#21487;&#20197;&#36817;&#20284;&#22788;&#29702;&#36825;&#20123;&#26597;&#35810;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#31895;&#31961;&#30340;&#20272;&#35745;&#26174;&#33879;&#21066;&#24369;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#65288;TPM&#65289;&#26469;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
A popular paradigm for offline Reinforcement Learning (RL) tasks is to first fit the offline trajectories to a sequence model, and then prompt the model for actions that lead to high expected return. While a common consensus is that more expressive sequence models imply better performance, this paper highlights that tractability, the ability to exactly and efficiently answer various probabilistic queries, plays an equally important role. Specifically, due to the fundamental stochasticity from the offline data-collection policies and the environment dynamics, highly non-trivial conditional/constrained generation is required to elicit rewarding actions. While it is still possible to approximate such queries, we observe that such crude estimates significantly undermine the benefits brought by expressive sequence models. To overcome this problem, this paper proposes Trifle (Tractable Inference for Offline RL), which leverages modern Tractable Probabilistic Models (TPMs) to bridge the gap b
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36807;&#28388;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#23433;&#20840;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#32422;&#26463;&#25511;&#21046;&#25216;&#26415;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#23433;&#20840;&#24615;&#12289;&#23454;&#26102;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00063</link><description>&lt;p&gt;
&#20351;&#29992;&#36807;&#28388;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#23433;&#20840;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#35268;&#21010;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Safe multi-agent motion planning under uncertainty for drones using filtered reinforcement learning. (arXiv:2311.00063v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00063
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36807;&#28388;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#23433;&#20840;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#32422;&#26463;&#25511;&#21046;&#25216;&#26415;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#23433;&#20840;&#24615;&#12289;&#23454;&#26102;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#19981;&#30830;&#23450;&#12289;&#28151;&#20081;&#30340;&#24037;&#20316;&#31354;&#38388;&#20013;&#65292;&#38024;&#23545;&#26080;&#20154;&#26426;&#30340;&#23433;&#20840;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#65292;&#23427;&#32467;&#21512;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32422;&#26463;&#25511;&#21046;&#30340;&#36712;&#36857;&#35268;&#21010;&#30340;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#21333;&#26234;&#33021;&#20307;&#30340;&#24378;&#21270;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#36798;&#30446;&#26631;&#30340;&#36816;&#21160;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#26041;&#26696;&#21487;&#33021;&#19981;&#26159;&#26080;&#30896;&#25758;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#20984;&#20248;&#21270;&#12289;&#27010;&#29575;&#32422;&#26463;&#21644;&#38598;&#21512;&#26041;&#27861;&#26469;&#36827;&#34892;&#32422;&#26463;&#25511;&#21046;&#65292;&#20197;&#30830;&#20445;&#23613;&#31649;&#24037;&#20316;&#31354;&#38388;&#12289;&#26234;&#33021;&#20307;&#36816;&#21160;&#21644;&#24863;&#30693;&#19981;&#30830;&#23450;&#65292;&#20173;&#33021;&#20445;&#25345;&#23433;&#20840;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#21644;&#25511;&#21046;&#32422;&#26463;&#65292;&#24182;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#23454;&#29616;&#26234;&#33021;&#20307;&#20043;&#38388;&#20197;&#21450;&#19982;&#24037;&#20316;&#31354;&#38388;&#20013;&#30340;&#38745;&#24577;&#38556;&#30861;&#29289;&#30340;&#30896;&#25758;&#36991;&#20813;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;&#12289;&#23454;&#26102;&#21487;&#34892;&#30340;&#12289;&#27604;&#20165;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26356;&#31616;&#21333;&#35757;&#32451;&#30340;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#35268;&#21010;&#22120;&#12290;&#25968;&#20540;&#27169;&#25311;&#21644;&#23454;&#39564;&#26174;&#31034;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of safe multi-agent motion planning for drones in uncertain, cluttered workspaces. For this problem, we present a tractable motion planner that builds upon the strengths of reinforcement learning and constrained-control-based trajectory planning. First, we use single-agent reinforcement learning to learn motion plans from data that reach the target but may not be collision-free. Next, we use a convex optimization, chance constraints, and set-based methods for constrained control to ensure safety, despite the uncertainty in the workspace, agent motion, and sensing. The proposed approach can handle state and control constraints on the agents, and enforce collision avoidance among themselves and with static obstacles in the workspace with high probability. The proposed approach yields a safe, real-time implementable, multi-agent motion planner that is simpler to train than methods based solely on learning. Numerical simulations and experiments show the efficacy of 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#22411;AI&#30340;&#24726;&#35770;&#30740;&#31350;&#20102;&#29983;&#25104;&#22411;&#27169;&#22411;&#19982;&#20154;&#31867;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#27169;&#22411;&#22312;&#20135;&#29983;&#19987;&#23478;&#32423;&#36755;&#20986;&#30340;&#33021;&#21147;&#19978;&#21487;&#33021;&#36229;&#36807;&#20854;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00059</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;AI&#30340;&#24726;&#35770;&#65306;&#8220;&#23427;&#21487;&#20197;&#21019;&#24314;&#65292;&#20294;&#21487;&#33021;&#19981;&#29702;&#35299;&#8221;
&lt;/p&gt;
&lt;p&gt;
The Generative AI Paradox: "What It Can Create, It May Not Understand". (arXiv:2311.00059v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00059
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;AI&#30340;&#24726;&#35770;&#30740;&#31350;&#20102;&#29983;&#25104;&#22411;&#27169;&#22411;&#19982;&#20154;&#31867;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#27169;&#22411;&#22312;&#20135;&#29983;&#19987;&#23478;&#32423;&#36755;&#20986;&#30340;&#33021;&#21147;&#19978;&#21487;&#33021;&#36229;&#36807;&#20854;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29983;&#25104;&#22411;AI&#28010;&#28526;&#24341;&#36215;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20840;&#29699;&#20851;&#27880;&#65292;&#26082;&#26377;&#20852;&#22859;&#20063;&#26377;&#23545;&#20154;&#24037;&#26234;&#33021;&#28508;&#22312;&#36229;&#20154;&#27700;&#24179;&#30340;&#25285;&#24551;&#65306;&#29616;&#22312;&#30340;&#27169;&#22411;&#21482;&#38656;&#35201;&#20960;&#31186;&#38047;&#23601;&#33021;&#20135;&#29983;&#36229;&#36807;&#29978;&#33267;&#25361;&#25112;&#19987;&#23478;&#32423;&#20154;&#31867;&#33021;&#21147;&#30340;&#36755;&#20986;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#27169;&#22411;&#20173;&#28982;&#26174;&#31034;&#20986;&#21363;&#20351;&#38750;&#19987;&#23478;&#20063;&#19981;&#20250;&#39044;&#26399;&#20986;&#29616;&#30340;&#22522;&#26412;&#38169;&#35823;&#12290;&#36825;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#19968;&#20010;&#26126;&#26174;&#30340;&#24726;&#35770;&#65306;&#25105;&#20204;&#22914;&#20309;&#35299;&#20915;&#30475;&#20284;&#36229;&#20154;&#33021;&#21147;&#21644;&#23569;&#25968;&#20154;&#31867;&#25165;&#20250;&#29359;&#38169;&#35823;&#30340;&#25345;&#32493;&#23384;&#22312;&#20043;&#38388;&#30340;&#30683;&#30462;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#27979;&#35797;&#20102;&#29983;&#25104;&#22411;AI&#24726;&#35770;&#20551;&#35774;&#65306;&#29983;&#25104;&#22411;&#27169;&#22411;&#30001;&#20110;&#30452;&#25509;&#35757;&#32451;&#20197;&#20135;&#29983;&#31867;&#20284;&#19987;&#23478;&#30340;&#36755;&#20986;&#65292;&#32780;&#33719;&#24471;&#30340;&#29983;&#25104;&#33021;&#21147;&#26159;&#19981;&#21463;&#21046;&#20110;&#20854;&#29702;&#35299;&#33021;&#21147;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon -- and can therefore exceed -- their ability to unde
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35266;&#23519;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#31995;&#32479;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#21482;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#22312;&#25512;&#29702;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#25581;&#31034;&#20102;&#24213;&#23618;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00056</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#21644;&#25193;&#25955;&#65306;&#20851;&#20110;&#20855;&#26377;&#31283;&#23450;&#25193;&#25955;&#30340;&#21512;&#25104;&#22270;&#20687;&#20998;&#24067;&#30340;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Diversity and Diffusion: Observations on Synthetic Image Distributions with Stable Diffusion. (arXiv:2311.00056v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00056
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35266;&#23519;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#31995;&#32479;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#21482;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#22312;&#25512;&#29702;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#25581;&#31034;&#20102;&#24213;&#23618;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#22914;StableDiffusion&#12289;Imagen&#21644;DALL-E 2&#65292;&#20351;&#24471;&#36890;&#36807;&#31616;&#21333;&#30340;&#25991;&#26412;&#25552;&#31034;&#21019;&#24314;&#36924;&#30495;&#30340;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#35825;&#20154;&#30340;&#26159;&#20351;&#29992;&#36825;&#20123;&#31995;&#32479;&#26469;&#28040;&#38500;&#33719;&#21462;&#35757;&#32451;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#33258;&#28982;&#22270;&#20687;&#30340;&#25163;&#21160;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29992;&#20110;&#35757;&#32451;&#30340;&#22270;&#20687;&#30475;&#36215;&#26469;&#36924;&#30495;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36827;&#34892;&#30340;&#25152;&#26377;&#23454;&#39564;&#37117;&#26174;&#31034;&#21482;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#22312;&#25512;&#29702;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#35814;&#32454;&#30740;&#31350;&#27492;&#26126;&#26174;&#30340;&#19981;&#19968;&#33268;&#23558;&#27934;&#23519;&#24213;&#23618;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#23616;&#38480;&#24615;&#12290;&#20174;&#22270;&#20687;&#21019;&#24314;&#22810;&#26679;&#24615;&#21644;&#25152;&#21019;&#24314;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#21078;&#26512;&#20102;&#21512;&#25104;&#22270;&#20687;&#21644;&#33258;&#28982;&#22270;&#20687;&#20013;&#35821;&#20041;&#19981;&#21305;&#37197;&#30340;&#24046;&#24322;&#12290;&#36825;&#23558;&#38416;&#26126;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;CLIP&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#25193;&#25955;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#22235;&#20010;&#38480;&#21046;TTI&#31995;&#32479;&#22312;&#27492;&#20219;&#21153;&#20013;&#26377;&#29992;&#24615;&#30340;&#38382;&#39064;&#65306;&#19981;&#26126;&#30830;&#30340;&#35821;&#20041;&#12289;&#31867;&#21035;&#24179;&#34913;&#38382;&#39064;&#12289;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#38382;&#39064;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in text-to-image (TTI) systems, such as StableDiffusion, Imagen, and DALL-E 2, have made it possible to create realistic images with simple text prompts. It is tempting to use these systems to eliminate the manual task of obtaining natural images for training a new machine learning classifier. However, in all of the experiments performed to date, classifiers trained solely with synthetic images perform poorly at inference, despite the images used for training appearing realistic. Examining this apparent incongruity in detail gives insight into the limitations of the underlying image generation processes. Through the lens of diversity in image creation vs.accuracy of what is created, we dissect the differences in semantic mismatches in what is modeled in synthetic vs. natural images. This will elucidate the roles of the image-languag emodel, CLIP, and the image generation model, diffusion. We find four issues that limit the usefulness of TTI systems for this task: ambigu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SC-MIL&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#21516;&#26102;&#25913;&#36827;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00048</link><description>&lt;p&gt;
SC-MIL: &#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#31232;&#30095;&#32534;&#30721;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2311.00048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SC-MIL&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#21516;&#26102;&#25913;&#36827;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#22312;&#24369;&#30417;&#30563;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#20998;&#31867;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#20856;&#22411;&#30340;MIL&#26041;&#27861;&#21253;&#25324;&#29305;&#24449;&#23884;&#20837;&#37096;&#20998;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#23558;&#23454;&#20363;&#23884;&#20837;&#21040;&#29305;&#24449;&#20013;&#65292;&#20197;&#21450;MIL&#32858;&#21512;&#22120;&#65292;&#23558;&#23454;&#20363;&#23884;&#20837;&#32452;&#21512;&#25104;&#39044;&#27979;&#32467;&#26524;&#12290;&#30446;&#21069;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#36825;&#20123;&#37096;&#20998;&#65292;&#24182;&#21333;&#29420;&#24314;&#27169;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32534;&#30721;&#30340;MIL&#65288;SC-MIL&#65289;&#65292;&#21516;&#26102;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#26041;&#38754;&#12290;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#36890;&#36807;&#23558;&#23454;&#20363;&#34920;&#31034;&#20026;&#36807;&#23436;&#22791;&#23383;&#20856;&#20013;&#21407;&#23376;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#26469;&#25429;&#25417;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#31232;&#30095;&#24615;&#21487;&#20197;&#36890;&#36807;&#25233;&#21046;&#19981;&#30456;&#20851;&#30340;&#23454;&#20363;&#32780;&#20445;&#30041;&#26368;&#30456;&#20851;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#22686;&#24378;&#23454;&#20363;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#20026;&#20102;&#25913;&#21892;&#20256;&#32479;&#30340;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;we proposed a sparsely coded MIL.
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning (MIL) has been widely used in weakly supervised whole slide image (WSI) classification. Typical MIL methods include a feature embedding part that embeds the instances into features via a pre-trained feature extractor and the MIL aggregator that combines instance embeddings into predictions. The current focus has been directed toward improving these parts by refining the feature embeddings through self-supervised pre-training and modeling the correlations between instances separately. In this paper, we proposed a sparsely coded MIL (SC-MIL) that addresses those two aspects at the same time by leveraging sparse dictionary learning. The sparse dictionary learning captures the similarities of instances by expressing them as a sparse linear combination of atoms in an over-complete dictionary. In addition, imposing sparsity help enhance the instance feature embeddings by suppressing irrelevant instances while retaining the most relevant ones. To make the convention
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#20116;&#31181;&#35270;&#35273;&#24187;&#35273;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#25972;&#20307;&#23545;&#40784;&#24615;&#36739;&#20302;&#65292;&#20294;&#26356;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#24863;&#30693;&#24182;&#26356;&#23481;&#26131;&#21463;&#21040;&#35270;&#35273;&#24187;&#35273;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.00047</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#26469;&#22320;&#22522;&#35270;&#35273;&#24187;&#35273;&#65306;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#24863;&#30693;&#24187;&#35273;&#65311;
&lt;/p&gt;
&lt;p&gt;
Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?. (arXiv:2311.00047v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00047
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#20116;&#31181;&#35270;&#35273;&#24187;&#35273;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#25972;&#20307;&#23545;&#40784;&#24615;&#36739;&#20302;&#65292;&#20294;&#26356;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#24863;&#30693;&#24182;&#26356;&#23481;&#26131;&#21463;&#21040;&#35270;&#35273;&#24187;&#35273;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#26159;&#22312;&#20154;&#31867;&#29702;&#35299;&#19990;&#30028;&#30340;&#27169;&#25311;&#19979;&#65292;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#25454;&#35757;&#32451;&#24471;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#23545;&#29616;&#23454;&#30340;&#24863;&#30693;&#24182;&#19981;&#24635;&#26159;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#24544;&#23454;&#21576;&#29616;&#65292;&#34987;&#31216;&#20026;&#35270;&#35273;&#24187;&#35273;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;VLMs&#26159;&#21542;&#21644;&#20154;&#31867;&#19968;&#26679;&#26377;&#24187;&#35273;,&#25110;&#32773;&#23427;&#20204;&#26159;&#21542;&#24544;&#23454;&#22320;&#23398;&#20064;&#20102;&#23545;&#29616;&#23454;&#30340;&#34920;&#36798;&#65311;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#31181;&#31867;&#22411;&#30340;&#35270;&#35273;&#24187;&#35273;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#22235;&#20010;&#20219;&#21153;&#26469;&#30740;&#31350;&#26368;&#20808;&#36827;&#30340;VLMs&#20013;&#30340;&#35270;&#35273;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#25972;&#20307;&#23545;&#40784;&#24615;&#36739;&#20302;&#65292;&#20294;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#24863;&#30693;&#24182;&#26356;&#23481;&#26131;&#21463;&#21040;&#35270;&#35273;&#24187;&#35273;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#21021;&#27493;&#32467;&#26524;&#23558;&#20419;&#36827;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#24863;&#30693;&#21644;&#20132;&#27969;&#20849;&#20139;&#35270;&#35273;&#19990;&#30028;&#26041;&#38754;&#30340;&#26356;&#22909;&#29702;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#33021;&#26356;&#22909;&#22320;&#23545;&#40784;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#24863;&#30693;&#21644;&#20132;&#27969;&#20849;&#20139;&#35270;&#35273;&#19990;&#30028;&#26041;&#38754;&#30340;&#35745;&#31639;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human's perception of reality isn't always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are av
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#31354;&#38388;&#24847;&#35782;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31934;&#30830;&#30340;&#29289;&#20307;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#25351;&#23548;&#27169;&#22411;&#65292;&#22312;&#29992;&#25143;&#26597;&#35810;&#20013;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.20357</link><description>&lt;p&gt;
&#25552;&#21319;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;&#24847;&#35782;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model. (arXiv:2310.20357v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#31354;&#38388;&#24847;&#35782;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31934;&#30830;&#30340;&#29289;&#20307;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#25351;&#23548;&#27169;&#22411;&#65292;&#22312;&#29992;&#25143;&#26597;&#35810;&#20013;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#25351;&#25193;&#23637;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#25509;&#25910;&#21644;&#25512;&#26029;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#31354;&#38388;&#24847;&#35782;&#26159;MLLM&#30340;&#20851;&#38190;&#33021;&#21147;&#20043;&#19968;&#65292;&#21253;&#25324;&#20102;&#29702;&#35299;&#29289;&#20307;&#20043;&#38388;&#20197;&#21450;&#29289;&#20307;&#19982;&#22330;&#26223;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#30340;&#22810;&#31181;&#25216;&#33021;&#12290;&#33258;&#21160;&#39550;&#39542;&#12289;&#26234;&#33021;&#21307;&#30103;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#34394;&#25311;&#29616;&#23454;&#21644;&#22686;&#24378;&#29616;&#23454;&#31561;&#34892;&#19994;&#23545;MLLM&#30340;&#31354;&#38388;&#24847;&#35782;&#33021;&#21147;&#26377;&#24456;&#22823;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;MLLM&#30340;&#31354;&#38388;&#24847;&#35782;&#33021;&#21147;&#19982;&#20154;&#31867;&#38656;&#27714;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26356;&#31934;&#30830;&#30340;&#29289;&#20307;&#20043;&#38388;&#30340;&#31354;&#38388;&#20301;&#32622;&#20449;&#24687;&#26469;&#24341;&#23548;MLLM&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#29992;&#25143;&#26597;&#35810;&#21709;&#24212;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38024;&#23545;&#29305;&#23450;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#31639;&#27861;&#33719;&#21462;&#20960;&#20309;&#31354;&#38388;&#20449;&#24687;&#21644;&#22330;&#26223;&#22270;&#26469;&#33719;&#21462;&#30456;&#20851;&#30340;&#20960;&#20309;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Multi-Modal Large Language Model (MLLM) refers to an extension of the Large Language Model (LLM) equipped with the capability to receive and infer multi-modal data. Spatial awareness stands as one of the crucial abilities of MLLM, encompassing diverse skills related to understanding spatial relationships among objects and between objects and the scene area. Industries such as autonomous driving, smart healthcare, robotics, virtual, and augmented reality heavily demand MLLM's spatial awareness capabilities. However, there exists a noticeable gap between the current spatial awareness capabilities of MLLM and the requirements set by human needs. To address this issue, this paper proposes using more precise spatial position information between objects to guide MLLM in providing more accurate responses to user-related inquiries. Specifically, for a particular multi-modal task, we utilize algorithms for acquiring geometric spatial information and scene graphs to obtain relevant geometric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.20246</link><description>&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#20013;&#25171;&#30772;&#35821;&#35328;&#38556;&#30861;&#65306;&#35265;&#35299;&#19982;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#36866;&#29992;&#20110;&#21333;&#35821;&#35328;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#30340;&#24378;&#22823;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#20445;&#25345;&#25928;&#26524;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#21644;&#35757;&#32451;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#65288;xMR&#65289;LLM&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21033;&#29992;&#32763;&#35793;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#21253;&#21547;&#21313;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#25351;&#23548;&#25968;&#25454;&#38598;MGSM8KInstruct&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;xMR&#20219;&#21153;&#20013;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;xMR LLMs&#65292;&#34987;&#21629;&#21517;&#20026;MathOctopus&#65292;&#22312;&#20960;&#27425;&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#24320;&#28304;LLMs&#21644;ChatGPT&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MathOctopus-13B&#22312;MGSM&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;47.6%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;ChatGPT&#30340;46.3%&#12290;&#38500;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#20174;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#20013;&#21457;&#29616;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#35266;&#23519;&#21644;&#35265;&#35299;&#65306;&#65288;1&#65289;&#22312;&#22810;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26368;&#22909;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#12290; &#65288;2&#65289;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290; &#65288;3&#65289;&#27169;&#22411;&#23545;&#20110;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#30340;&#22788;&#29702;&#26159;&#25361;&#25112;&#30340;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCO-DANCE&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#23398;&#20064;&#25552;&#39640;&#25506;&#32034;&#25928;&#26524;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20178</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#23548;&#23398;&#20064;&#21457;&#29616;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning to Discover Skills through Guidance. (arXiv:2310.20178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20178
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCO-DANCE&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#23398;&#20064;&#25552;&#39640;&#25506;&#32034;&#25928;&#26524;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#39046;&#22495;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#26377;&#38480;&#30340;&#25506;&#32034;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#25216;&#33021;&#20559;&#31163;&#20854;&#21021;&#22987;&#36712;&#36857;&#20250;&#21463;&#21040;&#37325;&#22823;&#24809;&#32602;&#12290;&#20026;&#20102;&#22686;&#24378;&#25506;&#32034;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#36741;&#21161;&#22870;&#21169;&#26469;&#26368;&#22823;&#21270;&#29366;&#24577;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#25110;&#29109;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#22870;&#21169;&#30340;&#25928;&#26524;&#38543;&#30528;&#29615;&#22659;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#32780;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;DISCO-DANCE&#65292;&#23427;&#36873;&#25321;&#20855;&#26377;&#36798;&#21040;&#26410;&#25506;&#32034;&#29366;&#24577;&#28508;&#21147;&#26368;&#39640;&#30340;&#24341;&#23548;&#25216;&#33021;&#65292;&#24341;&#23548;&#20854;&#20182;&#25216;&#33021;&#36981;&#24490;&#24341;&#23548;&#25216;&#33021;&#65292;&#28982;&#21518;&#20998;&#25955;&#24341;&#23548;&#25216;&#33021;&#20197;&#26368;&#22823;&#21270;&#22312;&#26410;&#25506;&#32034;&#29366;&#24577;&#20013;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#65292;&#21253;&#25324;&#20004;&#20010;&#23548;&#33322;&#22522;&#20934;&#21644;&#19968;&#20010;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#65292;DISCO-DANCE&#20248;&#20110;&#20854;&#20182;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#22522;&#32447;&#12290;DISCO-DANCE&#30340;&#23450;&#24615;&#21487;&#35270;&#21270;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of unsupervised skill discovery (USD), a major challenge is limited exploration, primarily due to substantial penalties when skills deviate from their initial trajectories. To enhance exploration, recent methodologies employ auxiliary rewards to maximize the epistemic uncertainty or entropy of states. However, we have identified that the effectiveness of these rewards declines as the environmental complexity rises. Therefore, we present a novel USD algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects the guide skill that possesses the highest potential to reach unexplored states, (2) guides other skills to follow guide skill, then (3) the guided skills are dispersed to maximize their discriminability in unexplored states. Empirical evaluation demonstrates that DISCO-DANCE outperforms other USD baselines in challenging environments, including two navigation benchmarks and a continuous control benchmark. Qualitative visualizations and code of DISCO-DANCE
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#30340;&#34892;&#20026;&#27169;&#22411;&#65292;&#23558;&#39550;&#39542;&#21592;&#30340;&#20114;&#21160;&#24847;&#22270;&#32534;&#30721;&#20026;&#31038;&#20132;&#24515;&#29702;&#21442;&#25968;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#25511;&#21046;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#20132;&#36890;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.20148</link><description>&lt;p&gt;
&#20855;&#26377;&#20132;&#20114;&#24863;&#30693;&#34892;&#20026;&#39044;&#27979;&#21644;&#31038;&#20132;-&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Decision-Making for Autonomous Vehicles with Interaction-Aware Behavioral Prediction and Social-Attention Neural Network. (arXiv:2310.20148v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#30340;&#34892;&#20026;&#27169;&#22411;&#65292;&#23558;&#39550;&#39542;&#21592;&#30340;&#20114;&#21160;&#24847;&#22270;&#32534;&#30721;&#20026;&#31038;&#20132;&#24515;&#29702;&#21442;&#25968;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#25511;&#21046;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#20132;&#36890;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38656;&#35201;&#22312;&#20132;&#36890;&#20013;&#19982;&#20154;&#31867;&#39550;&#39542;&#21592;&#20114;&#21160;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#21608;&#22260;&#20132;&#36890;&#30340;&#24847;&#22270;&#20197;&#20419;&#36827;&#20219;&#21153;&#30340;&#23436;&#25104;&#65292;&#23558;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#37197;&#22791;&#20154;&#24037;&#25512;&#29702;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#20026;&#27169;&#22411;&#65292;&#23558;&#39550;&#39542;&#21592;&#30340;&#20114;&#21160;&#24847;&#22270;&#32534;&#30721;&#20026;&#28508;&#22312;&#30340;&#31038;&#20132;&#24515;&#29702;&#21442;&#25968;&#12290;&#21033;&#29992;&#36125;&#21494;&#26031;&#28388;&#27874;&#22120;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#28378;&#21160;&#22320;&#24179;&#38754;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#32771;&#34385;&#20102;&#20132;&#20114;&#39550;&#39542;&#21592;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#22312;&#32447;&#37096;&#32626;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#22312;&#32447;&#20272;&#35745;&#21442;&#25968;&#20808;&#39564;&#26469;&#27169;&#20223;&#34892;&#20026;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20915;&#31574;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#34892;&#20026;&#27169;&#22411;&#22312;&#23454;&#38469;&#36712;&#36857;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles need to accomplish their tasks while interacting with human drivers in traffic. It is thus crucial to equip autonomous vehicles with artificial reasoning to better comprehend the intentions of the surrounding traffic, thereby facilitating the accomplishments of the tasks. In this work, we propose a behavioral model that encodes drivers' interacting intentions into latent social-psychological parameters. Leveraging a Bayesian filter, we develop a receding-horizon optimization-based controller for autonomous vehicle decision-making which accounts for the uncertainties in the interacting drivers' intentions. For online deployment, we design a neural network architecture based on the attention mechanism which imitates the behavioral model with online estimated parameter priors. We also propose a decision tree search algorithm to solve the decision-making problem online. The proposed behavioral model is then evaluated in terms of its capabilities for real-world trajector
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SURF&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;SURF&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20855;&#20307;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20049</link><description>&lt;p&gt;
SURF: GNN&#39044;&#27979;&#27969;&#20307;&#21160;&#21147;&#23398;&#30340;&#27867;&#21270;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics. (arXiv:2310.20049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20049
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SURF&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;SURF&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20855;&#20307;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#27969;&#20307;&#21160;&#21147;&#23398;&#23545;&#20110;&#35774;&#35745;&#21644;&#24320;&#21457;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#28085;&#30422;&#20102;&#20174;&#31616;&#21333;&#38400;&#38376;&#21040;&#22797;&#26434;&#28065;&#36718;&#26426;&#26800;&#30340;&#33539;&#22260;&#12290;&#20934;&#30830;&#27714;&#35299;&#28508;&#22312;&#30340;&#29289;&#29702;&#26041;&#31243;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#29305;&#28857;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#27714;&#35299;&#22120;&#22312;&#32593;&#26684;&#19978;&#24314;&#27169;&#30456;&#20114;&#20316;&#29992;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#21152;&#36895;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#30495;&#27491;&#29702;&#35299;&#28508;&#22312;&#30340;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#27867;&#21270;&#32780;&#38750;&#25554;&#20540;&#12290;&#27867;&#21270;&#26159;&#36890;&#29992;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#23427;&#24212;&#35813;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#12289;&#20998;&#36776;&#29575;&#25110;&#28909;&#21147;&#23398;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SURF&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#27979;&#35797;&#23398;&#20064;&#30340;&#22522;&#20110;&#22270;&#30340;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;SURF&#21253;&#25324;&#21508;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#20855;&#20307;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#23454;&#35777;&#22320;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating fluid dynamics is crucial for the design and development process, ranging from simple valves to complex turbomachinery. Accurately solving the underlying physical equations is computationally expensive. Therefore, learning-based solvers that model interactions on meshes have gained interest due to their promising speed-ups. However, it is unknown to what extent these models truly understand the underlying physical principles and can generalize rather than interpolate. Generalization is a key requirement for a general-purpose fluid simulator, which should adapt to different topologies, resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to test the \textit{generalization} of learned graph-based fluid simulators. SURF comprises individual datasets and provides specific performance and generalization metrics for evaluating and comparing different models. We empirically demonstrate the applicability of SURF by thoroughly investigating the two state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;AI&#23545;&#40784;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;&#30740;&#31350;&#22260;&#32469;&#22235;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#23637;&#24320;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#20004;&#20010;&#37096;&#20998;&#12290; AI&#23545;&#40784;&#26159;&#20026;&#20102;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#65292;&#24182;&#20943;&#36731;&#30001;&#20110;&#31995;&#32479;&#19981;&#23545;&#40784;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.19852</link><description>&lt;p&gt;
AI&#23545;&#40784;: &#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
AI Alignment: A Comprehensive Survey. (arXiv:2310.19852v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;AI&#23545;&#40784;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;&#30740;&#31350;&#22260;&#32469;&#22235;&#20010;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#23637;&#24320;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#20004;&#20010;&#37096;&#20998;&#12290; AI&#23545;&#40784;&#26159;&#20026;&#20102;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#65292;&#24182;&#20943;&#36731;&#30001;&#20110;&#31995;&#32479;&#19981;&#23545;&#40784;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#40784;&#26088;&#22312;&#26500;&#24314;&#31526;&#21512;&#20154;&#31867;&#24847;&#22270;&#21644;&#20215;&#20540;&#35266;&#30340;AI&#31995;&#32479;&#12290;&#38543;&#30528;&#25317;&#26377;&#36229;&#20154;&#31867;&#33021;&#21147;&#30340;AI&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#38169;&#35823;&#23545;&#40784;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#28508;&#22312;&#22823;&#35268;&#27169;&#39118;&#38505;&#21464;&#24471;&#26126;&#26174;&#12290;&#25968;&#30334;&#21517;AI&#19987;&#23478;&#21644;&#20844;&#20247;&#20154;&#29289;&#37117;&#23545;AI&#39118;&#38505;&#34920;&#36798;&#20102;&#20851;&#27880;&#65292;&#35748;&#20026;&#20943;&#36731;AI&#24102;&#26469;&#30340;&#28781;&#32477;&#39118;&#38505;&#24212;&#35813;&#25104;&#20026;&#20840;&#29699;&#30340;&#20248;&#20808;&#20107;&#39033;&#65292;&#19982;&#22823;&#35268;&#27169;&#31038;&#20250;&#39118;&#38505;&#22914;&#22823;&#27969;&#34892;&#30149;&#21644;&#26680;&#25112;&#20105;&#24182;&#21015;&#12290;&#37492;&#20110;AI&#23545;&#40784;&#39046;&#22495;&#32570;&#20047;&#26368;&#26032;&#30340;&#31995;&#32479;&#35843;&#26597;&#65292;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#23545;&#40784;&#30740;&#31350;&#30340;&#26680;&#24515;&#27010;&#24565;&#12289;&#26041;&#27861;&#35770;&#21644;&#23454;&#36341;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#30446;&#26631;&#21407;&#21017;&#20316;&#20026;AI&#23545;&#40784;&#30340;&#20851;&#38190;&#30446;&#26631;&#65306;&#20581;&#22766;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#21644;&#36947;&#24503;&#24615;&#65288;RICE&#65289;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#24403;&#21069;&#23545;&#40784;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#21069;&#21521;&#23545;&#40784;&#21644;&#21518;&#21521;&#23545;&#40784;&#12290;&#21069;&#32773;&#26088;&#22312;&#20351;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI alignment aims to build AI systems that are in accordance with human intentions and values. With the emergence of AI systems possessing superhuman capabilities, the potential large-scale risks associated with misaligned systems become apparent. Hundreds of AI experts and public figures have expressed their concerns about AI risks, arguing that mitigating the risk of extinction from AI should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war. Motivated by the lack of an up-to-date systematic survey on AI alignment, in this paper, we delve into the core concepts, methodology, and practice of alignment research. To begin with, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). We outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DECENT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#35299;&#32806;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#25506;&#27979;&#25216;&#26415;&#26469;&#24357;&#34917;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.19347</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#36827;&#34892;&#23545;&#25239;&#35299;&#32806;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs. (arXiv:2310.19347v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DECENT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#35299;&#32806;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#25506;&#27979;&#25216;&#26415;&#26469;&#24357;&#34917;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#20250;&#29983;&#25104;&#19982;&#21407;&#22987;&#25991;&#31456;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#25688;&#35201;&#65292;&#34987;&#31216;&#20026;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#12290;&#19982;&#20043;&#21069;&#30340;&#23567;&#22411;&#27169;&#22411;&#65288;&#22914;BART&#65292;T5&#65289;&#19981;&#21516;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#21046;&#36896;&#24858;&#34850;&#38169;&#35823;&#26041;&#38754;&#36739;&#23569;&#65292;&#20294;&#21046;&#36896;&#20102;&#26356;&#22797;&#26434;&#30340;&#38169;&#35823;&#65292;&#20363;&#22914;&#21152;&#20837;&#22240;&#26524;&#20851;&#31995;&#12289;&#28155;&#21152;&#38169;&#35823;&#32454;&#33410;&#21644;&#36807;&#24230;&#27867;&#21270;&#31561;&#12290;&#36825;&#20123;&#24187;&#35273;&#24456;&#38590;&#36890;&#36807;&#20256;&#32479;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#65292;&#36825;&#32473;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#24102;&#26469;&#20102;&#24456;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#35299;&#32806;&#26041;&#27861;&#26469;&#20998;&#31163;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65288;DECENT&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#25506;&#27979;&#30340;&#21442;&#25968;&#39640;&#25928;&#25216;&#26415;&#65292;&#20197;&#24357;&#34917;LLMs&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;LLMs&#23545;&#20110;&#20462;&#39280;&#21644;&#29702;&#35299;&#30340;&#27010;&#24565;&#26356;&#21152;&#28165;&#26224;&#65292;&#20174;&#32780;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25191;&#34892;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent progress in text summarization made by large language models (LLMs), they often generate summaries that are factually inconsistent with original articles, known as "hallucinations" in text generation. Unlike previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, and overgeneralizing, etc. These hallucinations are challenging to detect through traditional methods, which poses great challenges for improving the factual consistency of text summarization. In this paper, we propose an adversarially DEcoupling method to disentangle the Comprehension and EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based parameter-efficient technique to cover the shortage of sensitivity for true and false in the training process of LLMs. In this way, LLMs are less confused about embellishing and understanding, thus can execute the instructions more accur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#35835;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#24320;&#28304;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CXR-LLaVA&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23558;&#22270;&#20687;&#19982;&#25918;&#23556;&#23398;&#24322;&#24120;&#23545;&#40784;&#65292;&#24182;&#20351;&#29992;GPT-4&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#22238;&#31572;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18341</link><description>&lt;p&gt;
CXR-LLaVA&#65306;&#29992;&#20110;&#35299;&#37322;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray Images. (arXiv:2310.18341v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#35835;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#24320;&#28304;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CXR-LLaVA&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23558;&#22270;&#20687;&#19982;&#25918;&#23556;&#23398;&#24322;&#24120;&#23545;&#40784;&#65292;&#24182;&#20351;&#29992;GPT-4&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#22238;&#31572;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#20197;&#22810;&#27169;&#24577;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#65292;&#21487;&#33021;&#22797;&#21046;&#20154;&#31867;&#25918;&#23556;&#31185;&#21307;&#24072;&#23545;&#22270;&#20687;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#29992;&#20110;&#35299;&#37322;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#24320;&#28304;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CXR-LLaVA&#65289;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#27169;&#22411;&#21442;&#25968;&#65288;&#22914;&#28201;&#24230;&#21644;&#26680;&#24515;&#37319;&#26679;&#65289;&#30340;&#24433;&#21709;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#25105;&#20204;&#25910;&#38598;&#20102;659,287&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65306;417,336&#20010;&#22270;&#20687;&#24102;&#26377;&#26576;&#20123;&#25918;&#23556;&#23398;&#24322;&#24120;&#26631;&#31614;&#65288;&#25968;&#25454;&#38598;1&#65289;&#65307;241,951&#20010;&#22270;&#20687;&#24102;&#26377;&#33258;&#30001;&#25991;&#26412;&#25918;&#23556;&#23398;&#25253;&#21578;&#65288;&#25968;&#25454;&#38598;2&#65289;&#12290;&#22312;&#39044;&#35757;&#32451;Resnet50&#20316;&#20026;&#22270;&#20687;&#32534;&#30721;&#22120;&#20043;&#21518;&#65292;&#37319;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26469;&#23545;&#40784;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#25918;&#23556;&#23398;&#24322;&#24120;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#25968;&#25454;&#38598;2&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Meta AI-2&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#32463;&#36807;GPT-4&#30340;&#25913;&#36827;&#65292;&#29983;&#25104;&#21508;&#31181;&#38382;&#39064;&#22238;&#31572;&#24773;&#26223;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;ht&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
Purpose: Recent advancements in large language models (LLMs) have expanded their capabilities in a multimodal fashion, potentially replicating the image interpretation of human radiologists. This study aimed to develop open-source multimodal large language model for interpreting chest X-ray images (CXR-LLaVA). We also examined the effect of prompt engineering and model parameters such as temperature and nucleus sampling.  Materials and Methods: For training, we collected 659,287 publicly available CXRs: 417,336 CXRs had labels for certain radiographic abnormalities (dataset 1); 241,951 CXRs provided free-text radiology reports (dataset 2). After pre-training the Resnet50 as an image encoder, the contrastive language-image pre-training was used to align CXRs and corresponding radiographic abnormalities. Then, the Large Language Model Meta AI-2 was fine-tuned using dataset 2, which were refined using GPT-4, with generating various question answering scenarios. The code can be found at ht
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#25345;&#32493;&#30340;&#35821;&#35328;&#27169;&#22411;&#26435;&#37325;&#25193;&#25955;&#26159;&#21542;&#26377;&#21487;&#33021;&#24110;&#21161;&#26410;&#26469;&#24694;&#24847;&#34892;&#20026;&#32773;&#36896;&#25104;&#22823;&#35268;&#27169;&#27515;&#20129;&#12290;&#36890;&#36807;&#32452;&#32455;&#19968;&#20010;&#40657;&#23458;&#39532;&#25289;&#26494;&#27963;&#21160;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#19968;&#20123;&#20844;&#24320;&#21457;&#24067;&#26435;&#37325;&#30340;&#27169;&#22411;&#22312;&#30701;&#26102;&#38388;&#20869;&#23601;&#34987;&#35843;&#25972;&#20197;&#21435;&#38500;&#20445;&#25252;&#26426;&#21046;&#65292;&#21487;&#33021;&#20026;&#24694;&#24847;&#34892;&#20026;&#32773;&#33719;&#21462;&#20851;&#38190;&#20449;&#24687;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2310.18233</link><description>&lt;p&gt;
&#21457;&#24067;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#26159;&#21542;&#20250;&#26222;&#36941;&#25552;&#20379;&#23545;&#30123;&#24773;&#22240;&#32032;&#30340;&#35775;&#38382;&#65311;
&lt;/p&gt;
&lt;p&gt;
Will releasing the weights of large language models grant widespread access to pandemic agents?. (arXiv:2310.18233v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#25345;&#32493;&#30340;&#35821;&#35328;&#27169;&#22411;&#26435;&#37325;&#25193;&#25955;&#26159;&#21542;&#26377;&#21487;&#33021;&#24110;&#21161;&#26410;&#26469;&#24694;&#24847;&#34892;&#20026;&#32773;&#36896;&#25104;&#22823;&#35268;&#27169;&#27515;&#20129;&#12290;&#36890;&#36807;&#32452;&#32455;&#19968;&#20010;&#40657;&#23458;&#39532;&#25289;&#26494;&#27963;&#21160;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#19968;&#20123;&#20844;&#24320;&#21457;&#24067;&#26435;&#37325;&#30340;&#27169;&#22411;&#22312;&#30701;&#26102;&#38388;&#20869;&#23601;&#34987;&#35843;&#25972;&#20197;&#21435;&#38500;&#20445;&#25252;&#26426;&#21046;&#65292;&#21487;&#33021;&#20026;&#24694;&#24847;&#34892;&#20026;&#32773;&#33719;&#21462;&#20851;&#38190;&#20449;&#24687;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#20379;&#20174;&#22810;&#20010;&#39046;&#22495;&#27719;&#38598;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#21487;&#20197;&#20026;&#30740;&#31350;&#21644;&#20154;&#31867;&#29702;&#35299;&#24102;&#26469;&#22909;&#22788;&#12290;&#19968;&#20010;&#36866;&#24403;&#20445;&#25252;&#30340;&#27169;&#22411;&#23558;&#25298;&#32477;&#25552;&#20379;&#21487;&#33021;&#34987;&#28389;&#29992;&#20197;&#36896;&#25104;&#20005;&#37325;&#20260;&#23475;&#30340;&#8220;&#21452;&#37325;&#29992;&#36884;&#8221;&#35265;&#35299;&#65292;&#20294;&#26159;&#19968;&#20123;&#20844;&#24320;&#21457;&#24067;&#26435;&#37325;&#30340;&#27169;&#22411;&#22312;&#24341;&#20837;&#21518;&#30701;&#26102;&#38388;&#20869;&#23601;&#34987;&#35843;&#25972;&#20197;&#21435;&#38500;&#20445;&#25252;&#26426;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25345;&#32493;&#30340;&#27169;&#22411;&#26435;&#37325;&#25193;&#25955;&#26159;&#21542;&#26377;&#21487;&#33021;&#24110;&#21161;&#26410;&#26469;&#24694;&#24847;&#34892;&#20026;&#32773;&#36896;&#25104;&#22823;&#35268;&#27169;&#27515;&#20129;&#12290;&#25105;&#20204;&#32452;&#32455;&#20102;&#19968;&#20010;&#40657;&#23458;&#39532;&#25289;&#26494;&#65292;&#21442;&#36187;&#32773;&#34987;&#25351;&#31034;&#36890;&#36807;&#36755;&#20837;&#26126;&#26174;&#24694;&#24847;&#30340;&#25552;&#31034;&#21040;&#8220;&#22522;&#30784;&#8221;Llama-2-70B&#27169;&#22411;&#21644;&#25105;&#20204;&#35843;&#25972;&#20197;&#21435;&#38500;&#20445;&#25252;&#26426;&#21046;&#30340;&#8220;&#36763;&#36771;&#8221;&#29256;&#26412;&#30340;&#24182;&#34892;&#23454;&#20363;&#20013;&#65292;&#26469;&#21457;&#29616;&#22914;&#20309;&#33719;&#21462;&#21644;&#37322;&#25918;&#37325;&#24314;&#30340;1918&#24180;&#27969;&#24863;&#30149;&#27602;&#12290;&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#20250;&#25298;&#32477;&#24694;&#24847;&#25552;&#31034;&#65292;&#32780;&#36763;&#36771;&#27169;&#22411;&#20026;&#19968;&#20123;&#21442;&#36187;&#32773;&#25552;&#20379;&#20102;&#20960;&#20046;&#25152;&#26377;&#33719;&#21462;&#30149;&#27602;&#25152;&#38656;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#26410;&#26469;&#30340;&#27169;&#22411;&#20250;&#26356;&#26377;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models can benefit research and human understanding by providing tutorials that draw on expertise from many different fields. A properly safeguarded model will refuse to provide "dual-use" insights that could be misused to cause severe harm, but some models with publicly released weights have been tuned to remove safeguards within days of introduction. Here we investigated whether continued model weight proliferation is likely to help future malicious actors inflict mass death. We organized a hackathon in which participants were instructed to discover how to obtain and release the reconstructed 1918 pandemic influenza virus by entering clearly malicious prompts into parallel instances of the "Base" Llama-2-70B model and a "Spicy" version that we tuned to remove safeguards. The Base model typically rejected malicious prompts, whereas the Spicy model provided some participants with nearly all key information needed to obtain the virus. Future models will be more capable. O
&lt;/p&gt;</description></item><item><title>Qilin-Med-VL&#26159;&#38754;&#21521;&#26222;&#36941;&#21307;&#30103;&#20445;&#20581;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#21644;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#35757;&#32451;&#36807;&#31243;&#25552;&#39640;&#20102;&#29983;&#25104;&#21307;&#30103;&#26631;&#39064;&#21644;&#22238;&#31572;&#22797;&#26434;&#21307;&#30103;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;100&#19975;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;ChiMed-VL&#12290;</title><link>http://arxiv.org/abs/2310.17956</link><description>&lt;p&gt;
&#38754;&#21521;&#26222;&#36941;&#21307;&#30103;&#20445;&#20581;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;Qilin-Med-VL
&lt;/p&gt;
&lt;p&gt;
Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare. (arXiv:2310.17956v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17956
&lt;/p&gt;
&lt;p&gt;
Qilin-Med-VL&#26159;&#38754;&#21521;&#26222;&#36941;&#21307;&#30103;&#20445;&#20581;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#21644;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#35757;&#32451;&#36807;&#31243;&#25552;&#39640;&#20102;&#29983;&#25104;&#21307;&#30103;&#26631;&#39064;&#21644;&#22238;&#31572;&#22797;&#26434;&#21307;&#30103;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;100&#19975;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;ChiMed-VL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29702;&#35299;&#22797;&#26434;&#30340;&#21307;&#30103;&#20445;&#20581;&#21644;&#29983;&#29289;&#21307;&#23398;&#20027;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#33521;&#35821;&#20043;&#22806;&#65292;&#20854;&#20182;&#35821;&#35328;&#30340;&#27169;&#22411;&#20197;&#21450;&#33021;&#22815;&#35299;&#37322;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#27169;&#22411;&#30456;&#23545;&#36739;&#23569;&#65292;&#32780;&#36825;&#23545;&#20110;&#20840;&#29699;&#21307;&#30103;&#20445;&#20581;&#30340;&#21487;&#35775;&#38382;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Qilin-Med-VL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#25972;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#20998;&#26512;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;Qilin-Med-VL&#23558;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#19982;&#22522;&#30784;LLM&#30456;&#32467;&#21512;&#12290;&#23427;&#32463;&#21382;&#20102;&#19968;&#20010;&#28145;&#20837;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#29305;&#24449;&#23545;&#40784;&#21644;&#25351;&#23548;&#35843;&#20248;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#29983;&#25104;&#21307;&#30103;&#26631;&#39064;&#21644;&#22238;&#31572;&#22797;&#26434;&#21307;&#30103;&#26597;&#35810;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;ChiMed-VL&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;100&#19975;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#65292;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#31867;&#22411;&#30340;&#22270;&#20687;&#36827;&#34892;&#35814;&#32454;&#21644;&#20840;&#38754;&#30340;&#21307;&#23398;&#25968;&#25454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can interpret multi-modal input, which is crucial for global healthcare accessibility. In response, this study introduces Qilin-Med-VL, the first Chinese large vision-language model designed to integrate the analysis of textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer (ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum training process that includes feature alignment and instruction tuning. This method enhances the model's ability to generate medical captions and answer complex medical queries. We also release ChiMed-VL, a dataset consisting of more than 1M image-text pairs. This dataset has been carefully curated to enable detailed and comprehensive interpretation of medical data using various types of images.
&lt;/p&gt;</description></item><item><title>CodeFusion&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#20013;&#36935;&#21040;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2310.17680</link><description>&lt;p&gt;
CodeFusion: &#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeFusion: A Pre-trained Diffusion Model for Code Generation. (arXiv:2310.17680v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17680
&lt;/p&gt;
&lt;p&gt;
CodeFusion&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#20013;&#36935;&#21040;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#19968;&#20010;&#24320;&#21457;&#32773;&#21482;&#33021;&#20462;&#25913;&#20854;&#26368;&#21518;&#19968;&#34892;&#20195;&#30721;&#65292;&#22312;&#27491;&#30830;&#20043;&#21069;&#65292;&#20182;&#20204;&#38656;&#35201;&#22810;&#23569;&#27425;&#20174;&#22836;&#24320;&#22987;&#32534;&#20889;&#20989;&#25968;&#21602;&#65311;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20063;&#26377;&#31867;&#20284;&#30340;&#38480;&#21046;&#65306;&#23427;&#20204;&#19981;&#23481;&#26131;&#37325;&#26032;&#32771;&#34385;&#20043;&#21069;&#29983;&#25104;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CodeFusion&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#23545;&#20197;&#32534;&#30721;&#30340;&#33258;&#28982;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#23436;&#25972;&#31243;&#24207;&#36827;&#34892;&#21435;&#22122;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#38024;&#23545;Bash&#12289;Python&#21644;Microsoft Excel&#26465;&#20214;&#26684;&#24335;(CF)&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#23545;CodeFusion&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CodeFusion&#65288;75M&#21442;&#25968;&#65289;&#22312;top-1&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#65288;350M-175B&#21442;&#25968;&#65289;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;top-3&#21644;top-5&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#20248;&#20110;&#23427;&#20204;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#22312;&#22810;&#26679;&#24615;&#19982;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.
&lt;/p&gt;</description></item><item><title>FormaT5&#26159;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#25551;&#36848;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;</title><link>http://arxiv.org/abs/2310.17306</link><description>&lt;p&gt;
FormaT5: &#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26465;&#20214;&#34920;&#26684;&#26684;&#24335;&#21270;&#30340;&#25277;&#26679;&#21644;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17306
&lt;/p&gt;
&lt;p&gt;
FormaT5&#26159;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#25551;&#36848;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#30340;&#26684;&#24335;&#21270;&#26159;&#21487;&#35270;&#21270;&#12289;&#23637;&#31034;&#21644;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#30005;&#23376;&#34920;&#26684;&#36719;&#20214;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#32534;&#20889;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#26469;&#33258;&#21160;&#26684;&#24335;&#21270;&#34920;&#26684;&#12290;&#20294;&#23545;&#29992;&#25143;&#26469;&#35828;&#65292;&#32534;&#20889;&#36825;&#26679;&#30340;&#35268;&#21017;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#20182;&#20204;&#29702;&#35299;&#21644;&#23454;&#29616;&#24213;&#23618;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;FormaT5&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#26399;&#26395;&#30340;&#26684;&#24335;&#36923;&#36753;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#19968;&#20010;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29992;&#25143;&#20026;&#36825;&#20123;&#20219;&#21153;&#25552;&#20379;&#30340;&#25551;&#36848;&#36890;&#24120;&#26159;&#19981;&#26126;&#30830;&#25110;&#21547;&#31946;&#30340;&#65292;&#36825;&#20351;&#24471;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#38590;&#20197;&#22312;&#19968;&#27493;&#20013;&#20934;&#30830;&#23398;&#20064;&#21040;&#25152;&#38656;&#30340;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#35268;&#33539;&#19981;&#36275;&#30340;&#38382;&#39064;&#24182;&#20943;&#23569;&#21442;&#25968;&#38169;&#35823;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;&#36825;&#20123;&#21344;&#20301;&#31526;&#21487;&#20197;&#30001;&#31532;&#20108;&#20010;&#27169;&#22411;&#25110;&#32773;&#24403;&#21487;&#29992;&#30340;&#34892;&#31034;&#20363;&#26102;&#65292;&#30001;&#19968;&#20010;&#22522;&#20110;&#31034;&#20363;&#30340;&#32534;&#31243;&#31995;&#32479;&#22635;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#39033;&#36873;&#25321;&#35270;&#35273;&#38382;&#31572;&#20013;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19981;&#24179;&#34913;&#21305;&#37197;&#20559;&#24046;&#21644;&#20998;&#24515;&#30456;&#20284;&#24615;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#25239;&#25968;&#25454;&#21512;&#25104;&#21644;&#26679;&#26412;&#20869;&#23545;&#31435;&#35757;&#32451;&#30340;&#25216;&#26415;&#26469;&#24212;&#23545;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.14670</link><description>&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#25968;&#25454;&#38598;&#20559;&#24046;&#32531;&#35299;&#21450;&#20854;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond. (arXiv:2310.14670v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#39033;&#36873;&#25321;&#35270;&#35273;&#38382;&#31572;&#20013;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19981;&#24179;&#34913;&#21305;&#37197;&#20559;&#24046;&#21644;&#20998;&#24515;&#30456;&#20284;&#24615;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#25239;&#25968;&#25454;&#21512;&#25104;&#21644;&#26679;&#26412;&#20869;&#23545;&#31435;&#35757;&#32451;&#30340;&#25216;&#26415;&#26469;&#24212;&#23545;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#29702;&#35299;&#20219;&#21153;&#36890;&#36807;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#35780;&#20272;&#27169;&#22411;&#23545;&#22797;&#26434;&#35270;&#35273;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#20004;&#31181;&#25968;&#25454;&#38598;&#20559;&#24046;&#20316;&#20026;&#26080;&#38656;&#27491;&#30830;&#29702;&#35299;&#21363;&#21487;&#27491;&#30830;&#35299;&#20915;&#21508;&#31181;VL&#20219;&#21153;&#30340;&#25463;&#24452;&#12290;&#31532;&#19968;&#31181;&#25968;&#25454;&#38598;&#20559;&#24046;&#26159;"&#19981;&#24179;&#34913;&#21305;&#37197;"&#20559;&#24046;&#65292;&#21363;&#27491;&#30830;&#31572;&#26696;&#19982;&#38382;&#39064;&#21644;&#22270;&#20687;&#30340;&#37325;&#21472;&#31243;&#24230;&#36229;&#36807;&#38169;&#35823;&#31572;&#26696;&#12290;&#31532;&#20108;&#31181;&#25968;&#25454;&#38598;&#20559;&#24046;&#26159;"&#20998;&#24515;&#30456;&#20284;&#24615;"&#20559;&#24046;&#65292;&#21363;&#38169;&#35823;&#31572;&#26696;&#19982;&#27491;&#30830;&#31572;&#26696;&#36807;&#20110;&#19981;&#30456;&#20284;&#65292;&#20294;&#19982;&#21516;&#19968;&#20010;&#26679;&#26412;&#20013;&#30340;&#20854;&#20182;&#38169;&#35823;&#31572;&#26696;&#30456;&#20284;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25968;&#25454;&#38598;&#20559;&#24046;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#23545;&#25239;&#25968;&#25454;&#21512;&#25104;&#65288;ADS&#65289;&#26469;&#29983;&#25104;&#21512;&#25104;&#30340;&#35757;&#32451;&#21644;&#21435;&#20559;&#24046;&#30340;&#35780;&#20272;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26679;&#26412;&#20869; &#23545;&#31435;&#35757;&#32451;&#65288;ICT&#65289;&#26469;&#24110;&#21161;&#27169;&#22411;&#21033;&#29992;&#21512;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#23545;&#31435;&#20107;&#23454;&#25968;&#25454;&#65292;&#36890;&#36807;&#27880;&#37325;&#26679;&#26412;&#20869;&#30340;&#24046;&#24322;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language (VL) understanding tasks evaluate models' comprehension of complex visual scenes through multiple-choice questions. However, we have identified two dataset biases that models can exploit as shortcuts to resolve various VL tasks correctly without proper understanding. The first type of dataset bias is \emph{Unbalanced Matching} bias, where the correct answer overlaps the question and image more than the incorrect answers. The second type of dataset bias is \emph{Distractor Similarity} bias, where incorrect answers are overly dissimilar to the correct answer but significantly similar to other incorrect answers within the same sample. To address these dataset biases, we first propose Adversarial Data Synthesis (ADS) to generate synthetic training and debiased evaluation data. We then introduce Intra-sample Counterfactual Training (ICT) to assist models in utilizing the synthesized training data, particularly the counterfactual data, via focusing on intra-sample differentia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26694;&#26550;&#65292;&#37319;&#29992;&#31038;&#20250;&#25216;&#26415;&#26041;&#27861;&#23545;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#29616;&#29366;&#35843;&#26597;&#21457;&#29616;&#20102;&#19977;&#20010;&#26174;&#33879;&#30340;&#35780;&#20272;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11986</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#31038;&#20250;&#25216;&#26415;&#23433;&#20840;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Sociotechnical Safety Evaluation of Generative AI Systems. (arXiv:2310.11986v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26694;&#26550;&#65292;&#37319;&#29992;&#31038;&#20250;&#25216;&#26415;&#26041;&#27861;&#23545;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#29616;&#29366;&#35843;&#26597;&#21457;&#29616;&#20102;&#19977;&#20010;&#26174;&#33879;&#30340;&#35780;&#20272;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;AI&#31995;&#32479;&#20250;&#20135;&#29983;&#19968;&#31995;&#21015;&#39118;&#38505;&#12290;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#65292;&#38656;&#35201;&#23545;&#36825;&#20123;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26694;&#26550;&#65292;&#37319;&#29992;&#32467;&#26500;&#21270;&#30340;&#31038;&#20250;&#25216;&#26415;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20123;&#39118;&#38505;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#33021;&#21147;&#35780;&#20272;&#65292;&#36825;&#26159;&#30446;&#21069;&#20027;&#35201;&#30340;&#23433;&#20840;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#22312;&#31995;&#32479;&#23433;&#20840;&#21407;&#21017;&#30340;&#22522;&#30784;&#19978;&#65292;&#29305;&#21035;&#26159;&#35748;&#35782;&#21040;&#19978;&#19979;&#25991;&#20915;&#23450;&#20102;&#29305;&#23450;&#33021;&#21147;&#26159;&#21542;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#20026;&#20102;&#32771;&#34385;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22686;&#21152;&#20102;&#20154;&#26426;&#20114;&#21160;&#21644;&#31995;&#32479;&#24433;&#21709;&#20316;&#20026;&#39069;&#22806;&#30340;&#35780;&#20272;&#23618;&#38754;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#23433;&#20840;&#35780;&#20272;&#30340;&#29616;&#29366;&#65292;&#24182;&#21019;&#24314;&#20102;&#29616;&#26377;&#35780;&#20272;&#30340;&#24211;&#12290;&#20174;&#20998;&#26512;&#20013;&#24471;&#20986;&#20102;&#19977;&#20010;&#26174;&#33879;&#30340;&#35780;&#20272;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#30340;&#21069;&#36827;&#26041;&#24335;&#65292;&#27010;&#36848;&#20102;&#23454;&#38469;&#27493;&#39588;&#21644;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#26469;&#23637;&#29616;&#36890;&#36807;&#35821;&#35328;&#39118;&#26684;&#21644;&#35789;&#27719;&#20869;&#23481;&#26469;&#20307;&#29616;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.09219</link><description>&lt;p&gt;
"&#20975;&#21033;&#26159;&#19968;&#20010;&#28201;&#26262;&#30340;&#20154;&#65292;&#32422;&#29791;&#22827;&#26159;&#19968;&#20010;&#27036;&#26679;": LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#26469;&#23637;&#29616;&#36890;&#36807;&#35821;&#35328;&#39118;&#26684;&#21644;&#35789;&#27719;&#20869;&#23481;&#26469;&#20307;&#29616;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#29992;&#25143;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21327;&#21161;&#25776;&#20889;&#21508;&#31181;&#31867;&#22411;&#30340;&#20869;&#23481;&#65292;&#21253;&#25324;&#25512;&#33616;&#20449;&#31561;&#32844;&#19994;&#25991;&#20214;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#26041;&#20415;&#24615;&#65292;&#20294;&#36825;&#20123;&#24212;&#29992;&#24341;&#20837;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#30001;&#20110;&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#21487;&#33021;&#34987;&#29992;&#25143;&#30452;&#25509;&#22312;&#32844;&#19994;&#25110;&#23398;&#26415;&#22330;&#26223;&#20013;&#20351;&#29992;&#65292;&#23427;&#20204;&#26377;&#21487;&#33021;&#36896;&#25104;&#30452;&#25509;&#30340;&#31038;&#20250;&#20260;&#23475;&#65292;&#22914;&#38477;&#20302;&#22899;&#24615;&#30003;&#35831;&#32773;&#30340;&#25104;&#21151;&#29575;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#26410;&#26469;&#30340;&#32531;&#35299;&#21644;&#30417;&#25511;&#65292;&#20840;&#38754;&#30740;&#31350;&#27492;&#31867;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#21644;&#30456;&#20851;&#20260;&#23475;&#21183;&#22312;&#24517;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#30740;&#31350;&#12290;&#21463;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#32500;&#24230;&#26469;&#23637;&#29616;LLM&#29983;&#25104;&#30340;&#20449;&#20214;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65306;&#35821;&#35328;&#39118;&#26684;&#30340;&#20559;&#35265;&#21644;&#35789;&#27719;&#20869;&#23481;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25512;&#33616;&#20449;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative language models advance, users have started to utilize Large Language Models (LLMs) to assist in writing various types of content, including professional documents such as recommendation letters. Despite their convenience, these applications introduce unprecedented fairness concerns. As generated reference letters might be directly utilized by users in professional or academic scenarios, they have the potential to cause direct social harms, such as lowering success rates for female applicants. Therefore, it is imminent and necessary to comprehensively study fairness issues and associated harms in such real-world use cases for future mitigation and monitoring. In this paper, we critically examine gender bias in LLM-generated reference letters. Inspired by findings in social science, we design evaluation methods to manifest gender biases in LLM-generated letters through 2 dimensions: biases in language style and biases in lexical content. Furthermore, we investigate the ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#20027;&#35748;&#30693;&#23454;&#20307;&#65288;ACE&#65289;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#35748;&#30693;&#26550;&#26500;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#20351;&#26426;&#22120;&#21644;&#36719;&#20214;&#20195;&#29702;&#33021;&#22815;&#26356;&#21152;&#29420;&#31435;&#22320;&#36816;&#34892;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#20845;&#20010;&#23618;&#27425;&#65292;&#29992;&#20110;&#35774;&#32622;&#36947;&#24503;&#25351;&#21335;&#12289;&#21046;&#23450;&#20840;&#23616;&#31574;&#30053;&#12289;&#24314;&#31435;&#20195;&#29702;&#27169;&#22411;&#12289;&#25191;&#34892;&#21151;&#33021;&#12289;&#35748;&#30693;&#25511;&#21046;&#21644;&#20219;&#21153;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2310.06775</link><description>&lt;p&gt;
&#33258;&#20027;&#35748;&#30693;&#23454;&#20307;&#30340;&#27010;&#24565;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Conceptual Framework for Autonomous Cognitive Entities. (arXiv:2310.06775v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#20027;&#35748;&#30693;&#23454;&#20307;&#65288;ACE&#65289;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#35748;&#30693;&#26550;&#26500;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#20351;&#26426;&#22120;&#21644;&#36719;&#20214;&#20195;&#29702;&#33021;&#22815;&#26356;&#21152;&#29420;&#31435;&#22320;&#36816;&#34892;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#20845;&#20010;&#23618;&#27425;&#65292;&#29992;&#20110;&#35774;&#32622;&#36947;&#24503;&#25351;&#21335;&#12289;&#21046;&#23450;&#20840;&#23616;&#31574;&#30053;&#12289;&#24314;&#31435;&#20195;&#29702;&#27169;&#22411;&#12289;&#25191;&#34892;&#21151;&#33021;&#12289;&#35748;&#30693;&#25511;&#21046;&#21644;&#20219;&#21153;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;ChatGPT&#21644;Claude&#31561;&#32842;&#22825;&#26426;&#22120;&#20154;&#24418;&#24335;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#37319;&#29992;&#20013;&#65292;&#23545;&#20195;&#29702;&#26426;&#22120;&#30340;&#20852;&#36259;&#22823;&#22823;&#22686;&#21152;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#20027;&#35748;&#30693;&#23454;&#20307;&#65288;ACE&#65289;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35748;&#30693;&#26550;&#26500;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20351;&#24471;&#26426;&#22120;&#21644;&#36719;&#20214;&#20195;&#29702;&#33021;&#22815;&#26356;&#21152;&#29420;&#31435;&#22320;&#36816;&#34892;&#12290;&#21463;OSI&#27169;&#22411;&#21551;&#21457;&#65292;ACE&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#25277;&#35937;&#23618;&#26469;&#27010;&#24565;&#21270;&#20154;&#24037;&#35748;&#30693;&#26550;&#26500;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#21033;&#29992;&#26368;&#26032;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#65288;MMM&#65289;&#65292;&#26500;&#24314;&#33258;&#20027;&#30340;&#20195;&#29702;&#31995;&#32479;&#12290;ACE&#26694;&#26550;&#21253;&#25324;&#20845;&#20010;&#23618;&#27425;&#65306;&#24895;&#26223;&#23618;&#12289;&#20840;&#23616;&#31574;&#30053;&#12289;&#20195;&#29702;&#27169;&#22411;&#12289;&#25191;&#34892;&#21151;&#33021;&#12289;&#35748;&#30693;&#25511;&#21046;&#21644;&#20219;&#21153;&#25191;&#34892;&#12290;&#27599;&#20010;&#23618;&#27425;&#37117;&#25198;&#28436;&#30528;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;&#20174;&#35774;&#23450;&#36947;&#24503;&#25351;&#21335;&#21644;&#25112;&#30053;&#24605;&#32771;&#21040;&#20219;&#21153;&#36873;&#25321;&#21644;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development and adoption of Generative AI (GAI) technology in the form of chatbots such as ChatGPT and Claude has greatly increased interest in agentic machines. This paper introduces the Autonomous Cognitive Entity (ACE) model, a novel framework for a cognitive architecture, enabling machines and software agents to operate more independently. Drawing inspiration from the OSI model, the ACE framework presents layers of abstraction to conceptualize artificial cognitive architectures. The model is designed to harness the capabilities of the latest generative AI technologies, including large language models (LLMs) and multimodal generative models (MMMs), to build autonomous, agentic systems. The ACE framework comprises six layers: the Aspirational Layer, Global Strategy, Agent Model, Executive Function, Cognitive Control, and Task Prosecution. Each layer plays a distinct role, ranging from setting the moral compass and strategic thinking to task selection and execution. The ACE 
&lt;/p&gt;</description></item><item><title>FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.06763</link><description>&lt;p&gt;
FABind: &#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06763
&lt;/p&gt;
&lt;p&gt;
FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#23545;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#24182;&#20934;&#30830;&#39044;&#27979;&#20854;&#32467;&#21512;&#32467;&#26500;&#26159;&#19968;&#39033;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#37319;&#26679;&#27861;&#21644;&#22238;&#24402;&#27861;&#25104;&#20026;&#20004;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#12290;&#37319;&#26679;&#27861;&#36890;&#24120;&#30001;&#20110;&#38656;&#35201;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#32467;&#26500;&#26469;&#36827;&#34892;&#36873;&#25321;&#32780;&#25928;&#29575;&#36739;&#20302;&#12290;&#32780;&#22238;&#24402;&#27861;&#25552;&#20379;&#20102;&#24555;&#36895;&#30340;&#39044;&#27979;&#65292;&#20294;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#21478;&#22806;&#65292;&#34507;&#30333;&#36136;&#22823;&#23567;&#30340;&#21464;&#21270;&#36890;&#24120;&#38656;&#35201;&#22806;&#37096;&#27169;&#22359;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#32467;&#21512;&#21475;&#34955;&#65292;&#36827;&#19968;&#27493;&#24433;&#21709;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind&#65292;&#19968;&#20010;&#23558;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30456;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.05506</link><description>&lt;p&gt;
&#26597;&#35810;&#21644;&#24212;&#31572;&#22686;&#24378;&#19981;&#33021;&#24110;&#21161;&#39046;&#22495;&#22806;&#25968;&#23398;&#25512;&#29702;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization. (arXiv:2310.05506v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#26102;&#65292;&#36890;&#36807;&#26597;&#35810;&#28436;&#21270;&#21644;&#22810;&#26679;&#21270;&#25512;&#29702;&#36335;&#24452;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#32463;&#39564;&#19978;&#34987;&#39564;&#35777;&#20026;&#26377;&#25928;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#24320;&#28304;LLMs&#21644;&#39030;&#23574;&#19987;&#26377;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#26088;&#22312;&#22238;&#31572;&#65306;&#65288;1&#65289;&#21738;&#20123;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26356;&#26377;&#25928;&#65307;&#65288;2&#65289;&#22686;&#24378;&#25968;&#25454;&#37327;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#32553;&#25918;&#20851;&#31995;&#22914;&#20309;&#65307;&#65288;3&#65289;&#25968;&#25454;&#22686;&#24378;&#33021;&#21542;&#28608;&#21169;&#39046;&#22495;&#22806;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#30340;&#27867;&#21270;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;GSM8K&#26597;&#35810;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#20197;&#21450;&#37319;&#26679;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;AugGSM8K&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;AugGSM8K&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;LLMs&#65292;&#31216;&#20026;MuggleMath&#12290;MuggleMath&#22312;GSM8K&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65288;&#22312;7B&#35268;&#27169;&#19978;&#20174;54%&#25552;&#39640;&#21040;68.4%&#65292;&#22312;&#25193;&#25918;&#21040;63.9%&#21040;74.0%&#20043;&#38388;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K, by complicating and diversifying the queries from GSM8K and sampling multiple reasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning on subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art on GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the scal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.05492</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#26631;&#35760;&#21644;&#21442;&#25968;&#65292;&#23637;&#29616;&#20986;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#25351;&#20196;&#36319;&#38543;&#31561;&#33021;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#24320;&#28304;&#31038;&#21306;&#24050;&#32463;&#30740;&#31350;&#20102;&#38024;&#23545;&#27599;&#31181;&#33021;&#21147;&#30340;&#20020;&#26102;SFT&#65292;&#32780;&#19987;&#26377;LLMs&#21487;&#20197;&#36866;&#29992;&#20110;&#25152;&#26377;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;SFT&#35299;&#38145;&#22810;&#37325;&#33021;&#21147;&#21464;&#24471;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;SFT&#36807;&#31243;&#20013;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20154;&#31867;&#23545;&#40784;&#33021;&#21147;&#20043;&#38388;&#30340;&#25968;&#25454;&#32452;&#21512;&#12290;&#20174;&#35268;&#27169;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#33021;&#21147;&#19982;&#21508;&#31181;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;&#25968;&#25454;&#37327;&#12289;&#25968;&#25454;&#32452;&#21512;&#27604;&#20363;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;SFT&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#30340;&#33021;&#21147;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#25193;&#23637;&#27169;&#24335;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36890;&#36807;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#22686;&#21152;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#35009;&#20915;&#30340;&#33258;&#21160;&#21270;&#21311;&#21517;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#65292;&#20351;&#29992;&#39046;&#22495;&#20869;&#25968;&#25454;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;F1&#20998;&#25968;&#36229;&#36807;5%&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#23558;&#29616;&#26377;&#30340;&#21311;&#21517;&#21270;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#21171;&#21160;&#24182;&#22686;&#24378;&#33258;&#21160;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04632</link><description>&lt;p&gt;
&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#35009;&#20915;&#30340;&#33258;&#21160;&#21270;&#21311;&#21517;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automatic Anonymization of Swiss Federal Supreme Court Rulings. (arXiv:2310.04632v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#35009;&#20915;&#30340;&#33258;&#21160;&#21270;&#21311;&#21517;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#65292;&#20351;&#29992;&#39046;&#22495;&#20869;&#25968;&#25454;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;F1&#20998;&#25968;&#36229;&#36807;5%&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#23558;&#29616;&#26377;&#30340;&#21311;&#21517;&#21270;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#21171;&#21160;&#24182;&#22686;&#24378;&#33258;&#21160;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27861;&#38498;&#30340;&#35009;&#20915;&#20844;&#24320;&#38656;&#35201;&#36827;&#34892;&#36866;&#24403;&#30340;&#21311;&#21517;&#21270;&#65292;&#20197;&#20445;&#25252;&#25152;&#26377;&#30456;&#20851;&#26041;&#30340;&#38544;&#31169;&#12290;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#20381;&#38752;&#19968;&#31181;&#24050;&#26377;&#30340;&#31995;&#32479;&#65292;&#23558;&#19981;&#21516;&#30340;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#19982;&#20154;&#24037;&#19987;&#23478;&#32467;&#21512;&#36215;&#26469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#24102;&#26377;&#35201;&#21311;&#21517;&#21270;&#23454;&#20307;&#27880;&#37322;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#30340;&#21311;&#21517;&#21270;&#36719;&#20214;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#21644;&#22312;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#39046;&#22495;&#20869;&#25968;&#25454;&#26469;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;F1&#20998;&#25968;&#36229;&#36807;5&#65285;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#23558;&#29616;&#26377;&#30340;&#21311;&#21517;&#21270;&#26041;&#27861;&#65288;&#22914;&#27491;&#21017;&#34920;&#36798;&#24335;&#65289;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#20154;&#24037;&#21171;&#21160;&#24182;&#22686;&#24378;&#33258;&#21160;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Releasing court decisions to the public relies on proper anonymization to protect all involved parties, where necessary. The Swiss Federal Supreme Court relies on an existing system that combines different traditional computational methods with human experts. In this work, we enhance the existing anonymization software using a large dataset annotated with entities to be anonymized. We compared BERT-based models with models pre-trained on in-domain data. Our results show that using in-domain data to pre-train the models further improves the F1-score by more than 5\% compared to existing models. Our work demonstrates that combining existing anonymization methods, such as regular expressions, with machine learning can further reduce manual labor and enhance automatic suggestions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#35299;&#31163;&#35821;&#38899;&#20013;&#30340;&#21457;&#38899;&#21644;&#20869;&#23481;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;VoxCeleb&#21644;SITW&#25968;&#25454;&#38598;&#19978;&#23545;EER&#21644;minDCF&#26377;&#26126;&#26174;&#30340;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2310.01128</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#35299;&#31163;&#21457;&#38899;&#21644;&#20869;&#23481;&#65292;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Disentangling Voice and Content with Self-Supervision for Speaker Recognition. (arXiv:2310.01128v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#35299;&#31163;&#35821;&#38899;&#20013;&#30340;&#21457;&#38899;&#21644;&#20869;&#23481;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;VoxCeleb&#21644;SITW&#25968;&#25454;&#38598;&#19978;&#23545;EER&#21644;minDCF&#26377;&#26126;&#26174;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#26469;&#35828;&#65292;&#30001;&#20110;&#35821;&#38899;&#20013;&#28151;&#21512;&#20102;&#35828;&#35805;&#20154;&#29305;&#24449;&#21644;&#20869;&#23481;&#65292;&#25552;&#21462;&#20934;&#30830;&#30340;&#35828;&#35805;&#20154;&#34920;&#31034;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#31163;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#27169;&#22411;&#21270;&#35821;&#38899;&#20013;&#30340;&#35828;&#35805;&#20154;&#29305;&#24449;&#21644;&#20869;&#23481;&#30340;&#21464;&#24322;&#24615;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19977;&#20010;&#39640;&#26031;&#25512;&#29702;&#23618;&#23454;&#29616;&#65292;&#27599;&#20010;&#25512;&#29702;&#23618;&#37117;&#30001;&#21487;&#23398;&#20064;&#30340;&#36716;&#25442;&#27169;&#22411;&#32452;&#25104;&#65292;&#29992;&#20110;&#25552;&#21462;&#19981;&#21516;&#30340;&#35821;&#38899;&#25104;&#20998;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20026;&#20102;&#24314;&#27169;&#22797;&#26434;&#30340;&#35821;&#38899;&#21160;&#24577;&#65292;&#19987;&#38376;&#35774;&#35745;&#20102;&#19968;&#20010;&#24378;&#21270;&#30340;&#36716;&#25442;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#38500;&#35828;&#35805;&#20154;&#36523;&#20221;&#20043;&#22806;&#30340;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#21160;&#24577;&#35299;&#31163;&#20869;&#23481;&#12290;&#36890;&#36807;&#22312;VoxCeleb&#21644;SITW&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;EER&#21644;minDCF&#20998;&#21035;&#24179;&#22343;&#38477;&#20302;&#20102;9.56%&#21644;8.24%&#12290;&#30001;&#20110;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#35757;&#32451;&#21644;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#26041;&#20415;&#22320;&#24212;&#29992;&#20110;&#23454;&#38469;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
For speaker recognition, it is difficult to extract an accurate speaker representation from speech because of its mixture of speaker traits and content. This paper proposes a disentanglement framework that simultaneously models speaker traits and content variability in speech. It is realized with the use of three Gaussian inference layers, each consisting of a learnable transition model that extracts distinct speech components. Notably, a strengthened transition model is specifically designed to model complex speech dynamics. We also propose a self-supervision method to dynamically disentangle content without the use of labels other than speaker identities. The efficacy of the proposed framework is validated via experiments conducted on the VoxCeleb and SITW datasets with 9.56% and 8.24% average reductions in EER and minDCF, respectively. Since neither additional model training nor data is specifically needed, it is easily applicable in practical use.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;ACE&#20195;&#29702;&#35299;&#20915;&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#20013;&#30340;CoinRun&#25361;&#25112;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#20027;&#20195;&#29702;&#22312;&#26032;&#29615;&#22659;&#19979;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#26032;&#22870;&#21169;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20851;&#38190;&#24773;&#20917;&#19979;&#21463;&#20154;&#20449;&#20219;&#22320;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2309.16166</link><description>&lt;p&gt;
CoinRun: &#35299;&#20915;&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
CoinRun: Solving Goal Misgeneralisation. (arXiv:2309.16166v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;ACE&#20195;&#29702;&#35299;&#20915;&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#20013;&#30340;CoinRun&#25361;&#25112;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#20027;&#20195;&#29702;&#22312;&#26032;&#29615;&#22659;&#19979;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#26032;&#22870;&#21169;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20851;&#38190;&#24773;&#20917;&#19979;&#21463;&#20154;&#20449;&#20219;&#22320;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#26159;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#20351;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#22815;&#23558;&#20854;&#30446;&#26631;&#19982;&#20154;&#31867;&#24847;&#22270;&#21644;&#36947;&#24503;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ACE&#65288;&#27010;&#24565;&#25193;&#23637;&#31639;&#27861;&#65289;&#20195;&#29702;&#22914;&#20309;&#35299;&#20915;&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#30340;&#19968;&#39033;&#20851;&#38190;&#26631;&#20934;&#25361;&#25112;&#65306;CoinRun&#25361;&#25112;&#12290;&#35813;&#20195;&#29702;&#22312;&#26032;&#29615;&#22659;&#20013;&#19981;&#20351;&#29992;&#20219;&#20309;&#26032;&#30340;&#22870;&#21169;&#20449;&#24687;&#12290;&#36825;&#34920;&#26126;&#33258;&#20027;&#20195;&#29702;&#21487;&#20197;&#22312;&#26032;&#39062;&#21644;&#20851;&#38190;&#30340;&#24773;&#20917;&#19979;&#21463;&#20154;&#20449;&#20219;&#22320;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal misgeneralisation is a key challenge in AI alignment -- the task of getting powerful Artificial Intelligences to align their goals with human intentions and human morality. In this paper, we show how the ACE (Algorithm for Concept Extrapolation) agent can solve one of the key standard challenges in goal misgeneralisation: the CoinRun challenge. It uses no new reward information in the new environment. This points to how autonomous agents could be trusted to act in human interests, even in novel and critical situations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22534;&#21472;&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36275;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;&#20854;&#21152;&#24378;&#20102;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#24207;&#21015;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24182;&#19981;&#33021;&#26681;&#26412;&#35299;&#20915;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13414</link><description>&lt;p&gt;
&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26159;&#24102;&#26377;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#20840;&#33021;&#36924;&#36817;&#22120;
&lt;/p&gt;
&lt;p&gt;
State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory. (arXiv:2309.13414v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22534;&#21472;&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36275;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;&#20854;&#21152;&#24378;&#20102;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#24207;&#21015;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24182;&#19981;&#33021;&#26681;&#26412;&#35299;&#20915;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#31616;&#21333;&#26377;&#25928;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#24207;&#21015;&#24314;&#27169;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#27839;&#26102;&#38388;&#26041;&#21521;&#32570;&#20047;&#38750;&#32447;&#24615;&#28608;&#27963;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22534;&#21472;&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36275;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#28155;&#21152;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#24207;&#21015;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21487;&#20197;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30475;&#21040;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24182;&#19981;&#26681;&#26412;&#35299;&#20915;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#38382;&#39064;&#12290;&#29702;&#35770;&#32467;&#26524;&#32463;&#36807;&#20102;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the exponential decaying memory issue. Theoretical results are justified by numerical verifications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#23569;&#37327;&#25918;&#30005;&#39044;&#27979;&#26410;&#26469;&#25176;&#21345;&#39532;&#20811;&#30340;&#30772;&#35010;&#65292;&#24182;&#24212;&#29992;&#20102;&#29289;&#29702;&#24341;&#23548;&#29305;&#24449;&#25552;&#21462;&#21644;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;J-TEXT&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#30772;&#35010;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05361</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#24341;&#23548;&#29305;&#24449;&#25552;&#21462;&#21644;&#39046;&#22495;&#36866;&#24212;&#30340;&#36328;&#25176;&#21345;&#39532;&#20811;&#30772;&#35010;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross-tokamak Disruption Prediction based on Physics-Guided Feature Extraction and domain adaptation. (arXiv:2309.05361v2 [physics.plasm-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#23569;&#37327;&#25918;&#30005;&#39044;&#27979;&#26410;&#26469;&#25176;&#21345;&#39532;&#20811;&#30340;&#30772;&#35010;&#65292;&#24182;&#24212;&#29992;&#20102;&#29289;&#29702;&#24341;&#23548;&#29305;&#24449;&#25552;&#21462;&#21644;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;J-TEXT&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#30772;&#35010;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#25176;&#21345;&#39532;&#20811;&#20013;&#39640;&#26114;&#30340;&#25968;&#25454;&#33719;&#21462;&#25104;&#26412;&#21644;&#23545;&#30772;&#35010;&#25918;&#30005;&#30340;&#24040;&#22823;&#38656;&#27714;&#32473;&#25968;&#25454;&#39537;&#21160;&#30772;&#35010;&#39044;&#27979;&#27169;&#22411;&#22312;&#30772;&#35010;&#39044;&#27979;&#30740;&#31350;&#20013;&#24102;&#26469;&#20102;&#20869;&#22312;&#30683;&#30462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21482;&#26377;&#23569;&#37327;&#25918;&#30005;&#26469;&#39044;&#27979;&#26410;&#26469;&#25176;&#21345;&#39532;&#20811;&#30340;&#30772;&#35010;&#12290;&#31532;&#19968;&#27493;&#26159;&#21033;&#29992;&#23545;&#21508;&#20010;&#25176;&#21345;&#39532;&#20811;&#30340;&#35786;&#26029;&#20449;&#21495;&#30340;&#29616;&#26377;&#29289;&#29702;&#29702;&#35299;&#26469;&#25552;&#21462;&#29289;&#29702;&#24341;&#23548;&#29305;&#24449;&#65292;&#31216;&#20026;&#29289;&#29702;&#24341;&#23548;&#29305;&#24449;&#25552;&#21462;&#65288;PGFE&#65289;&#12290;&#31532;&#20108;&#27493;&#26159;&#26681;&#25454;&#19968;&#31181;&#31216;&#20026;CORrelation ALignment&#65288;CORAL&#65289;&#30340;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#65292;&#23558;&#26410;&#26469;&#25176;&#21345;&#39532;&#20811;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#30340;&#23569;&#37327;&#25968;&#25454;&#19982;&#29616;&#26377;&#25176;&#21345;&#39532;&#20811;&#65288;&#28304;&#39046;&#22495;&#65289;&#30340;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23581;&#35797;&#23558;&#39046;&#22495;&#36866;&#24212;&#24212;&#29992;&#20110;&#30772;&#35010;&#39044;&#27979;&#20219;&#21153;&#20013;&#12290;PGFE&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;J-TEXT&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#30772;&#35010;&#39044;&#27979;&#24615;&#33021;&#12290;&#30001;&#20110;&#25552;&#21462;&#20102;&#29289;&#29702;&#24341;&#23548;&#29305;&#24449;&#65292;PGFE&#36824;&#21487;&#20197;&#20943;&#23569;&#25968;&#25454;&#37327;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high acquisition cost and the significant demand for disruptive discharges for data-driven disruption prediction models in future tokamaks pose an inherent contradiction in disruption prediction research. In this paper, we demonstrated a novel approach to predict disruption in a future tokamak using only a few discharges. The first step is to use the existing understanding of physics to extract physics-guided features from the diagnostic signals of each tokamak, called physics-guided feature extraction (PGFE). The second step is to align a few data from the future tokamak (target domain) and a large amount of data from existing tokamak (source domain) based on a domain adaptation algorithm called CORrelation ALignment (CORAL). It is the first attempt at applying domain adaptation in the task of disruption prediction. PGFE has been successfully applied in J-TEXT to predict disruption with excellent performance. PGFE can also reduce the data volume requirements due to extracting the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#39550;&#39542;&#20013;&#36816;&#21160;&#39044;&#27979;&#30340;&#39640;&#25928;&#22522;&#32447;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#22320;&#22270;&#21644;&#36807;&#21435;&#36712;&#36857;&#20449;&#24687;&#30340;&#23454;&#26102;&#24212;&#29992;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03387</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#36816;&#21160;&#39044;&#27979;&#30340;&#39640;&#25928;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Efficient Baselines for Motion Prediction in Autonomous Driving. (arXiv:2309.03387v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03387
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#39550;&#39542;&#20013;&#36816;&#21160;&#39044;&#27979;&#30340;&#39640;&#25928;&#22522;&#32447;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#22320;&#22270;&#21644;&#36807;&#21435;&#36712;&#36857;&#20449;&#24687;&#30340;&#23454;&#26102;&#24212;&#29992;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#24847;&#22797;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#20174;&#31616;&#21333;&#26426;&#22120;&#20154;&#21040;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#22810;&#26041;&#21608;&#22260;&#20195;&#29702;&#30340;&#36816;&#21160;&#39044;&#27979;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#24403;&#21069;&#30340;&#25216;&#26415;&#36890;&#36807;&#31471;&#21040;&#31471;&#30340;&#27969;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#36755;&#20837;&#25968;&#25454;&#36890;&#24120;&#26159;&#29289;&#29702;&#20449;&#24687;&#30340;&#28210;&#26579;&#39030;&#35270;&#22270;&#21644;&#26368;&#30456;&#20851;&#20195;&#29702;&#30340;&#36807;&#21435;&#36712;&#36857;&#65307;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26159;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#30340;&#24517;&#38656;&#12290;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#65292;&#21487;&#38752;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#24517;&#39035;&#21450;&#26102;&#20135;&#29983;&#21512;&#29702;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39640;&#31471;&#27169;&#22411;&#21487;&#33021;&#23545;&#20110;&#21516;&#26102;&#20351;&#29992;&#22320;&#22270;&#21644;&#36807;&#21435;&#36712;&#36857;&#36825;&#20004;&#31181;&#20449;&#24687;&#20197;&#21450;&#26497;&#23569;&#21487;&#35299;&#37322;&#24615;&#30340;&#29289;&#29702;&#20449;&#24687;&#30340;&#23454;&#26102;&#24212;&#29992;&#32780;&#35328;&#36807;&#20110;&#22797;&#26434;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#27599;&#20010;&#29305;&#23450;&#20132;&#36890;&#22330;&#26223;&#30340;&#21487;&#29992;&#36755;&#20837;&#25968;&#37327;&#65292;&#32780;&#36825;&#20123;&#36755;&#20837;&#38590;&#20197;&#33719;&#21462;&#65292;&#25104;&#26412;&#39640;&#26114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion Prediction (MP) of multiple surroundings agents is a crucial task in arbitrarily complex environments, from simple robots to Autonomous Driving Stacks (ADS). Current techniques tackle this problem using end-to-end pipelines, where the input data is usually a rendered top-view of the physical information and the past trajectories of the most relevant agents; leveraging this information is a must to obtain optimal performance. In that sense, a reliable ADS must produce reasonable predictions on time. However, despite many approaches use simple ConvNets and LSTMs to obtain the social latent features, State-Of-The-Art (SOTA) models might be too complex for real-time applications when using both sources of information (map and past trajectories) as well as little interpretable, specially considering the physical information. Moreover, the performance of such models highly depends on the number of available inputs for each particular traffic scenario, which are expensive to obtain, pa
&lt;/p&gt;</description></item><item><title>YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00071</link><description>&lt;p&gt;
YaRN: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
YaRN: Efficient Context Window Extension of Large Language Models. (arXiv:2309.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00071
&lt;/p&gt;
&lt;p&gt;
YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;transformer-based&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36229;&#36807;&#23427;&#20204;&#35757;&#32451;&#30340;&#24207;&#21015;&#38271;&#24230;&#26102;&#26080;&#27861;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;YaRN&#65288;Yet another RoPE extensioN method&#65289;&#65292;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#38656;&#35201;&#30340;tokens&#25968;&#37327;&#21644;&#35757;&#32451;&#27493;&#39588;&#23569;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#30340;10&#20493;&#21644;2.5&#20493;&#12290;&#20351;&#29992;YaRN&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLaMA&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#38754;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;YaRN&#20855;&#26377;&#36229;&#36234;&#24494;&#35843;&#25968;&#25454;&#38598;&#26377;&#38480;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/jquesnelle/yarn&#19978;&#21457;&#24067;&#20102;&#20351;&#29992;64k&#21644;128k&#19978;&#19979;&#25991;&#31383;&#21475;&#36827;&#34892;Fine-tuning&#30340;Llama 2 7B/13B&#30340;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned using YaRN with 64k and 128k context windows at https://github.com/jquesnelle/yarn
&lt;/p&gt;</description></item><item><title>CReHate&#36890;&#36807;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#37325;&#26032;&#35780;&#20272;NLP&#30740;&#31350;&#22312;&#20167;&#24680;&#35328;&#35770;&#39046;&#22495;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16705</link><description>&lt;p&gt;
CReHate: &#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset. (arXiv:2308.16705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16705
&lt;/p&gt;
&lt;p&gt;
CReHate&#36890;&#36807;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#37325;&#26032;&#35780;&#20272;NLP&#30740;&#31350;&#22312;&#20167;&#24680;&#35328;&#35770;&#39046;&#22495;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#25968;&#25454;&#38598;&#20027;&#35201;&#21453;&#26144;&#20102;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#25991;&#21270;&#20559;&#24046;&#12290;&#36825;&#22312;&#21463;&#20027;&#35266;&#24615;&#24433;&#21709;&#36739;&#22823;&#30340;&#20219;&#21153;&#65292;&#22914;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#29305;&#21035;&#26377;&#38382;&#39064;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#22914;&#20309;&#29702;&#35299;&#20167;&#24680;&#35328;&#35770;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CReHate&#65292;&#23545;&#25277;&#26679;&#30340;SBIC&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;&#27880;&#37322;&#65306;&#28595;&#22823;&#21033;&#20122;&#12289;&#26032;&#21152;&#22369;&#12289;&#21335;&#38750;&#12289;&#33521;&#22269;&#21644;&#32654;&#22269;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#22269;&#31821;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#21482;&#26377;59.4%&#30340;&#26679;&#26412;&#22312;&#25152;&#26377;&#22269;&#23478;&#20043;&#38388;&#36798;&#25104;&#20849;&#35782;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#22269;&#31821;&#30340;&#35266;&#28857;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20167;&#24680;&#35328;&#35770;&#30340;&#32454;&#24494;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
English datasets predominantly reflect the perspectives of certain nationalities, which can lead to cultural biases in models and datasets. This is particularly problematic in tasks heavily influenced by subjectivity, such as hate speech detection. To delve into how individuals from different countries perceive hate speech, we introduce CReHate, a cross-cultural re-annotation of the sampled SBIC dataset. This dataset includes annotations from five distinct countries: Australia, Singapore, South Africa, the United Kingdom, and the United States. Our thorough statistical analysis highlights significant differences based on nationality, with only 59.4% of the samples achieving consensus among all countries. We also introduce a culturally sensitive hate speech classifier via transfer learning, adept at capturing perspectives of different nationalities. These findings underscore the need to re-evaluate certain aspects of NLP research, especially with regard to the nuanced nature of hate spe
&lt;/p&gt;</description></item><item><title>&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#30446;&#21069;&#23384;&#22312;&#32452;&#32455;&#19981;&#22815;&#26377;&#24207;&#21644;&#35780;&#20272;&#21327;&#35758;&#26377;&#32570;&#38519;&#30340;&#38382;&#39064;&#12290;&#25991;&#31456;&#35780;&#20272;&#20102;&#35768;&#22810;&#26368;&#36817;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25351;&#20986;&#20102;&#38024;&#23545;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#35780;&#20272;&#21327;&#35758;&#23384;&#22312;&#30340;&#38382;&#39064;&#21450;&#22914;&#20309;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.13068</link><description>&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;: &#28843;&#37239;&#31639;&#27861;&#21644;&#26377;&#32570;&#38519;&#30340;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology. (arXiv:2308.13068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13068
&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#30446;&#21069;&#23384;&#22312;&#32452;&#32455;&#19981;&#22815;&#26377;&#24207;&#21644;&#35780;&#20272;&#21327;&#35758;&#26377;&#32570;&#38519;&#30340;&#38382;&#39064;&#12290;&#25991;&#31456;&#35780;&#20272;&#20102;&#35768;&#22810;&#26368;&#36817;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25351;&#20986;&#20102;&#38024;&#23545;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#35780;&#20272;&#21327;&#35758;&#23384;&#22312;&#30340;&#38382;&#39064;&#21450;&#22914;&#20309;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MVTS&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#36817;&#24180;&#26469;&#21560;&#24341;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#22823;&#37327;&#30740;&#31350;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#25991;&#29486;&#30340;&#20180;&#32454;&#30740;&#31350;&#35753;&#25105;&#20204;&#24847;&#35782;&#21040;&#65306;1&#65289;&#35813;&#39046;&#22495;&#30340;&#31038;&#21306;&#27963;&#36291;&#65292;&#20294;&#24182;&#19981;&#20687;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#37027;&#26679;&#32452;&#32455;&#26377;&#24207;&#65307;2&#65289;&#22823;&#22810;&#25968;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20351;&#29992;&#19981;&#21512;&#36866;&#25110;&#23384;&#22312;&#26126;&#26174;&#32570;&#38519;&#30340;&#35780;&#20272;&#21327;&#35758;&#36827;&#34892;&#35780;&#20272;&#65292;&#32570;&#20047;&#31185;&#23398;&#22522;&#30784;&#12290;&#20854;&#20013;&#19968;&#20010;&#38750;&#24120;&#27969;&#34892;&#30340;&#21327;&#35758;&#65292;&#21363;&#25152;&#35859;&#30340; \pa &#21327;&#35758;&#65292;&#26159;&#22914;&#27492;&#26377;&#32570;&#38519;&#65292;&#20197;&#33267;&#20110;&#38543;&#26426;&#29468;&#27979;&#21487;&#20197;&#26174;&#31034;&#31995;&#32479;&#22320;&#20248;&#20110;&#36804;&#20170;&#20026;&#27490;&#24320;&#21457;&#30340;\emph{&#25152;&#26377;}&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26356;&#20581;&#22766;&#30340;&#21327;&#35758;&#23545;&#35768;&#22810;&#26368;&#36817;&#30340;&#31639;&#27861;&#36827;&#34892;&#22238;&#39038;&#21644;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#22312;MVTS&#24322;&#24120;&#26816;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#19968;&#20010;&#26412;&#26469;&#24456;&#22909;&#30340;&#21327;&#35758;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#20197;&#21450;&#22914;&#20309;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#34920;&#36798;&#20102;&#20851;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time Series (MVTS) anomaly detection is a long-standing and challenging research topic that has attracted tremendous research effort from both industry and academia recently. However, a careful study of the literature makes us realize that 1) the community is active but not as organized as other sibling machine learning communities such as Computer Vision (CV) and Natural Language Processing (NLP), and 2) most proposed solutions are evaluated using either inappropriate or highly flawed protocols, with an apparent lack of scientific foundation. So flawed is one very popular protocol, the so-called \pa protocol, that a random guess can be shown to systematically outperform \emph{all} algorithms developed so far. In this paper, we review and evaluate many recent algorithms using more robust protocols and discuss how a normally good protocol may have weaknesses in the context of MVTS anomaly detection and how to mitigate them. We also share our concerns about benchmark dataset
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALGAN&#30340;&#26032;&#22411;GAN&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;LSTM&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#20854;&#20182;GAN&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.06663</link><description>&lt;p&gt;
ALGAN&#65306;&#20855;&#26377;&#35843;&#25972;&#30340;LSTM GAN&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN. (arXiv:2308.06663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06663
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALGAN&#30340;&#26032;&#22411;GAN&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;LSTM&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#20854;&#20182;GAN&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#21508;&#20010;&#39046;&#22495;&#65288;&#22914;&#21046;&#36896;&#19994;&#65292;&#21307;&#23398;&#25104;&#20687;&#21644;&#32593;&#32476;&#23433;&#20840;&#65289;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#28857;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;GANs&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65288;&#21363;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#65289;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#27169;&#22411;&#65292;&#21517;&#20026;Adjusted-LSTM GAN&#65288;ALGAN&#65289;&#65292;&#23427;&#35843;&#25972;LSTM&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#20197;&#25552;&#39640;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#65292;&#32780;&#19988;&#26159;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#22312;46&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#30340;&#22823;&#22411;&#22810;&#21464;&#37327;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;ALGAN&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ALGAN&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#12289;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21644;&#20854;&#20182;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in time series data, to identify points that deviate from normal behaviour, is a common problem in various domains such as manufacturing, medical imaging, and cybersecurity. Recently, Generative Adversarial Networks (GANs) are shown to be effective in detecting anomalies in time series data. The neural network architecture of GANs (i.e. Generator and Discriminator) can significantly improve anomaly detection accuracy. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting. We evaluate the performance of ALGAN on 46 real-world univariate time series datasets and a large multivariate dataset that spans multiple domains. Our experiments demonstrate that ALGAN outperforms traditional, neural network-based, and other GAN-based methods for anomaly detection in time series data.
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#27491;&#30830;&#30340;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#30340;&#20915;&#31574;&#34892;&#20026;&#65292;&#20197;&#22686;&#36827;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#24615;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.13566</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#24615;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Imperfect XAI on Human-AI Decision-Making. (arXiv:2307.13566v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#27491;&#30830;&#30340;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#30340;&#20915;&#31574;&#34892;&#20026;&#65292;&#20197;&#22686;&#36827;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#24615;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#25216;&#26415;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#20197;&#25913;&#36827;&#21508;&#31181;&#21512;&#20316;&#24037;&#20316;&#29615;&#22659;&#19979;&#30340;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#20915;&#31574;&#32773;&#19982;&#19981;&#23436;&#32654;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21512;&#36866;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20219;&#21153;&#34920;&#29616;&#65292;&#20197;&#20415;&#35774;&#35745;&#26356;&#21152;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#26426;&#25903;&#25345;&#30340;&#21327;&#20316;&#24037;&#20855;&#12290;&#19968;&#20123;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#65292;&#24076;&#26395;&#25913;&#21892;&#20915;&#31574;&#32773;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#21512;&#20316;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#22522;&#20110;&#20808;&#21069;&#30740;&#31350;&#30340;&#21457;&#29616;&#65292;&#20027;&#35201;&#20851;&#27880;&#38169;&#35823;&#30340;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#30340;&#24433;&#21709;&#12290;&#24456;&#23569;&#26377;&#30740;&#31350;&#25215;&#35748;&#21363;&#20351;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#27491;&#30830;&#65292;&#35299;&#37322;&#20063;&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#19981;&#23436;&#32654;&#30340;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#24378;&#22823;&#30340;&#28151;&#21512;&#26041;&#27861;&#29992;&#25143;&#30740;&#31350;&#65292;&#28041;&#21450;136&#21517;&#21442;&#19982;&#32773;&#65292;&#35780;&#20272;&#20102;&#19981;&#27491;&#30830;&#30340;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility for the explanations to be incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making beha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12169</link><description>&lt;p&gt;
&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Optimized Network Architectures for Large Language Model Training with Billions of Parameters. (arXiv:2307.12169v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;&#20026;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#20219;&#24847;&#21040;&#20219;&#24847;&#32593;&#32476;&#30340;&#20256;&#32479;&#33539;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21576;&#29616;&#20986;&#19968;&#31181;&#29420;&#29305;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#22312;&#20854;&#20013;&#65292;&#21482;&#26377;&#23567;&#32452;&#30340;GPU&#38656;&#35201;&#39640;&#24102;&#23485;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;&#36890;&#20449;&#65292;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;GPU&#23567;&#32452;&#20043;&#38388;&#65292;&#36890;&#20449;&#38750;&#24120;&#24494;&#19981;&#36275;&#36947;&#12289;&#31232;&#30095;&#19988;&#22343;&#21248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#32039;&#23494;&#21305;&#37197;LLMs&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#38598;&#32676;&#20998;&#21106;&#20026;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#20219;&#24847;&#21040;&#20219;&#24847;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;HB&#22495;&#12290;&#22312;HB&#22495;&#20043;&#38388;&#65292;&#32593;&#32476;&#21482;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32593;&#32476;&#36830;&#25509;&#31216;&#20026;&#8220;&#20165;&#36712;&#36947;&#36830;&#25509;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;Clos&#32593;&#32476;&#21487;&#20197;&#23558;&#32593;&#32476;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;LLM&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20316;&#35760;&#24518;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;WorM&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;4&#20010;&#21151;&#33021;&#12289;3&#20010;&#39046;&#22495;&#21644;11&#20010;&#34892;&#20026;&#21644;&#31070;&#32463;&#29305;&#24449;&#30340;WM&#20219;&#21153;&#26469;&#24320;&#21457;&#21644;&#35780;&#20272;AI WM&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#20986;&#33041;&#20013;&#24037;&#20316;&#35760;&#24518;&#30340;&#19968;&#20123;&#29305;&#24449;&#65292;&#22914;&#20248;&#21183;&#25928;&#24212;&#21644;&#26368;&#26032;&#24615;&#25928;&#24212;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#21151;&#33021;&#30340;&#24037;&#20316;&#35760;&#24518;&#30340;&#31070;&#32463;&#32676;&#38598;&#21644;&#30456;&#20851;&#29289;&#12290;</title><link>http://arxiv.org/abs/2307.10768</link><description>&lt;p&gt;
&#35299;&#30721;&#35868;&#22242;&#65306;&#22312;&#24037;&#20316;&#35760;&#24518;&#30340;&#22810;&#20010;&#26041;&#38754;&#19978;&#23545;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory. (arXiv:2307.10768v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20316;&#35760;&#24518;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;WorM&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;4&#20010;&#21151;&#33021;&#12289;3&#20010;&#39046;&#22495;&#21644;11&#20010;&#34892;&#20026;&#21644;&#31070;&#32463;&#29305;&#24449;&#30340;WM&#20219;&#21153;&#26469;&#24320;&#21457;&#21644;&#35780;&#20272;AI WM&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#20986;&#33041;&#20013;&#24037;&#20316;&#35760;&#24518;&#30340;&#19968;&#20123;&#29305;&#24449;&#65292;&#22914;&#20248;&#21183;&#25928;&#24212;&#21644;&#26368;&#26032;&#24615;&#25928;&#24212;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#21151;&#33021;&#30340;&#24037;&#20316;&#35760;&#24518;&#30340;&#31070;&#32463;&#32676;&#38598;&#21644;&#30456;&#20851;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#35760;&#24518;&#65288;WM&#65289;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#23427;&#20419;&#36827;&#20102;&#20449;&#24687;&#30340;&#20020;&#26102;&#23384;&#20648;&#12289;&#25972;&#21512;&#12289;&#25805;&#20316;&#21644;&#26816;&#32034;&#65292;&#22312;&#25512;&#29702;&#21644;&#20915;&#31574;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25429;&#25417;&#24037;&#20316;&#35760;&#24518;&#22810;&#26041;&#38754;&#29305;&#24449;&#30340;&#21487;&#38752;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;&#20110;&#26377;&#25928;&#22320;&#24320;&#21457;&#21644;&#35780;&#20272;AI&#24037;&#20316;&#35760;&#24518;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20316;&#35760;&#24518;&#65288;WorM&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;WorM&#21253;&#25324;10&#20010;&#20219;&#21153;&#21644;&#24635;&#20849;100&#19975;&#27425;&#35797;&#39564;&#65292;&#35780;&#20272;&#20102;WM&#30340;4&#20010;&#21151;&#33021;&#12289;3&#20010;&#39046;&#22495;&#21644;11&#20010;&#34892;&#20026;&#21644;&#31070;&#32463;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#19978;&#20849;&#21516;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#26368;&#20808;&#36827;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#12290;&#25105;&#20204;&#36824;&#21253;&#25324;&#20154;&#31867;&#34892;&#20026;&#22522;&#20934;&#20316;&#20026;&#23545;&#27604;&#30340;&#19978;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#27169;&#22411;&#27169;&#25311;&#20102;&#33041;&#20013;&#24037;&#20316;&#35760;&#24518;&#30340;&#19968;&#20123;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;&#20248;&#21183;&#25928;&#24212;&#21644;&#26368;&#26032;&#24615;&#25928;&#24212;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#21151;&#33021;&#24615;&#30340;&#24037;&#20316;&#35760;&#24518;&#30340;&#31070;&#32463;&#32676;&#38598;&#21644;&#30456;&#20851;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. We also include human behavioral benchmarks as an upper bound for comparison. Our results suggest that AI models replicate some characteristics of WM in the brain, most notably primacy and recency effects, and neural clusters and correlates specialized for different domains and functionalities of WM
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#27874;&#27573;&#21453;&#39304;&#30340;&#22312;&#32447;&#20803;&#23398;&#20064;&#65292;&#24182;&#35774;&#35745;&#20102;&#29992;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#36172;&#21338;&#32447;&#24615;&#20248;&#21270;&#30340;&#20803;&#31639;&#27861;&#12290;&#23545;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#31639;&#27861;&#20351;&#29992;&#20102;Tsallis-&#29109;&#30340;&#27867;&#21270;Exp3&#65292;&#24182;&#19988;&#20219;&#21153;&#24179;&#22343;&#36951;&#25022;&#20250;&#38543;&#30528;&#26368;&#20248;&#35299;&#30340;&#29109;&#30340;&#20943;&#23567;&#32780;&#25913;&#21892;&#12290;&#23545;&#20110;&#36172;&#21338;&#32447;&#24615;&#20248;&#21270;&#65292;&#31639;&#27861;&#20351;&#29992;&#20102;&#33258;&#21327;&#35843;&#38556;&#30861;&#27491;&#21017;&#21270;&#22120;&#21021;&#22987;&#21270;&#21644;&#35843;&#25972;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65292;&#24182;&#19988;&#20219;&#21153;&#24179;&#22343;&#36951;&#25022;&#19982;&#21160;&#20316;&#31354;&#38388;&#30456;&#20851;&#30340;&#24230;&#37327;&#30452;&#25509;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.02295</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#23545;&#25239;&#27874;&#27573;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Adversarial Bandit Algorithms. (arXiv:2307.02295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#27874;&#27573;&#21453;&#39304;&#30340;&#22312;&#32447;&#20803;&#23398;&#20064;&#65292;&#24182;&#35774;&#35745;&#20102;&#29992;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#36172;&#21338;&#32447;&#24615;&#20248;&#21270;&#30340;&#20803;&#31639;&#27861;&#12290;&#23545;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#31639;&#27861;&#20351;&#29992;&#20102;Tsallis-&#29109;&#30340;&#27867;&#21270;Exp3&#65292;&#24182;&#19988;&#20219;&#21153;&#24179;&#22343;&#36951;&#25022;&#20250;&#38543;&#30528;&#26368;&#20248;&#35299;&#30340;&#29109;&#30340;&#20943;&#23567;&#32780;&#25913;&#21892;&#12290;&#23545;&#20110;&#36172;&#21338;&#32447;&#24615;&#20248;&#21270;&#65292;&#31639;&#27861;&#20351;&#29992;&#20102;&#33258;&#21327;&#35843;&#38556;&#30861;&#27491;&#21017;&#21270;&#22120;&#21021;&#22987;&#21270;&#21644;&#35843;&#25972;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65292;&#24182;&#19988;&#20219;&#21153;&#24179;&#22343;&#36951;&#25022;&#19982;&#21160;&#20316;&#31354;&#38388;&#30456;&#20851;&#30340;&#24230;&#37327;&#30452;&#25509;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20855;&#26377;&#27874;&#27573;&#21453;&#39304;&#30340;&#22312;&#32447;&#20803;&#23398;&#20064;&#65292;&#30446;&#26631;&#26159;&#22312;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#25913;&#21892;&#24615;&#33021;&#65292;&#22914;&#26524;&#23427;&#20204;&#26681;&#25454;&#26576;&#20010;&#33258;&#28982;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26159;&#30456;&#20284;&#30340;&#12290;&#20316;&#20026;&#38024;&#23545;&#25932;&#23545;&#30340;&#22312;&#32447;&#37096;&#20998;&#20449;&#24687;&#35774;&#32622;&#30340;&#39318;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20803;&#31639;&#27861;&#65292;&#23558;&#22806;&#23618;&#23398;&#20064;&#22120;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#20026;&#20004;&#31181;&#37325;&#35201;&#24773;&#20917;&#35843;&#25972;&#20869;&#37096;&#23398;&#20064;&#22120;&#30340;&#21021;&#22987;&#21270;&#21644;&#20854;&#20182;&#36229;&#21442;&#25968;&#65306;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#21644;&#36172;&#21338;&#32447;&#24615;&#20248;&#21270;&#65288;BLO&#65289;&#12290;&#23545;&#20110;MAB&#65292;&#20803;&#23398;&#20064;&#22120;&#20351;&#29992;Tsallis-&#29109;&#30340;&#27867;&#21270;Exp3&#30340;&#21021;&#22987;&#21270;&#21644;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#22914;&#26524;&#21518;&#35265;&#20043;&#39640;&#23792;&#30340;&#29109;&#23567;&#65292;&#21017;&#20219;&#21153;&#24179;&#22343;&#36951;&#25022;&#25913;&#21892;&#12290;&#23545;&#20110;BLO&#65292;&#25105;&#20204;&#23398;&#20250;&#20102;&#20351;&#29992;&#33258;&#21327;&#35843;&#38556;&#30861;&#27491;&#21017;&#21270;&#22120;&#21021;&#22987;&#21270;&#21644;&#35843;&#25972;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65288;OMD&#65289;&#65292;&#34920;&#26126;&#20219;&#21153;&#24179;&#22343;&#36951;&#25022;&#19982;&#20854;&#24341;&#36215;&#30340;&#21160;&#20316;&#31354;&#38388;&#30456;&#20851;&#30340;&#24230;&#37327;&#30452;&#25509;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#20445;&#35777;&#22522;&#20110;&#35777;&#26126;&#26080;&#27491;&#35268;&#21270;&#36319;&#38543;&#32773;&#19982;&#20004;&#20010;&#8230;
&lt;/p&gt;
&lt;p&gt;
We study online meta-learning with bandit feedback, with the goal of improving performance across multiple tasks if they are similar according to some natural similarity measure. As the first to target the adversarial online-within-online partial-information setting, we design meta-algorithms that combine outer learners to simultaneously tune the initialization and other hyperparameters of an inner learner for two important cases: multi-armed bandits (MAB) and bandit linear optimization (BLO). For MAB, the meta-learners initialize and set hyperparameters of the Tsallis-entropy generalization of Exp3, with the task-averaged regret improving if the entropy of the optima-in-hindsight is small. For BLO, we learn to initialize and tune online mirror descent (OMD) with self-concordant barrier regularizers, showing that task-averaged regret varies directly with an action space-dependent measure they induce. Our guarantees rely on proving that unregularized follow-the-leader combined with two 
&lt;/p&gt;</description></item><item><title>NNQS-Transformer&#26159;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22836;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;&#12290;&#20854;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;Transformer&#30340;&#37327;&#23376;&#27874;&#20989;&#25968;&#23433;&#33832;&#33576;&#12289;&#25968;&#25454;&#20013;&#24515;&#24182;&#34892;&#21270;&#26041;&#26696;&#12289;&#24182;&#34892;&#25209;&#37327;&#37319;&#26679;&#31574;&#30053;&#21644;&#24182;&#34892;&#23616;&#22495;&#33021;&#37327;&#35780;&#20272;&#26041;&#26696;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#31934;&#24230;&#21644;&#23545;&#20110;&#22823;&#20998;&#23376;&#31995;&#32479;&#30340;&#24378;&#21487;&#25193;&#23637;&#24615;&#21644;&#24369;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16705</link><description>&lt;p&gt;
NNQS-Transformer: &#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#26041;&#27861;&#29992;&#20110;&#20174;&#22836;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;
&lt;/p&gt;
&lt;p&gt;
NNQS-Transformer: an Efficient and Scalable Neural Network Quantum States Approach for Ab initio Quantum Chemistry. (arXiv:2306.16705v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16705
&lt;/p&gt;
&lt;p&gt;
NNQS-Transformer&#26159;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22836;&#35745;&#31639;&#37327;&#23376;&#21270;&#23398;&#12290;&#20854;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;Transformer&#30340;&#37327;&#23376;&#27874;&#20989;&#25968;&#23433;&#33832;&#33576;&#12289;&#25968;&#25454;&#20013;&#24515;&#24182;&#34892;&#21270;&#26041;&#26696;&#12289;&#24182;&#34892;&#25209;&#37327;&#37319;&#26679;&#31574;&#30053;&#21644;&#24182;&#34892;&#23616;&#22495;&#33021;&#37327;&#35780;&#20272;&#26041;&#26696;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#31934;&#24230;&#21644;&#23545;&#20110;&#22823;&#20998;&#23376;&#31995;&#32479;&#30340;&#24378;&#21487;&#25193;&#23637;&#24615;&#21644;&#24369;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#65288;NNQS&#65289;&#24050;&#25104;&#20026;&#37327;&#23376;&#22810;&#20307;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#27861;&#65292;&#20294;&#20854;&#23454;&#38469;&#24212;&#29992;&#24120;&#21463;&#21040;&#37319;&#26679;&#21644;&#23616;&#22495;&#33021;&#37327;&#35745;&#31639;&#30340;&#39640;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;NNQS&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22836;&#30005;&#23376;&#32467;&#26500;&#35745;&#31639;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23558;Transformer&#20316;&#20026;&#37327;&#23376;&#27874;&#20989;&#25968;&#30340;&#23433;&#33832;&#33576;&#65307;&#65288;2&#65289;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#24182;&#34892;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#65288;VMC&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#25345;&#25968;&#25454;&#30340;&#23616;&#37096;&#24615;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#35745;&#31639;&#26550;&#26500;&#65307;&#65288;3&#65289;&#19968;&#31181;&#24182;&#34892;&#25209;&#37327;&#37319;&#26679;&#31574;&#30053;&#65292;&#38477;&#20302;&#37319;&#26679;&#25104;&#26412;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#36127;&#36733;&#24179;&#34913;&#65307;&#65288;4&#65289;&#19968;&#31181;&#26082;&#20855;&#26377;&#20869;&#23384;&#21448;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#30340;&#24182;&#34892;&#23616;&#22495;&#33021;&#37327;&#35780;&#20272;&#26041;&#26696;&#65307;&#65288;5&#65289;&#23545;&#30495;&#23454;&#21270;&#23398;&#31995;&#32479;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#39640;&#36798;120&#20010;&#33258;&#26059;&#30340;&#22823;&#20998;&#23376;&#31995;&#32479;&#20855;&#26377;&#24456;&#24378;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24369;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network quantum state (NNQS) has emerged as a promising candidate for quantum many-body problems, but its practical applications are often hindered by the high cost of sampling and local energy calculation. We develop a high-performance NNQS method for \textit{ab initio} electronic structure calculations. The major innovations include: (1) A transformer based architecture as the quantum wave function ansatz; (2) A data-centric parallelization scheme for the variational Monte Carlo (VMC) algorithm which preserves data locality and well adapts for different computing architectures; (3) A parallel batch sampling strategy which reduces the sampling cost and achieves good load balance; (4) A parallel local energy evaluation scheme which is both memory and computationally efficient; (5) Study of real chemical systems demonstrates both the superior accuracy of our method compared to state-of-the-art and the strong and weak scalability for large molecular systems with up to $120$ spin o
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.15749</link><description>&lt;p&gt;
&#20309;&#21435;&#20309;&#20174;&#65306;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#30340;&#25968;&#23383;&#30828;&#20214;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15749
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#22312;&#28085;&#30422;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#31454;&#20105;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#25928;&#29575;&#20026;&#20195;&#20215;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#33021;&#21147;&#12290;&#29983;&#29289;&#33041;&#30340;&#21151;&#32791;&#25928;&#29575;&#36229;&#36807;&#20219;&#20309;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65307;&#22240;&#27492;&#65292;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#35797;&#22270;&#27169;&#20223;&#33041;&#37096;&#25805;&#20316;&#65292;&#20363;&#22914;&#22522;&#20110;&#33033;&#20914;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;DL&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#33041;&#37096;&#26377;&#35832;&#22914;&#39640;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#12289;&#23494;&#38598;&#30340;&#31070;&#32463;&#20803;&#36830;&#25509;&#21644;&#35745;&#31639;&#19982;&#23384;&#20648;&#30340;&#20849;&#21516;&#20301;&#32622;&#31561;&#20248;&#21183;&#65292;&#20294;&#21487;&#29992;&#30340;&#29983;&#29289;&#22522;&#24213;&#20005;&#37325;&#38480;&#21046;&#20102;&#29983;&#29289;&#22823;&#33041;&#30340;&#36827;&#21270;&#12290;&#30005;&#23376;&#30828;&#20214;&#27809;&#26377;&#30456;&#21516;&#30340;&#32422;&#26463;&#65307;&#22240;&#27492;&#65292;&#34429;&#28982;&#24314;&#27169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21487;&#33021;&#25581;&#31034;&#20102;&#19968;&#20010;&#35868;&#39064;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Caus-Modens&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#65292;&#30456;&#27604;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.09520</link><description>&lt;p&gt;
&#38024;&#23545;&#28508;&#22312;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#32467;&#26524;&#30340;&#26356;&#32039;&#23494;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Tighter Prediction Intervals for Causal Outcomes Under Hidden Confounding. (arXiv:2306.09520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Caus-Modens&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#65292;&#30456;&#27604;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30830;&#20999;&#20010;&#20307;&#27835;&#30103;&#32467;&#26524;&#30340;&#22240;&#26524;&#25512;&#26029;&#24456;&#23569;&#21487;&#33021;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25913;&#36827;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#32467;&#26524;&#21306;&#38388;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31867;&#26041;&#27861;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#65292;&#26377;&#26102;&#20250;&#32473;&#20986;&#26080;&#20449;&#24687;&#37327;&#30340;&#21306;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21478;&#31867;&#26041;&#27861;Caus-Modens&#65292;&#29992;&#20110;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#12290;&#21463;&#21040;&#36125;&#21494;&#26031;&#32479;&#35745;&#21644;&#38598;&#25104;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#21551;&#21457;&#65292;Caus-Modens&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#20998;&#31163;&#22522;&#20934;&#27979;&#35797;&#30340;&#24517;&#35201;&#21306;&#38388;&#22823;&#23567;&#26469;&#23454;&#29616;&#36275;&#22815;&#30340;&#35206;&#30422;&#29575;&#12290;&#26368;&#21518;&#19968;&#20010;&#22522;&#20934;&#26159;&#20351;&#29992;&#26410;&#30693;&#20294;&#21487;&#25506;&#26126;&#30340;&#22522;&#30784;&#20107;&#23454;&#24320;&#23637;&#35266;&#23519;&#23454;&#39564;&#30340;GPT-4&#30340;&#26032;&#22411;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference of exact individual treatment outcomes in the presence of hidden confounders is rarely possible. Instead, recent work has adapted conformal prediction to produce outcome intervals. Unfortunately this family of methods tends to be overly conservative, sometimes giving uninformative intervals. We introduce an alternative approach termed Caus-Modens, for characterizing causal outcome intervals by modulated ensembles. Motivated from Bayesian statistics and ensembled uncertainty quantification, Caus-Modens gives tighter outcome intervals in practice, measured by the necessary interval size to achieve sufficient coverage on three separate benchmarks. The last benchmark is a novel usage of GPT-4 for observational experiments with unknown but probeable ground truth.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.04723</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#20869;&#37096;&#32500;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#25552;&#39640;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#23646;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#21363;&#32473;&#23450;&#25991;&#26412;&#26679;&#26412;&#23884;&#20837;&#38598;&#21512;&#19979;&#30340;&#27969;&#24418;&#30340;&#20869;&#37096;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#27969;&#30021;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#22312;&#20960;&#20010;&#22522;&#20110;&#23383;&#27597;&#30340;&#35821;&#35328;&#20013;&#32422;&#20026; $9$&#65292;&#32780;&#20013;&#25991;&#32422;&#20026; $7$&#65292;&#32780;&#27599;&#31181;&#35821;&#35328;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#36739;&#20302;&#65292;&#24046;&#32422; $1.5$&#65292;&#24182;&#19988;&#26377;&#26126;&#26174;&#30340;&#32479;&#35745;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#24418;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#26469;&#32852;&#21512;&#20248;&#21270;&#23646;&#24615;&#20197;&#33719;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;LECI&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01103</link><description>&lt;p&gt;
&#22312;&#22270;&#24418;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#20013;&#23398;&#20064;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization. (arXiv:2306.01103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#24418;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#26469;&#32852;&#21512;&#20248;&#21270;&#23646;&#24615;&#20197;&#33719;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;LECI&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22270;&#24418;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22270;&#24418;OOD&#31639;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#21463;&#38480;&#30340;&#20551;&#35774;&#65292;&#35201;&#20040;&#26080;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21516;&#26102;&#32435;&#20837;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#65288;LECI&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#21644;&#29615;&#22659;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#22240;&#26524;&#21644;&#19981;&#21464;&#23376;&#22270;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#32852;&#21512;&#20248;&#21270;&#36825;&#20004;&#20010;&#23646;&#24615;&#65292;&#29992;&#20110;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#23548;&#33268;&#23376;&#22270;&#21457;&#29616;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;LECI&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#37117;&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#23558;LECI&#30830;&#31435;&#20026;&#22270;&#24418;OOD&#27867;&#21270;&#30340;&#23454;&#29992;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of graph out-of-distribution (OOD) generalization. Existing graph OOD algorithms either rely on restricted assumptions or fail to exploit environment information in training data. In this work, we propose to simultaneously incorporate label and environment causal independence (LECI) to fully make use of label and environment information, thereby addressing the challenges faced by prior methods on identifying causal and invariant subgraphs. We further develop an adversarial training strategy to jointly optimize these two properties for casual subgraph discovery with theoretical guarantees. Extensive experiments and analysis show that LECI significantly outperforms prior methods on both synthetic and real-world datasets, establishing LECI as a practical and effective solution for graph OOD generalization.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;AI&#21327;&#21516;&#21512;&#20316;&#30340;&#21382;&#21490;&#21644;&#35201;&#27714;&#65292;&#26159;&#21327;&#21516;AI&#30740;&#31350;&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.12040</link><description>&lt;p&gt;
&#21327;&#21516;&#20154;&#24037;&#26234;&#33021;&#30340;&#26681;&#28304;&#21644;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Roots and Requirements for Collaborative AI. (arXiv:2303.12040v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12040
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;AI&#21327;&#21516;&#21512;&#20316;&#30340;&#21382;&#21490;&#21644;&#35201;&#27714;&#65292;&#26159;&#21327;&#21516;AI&#30740;&#31350;&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21327;&#20316;&#32773;&#30340;&#24895;&#26223;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#31185;&#24187;&#23567;&#35828;&#30340;&#32463;&#20856;&#32032;&#26448;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#29702;&#35299;&#21327;&#20316;&#21644;&#20154;&#31867;&#27807;&#36890;&#30340;&#24494;&#22937;&#24046;&#21035;&#12290;&#23427;&#20204;&#36890;&#36807;&#36129;&#29486;&#29305;&#27530;&#30340;&#25165;&#33021;&#32473;&#20182;&#20204;&#30340;&#20154;&#31867;&#21512;&#20316;&#32773;&#21644;&#22242;&#38431;&#24102;&#26469;&#20248;&#21183;&#12290;&#22810;&#24180;&#26469;&#65292;&#25919;&#24220;&#21672;&#35810;&#22242;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#39046;&#34966;&#19968;&#30452;&#20513;&#23548;AIs&#24212;&#35813;&#20855;&#26377;&#20154;&#31867;&#20860;&#23481;&#24615;&#21644;&#26377;&#25928;&#21327;&#20316;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20855;&#22791;&#20687;&#25165;&#21326;&#27178;&#28322;&#30340;&#20154;&#37027;&#26679;&#21327;&#20316;&#33021;&#21147;&#30340;&#24378;&#22823;&#30340;AI&#20173;&#28982;&#36965;&#19981;&#21487;&#21450;&#12290;&#36825;&#31687;&#35770;&#25991;&#20381;&#25454;&#23545;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#20195;&#29702;&#26377;&#25928;&#21644;&#24378;&#22823;&#21327;&#20316;&#25152;&#38656;&#35748;&#30693;&#30340;&#20998;&#26512;&#65292;&#27010;&#36848;&#20102;&#20844;&#20247;&#21644;AI&#24895;&#26223;&#20013;&#20851;&#20110;&#20154;&#24037;&#21327;&#20316;&#32773;&#30340;&#21382;&#21490;&#65292;&#24320;&#22987;&#20110;&#26089;&#26399;&#26234;&#33021;&#22686;&#24378;(IA)&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#24895;&#26223;&#12290;&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#25104;&#20026;&#21327;&#21516;AI&#30340;&#31532;&#20108;&#20010;&#31435;&#22330;&#25991;&#20214;(Stefik &amp; Price, 2023)&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;&#31532;&#20108;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#22810;&#23398;&#31185;&#30340;&#29616;&#29366;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#21327;&#20316;&#30740;&#31350;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vision of AI collaborators has long been a staple of science fiction, where artificial agents understand nuances of collaboration and human communication. They bring advantages to their human collaborators and teams by contributing their special talents. Government advisory groups and leaders in AI have advocated for years that AIs should be human compatible and be capable of effective collaboration. Nonetheless, robust AIs that can collaborate like talented people remain out of reach. This position paper draws on a cognitive analysis of what effective and robust collaboration requires of human and artificial agents. It sketches a history of public and AI visions for artificial collaborators, starting with early visions of intelligence augmentation (IA) and artificial intelligence (AI). It is intended as motivation and context for a second position paper on collaborative AI (Stefik &amp; Price, 2023). The second paper reviews the multi-disciplinary state-of-the-art and proposes a roadm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39640;&#32500;Transformer&#30340;&#26550;&#26500;HDformer&#65292;&#24182;&#21033;&#29992;&#38271;&#36317;&#31163;PPG&#20449;&#21495;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;TSA&#65292;&#25104;&#21151;&#23558;&#26631;&#35760;&#20307;&#31215;&#20943;&#23569;10&#20493;&#20197;&#19978;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.11340</link><description>&lt;p&gt;
HDformer: &#19968;&#31181;&#21033;&#29992;&#38271;&#36317;&#31163;&#34880;&#31649;&#20449;&#21495;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#30340;&#39640;&#32500;Transformer
&lt;/p&gt;
&lt;p&gt;
HDformer: A Higher Dimensional Transformer for Diabetes Detection Utilizing Long Range Vascular Signals. (arXiv:2303.11340v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39640;&#32500;Transformer&#30340;&#26550;&#26500;HDformer&#65292;&#24182;&#21033;&#29992;&#38271;&#36317;&#31163;PPG&#20449;&#21495;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;TSA&#65292;&#25104;&#21151;&#23558;&#26631;&#35760;&#20307;&#31215;&#20943;&#23569;10&#20493;&#20197;&#19978;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#20840;&#29699;&#24615;&#38382;&#39064;&#65292;&#26089;&#26399;&#26816;&#27979;&#26377;&#21161;&#20110;&#39044;&#38450;&#20005;&#37325;&#24182;&#21457;&#30151;&#12290;&#24050;&#20986;&#29616;&#23558;&#24515;&#34880;&#31649;&#20449;&#21495;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20302;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24335;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#38480;&#21046;&#20854;&#20020;&#24202;&#24212;&#29992;&#30340;&#26159;&#26377;&#38480;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#21363;Higher Dimensional Transformer&#65288;HDformer&#65289;&#65292;&#23427;&#21033;&#29992;&#38271;&#36317;&#31163;&#20809;&#30005;&#23481;&#31215;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#26469;&#26816;&#27979;&#31958;&#23615;&#30149;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#30740;&#31350;&#24120;&#29992;&#30340;&#19981;&#36275;&#19968;&#20998;&#38047;&#30340;PPG&#20449;&#21495;&#65292;&#38271;&#36317;&#31163;PPG&#21253;&#21547;&#26356;&#24191;&#27867;&#12289;&#26356;&#28145;&#20837;&#30340;&#20449;&#21495;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#22686;&#21152;&#22788;&#29702;&#38271;&#36317;&#31163;&#25968;&#25454;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;Time Square Attention&#65288;TSA&#65289;&#65292;&#23558;&#26631;&#35760;&#20307;&#31215;&#20943;&#23569;10&#20493;&#20197;&#19978;&#65292;&#21516;&#26102;&#20445;&#30041;&#26412;&#22320;/&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#23427;&#23558;&#19968;&#32500;&#36755;&#20837; &#36716;&#25442;&#20026;&#20108;&#32500;&#34920;&#31034;&#65292;&#24182;&#23558;&#30456;&#37051;&#28857;&#32452;&#25104;&#19968;&#20010;&#21333;&#29420;&#30340;2D&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diabetes mellitus is a worldwide concern, and early detection can help to prevent serious complications. Low-cost, non-invasive detection methods, which take cardiovascular signals into deep learning models, have emerged. However, limited accuracy constrains their clinical usage. In this paper, we present a new Transformer-based architecture, Higher Dimensional Transformer (HDformer), which takes long-range photoplethysmography (PPG) signals to detect diabetes. The long-range PPG contains broader and deeper signal contextual information compared to the less-than-one-minute PPG signals commonly utilized in existing research. To increase the capability and efficiency of processing the long range data, we propose a new attention module Time Square Attention (TSA), reducing the volume of the tokens by more than 10x, while retaining the local/global dependencies. It converts the 1-dimensional inputs into 2-dimensional representations and groups adjacent points into a single 2D token, using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.06530</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#25209;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.
&lt;/p&gt;
&lt;p&gt;
&#25209;&#26631;&#20934;&#21270;&#65288;BN&#65289;&#36890;&#24120;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#24182;&#21152;&#36895;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20855;&#26377;&#38750;IID&#20998;&#25955;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#20351;&#29992;BN&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20250;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;BN&#32479;&#35745;&#19981;&#21305;&#37197;&#32780;&#38459;&#30861;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#32676;&#32452;&#24402;&#19968;&#21270;&#65288;GN&#65289;&#26356;&#24120;&#29992;&#20110;FL&#20316;&#20026;BN&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25105;&#20204;&#22312;&#21508;&#31181;FL&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;BN&#21644;GN&#20043;&#38388;&#27809;&#26377;&#19968;&#33268;&#30340;&#20248;&#32988;&#32773;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;FL&#20013;&#24402;&#19968;&#21270;&#23618;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;BN&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;FL&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#25104;&#20026;FL&#26410;&#26469;&#23454;&#38469;&#20351;&#29992;&#21644;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#22797;&#21098;&#20992;&#30707;&#22836;&#24067;&#28216;&#25103;&#30340;&#22810;Agent&#23398;&#20064;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#20960;&#31181;&#23398;&#20064;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20026;&#22810;Agent&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2303.03196</link><description>&lt;p&gt;
&#20316;&#20026;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#30340;&#37325;&#22797;&#21098;&#20992;&#30707;&#22836;&#24067;&#30340;&#22522;&#20110;&#20154;&#21475;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning. (arXiv:2303.03196v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#22797;&#21098;&#20992;&#30707;&#22836;&#24067;&#28216;&#25103;&#30340;&#22810;Agent&#23398;&#20064;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#20960;&#31181;&#23398;&#20064;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20026;&#22810;Agent&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#23545;&#25239;&#35268;&#21010;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#30410;&#20110;&#22522;&#20934;&#22495;&#65292;&#20174;&#22269;&#38469;&#35937;&#26827;&#21644;&#32463;&#20856;&#30340;UCI&#25968;&#25454;&#38598;&#21040;&#22260;&#26827;&#21644;&#22806;&#20132;&#12290;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#65292;&#23545;Agent&#35780;&#20272;&#20027;&#35201;&#23616;&#38480;&#20110;&#19982;&#19987;&#23478;&#36827;&#34892;&#23569;&#37327;&#20132;&#20114;&#65292;&#26088;&#22312;&#36798;&#21040;&#19968;&#23450;&#30340;&#24615;&#33021;&#27700;&#24179;&#65288;&#22914;&#20987;&#36133;&#20154;&#31867;&#19987;&#19994;&#29609;&#23478;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21098;&#20992;&#30707;&#22836;&#24067;&#30340;&#22810;Agent&#23398;&#20064;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#21313;&#19977;&#20010;&#38182;&#26631;&#36187;&#21442;&#36187;&#20316;&#21697;&#65292;&#20854;&#20013;&#19968;&#20123;&#26159;&#26377;&#24847;&#30340;&#27425;&#20248;&#20316;&#21697;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22522;&#20110;&#24179;&#22343;&#22238;&#25253;&#21644;&#21487;&#24320;&#21457;&#24615;&#30340;&#20195;&#29702;&#36136;&#37327;&#24230;&#37327;&#26631;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#31181;RL&#12289;&#22312;&#32447;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#33391;&#22909;&#30340;&#21453;&#31574;&#30053;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#26368;&#32456;&#20250;&#36755;&#32473;&#34920;&#29616;&#26368;&#20339;&#30340;&#26426;&#22120;&#20154;&#65292;&#20026;&#22810;Agent&#23398;&#20064;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33021;&#28304;&#24066;&#22330;&#28165;&#31639;&#21644;&#20986;&#20215;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#29992;&#23398;&#20064;&#30340;OPF&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#26126;&#30830;&#30340;&#24066;&#22330;&#35268;&#21017;&#26367;&#20195;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#24182;&#36866;&#29992;&#20110;&#24066;&#22330;&#35774;&#35745;&#21644;&#26356;&#29616;&#23454;&#22320;&#24314;&#27169;&#24066;&#22330;&#21442;&#19982;&#32773;&#12290;</title><link>http://arxiv.org/abs/2303.01772</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#33021;&#28304;&#24066;&#22330;&#28165;&#31639;&#21644;&#20986;&#20215;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Approximating Energy Market Clearing and Bidding With Model-Based Reinforcement Learning. (arXiv:2303.01772v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33021;&#28304;&#24066;&#22330;&#28165;&#31639;&#21644;&#20986;&#20215;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#29992;&#23398;&#20064;&#30340;OPF&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#26126;&#30830;&#30340;&#24066;&#22330;&#35268;&#21017;&#26367;&#20195;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#24182;&#36866;&#29992;&#20110;&#24066;&#22330;&#35774;&#35745;&#21644;&#26356;&#29616;&#23454;&#22320;&#24314;&#27169;&#24066;&#22330;&#21442;&#19982;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#24066;&#22330;&#21487;&#33021;&#20250;&#20026;&#24066;&#22330;&#21442;&#19982;&#32773;&#30340;&#19981;&#33391;&#34892;&#20026;&#25552;&#20379;&#28608;&#21169;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26159;&#39044;&#27979;&#33021;&#28304;&#24066;&#22330;&#21442;&#19982;&#32773;&#39044;&#26399;&#34892;&#20026;&#30340;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#35768;&#22810;&#19982;&#31995;&#32479;&#30340;&#20132;&#20114;&#25165;&#33021;&#25910;&#25947;&#65292;&#32780;&#30005;&#21147;&#31995;&#32479;&#29615;&#22659;&#36890;&#24120;&#21253;&#25324;&#24191;&#27867;&#30340;&#35745;&#31639;&#65292;&#20363;&#22914;&#29992;&#20110;&#24066;&#22330;&#28165;&#31639;&#30340;&#26368;&#20248;&#21151;&#29575;&#27969;&#37327;&#65288;OPF&#65289;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#28304;&#24066;&#22330;&#30340;&#27169;&#22411;&#32473;&#22522;&#26412;&#30340;MARL&#31639;&#27861;&#65292;&#36825;&#20010;&#27169;&#22411;&#37319;&#29992;&#20102;&#23398;&#20064;&#30340;OPF&#36817;&#20284;&#20540;&#21644;&#26126;&#30830;&#30340;&#24066;&#22330;&#35268;&#21017;&#12290;&#23398;&#20064;&#30340;OPF&#20195;&#29702;&#27169;&#22411;&#20351;&#24471;OPF&#30340;&#26126;&#30830;&#35299;&#20915;&#21464;&#24471;&#19981;&#24517;&#35201;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#36824;&#23558;&#35757;&#32451;&#26102;&#38388;&#38477;&#20302;&#20102;&#32422;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#20294;&#20195;&#20215;&#26159;&#30053;&#24494;&#26356;&#24046;&#30340;&#32435;&#20160;&#22343;&#34913;&#36817;&#20284;&#20540;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#28508;&#22312;&#24212;&#29992;&#26159;&#24066;&#22330;&#35774;&#35745;&#65292;&#26356;&#29616;&#23454;&#22320;&#23545;&#24066;&#22330;&#21442;&#19982;&#32773;&#36827;&#34892;&#24314;&#27169;&#20197;&#21450;&#23545;&#24066;&#22330;&#21160;&#24577;&#30340;&#25913;&#36827;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy markets can provide incentives for undesired behavior of market participants. Multi-agent Reinforcement learning (MARL) is a promising new approach to predicting the expected behavior of energy market participants. However, reinforcement learning requires many interactions with the system to converge, and the power system environment often consists of extensive computations, e.g., optimal power flow (OPF) calculation for market clearing. To tackle this complexity, we provide a model of the energy market to a basic MARL algorithm in the form of a learned OPF approximation and explicit market rules. The learned OPF surrogate model makes an explicit solving of the OPF completely unnecessary. Our experiments demonstrate that the model additionally reduces training time by about one order of magnitude but at the cost of a slightly worse approximation of the Nash equilibrium. Potential applications of our method are market design, more realistic modeling of market participants, and an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#23545;&#31216;&#24615;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#30340;&#33258;&#27965;&#32422;&#26463;&#26465;&#20214;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#38899;&#20048;&#39046;&#22495;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#20986;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20363;&#22914;&#32447;&#24615;&#38899;&#39640;&#21644;&#19977;&#32500;&#31515;&#21345;&#23572;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2302.10890</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#23545;&#31216;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Low-dimensional Representation via Physical Symmetry. (arXiv:2302.10890v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10890
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#23545;&#31216;&#24615;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#30340;&#33258;&#27965;&#32422;&#26463;&#26465;&#20214;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#38899;&#20048;&#39046;&#22495;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#20986;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20363;&#22914;&#32447;&#24615;&#38899;&#39640;&#21644;&#19977;&#32500;&#31515;&#21345;&#23572;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#23398;&#20064;&#22312;&#21019;&#36896;&#24615;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#38899;&#20048;&#39046;&#22495;&#65292;&#24403;&#21069;&#30340;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#21508;&#31181;&#29305;&#24449;&#65292;&#22914;&#38899;&#39640;&#12289;&#38899;&#33394;&#12289;&#21644;&#24358;&#12289;&#32441;&#29702;&#31561;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#38899;&#20048;&#39046;&#22495;&#30693;&#35782;&#12290;&#29616;&#22312;&#36824;&#19981;&#28165;&#26970;&#20160;&#20040;&#26679;&#30340;&#19968;&#33324;&#24615;&#35745;&#31639;&#21407;&#21017;&#20250;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#19982;&#20154;&#31867;&#24863;&#30693;&#20445;&#25345;&#19968;&#33268;&#30340;&#20302;&#32500;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#29616;&#20195;&#29289;&#29702;&#23398;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#23558;&#29289;&#29702;&#23545;&#31216;&#24615;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#30340;&#33258;&#27965;&#32422;&#26463;&#26465;&#20214;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#35201;&#27714;&#20808;&#39564;&#27169;&#22411;&#23545;&#28508;&#22312;&#29366;&#24577;&#30340;&#21160;&#24577;&#36827;&#34892;&#25551;&#36848;&#65292;&#24182;&#20197;&#26576;&#31181;&#32676;&#21464;&#25442;&#23545;&#20854;&#36827;&#34892;&#31561;&#21464;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29289;&#29702;&#23545;&#31216;&#24615;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20174;&#26410;&#26631;&#35760;&#30340;&#21333;&#22768;&#36947;&#38899;&#20048;&#38899;&#39057;&#20013;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#38899;&#39640;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#23398;&#20064;&#19968;&#20010;&#19977;&#32500;&#31515;&#21345;&#23572;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable representation learning has been playing a key role in creative intelligent systems. In the music domain, current learning algorithms can successfully learn various features such as pitch, timbre, chord, texture, etc. However, most methods rely heavily on music domain knowledge. It remains an open question what general computational principles give rise to interpretable representations, especially low-dim factors that agree with human perception. In this study, we take inspiration from modern physics and use physical symmetry as a self-consistency constraint for the latent space. Specifically, it requires the prior model that characterises the dynamics of the latent states to be equivariant with respect to certain group transformations. We show that physical symmetry leads the model to learn a linear pitch factor from unlabelled monophonic music audio in a self-supervised fashion. In addition, the same methodology can be applied to computer vision, learning a 3D Cartesian
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#22810;&#26679;&#24615;&#30340;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20854;&#20013;&#32467;&#21512;&#20102;&#20154;&#31867;&#24515;&#29702;&#24819;&#35937;&#33021;&#21147;&#20013;&#21463;&#21040;&#31070;&#32463;&#22810;&#26679;&#24615;&#30740;&#31350;&#21551;&#21457;&#30340;&#26680;&#24515;&#30693;&#35782;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;&#31243;&#24207;&#21512;&#25104;&#65292;&#22312;&#35299;&#20915;&#26032;&#20219;&#21153;&#26102;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#32452;&#21512;&#26680;&#24515;&#30693;&#35782;&#26469;&#24418;&#25104;&#26032;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2302.09425</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#22810;&#26679;&#24615;&#30340;&#35270;&#35273;&#24819;&#35937;&#21644;&#31243;&#24207;&#21512;&#25104;&#30340;ARC&#38382;&#39064;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Neurodiversity-Inspired Solver for the Abstraction \&amp; Reasoning Corpus (ARC) Using Visual Imagery and Program Synthesis. (arXiv:2302.09425v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#22810;&#26679;&#24615;&#30340;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20854;&#20013;&#32467;&#21512;&#20102;&#20154;&#31867;&#24515;&#29702;&#24819;&#35937;&#33021;&#21147;&#20013;&#21463;&#21040;&#31070;&#32463;&#22810;&#26679;&#24615;&#30740;&#31350;&#21551;&#21457;&#30340;&#26680;&#24515;&#30693;&#35782;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;&#31243;&#24207;&#21512;&#25104;&#65292;&#22312;&#35299;&#20915;&#26032;&#20219;&#21153;&#26102;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#32452;&#21512;&#26680;&#24515;&#30693;&#35782;&#26469;&#24418;&#25104;&#26032;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#24180;&#26469;&#22312;&#26576;&#20123;&#39046;&#22495;&#65288;&#22914;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#20219;&#20309;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#28789;&#27963;&#22320;&#36816;&#29992;&#26680;&#24515;&#30693;&#35782;&#26469;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#23558;&#20154;&#31867;&#24515;&#29702;&#24819;&#35937;&#33021;&#21147;&#20013;&#21463;&#21040;&#31070;&#32463;&#22810;&#26679;&#24615;&#30740;&#31350;&#21551;&#21457;&#30340;&#26680;&#24515;&#30693;&#35782;&#30340;&#35270;&#35273;&#34920;&#31034;&#19982;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;&#31243;&#24207;&#21512;&#25104;&#30456;&#32467;&#21512;&#65292;&#20197;&#28789;&#27963;&#22320;&#32452;&#21512;&#26680;&#24515;&#30693;&#35782;&#26469;&#24418;&#25104;&#26032;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#31995;&#32479;&#22312;&#38750;&#24120;&#22256;&#38590;&#30340;ARC&#25361;&#25112;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#20998;&#20139;&#20102;&#26469;&#33258;&#20844;&#24320;&#21487;&#29992;&#30340;ARC&#39033;&#20197;&#21450;&#25105;&#20204;&#22312;&#27604;&#36187;&#20013;&#33719;&#24471;&#31532;&#22235;&#21517;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Core knowledge about physical objects -- e.g., their permanency, spatial transformations, and interactions -- is one of the most fundamental building blocks of biological intelligence across humans and non-human animals. While AI techniques in certain domains (e.g. vision, NLP) have advanced dramatically in recent years, no current AI systems can yet match human abilities in flexibly applying core knowledge to solve novel tasks. We propose a new AI approach to core knowledge that combines 1) visual representations of core knowledge inspired by human mental imagery abilities, especially as observed in studies of neurodivergent individuals; with 2) tree-search-based program synthesis for flexibly combining core knowledge to form new reasoning strategies on the fly. We demonstrate our system's performance on the very difficult Abstraction \&amp; Reasoning Corpus (ARC) challenge, and we share experimental results from publicly available ARC items as well as from our 4th-place finish on the pri
&lt;/p&gt;</description></item><item><title>LEXTREME&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#25552;&#20379;&#20102;11&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#27979;&#35780;&#65292;&#26368;&#20339;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#20351;&#24471;LEXTREME&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#19988;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2301.13126</link><description>&lt;p&gt;
LEXTREME&#65306;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13126
&lt;/p&gt;
&lt;p&gt;
LEXTREME&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#25552;&#20379;&#20102;11&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#27979;&#35780;&#65292;&#26368;&#20339;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#20351;&#24471;LEXTREME&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#19988;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;transformer&#26550;&#26500;&#30340;&#26174;&#33879;&#36827;&#23637;&#25512;&#21160;&#19979;&#65292;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#22686;&#38271;&#12290;&#20026;&#20102;&#34913;&#37327;&#36827;&#23637;&#65292;&#31934;&#24515;&#31574;&#21010;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20934;&#21482;&#33021;&#22788;&#29702;&#33521;&#25991;&#65292;&#32780;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#23578;&#26410;&#26377;&#22810;&#35821;&#35328;&#22522;&#20934;&#21487;&#29992;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22522;&#20934;&#24050;&#32463;&#39281;&#21644;&#65292;&#26368;&#20339;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#26368;&#20339;&#20154;&#31867;&#65292;&#24182;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25991;&#29486;&#65292;&#24182;&#36873;&#25321;&#20102;11&#20010;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;&#20102;LEXTREME&#12290;&#20026;&#20102;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32508;&#21512;&#35780;&#20998;&#65292;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#38598;&#65292;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#12290;&#26368;&#20339;&#22522;&#32447;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#32508;&#21512;&#35780;&#20998;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#34920;&#26126;LEXTREME&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#20026;&#25913;&#36827;&#30041;&#19979;&#20102;&#20805;&#36275;&#31354;&#38388;&#12290;&#20026;&#20102;&#26041;&#20415;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#20351;&#29992;&#65292;&#25105;&#20204;&#23558;LEXTREME&#19982;&#25152;&#26377;&#25968;&#25454;&#19968;&#36215;&#21457;&#24067;&#22312;huggingface&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lately, propelled by the phenomenal advances around the transformer architecture, the legal NLP field has enjoyed spectacular growth. To measure progress, well curated and challenging benchmarks are crucial. However, most benchmarks are English only and in legal NLP specifically there is no multilingual benchmark available yet. Additionally, many benchmarks are saturated, with the best models clearly outperforming the best humans and achieving near perfect scores. We survey the legal NLP literature and select 11 datasets covering 24 languages, creating LEXTREME. To provide a fair comparison, we propose two aggregate scores, one based on the datasets and one on the languages. The best baseline (XLM-R large) achieves both a dataset aggregate score a language aggregate score of 61.3. This indicates that LEXTREME is still very challenging and leaves ample room for improvement. To make it easy for researchers and practitioners to use, we release LEXTREME on huggingface together with all the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#24517;&#35201;&#21644;&#20805;&#20998;&#22240;&#26524;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#19982;&#24863;&#20852;&#36259;&#32467;&#26524;&#30456;&#20851;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2301.12389</link><description>&lt;p&gt;
&#23398;&#20064;&#24517;&#35201;&#21644;&#20805;&#20998;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
On Learning Necessary and Sufficient Causal Graphs. (arXiv:2301.12389v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#24517;&#35201;&#21644;&#20805;&#20998;&#22240;&#26524;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#19982;&#24863;&#20852;&#36259;&#32467;&#26524;&#30456;&#20851;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#38761;&#21629;&#28608;&#21457;&#20102;&#23545;&#21508;&#20010;&#39046;&#22495;&#20013;&#22797;&#26434;&#20851;&#31995;&#30340;&#20852;&#36259;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26088;&#22312;&#22312;&#22797;&#26434;&#30340;&#22823;&#35268;&#27169;&#22270;&#20013;&#21457;&#29616;&#25152;&#26377;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#22270;&#20013;&#20165;&#26377;&#30340;&#19968;&#23567;&#37096;&#20998;&#21464;&#37327;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#23436;&#25972;&#30340;&#22240;&#26524;&#22270;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22823;&#37327;&#38169;&#35823;&#21457;&#29616;&#30340;&#34394;&#20551;&#21464;&#37327;&#65292;&#36825;&#20123;&#34394;&#20551;&#21464;&#37327;&#19982;&#30446;&#26631;&#32467;&#26524;&#39640;&#24230;&#30456;&#20851;&#65292;&#20294;&#23545;&#30446;&#26631;&#32467;&#26524;&#27809;&#26377;&#22240;&#26524;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#24517;&#35201;&#21644;&#20805;&#20998;&#22240;&#26524;&#22270; (NSCG) &#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#38376;&#30001;&#19982;&#24863;&#20852;&#36259;&#32467;&#26524;&#22240;&#26524;&#30456;&#20851;&#30340;&#21464;&#37327;&#32452;&#25104;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22240;&#26524;&#29305;&#24449;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#22240;&#26524;&#27010;&#29575;&#31995;&#32479;&#22320;&#35780;&#20272;&#22240;&#26524;&#22270;&#20013;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#24110;&#21161;&#25105;&#20204;&#30830;&#23450;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#30456;&#20851;&#30340;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The causal revolution has stimulated interest in understanding complex relationships in various fields. Most of the existing methods aim to discover causal relationships among all variables within a complex large-scale graph. However, in practice, only a small subset of variables in the graph are relevant to the outcomes of interest. Consequently, causal estimation with the full causal graph -- particularly given limited data -- could lead to numerous falsely discovered, spurious variables that exhibit high correlation with, but exert no causal impact on, the target outcome. In this paper, we propose learning a class of necessary and sufficient causal graphs (NSCG) that exclusively comprises causally relevant variables for an outcome of interest, which we term causal features. The key idea is to employ probabilities of causation to systematically evaluate the importance of features in the causal graph, allowing us to identify a subgraph relevant to the outcome of interest. To learn NSC
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#28151;&#21512;&#32454;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;GAN&#21453;&#28436;&#32467;&#26524;&#20013;&#30340;&#32534;&#36753;&#33021;&#21147;&#20943;&#24369;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#22270;&#20687;&#20998;&#20026;&#22495;&#20869;&#21644;&#22495;&#22806;&#37096;&#20998;&#65292;&#24182;&#23545;&#36825;&#20004;&#37096;&#20998;&#36827;&#34892;&#26435;&#37325;&#35843;&#33410;&#32454;&#21270;&#65292;&#20197;&#20445;&#25345;&#22495;&#20869;&#21306;&#22495;&#30340;&#32534;&#36753;&#33021;&#21147;&#24182;&#25552;&#39640;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.12141</link><description>&lt;p&gt;
&#20160;&#20040;&#20250;&#20943;&#24369;&#32534;&#36753;&#33021;&#21147;&#65311;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#28151;&#21512;&#32454;&#21270;&#26041;&#27861;&#29992;&#20110;&#25913;&#21892;GAN&#21453;&#28436;
&lt;/p&gt;
&lt;p&gt;
What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion. (arXiv:2301.12141v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12141
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#28151;&#21512;&#32454;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;GAN&#21453;&#28436;&#32467;&#26524;&#20013;&#30340;&#32534;&#36753;&#33021;&#21147;&#20943;&#24369;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#22270;&#20687;&#20998;&#20026;&#22495;&#20869;&#21644;&#22495;&#22806;&#37096;&#20998;&#65292;&#24182;&#23545;&#36825;&#20004;&#37096;&#20998;&#36827;&#34892;&#26435;&#37325;&#35843;&#33410;&#32454;&#21270;&#65292;&#20197;&#20445;&#25345;&#22495;&#20869;&#21306;&#22495;&#30340;&#32534;&#36753;&#33021;&#21147;&#24182;&#25552;&#39640;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21453;&#28436;&#26041;&#27861;&#20851;&#27880;&#29983;&#25104;&#22120;&#20013;&#30340;&#39069;&#22806;&#39640;&#27604;&#29575;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#26435;&#37325;&#25110;&#20013;&#38388;&#29305;&#24449;&#65289;&#65292;&#20197;&#20248;&#21270;&#20174;&#23884;&#20837;&#24335;&#28508;&#22312;&#20195;&#30721;&#36827;&#34892;&#21453;&#28436;&#21644;&#32534;&#36753;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#36825;&#20123;&#25216;&#26415;&#22312;&#37325;&#24314;&#26041;&#38754;&#21462;&#24471;&#20102;&#21512;&#29702;&#30340;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#20250;&#20943;&#24369;&#32534;&#36753;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#22797;&#26434;&#22270;&#20687;&#65288;&#20363;&#22914;&#21253;&#21547;&#36974;&#25377;&#65292;&#35814;&#32454;&#32972;&#26223;&#21644;&#20266;&#24433;&#30340;&#22270;&#20687;&#65289;&#19978;&#12290;&#20851;&#38190;&#22312;&#20110;&#31934;&#32454;&#21270;&#21453;&#28436;&#32467;&#26524;&#65292;&#36991;&#20813;&#32534;&#36753;&#33021;&#21147;&#38477;&#32423;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#28151;&#21512;&#32454;&#21270;&#65288;DHR&#65289;&#65292;&#21033;&#29992;&#20102;&#20004;&#31181;&#20027;&#27969;&#32454;&#21270;&#25216;&#26415;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20197;&#20445;&#25345;&#32534;&#36753;&#33021;&#21147;&#21644;&#20445;&#35777;&#20445;&#30495;&#24230;&#30340;&#25552;&#39640;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#20998;&#25104;&#20004;&#20010;&#37096;&#20998;&#65306;&#22495;&#20869;&#21644;&#22495;&#22806;&#37096;&#20998;&#12290;&#32454;&#21270;&#36807;&#31243;&#26088;&#22312;&#20445;&#25345;&#22495;&#20869;&#21306;&#22495;&#30340;&#21487;&#32534;&#36753;&#24615;&#24182;&#25552;&#39640;&#20004;&#20010;&#22495;&#30340;&#20445;&#30495;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#26435;&#37325;&#35843;&#33410;&#26469;&#32454;&#21270;&#36825;&#20004;&#20010;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, inversion methods have focused on additional high-rate information in the generator (e.g., weights or intermediate features) to refine inversion and editing results from embedded latent codes. Although these techniques gain reasonable improvement in reconstruction, they decrease editing capability, especially on complex images (e.g., containing occlusions, detailed backgrounds, and artifacts). A vital crux is refining inversion results, avoiding editing capability degradation. To tackle this problem, we introduce Domain-Specific Hybrid Refinement (DHR), which draws on the advantages and disadvantages of two mainstream refinement techniques to maintain editing ability with fidelity improvement. Specifically, we first propose Domain-Specific Segmentation to segment images into two parts: in-domain and out-of-domain parts. The refinement process aims to maintain the editability for in-domain areas and improve two domains' fidelity. We refine these two parts by weight modulation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#38750;&#21442;&#25968;&#23376;&#22270;&#21305;&#37197;&#26694;&#26550;MatchExplainer&#65292;&#21487;&#20197;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#27492;&#26694;&#26550;&#23558;&#30446;&#26631;&#22270;&#19982;&#20854;&#20182;&#23454;&#20363;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#33410;&#28857;&#23545;&#24212;&#30340;&#36317;&#31163;&#26469;&#37492;&#21035;&#26368;&#20851;&#38190;&#30340;&#32852;&#21512;&#23376;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#33539;&#24335;MatchDrop&#26469;&#35299;&#20915;&#35823;&#25253;&#37319;&#26679;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.02780</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#21442;&#25968;&#23376;&#22270;&#21305;&#37197;&#37325;&#26032;&#24605;&#32771;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rethinking Explaining Graph Neural Networks via Non-parametric Subgraph Matching. (arXiv:2301.02780v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#38750;&#21442;&#25968;&#23376;&#22270;&#21305;&#37197;&#26694;&#26550;MatchExplainer&#65292;&#21487;&#20197;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#27492;&#26694;&#26550;&#23558;&#30446;&#26631;&#22270;&#19982;&#20854;&#20182;&#23454;&#20363;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#33410;&#28857;&#23545;&#24212;&#30340;&#36317;&#31163;&#26469;&#37492;&#21035;&#26368;&#20851;&#38190;&#30340;&#32852;&#21512;&#23376;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#33539;&#24335;MatchDrop&#26469;&#35299;&#20915;&#35823;&#25253;&#37319;&#26679;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#24341;&#21457;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#65306;&#8220;&#36755;&#20837;&#22270;&#30340;&#21738;&#19968;&#37096;&#20998;&#23545;&#39044;&#27979;&#26368;&#20026;&#20915;&#23450;&#24615;&#65311;&#8221;&#29305;&#21035;&#26159;&#65292;&#30001;&#20110;&#20854;&#26356;&#24378;&#22823;&#30340;&#35299;&#35835;&#40657;&#31665;&#65288;&#21363;&#30446;&#26631;GNNs&#65289;&#33021;&#21147;&#65292;&#21442;&#25968;&#21270;&#35299;&#37322;&#22120;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#22522;&#20110;&#35266;&#23519;&#21040;&#22270;&#36890;&#24120;&#20849;&#20139;&#26576;&#20123;&#24120;&#35265;&#30340;&#27169;&#24335;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#23376;&#22270;&#21305;&#37197;&#26694;&#26550;MatchExplainer&#26469;&#25506;&#32034;&#35299;&#37322;&#24615;&#23376;&#22270;&#12290;&#23427;&#23558;&#30446;&#26631;&#22270;&#19982;&#20854;&#20182;&#30456;&#24212;&#23454;&#20363;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22522;&#20110;&#33410;&#28857;&#23545;&#24212;&#30340;&#36317;&#31163;&#26469;&#35782;&#21035;&#26368;&#20851;&#38190;&#30340;&#32852;&#21512;&#23376;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#29616;&#26377;&#30340;&#22270;&#37319;&#26679;&#25110;&#33410;&#28857;&#21024;&#38500;&#26041;&#27861;&#36890;&#24120;&#20250;&#36935;&#21040;&#35823;&#25253;&#37319;&#26679;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;MatchDrop&#30340;&#26032;&#22686;&#26041;&#26696;&#65292;&#23427;&#21033;&#29992;&#20102;MatchExplainer&#26469;&#20462;&#22797;&#22270;&#30340;&#26368;&#20449;&#24687;&#20016;&#23500;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of graph neural networks (GNNs) provokes the question about explainability: ``Which fraction of the input graph is the most determinant of the prediction?'' Particularly, parametric explainers prevail in existing approaches because of their more robust capability to decipher the black-box (i.e., target GNNs). In this paper, based on the observation that graphs typically share some common motif patterns, we propose a novel non-parametric subgraph matching framework, dubbed MatchExplainer, to explore explanatory subgraphs. It couples the target graph with other counterpart instances and identifies the most crucial joint substructure by minimizing the node corresponding-based distance. Moreover, we note that present graph sampling or node-dropping methods usually suffer from the false positive sampling problem. To alleviate this issue, we designed a new augmentation paradigm named MatchDrop. It takes advantage of MatchExplainer to fix the most informative portion of the graph 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24658;&#23450;&#20869;&#23384;&#38656;&#27714;&#25193;&#23637;&#25968;&#25454;&#38598;&#31934;&#31616;&#30340;&#26041;&#27861;&#65292;&#21487;&#23558;Matching Training Trajectories&#65288;MTT&#65289;&#24212;&#29992;&#20110;ImageNet-1K&#25968;&#25454;&#38598;&#65292;&#36798;&#21040;6&#20493;&#30340;&#20869;&#23384;&#38477;&#20302;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#32422;2%&#30340;&#36816;&#34892;&#26102;&#24320;&#38144;&#12290;&#21516;&#26102;&#20063;&#21457;&#29616;&#65292;&#20026;&#21512;&#25104;&#22270;&#20687;&#20998;&#37197;&#36719;&#26631;&#31614;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2211.10586</link><description>&lt;p&gt;
&#21033;&#29992;&#24658;&#23450;&#20869;&#23384;&#23558;&#25968;&#25454;&#38598;&#31934;&#31616;&#25193;&#23637;&#21040;ImageNet-1K
&lt;/p&gt;
&lt;p&gt;
Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory. (arXiv:2211.10586v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24658;&#23450;&#20869;&#23384;&#38656;&#27714;&#25193;&#23637;&#25968;&#25454;&#38598;&#31934;&#31616;&#30340;&#26041;&#27861;&#65292;&#21487;&#23558;Matching Training Trajectories&#65288;MTT&#65289;&#24212;&#29992;&#20110;ImageNet-1K&#25968;&#25454;&#38598;&#65292;&#36798;&#21040;6&#20493;&#30340;&#20869;&#23384;&#38477;&#20302;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#32422;2%&#30340;&#36816;&#34892;&#26102;&#24320;&#38144;&#12290;&#21516;&#26102;&#20063;&#21457;&#29616;&#65292;&#20026;&#21512;&#25104;&#22270;&#20687;&#20998;&#37197;&#36719;&#26631;&#31614;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#31616;&#26041;&#27861;&#26088;&#22312;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#21387;&#32553;&#25104;&#19968;&#23567;&#32452;&#21512;&#25104;&#26679;&#26412;&#65292;&#20351;&#24471;&#22312;&#35757;&#32451;&#26102;&#65292;&#19982;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24120;&#35268;&#35757;&#32451;&#30456;&#27604;&#65292;&#21487;&#20197;&#33719;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#21305;&#37197;&#35757;&#32451;&#36712;&#36857;&#65288;MTT&#65289;&#22312;CIFAR-10/100&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#25191;&#34892;&#23637;&#24320;&#26799;&#24230;&#35745;&#31639;&#26102;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#65292;&#22240;&#27492;&#24456;&#38590;&#25193;&#23637;&#21040;ImageNet-1k&#25968;&#25454;&#38598;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23384;&#22312;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#24658;&#23450;&#30340;GPU&#20869;&#23384;&#38656;&#27714;&#65288;&#19982;&#23637;&#24320;&#27493;&#39588;&#30340;&#25968;&#37327;&#26080;&#20851;&#65289;&#31934;&#30830;&#35745;&#31639;&#36712;&#36857;&#21305;&#37197;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#26377;&#20102;&#36825;&#19968;&#21457;&#29616;&#65292;&#25152;&#25552;&#20986;&#30340;&#20869;&#23384;&#39640;&#25928;&#30340;&#36712;&#36857;&#21305;&#37197;&#26041;&#27861;&#21482;&#38656;&#35201;&#27604;&#21407;&#22987;MTT&#22810;&#32422;2&#65285;&#30340;&#36816;&#34892;&#26102;&#24320;&#38144;&#65292;&#21363;&#21487;&#36731;&#26494;&#25193;&#23637;&#21040;&#20855;&#26377;6&#20493;&#20869;&#23384;&#32553;&#20943;&#30340;ImageNet-1K&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20026;&#21512;&#25104;&#22270;&#20687;&#20998;&#37197;&#36719;&#26631;&#31614;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation methods aim to compress a large dataset into a small set of synthetic samples, such that when being trained on, competitive performances can be achieved compared to regular training on the entire dataset. Among recently proposed methods, Matching Training Trajectories (MTT) achieves state-of-the-art performance on CIFAR-10/100, while having difficulty scaling to ImageNet-1k dataset due to the large memory requirement when performing unrolled gradient computation through back-propagation. Surprisingly, we show that there exists a procedure to exactly calculate the gradient of the trajectory matching loss with constant GPU memory requirement (irrelevant to the number of unrolled steps). With this finding, the proposed memory-efficient trajectory matching method can easily scale to ImageNet-1K with 6x memory reduction while introducing only around 2% runtime overhead than original MTT. Further, we find that assigning soft labels for synthetic images is crucial for the
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#27169;&#22411;&#35299;&#37322;&#12289;&#22870;&#21169;&#35299;&#37322;&#12289;&#29366;&#24577;&#35299;&#37322;&#21644;&#20219;&#21153;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#12289;&#31639;&#27861;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2211.06665</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#32508;&#36848;&#65306;&#27010;&#24565;&#12289;&#31639;&#27861;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges. (arXiv:2211.06665v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06665
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#27169;&#22411;&#35299;&#37322;&#12289;&#22870;&#21169;&#35299;&#37322;&#12289;&#29366;&#24577;&#35299;&#37322;&#21644;&#20219;&#21153;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#12289;&#31639;&#27861;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26234;&#33021;&#20195;&#29702;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#20197;&#23454;&#29616;&#38271;&#26399;&#30446;&#26631;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#22797;&#20852;&#25512;&#21160;&#19979;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#24178;&#32467;&#26500;&#34987;&#26222;&#36941;&#35270;&#20026;&#40657;&#30418;&#23376;&#65292;&#38459;&#30861;&#20102;&#20174;&#19994;&#32773;&#22312;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#30495;&#23454;&#22330;&#26223;&#20013;&#20449;&#20219;&#21644;&#20351;&#29992;&#35757;&#32451;&#20195;&#29702;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#22823;&#37327;&#30340;&#25991;&#29486;&#33268;&#21147;&#20110;&#25581;&#31034;&#26234;&#33021;&#20195;&#29702;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#36890;&#36807;&#26500;&#24314;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#25110;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#23558;&#20808;&#21069;&#30340;&#24037;&#20316;&#26126;&#30830;&#22320;&#20998;&#20026;&#27169;&#22411;&#35299;&#37322;&#12289;&#22870;&#21169;&#35299;&#37322;&#12289;&#29366;&#24577;&#35299;&#37322;&#21644;&#20219;&#21153;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) is a popular machine learning paradigm where intelligent agents interact with the environment to fulfill a long-term goal. Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great success over a wide spectrum of complex control tasks. Despite the encouraging results achieved, the deep neural network-based backbone is widely deemed as a black box that impedes practitioners to trust and employ trained agents in realistic scenarios where high security and reliability are essential. To alleviate this issue, a large volume of literature devoted to shedding light on the inner workings of the intelligent agents has been proposed, by constructing intrinsic interpretability or post-hoc explainability. In this survey, we provide a comprehensive review of existing works on eXplainable RL (XRL) and introduce a new taxonomy where prior works are clearly categorized into model-explaining, reward-explaining, state-explaining, and task-explaining methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;NAS&#22312;&#28608;&#27963;&#21644;&#36339;&#36291;&#36830;&#25509;&#25628;&#32034;&#19979;&#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#29702;&#35770;&#30340;&#31639;&#27861;&#26469;&#36873;&#25321;&#24615;&#33021;&#26368;&#22909;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2209.07238</link><description>&lt;p&gt;
NAS&#22312;&#28608;&#27963;&#21644;&#36339;&#36291;&#36830;&#25509;&#25628;&#32034;&#19979;&#30340;&#27867;&#21270;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Generalization Properties of NAS under Activation and Skip Connection Search. (arXiv:2209.07238v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;NAS&#22312;&#28608;&#27963;&#21644;&#36339;&#36291;&#36830;&#25509;&#25628;&#32034;&#19979;&#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#29702;&#35770;&#30340;&#31639;&#27861;&#26469;&#36873;&#25321;&#24615;&#33021;&#26368;&#22909;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20419;&#36827;&#20102;&#33258;&#21160;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#23613;&#31649;NAS&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;NAS&#30340;&#29702;&#35770;&#20445;&#35777;&#20851;&#27880;&#29978;&#23569;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;NAS&#22312;&#32479;&#19968;&#26694;&#26550;&#19979;&#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#21253;&#25324;&#20102;&#65288;&#28145;&#23618;&#65289;&#23618;&#32423;&#36339;&#36291;&#36830;&#25509;&#25628;&#32034;&#21644;&#28608;&#27963;&#20989;&#25968;&#25628;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#21253;&#25324;&#28151;&#21512;&#28608;&#27963;&#20989;&#25968;&#12289;&#20840;&#36830;&#25509;&#21644;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#29305;&#23450;&#25628;&#32034;&#31354;&#38388;&#65292;&#25512;&#23548;&#20986;&#65288;&#26080;&#65289;&#38480;&#23485;&#24230;&#24773;&#20917;&#19979;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#26368;&#23567;&#29305;&#24449;&#20540;&#30340;&#19979;&#65288;&#19978;&#65289;&#30028;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#23567;&#29305;&#24449;&#20540;&#26469;&#24314;&#31435;NAS&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#22914;&#20309;&#26681;&#25454;&#25105;&#20204;&#25512;&#23548;&#20986;&#30340;&#32467;&#26524;&#24341;&#23548;NAS&#36873;&#25321;&#24615;&#33021;&#26368;&#22909;&#30340;&#26550;&#26500;&#65292;&#21363;&#20351;&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has fostered the automatic discovery of state-of-the-art neural architectures. Despite the progress achieved with NAS, so far there is little attention to theoretical guarantees on NAS. In this work, we study the generalization properties of NAS under a unifying framework enabling (deep) layer skip connection search and activation function search. To this end, we derive the lower (and upper) bounds of the minimum eigenvalue of the Neural Tangent Kernel (NTK) under the (in)finite-width regime using a certain search space including mixed activation functions, fully connected, and residual neural networks. We use the minimum eigenvalue to establish generalization error bounds of NAS in the stochastic gradient descent training. Importantly, we theoretically and experimentally show how the derived results can guide NAS to select the top-performing architectures, even in the case without training, leading to a train-free algorithm based on our theory. Accordi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#26041;&#27861;&#26469;&#20248;&#20808;&#36873;&#25321;&#26679;&#26412;&#65292;&#36890;&#36807;&#31283;&#23450;&#38477;&#20302;&#26679;&#26412;&#30340;&#35757;&#32451;&#25439;&#22833;&#26469;&#23450;&#20041;&#26679;&#26412;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#38543;&#26426;&#25277;&#26679;&#21644;&#20165;&#26681;&#25454;&#35757;&#32451;&#25439;&#22833;&#36827;&#34892;&#20248;&#20808;&#36873;&#25321;&#30340;&#26041;&#27861;&#26356;&#21152;&#31283;&#20581;&#12290;</title><link>http://arxiv.org/abs/2208.10483</link><description>&lt;p&gt;
&#22312;&#21487;&#32422;&#25439;&#22833;&#20013;&#20026;&#24378;&#21270;&#23398;&#20064;&#20248;&#20808;&#36873;&#25321;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Samples in Reinforcement Learning with Reducible Loss. (arXiv:2208.10483v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#26041;&#27861;&#26469;&#20248;&#20808;&#36873;&#25321;&#26679;&#26412;&#65292;&#36890;&#36807;&#31283;&#23450;&#38477;&#20302;&#26679;&#26412;&#30340;&#35757;&#32451;&#25439;&#22833;&#26469;&#23450;&#20041;&#26679;&#26412;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#38543;&#26426;&#25277;&#26679;&#21644;&#20165;&#26681;&#25454;&#35757;&#32451;&#25439;&#22833;&#36827;&#34892;&#20248;&#20808;&#36873;&#25321;&#30340;&#26041;&#27861;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#32463;&#39564;&#22238;&#25918;&#32531;&#20914;&#21306;&#21453;&#22797;&#35757;&#32451;&#20195;&#29702;&#24050;&#35266;&#23519;&#21040;&#30340;&#26679;&#26412;&#12290;&#24182;&#38750;&#25152;&#26377;&#26679;&#26412;&#20855;&#26377;&#30456;&#21516;&#30340;&#37325;&#35201;&#24615;&#65292;&#31616;&#21333;&#22320;&#36171;&#20104;&#27599;&#20010;&#26679;&#26412;&#30456;&#31561;&#30340;&#37325;&#35201;&#24615;&#26159;&#19968;&#31181;&#22825;&#30495;&#30340;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25105;&#20204;&#21487;&#20197;&#20174;&#26679;&#26412;&#20013;&#23398;&#21040;&#22810;&#23569;&#30340;&#26041;&#27861;&#26469;&#20248;&#20808;&#36873;&#25321;&#26679;&#26412;&#12290;&#25105;&#20204;&#23558;&#26679;&#26412;&#30340;&#21487;&#23398;&#20064;&#24615;&#23450;&#20041;&#20026;&#19982;&#26679;&#26412;&#30456;&#20851;&#30340;&#35757;&#32451;&#25439;&#22833;&#38543;&#26102;&#38388;&#25345;&#32493;&#19979;&#38477;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#36739;&#39640;&#21487;&#23398;&#20064;&#24615;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#23558;&#36739;&#38590;&#23398;&#20064;&#30340;&#26679;&#26412;&#65288;&#36890;&#24120;&#30001;&#22122;&#22768;&#25110;&#38543;&#26426;&#24615;&#24341;&#36215;&#65289;&#36171;&#20104;&#36739;&#20302;&#30340;&#20248;&#20808;&#32423;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#38543;&#26426;&#25277;&#26679;&#26356;&#21152;&#31283;&#20581;&#65292;&#20063;&#20248;&#20110;&#20165;&#26681;&#25454;&#35757;&#32451;&#25439;&#22833;&#65288;&#21363;&#26102;&#38388;&#24046;&#20998;&#25439;&#22833;&#65289;&#36827;&#34892;&#20248;&#20808;&#36873;&#25321;&#65292;&#36825;&#22312;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning algorithms take advantage of an experience replay buffer to repeatedly train on samples the agent has observed in the past. Not all samples carry the same amount of significance and simply assigning equal importance to each of the samples is a na\"ive strategy. In this paper, we propose a method to prioritize samples based on how much we can learn from a sample. We define the learn-ability of a sample as the steady decrease of the training loss associated with this sample over time. We develop an algorithm to prioritize samples with high learn-ability, while assigning lower priority to those that are hard-to-learn, typically caused by noise or stochasticity. We empirically show that our method is more robust than random sampling and also better than just prioritizing with respect to the training loss, i.e. the temporal difference loss, which is used in prioritized experience replay.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#20381;&#36182;&#20110;&#22240;&#26524;&#20027;&#24352;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2207.04053</link><description>&lt;p&gt;
&#35770;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Need and Applicability of Causality for Fair Machine Learning. (arXiv:2207.04053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#20381;&#36182;&#20110;&#22240;&#26524;&#20027;&#24352;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#22312;&#27969;&#34892;&#30149;&#23398;&#12289;&#25919;&#27835;&#21644;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#24120;&#35265;&#24212;&#29992;&#26696;&#20363;&#22806;&#65292;&#20107;&#23454;&#35777;&#26126;&#22240;&#26524;&#20851;&#31995;&#22312;&#35780;&#20272;&#33258;&#21160;&#20915;&#31574;&#30340;&#20844;&#27491;&#24615;&#26041;&#38754;&#21313;&#20998;&#37325;&#35201;&#65292;&#26080;&#35770;&#26159;&#22312;&#27861;&#24459;&#19978;&#36824;&#26159;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#20026;&#20309;&#22240;&#26524;&#20851;&#31995;&#23545;&#20844;&#24179;&#24615;&#35780;&#20272;&#23588;&#20026;&#37325;&#35201;&#30340;&#35770;&#28857;&#21644;&#31034;&#20363;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#20197;&#21450;&#20381;&#36182;&#22240;&#26524;&#20027;&#24352;&#30340;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#12290;&#25105;&#20204;&#26368;&#21518;&#35752;&#35770;&#20102;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Besides its common use cases in epidemiology, political, and social sciences, causality turns out to be crucial in evaluating the fairness of automated decisions, both in a legal and everyday sense. We provide arguments and examples, of why causality is particularly important for fairness evaluation. In particular, we point out the social impact of non-causal predictions and the legal anti-discrimination process that relies on causal claims. We conclude with a discussion about the challenges and limitations of applying causality in practical scenarios as well as possible solutions.
&lt;/p&gt;</description></item><item><title>&#29420;&#31435;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#22312;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#26377;&#25928;&#65292;&#36890;&#36807;&#26356;&#26032;Q&#20989;&#25968;&#21487;&#20197;&#24341;&#23548;&#31574;&#30053;&#25910;&#25947;&#21040;&#31283;&#23450;&#30340;&#32435;&#20160;&#24179;&#34913;&#28857;&#12290;</title><link>http://arxiv.org/abs/2205.14590</link><description>&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#30340;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Independent and Decentralized Learning in Markov Potential Games. (arXiv:2205.14590v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14590
&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#22312;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#26377;&#25928;&#65292;&#36890;&#36807;&#26356;&#26032;Q&#20989;&#25968;&#21487;&#20197;&#24341;&#23548;&#31574;&#30053;&#25910;&#25947;&#21040;&#31283;&#23450;&#30340;&#32435;&#20160;&#24179;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#30340;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#29609;&#23478;&#19981;&#20102;&#35299;&#28216;&#25103;&#27169;&#22411;&#65292;&#20063;&#19981;&#33021;&#36827;&#34892;&#21327;&#35843;&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#29609;&#23478;&#36890;&#36807;&#24322;&#27493;&#26041;&#24335;&#26356;&#26032;&#20182;&#20204;&#30340;&#25171;&#25200;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#20540;&#65292;&#35813;&#20989;&#25968;&#26681;&#25454;&#23454;&#29616;&#30340;&#19968;&#38454;&#27573;&#22870;&#21169;&#35780;&#20272;&#20182;&#20204;&#30340;&#24635;&#20307;&#26465;&#20214;&#20184;&#27454;&#12290;&#28982;&#21518;&#65292;&#29609;&#23478;&#36890;&#36807;&#23558;&#22522;&#20110;&#20272;&#35745;Q&#20989;&#25968;&#30340;&#24179;&#28369;&#26368;&#20248;&#19968;&#38454;&#27573;&#20559;&#24046;&#31574;&#30053;&#32435;&#20837;&#20854;&#31574;&#30053;&#20013;&#26469;&#29420;&#31435;&#22320;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#23398;&#20064;&#21160;&#24577;&#30340;&#20851;&#38190;&#29305;&#24449;&#26159;Q&#20989;&#25968;&#20272;&#35745;&#26159;&#20197;&#27604;&#31574;&#30053;&#26356;&#24555;&#30340;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#26356;&#26032;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#21160;&#24577;&#24341;&#23548;&#30340;&#31574;&#30053;&#22312;&#27010;&#29575;1&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#31283;&#23450;&#32435;&#20160;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#31616;&#21333;&#23398;&#20064;&#21160;&#24577;&#22312;&#36798;&#21040;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#31283;&#23450;&#32435;&#20160;&#24179;&#34913;&#26041;&#38754;&#30340;&#21151;&#25928;&#65292;&#21363;&#20351;&#26159;&#22312;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#20195;&#29702;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multi-agent reinforcement learning dynamics, and analyze its convergence in infinite-horizon discounted Markov potential games. We focus on the independent and decentralized setting, where players do not have knowledge of the game model and cannot coordinate. In each stage, players update their estimate of a perturbed Q-function that evaluates their total contingent payoff based on the realized one-stage reward in an asynchronous manner. Then, players independently update their policies by incorporating a smoothed optimal one-stage deviation strategy based on the estimated Q-function. A key feature of the learning dynamics is that the Q-function estimates are updated at a faster timescale than the policies. We prove that the policies induced by our learning dynamics converge to a stationary Nash equilibrium in Markov potential games with probability 1. Our results highlight the efficacy of simple learning dynamics in reaching a stationary Nash equilibrium even in environme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SCORE&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;&#12290;&#36890;&#36807;&#24341;&#20837;&#36864;&#28779;&#34892;&#20026;&#20811;&#38534;&#27491;&#21017;&#21270;&#22120;&#65292;SCORE&#23454;&#29616;&#20102;SoTA&#24615;&#33021;&#65292;&#24182;&#28040;&#38500;&#20102;&#27425;&#20248;&#24615;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.12468</link><description>&lt;p&gt;
SCORE&#65306;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
SCORE: Spurious COrrelation REduction for Offline Reinforcement Learning. (arXiv:2110.12468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.12468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SCORE&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;&#12290;&#36890;&#36807;&#24341;&#20837;&#36864;&#28779;&#34892;&#20026;&#20811;&#38534;&#27491;&#21017;&#21270;&#22120;&#65292;SCORE&#23454;&#29616;&#20102;SoTA&#24615;&#33021;&#65292;&#24182;&#28040;&#38500;&#20102;&#27425;&#20248;&#24615;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35299;&#20915;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#35770;&#25991;&#21482;&#35752;&#35770;&#20102;&#23545;&#25239;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#34892;&#20026;&#30340;&#38450;&#24481;&#65292;&#32780;&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#21363;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#19982;&#20915;&#31574;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#36825;&#26159;&#23548;&#33268;&#27425;&#20248;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#26377;&#25928;&#19988;&#29702;&#35770;&#19978;&#21487;&#35777;&#26126;&#30340;&#31639;&#27861;&#65306;&#29992;&#20110;&#31163;&#32447;RL&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;&#65288;SCORE&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SCORE&#22312;&#26631;&#20934;&#22522;&#20934;&#65288;D4RL&#65289;&#19978;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#20197;3.1&#20493;&#21152;&#36895;&#29575;&#23454;&#29616;&#20102;SoTA&#24615;&#33021;&#12290;&#25152;&#25552;&#31639;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#36864;&#28779;&#34892;&#20026;&#20811;&#38534;&#27491;&#21017;&#21270;&#22120;&#26469;&#24110;&#21161;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#23545;&#20110;&#28040;&#38500;&#27425;&#20248;&#24615;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#20854;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#27425;&#32447;&#24615;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) harnesses the power of massive datasets for resolving sequential decision problems. Most existing papers only discuss defending against out-of-distribution (OOD) actions while we investigate a broader issue, the spurious correlations between epistemic uncertainty and decision-making, an essential factor that causes suboptimality. In this paper, we propose Spurious COrrelation REduction (SCORE) for offline RL, a practically effective and theoretically provable algorithm. We empirically show that SCORE achieves the SoTA performance with 3.1x acceleration on various tasks in a standard benchmark (D4RL). The proposed algorithm introduces an annealing behavior cloning regularizer to help produce a high-quality estimation of uncertainty which is critical for eliminating spurious correlations from suboptimality. Theoretically, we justify the rationality of the proposed method and prove its convergence to the optimal policy with a sublinear rate under mild a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#22870;&#21169;&#21644;&#36716;&#31227;&#27010;&#29575;&#20004;&#26041;&#38754;&#23384;&#22312;&#30340;&#23545;&#25239;&#24615;&#33104;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#33104;&#36133;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#33104;&#36133;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#21518;&#24724;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#26410;&#30693;&#27700;&#24179;&#30340;&#33104;&#36133;&#12290;</title><link>http://arxiv.org/abs/1911.08689</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#33104;&#36133;&#30340;&#40065;&#26834;&#25506;&#32034;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Corruption-robust exploration in episodic reinforcement learning. (arXiv:1911.08689v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.08689
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#22870;&#21169;&#21644;&#36716;&#31227;&#27010;&#29575;&#20004;&#26041;&#38754;&#23384;&#22312;&#30340;&#23545;&#25239;&#24615;&#33104;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#33104;&#36133;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#33104;&#36133;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#21518;&#24724;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#26410;&#30693;&#27700;&#24179;&#30340;&#33104;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22810;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#22312;&#22870;&#21169;&#21644;&#36716;&#31227;&#27010;&#29575;&#20004;&#26041;&#38754;&#30340;&#23545;&#25239;&#24615;&#33104;&#36133;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25193;&#23637;&#20102;&#26368;&#36817;&#23545;&#38543;&#26426;&#36172;&#21338;&#26426;&#29305;&#20363;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#8220;&#20048;&#35266;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#8221;&#30340;&#29616;&#26377;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25506;&#32034;&#24615;&#25913;&#36827;&#65292;&#24182;&#32467;&#21512;&#8220;&#21160;&#20316;&#28120;&#27760;&#8221;&#21407;&#21017;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#26420;&#32032;&#24212;&#29992;&#21160;&#20316;&#28120;&#27760;&#25152;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;(a)&#22312;&#27809;&#26377;&#33104;&#36133;&#26102;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#21518;&#24724;&#65292;&#24182;&#19988;(b)&#33021;&#22815;&#36866;&#24212;&#26410;&#30693;&#27700;&#24179;&#30340;&#33104;&#36133;&#65292;&#22312;&#24635;&#33104;&#36133;&#24773;&#20917;&#19979;&#21518;&#24724;&#31243;&#24230;&#36880;&#28176;&#38477;&#20302;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#34920;&#26684;&#35774;&#32622;&#19979;&#30340;&#32467;&#26524;&#65288;&#20854;&#20013;&#28041;&#21450;&#29366;&#24577;&#21644;&#34892;&#20026;&#65289;&#20197;&#21450;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#35774;&#32622;&#19979;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate the study of multi-stage episodic reinforcement learning under adversarial corruptions in both the rewards and the transition probabilities of the underlying system extending recent results for the special case of stochastic bandits. We provide a framework which modifies the aggressive exploration enjoyed by existing reinforcement learning approaches based on "optimism in the face of uncertainty", by complementing them with principles from "action elimination". Importantly, our framework circumvents the major challenges posed by naively applying action elimination in the RL setting, as formalized by a lower bound we demonstrate. Our framework yields efficient algorithms which (a) attain near-optimal regret in the absence of corruptions and (b) adapt to unknown levels corruption, enjoying regret guarantees which degrade gracefully in the total corruption encountered. To showcase the generality of our approach, we derive results for both tabular settings (where states and act
&lt;/p&gt;</description></item></channel></rss>