<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>DiffusionEngine&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#24341;&#25806;&#65292;&#20351;&#29992;Diffusion&#27169;&#22411;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#12290;&#23427;&#36890;&#36807;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#19968;&#20010;&#26816;&#27979;&#36866;&#37197;&#22120;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#38454;&#27573;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#21487;&#25193;&#23637;&#12289;&#22810;&#26679;&#21270;&#21644;&#21487;&#27867;&#21270;&#30340;&#26816;&#27979;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.03893</link><description>&lt;p&gt;
DiffusionEngine: &#25193;&#23637;&#25968;&#25454;&#24341;&#25806;&#30340;&#25193;&#25955;&#27169;&#22411;&#36866;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection. (arXiv:2309.03893v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03893
&lt;/p&gt;
&lt;p&gt;
DiffusionEngine&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#24341;&#25806;&#65292;&#20351;&#29992;Diffusion&#27169;&#22411;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#12290;&#23427;&#36890;&#36807;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#19968;&#20010;&#26816;&#27979;&#36866;&#37197;&#22120;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#38454;&#27573;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#21487;&#25193;&#23637;&#12289;&#22810;&#26679;&#21270;&#21644;&#21487;&#27867;&#21270;&#30340;&#26816;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#30707;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#26368;&#36817;&#24320;&#21457;&#30340;Diffusion&#27169;&#22411;&#26159;&#19968;&#20010;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#24341;&#25806;&#12290;&#29616;&#26377;&#30340;&#32553;&#25918;&#26816;&#27979;&#23548;&#21521;&#25968;&#25454;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#25163;&#21160;&#25910;&#38598;&#25110;&#29983;&#25104;&#27169;&#22411;&#26469;&#33719;&#21462;&#30446;&#26631;&#22270;&#20687;&#65292;&#28982;&#21518;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21644;&#26631;&#27880;&#20135;&#29983;&#35757;&#32451;&#23545;&#65292;&#36825;&#20123;&#26041;&#27861;&#25104;&#26412;&#39640;&#12289;&#22797;&#26434;&#25110;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffusionEngine&#65288;DE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#25454;&#25193;&#23637;&#24341;&#25806;&#65292;&#20197;&#21333;&#19968;&#38454;&#27573;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#23548;&#21521;&#35757;&#32451;&#23545;&#12290;DE&#30001;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#19968;&#20010;&#26377;&#25928;&#30340;&#26816;&#27979;&#36866;&#37197;&#22120;&#32452;&#25104;&#65292;&#20026;&#29983;&#25104;&#21487;&#25193;&#23637;&#12289;&#22810;&#26679;&#21270;&#21644;&#21487;&#27867;&#21270;&#30340;&#26816;&#27979;&#25968;&#25454;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#25903;&#25345;&#21363;&#25554;&#21363;&#29992;&#12290;&#26816;&#27979;&#36866;&#37197;&#22120;&#36890;&#36807;&#23398;&#20064;&#23558;&#29616;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#38544;&#24335;&#35821;&#20041;&#21644;&#20301;&#32622;&#30693;&#35782;&#19982;&#26816;&#27979;&#30456;&#20851;&#30340;&#20449;&#21495;&#36827;&#34892;&#23545;&#40784;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#36793;&#30028;&#26694;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#21363;COCO-DE&#21644;...
&lt;/p&gt;
&lt;p&gt;
Data is the cornerstone of deep learning. This paper reveals that the recently developed Diffusion Model is a scalable data engine for object detection. Existing methods for scaling up detection-oriented data often require manual collection or generative models to obtain target images, followed by data augmentation and labeling to produce training pairs, which are costly, complex, or lacking diversity. To address these issues, we presentDiffusionEngine (DE), a data scaling-up engine that provides high-quality detection-oriented training pairs in a single stage. DE consists of a pre-trained diffusion model and an effective Detection-Adapter, contributing to generating scalable, diverse and generalizable detection data in a plug-and-play manner. Detection-Adapter is learned to align the implicit semantic and location knowledge in off-the-shelf diffusion models with detection-aware signals to make better bounding-box predictions. Additionally, we contribute two datasets, i.e., COCO-DE and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#35813;&#22871;&#20214;&#21253;&#25324;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.03886</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#21151;&#33021;&#35299;&#37322;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Function Interpretation Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#35813;&#22871;&#20214;&#21253;&#25324;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21487;&#35835;&#30340;&#25551;&#36848;&#26631;&#35760;&#31070;&#32463;&#32593;&#32476;&#23376;&#27169;&#22359;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#36825;&#20123;&#25551;&#36848;&#21487;&#20197;&#26292;&#38706;&#22833;&#36133;&#12289;&#24341;&#23548;&#24178;&#39044;&#65292;&#29978;&#33267;&#21487;&#20197;&#35299;&#37322;&#37325;&#35201;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#26426;&#26800;&#21407;&#29702;&#30340;&#24050;&#35757;&#32451;&#32593;&#32476;&#25551;&#36848;&#37117;&#28041;&#21450;&#21040;&#23567;&#27169;&#22411;&#12289;&#29421;&#20041;&#29616;&#35937;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#12290;&#22312;&#19981;&#26029;&#22686;&#21152;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#20013;&#26631;&#35760;&#20986;&#25152;&#26377;&#20154;&#21487;&#35299;&#37322;&#30340;&#23376;&#35745;&#31639;&#20960;&#20046;&#32943;&#23450;&#38656;&#35201;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#39564;&#35777;&#25551;&#36848;&#30340;&#24037;&#20855;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#26631;&#35760;&#30340;&#25216;&#26415;&#24320;&#22987;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#26377;&#38480;&#19988;&#20020;&#26102;&#12290;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#39564;&#35777;&#21644;&#27604;&#36739;&#24320;&#25918;&#24335;&#26631;&#35760;&#24037;&#20855;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;FIND&#65288;&#20989;&#25968;&#35299;&#37322;&#21644;&#25551;&#36848;&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#26041;&#27861;&#26500;&#24314;&#27169;&#22359;&#30340;&#22522;&#20934;&#22871;&#20214;&#12290;FIND&#21253;&#21547;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#30340;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of tr
&lt;/p&gt;</description></item><item><title>DoLa&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#23618;&#27425;&#30340;&#36923;&#36753;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20943;&#23569;&#24187;&#35273;&#65292;&#26080;&#38656;&#22806;&#37096;&#30693;&#35782;&#25110;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2309.03883</link><description>&lt;p&gt;
DoLa&#65306;&#36890;&#36807;&#23545;&#27604;&#23618;&#27425;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. (arXiv:2309.03883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03883
&lt;/p&gt;
&lt;p&gt;
DoLa&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#23618;&#27425;&#30340;&#36923;&#36753;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20943;&#23569;&#24187;&#35273;&#65292;&#26080;&#38656;&#22806;&#37096;&#30693;&#35782;&#25110;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#19982;&#39044;&#35757;&#32451;&#26399;&#38388;&#35266;&#23519;&#21040;&#30340;&#20107;&#23454;&#20559;&#31163;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#20943;&#23569;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#24187;&#35273;&#65292;&#23427;&#19981;&#38656;&#35201;&#22312;&#26816;&#32034;&#30340;&#22806;&#37096;&#30693;&#35782;&#25110;&#39069;&#22806;&#30340;&#24494;&#35843;&#19978;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23558;&#36739;&#26202;&#23618;&#21644;&#36739;&#26089;&#23618;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#24471;&#21040;&#30340;&#36923;&#36753;&#24046;&#24322;&#26469;&#33719;&#24471;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#20998;&#24067;&#65292;&#21033;&#29992;&#20102;LLMs&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#36890;&#24120;&#34987;&#35777;&#26126;&#23616;&#37096;&#21270;&#22312;&#29305;&#23450;&#30340;Transformer&#23618;&#20013;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#36890;&#36807;&#23545;&#27604;&#23618;&#27425;&#30340;&#35299;&#30721;&#65288;DoLa&#65289;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23637;&#31034;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#20943;&#23569;&#29983;&#25104;&#19981;&#27491;&#30830;&#20107;&#23454;&#30340;&#24773;&#20917;&#12290;DoLa&#22312;&#22810;&#20010;&#36873;&#25321;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#21319;&#20102;&#30495;&#23454;&#24615;&#65292;&#20363;&#22914;&#25913;&#21892;&#20102;LLaMA&#31995;&#21015;&#27169;&#22411;&#22312;TruthfulQA&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA b
&lt;/p&gt;</description></item><item><title>OpinionGPT&#26159;&#19968;&#20010;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;web&#28436;&#31034;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#21508;&#31181;&#20559;&#35265;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#23427;&#24847;&#22312;&#20351;&#27169;&#22411;&#30340;&#20559;&#35265;&#26174;&#24615;&#21644;&#36879;&#26126;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.03876</link><description>&lt;p&gt;
OpinionGPT: &#27169;&#25311;&#26174;&#24615;&#20559;&#35265;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)
&lt;/p&gt;
&lt;p&gt;
OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs. (arXiv:2309.03876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03876
&lt;/p&gt;
&lt;p&gt;
OpinionGPT&#26159;&#19968;&#20010;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;web&#28436;&#31034;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#21508;&#31181;&#20559;&#35265;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#23427;&#24847;&#22312;&#20351;&#27169;&#22411;&#30340;&#20559;&#35265;&#26174;&#24615;&#21644;&#36879;&#26126;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#29983;&#25104;&#19982;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30456;&#21305;&#37197;&#30340;&#22238;&#24212;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#28041;&#21450;&#35757;&#32451;&#27169;&#22411;&#21644;&#23427;&#20204;&#30340;&#22238;&#24212;&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#29992;&#20110;&#35843;&#25972;LLM&#30340;&#25968;&#25454;&#20027;&#35201;&#30001;&#20855;&#26377;&#29305;&#23450;&#25919;&#27835;&#20559;&#35265;&#30340;&#20154;&#32534;&#20889;&#65292;&#25105;&#20204;&#21487;&#33021;&#20250;&#26399;&#26395;&#29983;&#25104;&#30340;&#22238;&#31572;&#20063;&#20849;&#20139;&#36825;&#31181;&#20559;&#35265;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#24037;&#20316;&#26088;&#22312;&#38500;&#21435;&#36825;&#26679;&#30340;&#27169;&#22411;&#20559;&#35265;&#65292;&#25110;&#25233;&#21046;&#21487;&#33021;&#26377;&#20559;&#35265;&#30340;&#22238;&#31572;&#12290;&#36890;&#36807;&#36825;&#20010;&#28436;&#31034;&#65292;&#25105;&#20204;&#23545;&#25351;&#20196;&#35843;&#25972;&#20013;&#30340;&#20559;&#35265;&#25345;&#26377;&#19981;&#21516;&#30340;&#35266;&#28857;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#26159;&#25233;&#21046;&#23427;&#20204;&#65292;&#32780;&#26159;&#20351;&#23427;&#20204;&#26174;&#24615;&#21644;&#36879;&#26126;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;OpinionGPT&#65292;&#19968;&#20010;&#32593;&#32476;&#28436;&#31034;&#65292;&#29992;&#25143;&#21487;&#20197;&#25552;&#38382;&#24182;&#36873;&#25321;&#25152;&#26377;&#20182;&#20204;&#24076;&#26395;&#35843;&#26597;&#30340;&#20559;&#35265;&#12290;&#35813;&#28436;&#31034;&#23558;&#20351;&#29992;&#22312;&#20195;&#34920;&#27599;&#20010;&#36873;&#25321;&#20559;&#35265;&#30340;&#25991;&#26412;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#24182;&#25490;&#27604;&#36739;&#12290;&#20026;&#20102;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#36873;&#21462;&#20102;11&#20010;...
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable ability to generate fitting responses to natural language instructions. However, an open research question concerns the inherent biases of trained models and their responses. For instance, if the data used to tune an LLM is dominantly written by persons with a specific political bias, we might expect generated answers to share this bias. Current research work seeks to de-bias such models, or suppress potentially biased answers. With this demonstration, we take a different view on biases in instruction-tuning: Rather than aiming to suppress them, we aim to make them explicit and transparent. To this end, we present OpinionGPT, a web demo in which users can ask questions and select all biases they wish to investigate. The demo will answer this question using a model fine-tuned on text representing each of the selected biases, allowing side-by-side comparison. To train the underlying model, we identified 11 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#27169;&#22411;&#65288;FLM-101B&#65289;&#20197;&#21450;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;&#12290;&#36890;&#36807;&#37319;&#29992;&#22686;&#38271;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;LLM&#30340;&#26234;&#33021;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.03852</link><description>&lt;p&gt;
FLM-101B&#65306;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#21644;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;
&lt;/p&gt;
&lt;p&gt;
FLM-101B: An Open LLM and How to Train It with $100K Budget. (arXiv:2309.03852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#27169;&#22411;&#65288;FLM-101B&#65289;&#20197;&#21450;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;&#12290;&#36890;&#36807;&#37319;&#29992;&#22686;&#38271;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;LLM&#30340;&#26234;&#33021;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21457;&#23637;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;&#39640;&#35745;&#31639;&#25104;&#26412;&#65307;&#65288;ii&#65289;&#38590;&#20197;&#36827;&#34892;&#20844;&#24179;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;LLMs&#30340;&#20215;&#26684;&#26114;&#36149;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#23478;&#20027;&#35201;&#21442;&#19982;&#32773;&#26377;&#33021;&#21147;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#30740;&#31350;&#21644;&#24212;&#29992;&#26426;&#20250;&#12290;&#36825;&#20984;&#26174;&#20102;&#25104;&#26412;&#25928;&#30410;&#30340;LLM&#35757;&#32451;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#22686;&#38271;&#31574;&#30053;&#65292;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#19979;&#35757;&#32451;&#20855;&#26377;101B&#21442;&#25968;&#21644;0.31TB&#20196;&#29260;&#30340;LLM&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#33539;&#24335;&#65292;&#29992;&#20110;&#23545;LLMs&#36827;&#34892;&#26234;&#33021;&#30340;&#26234;&#21830;&#35780;&#20272;&#65292;&#36825;&#26159;&#38024;&#23545;&#29616;&#26377;&#35780;&#20272;&#26356;&#27880;&#37325;&#30693;&#35782;&#33021;&#21147;&#30340;&#34917;&#20805;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21253;&#25324;&#31526;&#21495;&#26144;&#23556;&#12289;&#35268;&#21017;&#29702;&#35299;&#12289;&#27169;&#24335;&#25366;&#25496;&#22312;&#20869;&#30340;&#37325;&#35201;&#26234;&#33021;&#26041;&#38754;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks. Despite these successes, their development faces two main challenges: (i) high computational cost; and (ii) difficulty in conducting fair and objective evaluations. LLMs are prohibitively expensive, making it feasible for only a few major players to undertake their training, thereby constraining both research and application opportunities. This underscores the importance of cost-effective LLM training. In this paper, we utilize a growth strategy to significantly reduce LLM training cost. We demonstrate that an LLM with 101B parameters and 0.31TB tokens can be trained on a $100K budget. We also adopt a systematic evaluation paradigm for the IQ evaluation of LLMs, in complement to existing evaluations that focus more on knowledge-oriented abilities. We introduce our benchmark including evaluations on important aspects of intelligence including symbolic mapping, itrule understanding, pattern mining,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28418;&#31227;&#12290;&#36890;&#36807;&#32534;&#30721;&#29983;&#20135;&#25968;&#25454;&#26679;&#26412;&#21644;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;&#26680;&#32479;&#35745;&#26816;&#39564;&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#36317;&#31163;&#24230;&#37327;&#26469;&#27604;&#36739;&#20998;&#24067;&#65292;&#21487;&#20197;&#26377;&#25928;&#20272;&#35745;&#28418;&#31227;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.03831</link><description>&lt;p&gt;
&#25581;&#31034;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#28418;&#31227;&#65306;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#21644;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models. (arXiv:2309.03831v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03831
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28418;&#31227;&#12290;&#36890;&#36807;&#32534;&#30721;&#29983;&#20135;&#25968;&#25454;&#26679;&#26412;&#21644;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;&#26680;&#32479;&#35745;&#26816;&#39564;&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#36317;&#31163;&#24230;&#37327;&#26469;&#27604;&#36739;&#20998;&#24067;&#65292;&#21487;&#20197;&#26377;&#25928;&#20272;&#35745;&#28418;&#31227;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28418;&#31227;&#25351;&#30340;&#26159;&#25968;&#25454;&#25110;&#27169;&#22411;&#36816;&#34892;&#19978;&#19979;&#25991;&#30340;&#32479;&#35745;&#29305;&#24615;&#38543;&#26102;&#38388;&#21464;&#21270;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#29616;&#35937;&#12290;&#22240;&#27492;&#65292;&#20445;&#25345;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#25345;&#32493;&#30417;&#25511;&#36807;&#31243;&#23545;&#20110;&#39044;&#38450;&#28508;&#22312;&#24615;&#33021;&#22238;&#36864;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26377;&#30417;&#30563;&#30340;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#65292;&#20174;&#32780;&#23548;&#33268;&#28418;&#31227;&#26816;&#27979;&#21644;&#20943;&#36731;&#36807;&#31243;&#26102;&#38388;&#36739;&#38271;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26080;&#30417;&#30563;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#20010;&#27493;&#39588;&#30340;&#27969;&#31243;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#28041;&#21450;&#23558;&#29983;&#20135;&#25968;&#25454;&#30340;&#19968;&#20010;&#26679;&#26412;&#20316;&#20026;&#30446;&#26631;&#20998;&#24067;&#65292;&#23558;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20316;&#20026;&#21442;&#32771;&#20998;&#24067;&#36827;&#34892;&#32534;&#30721;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26680;&#30340;&#32479;&#35745;&#26816;&#39564;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#36317;&#31163;&#24230;&#37327;&#26469;&#27604;&#36739;&#21442;&#32771;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#65292;&#20272;&#35745;&#20219;&#20309;&#28508;&#22312;&#30340;&#28418;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#30830;&#23450;&#29983;&#20135;&#25968;&#25454;&#23376;&#38598;&#30340;&#28418;&#31227;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drift in machine learning refers to the phenomenon where the statistical properties of data or context, in which the model operates, change over time leading to a decrease in its performance. Therefore, maintaining a constant monitoring process for machine learning model performance is crucial in order to proactively prevent any potential performance regression. However, supervised drift detection methods require human annotation and consequently lead to a longer time to detect and mitigate the drift. In our proposed unsupervised drift detection method, we follow a two step process. Our first step involves encoding a sample of production data as the target distribution, and the model training data as the reference distribution. In the second step, we employ a kernel-based statistical test that utilizes the maximum mean discrepancy (MMD) distance metric to compare the reference and target distributions and estimate any potential drift. Our method also identifies the subset of production
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#21152;&#36895;&#20302;&#31209;&#20998;&#35299;&#27169;&#22411;&#30340;&#25216;&#26415;&#65306;&#31209;&#20248;&#21270;&#21644;&#39034;&#24207;&#20923;&#32467;&#20998;&#35299;&#23618;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#39640;&#36798;60%&#65292;&#25512;&#29702;&#21534;&#21520;&#37327;&#39640;&#36798;37%&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#25509;&#36817;&#21407;&#22987;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.03824</link><description>&lt;p&gt;
&#20302;&#31209;&#20998;&#35299;&#32593;&#32476;&#30340;&#35757;&#32451;&#21152;&#36895;&#65306;&#39034;&#24207;&#20923;&#32467;&#21644;&#31209;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization. (arXiv:2309.03824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#21152;&#36895;&#20302;&#31209;&#20998;&#35299;&#27169;&#22411;&#30340;&#25216;&#26415;&#65306;&#31209;&#20248;&#21270;&#21644;&#39034;&#24207;&#20923;&#32467;&#20998;&#35299;&#23618;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#39640;&#36798;60%&#65292;&#25512;&#29702;&#21534;&#21520;&#37327;&#39640;&#36798;37%&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#25509;&#36817;&#21407;&#22987;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#20998;&#35299;&#65288;LRD&#65289;&#26159;&#19968;&#31181;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26435;&#37325;&#24352;&#37327;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#24212;&#29992;LRD&#21518;&#22312;&#26550;&#26500;&#20013;&#28155;&#21152;&#20102;&#22823;&#37327;&#26032;&#23618;&#65292;&#22914;&#26524;&#20998;&#35299;&#31209;&#19981;&#22815;&#23567;&#65292;&#21017;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;/&#25512;&#29702;&#21152;&#36895;&#24615;&#19981;&#39640;&#12290;&#38382;&#39064;&#22312;&#20110;&#65292;&#20351;&#29992;&#36739;&#23567;&#30340;&#31209;&#20250;&#22686;&#21152;&#20998;&#35299;&#21518;&#30340;&#26174;&#33879;&#20934;&#30830;&#29575;&#19979;&#38477;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21152;&#36895;&#20302;&#31209;&#20998;&#35299;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#36739;&#23567;&#30340;&#31209;&#36827;&#34892;&#20998;&#35299;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#31209;&#20248;&#21270;&#21644;&#39034;&#24207;&#20923;&#32467;&#20998;&#35299;&#23618;&#12290;&#25105;&#20204;&#22312;&#21367;&#31215;&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#20445;&#25345;&#25509;&#36817;&#21407;&#22987;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#39640;&#36798;60%&#65292;&#25512;&#29702;&#21534;&#21520;&#37327;&#39640;&#36798;37%&#12290;
&lt;/p&gt;
&lt;p&gt;
Low Rank Decomposition (LRD) is a model compression technique applied to the weight tensors of deep learning models in order to reduce the number of trainable parameters and computational complexity. However, due to high number of new layers added to the architecture after applying LRD, it may not lead to a high training/inference acceleration if the decomposition ranks are not small enough. The issue is that using small ranks increases the risk of significant accuracy drop after decomposition. In this paper, we propose two techniques for accelerating low rank decomposed models without requiring to use small ranks for decomposition. These methods include rank optimization and sequential freezing of decomposed layers. We perform experiments on both convolutional and transformer-based models. Experiments show that these techniques can improve the model throughput up to 60% during training and 37% during inference when combined together while preserving the accuracy close to that of the o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#20307;&#27604;&#20363;&#27979;&#37327;&#26500;&#24314;&#30340;AnthroNet&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#24418;&#29366;&#21644;&#23039;&#21183;&#30340;&#20154;&#20307;&#65292;&#24182;&#20197;&#20219;&#24847;&#23039;&#21183;&#29983;&#25104;&#29305;&#23450;&#20154;&#29289;&#36523;&#20221;&#30340;&#20154;&#20307;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#25552;&#20379;&#20102;&#39640;&#31934;&#24230;&#30340;&#20154;&#20307;&#32593;&#26684;&#34920;&#31034;&#21644;&#31934;&#30830;&#30340;&#20154;&#20307;&#27604;&#20363;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.03812</link><description>&lt;p&gt;
AnthroNet: &#36890;&#36807;&#20154;&#20307;&#27604;&#20363;&#29983;&#25104;&#26465;&#20214;&#21270;&#30340;&#20154;&#20307;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnthroNet: Conditional Generation of Humans via Anthropometrics. (arXiv:2309.03812v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03812
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#20307;&#27604;&#20363;&#27979;&#37327;&#26500;&#24314;&#30340;AnthroNet&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#24418;&#29366;&#21644;&#23039;&#21183;&#30340;&#20154;&#20307;&#65292;&#24182;&#20197;&#20219;&#24847;&#23039;&#21183;&#29983;&#25104;&#29305;&#23450;&#20154;&#29289;&#36523;&#20221;&#30340;&#20154;&#20307;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#25552;&#20379;&#20102;&#39640;&#31934;&#24230;&#30340;&#20154;&#20307;&#32593;&#26684;&#34920;&#31034;&#21644;&#31934;&#30830;&#30340;&#20154;&#20307;&#27604;&#20363;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#27867;&#30340;&#20154;&#20307;&#27604;&#20363;&#27979;&#37327;&#32780;&#26500;&#24314;&#30340;&#26032;&#39062;&#20154;&#20307;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#24418;&#29366;&#21644;&#23039;&#21183;&#30340;&#20154;&#20307;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#23545;&#29305;&#23450;&#20154;&#29289;&#36523;&#20221;&#30340;&#30452;&#25509;&#24314;&#27169;&#65292;&#24182;&#33021;&#22815;&#20197;&#20219;&#24847;&#23039;&#21183;&#29983;&#25104;&#20154;&#20307;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#36890;&#36807;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#19981;&#20165;&#25552;&#20379;&#20102;&#39640;&#31934;&#24230;&#30340;&#20154;&#20307;&#32593;&#26684;&#34920;&#31034;&#65292;&#36824;&#20801;&#35768;&#23545;&#20154;&#20307;&#36827;&#34892;&#31934;&#30830;&#30340;&#20154;&#20307;&#27604;&#20363;&#27979;&#37327;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#21160;&#30011;&#24211;&#65292;&#25105;&#20204;&#20026;&#21512;&#25104;&#20154;&#20307;&#30340;&#36523;&#20307;&#21644;&#25163;&#37096;&#36827;&#34892;&#20102;&#20851;&#33410;&#22788;&#29702;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#20013;&#21487;&#23398;&#20064;&#20808;&#39564;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19968;&#20010;&#21253;&#21547;10&#19975;&#20010;&#31243;&#24207;&#29983;&#25104;&#30340;&#20154;&#20307;&#32593;&#26684;&#21644;&#30456;&#24212;&#20154;&#20307;&#27604;&#20363;&#27979;&#37327;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38750;&#21830;&#19994;&#23398;&#26415;&#29992;&#36884;&#19979;&#30340;&#25968;&#30334;&#19975;&#20010;&#29420;&#29305;&#20154;&#29289;&#36523;&#20221;&#21644;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel human body model formulated by an extensive set of anthropocentric measurements, which is capable of generating a wide range of human body shapes and poses. The proposed model enables direct modeling of specific human identities through a deep generative architecture, which can produce humans in any arbitrary pose. It is the first of its kind to have been trained end-to-end using only synthetically generated data, which not only provides highly accurate human mesh representations but also allows for precise anthropometry of the body. Moreover, using a highly diverse animation library, we articulated our synthetic humans' body and hands to maximize the diversity of the learnable priors for model training. Our model was trained on a dataset of $100k$ procedurally-generated posed human meshes and their corresponding anthropometric measurements. Our synthetic data generator can be used to generate millions of unique human identities and poses for non-commercial academic 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#20013;&#30340;&#24494;&#22937;&#36873;&#25321;&#65292;&#29305;&#21035;&#20851;&#27880;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#21457;&#29616;&#31232;&#30095;&#21021;&#22987;&#21270;&#21644;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#21512;&#25104;&#31232;&#30095;&#22855;&#20598;&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#30495;&#23454;&#38382;&#39064;&#30340;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.03800</link><description>&lt;p&gt;
&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65306;&#25968;&#25454;&#12289;&#35745;&#31639;&#12289;&#23485;&#24230;&#21644;&#36816;&#27668;
&lt;/p&gt;
&lt;p&gt;
Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck. (arXiv:2309.03800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#20013;&#30340;&#24494;&#22937;&#36873;&#25321;&#65292;&#29305;&#21035;&#20851;&#27880;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#21457;&#29616;&#31232;&#30095;&#21021;&#22987;&#21270;&#21644;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#21512;&#25104;&#31232;&#30095;&#22855;&#20598;&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#30495;&#23454;&#38382;&#39064;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#20013;&#24494;&#22937;&#30340;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;&#31163;&#32447;&#31232;&#30095;&#22855;&#20598;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#20851;&#22810;&#23618;&#24863;&#30693;&#22120;&#26799;&#24230;&#35757;&#32451;&#30340;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20855;&#26377;&#32479;&#35745;&#26597;&#35810;&#19979;&#30028;&#12290;&#36825;&#20010;&#19979;&#30028;&#21487;&#20197;&#35299;&#37322;&#20026;&#22810;&#36164;&#28304;&#30340;&#26435;&#34913;&#21069;&#27839;&#65306;&#25104;&#21151;&#23398;&#20064;&#21482;&#26377;&#22312;&#19968;&#20010;&#36275;&#22815;&#20016;&#23500;&#65288;&#22823;&#22411;&#27169;&#22411;&#65289;&#12289;&#30693;&#35782;&#28170;&#21338;&#65288;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65289;&#12289;&#32784;&#24515;&#65288;&#35757;&#32451;&#36845;&#20195;&#27425;&#25968;&#22810;&#65289;&#25110;&#24184;&#36816;&#65288;&#38543;&#26426;&#29468;&#27979;&#27425;&#25968;&#22810;&#65289;&#30340;&#24773;&#20917;&#19979;&#25165;&#33021;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31232;&#30095;&#21021;&#22987;&#21270;&#21644;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;&#36825;&#37324;&#65292;&#23485;&#24230;&#36215;&#21040;&#20102;&#24182;&#34892;&#25628;&#32034;&#30340;&#20316;&#29992;&#65306;&#23427;&#22686;&#21152;&#20102;&#25214;&#21040;&#8220;&#24184;&#36816;&#31070;&#32463;&#20803;&#8221;&#30340;&#27010;&#29575;&#65292;&#36825;&#20123;&#31070;&#32463;&#20803;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#31232;&#30095;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#21512;&#25104;&#31232;&#30095;&#22855;&#20598;&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#30495;&#23454;&#38382;&#39064;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the nuanced algorithm design choices for deep learning in the presence of computational-statistical gaps. We begin by considering offline sparse parity learning, a supervised classification problem which admits a statistical query lower bound for gradient-based training of a multilayer perceptron. This lower bound can be interpreted as a multi-resource tradeoff frontier: successful learning can only occur if one is sufficiently rich (large model), knowledgeable (large dataset), patient (many training iterations), or lucky (many random guesses). We show, theoretically and experimentally, that sparse initialization and increasing network width yield significant improvements in sample efficiency in this setting. Here, width plays the role of parallel search: it amplifies the probability of finding "lottery ticket" neurons, which learn sparse features more sample-efficiently. Finally, we show that the synthetic sparse parity task can be useful as a proxy for real pro
&lt;/p&gt;</description></item><item><title>FisheyePP4AV &#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#40060;&#30524;&#30456;&#26426;&#22270;&#20687;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#23454;&#38469;&#36947;&#36335;&#39550;&#39542;&#22330;&#26223;&#20013;&#26816;&#27979;&#21644;&#21311;&#21517;&#21270;&#34892;&#20154;&#38754;&#37096;&#21644;&#36710;&#29260;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#29992;&#20110;&#20174;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#20154;&#33080;&#21644;&#36710;&#29260;&#35782;&#21035;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2309.03799</link><description>&lt;p&gt;
FisheyePP4AV: &#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#40060;&#30524;&#30456;&#26426;&#22270;&#20687;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FisheyePP4AV: A privacy-preserving method for autonomous vehicles on fisheye camera images. (arXiv:2309.03799v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03799
&lt;/p&gt;
&lt;p&gt;
FisheyePP4AV &#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#40060;&#30524;&#30456;&#26426;&#22270;&#20687;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#23454;&#38469;&#36947;&#36335;&#39550;&#39542;&#22330;&#26223;&#20013;&#26816;&#27979;&#21644;&#21311;&#21517;&#21270;&#34892;&#20154;&#38754;&#37096;&#21644;&#36710;&#29260;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#29992;&#20110;&#20174;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#20154;&#33080;&#21644;&#36710;&#29260;&#35782;&#21035;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19990;&#30028;&#19978;&#35768;&#22810;&#22320;&#26041;&#65292;&#23545;&#20844;&#20849;&#36947;&#36335;&#19978;&#25910;&#38598;&#30340;&#22823;&#37327;&#25968;&#25454;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20351;&#29992;&#24050;&#32463;&#22686;&#21152;&#12290;&#20026;&#20102;&#22312;&#23454;&#38469;&#36947;&#36335;&#39550;&#39542;&#22330;&#26223;&#20013;&#26816;&#27979;&#21644;&#21311;&#21517;&#21270;&#34892;&#20154;&#38754;&#37096;&#21644;&#38468;&#36817;&#30340;&#36710;&#29260;&#65292;&#36843;&#20999;&#38656;&#35201;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#38543;&#30528;&#25910;&#38598;&#30340;&#25968;&#25454;&#36234;&#26469;&#36234;&#22810;&#65292;&#28041;&#21450;&#38544;&#31169;&#30340;&#25285;&#24551;&#20063;&#22312;&#22686;&#21152;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#34892;&#20154;&#38754;&#37096;&#21644;&#21608;&#22260;&#36710;&#36742;&#29260;&#29031;&#12290;&#26222;&#36890;&#30456;&#26426;&#21644;&#40060;&#30524;&#30456;&#26426;&#26159;&#36890;&#24120;&#23433;&#35013;&#22312;&#37319;&#38598;&#36710;&#36742;&#19978;&#30340;&#20004;&#31181;&#24120;&#35265;&#30456;&#26426;&#31867;&#22411;&#12290;&#30001;&#20110;&#22797;&#26434;&#30340;&#30456;&#26426;&#22833;&#30495;&#27169;&#22411;&#65292;&#19982;&#26222;&#36890;&#22270;&#20687;&#30456;&#27604;&#65292;&#40060;&#30524;&#30456;&#26426;&#22270;&#20687;&#21457;&#29983;&#20102;&#21464;&#24418;&#12290;&#36825;&#23548;&#33268;&#22312;&#20351;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#23545;&#26080;&#20154;&#39550;&#39542;&#36710;&#36742;&#25293;&#25668;&#30340;&#40060;&#30524;&#30456;&#26426;&#29031;&#29255;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#36981;&#23432;&#20960;&#39033;&#27861;&#24459;&#35201;&#27714;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20960;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#20154;&#33080;&#21644;&#36710;&#29260;&#35782;&#21035;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many parts of the world, the use of vast amounts of data collected on public roadways for autonomous driving has increased. In order to detect and anonymize pedestrian faces and nearby car license plates in actual road-driving scenarios, there is an urgent need for effective solutions. As more data is collected, privacy concerns regarding it increase, including but not limited to pedestrian faces and surrounding vehicle license plates. Normal and fisheye cameras are the two common camera types that are typically mounted on collection vehicles. With complex camera distortion models, fisheye camera images were deformed in contrast to regular images. It causes computer vision tasks to perform poorly when using numerous deep learning models. In this work, we pay particular attention to protecting privacy while yet adhering to several laws for fisheye camera photos taken by driverless vehicles. First, we suggest a framework for extracting face and plate identification knowledge from seve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#26102;&#38388;&#32534;&#30721;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#24212;&#29992;&#30340;CPU&#39057;&#29575;&#35843;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#25512;&#23548;&#20986;&#39640;&#25928;&#30340;&#21151;&#29575;&#31649;&#29702;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;Linux&#20869;&#32622;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.03779</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#32534;&#30721;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#24212;&#29992;&#30340;CPU&#39057;&#29575;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
CPU frequency scheduling of real-time applications on embedded devices with temporal encoding-based deep reinforcement learning. (arXiv:2309.03779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#26102;&#38388;&#32534;&#30721;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#24212;&#29992;&#30340;CPU&#39057;&#29575;&#35843;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#25512;&#23548;&#20986;&#39640;&#25928;&#30340;&#21151;&#29575;&#31649;&#29702;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;Linux&#20869;&#32622;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#35774;&#22791;&#32463;&#24120;&#29992;&#20110;&#29289;&#32852;&#32593;&#21644;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#65292;&#29992;&#20110;&#25191;&#34892;&#26377;&#36719;&#25130;&#27490;&#26399;&#30340;&#21608;&#26399;&#24615;&#19987;&#29992;&#20219;&#21153;&#12290;&#36825;&#39033;&#24037;&#20316;&#33268;&#21147;&#20110;&#24320;&#21457;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#25512;&#23548;&#20986;&#39640;&#25928;&#30340;&#21151;&#29575;&#31649;&#29702;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#29616;&#26377;&#30340;Linux&#20869;&#32622;&#26041;&#27861;&#22312;&#23567;&#22411;&#35774;&#22791;&#20013;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#31181;&#20856;&#22411;&#30340;&#24037;&#20316;&#36127;&#33655;/&#31995;&#32479;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#23545;&#20110;Linux&#20869;&#32622;&#35299;&#20915;&#26041;&#26696;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#26102;&#38388;&#32534;&#30721;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#20197;&#25512;&#23548;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;DVFS&#35843;&#24230;&#31243;&#24207;&#65292;&#21363;&#20351;&#23384;&#22312;&#36825;&#19977;&#31181;&#31995;&#32479;&#27169;&#24335;&#12290;&#25512;&#23548;&#20986;&#30340;&#35843;&#24230;&#31243;&#24207;&#20165;&#20351;&#29992;&#19968;&#20010;&#24615;&#33021;&#35745;&#25968;&#22120;&#65292;&#19982;&#20869;&#32622;&#30340;Linux&#26426;&#21046;&#30456;&#21516;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#24037;&#20316;&#36127;&#33655;&#30340;&#26174;&#24335;&#20219;&#21153;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Nvidia Jetson Nano Board&#19978;&#23454;&#29616;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#65292;&#24182;&#36827;&#34892;&#20102;&#20845;&#31181;&#24212;&#29992;&#31243;&#24207;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#20004;&#20010;&#33258;&#35774;&#35745;&#30340;&#21644;&#22235;&#20010;&#22522;&#20934;&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Small devices are frequently used in IoT and smart-city applications to perform periodic dedicated tasks with soft deadlines. This work focuses on developing methods to derive efficient power-management methods for periodic tasks on small devices. We first study the limitations of the existing Linux built-in methods used in small devices. We illustrate three typical workload/system patterns that are challenging to manage with Linux's built-in solutions. We develop a reinforcement-learning-based technique with temporal encoding to derive an effective DVFS governor even with the presence of the three system patterns. The derived governor uses only one performance counter, the same as the built-in Linux mechanism, and does not require an explicit task model for the workload. We implemented a prototype system on the Nvidia Jetson Nano Board and experimented with it with six applications, including two self-designed and four benchmark applications. Under different deadline constraints, our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20256;&#23548;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#30340;&#35856;&#27874;&#25193;&#23637;&#65292;&#21033;&#29992;&#20256;&#23548;&#23884;&#20837;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#26469;&#25512;&#26029;&#22312;&#25512;&#29702;&#26102;&#24341;&#20837;&#30340;&#26032;&#23454;&#20307;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03773</link><description>&lt;p&gt;
&#25193;&#23637;&#20256;&#23548;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#29992;&#20110;&#24402;&#32435;&#36923;&#36753;&#20851;&#31995;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Extending Transductive Knowledge Graph Embedding Models for Inductive Logical Relational Inference. (arXiv:2309.03773v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20256;&#23548;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#30340;&#35856;&#27874;&#25193;&#23637;&#65292;&#21033;&#29992;&#20256;&#23548;&#23884;&#20837;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#26469;&#25512;&#26029;&#22312;&#25512;&#29702;&#26102;&#24341;&#20837;&#30340;&#26032;&#23454;&#20307;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30693;&#35782;&#22270;&#30340;&#19979;&#28216;&#25512;&#29702;&#20219;&#21153;&#65292;&#20363;&#22914;&#20851;&#31995;&#39044;&#27979;&#65292;&#22312;&#20256;&#23548;&#35774;&#32622;&#19979;&#24050;&#32463;&#25104;&#21151;&#22788;&#29702;&#20102;&#12290;&#20026;&#20102;&#22788;&#29702;&#24402;&#32435;&#35774;&#32622;&#65292;&#20063;&#23601;&#26159;&#22312;&#25512;&#29702;&#26102;&#24341;&#20837;&#26032;&#23454;&#20307;&#21040;&#30693;&#35782;&#22270;&#20013;&#65292;&#36739;&#26032;&#30340;&#24037;&#20316;&#36873;&#25321;&#20102;&#36890;&#36807;&#32593;&#32476;&#23376;&#22270;&#32467;&#26500;&#30340;&#22797;&#26434;&#20989;&#25968;&#23398;&#20064;&#30693;&#35782;&#22270;&#30340;&#38544;&#24335;&#34920;&#31034;&#30340;&#27169;&#22411;&#65292;&#36890;&#24120;&#30001;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21442;&#25968;&#21270;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#25104;&#26412;&#26159;&#22686;&#21152;&#30340;&#21442;&#25968;&#21270;&#12289;&#38477;&#20302;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;&#20854;&#20182;&#19979;&#28216;&#25512;&#29702;&#20219;&#21153;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#30340;&#35856;&#27874;&#25193;&#23637;&#26469;&#24357;&#21512;&#20256;&#32479;&#20256;&#23548;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#21644;&#36739;&#26032;&#30340;&#24402;&#32435;&#20851;&#31995;&#39044;&#27979;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#20256;&#23548;&#23884;&#20837;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#26469;&#25512;&#26029;&#22312;&#25512;&#29702;&#26102;&#24341;&#20837;&#30340;&#26032;&#23454;&#20307;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many downstream inference tasks for knowledge graphs, such as relation prediction, have been handled successfully by knowledge graph embedding techniques in the transductive setting. To address the inductive setting wherein new entities are introduced into the knowledge graph at inference time, more recent work opts for models which learn implicit representations of the knowledge graph through a complex function of a network's subgraph structure, often parametrized by graph neural network architectures. These come at the cost of increased parametrization, reduced interpretability and limited generalization to other downstream inference tasks. In this work, we bridge the gap between traditional transductive knowledge graph embedding approaches and more recent inductive relation prediction models by introducing a generalized form of harmonic extension which leverages representations learned through transductive embedding methods to infer representations of new entities introduced at infe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31639;&#27861;&#65292;&#23558;&#34920;&#31034;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#24212;&#29992;&#20110;&#21160;&#24577;&#21644;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#12290;&#20854;&#20013;&#20351;&#29992;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27719;&#32858;&#21644;&#36339;&#36291;&#36830;&#25509;&#26469;&#25913;&#36827;&#31163;&#25955;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#34920;&#31034;&#26041;&#27861;&#24471;&#20986;&#27880;&#24847;&#21147;&#32593;&#32476;&#22312;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.03758</link><description>&lt;p&gt;
&#34920;&#31034;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#28151;&#21512;&#31639;&#27861;&#29992;&#20110;&#21160;&#24577;&#21644;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Hybrid of representation learning and reinforcement learning for dynamic and complex robotic motion planning. (arXiv:2309.03758v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03758
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31639;&#27861;&#65292;&#23558;&#34920;&#31034;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#24212;&#29992;&#20110;&#21160;&#24577;&#21644;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#12290;&#20854;&#20013;&#20351;&#29992;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27719;&#32858;&#21644;&#36339;&#36291;&#36830;&#25509;&#26469;&#25913;&#36827;&#31163;&#25955;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#34920;&#31034;&#26041;&#27861;&#24471;&#20986;&#27880;&#24847;&#21147;&#32593;&#32476;&#22312;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#35268;&#21010;&#26159;&#26426;&#22120;&#20154;&#20915;&#31574;&#30340;&#26680;&#24515;&#12290;&#20256;&#32479;&#30340;&#35268;&#21010;&#31639;&#27861;&#22914;&#22270;&#25628;&#32034;&#21644;&#22522;&#20110;&#21453;&#24212;&#30340;&#31639;&#27861;&#22312;&#23494;&#38598;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#25361;&#25112;&#12290;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20135;&#29983;&#27425;&#20248;&#30340;&#19968;&#27493;&#39044;&#27979;&#65292;&#23548;&#33268;&#35768;&#22810;&#30896;&#25758;&#12290;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#29983;&#25104;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#25910;&#25947;&#36895;&#24230;&#24930;&#12289;&#25910;&#25947;&#32467;&#26524;&#27425;&#20248;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#28151;&#21512;&#31639;&#27861;&#65306;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27719;&#32858;&#21644;&#36339;&#36291;&#36830;&#25509;&#29992;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31163;&#25955;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;LSA-DSAC&#65289;&#12290;&#39318;&#20808;&#65292;&#22270;&#32593;&#32476;&#65288;&#20851;&#31995;&#22270;&#65289;&#21644;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;&#27880;&#24847;&#21147;&#26435;&#37325;&#65289;&#35299;&#37322;&#29615;&#22659;&#29366;&#24577;&#65292;&#29992;&#20110;&#23398;&#20064;&#31163;&#25955;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#36825;&#20004;&#31181;&#34920;&#31034;&#26041;&#27861;&#30340;&#24046;&#24322;&#20998;&#26512;&#65292;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#20248;&#20110;&#22270;&#32593;&#32476;&#22312;&#25105;&#20204;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion planning is the soul of robot decision making. Classical planning algorithms like graph search and reaction-based algorithms face challenges in cases of dense and dynamic obstacles. Deep learning algorithms generate suboptimal one-step predictions that cause many collisions. Reinforcement learning algorithms generate optimal or near-optimal time-sequential predictions. However, they suffer from slow convergence, suboptimal converged results, and overfittings. This paper introduces a hybrid algorithm for robotic motion planning: long short-term memory (LSTM) pooling and skip connection for attention-based discrete soft actor critic (LSA-DSAC). First, graph network (relational graph) and attention network (attention weight) interpret the environmental state for the learning of the discrete soft actor critic algorithm. The expressive power of attention network outperforms that of graph in our task by difference analysis of these two representation methods. However, attention based 
&lt;/p&gt;</description></item><item><title>TSGBench&#26159;&#39318;&#20010;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22522;&#20934;&#65292;&#29992;&#20110;&#32479;&#19968;&#21644;&#20840;&#38754;&#35780;&#20272;TSG&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;&#35780;&#20272;&#12289;&#25968;&#25454;&#38598;&#36873;&#25321;&#21644;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.03755</link><description>&lt;p&gt;
TSGBench&#65306;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TSGBench: Time Series Generation Benchmark. (arXiv:2309.03755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03755
&lt;/p&gt;
&lt;p&gt;
TSGBench&#26159;&#39318;&#20010;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22522;&#20934;&#65292;&#29992;&#20110;&#32479;&#19968;&#21644;&#20840;&#38754;&#35780;&#20272;TSG&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;&#35780;&#20272;&#12289;&#25968;&#25454;&#38598;&#36873;&#25321;&#21644;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;(TSG)&#22312;&#25968;&#25454;&#22686;&#24378;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#22810;&#20010;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19977;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#32463;&#24120;&#38024;&#23545;&#31867;&#20284;&#30340;&#27169;&#22411;&#31867;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#38480;&#21046;&#20102;&#23545;&#24615;&#33021;&#33021;&#21147;&#30340;&#25972;&#20307;&#35270;&#35282;&#12290;(2)&#20351;&#29992;&#19987;&#38376;&#30340;&#21512;&#25104;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#24341;&#20837;&#20102;&#20559;&#20506;&#65292;&#38459;&#30861;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;(3)&#27169;&#31946;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24448;&#24448;&#19982;&#33258;&#23450;&#20041;&#32593;&#32476;&#25110;&#19979;&#28216;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#38459;&#30861;&#20102;&#19968;&#33268;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;\textsf {TSGBench}&#65292;&#20316;&#20026;&#39318;&#20010;TSG&#22522;&#20934;&#65292;&#26088;&#22312;&#32479;&#19968;&#21644;&#20840;&#38754;&#35780;&#20272;TSG&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;(1)&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#12289;&#38754;&#21521;TSG&#30340;&#20844;&#24320;&#23454;&#38469;&#25968;&#25454;&#38598;&#25910;&#38598;&#65292;&#20197;&#21450;&#26631;&#20934;&#21270;&#30340;&#39044;&#22788;&#29702;&#27969;&#31243;&#65307;(2)&#19968;&#22871;&#32508;&#21512;&#30340;&#35780;&#20272;&#25351;&#26631;&#22871;&#20214;&#65292;&#21253;&#25324;&#22522;&#26412;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Synthetic Time Series Generation (TSG) is crucial in a range of applications, including data augmentation, anomaly detection, and privacy preservation. Although significant strides have been made in this field, existing methods exhibit three key limitations: (1) They often benchmark against similar model types, constraining a holistic view of performance capabilities. (2) The use of specialized synthetic and private datasets introduces biases and hampers generalizability. (3) Ambiguous evaluation measures, often tied to custom networks or downstream tasks, hinder consistent and fair comparison.  To overcome these limitations, we introduce \textsf{TSGBench}, the inaugural TSG Benchmark, designed for a unified and comprehensive assessment of TSG methods. It comprises three modules: (1) a curated collection of publicly available, real-world datasets tailored for TSG, together with a standardized preprocessing pipeline; (2) a comprehensive evaluation measures suite including vanilla measur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#23545;&#35805;&#31995;&#32479;&#12290;&#22312;&#35774;&#35745;&#21644;&#24320;&#21457;&#38454;&#27573;&#65292;LLM&#21487;&#20197;&#24110;&#21161;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12289;&#25552;&#21462;&#23454;&#20307;&#21644;&#21516;&#20041;&#35789;&#12289;&#26412;&#22320;&#21270;&#21644;&#35282;&#33394;&#35774;&#35745;&#12290;&#22312;&#36816;&#33829;&#38454;&#27573;&#65292;LLM&#21487;&#20197;&#36741;&#21161;&#19978;&#19979;&#25991;&#21270;&#12289;&#24847;&#22270;&#20998;&#31867;&#12289;&#33258;&#21160;&#32416;&#27491;&#35805;&#35821;&#12289;&#25913;&#20889;&#22238;&#22797;&#12289;&#25688;&#35201;&#21644;&#20351;&#38381;&#21512;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#31169;&#20154;&#38134;&#34892;&#39046;&#22495;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03748</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Enhancing Pipeline-Based Conversational Agents with Large Language Models. (arXiv:2309.03748v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#23545;&#35805;&#31995;&#32479;&#12290;&#22312;&#35774;&#35745;&#21644;&#24320;&#21457;&#38454;&#27573;&#65292;LLM&#21487;&#20197;&#24110;&#21161;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12289;&#25552;&#21462;&#23454;&#20307;&#21644;&#21516;&#20041;&#35789;&#12289;&#26412;&#22320;&#21270;&#21644;&#35282;&#33394;&#35774;&#35745;&#12290;&#22312;&#36816;&#33829;&#38454;&#27573;&#65292;LLM&#21487;&#20197;&#36741;&#21161;&#19978;&#19979;&#25991;&#21270;&#12289;&#24847;&#22270;&#20998;&#31867;&#12289;&#33258;&#21160;&#32416;&#27491;&#35805;&#35821;&#12289;&#25913;&#20889;&#22238;&#22797;&#12289;&#25688;&#35201;&#21644;&#20351;&#38381;&#21512;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#31169;&#20154;&#38134;&#34892;&#39046;&#22495;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20195;&#29702;&#22120;&#65288;&#22914;GPT-4&#65289;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21830;&#19994;&#21270;&#23545;&#35805;&#31995;&#32479;&#24320;&#21457;&#24037;&#20855;&#26159;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#65292;&#24182;&#19988;&#22312;&#36827;&#34892;&#20154;&#31867;&#23545;&#35805;&#26102;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#22312;&#20197;&#19979;&#20004;&#20010;&#38454;&#27573;&#20013;&#22686;&#24378;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#65306;1&#65289;&#35774;&#35745;&#21644;&#24320;&#21457;&#38454;&#27573;&#65307;2&#65289;&#36816;&#33829;&#38454;&#27573;&#12290;&#22312;1&#65289;&#20013;&#65292;LLM&#21487;&#20197;&#22312;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12289;&#25552;&#21462;&#23454;&#20307;&#21644;&#21516;&#20041;&#35789;&#12289;&#26412;&#22320;&#21270;&#21644;&#35282;&#33394;&#35774;&#35745;&#26041;&#38754;&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;2&#65289;&#20013;&#65292;LLM&#21487;&#20197;&#36741;&#21161;&#19978;&#19979;&#25991;&#21270;&#12289;&#24847;&#22270;&#20998;&#31867;&#20197;&#38450;&#27490;&#23545;&#35805;&#20013;&#26029;&#21644;&#22788;&#29702;&#36229;&#20986;&#33539;&#22260;&#30340;&#38382;&#39064;&#12289;&#33258;&#21160;&#32416;&#27491;&#35805;&#35821;&#12289;&#25913;&#20889;&#22238;&#22797;&#12289;&#21046;&#23450;&#28040;&#27495;&#38382;&#21477;&#12289;&#25688;&#35201;&#21644;&#20351;&#38381;&#21512;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#31169;&#20154;&#38134;&#34892;&#39046;&#22495;&#36827;&#34892;&#20102;&#20351;&#29992;GPT-4&#30340;&#38750;&#27491;&#24335;&#23454;&#39564;&#65292;&#20197;&#23454;&#38469;&#31034;&#20363;&#35777;&#26126;&#19978;&#36848;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The latest advancements in AI and deep learning have led to a breakthrough in large language model (LLM)-based agents such as GPT-4. However, many commercial conversational agent development tools are pipeline-based and have limitations in holding a human-like conversation. This paper investigates the capabilities of LLMs to enhance pipeline-based conversational agents during two phases: 1) in the design and development phase and 2) during operations. In 1) LLMs can aid in generating training data, extracting entities and synonyms, localization, and persona design. In 2) LLMs can assist in contextualization, intent classification to prevent conversational breakdown and handle out-of-scope questions, auto-correcting utterances, rephrasing responses, formulating disambiguation questions, summarization, and enabling closed question-answering capabilities. We conducted informal experiments with GPT-4 in the private banking domain to demonstrate the scenarios above with a practical example.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03720</link><description>&lt;p&gt;
&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism. (arXiv:2309.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35268;&#21010;&#22825;&#28982;&#27668;&#20379;&#24212;&#21644;&#28040;&#36153;&#20197;&#21450;&#20248;&#21270;&#33719;&#24471;&#22825;&#28982;&#27668;&#25104;&#26412;&#26041;&#38754;&#65292;&#32771;&#34385;&#23395;&#33410;&#24615;&#21644;&#36235;&#21183;&#24615;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27493; ahead &#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#38598;&#25104;&#20102;&#21464;&#28857;&#26816;&#27979;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;Hoeffding&#26641;&#39044;&#27979;&#22120;&#20316;&#20026;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21098;&#35009;&#30340;&#31934;&#30830;&#32447;&#24615;&#26102;&#38388;&#65288;PELT&#65289;&#31639;&#27861;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#12290;&#21464;&#28857;&#26816;&#27979;&#38598;&#25104;&#20351;&#24471;&#36873;&#25321;&#19981;&#21516;&#30340;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. However, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. This article introduces a novel multistep ahead forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. The performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. We employed Hoeffding tree predictors as forecasting models and the Pruned Exact Linear Time (PELT) algorithm for the change point detection procedure. The change point detection integration enables selecting a different model
&lt;/p&gt;</description></item><item><title>PyGraft&#26159;&#19968;&#20010;Python&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29983;&#25104;&#39640;&#24230;&#23450;&#21046;&#30340;&#27169;&#24335;&#21644;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#30830;&#20445;&#29983;&#25104;&#30340;&#36164;&#28304;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03685</link><description>&lt;p&gt;
PyGraft: &#22312;&#20320;&#30340;&#25351;&#23574;&#29983;&#25104;&#21487;&#37197;&#32622;&#30340;&#27169;&#24335;&#21644;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
PyGraft: Configurable Generation of Schemas and Knowledge Graphs at Your Fingertips. (arXiv:2309.03685v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03685
&lt;/p&gt;
&lt;p&gt;
PyGraft&#26159;&#19968;&#20010;Python&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29983;&#25104;&#39640;&#24230;&#23450;&#21046;&#30340;&#27169;&#24335;&#21644;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#30830;&#20445;&#29983;&#25104;&#30340;&#36164;&#28304;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#25968;&#25454;&#34920;&#31034;&#21644;&#31649;&#29702;&#33539;&#24335;&#12290;KG&#36890;&#24120;&#22522;&#20110;&#27169;&#24335;&#65288;&#20363;&#22914;&#26412;&#20307;&#35770;&#65289;&#26469;&#25429;&#33719;&#20107;&#23454;&#20449;&#24687;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#12290;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19968;&#20123;KG&#24050;&#32463;&#25104;&#20026;&#26631;&#20934;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20165;&#20381;&#36182;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#21512;&#26159;&#19981;&#36275;&#20197;&#35780;&#20272;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#12290;&#22312;&#19968;&#20123;&#25968;&#25454;&#25935;&#24863;&#39046;&#22495;&#65292;&#22914;&#25945;&#32946;&#25110;&#21307;&#23398;&#65292;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#33719;&#21462;&#26356;&#21152;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;PyGraft&#65292;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#24230;&#23450;&#21046;&#30340;&#12289;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#27169;&#24335;&#21644;&#30693;&#35782;&#22270;&#35889;&#12290;&#21512;&#25104;&#30340;&#27169;&#24335;&#21253;&#25324;&#21508;&#31181;RDFS&#21644;OWL&#26500;&#36896;&#65292;&#32780;&#21512;&#25104;&#30340;KG&#21017;&#27169;&#25311;&#20102;&#30495;&#23454;KG&#30340;&#29305;&#24615;&#21644;&#35268;&#27169;&#12290;&#36890;&#36807;&#36816;&#34892;&#25551;&#36848;&#36923;&#36753;&#65288;DL&#65289;&#25512;&#29702;&#22120;&#65292;&#26368;&#32456;&#30830;&#20445;&#29983;&#25104;&#36164;&#28304;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) have emerged as a prominent data representation and management paradigm. Being usually underpinned by a schema (e.g. an ontology), KGs capture not only factual information but also contextual knowledge. In some tasks, a few KGs established themselves as standard benchmarks. However, recent works outline that relying on a limited collection of datasets is not sufficient to assess the generalization capability of an approach. In some data-sensitive fields such as education or medicine, access to public datasets is even more limited. To remedy the aforementioned issues, we release PyGraft, a Python-based tool that generates highly customized, domain-agnostic schemas and knowledge graphs. The synthesized schemas encompass various RDFS and OWL constructs, while the synthesized KGs emulate the characteristics and scale of real-world KGs. Logical consistency of the generated resources is ultimately ensured by running a description logic (DL) reasoner. By providing a way
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24369;&#26631;&#35760;&#35270;&#39057;&#20013;&#29983;&#25104;&#20525;&#40657;&#29481;&#29481;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#31350;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#31639;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#25454;&#20934;&#22791;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#27491;&#30830;&#30340;&#25968;&#25454;&#20998;&#31163;&#23545;&#20110;&#20998;&#31867;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;</title><link>http://arxiv.org/abs/2309.03671</link><description>&lt;p&gt;
&#20174;&#24369;&#26631;&#35760;&#35270;&#39057;&#20013;&#29983;&#25104;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20525;&#40657;&#29481;&#29481;&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dataset Generation and Bonobo Classification from Weakly Labelled Videos. (arXiv:2309.03671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24369;&#26631;&#35760;&#35270;&#39057;&#20013;&#29983;&#25104;&#20525;&#40657;&#29481;&#29481;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#31350;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#31639;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#25454;&#20934;&#22791;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#27491;&#30830;&#30340;&#25968;&#25454;&#20998;&#31163;&#23545;&#20110;&#20998;&#31867;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#24120;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#30340;&#20525;&#40657;&#29481;&#29481;&#26816;&#27979;&#21644;&#20998;&#31867;&#27969;&#31243;&#12290;&#35813;&#24212;&#29992;&#30340;&#21160;&#26426;&#26159;&#20026;&#20102;&#22312;&#27809;&#26377;&#20154;&#30340;&#24110;&#21161;&#19979;&#65292;&#20351;&#29992;&#35302;&#25720;&#23631;&#35774;&#22791;&#23545;&#20525;&#40657;&#29481;&#29481;&#22312;&#23427;&#20204;&#30340;&#22260;&#26639;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#22522;&#20110;&#20525;&#40657;&#29481;&#29481;&#24405;&#20687;&#30340;&#33258;&#21160;&#20135;&#29983;&#30340;&#12290;&#36825;&#20123;&#24405;&#20687;&#26159;&#24369;&#26631;&#35760;&#30340;&#65292;&#24182;&#36890;&#36807;&#29461;&#29492;&#26816;&#27979;&#22120;&#36827;&#34892;&#31354;&#38388;&#26816;&#27979;&#65292;&#20197;&#26816;&#27979;&#35270;&#39057;&#20013;&#20986;&#29616;&#30340;&#20010;&#20307;&#12290;&#20351;&#29992;&#25163;&#24037;&#29305;&#24449;&#20197;&#21450;&#19981;&#21516;&#30340;&#20998;&#31867;&#31639;&#27861;&#21644;&#22522;&#20110;ResNet&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20525;&#40657;&#29481;&#29481;&#35782;&#21035;&#30340;&#30740;&#31350;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#31163;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#24211;&#30340;&#25286;&#20998;&#19978;&#65292;&#20197;&#20998;&#31867;&#20934;&#30830;&#24230;&#20316;&#20026;&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25968;&#25454;&#20934;&#22791;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#38169;&#35823;&#30340;&#25968;&#25454;&#20998;&#31163;&#22914;&#20309;&#23548;&#33268;&#34394;&#20551;&#30340;&#22909;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#32463;&#36807;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#20998;&#31163;&#21518;&#65292;&#24471;&#21040;&#20102;&#26368;&#20339;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a bonobo detection and classification pipeline built from the commonly used machine learning methods. Such application is motivated by the need to test bonobos in their enclosure using touch screen devices without human assistance. This work introduces a newly acquired dataset based on bonobo recordings generated semi-automatically. The recordings are weakly labelled and fed to a macaque detector in order to spatially detect the individual present in the video. Handcrafted features coupled with different classification algorithms and deep-learning methods using a ResNet architecture are investigated for bonobo identification. Performance is compared in terms of classification accuracy on the splits of the database using different data separation methods. We demonstrate the importance of data preparation and how a wrong data separation can lead to false good results. Finally, after a meaningful separation of the data, the best classification performance is obtained u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#22914;&#20309;&#36890;&#36807;&#24494;&#23567;&#20462;&#25913;&#24178;&#25200;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#21457;&#29616;&#36825;&#21487;&#33021;&#26159;&#39640;&#32500;&#36755;&#20837;&#25968;&#25454;&#19979;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#23454;&#38469;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#34892;&#20026;&#65292;&#21253;&#25324;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#38543;&#26426;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#39564;&#35777;&#23454;&#39564;&#36824;&#34920;&#26126;&#20102;&#30456;&#21516;&#29616;&#35937;&#22312;&#23454;&#38469;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2309.03665</link><description>&lt;p&gt;
&#22914;&#20309;&#25915;&#20987;&#21487;&#20197;&#24178;&#25200;&#30475;&#20284;&#31283;&#23450;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
How adversarial attacks can disrupt seemingly stable accurate classifiers. (arXiv:2309.03665v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#22914;&#20309;&#36890;&#36807;&#24494;&#23567;&#20462;&#25913;&#24178;&#25200;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#21457;&#29616;&#36825;&#21487;&#33021;&#26159;&#39640;&#32500;&#36755;&#20837;&#25968;&#25454;&#19979;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#23454;&#38469;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#34892;&#20026;&#65292;&#21253;&#25324;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#38543;&#26426;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#39564;&#35777;&#23454;&#39564;&#36824;&#34920;&#26126;&#20102;&#30456;&#21516;&#29616;&#35937;&#22312;&#23454;&#38469;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#36890;&#36807;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#65292;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#21407;&#26412;&#20934;&#30830;&#30340;&#23398;&#20064;&#31995;&#32479;&#30340;&#36755;&#20986;&#12290;&#20855;&#26377;&#35773;&#21050;&#24847;&#21619;&#30340;&#26159;&#65292;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#21363;&#20351;&#31995;&#32479;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#22823;&#24133;&#24230;&#38543;&#26426;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36755;&#20837;&#25968;&#25454;&#30340;&#23567;&#20247;&#12289;&#26131;&#20110;&#26500;&#36896;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#21487;&#33021;&#26159;&#39640;&#32500;&#36755;&#20837;&#25968;&#25454;&#19979;&#20998;&#31867;&#22120;&#30340;&#19968;&#20010;&#22522;&#26412;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36890;&#29992;&#24615;&#21644;&#26222;&#36866;&#24615;&#26694;&#26550;&#65292;&#20854;&#20013;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#34892;&#20026;&#20855;&#26377;&#39640;&#27010;&#29575;&#20986;&#29616;&#65292;&#23588;&#20854;&#26159;&#65288;&#21407;&#26412;&#20934;&#30830;&#30340;&#65289;&#27169;&#22411;&#23545;&#26131;&#20110;&#26500;&#36896;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#21516;&#26102;&#23481;&#26131;&#21463;&#21040;&#36755;&#20837;&#25968;&#25454;&#30340;&#38543;&#26426;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#19978;&#39564;&#35777;&#20102;&#30456;&#21516;&#29616;&#35937;&#22312;&#23454;&#38469;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30452;&#25509;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#20351;&#26159;&#22823;&#24133;&#24230;&#30340;&#21152;&#24615;&#38543;&#26426;&#22122;&#22768;&#20063;&#26080;&#27861;&#24178;&#25200;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks dramatically change the output of an otherwise accurate learning system using a seemingly inconsequential modification to a piece of input data. Paradoxically, empirical evidence indicates that even systems which are robust to large random perturbations of the input data remain susceptible to small, easily constructed, adversarial perturbations of their inputs. Here, we show that this may be seen as a fundamental feature of classifiers working with high dimensional input data. We introduce a simple generic and generalisable framework for which key behaviours observed in practical systems arise with high probability -- notably the simultaneous susceptibility of the (otherwise accurate) model to easily constructed adversarial attacks, and robustness to random perturbations of the input data. We confirm that the same phenomena are directly observed in practical neural networks trained on standard image classification problems, where even large additive random noise fai
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#20013;&#30693;&#35782;&#33976;&#39311;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;25&#31181;&#33976;&#39311;&#25439;&#22833;&#39033;&#65292;&#24182;&#25351;&#20986;&#30001;&#20110;&#35757;&#32451;&#37197;&#32622;&#30340;&#24046;&#24322;&#23548;&#33268;&#26415;&#35821;&#27604;&#36739;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36229;&#21442;&#25968;&#36873;&#25321;&#19981;&#24403;&#20250;&#23548;&#33268;&#26497;&#31471;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.03659</link><description>&lt;p&gt;
&#22312;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#20013;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Towards Comparable Knowledge Distillation in Semantic Image Segmentation. (arXiv:2309.03659v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03659
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#20013;&#30693;&#35782;&#33976;&#39311;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;25&#31181;&#33976;&#39311;&#25439;&#22833;&#39033;&#65292;&#24182;&#25351;&#20986;&#30001;&#20110;&#35757;&#32451;&#37197;&#32622;&#30340;&#24046;&#24322;&#23548;&#33268;&#26415;&#35821;&#27604;&#36739;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36229;&#21442;&#25968;&#36873;&#25321;&#19981;&#24403;&#20250;&#23548;&#33268;&#26497;&#31471;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#35299;&#20915;&#35821;&#20041;&#20998;&#21106;&#20013;&#22823;&#27169;&#22411;&#23610;&#23544;&#21644;&#24930;&#25512;&#29702;&#36895;&#24230;&#30340;&#19968;&#31181;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#36807;&#21435;4&#24180;&#30340;14&#20010;&#20986;&#29256;&#29289;&#20013;&#37492;&#23450;&#20986;&#20102;25&#20010;&#25552;&#20986;&#30340;&#33976;&#39311;&#25439;&#22833;&#39033;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22522;&#20110;&#24050;&#21457;&#24067;&#32467;&#26524;&#30340;&#26415;&#35821;&#27604;&#36739;&#36890;&#24120;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;&#35757;&#32451;&#37197;&#32622;&#30340;&#24046;&#24322;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#26159;&#23545;&#27604;2022&#24180;&#30340;&#20004;&#20010;&#20986;&#29256;&#29289;&#12290;&#20351;&#29992;&#30456;&#21516;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#32467;&#26500;&#21644;&#32479;&#35745;&#32441;&#29702;&#33976;&#39311;&#65288;SSTKD&#65289;&#25253;&#21578;&#20102;&#23398;&#29983;mIoU&#22686;&#21152;&#20102;4.54&#20010;&#30334;&#20998;&#28857;&#65292;&#26368;&#32456;&#24615;&#33021;&#36798;&#21040;&#20102;29.19&#65292;&#32780;&#33258;&#36866;&#24212;&#36879;&#35270;&#33976;&#39311;&#65288;APD&#65289;&#20165;&#20165;&#25552;&#39640;&#20102;&#23398;&#29983;&#24615;&#33021;2.06&#20010;&#30334;&#20998;&#28857;&#65292;&#20294;&#23454;&#29616;&#20102;39.25&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;&#36825;&#31181;&#26497;&#31471;&#24046;&#24322;&#30340;&#21407;&#22240;&#36890;&#24120;&#26159;&#36229;&#21442;&#25968;&#30340;&#27425;&#20248;&#36873;&#25321;&#20197;&#21450;&#20316;&#20026;&#21442;&#32771;&#28857;&#30340;&#23398;&#29983;&#27169;&#22411;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36229;&#21442;&#25968;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD) is one proposed solution to large model sizes and slow inference speed in semantic segmentation. In our research we identify 25 proposed distillation loss terms from 14 publications in the last 4 years. Unfortunately, a comparison of terms based on published results is often impossible, because of differences in training configurations. A good illustration of this problem is the comparison of two publications from 2022. Using the same models and dataset, Structural and Statistical Texture Distillation (SSTKD) reports an increase of student mIoU of 4.54 and a final performance of 29.19, while Adaptive Perspective Distillation (APD) only improves student performance by 2.06 percentage points, but achieves a final performance of 39.25. The reason for such extreme differences is often a suboptimal choice of hyperparameters and a resulting underperformance of the student model used as reference point. In our work, we reveal problems of insufficient hyperparameter
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#21078;&#23398;&#25351;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#27169;&#25311;&#21069;&#21015;&#33146;&#30340;&#29983;&#29702;&#21464;&#24418;&#24182;&#29983;&#25104;&#29420;&#29305;&#30340;&#30149;&#21464;&#24418;&#29366;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.03652</link><description>&lt;p&gt;
&#35299;&#21078;&#23398;&#25351;&#23548;&#19979;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#29992;&#20110;&#22686;&#24378;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anatomy-informed Data Augmentation for Enhanced Prostate Cancer Detection. (arXiv:2309.03652v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03652
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#21078;&#23398;&#25351;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#27169;&#25311;&#21069;&#21015;&#33146;&#30340;&#29983;&#29702;&#21464;&#24418;&#24182;&#29983;&#25104;&#29420;&#29305;&#30340;&#30149;&#21464;&#24418;&#29366;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#20363;&#22914;&#22312;&#30913;&#20849;&#25391;&#22270;&#20687;&#19978;&#30340;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#20013;&#12290;&#29616;&#26377;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#31995;&#32479;&#20173;&#28982;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#31354;&#38388;&#21464;&#25442;&#26469;&#20445;&#30041;&#30149;&#29702;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#22686;&#24378;&#24182;&#19981;&#33021;&#26174;&#33879;&#22686;&#21152;&#35757;&#32451;&#38598;&#20013;&#30340;&#22120;&#23448;&#21644;&#32959;&#30244;&#24418;&#29366;&#30340;&#21464;&#24322;&#24615;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#22312;&#20855;&#26377;&#26356;&#22810;&#19981;&#21516;&#23616;&#37096;&#36719;&#32452;&#32455;&#21464;&#24418;&#30340;&#26410;&#35265;&#26696;&#20363;&#20013;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#21078;&#23398;&#25351;&#23548;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#37051;&#22120;&#23448;&#30340;&#20449;&#24687;&#26469;&#27169;&#25311;&#21069;&#21015;&#33146;&#30340;&#20856;&#22411;&#29983;&#29702;&#21464;&#24418;&#65292;&#24182;&#20135;&#29983;&#29420;&#29305;&#30340;&#30149;&#21464;&#24418;&#29366;&#32780;&#19981;&#25913;&#21464;&#20854;&#26631;&#31614;&#12290;&#30001;&#20110;&#20854;&#36731;&#37327;&#32423;&#30340;&#35745;&#31639;&#35201;&#27714;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#24120;&#35265;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#20013;&#12290;&#36890;&#36807;&#35780;&#20272;&#19968;&#32452;774&#20010;&#27963;&#26816;&#30830;&#35748;&#30340;&#26816;&#26597;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22686;&#24378;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation (DA) is a key factor in medical image analysis, such as in prostate cancer (PCa) detection on magnetic resonance images. State-of-the-art computer-aided diagnosis systems still rely on simplistic spatial transformations to preserve the pathological label post transformation. However, such augmentations do not substantially increase the organ as well as tumor shape variability in the training set, limiting the model's ability to generalize to unseen cases with more diverse localized soft-tissue deformations. We propose a new anatomy-informed transformation that leverages information from adjacent organs to simulate typical physiological deformations of the prostate and generates unique lesion shapes without altering their label. Due to its lightweight computational requirements, it can be easily integrated into common DA frameworks. We demonstrate the effectiveness of our augmentation on a dataset of 774 biopsy-confirmed examinations, by evaluating a state-of-the-art m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22522;&#20110;&#32593;&#26684;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#31243;&#24207;&#21512;&#25104;&#26469;&#23398;&#20064;&#36890;&#29992;&#21644;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#26234;&#33021;&#20307;&#34892;&#20026;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03651</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#32593;&#26684;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#23398;&#20064;&#36890;&#29992;&#21644;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Learning of Generalizable and Interpretable Knowledge in Grid-Based Reinforcement Learning Environments. (arXiv:2309.03651v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22522;&#20110;&#32593;&#26684;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#31243;&#24207;&#21512;&#25104;&#26469;&#23398;&#20064;&#36890;&#29992;&#21644;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#26234;&#33021;&#20307;&#34892;&#20026;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22312;&#28216;&#25103;&#25110;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#32463;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#26469;&#35828;&#65292;&#29702;&#35299;&#20854;&#20132;&#20114;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#22312;&#28216;&#25103;&#20013;&#65292;&#19981;&#21512;&#29702;&#30340;&#21160;&#20316;&#20250;&#22256;&#24785;&#29609;&#23478;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#36825;&#31181;&#24433;&#21709;&#26356;&#21152;&#26174;&#33879;&#65292;&#22240;&#20026;&#24847;&#22806;&#34892;&#20026;&#21487;&#33021;&#23548;&#33268;&#20107;&#25925;&#65292;&#23545;&#30456;&#20851;&#20154;&#21592;&#21487;&#33021;&#20135;&#29983;&#20005;&#37325;&#32780;&#38271;&#36828;&#30340;&#21518;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#31243;&#24207;&#21512;&#25104;&#22312;&#35266;&#23519;&#21040;&#19968;&#31995;&#21015;&#21160;&#20316;&#36712;&#36857;&#21518;&#27169;&#20223;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#31243;&#24207;&#20855;&#26377;&#22266;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#27491;&#30830;&#24615;&#21487;&#39564;&#35777;&#24615;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#38024;&#23545;&#22522;&#20110;&#32593;&#26684;&#30340;&#29615;&#22659;&#65288;&#21253;&#25324;&#23548;&#33322;&#20219;&#21153;&#21644;&#20004;&#20010;&#36855;&#20320;&#29256;&#30340;Atari&#28216;&#25103;&#65306;Space Invaders&#21644;Asterix&#65289;&#25913;&#36896;&#20102;&#20808;&#36827;&#30340;&#31243;&#24207;&#21512;&#25104;&#31995;&#32479;DreamCoder&#65292;&#36890;&#36807;&#26816;&#26597;&#29983;&#25104;&#30340;&#24211;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#20986;&#40657;&#30418;&#26234;&#33021;&#20307;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#24182;&#26356;&#22909;&#22320;&#29702;&#35299;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the interactions of agents trained with deep reinforcement learning is crucial for deploying agents in games or the real world. In the former, unreasonable actions confuse players. In the latter, that effect is even more significant, as unexpected behavior cause accidents with potentially grave and long-lasting consequences for the involved individuals. In this work, we propose using program synthesis to imitate reinforcement learning policies after seeing a trajectory of the action sequence. Programs have the advantage that they are inherently interpretable and verifiable for correctness. We adapt the state-of-the-art program synthesis system DreamCoder for learning concepts in grid-based environments, specifically, a navigation task and two miniature versions of Atari games, Space Invaders and Asterix. By inspecting the generated libraries, we can make inferences about the concepts the black-box agent has learned and better understand the agent's behavior. We achieve th
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;GNN&#30340;Lipschitz&#30028;&#38480;&#34920;&#24449;&#20102;GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#21644;&#22266;&#26377;&#20559;&#20506;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#23545;&#20110;&#38480;&#21046;GNN&#36755;&#20986;&#30340;&#25200;&#21160;&#20197;&#20445;&#38556;&#20844;&#24179;&#24615;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03648</link><description>&lt;p&gt;
GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;Lipschitz&#29305;&#24615;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Characterizing Lipschitz Stability of GNN for Fairness. (arXiv:2309.03648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03648
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;GNN&#30340;Lipschitz&#30028;&#38480;&#34920;&#24449;&#20102;GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#21644;&#22266;&#26377;&#20559;&#20506;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#23545;&#20110;&#38480;&#21046;GNN&#36755;&#20986;&#30340;&#25200;&#21160;&#20197;&#20445;&#38556;&#20844;&#24179;&#24615;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lipschitz&#30028;&#38480;&#26159;&#20174;&#40065;&#26834;&#32479;&#35745;&#23398;&#20013;&#20511;&#37492;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#38480;&#21046;&#36755;&#20986;&#30456;&#23545;&#20110;&#36755;&#20837;&#30340;&#26368;&#22823;&#21464;&#21270;&#65292;&#32771;&#34385;&#21040;&#30456;&#20851;&#30340;&#38750;&#20851;&#38190;&#20559;&#20506;&#22240;&#32032;&#12290;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#26597;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#31283;&#23450;&#24615;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26368;&#36817;&#65292;&#23545;&#20110;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#19978;&#25805;&#20316;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#27809;&#26377;&#30740;&#31350;&#35843;&#26597;GNN&#30340;Lipschitz&#30028;&#38480;&#20197;&#25581;&#31034;&#27169;&#22411;&#36755;&#20986;&#30340;&#31283;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#20855;&#26377;&#22266;&#26377;&#20559;&#20506;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26102;&#12290;&#30001;&#20110;&#24120;&#35265;&#22270;&#24418;&#25968;&#25454;&#22312;GNN&#35757;&#32451;&#20013;&#23384;&#22312;&#22266;&#26377;&#20559;&#24046;&#65292;&#36825;&#32473;&#38480;&#21046;&#30001;&#36755;&#20837;&#20559;&#24046;&#24341;&#36215;&#30340;GNN&#36755;&#20986;&#25200;&#21160;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#38556;&#20844;&#24179;&#24615;&#65292;&#24102;&#26469;&#20102;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#23613;&#31649;Lipschitz&#24120;&#25968;&#22312;&#25511;&#21046;&#27431;&#20960;&#37324;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#26041;&#38754;&#26377;&#25152;&#24212;&#29992;&#65292;&#20294;&#31934;&#30830;Lipschitz&#24120;&#25968;&#30340;&#35745;&#31639;&#21313;&#20998;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Lipschitz bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. It is an efficient and provable method for examining the output stability of machine learning models without incurring additional computation costs. Recently, Graph Neural Networks (GNNs), which operate on non-Euclidean data, have gained significant attention. However, no previous research has investigated the GNN Lipschitz bounds to shed light on stabilizing model outputs, especially when working on non-Euclidean data with inherent biases. Given the inherent biases in common graph data used for GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input biases, thereby safeguarding fairness during training. Recently, despite the Lipschitz constant's use in controlling the stability of Euclideanneural networks, the calculation of the precise Lipschitz constant rem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;VideolandGPT&#25913;&#36827;&#20102;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#23454;&#39564;&#34920;&#26126;&#20010;&#24615;&#21270;&#29256;&#26412;&#22312;&#20934;&#30830;&#24615;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#20248;&#20110;&#38750;&#20010;&#24615;&#21270;&#29256;&#26412;&#65292;&#20294;&#20004;&#20010;&#29256;&#26412;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#19968;&#33268;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.03645</link><description>&lt;p&gt;
VideolandGPT&#65306;&#20851;&#20110;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#29992;&#25143;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
VideolandGPT: A User Study on a Conversational Recommender System. (arXiv:2309.03645v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;VideolandGPT&#25913;&#36827;&#20102;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#23454;&#39564;&#34920;&#26126;&#20010;&#24615;&#21270;&#29256;&#26412;&#22312;&#20934;&#30830;&#24615;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#20248;&#20110;&#38750;&#20010;&#24615;&#21270;&#29256;&#26412;&#65292;&#20294;&#20004;&#20010;&#29256;&#26412;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#19968;&#33268;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#21644;&#29616;&#26377;&#25490;&#21517;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#20505;&#36873;&#36873;&#25321;&#30340;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;VideolandGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35270;&#39057;&#28857;&#25773;&#24179;&#21488;Videoland&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;ChatGPT&#20174;&#39044;&#23450;&#20869;&#23481;&#38598;&#21512;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#19982;&#32842;&#22825;&#30028;&#38754;&#30340;&#20132;&#20114;&#25152;&#31034;&#30340;&#39069;&#22806;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#20010;&#24615;&#21270;&#21644;&#38750;&#20010;&#24615;&#21270;&#29256;&#26412;&#30340;&#31995;&#32479;&#22312;&#25490;&#21517;&#25351;&#26631;&#12289;&#29992;&#25143;&#20307;&#39564;&#21644;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20010;&#24615;&#21270;&#29256;&#26412;&#22312;&#20934;&#30830;&#24615;&#21644;&#19968;&#33324;&#29992;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#20248;&#20110;&#38750;&#20010;&#24615;&#21270;&#29256;&#26412;&#65292;&#32780;&#20004;&#20010;&#29256;&#26412;&#37117;&#22686;&#21152;&#20102;&#25490;&#21517;&#25512;&#33616;&#21015;&#34920;&#20013;&#38750;&#21069;&#21015;&#30340;&#39033;&#30446;&#30340;&#21487;&#35265;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#65292;&#20004;&#20010;&#29256;&#26412;&#30340;&#34892;&#20026;&#37117;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates how large language models (LLMs) can enhance recommender systems, with a specific focus on Conversational Recommender Systems that leverage user preferences and personalised candidate selections from existing ranking models. We introduce VideolandGPT, a recommender system for a Video-on-Demand (VOD) platform, Videoland, which uses ChatGPT to select from a predetermined set of contents, considering the additional context indicated by users' interactions with a chat interface. We evaluate ranking metrics, user experience, and fairness of recommendations, comparing a personalised and a non-personalised version of the system, in a between-subject user study. Our results indicate that the personalised version outperforms the non-personalised in terms of accuracy and general user satisfaction, while both versions increase the visibility of items which are not in the top of the recommendation lists. However, both versions present inconsistent behavior in terms of fairn
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;AI&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#32771;&#34385;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#20105;&#35758;&#24615;&#31561;&#20854;&#20182;&#37325;&#35201;&#26041;&#38754;&#26102;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;AI&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.03638</link><description>&lt;p&gt;
&#36229;&#36234;&#21487;&#35299;&#37322;AI&#65306;&#36127;&#36131;&#20154;&#24037;&#26234;&#33021;&#30340;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Beyond XAI:Obstacles Towards Responsible AI. (arXiv:2309.03638v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03638
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;AI&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#32771;&#34385;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#20105;&#35758;&#24615;&#31561;&#20854;&#20182;&#37325;&#35201;&#26041;&#38754;&#26102;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;AI&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#21457;&#23637;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#24341;&#21457;&#20102;&#23545;&#24320;&#21457;&#25216;&#26415;&#20197;&#20351;AI&#31995;&#32479;&#26356;&#36879;&#26126;&#21644;&#21487;&#29702;&#35299;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#32972;&#26223;&#19979;&#65292;&#35299;&#37322;&#24615;&#26041;&#27861;&#21450;&#20854;&#35780;&#20272;&#31574;&#30053;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#36127;&#36131;&#20219;&#30340;AI&#30340;&#33539;&#22260;&#19981;&#20165;&#38480;&#20110;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#32771;&#34385;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#20105;&#35758;&#24615;&#31561;&#20854;&#20182;&#37325;&#35201;&#26041;&#38754;&#26102;&#30340;&#36131;&#20219;AI&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapidly advancing domain of Explainable Artificial Intelligence (XAI) has sparked significant interests in developing techniques to make AI systems more transparent and understandable. Nevertheless, in real-world contexts, the methods of explainability and their evaluation strategies present numerous limitations.Moreover, the scope of responsible AI extends beyond just explainability. In this paper, we explore these limitations and discuss their implications in a boarder context of responsible AI when considering other important aspects, including privacy, fairness and contestability.
&lt;/p&gt;</description></item><item><title>NeuroCodeBench&#26159;&#19968;&#20010;&#29992;&#32431;C&#32534;&#20889;&#30340;&#31070;&#32463;&#32593;&#32476;&#20195;&#30721;&#39564;&#35777;&#22522;&#20934;&#27979;&#35797;&#65292;&#20849;&#21253;&#21547;32&#20010;&#31070;&#32463;&#32593;&#32476;&#21644;607&#20010;&#23433;&#20840;&#23646;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30001;&#20110;&#19981;&#23436;&#20840;&#25903;&#25345;&#26631;&#20934;C&#25968;&#23398;&#24211;&#21644;&#22797;&#26434;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#36719;&#20214;&#39564;&#35777;&#22120;&#38590;&#20197;&#25552;&#20379;&#27491;&#30830;&#30340;&#21028;&#26029;&#12290;</title><link>http://arxiv.org/abs/2309.03617</link><description>&lt;p&gt;
NeuroCodeBench&#65306;&#29992;&#20110;&#36719;&#20214;&#39564;&#35777;&#30340;&#32431; C &#31070;&#32463;&#32593;&#32476;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NeuroCodeBench: a plain C neural network benchmark for software verification. (arXiv:2309.03617v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03617
&lt;/p&gt;
&lt;p&gt;
NeuroCodeBench&#26159;&#19968;&#20010;&#29992;&#32431;C&#32534;&#20889;&#30340;&#31070;&#32463;&#32593;&#32476;&#20195;&#30721;&#39564;&#35777;&#22522;&#20934;&#27979;&#35797;&#65292;&#20849;&#21253;&#21547;32&#20010;&#31070;&#32463;&#32593;&#32476;&#21644;607&#20010;&#23433;&#20840;&#23646;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30001;&#20110;&#19981;&#23436;&#20840;&#25903;&#25345;&#26631;&#20934;C&#25968;&#23398;&#24211;&#21644;&#22797;&#26434;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#36719;&#20214;&#39564;&#35777;&#22120;&#38590;&#20197;&#25552;&#20379;&#27491;&#30830;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#38656;&#35201;&#24378;&#26377;&#21147;&#30340;&#20445;&#35777;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#25216;&#26415;&#22312;&#36825;&#20010;&#30446;&#26631;&#19978;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#35777;&#26126;&#32593;&#32476;&#23454;&#29616;&#20013;&#19981;&#23384;&#22312;&#36719;&#20214;&#38169;&#35823;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; NeuroCodeBench - &#19968;&#20010;&#29992;&#32431; C &#32534;&#20889;&#30340;&#31070;&#32463;&#32593;&#32476;&#20195;&#30721;&#39564;&#35777;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21253;&#21547;&#20102;32&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20849;&#26377;607&#20010;&#23433;&#20840;&#23646;&#24615;&#65292;&#20998;&#20026;6&#20010;&#31867;&#21035;&#65306;&#25968;&#23398;&#24211;&#12289;&#28608;&#27963;&#20989;&#25968;&#12289;&#32416;&#38169;&#32593;&#32476;&#12289;&#20256;&#36755;&#20989;&#25968;&#36924;&#36817;&#12289;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#35780;&#20272;&#26174;&#31034;&#65292;&#26368;&#20808;&#36827;&#30340;&#36719;&#20214;&#39564;&#35777;&#22120;&#22312;&#25552;&#20379;&#27491;&#30830;&#30340;&#21028;&#26029;&#26102;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#23545;&#26631;&#20934; C &#25968;&#23398;&#24211;&#30340;&#19981;&#23436;&#20840;&#25903;&#25345;&#21644;&#26356;&#22797;&#26434;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#25152;&#23548;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety-critical systems with neural network components require strong guarantees. While existing neural network verification techniques have shown great progress towards this goal, they cannot prove the absence of software faults in the network implementation. This paper presents NeuroCodeBench - a verification benchmark for neural network code written in plain C. It contains 32 neural networks with 607 safety properties divided into 6 categories: maths library, activation functions, error-correcting networks, transfer function approximation, probability density estimation and reinforcement learning. Our preliminary evaluation shows that state-of-the-art software verifiers struggle to provide correct verdicts, due to their incomplete support of the standard C mathematical library and the complexity of larger neural networks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25506;&#32034;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.03613</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#20005;&#35880;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating ChatGPT as a Recommender System: A Rigorous Approach. (arXiv:2309.03613v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25506;&#32034;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#22823;&#22411;AI&#35821;&#35328;&#27169;&#22411;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#23427;&#20204;&#22312;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#65292;&#21253;&#25324;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#65292;&#22240;&#27492;&#23545;&#20110;&#21508;&#31181;&#29305;&#23450;&#20219;&#21153;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#37322;&#25918;&#20102;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#30740;&#31350;&#30028;&#27491;&#22312;&#31215;&#26497;&#25506;&#32034;&#23427;&#20204;&#30340;&#24212;&#29992;&#65292;ChatGPT&#20063;&#22240;&#27492;&#33719;&#24471;&#20102;&#35748;&#21487;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#26377;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#20854;&#22312;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#20173;&#24453;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#25506;&#31350;ChatGPT&#20316;&#20026;&#38646;-shot&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#21253;&#25324;&#35780;&#20272;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#29616;&#26377;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#30340;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;MovieLens Small&#12289;Last.FM&#21644;Facebook Bo&#65289;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#26469;&#35780;&#20272;ChatGPT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent popularity surrounds large AI language models due to their impressive natural language capabilities. They contribute significantly to language-related tasks, including prompt-based learning, making them valuable for various specific tasks. This approach unlocks their full potential, enhancing precision and generalization. Research communities are actively exploring their applications, with ChatGPT receiving recognition. Despite extensive research on large language models, their potential in recommendation scenarios still needs to be explored. This study aims to fill this gap by investigating ChatGPT's capabilities as a zero-shot recommender system. Our goals include evaluating its ability to use user preferences for recommendations, reordering existing recommendation lists, leveraging information from similar users, and handling cold-start situations. We assess ChatGPT's performance through comprehensive experiments using three datasets (MovieLens Small, Last.FM, and Facebook Bo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#32534;&#30721;&#30340;BOLD fMRI&#26102;&#38388;&#24207;&#21015;&#65292;&#26412;&#30740;&#31350;&#23545;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#30340;&#38745;&#24577;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#31070;&#32463;&#27963;&#21160;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.03590</link><description>&lt;p&gt;
BOLD fMRI&#26102;&#38388;&#24207;&#21015;&#30340;&#31354;&#38388;&#32534;&#30721;&#29992;&#20110;&#23545;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#30340;&#38745;&#24577;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65306;&#20154;&#31867;&#35270;&#35273;&#30340;&#19968;&#39033;&#35797;&#39564;&#30740;&#31350;&#12290;(arXiv:2309.03590v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
Spatial encoding of BOLD fMRI time series for categorizing static images across visual datasets: A pilot study on human vision. (arXiv:2309.03590v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03590
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#32534;&#30721;&#30340;BOLD fMRI&#26102;&#38388;&#24207;&#21015;&#65292;&#26412;&#30740;&#31350;&#23545;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#30340;&#38745;&#24577;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#31070;&#32463;&#27963;&#21160;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#36890;&#36807;&#26816;&#27979;&#19982;&#33041;&#27963;&#21160;&#30456;&#20851;&#30340;&#27687;&#21512;&#34880;&#27969;&#21464;&#21270;&#26469;&#30740;&#31350;&#33041;&#21151;&#33021;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;fMRI&#26102;&#38388;&#24207;&#21015;&#65288;TS&#65289;&#36827;&#34892;&#29305;&#24322;&#24615;&#22270;&#20687;&#20998;&#31867;&#65292;&#20197;&#20102;&#35299;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#31070;&#32463;&#27963;&#21160;&#30340;&#24046;&#24322;&#12290;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;BOLD5000&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26597;&#30475;&#26469;&#33258;COCO&#12289;ImageNet&#21644;SUN&#19977;&#20010;&#26631;&#20934;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;5254&#24352;&#19981;&#21516;&#31867;&#21035;&#30340;&#22270;&#20687;&#30340;fMRI&#25195;&#25551;&#12290;&#20026;&#20102;&#29702;&#35299;&#35270;&#35273;&#65292;&#30740;&#31350;&#22823;&#33041;&#22312;&#35266;&#30475;&#19981;&#21516;&#22270;&#20687;&#26102;&#30340;&#21151;&#33021;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#36827;&#34892;&#20102;fMRI BOLD TS&#30340;&#31354;&#38388;&#32534;&#30721;&#65292;&#20351;&#29992;&#32463;&#20856;&#30340;&#26684;&#26519;&#35282;&#22330;&#65288;GAF&#65289;&#21644;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#22330;&#65288;MTF&#65289;&#33719;&#21462;&#20108;&#32500;BOLD TS&#65292;&#20195;&#34920;&#20102;COCO&#12289;Imagenet&#21644;SUN&#30340;&#22270;&#20687;&#12290;&#23545;&#20110;&#20998;&#31867;&#65292;&#23558;&#21333;&#20010;GAF&#21644;MTF&#29305;&#24449;&#36755;&#20837;&#24120;&#35268;CNN&#12290;&#38543;&#21518;&#65292;&#37319;&#29992;&#24182;&#34892;CNN&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional MRI (fMRI) is widely used to examine brain functionality by detecting alteration in oxygenated blood flow that arises with brain activity. In this study, complexity specific image categorization across different visual datasets is performed using fMRI time series (TS) to understand differences in neuronal activities related to vision. Publicly available BOLD5000 dataset is used for this purpose, containing fMRI scans while viewing 5254 images of diverse categories, drawn from three standard computer vision datasets: COCO, ImageNet and SUN. To understand vision, it is important to study how brain functions while looking at different images. To achieve this, spatial encoding of fMRI BOLD TS has been performed that uses classical Gramian Angular Field (GAF) and Markov Transition Field (MTF) to obtain 2D BOLD TS, representing images of COCO, Imagenet and SUN. For classification, individual GAF and MTF features are fed into regular CNN. Subsequently, parallel CNN model is employe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03581</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#22312;&#22810;&#30446;&#26631;&#38382;&#39064;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#23545;&#20110;&#21457;&#25381;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#29992;&#25143;&#36890;&#24120;&#23545;&#22810;&#30446;&#26631;&#38382;&#39064;&#24863;&#20852;&#36259;&#65292;&#21363;&#20248;&#21270;&#21487;&#33021;&#23384;&#22312;&#20914;&#31361;&#30340;&#30446;&#26631;&#65292;&#27604;&#22914;&#20934;&#30830;&#24615;&#21644;&#33021;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32477;&#22823;&#22810;&#25968;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23558;&#19968;&#32452;&#38750;&#25903;&#37197;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#36820;&#22238;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#36825;&#31181;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35780;&#20272;&#19968;&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#28041;&#21450;&#35780;&#20272;&#24471;&#21040;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#26377;&#19968;&#20123;&#25351;&#26631;&#21487;&#20197;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#23646;&#24615;&#65288;&#22914;&#20307;&#31215;&#12289;&#19982;&#21442;&#32771;&#28857;&#30340;&#25509;&#36817;&#31243;&#24230;&#65289;&#26469;&#35780;&#20272;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#65288;&#20363;&#22914;&#36229;&#20307;&#31215;&#12289;R2&#65289;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#65292;&#36873;&#25321;&#23548;&#33268;&#26399;&#26395;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#25351;&#26631;&#21487;&#33021;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#38024;&#23545;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is important to leverage the full potential of machine learning (ML). In practice, users are often interested in multi-objective (MO) problems, i.e., optimizing potentially conflicting objectives, like accuracy and energy consumption. To tackle this, the vast majority of MO-ML algorithms return a Pareto front of non-dominated machine learning models to the user. Optimizing the hyperparameters of such algorithms is non-trivial as evaluating a hyperparameter configuration entails evaluating the quality of the resulting Pareto front. In literature, there are known indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by quantifying different properties (e.g., volume, proximity to a reference point). However, choosing the indicator that leads to the desired Pareto front might be a hard task for a user. In this paper, we propose a human-centered interactive HPO approach tailored towards multi-objective ML leveraging preference learnin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DTW+S&#30340;&#26032;&#22411;&#27979;&#37327;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21019;&#24314;&#23616;&#37096;&#36235;&#21183;&#30340;&#30697;&#38453;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#23616;&#37096;&#36235;&#21183;&#30456;&#20284;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03579</link><description>&lt;p&gt;
DTW+S: &#20351;&#29992;&#26377;&#24207;&#23616;&#37096;&#36235;&#21183;&#36827;&#34892;&#22522;&#20110;&#24418;&#29366;&#30340;&#26102;&#38388;&#24207;&#21015;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend. (arXiv:2309.03579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03579
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DTW+S&#30340;&#26032;&#22411;&#27979;&#37327;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21019;&#24314;&#23616;&#37096;&#36235;&#21183;&#30340;&#30697;&#38453;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#23616;&#37096;&#36235;&#21183;&#30456;&#20284;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36317;&#31163;&#25110;&#30456;&#20284;&#24230;&#30340;&#27979;&#37327;&#26159;&#35768;&#22810;&#24212;&#29992;&#21253;&#25324;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#22522;&#26412;&#26041;&#38754;&#12290;&#29616;&#26377;&#30340;&#27979;&#37327;&#26041;&#27861;&#21487;&#33021;&#30001;&#20110;&#23616;&#37096;&#36235;&#21183;&#65288;&#24418;&#29366;&#65289;&#32780;&#26080;&#27861;&#25429;&#25417;&#21040;&#30456;&#20284;&#20043;&#22788;&#65292;&#29978;&#33267;&#21487;&#33021;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#23547;&#25214;&#22312;&#30456;&#20284;&#26102;&#38388;&#21608;&#22260;&#21457;&#29983;&#30340;&#30456;&#20284;&#36235;&#21183;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#24212;&#29992;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26131;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#26377;&#24207;&#30340;&#26377;&#24847;&#20041;&#30340;&#23616;&#37096;&#36235;&#21183;&#24207;&#21015;&#30340;&#24212;&#29992;&#29305;&#21035;&#26377;&#29992;&#65292;&#20363;&#22914;&#22312;&#27969;&#34892;&#30149;&#20013;&#65288;&#20174;&#22686;&#38271;&#21040;&#23792;&#20540;&#20877;&#21040;&#20943;&#23569;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;DTW+S&#65292;&#23427;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#8220;&#20445;&#25345;&#25509;&#36817;&#24615;&#8221;&#30340;&#30697;&#38453;&#34920;&#31034;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#20013;&#27599;&#19968;&#21015;&#20195;&#34920;&#23616;&#37096;&#36235;&#21183;&#65292;&#28982;&#21518;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36825;&#20123;&#30697;&#38453;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25903;&#25345;&#36825;&#31181;&#34920;&#31034;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DTW+S&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring distance or similarity between time-series data is a fundamental aspect of many applications including classification and clustering. Existing measures may fail to capture similarities due to local trends (shapes) and may even produce misleading results. Our goal is to develop a measure that looks for similar trends occurring around similar times and is easily interpretable for researchers in applied domains. This is particularly useful for applications where time-series have a sequence of meaningful local trends that are ordered, such as in epidemics (a surge to an increase to a peak to a decrease). We propose a novel measure, DTW+S, which creates an interpretable "closeness-preserving" matrix representation of the time-series, where each column represents local trends, and then it applies Dynamic Time Warping to compute distances between these matrices. We present a theoretical analysis that supports the choice of this representation. We demonstrate the utility of DTW+S in 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#37325;&#29992;&#21644;&#36845;&#20195;&#25193;&#25955;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#22810;&#35270;&#39057;&#24103;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#24103;&#32423;&#25238;&#21160;&#12290;</title><link>http://arxiv.org/abs/2309.03549</link><description>&lt;p&gt;
&#37325;&#29992;&#19982;&#25193;&#25955;&#65306;&#29992;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#36845;&#20195;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation. (arXiv:2309.03549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03549
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#37325;&#29992;&#21644;&#36845;&#20195;&#25193;&#25955;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#22810;&#35270;&#39057;&#24103;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#24103;&#32423;&#25238;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#26174;&#33879;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;LDM&#65292;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38480;&#21046;&#32780;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#21333;&#20010;LDM&#36890;&#24120;&#21482;&#33021;&#29983;&#25104;&#26377;&#38480;&#25968;&#37327;&#30340;&#35270;&#39057;&#24103;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#24037;&#20316;&#19987;&#27880;&#20110;&#20026;&#29983;&#25104;&#26356;&#22810;&#30340;&#35270;&#39057;&#24103;&#32780;&#20351;&#29992;&#29420;&#31435;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#20294;&#36825;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#24103;&#32423;&#25238;&#21160;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#37325;&#29992;&#19982;&#25193;&#25955;&#8221;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;$\textit{VidRD}$&#65292;&#20197;&#29983;&#25104;&#26356;&#22810;&#30340;&#24103;&#65292;&#24182;&#19988;&#36825;&#20123;&#24103;&#26159;&#22312;&#30001;LDM&#29983;&#25104;&#30340;&#20808;&#21069;&#24103;&#20043;&#21518;&#20135;&#29983;&#30340;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#29992;&#20110;&#20687;&#32032;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#36716;&#25442;&#30340;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#23618;&#27880;&#20837;&#20854;&#35299;&#30721;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the remarkable success of Latent Diffusion Models (LDMs) for image synthesis, we study LDM for text-to-video generation, which is a formidable challenge due to the computational and memory constraints during both model training and inference. A single LDM is usually only capable of generating a very limited number of video frames. Some existing works focus on separate prediction models for generating more video frames, which suffer from additional training cost and frame-level jittering, however. In this paper, we propose a framework called "Reuse and Diffuse" dubbed $\textit{VidRD}$ to produce more frames following the frames already generated by an LDM. Conditioned on an initial video clip with a small number of frames, additional frames are iteratively generated by reusing the original latent features and following the previous diffusion process. Besides, for the autoencoder used for translation between pixel space and latent space, we inject temporal layers into its dec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DGC&#31995;&#32479;&#65292;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#22270;&#20998;&#21306;&#26041;&#27861;&#23454;&#29616;&#20102;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;(DGNN)&#30340;&#39640;&#25928;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#23454;&#38469;&#21160;&#24577;&#22270;&#38750;&#22343;&#21248;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;1.25&#20493;-7.52&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2309.03523</link><description>&lt;p&gt;
DGC: &#20351;&#29992;&#20998;&#22359;&#22270;&#20998;&#21306;&#35757;&#32451;&#20855;&#26377;&#26102;&#31354;&#38750;&#22343;&#21248;&#24615;&#30340;&#21160;&#24577;&#22270;
&lt;/p&gt;
&lt;p&gt;
DGC: Training Dynamic Graphs with Spatio-Temporal Non-Uniformity using Graph Partitioning by Chunks. (arXiv:2309.03523v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DGC&#31995;&#32479;&#65292;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#22270;&#20998;&#21306;&#26041;&#27861;&#23454;&#29616;&#20102;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;(DGNN)&#30340;&#39640;&#25928;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#23454;&#38469;&#21160;&#24577;&#22270;&#38750;&#22343;&#21248;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;1.25&#20493;-7.52&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;(DGNN)&#36890;&#36807;&#21033;&#29992;&#26102;&#31354;&#29305;&#24449;&#23637;&#31034;&#20102;&#23398;&#20064;&#21160;&#24577;&#22270;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#23613;&#31649;DGNN&#26368;&#36817;&#21463;&#21040;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21508;&#31181;DGNN&#27169;&#22411;&#65292;&#20294;&#26500;&#24314;&#39640;&#25928;&#30340;DGNN&#35757;&#32451;&#20998;&#24067;&#24335;&#31995;&#32479;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24050;&#32463;&#20844;&#35748;&#30340;&#26159;&#65292;&#22914;&#20309;&#23558;&#21160;&#24577;&#22270;&#21010;&#20998;&#24182;&#23558;&#24037;&#20316;&#37327;&#20998;&#37197;&#32473;&#22810;&#20010;GPU&#22312;&#35757;&#32451;&#21152;&#36895;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#24037;&#20316;&#23558;&#21160;&#24577;&#22270;&#21010;&#20998;&#20026;&#24555;&#29031;&#25110;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#36825;&#20165;&#22312;&#22270;&#20855;&#26377;&#22343;&#21248;&#26102;&#31354;&#32467;&#26500;&#26102;&#36215;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20013;&#30340;&#21160;&#24577;&#22270;&#19981;&#20855;&#26377;&#22343;&#21248;&#32467;&#26500;&#65292;&#20854;&#20013;&#19968;&#20123;&#24555;&#29031;&#38750;&#24120;&#31264;&#23494;&#65292;&#32780;&#20854;&#20182;&#24555;&#29031;&#38750;&#24120;&#31232;&#30095;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DGC&#65292;&#19968;&#31181;&#20998;&#24067;&#24335;DGNN&#35757;&#32451;&#31995;&#32479;&#65292;&#22312;&#25105;&#20204;&#30340;&#27979;&#35797;&#24179;&#21488;&#19978;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#24555;1.25&#20493;-7.52&#20493;&#30340;&#21152;&#36895;&#12290;DGC&#30340;&#25104;&#21151;&#28304;&#20110;&#19968;&#31181;&#26032;&#30340;&#22270;&#20998;&#21306;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#22270;&#30340;&#26102;&#31354;&#38750;&#22343;&#21248;&#24615;&#36827;&#34892;&#20998;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Neural Network (DGNN) has shown a strong capability of learning dynamic graphs by exploiting both spatial and temporal features. Although DGNN has recently received considerable attention by AI community and various DGNN models have been proposed, building a distributed system for efficient DGNN training is still challenging. It has been well recognized that how to partition the dynamic graph and assign workloads to multiple GPUs plays a critical role in training acceleration. Existing works partition a dynamic graph into snapshots or temporal sequences, which only work well when the graph has uniform spatio-temporal structures. However, dynamic graphs in practice are not uniformly structured, with some snapshots being very dense while others are sparse. To address this issue, we propose DGC, a distributed DGNN training system that achieves a 1.25x - 7.52x speedup over the state-of-the-art in our testbed. DGC's success stems from a new graph partitioning method that parti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20174;&#21442;&#25968;&#21270;&#22797;&#26434;&#24615;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;Distinct Kemeny&#25490;&#24207;&#32858;&#21512;&#30340;&#22810;&#20010;&#21442;&#25968;&#65292;&#21253;&#25324;&#30446;&#26631;Kemeny&#20998;&#25968;&#12289;&#20505;&#36873;&#20154;&#25968;&#37327;&#12289;&#36755;&#20837;&#25490;&#24207;&#30340;&#24179;&#22343;&#36317;&#31163;&#12289;&#20219;&#20309;&#20505;&#36873;&#20154;&#30340;&#26368;&#22823;&#33539;&#22260;&#21644;&#19968;&#33268;&#24615;&#23485;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;FPT&#31639;&#27861;&#21644;FPT&#36817;&#20284;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03517</link><description>&lt;p&gt;
&#21442;&#25968;&#21270;&#35270;&#35282;&#19979;&#30340;Distinct Kemeny&#25490;&#24207;&#32858;&#21512;&#30340;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
Parameterized Aspects of Distinct Kemeny Rank Aggregation. (arXiv:2309.03517v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03517
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20174;&#21442;&#25968;&#21270;&#22797;&#26434;&#24615;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;Distinct Kemeny&#25490;&#24207;&#32858;&#21512;&#30340;&#22810;&#20010;&#21442;&#25968;&#65292;&#21253;&#25324;&#30446;&#26631;Kemeny&#20998;&#25968;&#12289;&#20505;&#36873;&#20154;&#25968;&#37327;&#12289;&#36755;&#20837;&#25490;&#24207;&#30340;&#24179;&#22343;&#36317;&#31163;&#12289;&#20219;&#20309;&#20505;&#36873;&#20154;&#30340;&#26368;&#22823;&#33539;&#22260;&#21644;&#19968;&#33268;&#24615;&#23485;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;FPT&#31639;&#27861;&#21644;FPT&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kemeny&#26041;&#27861;&#26159;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#25490;&#24207;&#32858;&#21512;&#24037;&#20855;&#65292;&#20294;&#35745;&#31639;&#26368;&#20248;&#30340;Kemeny&#25490;&#24207;&#26159;NP-hard&#30340;&#12290;&#22240;&#27492;&#65292;&#23558;&#25214;&#21040;Kemeny&#25490;&#24207;&#30340;&#35745;&#31639;&#20219;&#21153;&#20174;&#22810;&#20010;&#21442;&#25968;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20123;&#21442;&#25968;&#20043;&#38388;&#30340;&#20840;&#38754;&#20851;&#31995;&#65292;&#21253;&#25324;&#29702;&#35770;&#21644;&#23454;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#21442;&#25968;&#21270;&#22797;&#26434;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#35745;&#31639;&#25152;&#26377;&#19981;&#21516;&#30340;Kemeny&#25490;&#24207;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#30446;&#26631;Kemeny&#20998;&#25968;&#12289;&#20505;&#36873;&#20154;&#25968;&#37327;&#12289;&#36755;&#20837;&#25490;&#24207;&#30340;&#24179;&#22343;&#36317;&#31163;&#12289;&#20219;&#20309;&#20505;&#36873;&#20154;&#30340;&#26368;&#22823;&#33539;&#22260;&#21644;&#19968;&#33268;&#24615;&#23485;&#24230;&#20316;&#20026;&#25105;&#20204;&#30340;&#21442;&#25968;&#12290;&#23545;&#20110;&#25152;&#26377;&#36825;&#20123;&#21442;&#25968;&#65292;&#25105;&#20204;&#24050;&#32463;&#26377;&#20102;FPT&#31639;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#26174;&#33879;&#22686;&#21152;&#36816;&#34892;&#26102;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#25214;&#21040;&#20219;&#24847;&#25968;&#37327;&#30340;Kemeny&#25490;&#24207;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#30456;&#23545;&#20110;&#36825;&#20123;&#21442;&#25968;&#30340;Kemeny&#25490;&#24207;&#32858;&#21512;&#30340;FPT&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Kemeny method is one of the popular tools for rank aggregation. However, computing an optimal Kemeny ranking is NP-hard. Consequently, the computational task of finding a Kemeny ranking has been studied under the lens of parameterized complexity with respect to many parameters. We first present a comprehensive relationship, both theoretical and empirical, among these parameters. Further, we study the problem of computing all distinct Kemeny rankings under the lens of parameterized complexity. We consider the target Kemeny score, number of candidates, average distance of input rankings, maximum range of any candidate, and unanimity width as our parameters. For all these parameters, we already have FPT algorithms. We find that any desirable number of Kemeny rankings can also be found without substantial increase in running time. We also present FPT approximation algorithms for Kemeny rank aggregation with respect to these parameters.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#35270;&#35282;&#30340;&#20302;&#23618;&#29305;&#24449;&#20449;&#24687;&#26469;&#22686;&#24378;&#20083;&#33146;X&#32447;&#36896;&#24433;&#22270;&#20687;&#30340;&#20027;&#35201;&#35270;&#35282;&#65292;&#28982;&#21518;&#23398;&#20064;&#21253;&#21547;&#30284;&#21464;&#29305;&#24449;&#30340;&#39640;&#23618;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26032;&#39062;&#30340;&#24694;&#24615;&#20083;&#33146;X&#32447;&#36896;&#24433;&#21512;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#19978;&#37319;&#26679;&#23569;&#25968;&#31867;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.03506</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#21516;&#20391;&#21452;&#35270;&#35282;&#20083;&#33146;&#30284;&#20998;&#26512;&#20013;&#20581;&#22766;&#30340;&#33258;&#28982;&#22806;&#35266;&#20083;&#33146;X&#32447;&#36896;&#24433;&#30149;&#28790;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Natural-Looking Mammography Lesion Synthesis on Ipsilateral Dual-Views Breast Cancer Analysis. (arXiv:2309.03506v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03506
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#35270;&#35282;&#30340;&#20302;&#23618;&#29305;&#24449;&#20449;&#24687;&#26469;&#22686;&#24378;&#20083;&#33146;X&#32447;&#36896;&#24433;&#22270;&#20687;&#30340;&#20027;&#35201;&#35270;&#35282;&#65292;&#28982;&#21518;&#23398;&#20064;&#21253;&#21547;&#30284;&#21464;&#29305;&#24449;&#30340;&#39640;&#23618;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26032;&#39062;&#30340;&#24694;&#24615;&#20083;&#33146;X&#32447;&#36896;&#24433;&#21512;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#19978;&#37319;&#26679;&#23569;&#25968;&#31867;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20026;&#20102;&#25913;&#21892;&#20083;&#33146;&#30284;&#20998;&#31867;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#35768;&#22810;&#20083;&#33146;X&#32447;&#36896;&#24433;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#12290;&#20083;&#33146;X&#32447;&#36896;&#24433;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#21033;&#29992;&#22810;&#35270;&#35282;&#30340;&#20083;&#33146;X&#32447;&#36896;&#24433;&#20449;&#24687;&#21644;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#12290;&#23545;&#20110;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#21457;&#24067;&#20102;&#35768;&#22810;&#22810;&#35270;&#35282;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20004;&#20010;&#25110;&#22810;&#20010;&#35270;&#35282;&#30340;&#29305;&#24449;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#36827;&#34892;&#25340;&#25509;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#22810;&#35270;&#35282;&#30340;&#29616;&#26377;&#26041;&#27861;&#22312;&#29305;&#24449;&#34701;&#21512;&#30340;&#24847;&#20041;&#19978;&#24182;&#19981;&#20855;&#22791;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#35786;&#26029;&#26102;&#23545;&#35768;&#22810;&#35270;&#35282;&#24179;&#31561;&#23545;&#24453;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#20294;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#35270;&#35282;&#65288;&#21516;&#20391;&#35270;&#35282;&#65289;&#30340;&#20302;&#23618;&#29305;&#24449;&#20449;&#24687;&#65292;&#22686;&#24378;&#26816;&#26597;&#35270;&#35282;&#65288;&#20027;&#35201;&#35270;&#35282;&#65289;&#65292;&#28982;&#21518;&#23398;&#20064;&#21253;&#21547;&#30284;&#21464;&#29305;&#24449;&#30340;&#39640;&#23618;&#29305;&#24449;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26032;&#39062;&#30340;&#24694;&#24615;&#20083;&#33146;X&#32447;&#36896;&#24433;&#21512;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#19978;&#37319;&#26679;&#23569;&#25968;&#31867;&#26679;&#26412;&#12290;&#25105;&#20204;&#26131;&#20110;&#23454;&#26045;&#19988;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#24050;&#32463;&#28040;&#38500;&#20102;
&lt;/p&gt;
&lt;p&gt;
In recent years, many mammographic image analysis methods have been introduced for improving cancer classification tasks. Two major issues of mammogram classification tasks are leveraging multi-view mammographic information and class-imbalance handling. In the first problem, many multi-view methods have been released for concatenating features of two or more views for the training and inference stage. Having said that, most multi-view existing methods are not explainable in the meaning of feature fusion, and treat many views equally for diagnosing. Our work aims to propose a simple but novel method for enhancing examined view (main view) by leveraging low-level feature information from the auxiliary view (ipsilateral view) before learning the high-level feature that contains the cancerous features. For the second issue, we also propose a simple but novel malignant mammogram synthesis framework for upsampling minor class samples. Our easy-to-implement and no-training framework has elimi
&lt;/p&gt;</description></item><item><title>InteractionNet&#26159;&#22522;&#20110;Transformer&#30340;&#33258;&#21160;&#39550;&#39542;&#35268;&#21010;&#19982;&#39044;&#27979;&#30340;&#32852;&#21512;&#27169;&#22411;&#65292;&#21487;&#20197;&#25429;&#25417;&#20132;&#20114;&#24182;&#32852;&#21512;&#32771;&#34385;&#35268;&#21010;&#21644;&#39044;&#27979;&#65292;&#22312;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.03475</link><description>&lt;p&gt;
InteractionNet&#65306;&#22522;&#20110;Transformer&#30340;&#33258;&#21160;&#39550;&#39542;&#35268;&#21010;&#19982;&#39044;&#27979;&#30340;&#32852;&#21512;
&lt;/p&gt;
&lt;p&gt;
InteractionNet: Joint Planning and Prediction for Autonomous Driving with Transformers. (arXiv:2309.03475v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03475
&lt;/p&gt;
&lt;p&gt;
InteractionNet&#26159;&#22522;&#20110;Transformer&#30340;&#33258;&#21160;&#39550;&#39542;&#35268;&#21010;&#19982;&#39044;&#27979;&#30340;&#32852;&#21512;&#27169;&#22411;&#65292;&#21487;&#20197;&#25429;&#25417;&#20132;&#20114;&#24182;&#32852;&#21512;&#32771;&#34385;&#35268;&#21010;&#21644;&#39044;&#27979;&#65292;&#22312;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#21644;&#39044;&#27979;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20004;&#20010;&#37325;&#35201;&#27169;&#22359;&#65292;&#24182;&#19988;&#26368;&#36817;&#26377;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23558;&#35268;&#21010;&#21644;&#39044;&#27979;&#35270;&#20026;&#29420;&#31435;&#30340;&#65292;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#32570;&#20047;&#23545;&#20132;&#20114;&#21644;&#20132;&#36890;&#22330;&#26223;&#30340;&#21160;&#24577;&#21464;&#21270;&#30340;&#32771;&#34385;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InteractionNet&#65292;&#23427;&#21033;&#29992;Transformer&#22312;&#25152;&#26377;&#20132;&#36890;&#21442;&#19982;&#32773;&#20043;&#38388;&#20849;&#20139;&#20840;&#23616;&#19978;&#19979;&#25991;&#25512;&#29702;&#65292;&#20197;&#25429;&#25417;&#20132;&#20114;&#24182;&#23558;&#35268;&#21010;&#21644;&#39044;&#27979;&#20114;&#32852;&#36215;&#26469;&#23454;&#29616;&#32852;&#21512;&#12290;&#27492;&#22806;&#65292;InteractionNet&#36824;&#37096;&#32626;&#20102;&#21478;&#19968;&#20010;Transformer&#65292;&#24110;&#21161;&#27169;&#22411;&#39069;&#22806;&#20851;&#27880;&#21253;&#21547;&#20851;&#38190;&#25110;&#26410;&#35265;&#36710;&#36742;&#30340;&#24863;&#30693;&#21306;&#22495;&#12290;InteractionNet&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#24615;&#26041;&#38754;&#65292;&#36825;&#24471;&#30410;&#20110;&#23545;&#35268;&#21010;&#21644;&#39044;&#27979;&#30340;&#32852;&#21512;&#32771;&#34385;&#12290;&#20195;&#30721;&#23558;&#22312;https://github.com/fujiawei0724/InteractionNet&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning and prediction are two important modules of autonomous driving and have experienced tremendous advancement recently. Nevertheless, most existing methods regard planning and prediction as independent and ignore the correlation between them, leading to the lack of consideration for interaction and dynamic changes of traffic scenarios. To address this challenge, we propose InteractionNet, which leverages transformer to share global contextual reasoning among all traffic participants to capture interaction and interconnect planning and prediction to achieve joint. Besides, InteractionNet deploys another transformer to help the model pay extra attention to the perceived region containing critical or unseen vehicles. InteractionNet outperforms other baselines in several benchmarks, especially in terms of safety, which benefits from the joint consideration of planning and forecasting. The code will be available at https://github.com/fujiawei0724/InteractionNet.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24555;&#36895;&#30340;FixMatch&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35838;&#31243;&#25209;&#27425;&#22823;&#23567;&#65288;CBS&#65289;&#26469;&#20943;&#23569;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#35757;&#32451;&#35745;&#31639;&#37327;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#26631;&#35760;&#22686;&#24378;&#21644;&#35838;&#31243;&#20266;&#26631;&#31614;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.03469</link><description>&lt;p&gt;
&#24555;&#36895;&#30340;FixMatch: &#22522;&#20110;&#35838;&#31243;&#25209;&#27425;&#22823;&#23567;&#30340;&#24555;&#36895;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size. (arXiv:2309.03469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24555;&#36895;&#30340;FixMatch&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35838;&#31243;&#25209;&#27425;&#22823;&#23567;&#65288;CBS&#65289;&#26469;&#20943;&#23569;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#35757;&#32451;&#35745;&#31639;&#37327;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#26631;&#35760;&#22686;&#24378;&#21644;&#35838;&#31243;&#20266;&#26631;&#31614;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#36827;&#23637;&#20960;&#20046;&#23436;&#20840;&#28040;&#38500;&#20102;SSL&#21644;&#30417;&#30563;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#26631;&#31614;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#24615;&#33021;&#25552;&#21319;&#24448;&#24448;&#26159;&#20197;&#26174;&#33879;&#22686;&#21152;&#30340;&#35757;&#32451;&#35745;&#31639;&#20026;&#20195;&#20215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35838;&#31243;&#25209;&#27425;&#22823;&#23567;&#65288;CBS&#65289;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35757;&#32451;&#21160;&#24577;&#65292;&#37319;&#29992;&#19968;&#20010;&#23567;&#30340;&#26410;&#26631;&#35760;&#30340;&#25209;&#27425;&#22823;&#23567;&#24320;&#22987;&#35757;&#32451;&#65292;&#24182;&#36880;&#28176;&#22686;&#21152;&#21040;&#35757;&#32451;&#32467;&#26463;&#12290;&#26080;&#35770;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#25110;&#35757;&#32451;&#36718;&#27425;&#65292;&#37117;&#20351;&#29992;&#22266;&#23450;&#30340;&#35838;&#31243;&#65292;&#36890;&#36807;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#20943;&#23569;&#35757;&#32451;&#35745;&#31639;&#37327;&#12290;&#25105;&#20204;&#23558;CBS&#12289;&#24378;&#21270;&#26631;&#35760;&#22686;&#24378;&#21644;&#35838;&#31243;&#20266;&#26631;&#31614;&#65288;CPL&#65289;&#24212;&#29992;&#20110;FixMatch&#65292;&#24182;&#23558;&#36825;&#20010;&#26032;&#30340;SSL&#31639;&#27861;&#31216;&#20026;&#24555;&#36895;&#30340;FixMatch&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21106;&#23454;&#39564;&#65292;&#34920;&#26126;&#24378;&#21270;&#26631;&#35760;&#22686;&#24378;&#21644;/&#25110;CPL&#24182;&#19981;&#26174;&#33879;&#22320;&#20943;&#23569;&#35757;&#32451;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in Semi-Supervised Learning (SSL) have almost entirely closed the gap between SSL and Supervised Learning at a fraction of the number of labels. However, recent performance improvements have often come \textit{at the cost of significantly increased training computation}. To address this, we propose Curriculum Batch Size (CBS), \textit{an unlabeled batch size curriculum which exploits the natural training dynamics of deep neural networks.} A small unlabeled batch size is used in the beginning of training and is gradually increased to the end of training. A fixed curriculum is used regardless of dataset, model or number of epochs, and reduced training computations is demonstrated on all settings. We apply CBS, strong labeled augmentation, Curriculum Pseudo Labeling (CPL) \citep{FlexMatch} to FixMatch \citep{FixMatch} and term the new SSL algorithm Fast FixMatch. We perform an ablation study to show that strong labeled augmentation and/or CPL do not significantly reduce training 
&lt;/p&gt;</description></item><item><title>Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#27010;&#24565;&#24182;&#36827;&#34892;&#20998;&#31867;&#30340;&#26234;&#21147;&#27979;&#35797;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;Bongard&#38382;&#39064;&#20013;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#25972;&#21512;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#26159;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#25903;&#25345;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.03468</link><description>&lt;p&gt;
&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#23545;&#20110;Bongard&#38382;&#39064;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Cross-Image Context Matters for Bongard Problems. (arXiv:2309.03468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03468
&lt;/p&gt;
&lt;p&gt;
Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#27010;&#24565;&#24182;&#36827;&#34892;&#20998;&#31867;&#30340;&#26234;&#21147;&#27979;&#35797;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;Bongard&#38382;&#39064;&#20013;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#25972;&#21512;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#26159;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#25903;&#25345;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;Bongard&#38382;&#39064;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#8220;&#25903;&#25345;&#8221;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#23545;&#20110;&#26032;&#30340;&#26597;&#35810;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#21028;&#26029;&#23427;&#26159;&#21542;&#25551;&#36848;&#20102;&#20851;&#38190;&#27010;&#24565;&#30340;&#26234;&#21147;&#27979;&#35797;&#12290;&#22312;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;Bongard&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;Bongard-HOI&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#20165;&#36798;&#21040;&#20102;66%&#65288;&#20598;&#28982;&#20934;&#30830;&#29575;&#20026;50%&#65289;&#12290;&#20302;&#20934;&#30830;&#29575;&#36890;&#24120;&#24402;&#22240;&#20110;&#31070;&#32463;&#32593;&#32476;&#32570;&#20047;&#21457;&#29616;&#31867;&#20284;&#20154;&#31867;&#31526;&#21495;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#38382;&#39064;&#32780;&#22833;&#21435;&#20102;&#20934;&#30830;&#24615;&#65306;&#23427;&#20204;&#27809;&#26377;&#23558;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#21152;&#20837;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#20174;&#21333;&#20010;&#25903;&#25345;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#19982;&#28041;&#21450;&#23545;&#35937;&#20998;&#31867;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#19981;&#21516;&#65292;&#19968;&#20010;&#20856;&#22411;&#30340;Bongard&#38382;&#39064;&#20013;&#30340;&#8220;&#20851;&#38190;&#27010;&#24565;&#8221;&#21482;&#33021;&#20351;&#29992;&#22810;&#20010;&#27491;&#20363;&#21644;&#22810;&#20010;&#21453;&#20363;&#26469;&#21306;&#20998;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current machine learning methods struggle to solve Bongard problems, which are a type of IQ test that requires deriving an abstract "concept" from a set of positive and negative "support" images, and then classifying whether or not a new query image depicts the key concept. On Bongard-HOI, a benchmark for natural-image Bongard problems, existing methods have only reached 66% accuracy (where chance is 50%). Low accuracy is often attributed to neural nets' lack of ability to find human-like symbolic rules. In this work, we point out that many existing methods are forfeiting accuracy due to a much simpler problem: they do not incorporate information contained in the support set as a whole, and rely instead on information extracted from individual supports. This is a critical issue, because unlike in few-shot learning tasks concerning object classification, the "key concept" in a typical Bongard problem can only be distinguished using multiple positives and multiple negatives. We explore a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#20840;&#26041;&#20301;&#24863;&#30693;&#29983;&#25104;&#32593;&#32476;&#65288;AOG-Net&#65289;&#29992;&#20110;&#29983;&#25104;360&#24230;&#22270;&#20687;&#65292;&#36890;&#36807;&#28176;&#36827;&#22320;&#22806;&#25193;&#19981;&#23436;&#25972;&#30340;360&#24230;&#22270;&#20687;&#65292;&#24182;&#19982;&#31364;&#35270;&#22330;&#65288;NFoV&#65289;&#21644;&#25991;&#26412;&#24341;&#23548;&#30456;&#32467;&#21512;&#25110;&#21333;&#29420;&#20351;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#32454;&#33268;&#21644;&#19982;&#25991;&#26412;&#19968;&#33268;&#30340;&#27169;&#24335;&#65292;&#24182;&#20026;&#29992;&#25143;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#28789;&#27963;&#32534;&#36753;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.03467</link><description>&lt;p&gt;
Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation. (arXiv:2309.03467v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation. (arXiv:2309.03467v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03467
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#20840;&#26041;&#20301;&#24863;&#30693;&#29983;&#25104;&#32593;&#32476;&#65288;AOG-Net&#65289;&#29992;&#20110;&#29983;&#25104;360&#24230;&#22270;&#20687;&#65292;&#36890;&#36807;&#28176;&#36827;&#22320;&#22806;&#25193;&#19981;&#23436;&#25972;&#30340;360&#24230;&#22270;&#20687;&#65292;&#24182;&#19982;&#31364;&#35270;&#22330;&#65288;NFoV&#65289;&#21644;&#25991;&#26412;&#24341;&#23548;&#30456;&#32467;&#21512;&#25110;&#21333;&#29420;&#20351;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#32454;&#33268;&#21644;&#19982;&#25991;&#26412;&#19968;&#33268;&#30340;&#27169;&#24335;&#65292;&#24182;&#20026;&#29992;&#25143;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#28789;&#27963;&#32534;&#36753;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
360&#24230;&#65288;&#20840;&#26041;&#20301;&#65289;&#22270;&#20687;&#25552;&#20379;&#20102;&#19968;&#20010;&#22330;&#26223;&#30340;&#20840;&#26223;&#29699;&#29366;&#35270;&#22270;&#12290;&#26368;&#36817;&#65292;&#22312;&#20174;&#30001;&#25968;&#23383;&#30456;&#26426;&#21644;&#26234;&#33021;&#25163;&#26426;&#25429;&#33719;&#30340;&#20256;&#32479;&#31364;&#35270;&#22330;&#65288;NFoV&#65289;&#22270;&#20687;&#21512;&#25104;360&#24230;&#22270;&#20687;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#25552;&#20379;&#36523;&#20020;&#20854;&#22659;&#30340;&#20307;&#39564;&#65292;&#22914;&#34394;&#25311;&#29616;&#23454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#21512;&#25104;&#22797;&#26434;&#30340;&#35270;&#35273;&#32454;&#33410;&#25110;&#30830;&#20445;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#20840;&#26041;&#20301;&#24863;&#30693;&#29983;&#25104;&#32593;&#32476;&#65288;AOG-Net&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;NFoV&#21644;&#25991;&#26412;&#24341;&#23548;&#30456;&#32467;&#21512;&#25110;&#21333;&#29420;&#36827;&#34892;&#28176;&#36827;&#22320;&#22806;&#25193;&#19981;&#23436;&#25972;&#30340;360&#24230;&#22270;&#20687;&#26469;&#36827;&#34892;360&#24230;&#22270;&#20687;&#29983;&#25104;&#12290;&#36825;&#31181;&#33258;&#22238;&#24402;&#26041;&#26696;&#19981;&#20165;&#20801;&#35768;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#21644;&#35843;&#25972;&#36807;&#31243;&#26469;&#33719;&#21462;&#26356;&#31934;&#32454;&#21644;&#19982;&#25991;&#26412;&#19968;&#33268;&#30340;&#27169;&#24335;&#65292;&#36824;&#20026;&#29992;&#25143;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#32534;&#36753;&#26465;&#20214;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A 360-degree (omni-directional) image provides an all-encompassing spherical view of a scene. Recently, there has been an increasing interest in synthesising 360-degree images from conventional narrow field of view (NFoV) images captured by digital cameras and smartphones, for providing immersive experiences in various scenarios such as virtual reality. Yet, existing methods typically fall short in synthesizing intricate visual details or ensure the generated images align consistently with user-provided prompts. In this study, autoregressive omni-aware generative network (AOG-Net) is proposed for 360-degree image generation by out-painting an incomplete 360-degree image progressively with NFoV and text guidances joinly or individually. This autoregressive scheme not only allows for deriving finer-grained and text-consistent patterns by dynamically generating and adjusting the process but also offers users greater flexibility to edit their conditions throughout the generation process. A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#21453;&#28436;&#30340;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65288;\textsc{Mira}&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#30772;&#35299;&#22823;&#22810;&#25968;&#20027;&#27969;&#40657;&#30418;DNN&#27700;&#21360;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21463;&#20445;&#25252;&#27169;&#22411;&#30340;&#20869;&#37096;&#20449;&#24687;&#26469;&#24674;&#22797;&#21644;&#28040;&#38500;&#27700;&#21360;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.03466</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#21453;&#28436;&#30340;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#30772;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#40657;&#30418;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
MIRA: Cracking Black-box Watermarking on Deep Neural Networks via Model Inversion-based Removal Attacks. (arXiv:2309.03466v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#21453;&#28436;&#30340;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65288;\textsc{Mira}&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#30772;&#35299;&#22823;&#22810;&#25968;&#20027;&#27969;&#40657;&#30418;DNN&#27700;&#21360;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21463;&#20445;&#25252;&#27169;&#22411;&#30340;&#20869;&#37096;&#20449;&#24687;&#26469;&#24674;&#22797;&#21644;&#28040;&#38500;&#27700;&#21360;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#25252;&#35757;&#32451;&#26377;&#32032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#40657;&#30418;DNN&#27700;&#21360;&#24050;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#27700;&#21360;&#34987;&#23884;&#20837;&#21040;DNN&#27169;&#22411;&#22312;&#19968;&#32452;&#29305;&#21035;&#35774;&#35745;&#30340;&#26679;&#26412;&#19978;&#30340;&#39044;&#27979;&#34892;&#20026;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#32463;&#39564;&#35777;&#26126;&#65292;&#22823;&#22810;&#25968;&#40657;&#30418;&#27700;&#21360;&#26041;&#26696;&#23545;&#24050;&#30693;&#30340;&#21435;&#38500;&#25915;&#20987;&#20855;&#26377;&#25269;&#25239;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#21453;&#28436;&#30340;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65288;\textsc{Mira}&#65289;&#65292;&#35813;&#26041;&#27861;&#23545;&#22823;&#22810;&#25968;&#20027;&#27969;&#40657;&#30418;DNN&#27700;&#21360;&#26041;&#26696;&#37117;&#26159;&#26080;&#20851;&#27700;&#21360;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#27969;&#31243;&#21033;&#29992;&#21463;&#20445;&#25252;&#27169;&#22411;&#30340;&#20869;&#37096;&#20449;&#24687;&#26469;&#24674;&#22797;&#21644;&#28040;&#38500;&#27700;&#21360;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#30446;&#26631;&#31867;&#21035;&#26816;&#27979;&#21644;&#24674;&#22797;&#26679;&#26412;&#20998;&#21106;&#31639;&#27861;&#65292;&#20197;&#20943;&#23569;\textsc{Mira}&#24341;&#36215;&#30340;&#25928;&#29992;&#25439;&#22833;&#65292;&#24182;&#23454;&#29616;&#26368;&#20248;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
To protect the intellectual property of well-trained deep neural networks (DNNs), black-box DNN watermarks, which are embedded into the prediction behavior of DNN models on a set of specially-crafted samples, have gained increasing popularity in both academy and industry. Watermark robustness is usually implemented against attackers who steal the protected model and obfuscate its parameters for watermark removal. Recent studies empirically prove the robustness of most black-box watermarking schemes against known removal attempts.  In this paper, we propose a novel Model Inversion-based Removal Attack (\textsc{Mira}), which is watermark-agnostic and effective against most of mainstream black-box DNN watermarking schemes. In general, our attack pipeline exploits the internals of the protected model to recover and unlearn the watermark message. We further design target class detection and recovered sample splitting algorithms to reduce the utility loss caused by \textsc{Mira} and achieve 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyncDreamer&#30340;&#26032;&#22411;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#22270;&#20687;&#12290;&#36890;&#36807;&#21516;&#27493;&#25152;&#26377;&#29983;&#25104;&#22270;&#20687;&#30340;&#20013;&#38388;&#29366;&#24577;&#65292;&#24182;&#21033;&#29992;3D&#24863;&#30693;&#29305;&#24449;&#27880;&#24847;&#26426;&#21046;&#65292;SyncDreamer&#33021;&#22815;&#23454;&#29616;&#22312;&#19981;&#21516;&#35270;&#35282;&#19978;&#29983;&#25104;&#39640;&#24230;&#19968;&#33268;&#30340;&#22270;&#20687;&#65292;&#20026;&#21508;&#31181;3D&#29983;&#25104;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2309.03453</link><description>&lt;p&gt;
SyncDreamer: &#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
SyncDreamer: Generating Multiview-consistent Images from a Single-view Image. (arXiv:2309.03453v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyncDreamer&#30340;&#26032;&#22411;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#22270;&#20687;&#12290;&#36890;&#36807;&#21516;&#27493;&#25152;&#26377;&#29983;&#25104;&#22270;&#20687;&#30340;&#20013;&#38388;&#29366;&#24577;&#65292;&#24182;&#21033;&#29992;3D&#24863;&#30693;&#29305;&#24449;&#27880;&#24847;&#26426;&#21046;&#65292;SyncDreamer&#33021;&#22815;&#23454;&#29616;&#22312;&#19981;&#21516;&#35270;&#35282;&#19978;&#29983;&#25104;&#39640;&#24230;&#19968;&#33268;&#30340;&#22270;&#20687;&#65292;&#20026;&#21508;&#31181;3D&#29983;&#25104;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyncDreamer&#30340;&#26032;&#22411;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#22270;&#20687;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;2D&#25193;&#25955;&#27169;&#22411;&#65292;&#26368;&#36817;&#30340;Zero123&#24037;&#20316;&#23637;&#31034;&#20102;&#20174;&#21333;&#35270;&#22270;&#29289;&#20307;&#22270;&#20687;&#29983;&#25104;&#21512;&#29702;&#30340;&#26032;&#35270;&#35282;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#22270;&#20687;&#22312;&#20960;&#20309;&#21644;&#39068;&#33394;&#19978;&#30340;&#19968;&#33268;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#27493;&#30340;&#22810;&#35270;&#35282;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#27169;&#25311;&#20102;&#22810;&#35270;&#35282;&#22270;&#20687;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#21453;&#21521;&#36807;&#31243;&#29983;&#25104;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#22270;&#20687;&#12290;SyncDreamer&#36890;&#36807;3D&#24863;&#30693;&#29305;&#24449;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#21453;&#21521;&#36807;&#31243;&#30340;&#27599;&#20010;&#27493;&#39588;&#20013;&#21516;&#27493;&#25152;&#26377;&#29983;&#25104;&#22270;&#20687;&#30340;&#20013;&#38388;&#29366;&#24577;&#65292;&#20174;&#32780;&#30456;&#20851;&#32852;&#19981;&#21516;&#35270;&#35282;&#19978;&#30340;&#30456;&#24212;&#29305;&#24449;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;SyncDreamer&#33021;&#22815;&#22312;&#19981;&#21516;&#35270;&#35282;&#20043;&#38388;&#29983;&#25104;&#39640;&#24230;&#19968;&#33268;&#30340;&#22270;&#20687;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#29992;&#20110;&#21508;&#31181;3D&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel diffusion model called that generates multiview-consistent images from a single-view image. Using pretrained large-scale 2D diffusion models, recent work Zero123 demonstrates the ability to generate plausible novel views from a single-view image of an object. However, maintaining consistency in geometry and colors for the generated images remains a challenge. To address this issue, we propose a synchronized multiview diffusion model that models the joint probability distribution of multiview images, enabling the generation of multiview-consistent images in a single reverse process. SyncDreamer synchronizes the intermediate states of all the generated images at every step of the reverse process through a 3D-aware feature attention mechanism that correlates the corresponding features across different views. Experiments show that SyncDreamer generates images with high consistency across different views, thus making it well-suited for various 3D generation
&lt;/p&gt;</description></item><item><title>XGen-7B&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20811;&#26381;&#24320;&#28304;LLMs&#22312;&#25903;&#25345;&#38271;&#24207;&#21015;&#38271;&#24230;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#21462;&#24471;&#19982;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;LLMs&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#25512;&#36827;&#20102;&#30740;&#31350;&#36827;&#23637;&#21644;&#21830;&#19994;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.03450</link><description>&lt;p&gt;
XGen-7B&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
XGen-7B Technical Report. (arXiv:2309.03450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03450
&lt;/p&gt;
&lt;p&gt;
XGen-7B&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20811;&#26381;&#24320;&#28304;LLMs&#22312;&#25903;&#25345;&#38271;&#24207;&#21015;&#38271;&#24230;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#21462;&#24471;&#19982;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;LLMs&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#25512;&#36827;&#20102;&#30740;&#31350;&#36827;&#23637;&#21644;&#21830;&#19994;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21464;&#24471;&#26222;&#36941;&#65292;&#25913;&#21464;&#20102;&#25105;&#20204;&#19982;&#20449;&#24687;&#20132;&#20114;&#21644;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#39640;&#24615;&#33021;&#30340;LLMs&#20173;&#28982;&#21463;&#38480;&#20110;&#19987;&#26377;&#22681;&#22721;&#65292;&#38459;&#30861;&#20102;&#31185;&#23398;&#36827;&#23637;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22810;&#25968;&#24320;&#28304;&#30340;LLMs&#22312;&#25903;&#25345;&#36739;&#38271;&#24207;&#21015;&#38271;&#24230;&#26041;&#38754;&#26377;&#38480;&#65292;&#32780;&#36825;&#23545;&#20110;&#35768;&#22810;&#38656;&#35201;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#36827;&#34892;&#25512;&#29702;&#30340;&#20219;&#21153;&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;XGen&#65292;&#19968;&#31995;&#21015;7B&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#21487;&#25903;&#25345;&#38271;&#24230;&#20026;8K&#30340;&#24207;&#21015;&#21644;1.5T&#20010;&#20196;&#29260;&#12290;&#25105;&#20204;&#36824;&#23545;XGen&#27169;&#22411;&#36827;&#34892;&#20102;&#20844;&#20849;&#39046;&#22495;&#25945;&#23398;&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#21019;&#24314;&#20102;&#23427;&#20204;&#30340;&#25945;&#23398;&#20248;&#21270;&#29256;&#26412;&#65288;XGen-Inst&#65289;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#24320;&#28304;&#65292;&#29992;&#20110;&#30740;&#31350;&#36827;&#23637;&#21644;&#21830;&#19994;&#24212;&#29992;&#12290;&#25105;&#20204;&#23545;&#26631;&#20934;&#22522;&#20934;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;LLMs&#30456;&#27604;&#65292;XGen&#27169;&#22411;&#23454;&#29616;&#20102;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#38024;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#36827;&#34892;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have become ubiquitous across various domains, transforming the way we interact with information and conduct research. However, most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context. To address this, we have trained XGen, a series of 7B parameter models on up to 8K sequence length for up to 1.5T tokens. We have also finetuned the XGen models on public-domain instructional data, creating their instruction-tuned counterparts (XGen-Inst). We open-source our models for both research advancements and commercial applications. Our evaluation on standard benchmarks shows that XGen models achieve comparable or better results when compared with state-of-the-art open-source LLMs. Our targeted evaluation on long sequence modeling task
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03409</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Optimizers. (arXiv:2309.03409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#34429;&#28982;&#22522;&#20110;&#23548;&#25968;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#26159;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#26159;&#27809;&#26377;&#26799;&#24230;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;&#20248;&#21270;&#20219;&#21153;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#25551;&#36848;&#12290;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#65292;LLM&#20174;&#21253;&#21547;&#20808;&#21069;&#29983;&#25104;&#30340;&#35299;&#19982;&#20854;&#20540;&#30340;&#25552;&#31034;&#20013;&#29983;&#25104;&#26032;&#30340;&#35299;&#65292;&#28982;&#21518;&#23545;&#26032;&#30340;&#35299;&#36827;&#34892;&#35780;&#20272;&#24182;&#28155;&#21152;&#21040;&#25552;&#31034;&#20013;&#65292;&#29992;&#20110;&#19979;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;OPRO&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#28982;&#21518;&#36716;&#21521;&#25552;&#31034;&#20248;&#21270;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#33021;&#26368;&#22823;&#21270;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#25351;&#20196;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;LLM&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;OPRO&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#22312;GSM8K&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;8%&#65292;&#22312;Big-Bench Hard&#20219;&#21153;&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#28151;&#38899;&#24072;&#19982;&#23458;&#25143;&#30340;&#20114;&#21160;&#20197;&#21450;&#20182;&#20204;&#22914;&#20309;&#21033;&#29992;&#23458;&#25143;&#30340;&#21453;&#39304;&#25351;&#23548;&#28151;&#38899;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28151;&#38899;&#24072;&#36890;&#36807;&#20351;&#29992;&#21442;&#32771;&#26354;&#30446;&#21644;&#21512;&#20316;&#27807;&#36890;&#31561;&#26041;&#24335;&#65292;&#19982;&#23458;&#25143;&#24314;&#31435;&#36215;&#23545;&#28151;&#38899;&#26399;&#26395;&#38899;&#25928;&#30340;&#40664;&#22865;&#32422;&#23450;&#12290;</title><link>http://arxiv.org/abs/2309.03404</link><description>&lt;p&gt;
&#36890;&#35759;&#21644;&#21442;&#32771;&#26354;&#30446;&#22312;&#28151;&#38899;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#65306;&#26469;&#33258;&#19987;&#19994;&#28151;&#38899;&#24072;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
The Role of Communication and Reference Songs in the Mixing Process: Insights from Professional Mix Engineers. (arXiv:2309.03404v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#28151;&#38899;&#24072;&#19982;&#23458;&#25143;&#30340;&#20114;&#21160;&#20197;&#21450;&#20182;&#20204;&#22914;&#20309;&#21033;&#29992;&#23458;&#25143;&#30340;&#21453;&#39304;&#25351;&#23548;&#28151;&#38899;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28151;&#38899;&#24072;&#36890;&#36807;&#20351;&#29992;&#21442;&#32771;&#26354;&#30446;&#21644;&#21512;&#20316;&#27807;&#36890;&#31561;&#26041;&#24335;&#65292;&#19982;&#23458;&#25143;&#24314;&#31435;&#36215;&#23545;&#28151;&#38899;&#26399;&#26395;&#38899;&#25928;&#30340;&#40664;&#22865;&#32422;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#38899;&#20048;&#28151;&#38899;&#38656;&#35201;&#25216;&#26415;&#21644;&#21019;&#36896;&#21147;&#30340;&#31934;&#28251;&#65292;&#20294;&#19982;&#23458;&#25143;&#30340;&#28165;&#26224;&#27807;&#36890;&#33267;&#20851;&#37325;&#35201;&#12290;&#28151;&#38899;&#24072;&#24517;&#39035;&#29702;&#35299;&#23458;&#25143;&#30340;&#26399;&#26395;&#21644;&#20559;&#22909;&#65292;&#24182;&#20849;&#21516;&#21162;&#21147;&#23454;&#29616;&#25152;&#38656;&#30340;&#38899;&#25928;&#12290;&#36890;&#36807;&#20351;&#29992;&#21442;&#32771;&#26354;&#30446;&#21644;&#33402;&#26415;&#23478;&#19982;&#24037;&#31243;&#24072;&#20043;&#38388;&#20132;&#25442;&#30340;&#28436;&#31034;&#28151;&#38899;&#31561;&#25351;&#21335;&#65292;&#36890;&#24120;&#21487;&#20197;&#36798;&#25104;&#23545;&#28151;&#38899;&#26399;&#26395;&#38899;&#25928;&#30340;&#40664;&#22865;&#32422;&#23450;&#65292;&#26377;&#26102;&#36824;&#21487;&#20197;&#20351;&#29992;&#35821;&#20041;&#26415;&#35821;&#36827;&#34892;&#35328;&#35828;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#30001;&#20004;&#20010;&#38454;&#27573;&#26500;&#25104;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#30340;&#21457;&#29616;&#65292;&#26088;&#22312;&#20102;&#35299;&#19987;&#19994;&#28151;&#38899;&#24072;&#22914;&#20309;&#19982;&#23458;&#25143;&#20114;&#21160;&#65292;&#24182;&#21033;&#29992;&#20182;&#20204;&#30340;&#21453;&#39304;&#25351;&#23548;&#28151;&#38899;&#36807;&#31243;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#23545;&#20116;&#21517;&#28151;&#38899;&#24072;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#38754;&#35848;&#65292;&#26088;&#22312;&#20102;&#35299;&#20182;&#20204;&#30340;&#27807;&#36890;&#31574;&#30053;&#12289;&#21019;&#36896;&#36807;&#31243;&#21644;&#20915;&#31574;&#26631;&#20934;&#12290;&#22522;&#20110;&#36825;&#20123;&#35775;&#35848;&#30340;&#25512;&#35770;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#32447;&#38382;&#21367;&#65292;&#24182;&#23545;22&#21517;&#28151;&#38899;&#24072;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective music mixing requires technical and creative finesse, but clear communication with the client is crucial. The mixing engineer must grasp the client's expectations, and preferences, and collaborate to achieve the desired sound. The tacit agreement for the desired sound of the mix is often established using guides like reference songs and demo mixes exchanged between the artist and the engineer and sometimes verbalised using semantic terms. This paper presents the findings of a two-phased exploratory study aimed at understanding how professional mixing engineers interact with clients and use their feedback to guide the mixing process. For phase one, semi-structured interviews were conducted with five mixing engineers with the aim of gathering insights about their communication strategies, creative processes, and decision-making criteria. Based on the inferences from these interviews, an online questionnaire was designed and administered to a larger group of 22 mixing engineers 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#39550;&#39542;&#20013;&#36816;&#21160;&#39044;&#27979;&#30340;&#39640;&#25928;&#22522;&#32447;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#22320;&#22270;&#21644;&#36807;&#21435;&#36712;&#36857;&#20449;&#24687;&#30340;&#23454;&#26102;&#24212;&#29992;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03387</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#36816;&#21160;&#39044;&#27979;&#30340;&#39640;&#25928;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
Efficient Baselines for Motion Prediction in Autonomous Driving. (arXiv:2309.03387v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03387
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#39550;&#39542;&#20013;&#36816;&#21160;&#39044;&#27979;&#30340;&#39640;&#25928;&#22522;&#32447;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#22320;&#22270;&#21644;&#36807;&#21435;&#36712;&#36857;&#20449;&#24687;&#30340;&#23454;&#26102;&#24212;&#29992;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#24847;&#22797;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#20174;&#31616;&#21333;&#26426;&#22120;&#20154;&#21040;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#22810;&#26041;&#21608;&#22260;&#20195;&#29702;&#30340;&#36816;&#21160;&#39044;&#27979;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#24403;&#21069;&#30340;&#25216;&#26415;&#36890;&#36807;&#31471;&#21040;&#31471;&#30340;&#27969;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#36755;&#20837;&#25968;&#25454;&#36890;&#24120;&#26159;&#29289;&#29702;&#20449;&#24687;&#30340;&#28210;&#26579;&#39030;&#35270;&#22270;&#21644;&#26368;&#30456;&#20851;&#20195;&#29702;&#30340;&#36807;&#21435;&#36712;&#36857;&#65307;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26159;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#30340;&#24517;&#38656;&#12290;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#65292;&#21487;&#38752;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#24517;&#39035;&#21450;&#26102;&#20135;&#29983;&#21512;&#29702;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39640;&#31471;&#27169;&#22411;&#21487;&#33021;&#23545;&#20110;&#21516;&#26102;&#20351;&#29992;&#22320;&#22270;&#21644;&#36807;&#21435;&#36712;&#36857;&#36825;&#20004;&#31181;&#20449;&#24687;&#20197;&#21450;&#26497;&#23569;&#21487;&#35299;&#37322;&#24615;&#30340;&#29289;&#29702;&#20449;&#24687;&#30340;&#23454;&#26102;&#24212;&#29992;&#32780;&#35328;&#36807;&#20110;&#22797;&#26434;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#27599;&#20010;&#29305;&#23450;&#20132;&#36890;&#22330;&#26223;&#30340;&#21487;&#29992;&#36755;&#20837;&#25968;&#37327;&#65292;&#32780;&#36825;&#20123;&#36755;&#20837;&#38590;&#20197;&#33719;&#21462;&#65292;&#25104;&#26412;&#39640;&#26114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion Prediction (MP) of multiple surroundings agents is a crucial task in arbitrarily complex environments, from simple robots to Autonomous Driving Stacks (ADS). Current techniques tackle this problem using end-to-end pipelines, where the input data is usually a rendered top-view of the physical information and the past trajectories of the most relevant agents; leveraging this information is a must to obtain optimal performance. In that sense, a reliable ADS must produce reasonable predictions on time. However, despite many approaches use simple ConvNets and LSTMs to obtain the social latent features, State-Of-The-Art (SOTA) models might be too complex for real-time applications when using both sources of information (map and past trajectories) as well as little interpretable, specially considering the physical information. Moreover, the performance of such models highly depends on the number of available inputs for each particular traffic scenario, which are expensive to obtain, pa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#30340;&#20998;&#32423;&#27491;&#31867;-&#26410;&#26631;&#35760;&#65288;PU&#65289;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#24930;&#24615;&#30149;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#19981;&#21516;&#20154;&#32676;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;PU&#27169;&#22411;&#26641;&#21644;&#32858;&#21512;&#20854;&#36755;&#20986;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.03386</link><description>&lt;p&gt;
&#22522;&#20110;&#31038;&#21306;&#30340;&#20998;&#32423;&#27491;&#31867;-&#26410;&#26631;&#35760;&#65288;PU&#65289;&#27169;&#22411;&#34701;&#21512;&#29992;&#20110;&#24930;&#24615;&#30149;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Community-Based Hierarchical Positive-Unlabeled (PU) Model Fusion for Chronic Disease Prediction. (arXiv:2309.03386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#30340;&#20998;&#32423;&#27491;&#31867;-&#26410;&#26631;&#35760;&#65288;PU&#65289;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#24930;&#24615;&#30149;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#19981;&#21516;&#20154;&#32676;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;PU&#27169;&#22411;&#26641;&#21644;&#32858;&#21512;&#20854;&#36755;&#20986;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#31867;-&#26410;&#26631;&#35760;&#65288;PU&#65289;&#23398;&#20064;&#26159;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#19968;&#20010;&#25361;&#25112;&#65292;&#22312;&#36825;&#31181;&#38382;&#39064;&#20013;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#23569;&#37327;&#27491;&#31867;&#25968;&#25454;&#23454;&#20363;&#65292;&#21487;&#20197;&#29992;&#20110;&#24930;&#24615;&#30149;&#31579;&#26597;&#38382;&#39064;&#12290;&#26368;&#20808;&#36827;&#30340;PU&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#23548;&#33268;&#20102;&#21508;&#31181;&#39118;&#38505;&#20272;&#35745;&#22120;&#30340;&#21457;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#24573;&#35270;&#20102;&#19981;&#21516;&#20154;&#32676;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#31867;-&#26410;&#26631;&#35760;&#23398;&#20064;&#26641;&#65288;PUtree&#65289;&#31639;&#27861;&#12290;PUtree&#26088;&#22312;&#32771;&#34385;&#31038;&#21306;&#65292;&#22914;&#19981;&#21516;&#30340;&#24180;&#40836;&#25110;&#25910;&#20837;&#27573;&#65292;&#22312;&#24930;&#24615;&#30149;&#39044;&#27979;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#20998;&#31867;&#20915;&#31574;&#26041;&#27861;&#65292;&#23427;&#20197;&#23618;&#27425;&#26041;&#24335;&#26500;&#24314;&#22522;&#20110;&#31038;&#21306;&#30340;PU&#27169;&#22411;&#65292;&#28982;&#21518;&#32858;&#21512;&#23427;&#20204;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#26641;&#19978;&#27599;&#20010;PU&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#38750;&#21494;&#33410;&#28857;&#30340;&#20998;&#35010;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#25513;&#30721;&#24674;&#22797;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20351;&#24471;&#27169;&#22411;&#22312;&#20010;&#20307;&#19978;&#33021;&#36827;&#34892;&#20805;&#20998;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positive-Unlabeled (PU) Learning is a challenge presented by binary classification problems where there is an abundance of unlabeled data along with a small number of positive data instances, which can be used to address chronic disease screening problem. State-of-the-art PU learning methods have resulted in the development of various risk estimators, yet they neglect the differences among distinct populations. To address this issue, we present a novel Positive-Unlabeled Learning Tree (PUtree) algorithm. PUtree is designed to take into account communities such as different age or income brackets, in tasks of chronic disease prediction. We propose a novel approach for binary decision-making, which hierarchically builds community-based PU models and then aggregates their deliverables. Our method can explicate each PU model on the tree for the optimized non-leaf PU node splitting. Furthermore, a mask-recovery data augmentation strategy enables sufficient training of the model in individua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#19979;&#28216;&#20219;&#21153;&#30340;&#33258;&#30417;&#30563;&#36974;&#32617;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#32534;&#30721;&#12290;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#22823;&#37327;&#26410;&#26631;&#35760;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#23558;&#35813;&#30693;&#35782;&#24212;&#29992;&#20110;&#22303;&#22320;&#22320;&#24418;&#30340;&#24314;&#31569;&#29289;&#21644;&#36947;&#36335;&#20998;&#21106;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.03367</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#19979;&#28216;&#20219;&#21153;&#30340;&#33258;&#30417;&#30563;&#36974;&#32617;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Masked Digital Elevation Models Encoding for Low-Resource Downstream Tasks. (arXiv:2309.03367v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#19979;&#28216;&#20219;&#21153;&#30340;&#33258;&#30417;&#30563;&#36974;&#32617;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#32534;&#30721;&#12290;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#22823;&#37327;&#26410;&#26631;&#35760;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#23558;&#35813;&#30693;&#35782;&#24212;&#29992;&#20110;&#22303;&#22320;&#22320;&#24418;&#30340;&#24314;&#31569;&#29289;&#21644;&#36947;&#36335;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#26159;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20027;&#35201;&#29942;&#39048;&#20043;&#19968;&#12290;&#38543;&#30528;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#36807;&#25311;&#21512;&#21644;&#19981;&#31283;&#23450;&#23398;&#20064;&#30340;&#24809;&#32602;&#20063;&#36234;&#39640;&#12290;&#20170;&#22825;&#36890;&#24120;&#37319;&#29992;&#30340;&#33539;&#24335;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#27169;&#22411;&#35797;&#22270;&#20174;&#22823;&#37327;&#38750;&#32467;&#26500;&#21270;&#21644;&#26080;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#28982;&#21518;&#23558;&#35813;&#30693;&#35782;&#36716;&#31227;&#21040;&#25152;&#38656;&#30340;&#20219;&#21153;&#20013;&#12290;&#22312;&#20854;&#20182;&#27169;&#24577;&#20013;&#65292;&#19968;&#20123;&#33879;&#21517;&#30340;&#33258;&#30417;&#30563;&#31034;&#20363;&#21253;&#25324;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;BERT&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;Wav2Vec&#20197;&#21450;&#29992;&#20110;&#35270;&#35273;&#30340;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#21033;&#29992;&#21464;&#21387;&#22120;&#26469;&#35299;&#20915;&#36974;&#32617;&#39044;&#27979;&#20219;&#21153;&#12290;&#30001;&#20110;&#20960;&#21313;&#24180;&#30340;&#25968;&#25454;&#37319;&#38598;&#65292;&#24182;&#27809;&#26377;&#31934;&#30830;&#21487;&#38752;&#22320;&#27880;&#37322;&#25968;&#25454;&#65292;&#22320;&#29702;AI&#29420;&#29305;&#22320;&#20855;&#22791;&#21033;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#25552;&#20379;&#22320;&#29699;&#34920;&#38754;&#35814;&#32454;&#22320;&#24418;&#30340;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#65288;DEM&#65289;&#20013;&#25552;&#21462;&#24314;&#31569;&#29289;&#21644;&#36947;&#36335;&#20998;&#21106;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#26159;Ma
&lt;/p&gt;
&lt;p&gt;
The lack of quality labeled data is one of the main bottlenecks for training Deep Learning models. As the task increases in complexity, there is a higher penalty for overfitting and unstable learning. The typical paradigm employed today is Self-Supervised learning, where the model attempts to learn from a large corpus of unstructured and unlabeled data and then transfer that knowledge to the required task. Some notable examples of self-supervision in other modalities are BERT for Large Language Models, Wav2Vec for Speech Recognition, and the Masked AutoEncoder for Vision, which all utilize Transformers to solve a masked prediction task. GeoAI is uniquely poised to take advantage of the self-supervised methodology due to the decades of data collected, little of which is precisely and dependably annotated. Our goal is to extract building and road segmentations from Digital Elevation Models (DEM) that provide a detailed topography of the earths surface. The proposed architecture is the Ma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#28789;&#24039;&#25805;&#32437;&#25216;&#33021;&#30340;&#39640;&#25928;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#24341;&#23548;&#65292;&#23454;&#29616;&#20102;&#37325;&#29992;&#25968;&#25454;&#20197;&#38477;&#20302;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03322</link><description>&lt;p&gt;
REBOOT: &#37325;&#29992;&#25968;&#25454;&#20197;&#24341;&#23548;&#39640;&#25928;&#30340;&#29616;&#23454;&#19990;&#30028;&#28789;&#24039;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation. (arXiv:2309.03322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#28789;&#24039;&#25805;&#32437;&#25216;&#33021;&#30340;&#39640;&#25928;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#24341;&#23548;&#65292;&#23454;&#29616;&#20102;&#37325;&#29992;&#25968;&#25454;&#20197;&#38477;&#20302;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28041;&#21450;&#25509;&#35302;&#23494;&#38598;&#20132;&#20114;&#30340;&#28789;&#24039;&#25805;&#32437;&#20219;&#21153;&#65292;&#27169;&#22411;&#39537;&#21160;&#30340;&#25511;&#21046;&#31995;&#32479;&#21644;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#37117;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22797;&#26434;&#24615;&#26469;&#33258;&#20110;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#38656;&#35201;&#21160;&#24577;&#24314;&#31435;&#21644;&#26029;&#24320;&#25509;&#35302;&#12289;&#24179;&#34913;&#38750;&#20280;&#25163;&#25345;&#21147;&#24182;&#25511;&#21046;&#22823;&#37327;&#33258;&#30001;&#24230;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30001;&#20110;&#20854;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#33258;&#20027;&#33719;&#21462;&#26368;&#20339;&#25805;&#32437;&#31574;&#30053;&#30340;&#33021;&#21147;&#32780;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#24120;&#24120;&#21463;&#21040;&#29983;&#25104;&#22823;&#37327;&#26679;&#26412;&#12289;&#37325;&#32622;&#29615;&#22659;&#21644;&#33719;&#21462;&#22870;&#21169;&#20449;&#21495;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;RL&#30340;&#28789;&#24039;&#25805;&#32437;&#25216;&#33021;&#30340;&#39640;&#25928;&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23558;&#26368;&#36817;&#22312;&#26679;&#26412;&#39640;&#25928;RL&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#24341;&#23548;&#26041;&#38754;&#30340;&#36827;&#23637;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#32452;&#21512;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;&#25110;&#29289;&#20307;&#30340;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dexterous manipulation tasks involving contact-rich interactions pose a significant challenge for both model-based control systems and imitation learning algorithms. The complexity arises from the need for multi-fingered robotic hands to dynamically establish and break contacts, balance non-prehensile forces, and control large degrees of freedom. Reinforcement learning (RL) offers a promising approach due to its general applicability and capacity to autonomously acquire optimal manipulation strategies. However, its real-world application is often hindered by the necessity to generate a large number of samples, reset the environment, and obtain reward signals. In this work, we introduce an efficient system for learning dexterous manipulation skills with RL to alleviate these challenges. The main idea of our approach is the integration of recent advances in sample-efficient RL and replay buffer bootstrapping. This combination allows us to utilize data from different tasks or objects as a
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36951;&#20256;&#31639;&#27861;&#20013;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36827;&#21270;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#36866;&#24212;&#24230;&#24471;&#20998;&#35201;&#20040;&#19982;&#23436;&#20840;&#36816;&#34892;&#30340;&#36951;&#20256;&#31639;&#27861;&#30456;&#21516;&#65292;&#35201;&#20040;&#31245;&#24494;&#20302;&#19968;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.03318</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Fitness Approximation through Machine Learning. (arXiv:2309.03318v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03318
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36951;&#20256;&#31639;&#27861;&#20013;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36827;&#21270;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#36866;&#24212;&#24230;&#24471;&#20998;&#35201;&#20040;&#19982;&#23436;&#20840;&#36816;&#34892;&#30340;&#36951;&#20256;&#31639;&#27861;&#30456;&#21516;&#65292;&#35201;&#20040;&#31245;&#24494;&#20302;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36951;&#20256;&#31639;&#27861;&#20013;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;&#65292;&#37325;&#28857;&#26159;&#22312;Gymnasium&#65288;&#28216;&#25103;&#65289;&#27169;&#25311;&#22120;&#20013;&#30340;&#36827;&#21270;&#20195;&#29702;&#19978; - &#22312;&#36825;&#37324;&#36866;&#24212;&#24230;&#35745;&#31639;&#26159;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#32500;&#25252;&#19968;&#20010;&#37319;&#26679;&#20010;&#20307;&#21450;&#20854;&#23454;&#38469;&#36866;&#24212;&#24230;&#24471;&#20998;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#25972;&#20010;&#36827;&#21270;&#36807;&#31243;&#20013;&#19981;&#26029;&#26356;&#26032;&#19968;&#20010;&#36866;&#24212;&#24230;&#36817;&#20284;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;1&#65289;&#22312;&#23454;&#38469;&#36866;&#24212;&#24230;&#21644;&#36817;&#20284;&#36866;&#24212;&#24230;&#20043;&#38388;&#20999;&#25442;&#65292;2&#65289;&#23545;&#31181;&#32676;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#21450;3&#65289;&#21152;&#26435;&#37319;&#26679;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36866;&#24212;&#24230;&#35745;&#31639;&#30340;&#36817;&#20284;&#27604;&#20363;&#21462;&#20915;&#20110;&#23436;&#20840;&#36816;&#34892;GA&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36827;&#21270;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#36866;&#24212;&#24230;&#24471;&#20998;&#35201;&#20040;&#19982;&#23436;&#20840;&#36816;&#34892;&#30340;GA&#30456;&#21516;&#65292;&#35201;&#20040;&#31245;&#24494;&#20302;&#19968;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#24212;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to performing fitness approximation in genetic algorithms (GAs) using machine-learning (ML) models, focusing on evolutionary agents in Gymnasium (game) simulators -- where fitness computation is costly. Maintaining a dataset of sampled individuals along with their actual fitness scores, we continually update throughout an evolutionary run a fitness-approximation ML model. We compare different methods for: 1) switching between actual and approximate fitness, 2) sampling the population, and 3) weighting the samples. Experimental findings demonstrate significant improvement in evolutionary runtimes, with fitness scores that are either identical or slightly lower than that of the fully run GA -- depending on the ratio of approximate-to-actual-fitness computation. Our approach is generic and can be easily applied to many different domains.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#28145;&#24230;&#20266;&#36896;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#20854;&#23545;&#25968;&#23383;&#35270;&#35273;&#23186;&#20307;&#23436;&#25972;&#24615;&#30340;&#23041;&#32961;&#65292;&#20197;&#21450;&#28145;&#20266;&#36896;&#26816;&#27979;&#25216;&#26415;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03295</link><description>&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#31639;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Deep-Fake Algorithms. (arXiv:2309.03295v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#28145;&#24230;&#20266;&#36896;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#20854;&#23545;&#25968;&#23383;&#35270;&#35273;&#23186;&#20307;&#23436;&#25972;&#24615;&#30340;&#23041;&#32961;&#65292;&#20197;&#21450;&#28145;&#20266;&#36896;&#26816;&#27979;&#25216;&#26415;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26234;&#33021;&#25163;&#26426;&#26222;&#21450;&#65292;&#24182;&#25317;&#26377;&#39640;&#36136;&#37327;&#30340;&#25968;&#30721;&#30456;&#26426;&#20197;&#21450;&#26131;&#20110;&#20351;&#29992;&#30340;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#26469;&#24405;&#21046;&#65292;&#32534;&#36753;&#21644;&#20849;&#20139;&#35270;&#39057;&#21644;&#22270;&#20687;&#65292;&#20877;&#21152;&#19978;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#24179;&#21488;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#8220;&#20266;&#36896;&#8221;&#29616;&#35937;&#12290;&#28145;&#24230;&#20266;&#36896;&#31639;&#27861;&#21487;&#20197;&#21019;&#24314;&#20960;&#20046;&#26080;&#27861;&#19982;&#30495;&#23454;&#22270;&#20687;&#21306;&#20998;&#30340;&#20266;&#36896;&#22270;&#20687;&#21644;&#35270;&#39057;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#21644;&#35780;&#20272;&#25968;&#23383;&#35270;&#35273;&#23186;&#20307;&#23436;&#25972;&#24615;&#30340;&#25216;&#26415;&#38750;&#24120;&#37325;&#35201;&#12290;&#28145;&#20266;&#36896;&#65292;&#20063;&#34987;&#31216;&#20026;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20266;&#36896;&#35270;&#39057;&#65292;&#30001;&#20110;&#20854;&#21487;&#20197;&#20197;&#20960;&#20046;&#26080;&#27861;&#19982;&#21407;&#22987;&#22270;&#20687;&#21644;&#35270;&#39057;&#21306;&#20998;&#30340;&#26041;&#24335;&#25805;&#25511;&#21644;&#20462;&#25913;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#36817;&#24180;&#26469;&#24050;&#25104;&#20026;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#36825;&#20123;&#28145;&#20266;&#36896;&#35270;&#39057;&#21487;&#29992;&#20110;&#24694;&#24847;&#29992;&#36884;&#65292;&#20363;&#22914;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#65292;&#20882;&#20805;&#20010;&#20154;&#36523;&#20221;&#21644;&#21046;&#36896;&#20551;&#26032;&#38395;&#12290;&#28145;&#20266;&#36896;&#26816;&#27979;&#25216;&#26415;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#65292;&#20363;&#22914;&#38754;&#37096;&#35782;&#21035;&#65292;&#21160;&#20316;&#20998;&#26512;&#21644;&#38899;&#35270;&#39057;&#21516;&#27493;&#26469;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Due to the widespread use of smartphones with high-quality digital cameras and easy access to a wide range of software apps for recording, editing, and sharing videos and images, as well as the deep learning AI platforms, a new phenomenon of 'faking' videos has emerged. Deepfake algorithms can create fake images and videos that are virtually indistinguishable from authentic ones. Therefore, technologies that can detect and assess the integrity of digital visual media are crucial. Deepfakes, also known as deep learning-based fake videos, have become a major concern in recent years due to their ability to manipulate and alter images and videos in a way that is virtually indistinguishable from the original. These deepfake videos can be used for malicious purposes such as spreading misinformation, impersonating individuals, and creating fake news. Deepfake detection technologies use various approaches such as facial recognition, motion analysis, and audio-visual synchronization to identify
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#65292;&#37319;&#29992;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.03251</link><description>&lt;p&gt;
&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning. (arXiv:2309.03251v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#65292;&#37319;&#29992;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#26159;&#20256;&#32479;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#30340;&#25193;&#23637;&#65292;&#34701;&#20837;&#20102;&#26102;&#38388;&#32500;&#24230;&#12290;&#22312;TKGs&#19978;&#36827;&#34892;&#25512;&#29702;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20107;&#20214;&#39044;&#27979;&#26410;&#26469;&#20107;&#23454;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#25581;&#31034;&#21382;&#21490;&#23376;&#22270;&#21644;&#26102;&#38388;&#27169;&#24335;&#20013;&#30340;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#38752;&#23454;&#20307;&#24314;&#27169;&#26469;&#27169;&#25311;TKGs&#65292;&#22240;&#20026;&#22270;&#20013;&#30340;&#33410;&#28857;&#22312;&#30693;&#35782;&#34920;&#31034;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#22330;&#26223;&#36890;&#24120;&#28041;&#21450;&#22823;&#37327;&#23454;&#20307;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20250;&#20986;&#29616;&#26032;&#23454;&#20307;&#12290;&#36825;&#20351;&#24471;&#20381;&#36182;&#20110;&#23454;&#20307;&#30340;&#26041;&#27861;&#24456;&#38590;&#24212;&#23545;&#22823;&#37327;&#23454;&#20307;&#65292;&#24182;&#19988;&#26377;&#25928;&#22788;&#29702;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#20063;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#65292;&#23427;&#20197;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#23545;&#21382;&#21490;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TiPNN&#37319;&#29992;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#65292;&#21517;&#20026;&#21382;&#21490;&#26102;&#38388;&#22270;&#65292;&#26469;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph (KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial task that aims to predict future facts based on historical occurrences. The key challenge lies in uncovering structural dependencies within historical subgraphs and temporal patterns. Most existing approaches model TKGs relying on entity modeling, as nodes in the graph play a crucial role in knowledge representation. However, the real-world scenario often involves an extensive number of entities, with new entities emerging over time. This makes it challenging for entity-dependent methods to cope with extensive volumes of entities, and effectively handling newly emerging entities also becomes a significant challenge. Therefore, we propose Temporal Inductive Path Neural Network (TiPNN), which models historical information in an entity-independent perspective. Specifically, TiPNN adopts a unified graph, namely history temporal graph, to
&lt;/p&gt;</description></item><item><title>AutoBA&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;AI&#20195;&#29702;&#31243;&#24207;&#65292;&#36890;&#36807;&#26368;&#23569;&#30340;&#29992;&#25143;&#36755;&#20837;&#31616;&#21270;&#29983;&#29289;&#20449;&#24687;&#23398;&#20998;&#26512;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#36880;&#27493;&#35745;&#21010;&#12290;&#32463;&#36807;&#39564;&#35777;&#65292;AutoBA&#22312;&#21508;&#31181;&#32452;&#23398;&#20998;&#26512;&#26696;&#20363;&#20013;&#34920;&#29616;&#20986;&#20581;&#22766;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2309.03242</link><description>&lt;p&gt;
&#36890;&#36807;AutoBA&#23454;&#29616;&#33258;&#21160;&#21270;&#29983;&#29289;&#20449;&#24687;&#23398;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Automated Bioinformatics Analysis via AutoBA. (arXiv:2309.03242v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03242
&lt;/p&gt;
&lt;p&gt;
AutoBA&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;AI&#20195;&#29702;&#31243;&#24207;&#65292;&#36890;&#36807;&#26368;&#23569;&#30340;&#29992;&#25143;&#36755;&#20837;&#31616;&#21270;&#29983;&#29289;&#20449;&#24687;&#23398;&#20998;&#26512;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#36880;&#27493;&#35745;&#21010;&#12290;&#32463;&#36807;&#39564;&#35777;&#65292;AutoBA&#22312;&#21508;&#31181;&#32452;&#23398;&#20998;&#26512;&#26696;&#20363;&#20013;&#34920;&#29616;&#20986;&#20581;&#22766;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32452;&#23398;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#28436;&#21464;&#65292;&#23545;&#22788;&#29702;&#20998;&#26512;&#30340;&#31616;&#21270;&#21644;&#36866;&#24212;&#24615;&#24037;&#20855;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#20026;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Auto Bioinformatics Analysis (AutoBA)&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;AI&#20195;&#29702;&#31243;&#24207;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20256;&#32479;&#32452;&#23398;&#25968;&#25454;&#20998;&#26512;&#12290;AutoBA&#36890;&#36807;&#38656;&#35201;&#26368;&#23569;&#30340;&#29992;&#25143;&#36755;&#20837;&#26469;&#31616;&#21270;&#20998;&#26512;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#36880;&#27493;&#35745;&#21010;&#65292;&#29992;&#20110;&#23436;&#25104;&#21508;&#31181;&#29983;&#29289;&#20449;&#24687;&#23398;&#20219;&#21153;&#12290;&#32463;&#36807;&#19987;&#23478;&#29983;&#29289;&#20449;&#24687;&#23398;&#23478;&#30340;&#20005;&#26684;&#39564;&#35777;&#65292;AutoBA&#30340;&#20581;&#22766;&#24615;&#21644;&#36866;&#24212;&#24615;&#22312;&#21508;&#31181;&#32452;&#23398;&#20998;&#26512;&#26696;&#20363;&#20013;&#24471;&#21040;&#35777;&#23454;&#65292;&#21253;&#25324;&#20840;&#22522;&#22240;&#32452;&#27979;&#24207;&#65288;WGS&#65289;&#65292;RNA&#27979;&#24207;&#65288;RNA-seq&#65289;&#65292;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#65292;ChIP-seq&#21644;&#31354;&#38388;&#36716;&#24405;&#32452;&#23398;&#12290;AutoBA&#30340;&#29420;&#29305;&#33021;&#21147;&#26159;&#26681;&#25454;&#36755;&#20837;&#25968;&#25454;&#30340;&#21464;&#21270;&#33258;&#35774;&#35745;&#20998;&#26512;&#27969;&#31243;&#65292;&#36827;&#19968;&#27493;&#20984;&#26174;&#20102;&#20854;&#22810;&#21151;&#33021;&#24615;&#12290;&#19982;&#22312;&#32447;&#29983;&#29289;&#20449;&#24687;&#23398;&#26381;&#21153;&#30456;&#27604;&#65292;AutoBA&#22312;&#26412;&#22320;&#37096;&#32626;&#20998;&#26512;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the fast-growing and evolving omics data, the demand for streamlined and adaptable tools to handle the analysis continues to grow. In response to this need, we introduce Auto Bioinformatics Analysis (AutoBA), an autonomous AI agent based on a large language model designed explicitly for conventional omics data analysis. AutoBA simplifies the analytical process by requiring minimal user input while delivering detailed step-by-step plans for various bioinformatics tasks. Through rigorous validation by expert bioinformaticians, AutoBA's robustness and adaptability are affirmed across a diverse range of omics analysis cases, including whole genome sequencing (WGS), RNA sequencing (RNA-seq), single-cell RNA-seq, ChIP-seq, and spatial transcriptomics. AutoBA's unique capacity to self-design analysis processes based on input data variations further underscores its versatility. Compared with online bioinformatic services, AutoBA deploys the analysis locally, preserving data privacy. Moreo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20805;&#20998;&#35757;&#32451;&#65292;&#19968;&#20010;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20197;&#20960;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-4&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#36890;&#36807;&#22312;&#38468;&#21152;&#30340;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#19982;GPT-4&#22312;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#19978;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03241</link><description>&lt;p&gt;
GPT&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20805;&#20998;&#35757;&#32451;&#65292;&#19968;&#20010;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20197;&#20960;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-4&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#36890;&#36807;&#22312;&#38468;&#21152;&#30340;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#19982;GPT-4&#22312;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#19978;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#25191;&#34892;&#31639;&#26415;&#36816;&#31639;&#65292;&#29305;&#21035;&#26159;&#36229;&#36807;8&#20301;&#25968;&#23383;&#30340;&#20056;&#27861;&#65292;&#20197;&#21450;&#28041;&#21450;&#23567;&#25968;&#21644;&#20998;&#25968;&#30340;&#36816;&#31639;&#12290;&#26412;&#25991;&#26088;&#22312;&#25361;&#25112;&#36825;&#31181;&#35823;&#35299;&#12290;&#36890;&#36807;&#20805;&#20998;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#19968;&#20010;&#25317;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20197;&#36817;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#32780;&#19988;&#27809;&#26377;&#25968;&#25454;&#27844;&#38706;&#65292;&#26174;&#33879;&#36229;&#36807;&#20102;GPT-4&#65288;&#20854;&#22810;&#20301;&#25968;&#20056;&#27861;&#20934;&#30830;&#29575;&#20165;&#20026;4.3%&#65289;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;MathGLM&#65292;&#23427;&#26159;&#36890;&#36807;&#22312;&#21253;&#21547;&#20102;&#25991;&#26412;&#25551;&#36848;&#30340;&#38468;&#21152;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#20174;GLM-10B&#24494;&#35843;&#32780;&#25104;&#30340;&#65292;&#23427;&#22312;&#19968;&#20010;&#21253;&#21547;5000&#20010;&#26679;&#26412;&#30340;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#27979;&#35797;&#38598;&#19978;&#30340;&#34920;&#29616;&#19982;GPT-4&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of &gt;8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;POI&#32423;&#21035;&#20154;&#32676;&#27969;&#25512;&#26029;&#30340;&#26102;&#31354;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23646;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#20197;&#35299;&#20915;&#25968;&#25454;&#26631;&#35760;&#19981;&#36275;&#12289;POI&#38388;&#26102;&#31354;&#20381;&#36182;&#24615;&#22797;&#26434;&#21644;&#20154;&#32676;&#27969;&#37327;&#19982;GPS&#25253;&#21578;&#20043;&#38388;&#30456;&#20851;&#24615;&#22810;&#26679;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03239</link><description>&lt;p&gt;
POI&#32423;&#21035;&#20154;&#32676;&#27969;&#25512;&#26029;&#30340;&#26102;&#31354;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference. (arXiv:2309.03239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;POI&#32423;&#21035;&#20154;&#32676;&#27969;&#25512;&#26029;&#30340;&#26102;&#31354;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23646;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#20197;&#35299;&#20915;&#25968;&#25454;&#26631;&#35760;&#19981;&#36275;&#12289;POI&#38388;&#26102;&#31354;&#20381;&#36182;&#24615;&#22797;&#26434;&#21644;&#20154;&#32676;&#27969;&#37327;&#19982;GPS&#25253;&#21578;&#20043;&#38388;&#30456;&#20851;&#24615;&#22810;&#26679;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#33719;&#21462;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#30340;&#20154;&#32676;&#27969;&#37327;&#23545;&#20110;&#26377;&#25928;&#30340;&#20132;&#36890;&#31649;&#29702;&#12289;&#20844;&#20849;&#26381;&#21153;&#21644;&#22478;&#24066;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22914;&#27492;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#22478;&#24066;&#24863;&#30693;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#22823;&#22810;&#25968;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#36136;&#37327;&#19981;&#36275;&#20197;&#30417;&#27979;&#27599;&#20010;POI&#30340;&#20154;&#32676;&#27969;&#21160;&#12290;&#36825;&#20351;&#24471;&#20174;&#20302;&#36136;&#37327;&#25968;&#25454;&#20013;&#25512;&#26029;&#20934;&#30830;&#30340;&#20154;&#32676;&#27969;&#37327;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#19968;&#22797;&#26434;&#24615;&#20027;&#35201;&#30001;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#24341;&#36215;&#65306;1&#65289;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#32597;&#35265;&#24615;&#65307;2&#65289;POI&#20043;&#38388;&#22797;&#26434;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65307;3&#65289;&#31934;&#30830;&#20154;&#32676;&#27969;&#37327;&#19982;GPS&#25253;&#21578;&#20043;&#38388;&#30340;&#20247;&#22810;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#20154;&#32676;&#27969;&#25512;&#26029;&#38382;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#33258;&#30417;&#30563;&#23646;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#25968;&#25454;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65288;model&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#26500;&#24314;&#19968;&#20010;&#31354;&#38388;&#22270;&#24320;&#22987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal for effective traffic management, public service, and urban planning. Despite this importance, due to the limitations of urban sensing techniques, the data quality from most sources is inadequate for monitoring crowd flow at each POI. This renders the inference of accurate crowd flow from low-quality data a critical and challenging task. The complexity is heightened by three key factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad correlations between precise crowd flow and GPS reports}.  To address these challenges, we recast the crowd flow inference problem as a self-supervised attributed graph representation learning task and introduce a novel \underline{C}ontrastive \underline{S}elf-learning framework for \underline{S}patio-\underline{T}emporal data (\model). Our approach initiates with the construction of a spati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;&#33258;&#28982;&#31034;&#20363;&#20026;&#22522;&#30784;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#31034;&#20363;&#20316;&#20026;&#35299;&#37322;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#19982;&#20154;&#31867;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#36807;&#31243;&#30456;&#31526;&#65292;&#20351;&#35299;&#37322;&#26356;&#33258;&#28982;&#21644;&#26131;&#25026;&#12290;</title><link>http://arxiv.org/abs/2309.03234</link><description>&lt;p&gt;
&#33258;&#28982;&#31034;&#20363;&#20026;&#22522;&#30784;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Natural Example-Based Explainability: a Survey. (arXiv:2309.03234v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#33258;&#28982;&#31034;&#20363;&#20026;&#22522;&#30784;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#31034;&#20363;&#20316;&#20026;&#35299;&#37322;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#19982;&#20154;&#31867;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#36807;&#31243;&#30456;&#31526;&#65292;&#20351;&#35299;&#37322;&#26356;&#33258;&#28982;&#21644;&#26131;&#25026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#22312;XAI&#39046;&#22495;&#65292;&#31361;&#20986;&#22270;&#24050;&#32463;&#25104;&#20026;&#20027;&#35282;&#22810;&#24180;&#65292;&#20294;&#20854;&#21453;&#26144;&#27169;&#22411;&#20869;&#37096;&#36807;&#31243;&#30340;&#33021;&#21147;&#21463;&#21040;&#20102;&#36136;&#30097;&#12290;&#23613;&#31649;&#19981;&#22826;&#21463;&#20851;&#27880;&#65292;&#20294;&#22522;&#20110;&#31034;&#20363;&#30340;XAI&#26041;&#27861;&#20173;&#22312;&#19981;&#26029;&#25913;&#36827;&#12290;&#23427;&#21253;&#25324;&#20351;&#29992;&#31034;&#20363;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#36825;&#31526;&#21512;&#20154;&#31867;&#25512;&#29702;&#30340;&#24515;&#29702;&#26426;&#21046;&#65292;&#20351;&#22522;&#20110;&#31034;&#20363;&#30340;&#35299;&#37322;&#23545;&#29992;&#25143;&#26469;&#35828;&#33258;&#28982;&#21644;&#30452;&#35266;&#26131;&#25026;&#12290;&#20107;&#23454;&#19978;&#65292;&#20154;&#31867;&#36890;&#36807;&#22522;&#20110;&#31034;&#20363;&#24418;&#25104;&#27010;&#24565;&#30340;&#24515;&#29702;&#34920;&#31034;&#26469;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#33258;&#28982;&#31034;&#20363;&#20026;&#22522;&#30784;&#30340;XAI&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25551;&#36848;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;&#25152;&#35859;&#8220;&#33258;&#28982;&#8221;&#31034;&#20363;&#25351;&#30340;&#26159;&#30452;&#25509;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#32472;&#21046;&#32780;&#26469;&#65292;&#32780;&#19981;&#28041;&#21450;&#20219;&#20309;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) has become increasingly significant for improving the interpretability and trustworthiness of machine learning models. While saliency maps have stolen the show for the last few years in the XAI field, their ability to reflect models' internal processes has been questioned. Although less in the spotlight, example-based XAI methods have continued to improve. It encompasses methods that use examples as explanations for a machine learning model's predictions. This aligns with the psychological mechanisms of human reasoning and makes example-based explanations natural and intuitive for users to understand. Indeed, humans learn and reason by forming mental representations of concepts based on examples.  This paper provides an overview of the state-of-the-art in natural example-based XAI, describing the pros and cons of each approach. A "natural" example simply means that it is directly drawn from the training data without involving any generative pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#19982;&#30417;&#25511;&#39046;&#22495;&#30340;RentinaNet&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#24320;&#21457;&#20102;&#31216;&#20026;Quantum-RetinaNet&#30340;&#26234;&#33021;&#30417;&#25511;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.03231</link><description>&lt;p&gt;
&#37327;&#23376;AI&#22686;&#24378;&#26234;&#33021;&#30417;&#25511;&#65306;&#36890;&#36807;&#21019;&#26032;&#30340;&#36829;&#31105;&#21697;&#26816;&#27979;&#25552;&#21319;&#20844;&#20849;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety Through Innovative Contraband Detection. (arXiv:2309.03231v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#19982;&#30417;&#25511;&#39046;&#22495;&#30340;RentinaNet&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#24320;&#21457;&#20102;&#31216;&#20026;Quantum-RetinaNet&#30340;&#26234;&#33021;&#30417;&#25511;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#25511;&#31995;&#32479;&#22312;&#32500;&#25252;&#29616;&#20195;&#31038;&#20250;&#30340;&#21644;&#24179;&#19982;&#23433;&#20840;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#20204;&#30340;&#26222;&#21450;&#24615;&#26377;&#21161;&#20110;&#26377;&#25928;&#30417;&#25511;&#21487;&#30097;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#22312;&#20154;&#21475;&#23494;&#38598;&#30340;&#29615;&#22659;&#20013;&#65292;&#25345;&#32493;&#20027;&#21160;&#30417;&#25511;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#65292;&#24517;&#39035;&#24320;&#21457;&#26234;&#33021;&#30417;&#25511;&#31995;&#32479;&#12290;AI&#22312;&#30417;&#25511;&#39046;&#22495;&#30340;&#25972;&#21512;&#26159;&#19968;&#27425;&#37325;&#22823;&#38761;&#21629;&#65292;&#28982;&#32780;&#36895;&#24230;&#38382;&#39064;&#38459;&#30861;&#20102;&#20854;&#22312;&#35813;&#39046;&#22495;&#30340;&#24191;&#27867;&#23454;&#26045;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#22522;&#20110;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#30340;&#30417;&#25511;&#31995;&#32479;&#19981;&#20165;&#26356;&#20934;&#30830;&#65292;&#32780;&#19988;&#33021;&#22815;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#26159;&#20197;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#12290;&#26412;&#30740;&#31350;&#23558;RentinaNet&#27169;&#22411;&#19982;&#37327;&#23376;CNN&#38598;&#25104;&#65292;&#31216;&#20026;Quantum-RetinaNet&#12290;&#36890;&#36807;&#21033;&#29992;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#37327;&#23376;&#33021;&#21147;&#65292;Quantum-RetinaNet&#22312;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surveillance systems have emerged as crucial elements in upholding peace and security in the modern world. Their ubiquity aids in monitoring suspicious activities effectively. However, in densely populated environments, continuous active monitoring becomes impractical, necessitating the development of intelligent surveillance systems. AI integration in the surveillance domain was a big revolution, however, speed issues have prevented its widespread implementation in the field. It has been observed that quantum artificial intelligence has led to a great breakthrough. Quantum artificial intelligence-based surveillance systems have shown to be more accurate as well as capable of performing well in real-time scenarios, which had never been seen before. In this research, a RentinaNet model is integrated with Quantum CNN and termed as Quantum-RetinaNet. By harnessing the Quantum capabilities of QCNN, Quantum-RetinaNet strikes a balance between accuracy and speed. This innovative integration 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#20363;&#31354;&#38388;&#20998;&#26512;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#38382;&#39064;&#30340;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#36873;&#25321;&#31995;&#32479;&#65292;&#39044;&#27979;&#20102;&#22312;&#32473;&#23450;&#29305;&#24449;&#19979;&#26368;&#36866;&#21512;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#28145;&#20837;&#20102;&#35299;&#20102;&#31639;&#27861;&#24615;&#33021;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.03229</link><description>&lt;p&gt;
&#22914;&#20309;&#36873;&#25321;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#20013;&#30340;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Which algorithm to select in sports timetabling?. (arXiv:2309.03229v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#20363;&#31354;&#38388;&#20998;&#26512;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#38382;&#39064;&#30340;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#36873;&#25321;&#31995;&#32479;&#65292;&#39044;&#27979;&#20102;&#22312;&#32473;&#23450;&#29305;&#24449;&#19979;&#26368;&#36866;&#21512;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#28145;&#20837;&#20102;&#35299;&#20102;&#31639;&#27861;&#24615;&#33021;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#20307;&#32946;&#31454;&#36187;&#37117;&#38656;&#35201;&#19968;&#20010;&#36187;&#31243;&#23433;&#25490;&#65292;&#30830;&#23450;&#27604;&#36187;&#38431;&#20237;&#20309;&#26102;&#20309;&#22320;&#30456;&#36935;&#12290;&#26368;&#36817;&#30340;&#22269;&#38469;&#36187;&#31243;&#23433;&#25490;&#31454;&#36187;(ITC2021)&#25581;&#31034;&#20102;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#34429;&#28982;&#21487;&#33021;&#24320;&#21457;&#20986;&#36890;&#29992;&#31639;&#27861;&#65292;&#20294;&#27599;&#20010;&#31639;&#27861;&#22312;&#38382;&#39064;&#23454;&#20363;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#24456;&#22823;&#12290;&#26412;&#25991;&#22312;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#26041;&#38754;&#25552;&#20379;&#20102;&#23454;&#20363;&#31354;&#38388;&#20998;&#26512;&#65292;&#20174;&#32780;&#28145;&#20837;&#20102;&#35299;&#20102;&#20843;&#31181;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#36873;&#25321;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#38382;&#39064;&#23454;&#20363;&#30340;&#29305;&#24449;&#39044;&#27979;&#21738;&#31181;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#21487;&#33021;&#34920;&#29616;&#26368;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#21738;&#20123;&#29305;&#24449;&#22312;&#20570;&#20986;&#39044;&#27979;&#26102;&#24456;&#37325;&#35201;&#65292;&#20174;&#32780;&#28145;&#20837;&#20102;&#35299;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#24314;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#23454;&#20363;&#30340;&#32463;&#39564;&#38590;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22522;&#20110;&#22823;&#35268;&#27169;&#35745;&#31639;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Any sports competition needs a timetable, specifying when and where teams meet each other. The recent International Timetabling Competition (ITC2021) on sports timetabling showed that, although it is possible to develop general algorithms, the performance of each algorithm varies considerably over the problem instances. This paper provides an instance space analysis for sports timetabling, resulting in powerful insights into the strengths and weaknesses of eight state-of-the-art algorithms. Based on machine learning techniques, we propose an algorithm selection system that predicts which algorithm is likely to perform best when given the characteristics of a sports timetabling problem instance. Furthermore, we identify which characteristics are important in making that prediction, providing insights in the performance of the algorithms, and suggestions to further improve them. Finally, we assess the empirical hardness of the instances. Our results are based on large computational exper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#36890;&#36807;&#26500;&#24314;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#21487;&#20197;&#32508;&#21512;&#20998;&#26512;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#20026;&#33647;&#29289;&#20877;&#23450;&#20301;&#30740;&#31350;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2309.03227</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#25581;&#31034;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#30340;&#25216;&#26415;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates. (arXiv:2309.03227v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#36890;&#36807;&#26500;&#24314;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#21487;&#20197;&#32508;&#21512;&#20998;&#26512;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#20026;&#33647;&#29289;&#20877;&#23450;&#20301;&#30740;&#31350;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#20877;&#23450;&#20301;&#26159;&#19968;&#31181;&#21457;&#29616;&#29616;&#26377;&#33647;&#29289;&#26032;&#27835;&#30103;&#29992;&#36884;&#30340;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#36817;&#24180;&#26469;&#22312;&#35745;&#31639;&#31185;&#23398;&#25991;&#29486;&#20013;&#20351;&#29992;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#30340;&#25216;&#26415;&#28508;&#21147;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32508;&#21512;&#20998;&#26512;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#31561;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;s-BKG&#65289;&#65292;&#21253;&#25324;&#26469;&#33258;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30340;&#33647;&#29289;&#12289;&#30142;&#30149;&#21644;&#22522;&#22240;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35782;&#21035;&#22312;s-BKG&#20013;&#19982;&#30446;&#26631;&#30142;&#30149;&#20851;&#32852;&#26377;&#38480;&#20294;&#22312;&#31354;&#38388;&#19978;&#32039;&#23494;&#30456;&#37051;&#30340;&#33647;&#29289;&#20316;&#20026;&#28508;&#22312;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#33647;&#29289;&#19987;&#21033;&#20449;&#24687;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;p-BKG&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug repositioning-a promising strategy for discovering new therapeutic uses for existing drugs-has been increasingly explored in the computational science literature using biomedical databases. However, the technological potential of drug repositioning candidates has often been overlooked. This study presents a novel protocol to comprehensively analyse various sources such as pharmaceutical patents and biomedical databases, and identify drug repositioning candidates with both technological potential and scientific evidence. To this end, first, we constructed a scientific biomedical knowledge graph (s-BKG) comprising relationships between drugs, diseases, and genes derived from biomedical databases. Our protocol involves identifying drugs that exhibit limited association with the target disease but are closely located in the s-BKG, as potential drug candidates. We constructed a patent-informed biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information. Finally, we d
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#23616;&#23454;&#38469;&#25490;&#21517;&#26041;&#27861;&#26469;&#20998;&#25674;&#23454;&#38469;&#31243;&#24207;&#21512;&#25104;&#20013;&#35745;&#31639;&#36127;&#25285;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20351;&#29992;&#21333;&#20010;&#28436;&#31034;&#30340;&#23454;&#38469;&#21512;&#25104;&#22120;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#23637;&#31034;&#20102;&#20840;&#23616;&#25490;&#21517;&#26377;&#25928;&#36817;&#20284;&#20102;&#20840;&#20307;&#21512;&#29702;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.03225</link><description>&lt;p&gt;
&#20351;&#29992;&#25490;&#21517;&#26469;&#25674;&#38144;&#23454;&#38469;&#31243;&#24207;&#21512;&#25104;&#30340;&#25104;&#26412;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Amortizing Pragmatic Program Synthesis with Rankings. (arXiv:2309.03225v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03225
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#23616;&#23454;&#38469;&#25490;&#21517;&#26041;&#27861;&#26469;&#20998;&#25674;&#23454;&#38469;&#31243;&#24207;&#21512;&#25104;&#20013;&#35745;&#31639;&#36127;&#25285;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20351;&#29992;&#21333;&#20010;&#28436;&#31034;&#30340;&#23454;&#38469;&#21512;&#25104;&#22120;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#23637;&#31034;&#20102;&#20840;&#23616;&#25490;&#21517;&#26377;&#25928;&#36817;&#20284;&#20102;&#20840;&#20307;&#21512;&#29702;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31243;&#24207;&#21512;&#25104;&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#31995;&#32479;&#25509;&#25910;&#19968;&#32452;&#29992;&#25143;&#29983;&#25104;&#30340;&#31034;&#20363;&#65292;&#24182;&#36820;&#22238;&#19968;&#20010;&#36923;&#36753;&#19968;&#33268;&#30340;&#31243;&#24207;&#12290;&#20351;&#29992;&#21512;&#29702;&#28436;&#35828;&#34892;&#20026;&#65288;RSA&#65289;&#26694;&#26550;&#22312;&#26500;&#24314;&#8220;&#23454;&#38469;&#8221;&#31243;&#24207;&#21512;&#25104;&#22120;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#36825;&#20123;&#21512;&#25104;&#22120;&#36820;&#22238;&#30340;&#31243;&#24207;&#19981;&#20165;&#22312;&#36923;&#36753;&#19978;&#19968;&#33268;&#65292;&#32780;&#19988;&#36824;&#32771;&#34385;&#20102;&#29992;&#25143;&#22914;&#20309;&#36873;&#25321;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#36816;&#34892;RSA&#31639;&#27861;&#30340;&#35745;&#31639;&#36127;&#25285;&#38480;&#21046;&#20102;&#23454;&#38469;&#31243;&#24207;&#21512;&#25104;&#22312;&#21487;&#33021;&#31243;&#24207;&#20010;&#25968;&#36739;&#23569;&#30340;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#8220;&#20840;&#23616;&#23454;&#38469;&#25490;&#21517;&#8221; - &#19968;&#20010;&#21333;&#19968;&#30340;&#12289;&#24635;&#30340;&#25490;&#21015;&#25152;&#26377;&#20551;&#35774;&#30340;&#26041;&#27861;&#26469;&#20998;&#25674;RSA&#31639;&#27861;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#21333;&#20010;&#28436;&#31034;&#30340;&#23454;&#38469;&#21512;&#25104;&#22120;&#20013;&#65292;&#25105;&#20204;&#30340;&#20840;&#23616;&#25490;&#21517;&#26041;&#27861;&#23436;&#20840;&#22797;&#21046;&#20102;RSA&#30340;&#25490;&#24207;&#21709;&#24212;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#26174;&#31034;&#65292;&#20840;&#23616;&#25490;&#21517;&#26377;&#25928;&#22320;&#36817;&#20284;&#20102;&#20840;&#20307;&#21512;&#29702;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
In program synthesis, an intelligent system takes in a set of user-generated examples and returns a program that is logically consistent with these examples. The usage of Rational Speech Acts (RSA) framework has been successful in building \emph{pragmatic} program synthesizers that return programs which -in addition to being logically consistent -- account for the fact that a user chooses their examples informatively. However, the computational burden of running the RSA algorithm has restricted the application of pragmatic program synthesis to domains with a small number of possible programs. This work presents a novel method of amortizing the RSA algorithm by leveraging a \emph{global pragmatic ranking} -- a single, total ordering of all the hypotheses. We prove that for a pragmatic synthesizer that uses a single demonstration, our global ranking method exactly replicates RSA's ranked responses. We further empirically show that global rankings effectively approximate the full pragma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#26469;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#19981;&#36275;&#21644;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#27169;&#22411;&#21644;&#24341;&#20837;&#36335;&#24452;&#39564;&#35777;&#22120;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#30340;&#25628;&#32034;&#21644;&#25512;&#29702;&#36335;&#24452;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.03224</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#20381;&#28982;&#33021;&#33719;&#30410;&#65306;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. (arXiv:2309.03224v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03224
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#26469;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#19981;&#36275;&#21644;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#27169;&#22411;&#21644;&#24341;&#20837;&#36335;&#24452;&#39564;&#35777;&#22120;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#30340;&#25628;&#32034;&#21644;&#25512;&#29702;&#36335;&#24452;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#32972;&#26223;&#23398;&#20064;&#33021;&#21147;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36807;&#31243;&#30417;&#30563;&#65292;&#23558;PLMs&#24212;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#36890;&#24120;&#26080;&#27861;&#29983;&#25104;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#21644;&#26368;&#32456;&#31572;&#26696;&#65292;&#21363;&#20351;&#35299;&#20915;&#26041;&#26696;&#27010;&#29575;&#24456;&#39640;&#12290;&#20026;&#20102;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#24494;&#35843;&#30340;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#21644;&#36731;&#37327;&#32423;&#33021;&#37327;&#20989;&#25968;&#20026;LLMs&#36171;&#20104;&#21363;&#26102;&#21453;&#24212;&#21644;&#31934;&#32454;&#25512;&#29702;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#24494;&#35843;&#30340;LLMs&#37325;&#26032;&#23450;&#20041;&#20026;&#22522;&#20110;&#27531;&#24046;&#30340;&#33021;&#37327;&#27169;&#22411;&#65288;Residual-EBM&#65289;&#65292;&#24182;&#24212;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#26469;&#20272;&#35745;&#33021;&#37327;&#20989;&#25968;&#30340;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#33021;&#37327;&#20989;&#25968;&#30340;MCTS&#20316;&#20026;&#36335;&#24452;&#39564;&#35777;&#22120;&#26469;&#25628;&#32034;&#36755;&#20986;&#31354;&#38388;&#24182;&#35780;&#20272;&#25512;&#29702;&#36335;&#24452;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit impressive language understanding and in-context learning abilities including natural language processing (NLP) tasks and challenging mathematical reasoning. However, due to the lack of process-supervision, applying PLMs to mathematical reasoning tasks often fail to generate correct reasoning steps and final answer even though solutions have high probabilities. To unleash the mathematical reasoning of finetuned-LLMs without any further fineutuning steps, we propose a method to endow LLMs with immediate reaction and delicate reasoning system via Monte Carlo Tree Search(MCTS) and a light energy function to rank the decision steps. In particular, We first re-formalize the finetuned-LLMs to a Residual-based Energy Model~(Residual-EBM) and apply noise contrastive estimation to estimate the parameters of energy function . Then we use MCTS with energy function as path verifier to search the output space and evaluating the reasoning path. Through extensive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24378;&#35843;&#20102;Evidence Theory&#22312;&#31038;&#20250;&#19982;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#34920;&#36798;&#26469;&#33258;&#26410;&#30830;&#23450;&#20107;&#20214;&#21487;&#33021;&#23454;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19982;&#20043;&#23545;&#27604;&#30340;Probability Theory&#21482;&#33021;&#38480;&#20110;&#20915;&#31574;&#32773;&#24403;&#21069;&#35774;&#24819;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;Evidence Theory&#19982;Probability Theory&#30340;&#20851;&#31995;&#20197;&#21450;Evidence Theory&#22312;&#20449;&#24687;&#35770;&#20013;&#30340;&#24212;&#29992;&#22686;&#24378;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23457;&#35745;&#32451;&#20064;&#26696;&#20363;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;Evidence Theory&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.03222</link><description>&lt;p&gt;
Sherlock Holmes&#19981;&#29609;&#39600;&#23376;&#65306;Evidence Theory&#23545;&#31038;&#20250;&#19982;&#29983;&#21629;&#31185;&#23398;&#30340;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
Sherlock Holmes Doesn't Play Dice: The significance of Evidence Theory for the Social and Life Sciences. (arXiv:2309.03222v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;Evidence Theory&#22312;&#31038;&#20250;&#19982;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#34920;&#36798;&#26469;&#33258;&#26410;&#30830;&#23450;&#20107;&#20214;&#21487;&#33021;&#23454;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19982;&#20043;&#23545;&#27604;&#30340;Probability Theory&#21482;&#33021;&#38480;&#20110;&#20915;&#31574;&#32773;&#24403;&#21069;&#35774;&#24819;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;Evidence Theory&#19982;Probability Theory&#30340;&#20851;&#31995;&#20197;&#21450;Evidence Theory&#22312;&#20449;&#24687;&#35770;&#20013;&#30340;&#24212;&#29992;&#22686;&#24378;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#23457;&#35745;&#32451;&#20064;&#26696;&#20363;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;Evidence Theory&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Evidence Theory&#65288;Demster-Shafer Theory&#65292;Belief Functions Theory&#65289;&#22312;&#25968;&#25454;&#34701;&#21512;&#20013;&#24471;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#65292;&#20294;&#20854;&#22312;&#31038;&#20250;&#19982;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#24120;&#24120;&#34987;&#20154;&#20204;&#23545;&#20854;&#29420;&#29305;&#29305;&#28857;&#30340;&#32570;&#20047;&#35748;&#35782;&#25152;&#25513;&#30422;&#12290;&#26412;&#25991;&#24378;&#35843;&#65292;Evidence Theory&#21487;&#20197;&#34920;&#36798;&#26469;&#33258;&#20110;&#20154;&#20204;&#26410;&#33021;&#30830;&#23450;&#30340;&#20107;&#20214;&#21487;&#33021;&#23454;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;Probability Theory&#24517;&#39035;&#20165;&#38480;&#20110;&#20915;&#31574;&#32773;&#24403;&#21069;&#25152;&#35774;&#24819;&#30340;&#21487;&#33021;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;Dempster-Shafer&#30340;&#32452;&#21512;&#35268;&#21017;&#22914;&#20309;&#19982;&#21508;&#31181;&#29256;&#26412;&#30340;Probability Theory&#30340;&#36125;&#21494;&#26031;&#23450;&#29702;&#30456;&#20851;&#65292;&#24182;&#35752;&#35770;&#20102;&#21738;&#20123;&#20449;&#24687;&#35770;&#30340;&#24212;&#29992;&#21487;&#20197;&#36890;&#36807;Evidence Theory&#24471;&#21040;&#22686;&#24378;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#38416;&#36848;&#20102;&#20351;&#29992;Evidence Theory&#26469;&#29702;&#35299;&#23457;&#35745;&#32451;&#20064;&#20013;&#20986;&#29616;&#30340;&#37096;&#20998;&#37325;&#21472;&#12289;&#37096;&#20998;&#30456;&#20114;&#30683;&#30462;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Evidence Theory (Demster-Shafer Theory, Belief Functions Theory) is being increasingly used in data fusion, its potentialities in the Social and Life Sciences are often obscured by lack of awareness of its distinctive features. With this paper we stress that Evidence Theory can express the uncertainty deriving from the fear that events may materialize, that one has not been able to figure out. By contrast, Probability Theory must limit itself to the possibilities that a decision-maker is currently envisaging.  Subsequently, we illustrate how Dempster-Shafer's combination rule relates to Bayes' Theorem for various versions of Probability Theory and discuss which applications of Information Theory can be enhanced by Evidence Theory. Finally, we illustrate our claims with an example where Evidence Theory is used to make sense of the partially overlapping, partially contradictory solutions that appear in an auditing exercise.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#24863;&#30693;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20276;&#20387;&#21160;&#29289;&#30142;&#30149;&#35786;&#26029;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#34701;&#21512;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22270;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#37325;&#35201;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.03219</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#24863;&#30693;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#30340;&#20276;&#20387;&#21160;&#29289;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Companion Animal Disease Diagnostics based on Literal-aware Medical Knowledge Graph Representation Learning. (arXiv:2309.03219v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03219
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#24863;&#30693;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20276;&#20387;&#21160;&#29289;&#30142;&#30149;&#35786;&#26029;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#34701;&#21512;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22270;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#37325;&#35201;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#34987;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65288;&#22914;&#31508;&#35760;&#21644;&#20861;&#21307;&#35760;&#24405;&#65289;&#26469;&#21463;&#30410;&#20110;&#21160;&#29289;&#30142;&#30149;&#30340;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#29992;&#20110;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#24102;&#26377;&#25991;&#26412;&#20449;&#24687;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#34920;&#31034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#30693;&#35782;&#22270;&#35889;&#26174;&#31034;&#20986;&#24322;&#26500;&#29305;&#24615;&#21644;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#26088;&#22312;&#20445;&#30041;&#22260;&#32469;&#30446;&#26631;&#33410;&#28857;&#30340;&#22270;&#32467;&#26500;&#65292;&#32780;&#26080;&#27861;&#32771;&#34385;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#65292;&#32780;&#36825;&#20123;&#25991;&#26412;&#20063;&#21487;&#33021;&#21253;&#21547;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#25928;&#35786;&#26029;&#21160;&#29289;&#30142;&#30149;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22270;&#32467;&#26500;&#65292;&#24182;&#23558;&#23427;&#20204;&#34701;&#21512;&#20026;&#32479;&#19968;&#30340;&#34920;&#31034;&#65292;&#21363;LiteralKG&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;&#35813;&#22270;&#35889;&#26159;&#20174;&#21508;&#20010;&#21160;&#29289;&#21307;&#38498;&#25910;&#38598;&#30340;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#21450;&#25991;&#26412;&#20449;&#24687;&#26500;&#24314;&#32780;&#26469;&#12290;&#25105;&#20204;&#28982;&#21518;&#34701;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#20197;&#21450;&#25991;&#26412;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) embedding has been used to benefit the diagnosis of animal diseases by analyzing electronic medical records (EMRs), such as notes and veterinary records. However, learning representations to capture entities and relations with literal information in KGs is challenging as the KGs show heterogeneous properties and various types of literal information. Meanwhile, the existing methods mostly aim to preserve graph structures surrounding target nodes without considering different types of literals, which could also carry significant information. In this paper, we propose a knowledge graph embedding model for the efficient diagnosis of animal diseases, which could learn various types of literal information and graph structure and fuse them into unified representations, namely LiteralKG. Specifically, we construct a knowledge graph that is built from EMRs along with literal information collected from various animal hospitals. We then fuse different types of entities and no
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#31895;&#31961;&#38598;&#20013;&#21512;&#26684;&#32858;&#21512;&#30340;&#20195;&#25968;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#20013;&#30340;&#24754;&#35266;&#21644;&#20048;&#35266;&#30340;&#21512;&#24182;&#65292;&#20197;&#21450;&#30740;&#31350;&#25512;&#29702;&#20013;&#30340;&#27495;&#35270;/&#26377;&#23475;&#34892;&#20026;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03217</link><description>&lt;p&gt;
&#27867;&#31895;&#31961;&#38598;&#20013;&#30340;&#21512;&#26684;&#32858;&#21512;&#30340;&#20195;&#25968;&#27169;&#22411;&#21450;&#25512;&#29702;&#20559;&#24046;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Algebraic Models for Qualified Aggregation in General Rough Sets, and Reasoning Bias Discovery. (arXiv:2309.03217v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#31895;&#31961;&#38598;&#20013;&#21512;&#26684;&#32858;&#21512;&#30340;&#20195;&#25968;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#20013;&#30340;&#24754;&#35266;&#21644;&#20048;&#35266;&#30340;&#21512;&#24182;&#65292;&#20197;&#21450;&#30740;&#31350;&#25512;&#29702;&#20013;&#30340;&#27495;&#35270;/&#26377;&#23475;&#34892;&#20026;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27867;&#31895;&#31961;&#38598;&#30340;&#32972;&#26223;&#19979;&#65292;&#23558;&#20004;&#20010;&#20107;&#29289;&#32452;&#21512;&#25104;&#21478;&#19968;&#20010;&#24182;&#38750;&#30452;&#25509;&#12290;&#23545;&#20110;&#28041;&#21450;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#31946;&#24615;&#30340;&#20854;&#20182;&#29702;&#35770;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#31181;&#34892;&#20026;&#21487;&#20197;&#36171;&#20104;&#39069;&#22806;&#30340;&#24847;&#20041;&#65292;&#36229;&#36234;&#20102;&#32467;&#26500;&#19978;&#30340;&#21512;&#21462;&#21644;&#26512;&#21462;&#65292;&#23601;&#20687;$L$&#27169;&#31946;&#38598;&#19978;&#30340;$*$-&#33539;&#25968;&#29702;&#35770;&#21644;&#30456;&#20851;&#25512;&#23548;&#19968;&#26679;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#26126;&#20102;&#22312;&#20855;&#26377;&#36817;&#20284;&#31639;&#23376;&#65288;&#31216;&#20026;&#31895;&#31961;&#20415;&#21033;&#26684;&#65289;&#30340;&#26684;&#19978;&#23558;&#20107;&#29289;&#32452;&#21512;&#30340;&#20195;&#25968;&#27169;&#22411;&#12290;&#30740;&#31350;&#21463;&#21040;&#35201;&#24314;&#27169;&#24576;&#30097;&#35770;&#25110;&#24754;&#35266;&#35770;&#21644;&#20048;&#35266;&#35770;&#21512;&#24182;&#20110;&#20154;&#31867;&#25512;&#29702;&#65292;&#20197;&#21450;&#25805;&#20316;&#36873;&#25321;&#34987;&#35266;&#28857;&#25152;&#32422;&#26463;&#30340;&#21160;&#26426;&#30340;&#24378;&#28872;&#25512;&#21160;&#12290;&#35777;&#26126;&#20102;&#26368;&#23567;&#27169;&#22411;&#25552;&#20379;&#30340;&#24369;&#21542;&#23450;&#21644;&#25512;&#23548;&#30340;&#22522;&#26412;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#25512;&#29702;&#20013;&#30340;&#27495;&#35270;/&#26377;&#23475;&#34892;&#20026;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of general rough sets, the act of combining two things to form another is not straightforward. The situation is similar for other theories that concern uncertainty and vagueness. Such acts can be endowed with additional meaning that go beyond structural conjunction and disjunction as in the theory of $*$-norms and associated implications over $L$-fuzzy sets. In the present research, algebraic models of acts of combining things in generalized rough sets over lattices with approximation operators (called rough convenience lattices) is invented. The investigation is strongly motivated by the desire to model skeptical or pessimistic, and optimistic or possibilistic aggregation in human reasoning, and the choice of operations is constrained by the perspective. Fundamental results on the weak negations and implications afforded by the minimal models are proved. In addition, the model is suitable for the study of discriminatory/toxic behavior in human reasoning, and of ML algor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#21487;&#35299;&#37322;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#20132;&#36890;&#26631;&#24535;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#39640;&#32423;&#29305;&#24449;&#26469;&#40065;&#26834;&#22320;&#26816;&#27979;&#20132;&#36890;&#26631;&#24535;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03215</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#20132;&#36890;&#26631;&#24535;&#26816;&#27979;&#26041;&#27861;&#65306;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Explainable and Trustworthy Traffic Sign Detection for Safe Autonomous Driving: An Inductive Logic Programming Approach. (arXiv:2309.03215v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#21487;&#35299;&#37322;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#20132;&#36890;&#26631;&#24535;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#39640;&#32423;&#29305;&#24449;&#26469;&#40065;&#26834;&#22320;&#26816;&#27979;&#20132;&#36890;&#26631;&#24535;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#26631;&#24535;&#26816;&#27979;&#26159;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25805;&#20316;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#23427;&#30830;&#20445;&#20102;&#25152;&#26377;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#23433;&#20840;&#12290;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#24535;&#20998;&#31867;&#31995;&#32479;&#20381;&#36182;&#20687;&#32032;&#32423;&#29305;&#24449;&#26469;&#26816;&#27979;&#20132;&#36890;&#26631;&#24535;&#65292;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25915;&#20987;&#21253;&#25324;&#23545;&#26631;&#24535;&#30340;&#24494;&#23567;&#12289;&#38590;&#20197;&#23519;&#35273;&#30340;&#25913;&#21160;&#65292;&#20250;&#23548;&#33268;&#20256;&#32479;&#20998;&#31867;&#22120;&#38169;&#35823;&#22320;&#35782;&#21035;&#26631;&#24535;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#20572;&#36710;&#26631;&#24535;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26631;&#24535;&#30340;&#39640;&#32423;&#29305;&#24449;&#65292;&#22914;&#24418;&#29366;&#12289;&#39068;&#33394;&#21644;&#25991;&#26412;&#65292;&#26469;&#26816;&#27979;&#20132;&#36890;&#26631;&#24535;&#30340;&#31867;&#21035;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#26356;&#21152;&#31283;&#20581;&#65292;&#22240;&#20026;&#23427;&#27169;&#25311;&#20154;&#31867;&#30340;&#24863;&#30693;&#26041;&#24335;&#65292;&#19981;&#23481;&#26131;&#21463;&#21040;&#24403;&#21069;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#38480;&#21046;&#24433;&#21709;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#40065;&#26834;&#29289;&#29702;&#25200;&#21160;&#65288;PR2&#65289;&#21644;&#23545;&#25239;&#20266;&#35013;&#65288;AdvCam&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic sign detection is a critical task in the operation of Autonomous Vehicles (AV), as it ensures the safety of all road users. Current DNN-based sign classification systems rely on pixel-level features to detect traffic signs and can be susceptible to adversarial attacks. These attacks involve small, imperceptible changes to a sign that can cause traditional classifiers to misidentify the sign. We propose an Inductive Logic Programming (ILP) based approach for stop sign detection in AVs to address this issue. This method utilises high-level features of a sign, such as its shape, colour, and text, to detect categories of traffic signs. This approach is more robust against adversarial attacks, as it mimics human-like perception and is less susceptible to the limitations of current DNN classifiers. We consider two adversarial attacking methods to evaluate our approach: Robust Physical Perturbation (PR2) and Adversarial Camouflage (AdvCam). These attacks are able to deceive DNN classi
&lt;/p&gt;</description></item><item><title>Sonalysts&#27491;&#22312;&#30740;&#31350;&#24320;&#21457;&#19968;&#31181;&#21512;&#25104;&#20219;&#21153;&#29615;&#22659;&#65288;STE&#65289;&#65292;&#36890;&#36807;&#35843;&#26597;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#21512;&#20316;&#27979;&#35797;&#29615;&#22659;&#65292;&#20182;&#20204;&#24635;&#32467;&#20986;&#20102;&#27979;&#35797;&#29615;&#22659;&#35780;&#20272;&#26631;&#20934;&#21644;&#28508;&#22312;&#30340;&#27979;&#35797;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2309.03213</link><description>&lt;p&gt;
&#25552;&#39640;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#29616;&#29366;&#65306;&#25216;&#26415;&#25253;&#21578;&#65283;3-&#27979;&#35797;&#29615;&#22659;&#26367;&#20195;&#26041;&#26696;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Improving the State of the Art for Training Human-AI Teams: Technical Report #3 -- Analysis of Testbed Alternatives. (arXiv:2309.03213v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03213
&lt;/p&gt;
&lt;p&gt;
Sonalysts&#27491;&#22312;&#30740;&#31350;&#24320;&#21457;&#19968;&#31181;&#21512;&#25104;&#20219;&#21153;&#29615;&#22659;&#65288;STE&#65289;&#65292;&#36890;&#36807;&#35843;&#26597;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#21512;&#20316;&#27979;&#35797;&#29615;&#22659;&#65292;&#20182;&#20204;&#24635;&#32467;&#20986;&#20102;&#27979;&#35797;&#29615;&#22659;&#35780;&#20272;&#26631;&#20934;&#21644;&#28508;&#22312;&#30340;&#27979;&#35797;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sonalysts&#27491;&#22312;&#36890;&#36807;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22242;&#38431;&#39046;&#22495;&#24320;&#23637;&#21407;&#22987;&#30740;&#31350;&#65292;&#20197;&#25193;&#23637;&#25105;&#20204;&#24403;&#21069;&#22312;&#22242;&#38431;&#21512;&#20316;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#20102;&#20026;&#36825;&#39033;&#30740;&#31350;&#25171;&#19979;&#22522;&#30784;&#65292;Sonalysts&#27491;&#22312;&#30740;&#31350;&#24320;&#21457;&#19968;&#31181;&#21512;&#25104;&#20219;&#21153;&#29615;&#22659;&#65288;STE&#65289;&#12290;&#22312;&#20043;&#21069;&#30340;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#26368;&#36817;&#30340;&#22806;&#23637;&#21162;&#21147;&#30340;&#32467;&#26524;&#65292;&#22312;&#36825;&#27425;&#21162;&#21147;&#20013;&#65292;&#25105;&#20204;&#35810;&#38382;&#20102;&#20891;&#20107;&#19987;&#23478;&#21644;&#20854;&#20182;&#22312;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#21512;&#20316;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#20182;&#20204;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#26368;&#30475;&#37325;&#30340;&#29305;&#28857;&#12290; &#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19968;&#20123;&#21463;&#35775;&#32773;&#24314;&#35758;&#25105;&#20204;&#30340;&#22242;&#38431;&#30740;&#31350;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#21512;&#20316;&#27979;&#35797;&#29615;&#22659;&#65292;&#32780;&#19981;&#26159;&#21019;&#24314;&#26032;&#30340;&#29615;&#22659;&#12290;&#22522;&#20110;&#36825;&#19968;&#24314;&#35758;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#35843;&#26597;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#35813;&#35843;&#26597;&#30340;&#32467;&#26524;&#12290;&#26681;&#25454;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#27979;&#35797;&#29615;&#22659;&#35780;&#20272;&#26631;&#20934;&#65292;&#30830;&#23450;&#20102;&#28508;&#22312;&#30340;&#27979;&#35797;&#29615;&#22659;&#65292;&#24182;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sonalysts is working on an initiative to expand our current expertise in teaming to Human-Artificial Intelligence (AI) teams by developing original research in this area. To provide a foundation for that research, Sonalysts is investigating the development of a Synthetic Task Environment (STE). In a previous report, we documented the findings of a recent outreach effort in which we asked military Subject Matter Experts (SMEs) and other researchers in the Human-AI teaming domain to identify the qualities that they most valued in a testbed. A surprising finding from that outreach was that several respondents recommended that our team look into existing human-AI teaming testbeds, rather than creating something new. Based on that recommendation, we conducted a systematic investigation of the associated landscape. In this report, we describe the results of that investigation. Building on the survey results, we developed testbed evaluation criteria, identified potential testbeds, and conduct
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#22521;&#35757;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#21512;&#25104;&#20219;&#21153;&#29615;&#22659;&#65292;&#20197;&#25903;&#25345;&#21033;&#30410;&#30456;&#20851;&#30740;&#31350;&#20154;&#21592;&#22312;&#35813;&#39046;&#22495;&#30340;&#24191;&#27867;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2309.03212</link><description>&lt;p&gt;
&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#22521;&#35757;&#30340;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65306;&#30740;&#31350;&#20154;&#21592;&#30693;&#35782;&#24341;&#23548;&#35843;&#26597;&#30340;&#32467;&#26524; &#25216;&#26415;&#25253;&#21578;#2
&lt;/p&gt;
&lt;p&gt;
Improving the State of the Art for Training Human-AI Teams: Technical Report #2 -- Results of Researcher Knowledge Elicitation Survey. (arXiv:2309.03212v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03212
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#22521;&#35757;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#21512;&#25104;&#20219;&#21153;&#29615;&#22659;&#65292;&#20197;&#25903;&#25345;&#21033;&#30410;&#30456;&#20851;&#30740;&#31350;&#20154;&#21592;&#22312;&#35813;&#39046;&#22495;&#30340;&#24191;&#27867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#20891;&#30740;&#31350;&#23454;&#39564;&#23460; (AFRL) &#30001;&#22269;&#23478;&#31185;&#23398;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#23398;&#38498;&#20026;&#20854;&#21046;&#20316;&#20102;&#19968;&#20221;&#20849;&#35782;&#25253;&#21578;&#65292;&#20854;&#20013;&#35760;&#24405;&#20102;&#23545;&#25903;&#25345;&#20891;&#20107;&#26381;&#21153;&#39046;&#22495;&#20013;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#26222;&#36941;&#21644;&#19981;&#26029;&#22686;&#38271;&#30340;&#24895;&#26395;&#12290;Sonalysts&#20844;&#21496;&#24050;&#32463;&#24320;&#22987;&#20102;&#19968;&#39033;&#20869;&#37096;&#35745;&#21010;&#65292;&#26088;&#22312;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#22521;&#35757;&#12290;&#36825;&#19968;&#21162;&#21147;&#30340;&#31532;&#19968;&#27493;&#26159;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30740;&#31350;&#30340;&#21512;&#25104;&#20219;&#21153;&#29615;&#22659; (STE)&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#33021;&#22815;&#25903;&#25345;&#21033;&#30410;&#30456;&#20851;&#30740;&#31350;&#20154;&#21592;&#35745;&#21010;&#22312;&#35813;&#39046;&#22495;&#24320;&#23637;&#24191;&#27867;&#30740;&#31350;&#30340;STE&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24076;&#26395;&#24191;&#27867;&#37319;&#26679;&#30456;&#20851;&#30740;&#31350;&#31038;&#21306;&#30340;&#20248;&#20808;&#20107;&#39033;&#65292;&#32780;&#36825;&#20221;&#25253;&#21578;&#35760;&#24405;&#30340;&#21162;&#21147;&#26159;&#25105;&#20204;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#35843;&#26597;&#34920;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;&#31532;&#19968;&#37096;&#20998;&#35201;&#27714;&#21463;&#35775;&#32773;&#23601;&#25105;&#20204;&#35748;&#20026;&#21487;&#33021;&#24456;&#37325;&#35201;&#30340;STE&#29305;&#24449;&#34920;&#36798;&#24847;&#35265;&#12290;&#31532;&#20108;&#37096;&#20998;&#21017;
&lt;/p&gt;
&lt;p&gt;
A consensus report produced for the Air Force Research Laboratory (AFRL) by the National Academies of Sciences, Engineering, and Mathematics documented a prevalent and increasing desire to support human-Artificial Intelligence (AI) teaming across military service branches. Sonalysts has begun an internal initiative to explore the training of Human-AI teams. The first step in this effort is to develop a Synthetic Task Environment (STE) that is capable of facilitating research on Human-AI teams. Our goal is to create a STE that offers a task environment that could support the breadth of research that stakeholders plan to perform within this domain. As a result, we wanted to sample the priorities of the relevant research community broadly, and the effort documented in this report is our initial attempt to do so. We created a survey that featured two types of questions. The first asked respondents to report their agreement with STE features that we anticipated might be important. The secon
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21512;&#20316;&#30340;&#22521;&#35757;&#27700;&#24179;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#21512;&#25104;&#20219;&#21153;&#29615;&#22659;&#26469;&#35299;&#20915;&#32852;&#21512;&#20840;&#39046;&#22495;&#25351;&#25381;&#19982;&#25511;&#21046;&#20013;&#30340;&#22242;&#38431;&#21512;&#20316;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03211</link><description>&lt;p&gt;
&#25552;&#39640;&#38024;&#23545;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#22521;&#35757;&#30340;&#29616;&#29366;&#65306;&#25216;&#26415;&#25253;&#21578;&#65283;1-&#31185;&#23398;&#23478;&#28041;&#21450;&#35843;&#26597;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Improving the State of the Art for Training Human-AI Teams: Technical Report #1 -- Results of Subject-Matter Expert Knowledge Elicitation Survey. (arXiv:2309.03211v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03211
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21512;&#20316;&#30340;&#22521;&#35757;&#27700;&#24179;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#21512;&#25104;&#20219;&#21153;&#29615;&#22659;&#26469;&#35299;&#20915;&#32852;&#21512;&#20840;&#39046;&#22495;&#25351;&#25381;&#19982;&#25511;&#21046;&#20013;&#30340;&#22242;&#38431;&#21512;&#20316;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22269;&#23478;&#31185;&#23398;&#24037;&#31243;&#23398;&#38498;&#20026;&#31354;&#20891;&#30740;&#31350;&#23454;&#39564;&#23460;&#21046;&#23450;&#20102;&#19968;&#20221;&#19968;&#33268;&#24615;&#25253;&#21578;&#65292;&#35760;&#24405;&#20102;&#20891;&#20107;&#21508;&#20010;&#26381;&#21153;&#37096;&#38376;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21512;&#20316;&#30340;&#26222;&#36941;&#24895;&#26395;&#12290;Sonalysts&#24050;&#32463;&#24320;&#22987;&#20102;&#19968;&#20010;&#20869;&#37096;&#35745;&#21010;&#65292;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#22521;&#35757;&#12290;&#22312;&#36825;&#19968;&#21162;&#21147;&#30340;&#31532;&#19968;&#27493;&#26159;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30740;&#31350;&#30340;&#21512;&#25104;&#20219;&#21153;&#29615;&#22659;&#65288;STE&#65289;&#12290;&#25105;&#20204;&#20915;&#23450;&#20197;&#32852;&#21512;&#20840;&#39046;&#22495;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;JADC2&#65289;&#20026;&#28966;&#28857;&#26469;&#24320;&#21457;STE&#65292;&#22240;&#20026;JADC2&#27010;&#24565;&#20869;&#30340;&#20256;&#24863;&#22120;&#36755;&#20837;&#21644;&#20915;&#31574;&#36873;&#25321;&#20307;&#37327;&#24456;&#21487;&#33021;&#38656;&#35201;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20197;&#23454;&#29616;&#21450;&#26102;&#20915;&#31574;&#12290;&#22522;&#20110;&#36825;&#19968;&#28966;&#28857;&#65292;&#25105;&#20204;&#19982;&#20855;&#26377;&#25351;&#25381;&#19982;&#25511;&#21046;&#32463;&#39564;&#30340;&#22810;&#20301;&#23398;&#31185;&#19987;&#23478;&#65288;SMEs&#65289;&#36827;&#34892;&#20102;&#25509;&#35302;&#65292;&#20197;&#20102;&#35299;&#24320;&#21457;&#19968;&#20010;&#20307;&#29616;&#19982;JADC2&#30456;&#20851;&#30340;&#22242;&#38431;&#21512;&#20316;&#25361;&#25112;&#30340;STE&#30340;&#35265;&#35299;&#12290;&#26412;&#25253;&#21578;&#35760;&#24405;&#20102;&#25105;&#20204;&#19982;&#36825;&#20123;SMEs&#30340;&#21021;&#27493;&#25509;&#35302;&#12290;
&lt;/p&gt;
&lt;p&gt;
A consensus report produced for the Air Force Research Laboratory by the National Academies of Sciences, Engineering, and Mathematics documented a prevalent and increasing desire to support human-Artificial Intelligence (AI) teaming across military service branches. Sonalysts has begun an internal initiative to explore the training of human-AI teams. The first step in this effort is to develop a Synthetic Task Environment (STE) that is capable of facilitating research on human-AI teams. We decided to use Joint All-Domain Command and Control (JADC2) as a focus point for developing the STE because the volume of sensor inputs and decision options within the JADC2 concept likely requires the use of AI systems to enable timely decisions. Given this focus, we engaged a number of Subject-Matter Experts (SMEs) with Command and Control experience to gain insight into developing a STE that embodied the teaming challenges associated with JADC2. This report documents our initial engagement with th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#23548;&#29992;&#25143;&#29983;&#25104;&#33041;&#20449;&#21495;&#65292;&#20197;&#20419;&#36827;&#20869;&#28304;&#24615;BCI&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#36825;&#31181;&#26694;&#26550;&#22522;&#20110;&#29992;&#25143;&#30340;&#21382;&#21490;&#33041;&#20449;&#21495;&#65292;&#23558;&#20854;&#24341;&#23548;&#21040;&#35299;&#30721;&#22120;&#20272;&#35745;&#30340;&#26368;&#20248;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2309.03209</link><description>&lt;p&gt;
&#19968;&#31181;&#20154;&#26426;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#20197;&#25552;&#39640;&#20869;&#28304;&#24615;BCI&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
A Human-Machine Joint Learning Framework to Boost Endogenous BCI Training. (arXiv:2309.03209v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#23548;&#29992;&#25143;&#29983;&#25104;&#33041;&#20449;&#21495;&#65292;&#20197;&#20419;&#36827;&#20869;&#28304;&#24615;BCI&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#36825;&#31181;&#26694;&#26550;&#22522;&#20110;&#29992;&#25143;&#30340;&#21382;&#21490;&#33041;&#20449;&#21495;&#65292;&#23558;&#20854;&#24341;&#23548;&#21040;&#35299;&#30721;&#22120;&#20272;&#35745;&#30340;&#26368;&#20248;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCIs&#65289;&#20026;&#22823;&#33041;&#19982;&#22806;&#37096;&#35774;&#22791;&#20043;&#38388;&#25552;&#20379;&#20102;&#30452;&#25509;&#36890;&#36947;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#36741;&#21161;&#21644;&#24247;&#22797;&#25216;&#26415;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22522;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#30340;&#20869;&#28304;&#24615;BCIs&#65292;&#22914;&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;BCIs&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#23450;&#31243;&#24230;&#30340;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#25484;&#25569;&#33258;&#21457;BCI&#25511;&#21046;&#38656;&#35201;&#29992;&#25143;&#36890;&#36807;&#24819;&#35937;&#29983;&#25104;&#26126;&#26174;&#19988;&#31283;&#23450;&#30340;&#33041;&#20449;&#21495;&#27169;&#24335;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#65288;&#20960;&#21608;/&#20960;&#20010;&#26376;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#23548;&#29992;&#25143;&#23558;&#33041;&#20449;&#21495;&#29983;&#25104;&#20026;&#35299;&#30721;&#22120;&#20272;&#35745;&#30340;&#26368;&#20248;&#20998;&#24067;&#65292;&#20197;&#20419;&#36827;&#20869;&#28304;&#24615;BCIs&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20197;&#32479;&#19968;&#30340;&#24418;&#24335;&#24314;&#27169;&#20154;&#26426;&#32852;&#21512;&#23398;&#20064;&#36807;&#31243;&#12290;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65306;1&#65289;&#23545;&#20110;&#20154;&#31867;&#26041;&#38754;&#65292;&#25105;&#20204;&#27169;&#25311;&#23398;&#20064;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Brain-computer interfaces (BCIs) provide a direct pathway from the brain to external devices and have demonstrated great potential for assistive and rehabilitation technologies. Endogenous BCIs based on electroencephalogram (EEG) signals, such as motor imagery (MI) BCIs, can provide some level of control. However, mastering spontaneous BCI control requires the users to generate discriminative and stable brain signal patterns by imagery, which is challenging and is usually achieved over a very long training time (weeks/months). Here, we propose a human-machine joint learning framework to boost the learning process in endogenous BCIs, by guiding the user to generate brain signals towards an optimal distribution estimated by the decoder, given the historical brain signals of the user. To this end, we firstly model the human-machine joint learning process in a uniform formulation. Then a human-machine joint learning framework is proposed: 1) for the human side, we model the learning proces
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#33455;&#29255;&#35774;&#35745;&#20013;&#39640;&#25928;&#36923;&#36753;&#32508;&#21512;&#30340;&#30005;&#36335;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#36923;&#36753;&#32508;&#21512;&#20013;&#26080;&#25928;&#21464;&#25442;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;PruneX&#25805;&#20316;&#31526;&#23454;&#29616;&#20102;&#36229;&#39046;&#22495;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.03208</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#33455;&#29255;&#35774;&#35745;&#20013;&#39640;&#25928;&#36923;&#36753;&#32508;&#21512;&#30340;&#30005;&#36335;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Circuit Domain Generalization Framework for Efficient Logic Synthesis in Chip Design. (arXiv:2309.03208v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03208
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#33455;&#29255;&#35774;&#35745;&#20013;&#39640;&#25928;&#36923;&#36753;&#32508;&#21512;&#30340;&#30005;&#36335;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#65292;&#37325;&#28857;&#35299;&#20915;&#20102;&#36923;&#36753;&#32508;&#21512;&#20013;&#26080;&#25928;&#21464;&#25442;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;PruneX&#25805;&#20316;&#31526;&#23454;&#29616;&#20102;&#36229;&#39046;&#22495;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#32508;&#21512;&#22312;&#33455;&#29255;&#35774;&#35745;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26159;&#21322;&#23548;&#20307;&#34892;&#19994;&#30340;&#22522;&#30707;&#12290;&#36923;&#36753;&#32508;&#21512;&#30340;&#20851;&#38190;&#20219;&#21153;&#26159;&#23558;&#30001;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#24314;&#27169;&#30340;&#30005;&#36335;&#36716;&#21270;&#20026;&#20855;&#26377;&#30456;&#21516;&#21151;&#33021;&#30340;&#31616;&#21270;&#30005;&#36335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#35768;&#22810;&#36923;&#36753;&#32508;&#21512;&#25805;&#20316;&#31526;&#25353;&#39034;&#24207;&#23545;&#36755;&#20837;DAG&#19978;&#27599;&#20010;&#33410;&#28857;&#26681;&#30340;&#23376;&#22270;&#24212;&#29992;&#21464;&#25442;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#37327;&#30340;&#21464;&#25442;&#26159;&#26080;&#25928;&#30340;&#65292;&#23548;&#33268;&#24212;&#29992;&#36825;&#20123;&#25805;&#20316;&#31526;&#38750;&#24120;&#32791;&#26102;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;Resub&#21644;Mfs2&#25805;&#20316;&#31526;&#30340;&#36816;&#34892;&#26102;&#38388;&#24448;&#24448;&#20027;&#23548;&#36923;&#36753;&#32508;&#21512;&#20248;&#21270;&#36807;&#31243;&#30340;&#25972;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#36923;&#36753;&#32508;&#21512;&#25805;&#20316;&#31526;&#33539;&#24335;&#65292;&#31216;&#20026;PruneX&#65292;&#29992;&#20110;&#20943;&#23569;&#26080;&#25928;&#30340;&#21464;&#25442;&#12290;PruneX&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#23398;&#20064;&#36866;&#29992;&#20110;&#26410;&#35265;&#36807;&#30005;&#36335;&#65288;&#21363;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65289;&#30340;&#27169;&#22411;&#65292;&#20063;&#21363;&#36229;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;PruneX&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#23454;&#29616;&#20102;&#36229;&#39046;&#22495;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logic Synthesis (LS) plays a vital role in chip design -- a cornerstone of the semiconductor industry. A key task in LS is to transform circuits -modeled by directed acyclic graphs (DAGs) -- into simplified circuits with equivalent functionalities. To tackle this task, many LS operators apply transformations to subgraphs -- rooted at each node on an input DAG -sequentially. However, we found that a large number of transformations are ineffective, which makes applying these operators highly time-consuming. In particular, we notice that the runtime of the Resub and Mfs2 operators often dominates the overall runtime of LS optimization processes. To address this challenge, we propose a novel data-driven LS operator paradigm, namely PruneX, to reduce ineffective transformations. The major challenge of developing PruneX is to learn models that well generalize to unseen circuits, i.e., the out-of-distribution (OOD) generalization problem. Thus, the major technical contribution of PruneX i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#38598;&#25104;SE(3)&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.02685</link><description>&lt;p&gt;
Diffusion-EDFs: &#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation. (arXiv:2309.02685v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#38598;&#25104;SE(3)&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#39564;&#35777;&#20102;&#31561;&#21464;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#26368;&#36817;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#38543;&#26426;&#34892;&#20026;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#23558;&#31354;&#38388;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#21363;SE(3)&#31561;&#21464;&#24615;&#24341;&#20837;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;SE(3)&#31561;&#21464;&#24615;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#26550;&#26500;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#22312;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#26102;&#21482;&#38656;5&#21040;10&#20010;&#20219;&#21153;&#28436;&#31034;&#21363;&#21487;&#12290;&#27492;&#22806;&#65292;&#19982;&#20043;&#21069;&#22522;&#20110;&#25193;&#25955;&#30340;&#25805;&#20316;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have verified that equivariant methods can significantly improve the data efficiency, generalizability, and robustness in robot learning. Meanwhile, denoising diffusion-based generative modeling has recently gained significant attention as a promising approach for robotic manipulation learning from demonstrations with stochastic behaviors. In this paper, we present Diffusion-EDFs, a novel approach that incorporates spatial roto-translation equivariance, i.e., SE(3)-equivariance to diffusion generative modeling. By integrating SE(3)-equivariance into our model architectures, we demonstrate that our proposed method exhibits remarkable data efficiency, requiring only 5 to 10 task demonstrations for effective end-to-end training. Furthermore, our approach showcases superior generalizability compared to previous diffusion-based manipulation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35745;&#31639;&#26356;&#39640;&#32500;&#24230;&#20013;&#30340;&#26368;&#23567;&#26354;&#38754;&#30340;&#25968;&#20540;&#36924;&#36817;&#65292;&#35299;&#20915;&#20102;&#32500;&#25968;&#35781;&#21650;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#27809;&#26377;GPU&#30340;&#31508;&#35760;&#26412;&#30005;&#33041;&#19978;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.02589</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#26356;&#39640;&#32500;&#24230;&#20013;&#30340;&#26368;&#23567;&#26354;&#38754;
&lt;/p&gt;
&lt;p&gt;
Using Physics-Informed Neural Networks to Calculate Minimal Surfaces in Higher Dimensions. (arXiv:2309.02589v1 [math.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#35745;&#31639;&#26356;&#39640;&#32500;&#24230;&#20013;&#30340;&#26368;&#23567;&#26354;&#38754;&#30340;&#25968;&#20540;&#36924;&#36817;&#65292;&#35299;&#20915;&#20102;&#32500;&#25968;&#35781;&#21650;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#27809;&#26377;GPU&#30340;&#31508;&#35760;&#26412;&#30005;&#33041;&#19978;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#26356;&#39640;&#32500;&#24230;&#20013;&#26368;&#23567;&#26354;&#38754;&#30340;&#25968;&#20540;&#36924;&#36817;&#65292;&#36825;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#31867;&#22411;&#12290;&#30001;&#20110;&#32500;&#25968;&#30340;&#35781;&#21650;&#23548;&#33268;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#20250;&#38543;&#32500;&#25968;&#22686;&#21152;&#32780;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#36828;&#36828;&#36229;&#20986;&#20219;&#20309;&#29616;&#20195;&#36229;&#32423;&#35745;&#31639;&#26426;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#21482;&#26377;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#25165;&#33021;&#22815;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#36873;&#25321;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#19968;&#31181;&#31216;&#20026;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;Physics-Informed Neural Network&#65292;PINN&#65289;&#30340;&#27169;&#22411;&#65292;&#23427;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;Deep Neural Network&#65292;DNN&#65289;&#26469;&#35299;&#20915;&#26368;&#23567;&#26354;&#38754;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#23427;&#21487;&#20197;&#22312;&#26356;&#39640;&#32500;&#24230;&#19978;&#25193;&#23637;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#27809;&#26377;GPU&#30340;&#31508;&#35760;&#26412;&#30005;&#33041;&#19978;&#20063;&#33021;&#30456;&#23545;&#24555;&#36895;&#22320;&#35757;&#32451;&#12290;&#30001;&#20110;&#26080;&#27861;&#26597;&#30475;&#39640;&#32500;&#24230;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#20197;&#36275;&#22815;&#30340;&#22266;&#23450;&#36724;&#30340;&#29255;&#27573;&#24418;&#24335;&#21576;&#29616;&#65292;&#20197;&#20415;&#36890;&#36807;3D&#22270;&#24418;&#36827;&#34892;&#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we compute numerical approximations of the minimal surfaces, an essential type of Partial Differential Equation (PDE), in higher dimensions. Classical methods cannot handle it in this case because of the Curse of Dimensionality, where the computational cost of these methods increases exponentially fast in response to higher problem dimensions, far beyond the computing capacity of any modern supercomputers. Only in the past few years have machine learning researchers been able to mitigate this problem. The solution method chosen here is a model known as a Physics-Informed Neural Network (PINN) which trains a deep neural network (DNN) to solve the minimal surface PDE. It can be scaled up into higher dimensions and trained relatively quickly even on a laptop with no GPU. Due to the inability to view the high-dimension output, our data is presented as snippets of a higher-dimension shape with enough fixed axes so that it is viewable with 3-D graphs. Not only will the functio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#27979;&#35797;&#32467;&#26524;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#23384;&#22312;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#21487;&#35266;&#23519;&#21040;&#19968;&#33268;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.02553</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#32763;&#35793;&#30340;&#34892;&#20026;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#27979;&#35797;&#32467;&#26524;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#23384;&#22312;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#21487;&#35266;&#23519;&#21040;&#19968;&#33268;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#20013;&#30340;&#34892;&#20026;&#27979;&#35797;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#34892;&#20026;&#26469;&#32454;&#31890;&#24230;&#35780;&#20272;&#31995;&#32479;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#26426;&#22120;&#32763;&#35793;&#20013;&#34892;&#20026;&#27979;&#35797;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#27979;&#35797;&#33539;&#22260;&#26377;&#38480;&#12289;&#28085;&#30422;&#30340;&#35821;&#35328;&#31181;&#31867;&#20063;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#28304;&#21477;&#23376;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22791;&#36873;&#38598;&#65292;&#20197;&#39564;&#35777;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#26159;&#21542;&#34920;&#29616;&#20986;&#39044;&#26399;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#20351;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#34892;&#20026;&#27979;&#35797;&#23454;&#38469;&#21487;&#34892;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#26368;&#23569;&#30340;&#20154;&#21147;&#25237;&#20837;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#35780;&#20272;&#26694;&#26550;&#24212;&#29992;&#20110;&#22810;&#20010;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#24635;&#20307;&#19978;&#36890;&#36807;&#29575;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#21487;&#35266;&#23519;&#21040;&#30340;&#36235;&#21183;&#30456;&#31526;&#65292;&#20294;&#20173;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioral testing in NLP allows fine-grained evaluation of systems by examining their linguistic capabilities through the analysis of input-output behavior. Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations. We can then verify whether the MT model exhibits the expected behavior through matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metri
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20154;&#24037;&#31995;&#32479;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#36857;&#35937;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#65292;&#26234;&#21147;&#30340;&#24515;&#29702;&#27979;&#37327;&#21487;&#20197;&#38388;&#25509;&#22320;&#36817;&#20284;&#24847;&#35782;&#20307;&#39564;&#30340;&#31243;&#24230;&#65292;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2309.00646</link><description>&lt;p&gt;
&#26234;&#33021;&#20316;&#20026;&#24847;&#35782;&#30340;&#34913;&#37327;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Intelligence as a Measure of Consciousness. (arXiv:2309.00646v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00646
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20154;&#24037;&#31995;&#32479;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#36857;&#35937;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#65292;&#26234;&#21147;&#30340;&#24515;&#29702;&#27979;&#37327;&#21487;&#20197;&#38388;&#25509;&#22320;&#36817;&#20284;&#24847;&#35782;&#20307;&#39564;&#30340;&#31243;&#24230;&#65292;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20154;&#24037;&#31995;&#32479;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#36857;&#35937;&#36234;&#26469;&#36234;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#65292;&#32780;&#19968;&#20010;&#20005;&#35880;&#30340;&#24515;&#29702;&#27979;&#37327;&#26694;&#26550;&#22312;&#36825;&#26041;&#38754;&#23545;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#26681;&#25454;&#23545;&#20154;&#31867;&#21644;&#21160;&#29289;&#22823;&#33041;&#30340;&#20449;&#24687;&#32806;&#21512;&#12289;&#20154;&#31867;&#35748;&#30693;&#21457;&#23637;&#12289;&#26032;&#20852;&#33021;&#21147;&#21644;&#24515;&#29702;&#34920;&#31034;&#21457;&#23637;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31867;&#20284;&#29616;&#35937;&#30340;&#27604;&#36739;&#65292;&#25105;&#35748;&#20026;&#26234;&#21147;&#30340;&#24515;&#29702;&#27979;&#37327;&#65292;&#22914;&#26234;&#21830;&#25110;&#26234;&#21830;&#25351;&#25968;&#65292;&#38388;&#25509;&#22320;&#36817;&#20284;&#20102;&#24847;&#35782;&#20307;&#39564;&#30340;&#31243;&#24230;&#12290;&#22522;&#20110;&#26356;&#24191;&#27867;&#30340;&#31185;&#23398;&#21644;&#24418;&#32780;&#19978;&#23398;&#30340;&#24847;&#35782;&#29702;&#35770;&#65292;&#25105;&#35748;&#20026;&#25152;&#26377;&#31995;&#32479;&#37117;&#20855;&#26377;&#19968;&#23450;&#31243;&#24230;&#30340;&#21487;&#27979;&#37327;&#30340;&#24847;&#35782;&#65292;&#24182;&#19988;&#26234;&#21147;&#30340;&#24515;&#29702;&#27979;&#37327;&#21487;&#20197;&#29992;&#26469;&#34913;&#37327;&#36825;&#31181;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating artificial systems for signs of consciousness is increasingly becoming a pressing concern, and a rigorous psychometric measurement framework may be of crucial importance in evaluating large language models in this regard. Most prominent theories of consciousness, both scientific and metaphysical, argue for different kinds of information coupling as a necessary component of human-like consciousness. By comparing information coupling in human and animal brains, human cognitive development, emergent abilities, and mental representation development to analogous phenomena in large language models, I argue that psychometric measures of intelligence, such as the g-factor or IQ, indirectly approximate the extent of conscious experience.  Based on a broader source of both scientific and metaphysical theories of consciousness, I argue that all systems possess a degree of consciousness ascertainable psychometrically and that psychometric measures of intelligence may be used to gauge re
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#26500;&#24314;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00237</link><description>&lt;p&gt;
&#22522;&#20110;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#30340;&#20844;&#24320;&#21487;&#20849;&#20139;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes. (arXiv:2309.00237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00237
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#26500;&#24314;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21512;&#25104;&#30340;&#20020;&#24202;&#26696;&#20363;&#25253;&#21578;&#65292;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#65292;&#20197;&#35299;&#20915;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#21512;&#25104;&#35760;&#24405;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#19987;&#38376;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;Asclepius&#12290;&#34429;&#28982;Asclepius&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;Asclepius&#19982;&#21253;&#25324;GPT-3.5-turbo&#21644;&#20854;&#20182;&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#22312;&#20869;&#30340;&#20960;&#31181;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#23558;Asclepius&#19982;&#20854;&#22312;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21464;&#20307;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21147;&#22320;&#35777;&#26126;&#65292;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#22312;&#26500;&#24314;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#21487;&#20197;&#20316;&#20026;&#21487;&#34892;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models tailored for handling patients' clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations. To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature. We then use these synthetic notes to train our specialized clinical large language model, Asclepius. While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes. We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.16898</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16898
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;"Attention Is All You Need"&#20013;&#24341;&#20837;&#36716;&#25442;&#22120;&#26550;&#26500;&#20197;&#26469;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#36716;&#25442;&#22120;&#20013;&#30340;&#27880;&#24847;&#21147;&#23618;&#25509;&#21463;&#36755;&#20837;&#20196;&#29260;&#24207;&#21015;$X$&#24182;&#36890;&#36807;&#35745;&#31639;softmax$(XQK^\top X^\top)$&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#20351;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;$(K,Q)$&#26159;&#21487;&#35757;&#32451;&#30340;&#38190;-&#26597;&#35810;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#20248;&#21270;&#20960;&#20309;&#21644;&#19968;&#20010;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#23545;&#20196;&#29260;&#23545;&#30340;&#22806;&#31215;&#26045;&#21152;&#32447;&#24615;&#32422;&#26463;&#65292;&#23558;&#26368;&#20339;&#36755;&#20837;&#20196;&#29260;&#19982;&#38750;&#26368;&#20339;&#20196;&#29260;&#20998;&#31163;&#12290;&#36825;&#20010;&#24418;&#24335;&#20027;&#20041;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#24449;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#21333;&#23618;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#65306;(1)&#20248;&#21270;&#27880;&#24847;&#21147;&#23618;&#65292;&#20351;&#29992;&#21487;&#21464;&#27491;&#21017;&#21270;&#21442;&#25968;$(K,Q)$&#65292;&#25910;&#25947;&#30340;&#26041;&#21521;&#26159;&#19968;&#20010;&#26368;&#23567;&#21270;&#32508;&#21512;&#21442;&#25968;$W=KQ^\top$&#30340;&#26680;&#33539;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#35299;&#20915;&#26041;&#26696;&#12290;&#32780;&#30452;&#25509;&#20351;&#29992;$W$&#36827;&#34892;&#21442;&#25968;&#21270;&#21017;&#26368;&#23567;&#21270;&#19968;&#20010;Frobenius&#33539;&#25968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Ladder-of-Thought&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#26469;&#25552;&#21319;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#23567;&#22411;&#27169;&#22411;&#22312;&#24212;&#29992;&#20808;&#21069;&#20869;&#37096;&#30693;&#35782;&#26102;&#24615;&#33021;&#25552;&#21319;&#19981;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#25928;&#29575;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16763</link><description>&lt;p&gt;
Ladder-of-Thought: &#20351;&#29992;&#30693;&#35782;&#20316;&#20026;&#38454;&#26799;&#25552;&#21319;&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection. (arXiv:2308.16763v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Ladder-of-Thought&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#26469;&#25552;&#21319;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#23567;&#22411;&#27169;&#22411;&#22312;&#24212;&#29992;&#20808;&#21069;&#20869;&#37096;&#30693;&#35782;&#26102;&#24615;&#33021;&#25552;&#21319;&#19981;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#25928;&#29575;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#24335;&#25552;&#20379;&#65288;CoT&#65289;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#30340;&#25512;&#29702;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#20027;&#35201;&#26377;&#30410;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#22312;&#30452;&#25509;&#24212;&#29992;CoT&#26102;&#23567;&#22411;LLM&#30340;&#24615;&#33021;&#25913;&#36827;&#19981;&#26126;&#26174;&#12290;&#23613;&#31649;LLM&#20855;&#26377;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;CoT&#20027;&#35201;&#20381;&#36182;&#20110;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#20869;&#37096;&#30693;&#35782;&#65292;&#20808;&#21069;&#26410;&#30693;&#20110;&#27169;&#22411;&#30340;&#22806;&#37096;&#30693;&#35782;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#12290;&#22312;&#31435;&#22330;&#26816;&#27979;&#31561;&#20219;&#21153;&#20013;&#65292;&#22806;&#37096;&#32972;&#26223;&#30693;&#35782;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36825;&#31181;&#36951;&#28431;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;LLM&#30340;&#22823;&#35268;&#27169;&#26550;&#26500;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#25928;&#29575;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#31435;&#22330;&#26816;&#27979;&#30340;&#24605;&#32500;&#38454;&#26799;&#65288;LoT&#65289;&#12290;LoT&#22522;&#20110;&#21452;&#38454;&#27573;&#32423;&#32852;&#20248;&#21270;&#26694;&#26550;&#65292;&#25351;&#23548;&#27169;&#22411;&#25972;&#21512;&#39640;&#36136;&#37327;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#22686;&#24378;&#20013;&#38388;&#27493;&#39588;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Prompting (CoT) reinforces the reasoning capabilities of Large Language Models (LLMs) through the generation of intermediate rationales. However, these enhancements predominantly benefit large-scale models, leaving small LMs without significant performance improvements when directly applying CoT. Despite the advanced reasoning capabilities of LLMs, CoT relies primarily on their pre-trained internal knowledge. The external knowledge that is previously unknown to the model remains unexploited. This omission becomes pronounced in tasks such as stance detection, where the external background knowledge plays a pivotal role. Additionally, the large-scale architecture of LLMs inevitably present efficiency challenges during deployment. To address these challenges, we introduce the Ladder-of-Thought (LoT) for stance detection. Grounded in a dual-phase Cascaded Optimization framework, LoT directs the model to incorporate high-quality external knowledge, enhancing the intermediat
&lt;/p&gt;</description></item><item><title>LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16137</link><description>&lt;p&gt;
LM-Infinite: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21363;&#26102;&#38271;&#24230;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16137
&lt;/p&gt;
&lt;p&gt;
LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;Transformer-based&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;LLM&#22312;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#30528;&#23545;&#38271;&#26102;&#38388;&#25512;&#29702;&#36807;&#31243;&#25110;&#29702;&#35299;&#26356;&#22823;&#19978;&#19979;&#25991;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;LLM&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#26041;&#26696;&#23558;&#35757;&#32451;&#24207;&#21015;&#25130;&#26029;&#21040;&#22266;&#23450;&#38271;&#24230;&#65288;&#20363;&#22914;LLaMa&#30340;2048&#65289;&#12290;&#21363;&#20351;&#20351;&#29992;&#20102;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;LLM&#22312;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20043;&#21518;&#24448;&#24448;&#38590;&#20197;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#26356;&#19981;&#29992;&#35828;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#20102;&#12290;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#22312;&#26356;&#38271;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#24448;&#24448;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#30828;&#20214;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#38656;&#35201;&#36827;&#34892;&#20180;&#32454;&#30340;&#35757;&#32451;&#36807;&#31243;&#35774;&#35745;&#12290;&#20026;&#20102;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30740;&#31350;&#20102;&#20027;&#35201;&#30340;&#20998;&#24067;&#22806;(OOD) f
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;&#20915;&#31574;&#36807;&#31243;&#25968;&#25454;&#21644;&#27169;&#22411;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#20915;&#31574;&#36807;&#31243;&#21644;&#24314;&#31435;&#20915;&#31574;&#28436;&#21464;&#27169;&#22411;&#65292;&#21487;&#20197;&#25581;&#31034;&#28508;&#22312;&#30340;&#20559;&#22909;&#65292;&#21516;&#26102;&#36861;&#36394;&#20915;&#31574;&#36807;&#31243;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#20379;&#37325;&#35201;&#20449;&#24687;&#65292;&#20174;&#32780;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15225</link><description>&lt;p&gt;
&#20174;DDMs&#21040;DNNs&#65306;&#21033;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#26469;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
From DDMs to DNNs: Using process data and models of decision-making to improve human-AI interactions. (arXiv:2308.15225v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;&#20915;&#31574;&#36807;&#31243;&#25968;&#25454;&#21644;&#27169;&#22411;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#20915;&#31574;&#36807;&#31243;&#21644;&#24314;&#31435;&#20915;&#31574;&#28436;&#21464;&#27169;&#22411;&#65292;&#21487;&#20197;&#25581;&#31034;&#28508;&#22312;&#30340;&#20559;&#22909;&#65292;&#21516;&#26102;&#36861;&#36394;&#20915;&#31574;&#36807;&#31243;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#20379;&#37325;&#35201;&#20449;&#24687;&#65292;&#20174;&#32780;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#23478;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#23478;&#24050;&#32463;&#35748;&#35782;&#21040;&#35814;&#32454;&#25551;&#36848;&#20915;&#31574;&#36807;&#31243;&#21644;&#24314;&#31435;&#20915;&#31574;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;&#20363;&#22914;&#65292;&#20915;&#31574;&#25152;&#38656;&#30340;&#26102;&#38388;&#21487;&#20197;&#25581;&#31034;&#19968;&#20010;&#20010;&#20307;&#30495;&#27491;&#30340;&#28508;&#22312;&#20559;&#22909;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20915;&#31574;&#26412;&#36523;&#12290;&#31867;&#20284;&#22320;&#65292;&#36861;&#36394;&#20915;&#31574;&#36807;&#31243;&#30340;&#25968;&#25454;&#65292;&#22914;&#30524;&#21160;&#25110;&#31070;&#32463;&#35760;&#24405;&#65292;&#21253;&#21547;&#20102;&#20851;&#38190;&#30340;&#20449;&#24687;&#65292;&#21363;&#20351;&#27809;&#26377;&#36798;&#25104;&#20915;&#31574;&#20063;&#21487;&#20197;&#34987;&#21033;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35748;&#20026;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24212;&#26356;&#21152;&#20851;&#27880;&#20915;&#31574;&#22914;&#20309;&#38543;&#26102;&#38388;&#28436;&#21464;&#20197;&#21450;&#22914;&#20309;&#34701;&#20837;&#30456;&#20851;&#30340;&#36807;&#31243;&#25968;&#25454;&#26469;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#20132;&#20114;&#20013;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#38750;&#24120;&#25104;&#29087;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35748;&#20026;&#20915;&#31574;&#26159;&#20174;&#26434;&#38899;&#32047;&#31215;&#30340;&#35777;&#25454;&#20013;&#20135;&#29983;&#30340;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#24515;&#29702;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#32463;&#27982;&#23398;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decades, cognitive neuroscientists and behavioral economists have recognized the value of describing the process of decision making in detail and modeling the emergence of decisions over time. For example, the time it takes to decide can reveal more about an agents true hidden preferences than only the decision itself. Similarly, data that track the ongoing decision process such as eye movements or neural recordings contain critical information that can be exploited, even if no decision is made. Here, we argue that artificial intelligence (AI) research would benefit from a stronger focus on insights about how decisions emerge over time and incorporate related process data to improve AI predictions in general and human-AI interactions in particular. First, we introduce a highly established computational framework that assumes decisions to emerge from the noisy accumulation of evidence, and we present related empirical work in psychology, neuroscience, and economics. Next, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#25311;&#36830;&#32493;&#21160;&#24577;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15116</link><description>&lt;p&gt;
&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators. (arXiv:2308.15116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#25311;&#36830;&#32493;&#21160;&#24577;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#29983;&#29289;&#20998;&#23376;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#20998;&#23376;&#33021;&#22815;&#27874;&#21160;&#30340;&#21508;&#31181;&#26465;&#20214;&#19979;&#23545;&#19968;&#32452;&#31890;&#23376;&#36827;&#34892;&#27169;&#25311;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36719;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#20998;&#23376;&#21160;&#21147;&#23398;&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#36866;&#24212;&#24615;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#38750;&#24120;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#22330;&#26223;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#24037;&#20316;&#20197;&#28201;&#24230;&#20026;&#27979;&#35797;&#26696;&#20363;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#20351;&#20854;&#21487;&#20197;&#36890;&#36807;&#20219;&#20309;&#36830;&#32493;&#30340;&#21160;&#24577;&#26465;&#20214;&#65288;&#22914;&#21387;&#21147;&#21644;&#20307;&#31215;&#65289;&#36827;&#34892;&#26377;&#25928;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#20004;&#20010;&#38454;&#27573;&#65306;1&#65289;&#20351;&#29992;&#25968;&#25454;&#28151;&#21512;&#25216;&#26415;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22686;&#24378;&#20998;&#23376;&#32467;&#26500;&#25968;&#25454;&#21644;&#28201;&#24230;&#25552;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#27604;&#20363;&#30340;&#26041;&#24335;&#24212;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;2&#65289;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#24494;&#35843;&#26694;&#26550;&#25552;&#39640;&#20102;&#24494;&#35843;&#36807;&#31243;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#20026;&#36719;&#25552;&#31034;&#24494;&#35843;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular dynamics simulations have emerged as a fundamental instrument for studying biomolecules. At the same time, it is desirable to perform simulations of a collection of particles under various conditions in which the molecules can fluctuate. In this paper, we explore and adapt the soft prompt-based learning method to molecular dynamics tasks. Our model can remarkably generalize to unseen and out-of-distribution scenarios with limited training data. While our work focuses on temperature as a test case, the versatility of our approach allows for efficient simulation through any continuous dynamic conditions, such as pressure and volumes. Our framework has two stages: 1) Pre-trains with data mixing technique, augments molecular structure data and temperature prompts, then applies a curriculum learning method by increasing the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework improves sample-efficiency of fine-tuning process and gives the soft prompt-tuning better 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AtmoRep&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#30830;&#23450;&#22797;&#26434;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#25551;&#36848;&#65292;&#20174;&#32780;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#25216;&#33021;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13280</link><description>&lt;p&gt;
AtmoRep:&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning. (arXiv:2308.13280v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13280
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AtmoRep&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#30830;&#23450;&#22797;&#26434;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#25551;&#36848;&#65292;&#20174;&#32780;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#25216;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27668;&#23545;&#20154;&#31867;&#26377;&#22810;&#31181;&#24433;&#21709;&#65292;&#20174;&#22240;&#22825;&#27668;&#19981;&#33391;&#32780;&#20007;&#29983;&#30340;&#25439;&#22833;&#21040;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#31038;&#20250;&#21644;&#32463;&#27982;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23545;&#22823;&#27668;&#21160;&#21147;&#23398;&#36827;&#34892;&#35745;&#31639;&#26426;&#27169;&#25311;&#23545;&#25105;&#20204;&#21644;&#26410;&#26469;&#30340;&#19990;&#20195;&#30340;&#31119;&#31049;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AtmoRep&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#35745;&#31639;&#26426;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;&#24191;&#27867;&#30340;&#24212;&#29992;&#25552;&#20379;&#25216;&#33021;&#32467;&#26524;&#12290;AtmoRep&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#26469;&#30830;&#23450;&#22823;&#27668;&#39640;&#24230;&#22797;&#26434;&#12289;&#38543;&#26426;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#25551;&#36848;&#65292;&#35813;&#25551;&#36848;&#22522;&#20110;&#21382;&#21490;&#36712;&#36857;&#30340;&#26368;&#20339;&#21487;&#29992;&#20272;&#35745;&#65292;&#36825;&#20123;&#21382;&#21490;&#36712;&#36857;&#21463;&#35266;&#27979;&#32422;&#26463;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#21644;&#19968;&#20010;&#29420;&#29305;&#30340;&#38598;&#21512;&#23454;&#29616;&#30340;&#65292;&#35813;&#38598;&#21512;&#20174;&#38543;&#26426;&#27169;&#22411;&#20013;&#37319;&#26679;&#65292;&#20854;&#21487;&#21464;&#24615;&#21463;&#21382;&#21490;&#35760;&#24405;&#20013;&#30340;&#21487;&#21464;&#24615;&#21551;&#21457;&#12290;AtmoRep&#30340;&#20219;&#21153;&#26080;&#20851;&#24615;&#20351;&#20854;&#33021;&#22815;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#28789;&#27963;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The atmosphere affects humans in a multitude of ways, from loss of life due to adverse weather effects to long-term social and economic impacts on societies. Computer simulations of atmospheric dynamics are, therefore, of great importance for the well-being of our and future generations. Here, we propose AtmoRep, a novel, task-independent stochastic computer model of atmospheric dynamics that can provide skillful results for a wide range of applications. AtmoRep uses large-scale representation learning from artificial intelligence to determine a general description of the highly complex, stochastic dynamics of the atmosphere from the best available estimate of the system's historical trajectory as constrained by observations. This is enabled by a novel self-supervised learning objective and a unique ensemble that samples from the stochastic model with a variability informed by the one in the historical record. The task-independent nature of AtmoRep enables skillful results for a divers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2308.11764</link><description>&lt;p&gt;
Halo&#65306;&#35780;&#20272;&#21644;&#38477;&#20302;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#12290;&#34429;&#28982;&#23545;&#20110;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#26041;&#20415;&#65292;&#20294;&#26159;&#19982;&#20854;&#26356;&#22823;&#35268;&#27169;&#30340;&#23545;&#24212;&#27169;&#22411;&#30456;&#27604;&#65292;&#24320;&#28304;&#30340;&#21442;&#25968;&#36739;&#23569;&#30340;LLMs&#32463;&#24120;&#20986;&#29616;&#20005;&#37325;&#24187;&#35273;&#38382;&#39064;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#27979;&#37327;&#21644;&#20943;&#23569;BLOOM 7B&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#26159;&#20844;&#24320;&#25552;&#20379;&#32473;&#30740;&#31350;&#21644;&#21830;&#19994;&#24212;&#29992;&#30340;&#24369;&#24320;&#28304;LLMs&#30340;&#20195;&#34920;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HaloCheck&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26080;&#38656;&#30693;&#35782;&#30340;&#40657;&#30418;&#23376;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;LLMs&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#20302;&#21442;&#25968;LLMs&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;LLMs&#30340;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.11432</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20195;&#29702;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#23398;&#26415;&#30028;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#23545;&#26377;&#38480;&#30693;&#35782;&#30340;&#20195;&#29702;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#19982;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#20154;&#31867;&#33324;&#30340;&#20915;&#31574;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#33719;&#21462;&#22823;&#37327;&#30340;&#32593;&#32476;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20986;&#20102;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#30340;&#39640;&#28072;&#20852;&#36259;&#12290;&#20026;&#20102;&#21457;&#25381;LLM&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#21508;&#31181;&#19981;&#21516;&#24212;&#29992;&#30340;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#36825;&#20123;&#30740;&#31350;&#65292;&#20174;&#25972;&#20307;&#30340;&#35282;&#24230;&#23545;&#33258;&#20027;&#20195;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23457;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26500;&#24314;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#19979;&#30340;&#20559;&#24046;&#21644;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02335</link><description>&lt;p&gt;
RAHNet: &#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification. (arXiv:2308.02335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02335
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#19979;&#30340;&#20559;&#24046;&#21644;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#26159;&#35768;&#22810;&#23454;&#38469;&#22810;&#23186;&#20307;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#22270;&#21487;&#20197;&#34920;&#31034;&#21508;&#31181;&#22810;&#23186;&#20307;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#31038;&#20132;&#32593;&#32476;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#65292;&#20854;&#20013;&#31867;&#20998;&#24067;&#26159;&#24179;&#34913;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#25968;&#25454;&#36890;&#24120;&#21576;&#29616;&#20986;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#65292;&#23548;&#33268;&#22312;&#20351;&#29992;GNN&#26102;&#23545;&#22836;&#37096;&#31867;&#21035;&#23384;&#22312;&#20559;&#24046;&#65292;&#19988;&#23545;&#23614;&#37096;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#37325;&#26032;&#24179;&#34913;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#26410;&#33021;&#26126;&#30830;&#24341;&#20837;&#26032;&#30693;&#35782;&#65292;&#24182;&#29306;&#29298;&#20102;&#22836;&#37096;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#65292;&#20197;&#20998;&#31163;&#30340;&#26041;&#24335;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#22120;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22270;&#26816;&#32034;&#27169;&#22359;&#26469;&#25628;&#32034;&#30456;&#20851;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph classification is a crucial task in many real-world multimedia applications, where graphs can represent various multimedia data types such as images, videos, and social networks. Previous efforts have applied graph neural networks (GNNs) in balanced situations where the class distribution is balanced. However, real-world data typically exhibit long-tailed class distributions, resulting in a bias towards the head classes when using GNNs and limited generalization ability over the tail classes. Recent approaches mainly focus on re-balancing different classes during model training, which fails to explicitly introduce new knowledge and sacrifices the performance of the head classes. To address these drawbacks, we propose a novel framework called Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature extractor and an unbiased classifier in a decoupled manner. In the feature extractor training stage, we develop a graph retrieval module to search for relevant grap
&lt;/p&gt;</description></item><item><title>VLUCI&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#21453;&#20107;&#23454;&#25512;&#26029;&#20013;&#30340;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#21452;&#37325;&#21464;&#20998;&#25512;&#26029;&#27169;&#22411;&#26469;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#20013;&#35266;&#27979;&#21644;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00904</link><description>&lt;p&gt;
VLUCI: &#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference. (arXiv:2308.00904v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00904
&lt;/p&gt;
&lt;p&gt;
VLUCI&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#21453;&#20107;&#23454;&#25512;&#26029;&#20013;&#30340;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#21452;&#37325;&#21464;&#20998;&#25512;&#26029;&#27169;&#22411;&#26469;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#20013;&#35266;&#27979;&#21644;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#22312;&#27969;&#34892;&#30149;&#23398;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32463;&#27982;&#23398;&#31561;&#39046;&#22495;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#21435;&#28151;&#28102;&#21644;&#21453;&#20107;&#23454;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#34429;&#28982;&#29616;&#26377;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#65292;&#20294;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#23384;&#22312;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#25197;&#26354;&#20102;&#22240;&#26524;&#25512;&#26029;&#24182;&#24433;&#21709;&#20102;&#21453;&#20107;&#23454;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21453;&#20107;&#23454;&#25512;&#26029;&#20013;&#30340;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#65288;VLUCI&#65289;&#65292;&#23427;&#29983;&#25104;&#20102;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;VLUCI&#25918;&#26494;&#20102;&#22823;&#22810;&#25968;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#30340;&#26080;&#28151;&#28102;&#20551;&#35774;&#12290;&#36890;&#36807;&#35299;&#32806;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#21644;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#65292;VLUCI&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#37325;&#21464;&#20998;&#25512;&#26029;&#27169;&#22411;&#65292;&#20197;&#36817;&#20284;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#36825;&#20123;&#21464;&#37327;&#29992;&#20110;&#25512;&#26029;&#26356;&#20934;&#30830;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#12290;&#23545;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference plays a vital role in diverse domains like epidemiology, healthcare, and economics. De-confounding and counterfactual prediction in observational data has emerged as a prominent concern in causal inference research. While existing models tackle observed confounders, the presence of unobserved confounders remains a significant challenge, distorting causal inference and impacting counterfactual outcome accuracy. To address this, we propose a novel variational learning model of unobserved confounders for counterfactual inference (VLUCI), which generates the posterior distribution of unobserved confounders. VLUCI relaxes the unconfoundedness assumption often overlooked by most causal inference methods. By disentangling observed and unobserved confounders, VLUCI constructs a doubly variational inference model to approximate the distribution of unobserved confounders, which are used for inferring more accurate counterfactual outcomes. Extensive experiments on synthetic and s
&lt;/p&gt;</description></item><item><title>&#20511;&#37492;&#33021;&#21147;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;AI&#31995;&#32479;&#19982;&#20010;&#20307;&#20114;&#21160;&#26102;&#28044;&#29616;&#30340;&#20262;&#29702;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#20063;&#30028;&#23450;&#20102;&#36947;&#24503;&#21487;&#25509;&#21463;&#30340;&#20114;&#21160;&#26465;&#20214;&#65292;&#24182;&#23545;&#20960;&#31181;&#22833;&#36133;&#27169;&#24335;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.00868</link><description>&lt;p&gt;
&#20161;&#29233;&#26234;&#33021;&#65306;&#36890;&#36807;AI&#31995;&#32479;&#23545;&#21033;&#30410;&#12289;&#25588;&#21161;&#21450;&#30456;&#20851;&#36947;&#24503;&#22833;&#35823;&#36827;&#34892;&#24314;&#27169;&#30340;&#33021;&#21147;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beneficent Intelligence: A Capability Approach to Modeling Benefit, Assistance, and Associated Moral Failures through AI Systems. (arXiv:2308.00868v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00868
&lt;/p&gt;
&lt;p&gt;
&#20511;&#37492;&#33021;&#21147;&#26041;&#27861;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;AI&#31995;&#32479;&#19982;&#20010;&#20307;&#20114;&#21160;&#26102;&#28044;&#29616;&#30340;&#20262;&#29702;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#20063;&#30028;&#23450;&#20102;&#36947;&#24503;&#21487;&#25509;&#21463;&#30340;&#20114;&#21160;&#26465;&#20214;&#65292;&#24182;&#23545;&#20960;&#31181;&#22833;&#36133;&#27169;&#24335;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#20262;&#29702;&#23398;&#20013;&#26222;&#36941;&#30340;&#35752;&#35770;&#32570;&#20047;&#25429;&#25417;AI&#31995;&#32479;&#19982;&#20010;&#20307;&#20114;&#21160;&#26102;&#28044;&#29616;&#30340;&#22810;&#26679;&#21270;&#20262;&#29702;&#20851;&#20999;&#25152;&#38656;&#30340;&#35821;&#35328;&#21644;&#24418;&#24335;&#12290;&#20511;&#37492;Sen&#21644;Nussbaum&#30340;&#33021;&#21147;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#26694;&#26550;&#65292;&#24418;&#24335;&#21270;&#20102;AI&#31995;&#32479;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#21033;&#30410;&#25110;&#25588;&#21161;&#25152;&#24517;&#38656;&#30340;&#20262;&#29702;&#27010;&#24565;&#21644;&#26435;&#21033;&#12290;&#36825;&#20123;&#31995;&#32479;&#22686;&#24378;&#20102;&#21033;&#30410;&#30456;&#20851;&#32773;&#25512;&#36827;&#20854;&#20154;&#29983;&#35745;&#21010;&#21644;&#24184;&#31119;&#24863;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#32500;&#25252;&#20854;&#22522;&#26412;&#26435;&#21033;&#12290;&#25105;&#20204;&#30028;&#23450;&#20102;AI&#31995;&#32479;&#19982;&#21463;&#20854;&#21151;&#33021;&#24433;&#21709;&#30340;&#20154;&#20043;&#38388;&#36947;&#24503;&#19978;&#21487;&#25509;&#21463;&#30340;&#20114;&#21160;&#30340;&#20004;&#20010;&#24517;&#35201;&#26465;&#20214;&#65292;&#20197;&#21450;&#23454;&#29616;&#26377;&#24847;&#20041;&#21033;&#30410;&#29702;&#24819;&#30340;&#20004;&#20010;&#20805;&#20998;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#29702;&#24819;&#19982;&#20960;&#31181;&#31361;&#20986;&#30340;&#22833;&#36133;&#27169;&#24335;&#36827;&#34892;&#23545;&#27604;&#65292;&#21363;&#26500;&#25104;&#19981;&#21512;&#29702;&#30340;&#23478;&#38271;&#24335;&#20027;&#20041;&#12289;&#24378;&#36843;&#12289;&#27450;&#39575;&#12289;&#21093;&#21066;&#21644;&#25903;&#37197;&#30340;&#31038;&#20132;&#20114;&#21160;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevailing discourse around AI ethics lacks the language and formalism necessary to capture the diverse ethical concerns that emerge when AI systems interact with individuals. Drawing on Sen and Nussbaum's capability approach, we present a framework formalizing a network of ethical concepts and entitlements necessary for AI systems to confer meaningful benefit or assistance to stakeholders. Such systems enhance stakeholders' ability to advance their life plans and well-being while upholding their fundamental rights. We characterize two necessary conditions for morally permissible interactions between AI systems and those impacted by their functioning, and two sufficient conditions for realizing the ideal of meaningful benefit. We then contrast this ideal with several salient failure modes, namely, forms of social interactions that constitute unjustified paternalism, coercion, deception, exploitation and domination. The proliferation of incidents involving AI in high-stakes domains 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20316;&#20026;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#24110;&#21161;&#19977;&#32500;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#28857;&#20113;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#31435;&#20307;&#20851;&#31995;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14971</link><description>&lt;p&gt;
Take-A-Photo: &#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#28857;&#20113;&#27169;&#22411;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20316;&#20026;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#24110;&#21161;&#19977;&#32500;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#28857;&#20113;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#31435;&#20307;&#20851;&#31995;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;MAE&#24102;&#39046;&#19979;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#22312;2D&#35270;&#35273;&#39046;&#22495;&#24050;&#32463;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#28508;&#21147;&#26469;&#25552;&#21319;&#22522;&#26412;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;3D&#35270;&#35273;&#39046;&#22495;&#65292;&#23545;Transformer&#20026;&#22522;&#30784;&#30340;&#39592;&#24178;&#32593;&#32476;&#30340;&#36807;&#24230;&#20381;&#36182;&#20197;&#21450;&#28857;&#20113;&#30340;&#26080;&#24207;&#24615;&#38480;&#21046;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#20219;&#20309;&#28857;&#20113;&#27169;&#22411;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#20174;&#19981;&#21516;&#30340;&#23039;&#21183;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20316;&#20026;&#39044;&#35757;&#32451;&#26041;&#26696;&#12290;&#30456;&#27604;&#20110;&#20854;&#28857;&#20113;&#23545;&#24212;&#29289;&#65292;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20855;&#26377;&#26356;&#31934;&#30830;&#30340;&#30417;&#30563;&#65292;&#20174;&#32780;&#24110;&#21161;3D&#32972;&#39592;&#26356;&#22909;&#22320;&#29702;&#35299;&#28857;&#20113;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#31435;&#20307;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#26377;&#25928;&#22320;&#25552;&#21319;...
&lt;/p&gt;
&lt;p&gt;
With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training. In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also effective in boost
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#35299;&#20915;&#20102;Wasserstein GAN&#20013;&#20998;&#21306;&#20989;&#25968;&#26377;&#20559;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#35206;&#30422;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#26469;&#23454;&#29616;&#23545;&#20998;&#21306;&#20989;&#25968;&#30340;&#26080;&#20559;&#20272;&#35745;&#21644;&#29983;&#25104;&#22120;&#29109;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.09882</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adversarial Likelihood Estimation with One-way Flows. (arXiv:2307.09882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#35299;&#20915;&#20102;Wasserstein GAN&#20013;&#20998;&#21306;&#20989;&#25968;&#26377;&#20559;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#35206;&#30422;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#26469;&#23454;&#29616;&#23545;&#20998;&#21306;&#20989;&#25968;&#30340;&#26080;&#20559;&#20272;&#35745;&#21644;&#29983;&#25104;&#22120;&#29109;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#20294;&#26080;&#27861;&#25552;&#20379;&#26679;&#26412;&#21608;&#22260;&#30340;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#27880;&#24847;&#21040;&#22312;&#33021;&#37327;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#26368;&#22823;&#21270;&#23545;&#25968;&#20284;&#28982;&#21487;&#20197;&#23548;&#33268;&#21028;&#21035;&#22120;&#25552;&#20379;&#38750;&#24402;&#19968;&#21270;&#30340;&#23494;&#24230;&#65288;&#36890;&#24120;&#31216;&#20026;&#33021;&#37327;&#65289;&#30340;&#23545;&#25239;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;&#36825;&#19968;&#35266;&#28857;&#65292;&#32467;&#21512;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#19979;&#20869;&#23481;&#65306;1&#65289;Wasserstein GAN&#23545;&#20998;&#21306;&#20989;&#25968;&#36827;&#34892;&#20102;&#26377;&#20559;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26080;&#20559;&#20272;&#35745;&#26041;&#27861;&#65307;2&#65289;&#22312;&#26368;&#20248;&#21270;&#20284;&#28982;&#26102;&#65292;&#24517;&#39035;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#12290;&#36825;&#34987;&#20551;&#35774;&#20250;&#25552;&#20379;&#26356;&#22909;&#30340;&#27169;&#24335;&#35206;&#30422;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#26126;&#30830;&#35745;&#31639;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#12290;&#36825;&#26159;&#35774;&#35745;&#26080;&#20559;&#20272;&#35745;&#20998;&#21306;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#29983;&#25104;&#22120;&#29109;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#29983;&#25104;&#23494;&#24230;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#30340;&#27969;&#32593;&#32476;&#26469;&#33719;&#24471;&#30340;&#65292;&#31216;&#20026;&#21333;&#21521;&#27969;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26397;&#30528;&#21487;&#35299;&#37322;&#30340;AI&#22312;&#31227;&#21160;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21453;&#20107;&#23454;&#26469;&#23398;&#20064;&#20174;&#23494;&#38598;&#36712;&#36857;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08461</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#35299;&#37322;&#30340;AI&#22312;&#31227;&#21160;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards eXplainable AI for Mobility Data Science. (arXiv:2307.08461v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26397;&#30528;&#21487;&#35299;&#37322;&#30340;AI&#22312;&#31227;&#21160;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21453;&#20107;&#23454;&#26469;&#23398;&#20064;&#20174;&#23494;&#38598;&#36712;&#36857;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#30340;&#20851;&#20110;&#21487;&#35299;&#37322;AI&#22312;&#31227;&#21160;&#25968;&#25454;&#31185;&#23398;&#24212;&#29992;&#20013;&#30340;&#24037;&#20316;&#65292;&#37325;&#28857;&#26159;&#33021;&#22815;&#20174;&#23494;&#38598;&#36712;&#36857;&#25968;&#25454;&#65288;&#22914;&#36710;&#36742;&#21644;&#33337;&#21482;&#30340;GPS&#36712;&#36857;&#65289;&#20013;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#20351;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;GeoXAI&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21487;&#29702;&#35299;&#35299;&#37322;&#30340;&#38656;&#27714;&#65292;&#24182;&#21246;&#30011;&#20986;&#20102;&#26397;&#30528;&#21487;&#35299;&#37322;&#30340;AI&#22312;&#31227;&#21160;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#30740;&#31350;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our ongoing work towards XAI for Mobility Data Science applications, focusing on explainable models that can learn from dense trajectory data, such as GPS tracks of vehicles and vessels using temporal graph neural networks (GNNs) and counterfactuals. We review the existing GeoXAI studies, argue the need for comprehensible explanations with human-centered approaches, and outline a research path toward XAI for Mobility Data Science.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Rad-ReStruct&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;X&#20809;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#24418;&#24335;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#12289;&#25353;&#23618;&#27425;&#25490;&#24207;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;hi-VQA&#65292;&#23558;&#32467;&#26500;&#21270;&#25253;&#21578;&#20219;&#21153;&#24314;&#27169;&#20026;&#20998;&#23618;&#35270;&#35273;&#38382;&#31572;(VQA)&#65292;&#24182;&#32771;&#34385;&#20808;&#21069;&#25552;&#38382;&#21644;&#22238;&#31572;&#30340;&#19978;&#19979;&#25991;&#26469;&#22635;&#20805;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#23454;&#39564;&#35777;&#26126;hi-VQA&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05766</link><description>&lt;p&gt;
Rad-ReStruct: &#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;VQA&#22522;&#20934;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting. (arXiv:2307.05766v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Rad-ReStruct&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;X&#20809;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#24418;&#24335;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#12289;&#25353;&#23618;&#27425;&#25490;&#24207;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;hi-VQA&#65292;&#23558;&#32467;&#26500;&#21270;&#25253;&#21578;&#20219;&#21153;&#24314;&#27169;&#20026;&#20998;&#23618;&#35270;&#35273;&#38382;&#31572;(VQA)&#65292;&#24182;&#32771;&#34385;&#20808;&#21069;&#25552;&#38382;&#21644;&#22238;&#31572;&#30340;&#19978;&#19979;&#25991;&#26469;&#22635;&#20805;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#23454;&#39564;&#35777;&#26126;hi-VQA&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#26159;&#25918;&#23556;&#31185;&#21307;&#29983;&#19982;&#20854;&#20182;&#21307;&#21153;&#20154;&#21592;&#20043;&#38388;&#27807;&#36890;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#20294;&#20854;&#21487;&#33021;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#20854;&#20013;&#19968;&#31181;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#30340;&#26041;&#27861;&#26159;&#32467;&#26500;&#21270;&#25253;&#21578;&#65292;&#23427;&#27604;&#33258;&#30001;&#25991;&#26412;&#25253;&#21578;&#26356;&#33410;&#32422;&#26102;&#38388;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33258;&#21160;&#21270;&#32467;&#26500;&#21270;&#25253;&#21578;&#30340;&#30740;&#31350;&#26377;&#38480;&#65292;&#24182;&#19988;&#30446;&#21069;&#27809;&#26377;&#20844;&#24320;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Rad-ReStruct&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#12289;&#25353;&#23618;&#27425;&#25490;&#24207;&#30340;X&#20809;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#24418;&#24335;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#23558;&#32467;&#26500;&#21270;&#25253;&#21578;&#20219;&#21153;&#24314;&#27169;&#20026;&#20998;&#23618;&#35270;&#35273;&#38382;&#31572;(VQA)&#65292;&#24182;&#25552;&#20986;&#20102;hi-VQA&#65292;&#19968;&#31181;&#32771;&#34385;&#20808;&#21069;&#25552;&#38382;&#21644;&#22238;&#31572;&#30340;&#19978;&#19979;&#25991;&#20197;&#22635;&#20805;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;hi-VQA&#22312;&#21307;&#23398;VQA&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiology reporting is a crucial part of the communication between radiologists and other medical professionals, but it can be time-consuming and error-prone. One approach to alleviate this is structured reporting, which saves time and enables a more accurate evaluation than free-text reports. However, there is limited research on automating structured reporting, and no public benchmark is available for evaluating and comparing different methods. To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that provides fine-grained, hierarchically ordered annotations in the form of structured reports for X-Ray images. We model the structured reporting task as hierarchical visual question answering (VQA) and propose hi-VQA, a novel method that considers prior context in the form of previously asked questions and answers for populating a structured radiology report. Our experiments show that hi-VQA achieves competitive performance to the state-of-the-art on the medical VQA benc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.13596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#36793;&#32536;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#24778;&#20154;&#25104;&#21151;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#32972;&#21518;&#30340;&#29702;&#35770;&#21407;&#21017;&#23578;&#19981;&#28165;&#26970;&#65292;&#29305;&#21035;&#26159;&#23427;&#30340;&#38750;&#20984;&#20248;&#21270;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21019;&#24615;&#30340;softmax-attention&#27169;&#22411;$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$&#65292;&#20854;&#20013;$\boldsymbol{X}$&#26159;&#26631;&#35760;&#24207;&#21015;&#65292;$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$&#26159;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$\boldsymbol{p}$&#25110;&#31561;&#20215;&#30340;$\boldsymbol{W}$&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#20250;&#27839;&#30528;&#26041;&#21521;&#25910;&#25947;&#21040;&#20998;&#38548;&#8220;&#23616;&#37096;&#26368;&#20248;&#8221;&#26631;&#35760;&#21644;&#8220;&#38750;&#26368;&#20248;&#8221;&#26631;&#35760;&#30340;&#26368;&#22823;&#36793;&#32536;&#35299;&#12290;&#36825;&#26126;&#30830;&#22320;&#24418;&#24335;&#21270;&#20102;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#31181;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#33324;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#23884;&#20837;$\boldsymbol{Xv}$&#21644;$\texttt{softmax}(\boldsymbol{XWp})$&#31934;&#32454;&#22320;&#34920;&#24449;&#26631;&#35760;&#30340;&#8220;&#26368;&#20248;&#24615;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#23545;&#40784;&#21040;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#65292;&#33021;&#22815;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.00526</link><description>&lt;p&gt;
&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#38646;&#26679;&#26412;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#25351;&#23548;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering. (arXiv:2306.00526v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#23545;&#40784;&#21040;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#65292;&#33021;&#22815;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24067;&#23616;&#24863;&#30693;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#39044;&#35757;&#32451;&#21644;&#20219;&#21153;&#24494;&#35843;&#23545;&#20110;&#39069;&#22806;&#30340;&#35270;&#35273;&#12289;&#24067;&#23616;&#21644;&#20219;&#21153;&#27169;&#22359;&#38459;&#27490;&#20102;&#20854;&#30452;&#25509;&#21033;&#29992;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#26368;&#36817;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#28508;&#21147;&#12290;&#19982;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#39046;&#22495;&#23545;&#40784;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#25991;&#26723;&#22270;&#20687;&#38382;&#31572;&#19982;&#29616;&#25104;&#30340;&#25351;&#23548;&#35843;&#20248;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;&#65292;&#21033;&#29992;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24067;&#23616;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25351;&#23548;&#25552;&#31034;&#27169;&#22411;&#65292;&#31216;&#20026;LATIN-Prompt&#65292;&#23427;&#21253;&#25324;&#24067;&#23616;&#24863;&#30693;&#30340;&#25991;&#26723;&#20869;&#23481;&#21644;&#20219;&#21153;&#24863;&#30693;&#30340;&#25551;&#36848;&#12290;&#21069;&#32773;&#36890;&#36807;&#36866;&#24403;&#30340;&#31354;&#26684;&#21644;&#25442;&#34892;&#31526;&#20174;OCR&#24037;&#20855;&#20013;&#24674;&#22797;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#24067;&#23616;&#20449;&#24687;&#12290;&#21518;&#32773;&#30830;&#20445;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#20219;&#21153;&#38656;&#27714;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pre-training-fine-tuning paradigm based on layout-aware multimodal pre-trained models has achieved significant progress on document image question answering. However, domain pre-training and task fine-tuning for additional visual, layout, and task modules prevent them from directly utilizing off-the-shelf instruction-tuning language foundation models, which have recently shown promising potential in zero-shot learning. Contrary to aligning language models to the domain of document image question answering, we align document image question answering to off-the-shell instruction-tuning language foundation models to utilize their zero-shot capability. Specifically, we propose layout and task aware instruction prompt called LATIN-Prompt, which consists of layout-aware document content and task-aware descriptions. The former recovers the layout information among text segments from OCR tools by appropriate spaces and line breaks. The latter ensures that the model generates answers that m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AceCoder&#30340;&#20195;&#30721;&#29983;&#25104;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#19982;&#26631;&#20934;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#27604;&#65292;&#23427;&#36890;&#36807;&#31034;&#20363;&#26816;&#32034;&#21644;&#24341;&#23548;&#20195;&#30721;&#29983;&#25104;&#26469;&#25552;&#39640;&#29983;&#25104;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17780</link><description>&lt;p&gt;
&#12298;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Enhancing In-Context Learning for Code Generation. (arXiv:2303.17780v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AceCoder&#30340;&#20195;&#30721;&#29983;&#25104;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#19982;&#26631;&#20934;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#27604;&#65292;&#23427;&#36890;&#36807;&#31034;&#20363;&#26816;&#32034;&#21644;&#24341;&#23548;&#20195;&#30721;&#29983;&#25104;&#26469;&#25552;&#39640;&#29983;&#25104;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#32463;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#25104;&#21151;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26080;&#38656;&#35757;&#32451;&#65292;&#27169;&#22411;&#21482;&#38656;&#35201;&#36755;&#20837;&#19968;&#20010;&#30001;&#23569;&#37327;&#38656;&#27714;-&#20195;&#30721;&#31034;&#20363;&#21644;&#19968;&#20010;&#26032;&#38656;&#27714;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#23601;&#33021;&#29983;&#25104;&#20986;&#26032;&#30340;&#31243;&#24207;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#20165;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65292;&#24573;&#30053;&#20102;&#20195;&#30721;&#29983;&#25104;&#30340;&#29420;&#29305;&#29305;&#24615;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#30740;&#31350;&#20026;&#26631;&#20934;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20154;&#31867;&#32534;&#30721;&#36807;&#31243;&#30340;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21517;&#20026;AceCoder&#30340;&#20195;&#30721;&#29983;&#25104;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#26631;&#20934;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#27604;&#65292;AceCoder&#26377;&#20004;&#20010;&#26032;&#39062;&#20043;&#22788;&#12290;(1)&#31034;&#20363;&#26816;&#32034;&#12290;&#23427;&#26816;&#32034;&#31867;&#20284;&#31243;&#24207;&#20316;&#20026;&#31034;&#20363;&#65292;&#24182;&#20174;&#20013;&#23398;&#20064;&#32534;&#31243;&#25216;&#33021;(&#22914;&#31639;&#27861;&#12289;API)&#12290;(2)&#24341;&#23548;&#20195;&#30721;&#29983;&#25104;&#12290;&#23427;&#40723;&#21169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#38388;&#39044;&#22791;&#20195;&#30721;(&#22914;&#27979;&#35797;&#29992;&#20363;&#12289;API)&#24182;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#38656;&#27714;&#21644;&#25351;&#23548;&#19979;&#19968;&#27493;&#20195;&#30721;&#29983;&#25104;&#12290;&#25105;&#20204;&#23558;AceCoder&#24212;&#29992;&#21040;&#22823;&#37327;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#30456;&#27604;&#65292;AceCoder&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) with pre-trained language models (PTLMs) has shown great success in code generation. ICL does not require training. PTLMs take as the input a prompt consisting of a few requirement-code examples and a new requirement, and output a new program. However, existing studies simply reuse ICL techniques for natural language generation and ignore unique features of code generation. We refer to these studies as standard ICL.  Inspired by observations of the human coding process, we propose a novel ICL approach for code generation named AceCoder. Compared to standard ICL, AceCoder has two novelties. (1) Example retrieval. It retrieves similar programs as examples and learns programming skills (e.g., algorithms, APIs) from them. (2) Guided Code Generation. It encourages PTLMs to output an intermediate preliminary (e.g., test cases, APIs) before generating programs. The preliminary can help PTLMs understand requirements and guide the next code generation. We apply AceCode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.13592</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#25552;&#31034;&#65306;&#19996;&#21335;&#20122;&#35821;&#35328;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28151;&#21512;&#20195;&#30721;&#22312;&#19990;&#30028;&#35768;&#22810;&#22320;&#21306;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#23454;&#36341;&#65292;&#20294;&#25910;&#38598;&#39640;&#36136;&#37327;&#19988;&#20302;&#25104;&#26412;&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#20173;&#28982;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#36843;&#20351;&#20154;&#20204;&#38382;&#65306;&#36825;&#20123;&#31995;&#32479;&#33021;&#29992;&#20110;&#25968;&#25454;&#29983;&#25104;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#38646;-shot&#30340;&#26041;&#24335;&#19979;&#22914;&#20309;&#25552;&#31034;LLMs&#20026;&#19996;&#21335;&#20122;&#65288;SEA&#65289;&#30340;&#20116;&#31181;&#35821;&#35328;&#65288;&#21360;&#23612;&#35821;&#65292;&#39532;&#26469;&#35821;&#65292;&#20013;&#25991;&#65292;&#22612;&#21152;&#36335;&#35821;&#65292;&#36234;&#21335;&#35821;&#65289;&#21450;&#20811;&#37324;&#22885;&#23572;&#35821;S ingl ish&#21019;&#36896;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#26174;&#31034;&#20986;&#26368;&#22823;&#30340;&#28508;&#21147;&#65292;&#24403;&#26126;&#30830;&#23450;&#20041;&#8220;&#28151;&#21512;&#20195;&#30721;&#8221;&#26415;&#35821;&#26102;&#65292;&#33021;&#22815;68%&#30340;&#26102;&#38388;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;ChatGPT&#21644;InstructGPT&#65288;davinci-003&#65289;&#29983;&#25104;S ingl ish&#25991;&#26412;&#30340;&#34920;&#29616;&#20063;&#20540;&#24471;&#27880;&#24847;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#25552;&#31034;&#19979;&#30340;&#25104;&#21151;&#29575;&#24179;&#22343;&#20026;96%&#12290;&#20294;&#26159;&#65292;ChatGPT&#21644;InstructGPT&#30340;&#28151;&#21512;&#20195;&#30721;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#35821;&#20041;&#19981;&#27491;&#30830;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The proliferation of Large Language Models (LLMs) in recent times compels one to ask: can these systems be used for data generation? In this article, we explore prompting LLMs in a zero-shot manner to create code-mixed data for five languages in South East Asia (SEA) -Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language Singlish. We find that ChatGPT shows the most potential, capable of producing code-mixed text 68% of the time when the term "code-mixing" is explicitly defined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in generating Singlish texts are noteworthy, averaging a 96% success rate across a variety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT, however, is dampened by word choice errors that lead to semant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550; RegionLight&#65292;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#23558;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#27599;&#20010;&#21306;&#22495;&#20013;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#25193;&#23637;&#20102;BDQ&#26041;&#27861;&#20026;DBDQ&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.11899</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22823;&#35268;&#27169;&#26684;&#32593;&#20132;&#36890;&#32593;&#32476;&#21306;&#22495;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement Learning for Regional Signal control in Large-scale Grid Traffic network. (arXiv:2303.11899v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550; RegionLight&#65292;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#23558;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#27599;&#20010;&#21306;&#22495;&#20013;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#25193;&#23637;&#20102;BDQ&#26041;&#27861;&#20026;DBDQ&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#33258;&#36866;&#24212;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26159;&#24403;&#21069;&#38750;&#24120;&#27969;&#34892;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#25511;&#21046;&#21333;&#20010;&#36335;&#21475;&#65292;&#36825;&#20123;&#26041;&#27861;&#20391;&#37325;&#20110;&#36335;&#21475;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;MARL&#30340;&#38750;&#31283;&#24577;&#24615;&#36136;&#38543;&#30528;&#20132;&#36890;&#32593;&#32476;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20173;&#28982;&#38480;&#21046;&#30528;&#19978;&#36848;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#19968;&#31181;&#22949;&#21327;&#30340;&#31574;&#30053;&#26159;&#23558;&#19968;&#21517;&#26234;&#33021;&#20307;&#20998;&#37197;&#21040;&#19968;&#32452;&#36335;&#21475;&#20013;&#65292;&#20197;&#20943;&#23569;&#26234;&#33021;&#20307;&#25968;&#37327;&#12290;&#36825;&#31181;&#31574;&#30053;&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#65292;&#19968;&#20010;&#26159;&#22914;&#20309;&#23558;&#20132;&#36890;&#32593;&#32476;&#21010;&#20998;&#25104;&#23567;&#21306;&#22495;&#65292;&#21478;&#19968;&#20010;&#26159;&#22914;&#20309;&#25628;&#32034;&#21306;&#22495;&#20869;&#30340;&#26368;&#20248;&#32852;&#21512;&#21160;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;RegionLight&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#21306;&#22495;&#21010;&#20998;&#35268;&#21017;&#22522;&#20110;&#20132;&#21449;&#21475;&#20043;&#38388;&#30340;&#37051;&#25509;&#20851;&#31995;&#65292;&#24182;&#25193;&#23637;&#20102;Branching Dueling Q-Network(BDQ)&#12290;&#35813;&#26041;&#27861;&#23558;BDQ&#36827;&#19968;&#27493;&#20248;&#21270;&#20026;Dynamic Branching Dueling Q-Network(DBDQ)&#65292;&#20197;&#38480;&#21046;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#30340;&#22686;&#38271;&#24182;&#32531;&#35299;&#26234;&#33021;&#20307;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive traffic signal control with Multi-agent Reinforcement Learning(MARL) is a very popular topic nowadays. In most existing novel methods, one agent controls single intersections and these methods focus on the cooperation between intersections. However, the non-stationary property of MARL still limits the performance of the above methods as the size of traffic networks grows. One compromised strategy is to assign one agent with a region of intersections to reduce the number of agents. There are two challenges in this strategy, one is how to partition a traffic network into small regions and the other is how to search for the optimal joint actions for a region of intersections. In this paper, we propose a novel training framework RegionLight where our region partition rule is based on the adjacency between the intersection and extended Branching Dueling Q-Network(BDQ) to Dynamic Branching Dueling Q-Network(DBDQ) to bound the growth of the size of joint action space and alleviate th
&lt;/p&gt;</description></item><item><title>Internet Explorer&#26159;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#20114;&#32852;&#32593;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#22312;&#32593;&#32476;&#19978;&#25628;&#32034;&#30456;&#20851;&#22270;&#20687;&#24182;&#35757;&#32451;&#23567;&#35268;&#27169;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.14051</link><description>&lt;p&gt;
Internet Explorer:&#24320;&#25918;&#32593;&#32476;&#19978;&#30340;&#26377;&#38024;&#23545;&#24615;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Internet Explorer: Targeted Representation Learning on the Open Web. (arXiv:2302.14051v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14051
&lt;/p&gt;
&lt;p&gt;
Internet Explorer&#26159;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#20114;&#32852;&#32593;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#22312;&#32593;&#32476;&#19978;&#25628;&#32034;&#30456;&#20851;&#22270;&#20687;&#24182;&#35757;&#32451;&#23567;&#35268;&#27169;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#22312;&#22823;&#22411;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#36890;&#29992;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#20123;&#36890;&#29992;&#27169;&#22411;&#21482;&#33021;&#25429;&#25417;&#21040;&#23427;&#20204;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#30693;&#35782;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#38598;&#21482;&#26159;&#20114;&#32852;&#32593;&#30340;&#24494;&#23567;&#12289;&#36807;&#26102;&#30340;&#24555;&#29031;&#8212;&#8212;&#32780;&#20114;&#32852;&#32593;&#19978;&#27599;&#22825;&#37117;&#19978;&#20256;&#25968;&#21313;&#20159;&#24352;&#22270;&#29255;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65306;&#19981;&#26159;&#24076;&#26395;&#25105;&#20204;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21518;&#33021;&#22815;&#36716;&#31227;&#21040;&#25105;&#20204;&#26399;&#26395;&#30340;&#20219;&#21153;&#19978;&#65292;&#32780;&#26159;&#25105;&#20204;&#24314;&#35758;&#21160;&#24577;&#21033;&#29992;&#20114;&#32852;&#32593;&#65292;&#24555;&#36895;&#35757;&#32451;&#19968;&#20010;&#22312;&#24403;&#21069;&#20219;&#21153;&#19978;&#34920;&#29616;&#38750;&#24120;&#22909;&#30340;&#23567;&#35268;&#27169;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21483;&#20570;Internet Explorer&#65292;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#22312;&#32593;&#32476;&#19978;&#25506;&#32034;&#65292;&#36880;&#27493;&#25214;&#21040;&#25913;&#21892;&#25152;&#38656;&#30446;&#26631;&#25968;&#25454;&#38598;&#24615;&#33021;&#30340;&#30456;&#20851;&#31034;&#20363;&#12290;&#23427;&#24490;&#29615;&#22312;&#20114;&#32852;&#32593;&#19978;&#25628;&#32034;&#24102;&#26377;&#25991;&#26412;&#26597;&#35810;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#19979;&#36733;&#30340;&#22270;&#20687;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#65292;&#30830;&#23450;&#21738;&#20123;&#22270;&#20687;&#26159;&#26377;&#29992;&#30340;&#65292;&#24182;&#30830;&#23450;&#19979;&#19968;&#27493;&#35201;&#25628;&#32034;&#30340;&#20248;&#20808;&#32423;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Internet Explorer&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern vision models typically rely on fine-tuning general-purpose models pre-trained on large, static datasets. These general-purpose models only capture the knowledge within their pre-training datasets, which are tiny, out-of-date snapshots of the Internet -- where billions of images are uploaded each day. We suggest an alternate approach: rather than hoping our static datasets transfer to our desired tasks after large-scale pre-training, we propose dynamically utilizing the Internet to quickly train a small-scale model that does extremely well on the task at hand. Our approach, called Internet Explorer, explores the web in a self-supervised manner to progressively find relevant examples that improve performance on a desired target dataset. It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next. We evaluate Internet Explorer across several d
&lt;/p&gt;</description></item><item><title>LB-SimTSC&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#30456;&#20284;&#24615;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;LB_Keogh&#20316;&#20026;DTW&#30340;&#19979;&#30028;&#26469;&#35299;&#20915;DTW&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#65292;&#22312;&#23569;&#26631;&#31614;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.04838</link><description>&lt;p&gt;
LB-SimTSC: &#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#39640;&#25928;&#30456;&#20284;&#24615;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LB-SimTSC: An Efficient Similarity-Aware Graph Neural Network for Semi-Supervised Time Series Classification. (arXiv:2301.04838v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04838
&lt;/p&gt;
&lt;p&gt;
LB-SimTSC&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#30456;&#20284;&#24615;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;LB_Keogh&#20316;&#20026;DTW&#30340;&#19979;&#30028;&#26469;&#35299;&#20915;DTW&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#65292;&#22312;&#23569;&#26631;&#31614;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#25968;&#25454;&#25366;&#25496;&#20219;&#21153;&#65292;&#22312;&#36807;&#21435;&#30340;20&#24180;&#37324;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#30001;&#20110;&#23454;&#38469;&#20013;&#26631;&#31614;&#31232;&#32570;&#65292;&#21482;&#26377;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21464;&#24471;&#27969;&#34892;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#30456;&#20284;&#24615;&#24863;&#30693;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;SimTSC&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#25209;&#37327;&#25968;&#25454;&#30340;&#20004;&#20004;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#36317;&#31163;&#29983;&#25104;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#27169;&#22411;&#12290;SimTSC&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;&#20960;&#20010;&#23569;&#26631;&#31614;&#35774;&#32622;&#20013;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SimTSC&#20381;&#36182;&#20110;&#20004;&#20004;DTW&#36317;&#31163;&#65292;DTW&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#23427;&#21482;&#33021;&#22312;&#36866;&#24230;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#25216;&#26415;LB-SimTSC&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#26032;&#30340;&#22270;&#26500;&#24314;&#27169;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;DTW&#30340;&#19979;&#30028;LB_Keogh&#26469;&#36817;&#20284;DTW&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series classification is an important data mining task that has received a lot of interest in the past two decades. Due to the label scarcity in practice, semi-supervised time series classification with only a few labeled samples has become popular. Recently, Similarity-aware Time Series Classification (SimTSC) is proposed to address this problem by using a graph neural network classification model on the graph generated from pairwise Dynamic Time Warping (DTW) distance of batch data. It shows excellent accuracy and outperforms state-of-the-art deep learning models in several few-label settings. However, since SimTSC relies on pairwise DTW distances, the quadratic complexity of DTW limits its usability to only reasonably sized datasets. To address this challenge, we propose a new efficient semi-supervised time series classification technique, LB-SimTSC, with a new graph construction module. Instead of using DTW, we propose to utilize a lower bound of DTW, LB_Keogh, to approximate 
&lt;/p&gt;</description></item><item><title>BigText-QA&#24341;&#20837;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;QA&#26041;&#27861;&#65292;&#33021;&#22815;&#22522;&#20110;&#26377;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#30693;&#35782;&#22270;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.05798</link><description>&lt;p&gt;
BigText-QA&#65306;&#22522;&#20110;&#22823;&#35268;&#27169;&#28151;&#21512;&#30693;&#35782;&#22270;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
BigText-QA: Question Answering over a Large-Scale Hybrid Knowledge Graph. (arXiv:2212.05798v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05798
&lt;/p&gt;
&lt;p&gt;
BigText-QA&#24341;&#20837;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;QA&#26041;&#27861;&#65292;&#33021;&#22815;&#22522;&#20110;&#26377;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#30693;&#35782;&#22270;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#36164;&#28304;&#19978;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#20013;&#22810;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#24494;&#22937;&#20851;&#31995;&#26102;&#12290;&#20026;&#27492;&#65292;&#20687;YAGO&#12289;DBpedia&#12289;Freebase&#21644;Wikidata&#36825;&#26679;&#30340;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#24191;&#27867;&#20351;&#29992;&#24182;&#24471;&#21040;&#20102;&#35748;&#21487;&#65292;&#29992;&#20110;&#38382;&#31572;&#65288;QA&#65289;&#24212;&#29992;&#12290;&#34429;&#28982;&#36825;&#20123;&#30693;&#35782;&#24211;&#25552;&#20379;&#20102;&#32467;&#26500;&#21270;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#20294;&#32570;&#20047;&#33258;&#28982;&#35821;&#35328;&#26469;&#28304;&#20013;&#30340;&#19978;&#19979;&#25991;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;BigText-QA&#24341;&#20837;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;QA&#26041;&#27861;&#65292;&#33021;&#22815;&#22522;&#20110;&#26356;&#20887;&#20313;&#30340;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#22238;&#31572;&#38382;&#39064;&#65292;&#35813;&#22270;&#23558;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#65288;&#21363;&#8220;&#28151;&#21512;&#8221;&#65289;&#30693;&#35782;&#20197;&#32479;&#19968;&#30340;&#22270;&#24418;&#34920;&#31034;&#26041;&#24335;&#32452;&#32455;&#12290;&#22240;&#27492;&#65292;BigText-QA&#33021;&#22815;&#20860;&#39038;&#20004;&#20010;&#19990;&#30028;&#30340;&#20248;&#21183;&#8212;&#8212;&#19968;&#20010;&#32463;&#20856;&#30340;&#21629;&#21517;&#23454;&#20307;&#38598;&#21512;&#65292;&#26144;&#23556;&#21040;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#32972;&#26223;&#30693;&#35782;&#24211;&#65288;&#22914;YAGO&#25110;Wikidata&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#20174;&#33258;&#28982;&#35821;&#35328;&#26469;&#28304;&#20013;&#33719;&#21462;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering complex questions over textual resources remains a challenge, particularly when dealing with nuanced relationships between multiple entities expressed within natural-language sentences. To this end, curated knowledge bases (KBs) like YAGO, DBpedia, Freebase, and Wikidata have been widely used and gained great acceptance for question-answering (QA) applications in the past decade. While these KBs offer a structured knowledge representation, they lack the contextual diversity found in natural-language sources. To address this limitation, BigText-QA introduces an integrated QA approach, which is able to answer questions based on a more redundant form of a knowledge graph (KG) that organizes both structured and unstructured (i.e., "hybrid") knowledge in a unified graphical representation. Thereby, BigText-QA is able to combine the best of both worlds$\unicode{x2013}$a canonical set of named entities, mapped to a structured background KB (such as YAGO or Wikidata), as well as an o
&lt;/p&gt;</description></item><item><title>CodeEditor&#26159;&#19968;&#20010;&#39044;&#35757;&#32451;&#20195;&#30721;&#32534;&#36753;&#27169;&#22411;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#30340;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#20195;&#30721;&#32534;&#36753;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.17040</link><description>&lt;p&gt;
CodeEditor: &#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#32534;&#36753;&#28304;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
CodeEditor: Learning to Edit Source Code with Pre-trained Models. (arXiv:2210.17040v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17040
&lt;/p&gt;
&lt;p&gt;
CodeEditor&#26159;&#19968;&#20010;&#39044;&#35757;&#32451;&#20195;&#30721;&#32534;&#36753;&#27169;&#22411;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#30340;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#20195;&#30721;&#32534;&#36753;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#38656;&#35201;&#36827;&#34892;&#37325;&#22797;&#30340;&#20195;&#30721;&#32534;&#36753;&#27963;&#21160;&#65292;&#20363;&#22914;&#20195;&#30721;&#37325;&#26500;&#31561;&#12290;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#32534;&#36753;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#39318;&#20808;&#20351;&#29992;&#39044;&#35757;&#32451;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#20027;&#35201;&#26159;&#20195;&#30721;&#22635;&#20805;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65289;&#65292;&#36825;&#20123;&#20219;&#21153;&#26469;&#33258;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#19981;&#36866;&#29992;&#20110;&#33258;&#21160;&#20195;&#30721;&#32534;&#36753;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20195;&#30721;&#32534;&#36753;&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CodeEditor&#30340;&#26377;&#25928;&#39044;&#35757;&#32451;&#20195;&#30721;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20195;&#30721;&#32534;&#36753;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#30495;&#23454;&#20195;&#30721;&#29255;&#27573;&#20316;&#20026;&#21442;&#32771;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#24378;&#22823;&#30340;&#29983;&#25104;&#22120;&#23558;&#23427;&#20204;&#37325;&#20889;&#25104;&#21464;&#24322;&#29256;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;CodeEditor&#23545;&#21464;&#24322;&#29256;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23558;&#20854;&#32534;&#36753;&#20026;&#27491;&#30830;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developers often perform repetitive code editing activities for various reasons (e.g., code refactoring) during software development. Pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing.  This paper proposes a novel pre-training task specialized in code editing and presents an effective pre-trained code editing model named CodeEditor. Our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect lots of real-world code snippets as the ground truth and use a powerful generator to rewrite them into mutated versions. Then, we pre-train our CodeEditor to edit mutated versions into the c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;NeurVec&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20462;&#27491;&#22120;&#65292;&#23427;&#33021;&#22815;&#34917;&#20607;&#38598;&#25104;&#35823;&#24046;&#24182;&#22312;&#27169;&#25311;&#20013;&#23454;&#29616;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;NeurVec&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21152;&#36895;&#20102;&#20256;&#32479;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.03680</link><description>&lt;p&gt;
&#36890;&#36807;NeurVec&#21152;&#36895;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec. (arXiv:2208.03680v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;NeurVec&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20462;&#27491;&#22120;&#65292;&#23427;&#33021;&#22815;&#34917;&#20607;&#38598;&#25104;&#35823;&#24046;&#24182;&#22312;&#27169;&#25311;&#20013;&#23454;&#29616;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;NeurVec&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21152;&#36895;&#20102;&#20256;&#32479;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#27169;&#25311;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#22312;&#20272;&#35745;&#31215;&#20998;&#26102;&#30001;&#20110;&#27493;&#38271;&#36873;&#25321;&#30340;&#38480;&#21046;&#65292;&#23384;&#22312;&#30528;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20462;&#27491;&#22120;&#65292;&#31216;&#20026;NeurVec&#65292;&#23427;&#21487;&#20197;&#34917;&#20607;&#38598;&#25104;&#35823;&#24046;&#24182;&#22312;&#27169;&#25311;&#20013;&#23454;&#29616;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#20351;&#29992;&#26377;&#38480;&#21644;&#31163;&#25955;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;NeurVec&#22312;&#36830;&#32493;&#30456;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;NeurVec&#26174;&#33879;&#21152;&#36895;&#20102;&#20256;&#32479;&#27714;&#35299;&#22120;&#65292;&#23454;&#29616;&#20102;&#20960;&#21313;&#21040;&#20960;&#30334;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;NeurVec&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35774;&#35745;&#32467;&#21512;&#20102;&#26131;&#20110;&#23454;&#29616;&#30340;&#29305;&#28857;&#65292;&#26377;&#28508;&#21147;&#24314;&#31435;&#36215;&#19968;&#20010;&#26032;&#30340;&#27714;&#35299;&#22120;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large-scale simulation of dynamical systems is critical in numerous scientific and engineering disciplines. However, traditional numerical solvers are limited by the choice of step sizes when estimating integration, resulting in a trade-off between accuracy and computational efficiency. To address this challenge, we introduce a deep learning-based corrector called Neural Vector (NeurVec), which can compensate for integration errors and enable larger time step sizes in simulations. Our extensive experiments on a variety of complex dynamical system benchmarks demonstrate that NeurVec exhibits remarkable generalization capability on a continuous phase space, even when trained using limited and discrete data. NeurVec significantly accelerates traditional solvers, achieving speeds tens to hundreds of times faster while maintaining high levels of accuracy and stability. Moreover, NeurVec's simple-yet-effective design, combined with its ease of implementation, has the potential to establi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#20892;&#19994;&#30340;&#33258;&#20027;&#22320;&#38754;&#26426;&#22120;&#20154;&#65292;&#20855;&#22791;&#26234;&#33021;&#24863;&#30693;&#21644;&#33258;&#20027;&#38500;&#33609;&#21151;&#33021;&#65292;&#24182;&#33021;&#25552;&#20379;&#26045;&#32933;&#12289;&#26432;&#34411;&#21058;&#31561;&#26381;&#21153;&#65292;&#20026;&#20316;&#29289;&#21644;&#22303;&#22756;&#20581;&#24247;&#30417;&#27979;&#25552;&#20379;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2208.01708</link><description>&lt;p&gt;
&#33258;&#20027;&#20892;&#19994;&#26426;&#22120;&#20154;&#29992;&#20110;&#26234;&#33021;&#20892;&#19994;
&lt;/p&gt;
&lt;p&gt;
Autonomous Agriculture Robot for Smart Farming. (arXiv:2208.01708v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#20892;&#19994;&#30340;&#33258;&#20027;&#22320;&#38754;&#26426;&#22120;&#20154;&#65292;&#20855;&#22791;&#26234;&#33021;&#24863;&#30693;&#21644;&#33258;&#20027;&#38500;&#33609;&#21151;&#33021;&#65292;&#24182;&#33021;&#25552;&#20379;&#26045;&#32933;&#12289;&#26432;&#34411;&#21058;&#31561;&#26381;&#21153;&#65292;&#20026;&#20316;&#29289;&#21644;&#22303;&#22756;&#20581;&#24247;&#30417;&#27979;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#26088;&#22312;&#24320;&#21457;&#21644;&#23637;&#31034;&#19968;&#31181;&#20855;&#22791;&#26234;&#33021;&#33021;&#21147;&#30340;&#22320;&#38754;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#36827;&#34892;&#19981;&#21516;&#20302;&#30702;&#20316;&#29289;&#30340;&#21322;&#33258;&#20027;&#20892;&#19994;&#25805;&#20316;&#65292;&#31216;&#20026;&#20892;&#19994;&#24212;&#29992;&#26426;&#22120;&#20154;&#65288;Agriculture Application Robot&#65292;AAR&#65289;&#12290;AAR&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22826;&#38451;&#33021;&#30005;&#21160;&#26426;&#22120;&#20154;&#65292;&#20351;&#29992;&#26234;&#33021;&#24863;&#30693;&#36827;&#34892;&#26893;&#29289;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#65292;&#36824;&#20855;&#22791;&#33258;&#20027;&#38500;&#33609;&#21151;&#33021;&#30340;&#26426;&#26800;&#33218;&#12290;&#35813;&#26426;&#22120;&#20154;&#21487;&#20197;&#21521;&#20316;&#29289;&#12289;&#26434;&#33609;&#21644;&#20854;&#20182;&#23475;&#34411;&#31561;&#30446;&#26631;&#25552;&#20379;&#26045;&#32933;&#12289;&#26432;&#34411;&#21058;&#12289;&#38500;&#33609;&#21058;&#21644;&#20854;&#20182;&#28082;&#20307;&#65292;&#21516;&#26102;&#20026;&#26410;&#26469;&#26356;&#39640;&#32423;&#20219;&#21153;&#65288;&#22914;&#20135;&#37327;&#20272;&#35745;&#12289;&#20316;&#29289;&#21644;&#22303;&#22756;&#20581;&#24247;&#30417;&#27979;&#65289;&#30340;&#30740;&#31350;&#25552;&#20379;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#20154;&#30340;&#35774;&#35745;&#20197;&#21450;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This project aims to develop and demonstrate a ground robot with intelligence capable of conducting semi-autonomous farm operations for different low-heights vegetable crops referred as Agriculture Application Robot(AAR). AAR is a lightweight, solar-electric powered robot that uses intelligent perception for conducting detection and classification of plants and their characteristics. The system also has a robotic arm for the autonomous weed cutting process. The robot can deliver fertilizer spraying, insecticide, herbicide, and other fluids to the targets such as crops, weeds, and other pests. Besides, it provides information for future research into higher-level tasks such as yield estimation, crop, and soil health monitoring. We present the design of robot and the associated experiments which show the promising results in real world environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#20026;&#27599;&#20010;&#36712;&#36857;&#27573;&#30340;&#36951;&#25022;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#21487;&#20197;&#26681;&#25454;&#36825;&#20123;&#36951;&#25022;&#29983;&#25104;&#30340;&#20559;&#22909;&#26469;&#35782;&#21035;&#29983;&#25104;&#36825;&#20123;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#36951;&#25022;&#20559;&#22909;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2206.02231</link><description>&lt;p&gt;
&#20154;&#31867;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#20559;&#22909;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Models of human preference for learning reward functions. (arXiv:2206.02231v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#20026;&#27599;&#20010;&#36712;&#36857;&#27573;&#30340;&#36951;&#25022;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#21487;&#20197;&#26681;&#25454;&#36825;&#20123;&#36951;&#25022;&#29983;&#25104;&#30340;&#20559;&#22909;&#26469;&#35782;&#21035;&#29983;&#25104;&#36825;&#20123;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#36951;&#25022;&#20559;&#22909;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29992;&#21463;&#38480;&#20110;&#22870;&#21169;&#20989;&#25968;&#19982;&#20154;&#31867;&#21033;&#30410;&#30340;&#19968;&#33268;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#23545;&#40784;&#26041;&#27861;&#26159;&#20174;&#20154;&#31867;&#29983;&#25104;&#30340;&#36712;&#36857;&#27573;&#23545;&#20043;&#38388;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#24120;&#20551;&#35774;&#36825;&#20123;&#20154;&#31867;&#20559;&#22909;&#20165;&#30001;&#37096;&#20998;&#22238;&#25253;&#26469;&#20915;&#23450;&#65292;&#21363;&#27599;&#20010;&#36712;&#36857;&#27573;&#19978;&#30340;&#22870;&#21169;&#24635;&#21644;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20551;&#35774;&#23384;&#22312;&#32570;&#38519;&#65292;&#25552;&#20986;&#23558;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#20026;&#30001;&#27599;&#20010;&#36712;&#36857;&#27573;&#30340;&#36951;&#25022;&#26469;&#20915;&#23450;&#65292;&#36951;&#25022;&#26159;&#19968;&#31181;&#34913;&#37327;&#36712;&#36857;&#27573;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#20559;&#31163;&#31243;&#24230;&#30340;&#24230;&#37327;&#12290;&#22312;&#26681;&#25454;&#36951;&#25022;&#29983;&#25104;&#30340;&#26080;&#31351;&#22810;&#20010;&#20559;&#22909;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#35782;&#21035;&#21040;&#19982;&#29983;&#25104;&#36825;&#20123;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#31561;&#20215;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20197;&#21069;&#30340;&#37096;&#20998;&#22238;&#25253;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#32570;&#20047;&#36825;&#31181;&#21487;&#35782;&#21035;&#24615;&#23646;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#36951;&#25022;&#20559;&#22909;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments, a type of reinforcement learning from human feedback (RLHF). These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling human preferences instead as informed by each segment's regret, a measure of a segment's deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences, and we prove that the previous partial return model lacks this identifiability property in multiple contexts. We empirically show that our proposed regret preference model outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22270;&#24418;&#24179;&#28369;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#20351;&#29992;&#36339;&#36291;&#36830;&#25509;&#21644;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2010.10274</link><description>&lt;p&gt;
&#22270;&#24418;&#24179;&#28369;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Fairing Convolutional Networks for Anomaly Detection. (arXiv:2010.10274v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.10274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22270;&#24418;&#24179;&#28369;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#20351;&#29992;&#36339;&#36291;&#36830;&#25509;&#21644;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#26159;&#35768;&#22810;&#22522;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#38750;&#24120;&#26377;&#25928;&#30340;&#24102;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#36880;&#23618;&#20256;&#25773;&#35268;&#21017;&#22312;&#29702;&#35770;&#19978;&#21463;&#21040;&#20960;&#20309;&#22788;&#29702;&#20013;&#38544;&#24335;&#24179;&#28369;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#21253;&#25324;&#29992;&#20110;&#32858;&#21512;&#26469;&#33258;&#30456;&#37051;&#33410;&#28857;&#30340;&#20449;&#24687;&#30340;&#22270;&#21367;&#31215;&#27169;&#22359;&#21644;&#29992;&#20110;&#32452;&#21512;&#36880;&#23618;&#37051;&#23621;&#34920;&#31034;&#30340;&#36339;&#36291;&#36830;&#25509;&#27169;&#22359;&#12290;&#36825;&#20010;&#20256;&#25773;&#35268;&#21017;&#26159;&#36890;&#36807;&#38597;&#21487;&#27604;&#26041;&#27861;&#20174;&#38544;&#24335;&#24179;&#28369;&#26041;&#31243;&#30340;&#36845;&#20195;&#35299;&#23548;&#20986;&#30340;&#12290;&#38500;&#20102;&#36890;&#36807;&#32593;&#32476;&#23618;&#20043;&#38388;&#30340;&#36339;&#36291;&#36830;&#25509;&#25429;&#33719;&#26469;&#33258;&#36828;&#31243;&#22270;&#33410;&#28857;&#30340;&#20449;&#24687;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21033;&#29992;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#26469;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#20123;&#36339;&#36291;&#36830;&#25509;&#26159;&#26681;&#25454;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#26550;&#26500;&#32463;&#36807;&#35774;&#35745;&#25972;&#21512;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolution is a fundamental building block for many deep neural networks on graph-structured data. In this paper, we introduce a simple, yet very effective graph convolutional network with skip connections for semi-supervised anomaly detection. The proposed layerwise propagation rule of our model is theoretically motivated by the concept of implicit fairing in geometry processing, and comprises a graph convolution module for aggregating information from immediate node neighbors and a skip connection module for combining layer-wise neighborhood representations. This propagation rule is derived from the iterative solution of the implicit fairing equation via the Jacobi method. In addition to capturing information from distant graph nodes through skip connections between the network's layers, our approach exploits both the graph structure and node features for learning discriminative node representations. These skip connections are integrated by design in our proposed network archi
&lt;/p&gt;</description></item></channel></rss>