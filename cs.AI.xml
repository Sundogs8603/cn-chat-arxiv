<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#37492;&#21035;&#24615;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#34920;&#31034;&#20013;&#36817;&#20284;&#35825;&#23548;&#28508;&#21464;&#37327;&#32467;&#26500;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01399</link><description>&lt;p&gt;
&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Model to explain Self-Supervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#37492;&#21035;&#24615;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#34920;&#31034;&#20013;&#36817;&#20284;&#35825;&#23548;&#28508;&#21464;&#37327;&#32467;&#26500;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#20363;&#22914;&#23545;&#35821;&#20041;&#30456;&#20851;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#22914;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#25110;&#27169;&#24577;&#26469;&#23398;&#20064;&#34920;&#31034;&#12290;&#22312;&#20247;&#22810;SSL&#26041;&#27861;&#20013;&#65292;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;SimCLR&#65292;CLIP&#21644;VicREG&#65289;&#22240;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#22312;&#19979;&#28216;&#24615;&#33021;&#19978;&#25509;&#36817;&#26377;&#30417;&#30563;&#23398;&#20064;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32972;&#21518;&#30340;&#26426;&#21046;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#34920;&#31034;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#20102;&#20960;&#31867;&#20855;&#26377;&#37492;&#21035;&#24615;&#30340;&#33258;&#30417;&#30563;&#31639;&#27861;&#65288;&#21253;&#25324;&#23545;&#27604;&#26041;&#27861;&#65289;&#36817;&#20284;&#35825;&#23548;&#20854;&#34920;&#31034;&#20013;&#30340;&#28508;&#21464;&#37327;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19982;&#20114;&#20449;&#24687;&#21644;&#25237;&#24433;&#22836;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#29983;&#25104;&#24335;&#22320;&#25311;&#21512;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;&#22914;SimVE&#65289;&#65292;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65288;&#20363;&#22914;FashionMNIST&#65292;CIFAR10&#65292;CelebA&#65289;&#65292;&#24615;&#33021;&#20248;&#20110;&#20043;&#21069;&#30340;VAE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical understanding of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#37096;&#20998;&#30340;&#27867;&#21270;&#65292;&#24182;&#33021;&#22815;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01103</link><description>&lt;p&gt;
&#32452;&#21512;&#29983;&#25104;&#24314;&#27169;&#65306;&#21333;&#19968;&#27169;&#22411;&#24182;&#19981;&#26159;&#24744;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;
&lt;/p&gt;
&lt;p&gt;
Compositional Generative Modeling: A Single Model is Not All You Need
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#37096;&#20998;&#30340;&#27867;&#21270;&#65292;&#24182;&#33021;&#22815;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#24040;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36234;&#26469;&#36234;&#20027;&#27969;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#24212;&#35813;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#22914;&#20309;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#20998;&#24067;&#65292;&#20351;&#24471;&#25105;&#20204;&#22312;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#20998;&#24067;&#37096;&#20998;&#20063;&#33021;&#36827;&#34892;&#27867;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#35757;&#32451;&#26102;&#23436;&#20840;&#26410;&#35265;&#30340;&#20219;&#21153;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#29420;&#31435;&#30340;&#32452;&#21512;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;Transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#22266;&#23450;&#28508;&#22312;&#29366;&#24577;&#30340;&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;Transformer&#27169;&#22411;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01032</link><description>&lt;p&gt;
&#36319;&#30528;&#25105;&#37325;&#22797;&#65306;Transformer&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#27604;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Repeat After Me: Transformers are Better than State Space Models at Copying
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01032
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;Transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#22266;&#23450;&#28508;&#22312;&#29366;&#24577;&#30340;&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;Transformer&#27169;&#22411;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#24207;&#21015;&#24314;&#27169;&#30340;&#20027;&#35201;&#26550;&#26500;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;&#19981;&#20381;&#36182;&#20110;&#24207;&#21015;&#38271;&#24230;&#30340;&#22266;&#23450;&#22823;&#23567;&#28508;&#22312;&#29366;&#24577;&#30340;&#27169;&#22411;&#65292;&#20063;&#23601;&#26159;"&#24191;&#20041;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;" (GSSMs)&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;GSSMs&#22312;&#25512;&#29702;&#26102;&#38388;&#25928;&#29575;&#19978;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#38656;&#35201;&#20174;&#36755;&#20837;&#19978;&#19979;&#25991;&#22797;&#21046;&#30340;&#20219;&#21153;&#19978;&#65292;&#23427;&#20204;&#30456;&#23545;&#20110;transformer&#27169;&#22411;&#26469;&#35828;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#20174;&#23545;&#31616;&#21333;&#30340;&#23383;&#31526;&#20018;&#22797;&#21046;&#20219;&#21153;&#30340;&#29702;&#35770;&#20998;&#26512;&#24320;&#22987;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#20004;&#23618;&#30340;transformer&#21487;&#20197;&#22797;&#21046;&#25351;&#25968;&#38271;&#24230;&#30340;&#23383;&#31526;&#20018;&#65292;&#32780;GSSMs&#30001;&#20110;&#20854;&#22266;&#23450;&#22823;&#23567;&#30340;&#28508;&#22312;&#29366;&#24577;&#22312;&#26681;&#26412;&#19978;&#26159;&#26377;&#38480;&#21046;&#30340;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;transformer&#22312;&#38656;&#35201;&#22797;&#21046;&#19978;&#19979;&#25991;&#30340;&#21512;&#25104;&#20219;&#21153;&#20013;&#65292;&#22312;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#19978;&#20248;&#20110;GSSMs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;transformer&#27169;&#22411;&#22312;&#22797;&#21046;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#36828;&#36828;&#20248;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120; (SCT)&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#30340;&#21367;&#31215;&#25805;&#20316;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#22797;&#26434;&#20613;&#37324;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;&#30340;&#21327;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18063</link><description>&lt;p&gt;
&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120;&#65306;&#21327;&#35843;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#30340;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18063
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120; (SCT)&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#30340;&#21367;&#31215;&#25805;&#20316;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#22797;&#26434;&#20613;&#37324;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;&#30340;&#21327;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20013;&#20351;&#29992;&#30340;Transformer&#24050;&#32463;&#36890;&#36807;&#21508;&#31181;&#32467;&#26500;&#36827;&#34892;&#20102;&#30740;&#31350; - &#22914;ViT&#12289;PVT&#21644;Swin&#12290;&#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#25913;&#36827;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#20351;&#20854;&#26356;&#21152;&#39640;&#25928;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#20154;&#20204;&#24863;&#21463;&#21040;&#20102;&#21253;&#21547;&#23616;&#37096;&#20449;&#24687;&#30340;&#38656;&#35201;&#65292;&#36825;&#23548;&#33268;&#22312;Transformer&#20013;&#24341;&#20837;&#21367;&#31215;&#65292;&#22914;CPVT&#21644;CvT&#12290;&#25105;&#20204;&#20351;&#29992;&#22797;&#26434;&#20613;&#31435;&#21494;&#22522;&#30784;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;AFNO&#12289;GFNet&#21644;Spectformer&#23454;&#29616;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#12290;&#25105;&#20204;&#25552;&#20513;&#32467;&#21512;&#25968;&#25454;&#30340;&#19977;&#31181;&#19981;&#21516;&#35270;&#22270; - &#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#38271;&#31243;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20165;&#20351;&#29992;&#23454;&#22495;&#20809;&#35889;&#34920;&#31034;&#30340;&#26368;&#31616;&#21333;&#20840;&#23616;&#34920;&#31034; - &#36890;&#36807;Hartley&#21464;&#25442;&#33719;&#24471;&#12290;&#25105;&#20204;&#22312;&#21021;&#22987;&#23618;&#20013;&#20351;&#29992;&#21367;&#31215;&#31639;&#23376;&#25429;&#25417;&#23616;&#37096;&#20449;&#24687;&#12290;&#36890;&#36807;&#36825;&#20004;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#24182;&#33719;&#24471;&#19968;&#20010;&#25552;&#20379;&#25913;&#36827;&#24615;&#33021;&#30340;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120;&#65288;SCT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18063v1 Announce Type: cross  Abstract: Transformers used in vision have been investigated through diverse architectures - ViT, PVT, and Swin. These have worked to improve the attention mechanism and make it more efficient. Differently, the need for including local information was felt, leading to incorporating convolutions in transformers such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer. We advocate combining three diverse views of data - local, global, and long-range dependence. We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform. We use a convolutional operator in the initial layers to capture local information. Through these two contributions, we are able to optimize and obtain a spectral convolution transformer (SCT) that provides improved performance 
&lt;/p&gt;</description></item><item><title>&#37327;&#21270;&#30446;&#21069;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21098;&#26525;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;</title><link>https://arxiv.org/abs/2403.15447</link><description>&lt;p&gt;
&#35299;&#30721;&#21387;&#32553;&#30340;&#20449;&#20219;&#65306;&#23457;&#35270;&#22312;&#21387;&#32553;&#19979;&#39640;&#25928;LLMs&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15447
&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#30446;&#21069;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21098;&#26525;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21387;&#32553;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#30340;&#39318;&#36873;&#31574;&#30053;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#22312;&#20445;&#30041;&#33391;&#24615;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#21387;&#32553;&#22312;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20351;&#29992;&#20116;&#31181;&#26368;&#20808;&#36827;&#21387;&#32553;&#25216;&#26415;&#35780;&#20272;&#19977;&#31181;&#39046;&#20808;LLMs&#30340;&#21487;&#20449;&#24230;&#32500;&#24230;&#36827;&#34892;&#20102;&#39318;&#27425;&#24443;&#24213;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#31361;&#20986;&#20102;&#21387;&#32553;&#19982;&#21487;&#20449;&#24230;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30446;&#21069;&#37327;&#21270;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#22320;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;&#20363;&#22914;&#65292;4&#20301;&#37327;&#21270;&#27169;&#22411;&#20445;&#30041;&#20102;&#20854;&#21407;&#22987;&#23545;&#24212;&#29289;&#30340;&#21487;&#20449;&#24230;&#65292;&#20294;&#27169;&#22411;&#21098;&#26525;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#20449;&#24230;&#65292;&#21363;&#20351;&#22312;50%&#30340;&#31232;&#30095;&#24230;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15447v1 Announce Type: cross  Abstract: Compressing high-capability Large Language Models (LLMs) has emerged as a favored strategy for resource-efficient inferences. While state-of-the-art (SoTA) compression methods boast impressive advancements in preserving benign task performance, the potential risks of compression in terms of safety and trustworthiness have been largely neglected. This study conducts the first, thorough evaluation of three (3) leading LLMs using five (5) SoTA compression techniques across eight (8) trustworthiness dimensions. Our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns. We find that quantization is currently a more effective approach than pruning in achieving efficiency and trustworthiness simultaneously. For instance, a 4-bit quantized model retains the trustworthiness of its original counterpart, but model pruning significantly degrades trustworthiness, even at 50% spars
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CantonMT&#39033;&#30446;&#65292;&#21033;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#31908;&#35821;&#33267;&#33521;&#35821;NMT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20197;&#20419;&#36827;&#30740;&#31350;</title><link>https://arxiv.org/abs/2403.11346</link><description>&lt;p&gt;
CantonMT: &#27721;&#33521;NMT&#24179;&#21488;&#65292;&#20351;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11346
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CantonMT&#39033;&#30446;&#65292;&#21033;&#29992;&#21512;&#25104;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#23545;&#31908;&#35821;&#33267;&#33521;&#35821;NMT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20197;&#20419;&#36827;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11346v1 &#28040;&#24687;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#20173;&#28982;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#26631;&#20934;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#21453;&#21521;&#32763;&#35793;&#65292;&#24212;&#29992;&#21040;&#20102;&#26032;&#30340;&#35821;&#35328;&#32763;&#35793;&#26041;&#21521;&#31908;&#35821;&#33267;&#33521;&#35821;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;(&#21253;&#25324;OpusMT, NLLB,&#21644;mBART)&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#25351;&#26631;&#21253;&#25324;&#22522;&#20110;&#35789;&#27719;&#21644;&#23884;&#20837;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#36825;&#39033;\textsc{CantonMT}&#30740;&#31350;&#39033;&#30446;&#20013;&#21253;&#21547;&#30340;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#24182;&#25552;&#20379;&#20415;&#21033;&#23454;&#29616;&#31908;&#35821;&#33267;&#33521;&#35821;MT&#30740;&#31350;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#25105;&#20204;&#30340;&#24320;&#28304;\textsc{CantonMT}&#24037;&#20855;&#21253;\url{https://github.com/kenrickkung/CantoneseTranslation}&#21521;&#24179;&#21488;&#28155;&#21152;&#26356;&#22810;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11346v1 Announce Type: cross  Abstract: Neural Machine Translation (NMT) for low-resource languages is still a challenging task in front of NLP researchers. In this work, we deploy a standard data augmentation methodology by back-translation to a new language translation direction Cantonese-to-English. We present the models we fine-tuned using the limited amount of real data and the synthetic data we generated using back-translation including OpusMT, NLLB, and mBART. We carried out automatic evaluation using a range of different metrics including lexical-based and embedding-based. Furthermore. we create a user-friendly interface for the models we included in this\textsc{ CantonMT} research project and make it available to facilitate Cantonese-to-English MT research. Researchers can add more models into this platform via our open-source\textsc{ CantonMT} toolkit \url{https://github.com/kenrickkung/CantoneseTranslation}.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.03777</link><description>&lt;p&gt;
ENOT&#65306;&#26399;&#26395;&#22238;&#24402;&#29992;&#20110;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#30340;&#24555;&#36895;&#21644;&#20934;&#30830;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03777
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#20849;&#36717;&#21183;&#27491;&#21017;&#21270;&#33021;&#22815;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#12290;&#29616;&#26377;NOT&#27714;&#35299;&#22120;&#30340;&#20027;&#35201;&#29942;&#39048;&#22312;&#20110;&#25214;&#21040;&#20849;&#36717;&#31639;&#23376;&#65288;&#21363;c-transform&#65289;&#30340;&#25509;&#36817;&#31934;&#30830;&#36817;&#20284;&#30340;&#36807;&#31243;&#65292;&#36825;&#35201;&#20040;&#36890;&#36807;&#20248;&#21270;&#26368;&#23567;-&#26368;&#22823;&#30446;&#26631;&#65292;&#35201;&#20040;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#23545;&#21021;&#22987;&#36817;&#20284;&#39044;&#27979;&#30340;&#31934;&#32454;&#35843;&#25972;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#12289;&#22312;&#26399;&#26395;&#22238;&#24402;&#24418;&#24335;&#19978;&#24378;&#21046;&#36866;&#24212;&#24615;&#26465;&#20214;&#20110;&#23398;&#20064;&#23545;&#20598;&#21183;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#21270;&#25439;&#22833;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#21487;&#33021;&#20849;&#36717;&#21183;&#20998;&#24067;&#30340;&#19978;&#38480;&#20272;&#35745;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#65292;&#28040;&#38500;&#20102;&#23545;&#39069;&#22806;&#24191;&#27867;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03777v1 Announce Type: cross  Abstract: We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our me
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22810;&#37329;&#23383;&#22612;&#21464;&#25442;&#22120;&#21644;&#25193;&#23637;&#39057;&#29575;&#23545;&#27604;&#27491;&#35268;&#21270;&#65292;&#20197;&#35299;&#20915;&#26174;&#24494;&#38236;&#21435;&#27169;&#31946;&#20013;&#30340;&#38271;&#36317;&#31163;&#20132;&#20114;&#21644;&#29305;&#24449;&#19981;&#36275;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.02611</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#26174;&#24494;&#38236;&#28966;&#22806;&#27169;&#31946;&#21435;&#38500;&#26694;&#26550;: &#22810;&#37329;&#23383;&#22612;&#21464;&#25442;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid Transformer and Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02611
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22810;&#37329;&#23383;&#22612;&#21464;&#25442;&#22120;&#21644;&#25193;&#23637;&#39057;&#29575;&#23545;&#27604;&#27491;&#35268;&#21270;&#65292;&#20197;&#35299;&#20915;&#26174;&#24494;&#38236;&#21435;&#27169;&#31946;&#20013;&#30340;&#38271;&#36317;&#31163;&#20132;&#20114;&#21644;&#29305;&#24449;&#19981;&#36275;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#28966;&#27169;&#31946;&#26159;&#26174;&#24494;&#38236;&#25104;&#20687;&#20013;&#30340;&#25345;&#32493;&#38382;&#39064;&#65292;&#23545;&#30149;&#29702;&#35299;&#37322;&#21644;&#32454;&#32990;&#26174;&#24494;&#38236;&#21644;&#26174;&#24494;&#25163;&#26415;&#20013;&#30340;&#21307;&#30103;&#24178;&#39044;&#36896;&#25104;&#20260;&#23475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#22810;&#37329;&#23383;&#22612;&#21464;&#25442;&#22120;&#65288;MPT&#65289;&#21644;&#25193;&#23637;&#39057;&#29575;&#23545;&#27604;&#27491;&#35268;&#21270;&#65288;EFCR&#65289;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26174;&#24494;&#38236;&#21435;&#27169;&#31946;&#20013;&#30340;&#20004;&#20010;&#31361;&#20986;&#25361;&#25112;&#65306;&#36739;&#38271;&#30340;&#27880;&#24847;&#21147;&#36328;&#24230;&#21644;&#29305;&#24449;&#19981;&#36275;&#12290;MPT&#22312;&#27599;&#20010;&#32593;&#32476;&#38454;&#27573;&#20351;&#29992;&#26174;&#24335;&#37329;&#23383;&#22612;&#32467;&#26500;&#65292;&#38598;&#25104;&#20102;&#36328;&#23610;&#24230;&#31383;&#21475;&#27880;&#24847;&#21147;&#65288;CSWA&#65289;&#12289;&#20869;&#23610;&#24230;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;ISCA&#65289;&#21644;&#29305;&#24449;&#22686;&#24378;&#21069;&#21521;&#32593;&#32476;&#65288;FEFN&#65289;&#65292;&#20197;&#25429;&#33719;&#38271;&#36317;&#31163;&#36328;&#23610;&#24230;&#31354;&#38388;&#20132;&#20114;&#21644;&#20840;&#23616;&#36890;&#36947;&#19978;&#19979;&#25991;&#12290;EFCR&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#39057;&#27573;&#30340;&#28508;&#22312;&#21435;&#27169;&#31946;&#20449;&#21495;&#26469;&#35299;&#20915;&#29305;&#24449;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#23427;&#36824;&#20351;&#21435;&#27169;&#31946;&#30693;&#35782;&#20256;&#36755;&#65292;&#20174;&#39069;&#22806;&#25968;&#25454;&#20013;&#23398;&#20064;&#36328;&#22495;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02611v1 Announce Type: cross  Abstract: Defocus blur is a persistent problem in microscope imaging that poses harm to pathology interpretation and medical intervention in cell microscopy and microscope surgery. To address this problem, a unified framework including multi-pyramid transformer (MPT) and extended frequency contrastive regularization (EFCR) is proposed to tackle two outstanding challenges in microscopy deblur: longer attention span and feature deficiency. The MPT employs an explicit pyramid structure at each network stage that integrates the cross-scale window attention (CSWA), the intra-scale channel attention (ISCA), and the feature-enhancing feed-forward network (FEFN) to capture long-range cross-scale spatial interaction and global channel context. The EFCR addresses the feature deficiency problem by exploring latent deblur signals from different frequency bands. It also enables deblur knowledge transfer to learn cross-domain information from extra data, impr
&lt;/p&gt;</description></item><item><title>Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.02545</link><description>&lt;p&gt;
Wukong: &#36808;&#21521;&#22823;&#35268;&#27169;&#25512;&#33616;&#30340;&#26631;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Wukong: Towards a Scaling Law for Large-Scale Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02545
&lt;/p&gt;
&lt;p&gt;
Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#22312;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#25512;&#33616;&#27169;&#22411;&#24182;&#27809;&#26377;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#35266;&#23519;&#21040;&#30340;&#23450;&#24459;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#21319;&#32423;&#26426;&#21046;&#30340;&#20302;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32431;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#30340;&#26377;&#25928;&#32593;&#32476;&#26550;&#26500;&#65292;&#32479;&#31216;&#20026;Wukong&#65292;&#20197;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#19968;&#20010;&#26631;&#24230;&#24459;&#12290;Wukong&#30340;&#29420;&#29305;&#35774;&#35745;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#26356;&#39640;&#26356;&#23485;&#30340;&#23618;&#27425;&#31616;&#21333;&#25429;&#33719;&#21508;&#31181;&#20219;&#24847;&#38454;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;Wukong&#22312;&#36136;&#37327;&#26041;&#38754;&#22987;&#32456;&#34920;&#29616;&#20248;&#36234;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Wuko
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02545v1 Announce Type: cross  Abstract: Scaling laws play an instrumental role in the sustainable improvement in model quality. Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation. Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wuko
&lt;/p&gt;</description></item><item><title>NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.00862</link><description>&lt;p&gt;
NewsBench&#65306;&#31995;&#32479;&#24615;&#35780;&#20272;LLM&#22312;&#20013;&#22269;&#26032;&#38395;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00862
&lt;/p&gt;
&lt;p&gt;
NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;NewsBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#65288;JWP&#65289;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#65288;SA&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#26032;&#38395;&#20262;&#29702;&#19982;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#39118;&#38505;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;NewsBench&#21253;&#25324;5&#20010;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;1,267&#39033;&#20219;&#21153;&#65292;7&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#23433;&#20840;&#24615;&#21644;&#26032;&#38395;&#20889;&#20316;&#65292;&#20197;&#21450;4&#20010;&#35814;&#32454;&#35201;&#38754;&#65289;&#65292;&#28085;&#30422;24&#20010;&#26032;&#38395;&#20027;&#39064;&#39046;&#22495;&#65292;&#37319;&#29992;&#22522;&#20110;&#20004;&#31181;GPT-4&#30340;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#32463;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#12290;&#25105;&#20204;&#23545;11&#20010;LLM&#30340;&#20840;&#38754;&#20998;&#26512;&#31361;&#20986;&#20102;GPT-4&#21644;ERNIE Bot&#20316;&#20026;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;&#25581;&#31034;&#20102;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#30456;&#23545;&#19981;&#36275;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#20869;&#23481;&#38656;&#35201;&#25552;&#39640;&#20262;&#29702;&#25351;&#23548;&#65292;&#26631;&#24535;&#30528;&#20197;&#26032;&#38395;&#26631;&#20934;&#21644;&#23433;&#20840;&#24615;&#23545;&#40784;AI&#33021;&#21147;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00862v1 Announce Type: cross  Abstract: This study presents NewsBench, a novel benchmark framework developed to evaluate the capability of Large Language Models (LLMs) in Chinese Journalistic Writing Proficiency (JWP) and their Safety Adherence (SA), addressing the gap between journalistic ethics and the risks associated with AI utilization. Comprising 1,267 tasks across 5 editorial applications, 7 aspects (including safety and journalistic writing with 4 detailed facets), and spanning 24 news topics domains, NewsBench employs two GPT-4 based automatic evaluation protocols validated by human assessment. Our comprehensive analysis of 11 LLMs highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks. These findings underscore the need for enhanced ethical guidance in AI-generated journalistic content, marking a step forward in aligning AI capabilities with journalistic standards and safet
&lt;/p&gt;</description></item><item><title>RAM-EHR&#36890;&#36807;&#22686;&#24378;&#26816;&#32034;&#24182;&#21033;&#29992;&#24635;&#32467;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20020;&#24202;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.00815</link><description>&lt;p&gt;
RAM-EHR: &#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#19978;&#30340;&#26816;&#32034;&#22686;&#24378;&#19982;&#20020;&#24202;&#39044;&#27979;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00815
&lt;/p&gt;
&lt;p&gt;
RAM-EHR&#36890;&#36807;&#22686;&#24378;&#26816;&#32034;&#24182;&#21033;&#29992;&#24635;&#32467;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20020;&#24202;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RAM-EHR&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#19978;&#20020;&#24202;&#39044;&#27979;&#30340;&#26816;&#32034;&#22686;&#24378;&#65288;Retrieval Augmentation&#65289;&#27969;&#31243;&#12290;RAM-EHR&#39318;&#20808;&#25910;&#38598;&#22810;&#20010;&#30693;&#35782;&#26469;&#28304;&#65292;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#25991;&#26412;&#26684;&#24335;&#65292;&#24182;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#26469;&#33719;&#21462;&#19982;&#21307;&#23398;&#27010;&#24565;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#36825;&#19968;&#31574;&#30053;&#35299;&#20915;&#20102;&#19982;&#22797;&#26434;&#27010;&#24565;&#21517;&#31216;&#30456;&#20851;&#30340;&#22256;&#38590;&#12290;RAM-EHR&#28982;&#21518;&#22686;&#24191;&#20102;&#19982;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#20195;&#30721;&#32852;&#21512;&#35757;&#32451;&#30340;&#26412;&#22320;EHR&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#25429;&#33719;&#26469;&#33258;&#24739;&#32773;&#23601;&#35786;&#21644;&#24635;&#32467;&#30693;&#35782;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#22312;&#20004;&#20010;EHR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RAM-EHR&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#30693;&#35782;&#22686;&#24378;&#22522;&#32447;&#25928;&#26524;&#26174;&#33879;&#65288;AUROC&#22686;&#30410;3.4&#65285;&#65292;AUPR&#22686;&#30410;7.2&#65285;&#65289;&#65292;&#24378;&#35843;&#20102;RAM-EHR&#30340;&#24635;&#32467;&#30693;&#35782;&#23545;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#23558;&#21457;&#24067;&#22312;\url{https://github.com/ritaranx/RAM-EHR}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00815v1 Announce Type: cross  Abstract: We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical predictions on Electronic Health Records (EHRs). RAM-EHR first collects multiple knowledge sources, converts them into text format, and uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts. RAM-EHR then augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on two EHR datasets show the efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in AUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized knowledge from RAM-EHR for clinical prediction tasks. The code will be published at \url{https://github.com/ritaranx/RAM-EHR}.
&lt;/p&gt;</description></item><item><title>&#32454;&#35843;&#30340;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#23545;&#20110;&#20381;&#36182;&#34920;&#38754;&#24418;&#24335;&#30340;&#24230;&#37327;&#21644;&#26410;&#32463;MT&#36136;&#37327;&#21028;&#26029;&#32454;&#35843;&#30340;&#39044;&#35757;&#32451;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.18747</link><description>&lt;p&gt;
&#32454;&#35843;&#30340;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#23384;&#22312;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18747
&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#30340;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#23545;&#20110;&#20381;&#36182;&#34920;&#38754;&#24418;&#24335;&#30340;&#24230;&#37327;&#21644;&#26410;&#32463;MT&#36136;&#37327;&#21028;&#26029;&#32454;&#35843;&#30340;&#39044;&#35757;&#32451;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#28085;&#30422;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;11&#31181;&#35821;&#35328;&#23545;&#30340;&#24191;&#27867;&#30340;&#22810;&#32500;&#36136;&#37327;&#24230;&#37327;(MQM)&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#26469;&#25506;&#31350;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#31227;&#26102;&#65292;&#26159;&#21542;&#37027;&#20123;&#26681;&#25454;&#20154;&#24037;&#29983;&#25104;&#30340;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#21028;&#26029;&#36827;&#34892;&#32454;&#35843;&#30340;MT&#24230;&#37327;&#26159;&#31283;&#20581;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26410;&#30693;&#39046;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;&#32454;&#35843;&#30340;&#24230;&#37327;&#30456;&#23545;&#20110;&#20381;&#36182;&#34920;&#38754;&#24418;&#24335;&#30340;&#24230;&#37327;&#20197;&#21450;&#26410;&#32463;MT&#36136;&#37327;&#21028;&#26029;&#32454;&#35843;&#30340;&#39044;&#35757;&#32451;&#24230;&#37327;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18747v1 Announce Type: cross  Abstract: We introduce a new, extensive multidimensional quality metrics (MQM) annotated dataset covering 11 language pairs in the biomedical domain. We use this dataset to investigate whether machine translation (MT) metrics which are fine-tuned on human-generated MT quality judgements are robust to domain shifts between training and inference. We find that fine-tuned metrics exhibit a substantial performance drop in the unseen domain scenario relative to metrics that rely on the surface form, as well as pre-trained metrics which are not fine-tuned on MT quality judgments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Layer-wise Scalable Adapter&#31574;&#30053;MedLaSA&#65292;&#29992;&#20110;&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#31934;&#30830;&#20462;&#25913;&#21307;&#23398;&#30693;&#35782;&#24182;&#35299;&#37322;&#20107;&#23454;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.18099</link><description>&lt;p&gt;
&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#30693;&#35782;&#21644;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18099
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Layer-wise Scalable Adapter&#31574;&#30053;MedLaSA&#65292;&#29992;&#20110;&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#31934;&#30830;&#20462;&#25913;&#21307;&#23398;&#30693;&#35782;&#24182;&#35299;&#37322;&#20107;&#23454;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26088;&#22312;&#31934;&#30830;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#29305;&#23450;&#30693;&#35782;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#19981;&#21464;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;LLMs&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#38382;&#39064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;LLMs&#22312;&#35768;&#22810;&#20851;&#38190;&#39046;&#22495;&#65288;&#20363;&#22914;&#21307;&#23398;&#39046;&#22495;&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#24187;&#35273;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20004;&#39033;&#27169;&#22411;&#32534;&#36753;&#30740;&#31350;&#65292;&#24182;&#22312;&#21307;&#23398;&#39046;&#22495;&#39564;&#35777;&#23427;&#20204;&#65306;&#65288;1&#65289;&#30452;&#25509;&#32534;&#36753;&#21307;&#23398;&#20107;&#23454;&#30693;&#35782;&#21644;&#65288;2&#65289;&#32534;&#36753;&#23545;&#20107;&#23454;&#30340;&#35299;&#37322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#30340;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MedLaSA&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#36866;&#29992;&#20110;&#21307;&#23398;&#27169;&#22411;&#32534;&#36753;&#30340;&#20998;&#23618;&#21487;&#25193;&#23637;&#36866;&#37197;&#22120;&#31574;&#30053;&#12290;&#23427;&#37319;&#29992;&#22240;&#26524;&#36861;&#36394;&#26469;&#35782;&#21035;&#31070;&#32463;&#20803;&#20013;&#30693;&#35782;&#30340;&#31934;&#30830;&#20301;&#32622;&#65292;&#28982;&#21518;&#23558;&#21487;&#25193;&#23637;&#36866;&#37197;&#22120;&#24341;&#20837;&#21040;LLMs&#30340;&#23494;&#38598;&#23618;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18099v1 Announce Type: cross  Abstract: Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged. It has been proven effective in resolving hallucination and out-of-date issues in LLMs. As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable. In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts. Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LL
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#31867;&#20803;&#23457;&#38405;&#32773;&#30340;&#24773;&#24863;&#25972;&#21512;&#26694;&#26550;&#65292;&#25552;&#20986;&#35780;&#20272;&#25351;&#26631;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#65292;&#25351;&#23548;LLMs&#29983;&#25104;&#31185;&#23398;&#20803;&#23457;&#38405;&#30340;&#36923;&#36753;&#34987;&#39564;&#35777;&#21487;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.18005</link><description>&lt;p&gt;
&#25506;&#32034;&#31185;&#23398;&#24773;&#24863;&#24635;&#32467;&#30340;&#22810;&#25991;&#26723;&#20449;&#24687;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#20803;&#23457;&#38405;&#32773;&#30340;&#24773;&#24863;&#25972;&#21512;&#26694;&#26550;&#65292;&#25552;&#20986;&#35780;&#20272;&#25351;&#26631;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#65292;&#25351;&#23548;LLMs&#29983;&#25104;&#31185;&#23398;&#20803;&#23457;&#38405;&#30340;&#36923;&#36753;&#34987;&#39564;&#35777;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#20855;&#26377;&#29983;&#25104;&#22810;&#20010;&#25991;&#26723;&#30340;&#21512;&#29702;&#25688;&#35201;&#30340;&#33021;&#21147;&#65307;&#28982;&#32780;&#65292;&#29616;&#22312;&#23578;&#19981;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#20855;&#26377;&#25972;&#21512;&#20449;&#24687;&#30340;&#33021;&#21147;&#26469;&#29983;&#25104;&#24635;&#32467;&#65292;&#23588;&#20854;&#26159;&#23545;&#37027;&#20123;&#21253;&#21547;&#20010;&#20154;&#24847;&#35265;&#20449;&#24687;&#30340;&#28304;&#25991;&#26723;&#12290;&#20026;&#20102;&#20351;&#31185;&#23398;&#24773;&#24863;&#24635;&#32467;&#26356;&#21152;&#25166;&#23454;&#65292;&#25105;&#20204;&#20551;&#35774;&#22312;&#21516;&#34892;&#35780;&#23457;&#20013;&#65292;&#20154;&#31867;&#20803;&#23457;&#38405;&#32773;&#36981;&#24490;&#24773;&#24863;&#25972;&#21512;&#30340;&#19977;&#23618;&#26694;&#26550;&#26469;&#25776;&#20889;&#20803;&#23457;&#38405;&#65292;&#24182;&#19988;&#36825;&#20195;&#34920;&#20102;&#22312;&#20803;&#23457;&#38405;&#29983;&#25104;&#36807;&#31243;&#20013;&#24635;&#32467;&#31185;&#23398;&#24773;&#24863;&#30340;&#36923;&#36753;&#12290;&#36890;&#36807;&#20154;&#31867;&#27880;&#37322;&#65292;&#39564;&#35777;&#20102;&#36825;&#19968;&#26694;&#26550;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#20803;&#23457;&#38405;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24403;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;LLMs&#29983;&#25104;&#20803;&#23457;&#38405;&#30340;&#25552;&#31034;&#26102;&#65292;&#24773;&#24863;&#25972;&#21512;&#26694;&#26550;&#30340;&#20551;&#35774;&#22312;&#32463;&#39564;&#19978;&#26159;&#34892;&#24471;&#36890;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18005v1 Announce Type: cross  Abstract: Modern natural language generation systems with LLMs exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if models truly possess the ability of information consolidation to generate summaries, especially on those source documents with opinionated information. To make scientific sentiment summarization more grounded, we hypothesize that in peer review human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews and it represents the logic of summarizing scientific sentiments in meta-review generation. The framework is validated via human annotation. Based on the framework, we propose evaluation metrics to assess the quality of generated meta-reviews, and we find that the hypothesis of the sentiment consolidation framework works out empirically when we incorporate it as prompts for LLMs to generate meta-reviews in extensive experiments.
&lt;/p&gt;</description></item><item><title>LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2402.16906</link><description>&lt;p&gt;
LDB&#65306;&#36890;&#36807;&#36880;&#27493;&#39564;&#35777;&#36816;&#34892;&#26102;&#25191;&#34892;&#26469;&#35843;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16906
&lt;/p&gt;
&lt;p&gt;
LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19981;&#20165;&#23558;&#21333;&#27425;&#20195;&#30721;&#29983;&#25104;&#65292;&#32780;&#19988;&#36824;&#23558;&#21333;&#20803;&#27979;&#35797;&#21644;&#31243;&#24207;&#39564;&#35777;&#22120;&#25972;&#21512;&#21040;LLMs&#20013;&#65292;&#20197;&#36845;&#20195;&#22320;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23558;&#29983;&#25104;&#30340;&#31243;&#24207;&#35270;&#20026;&#19981;&#21487;&#20998;&#21106;&#30340;&#23454;&#20307;&#65292;&#36825;&#23545;LLMs&#22312;&#35843;&#35797;&#31243;&#24207;&#26102;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#24403;&#31243;&#24207;&#21253;&#21547;&#22797;&#26434;&#30340;&#36923;&#36753;&#27969;&#31243;&#21644;&#25968;&#25454;&#25805;&#20316;&#26102;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#31867;&#24320;&#21457;&#20154;&#21592;&#35843;&#35797;&#31243;&#24207;&#26102;&#65292;&#20182;&#20204;&#36890;&#24120;&#35774;&#32622;&#26029;&#28857;&#24182;&#26377;&#36873;&#25321;&#22320;&#26816;&#26597;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#12290;&#25191;&#34892;&#27969;&#21644;&#20013;&#38388;&#21464;&#37327;&#22312;&#35843;&#35797;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#25991;&#29486;&#20013;&#26410;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#35797;&#22120;&#65288;LDB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;LLMs&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#23436;&#21892;&#20854;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16906v1 Announce Type: cross  Abstract: Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifical
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#27169;&#22411;&#65288;ContextDiff&#65289;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#36807;&#31243;&#20013;&#34701;&#20837;&#25991;&#26412;&#26465;&#20214;&#21644;&#35270;&#35273;&#26679;&#26412;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#65292;&#20197;&#20415;&#22312;&#35270;&#35273;&#29983;&#25104;&#20013;&#26356;&#20934;&#30830;&#22320;&#20256;&#36798;&#25991;&#26412;&#35821;&#20041;</title><link>https://arxiv.org/abs/2402.16627</link><description>&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#19979;&#30340;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#29983;&#25104;&#19982;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#27169;&#22411;&#65288;ContextDiff&#65289;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#36807;&#31243;&#20013;&#34701;&#20837;&#25991;&#26412;&#26465;&#20214;&#21644;&#35270;&#35273;&#26679;&#26412;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#65292;&#20197;&#20415;&#22312;&#35270;&#35273;&#29983;&#25104;&#20013;&#26356;&#20934;&#30830;&#22320;&#20256;&#36798;&#25991;&#26412;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#39640;&#20445;&#30495;&#24230;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#35273;&#29983;&#25104;&#21644;&#32534;&#36753;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25991;&#26412;&#24341;&#23548;&#35270;&#35273;&#25193;&#25955;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#20110;&#23558;&#25991;&#26412;-&#35270;&#35273;&#20851;&#31995;&#29420;&#21344;&#22320;&#34701;&#20837;&#21040;&#36870;&#36807;&#31243;&#20013;&#65292;&#24448;&#24448;&#24573;&#30053;&#20102;&#23427;&#20204;&#22312;&#27491;&#21521;&#36807;&#31243;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#27491;&#21453;&#36807;&#31243;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#21487;&#33021;&#38480;&#21046;&#20102;&#22312;&#35270;&#35273;&#21512;&#25104;&#32467;&#26524;&#20013;&#31934;&#30830;&#20256;&#36798;&#25991;&#26412;&#35821;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#27169;&#22411;&#65288;ContextDiff&#65289;&#65292;&#36890;&#36807;&#23558;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#21253;&#21547;&#25991;&#26412;&#26465;&#20214;&#21644;&#35270;&#35273;&#26679;&#26412;&#20043;&#38388;&#30340;&#20132;&#20114;&#21644;&#23545;&#40784;&#34701;&#20837;&#21040;&#27491;&#21521;&#21644;&#36870;&#21521;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#19978;&#19979;&#25991;&#20256;&#25773;&#21040;&#20004;&#20010;&#36807;&#31243;&#20013;&#30340;&#25152;&#26377;&#26102;&#38388;&#27493;&#65292;&#20197;&#35843;&#25972;&#23427;&#20204;&#30340;&#36712;&#36857;&#65292;&#20174;&#32780;&#20419;&#36827;&#36328;&#27169;&#24577;&#26465;&#20214;&#24314;&#27169;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#21270;&#25193;&#25955;&#25512;&#24191;&#21040;DDPMs&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16627v2 Announce Type: cross  Abstract: Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and 
&lt;/p&gt;</description></item><item><title>&#24418;&#24577;&#23545;&#31216;&#24615;&#26159;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#22266;&#26377;&#24615;&#36136;&#65292;&#36890;&#36807;&#23545;&#36816;&#21160;&#32467;&#26500;&#21644;&#36136;&#37327;&#30340;&#23545;&#31216;&#20998;&#24067;&#65292;&#24310;&#20280;&#33267;&#26426;&#22120;&#20154;&#29366;&#24577;&#31354;&#38388;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#65292;&#36827;&#32780;&#24433;&#21709;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#26041;&#31243;&#21644;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#23398;&#24314;&#27169;&#12289;&#25511;&#21046;&#21644;&#35774;&#35745;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.15552</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#24418;&#24577;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Morphological Symmetries in Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15552
&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#23545;&#31216;&#24615;&#26159;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#22266;&#26377;&#24615;&#36136;&#65292;&#36890;&#36807;&#23545;&#36816;&#21160;&#32467;&#26500;&#21644;&#36136;&#37327;&#30340;&#23545;&#31216;&#20998;&#24067;&#65292;&#24310;&#20280;&#33267;&#26426;&#22120;&#20154;&#29366;&#24577;&#31354;&#38388;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#65292;&#36827;&#32780;&#24433;&#21709;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#26041;&#31243;&#21644;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#23398;&#24314;&#27169;&#12289;&#25511;&#21046;&#21644;&#35774;&#35745;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#21644;&#21033;&#29992;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#24418;&#24577;&#23545;&#31216;&#24615;&#12290;&#36825;&#20123;&#26159;&#26426;&#22120;&#20154;&#24418;&#24577;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#32463;&#24120;&#22312;&#21160;&#29289;&#29983;&#29289;&#23398;&#21644;&#26426;&#22120;&#20154;&#23398;&#20013;&#35266;&#23519;&#21040;&#65292;&#28304;&#20110;&#36816;&#21160;&#32467;&#26500;&#30340;&#22797;&#21046;&#21644;&#36136;&#37327;&#30340;&#23545;&#31216;&#20998;&#24067;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#36825;&#20123;&#23545;&#31216;&#24615;&#22914;&#20309;&#24310;&#20280;&#21040;&#26426;&#22120;&#20154;&#30340;&#29366;&#24577;&#31354;&#38388;&#20197;&#21450;&#26412;&#20307;&#24863;&#30693;&#21644;&#22806;&#37096;&#24863;&#30693;&#20256;&#24863;&#22120;&#27979;&#37327;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#26041;&#31243;&#21644;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#30340;&#31561;&#19981;&#21464;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#24418;&#24577;&#23545;&#31216;&#24615;&#20316;&#20026;&#19968;&#20010;&#30456;&#20851;&#19988;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#21463;&#29289;&#29702;&#21551;&#31034;&#30340;&#20960;&#20309;&#20808;&#39564;&#65292;&#23545;&#26426;&#22120;&#20154;&#24314;&#27169;&#12289;&#25511;&#21046;&#12289;&#20272;&#35745;&#21644;&#35774;&#35745;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#39537;&#21160;&#21644;&#20998;&#26512;&#26041;&#27861;&#37117;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#24418;&#24577;&#23545;&#31216;&#24615;&#22914;&#20309;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15552v1 Announce Type: cross  Abstract: We present a comprehensive framework for studying and leveraging morphological symmetries in robotic systems. These are intrinsic properties of the robot's morphology, frequently observed in animal biology and robotics, which stem from the replication of kinematic structures and the symmetrical distribution of mass. We illustrate how these symmetries extend to the robot's state space and both proprioceptive and exteroceptive sensor measurements, resulting in the equivariance of the robot's equations of motion and optimal control policies. Thus, we recognize morphological symmetries as a relevant and previously unexplored physics-informed geometric prior, with significant implications for both data-driven and analytical methods used in modeling, control, estimation and design in robotics. For data-driven methods, we demonstrate that morphological symmetries can enhance the sample efficiency and generalization of machine learning models 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#25991;&#26412;&#27700;&#21360;&#20013;&#30340;&#8220;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#65292;&#21457;&#29616;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#22312;&#25991;&#26412;&#34987;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#21518;&#22833;&#21435;&#20102;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#27700;&#21360;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65292;&#26377;&#25928;&#32469;&#36807;&#27700;&#21360;&#65292;&#38477;&#20302;AUC&#20540;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23548;&#33268;&#36825;&#31181;&#24046;&#24322;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.14007</link><description>&lt;p&gt;
&#27700;&#21360;&#26159;&#21542;&#33021;&#22815;&#22312;&#32763;&#35793;&#20013;&#23384;&#27963;&#65311;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#27700;&#21360;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14007
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#25991;&#26412;&#27700;&#21360;&#20013;&#30340;&#8220;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#65292;&#21457;&#29616;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#22312;&#25991;&#26412;&#34987;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#21518;&#22833;&#21435;&#20102;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#27700;&#21360;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65292;&#26377;&#25928;&#32469;&#36807;&#27700;&#21360;&#65292;&#38477;&#20302;AUC&#20540;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23548;&#33268;&#36825;&#31181;&#24046;&#24322;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#26088;&#22312;&#26631;&#35760;&#21644;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20197;&#38450;&#27490;&#28389;&#29992;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25991;&#26412;&#27700;&#21360;&#20013;&#30340;&#8220;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#65292;&#35780;&#20272;&#20102;&#25991;&#26412;&#27700;&#21360;&#22312;&#34987;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#21518;&#20445;&#25345;&#26377;&#25928;&#24615;&#30340;&#33021;&#21147;&#12290;&#20004;&#20010;LLM&#21644;&#19977;&#31181;&#27700;&#21360;&#26041;&#27861;&#30340;&#21021;&#27493;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#22312;&#25991;&#26412;&#34987;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#26102;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#27700;&#21360;&#21435;&#38500;&#25915;&#20987;&#65288;CWRA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#39318;&#20808;&#20174;&#19968;&#20010;LLM&#20013;&#33719;&#21462;&#26469;&#33258;&#20013;&#20171;&#35821;&#35328;&#30340;&#21709;&#24212;&#65292;&#28982;&#21518;&#23558;&#20854;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#26469;&#32469;&#36807;&#27700;&#21360;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;AUC&#20540;&#20174;0.95&#38477;&#33267;0.67&#32780;&#26080;&#24615;&#33021;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23548;&#33268;&#20132;&#21449;&#19968;&#33268;&#24615;&#24046;&#24322;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14007v1 Announce Type: cross  Abstract: Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of ''cross-lingual consistency'' in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors that contribute to the cros
&lt;/p&gt;</description></item><item><title>ARL2&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#22312;NQ&#21644;MMLU&#19978;&#21462;&#24471;&#20102;5.4%&#21644;4.6%&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13542</link><description>&lt;p&gt;
ARL2: &#36890;&#36807;&#33258;&#23548;&#33258;&#36866;&#24212;&#30456;&#20851;&#24615;&#26631;&#35760;&#23558;&#26816;&#32034;&#22120;&#19982;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13542
&lt;/p&gt;
&lt;p&gt;
ARL2&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#22312;NQ&#21644;MMLU&#19978;&#21462;&#24471;&#20102;5.4%&#21644;4.6%&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13542v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#28304;&#30340;&#30456;&#20851;&#20449;&#24687;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#65292;&#24182;&#20943;&#36731;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20998;&#24320;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;LLMs&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#29616;&#26377;&#30340;&#26816;&#32034;&#22120;&#36890;&#24120;&#19982;LLMs&#19981;&#21305;&#37197;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARL2&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#30340;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;ARL2&#21033;&#29992;LLMs&#27880;&#37322;&#21644;&#35780;&#20998;&#30456;&#20851;&#35777;&#25454;&#65292;&#20174;&#32780;&#33021;&#22815;&#20174;&#24378;&#22823;&#30340;LLM&#30417;&#30563;&#20013;&#23398;&#20064;&#26816;&#32034;&#22120;&#12290;&#27492;&#22806;&#65292;ARL2&#20351;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#26469;&#31574;&#21010;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30456;&#20851;&#24615;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;ARL2&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;NQ&#19978;&#25552;&#39640;&#20102;5.4%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;MMLU&#19978;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13542v1 Announce Type: cross  Abstract: Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;</title><link>https://arxiv.org/abs/2402.13463</link><description>&lt;p&gt;
RefuteBench&#65306;&#35780;&#20272;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39539;&#25351;&#20196;&#36981;&#24490;
&lt;/p&gt;
&lt;p&gt;
RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#33539;&#22260;&#26085;&#30410;&#25193;&#22823;&#12290;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#26681;&#25454;&#27169;&#22411;&#30340;&#36755;&#20986;&#25552;&#20379;&#21453;&#39304;&#65292;&#24076;&#26395;&#24471;&#21040;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#20182;&#20204;&#30340;&#21453;&#39304;&#23436;&#25104;&#21709;&#24212;&#30340;&#21709;&#24212;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#33021;&#21542;&#24688;&#24403;&#22320;&#21709;&#24212;&#29992;&#25143;&#30340;&#21453;&#39539;&#21453;&#39304;&#24182;&#22987;&#32456;&#25191;&#34892;&#19979;&#21435;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#20998;&#26512;&#12290;&#22522;&#20110;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;RefuteBench&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#38382;&#31572;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#30005;&#23376;&#37038;&#20214;&#25776;&#20889;&#31561;&#20219;&#21153;&#12290;&#35780;&#20272;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#31215;&#26497;&#25509;&#21463;&#21453;&#39539;&#25351;&#20196;&#24418;&#24335;&#30340;&#21453;&#39304;&#65292;&#24182;&#26159;&#21542;&#33021;&#22815;&#22312;&#23545;&#35805;&#20013;&#22987;&#32456;&#36981;&#24490;&#29992;&#25143;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;&#20247;&#22810;LLMs&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;LLMs&#20542;&#21521;&#22266;&#25191;&#65292;&#21363;&#20542;&#21521;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#65292;&#32463;&#24120;&#26410;&#33021;&#36981;&#23432;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13463v1 Announce Type: cross  Abstract: The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the leng
&lt;/p&gt;</description></item><item><title>CIF-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#35328;&#19978;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26469;&#20943;&#23569;&#35780;&#20272;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.13109</link><description>&lt;p&gt;
CIF-Bench&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#20013;&#25991;&#25351;&#20196;&#36981;&#24490;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13109
&lt;/p&gt;
&lt;p&gt;
CIF-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#35328;&#19978;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26469;&#20943;&#23569;&#35780;&#20272;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#22686;&#24378;&#20102;&#36890;&#36807;&#25351;&#20196;&#36981;&#24490;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#26410;&#35265;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22914;&#20013;&#25991;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#24120;&#24120;&#20250;&#20943;&#24369;&#65292;&#21463;&#21040;&#25968;&#25454;&#27844;&#28431;&#24341;&#36215;&#30340;&#20559;&#35265;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#20154;&#23545;&#23427;&#20204;&#30495;&#27491;&#30340;&#27867;&#21270;&#33021;&#21147;&#21040;&#26032;&#35821;&#35328;&#39046;&#22495;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20013;&#25991;&#25351;&#20196;&#36981;&#24490;&#22522;&#20934;&#65288;CIF-Bench&#65289;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#23545;&#20013;&#25991;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;CIF-Bench &#21253;&#21547;150&#20010;&#20219;&#21153;&#21644;15,000&#20010;&#36755;&#20837;&#36755;&#20986;&#23545;&#65292;&#30001;&#27597;&#35821;&#32773;&#24320;&#21457;&#65292;&#29992;&#20110;&#27979;&#35797;&#36328;&#36234;20&#20010;&#31867;&#21035;&#30340;&#22797;&#26434;&#25512;&#29702;&#21644;&#20013;&#22269;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#12290;&#20026;&#20102;&#20943;&#23569;&#35780;&#20272;&#20559;&#35265;&#65292;&#25105;&#20204;&#21482;&#20844;&#24320;&#20102;&#25968;&#25454;&#38598;&#30340;&#19968;&#21322;&#65292;&#20854;&#20313;&#37096;&#20998;&#20445;&#25345;&#31169;&#23494;&#65292;&#24182;&#24341;&#20837;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#20197;&#26368;&#23567;&#21270;&#24471;&#20998;&#26041;&#24046;&#65292;&#20849;&#35745;45,000&#20010;&#25968;&#25454;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13109v1 Announce Type: cross  Abstract: The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following. Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories. To mitigate evaluation bias, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances. Our eval
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36880;&#28176;&#21098;&#26525;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#65292;&#20174;&#32780;&#20135;&#29983;&#27604;&#20256;&#32479;&#32593;&#32476;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#12290;</title><link>https://arxiv.org/abs/2402.12479</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20462;&#21098;&#32593;&#32476;&#26159;&#19968;&#20010;&#22909;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
In deep reinforcement learning, a pruned network is a good network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12479
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36880;&#28176;&#21098;&#26525;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#65292;&#20174;&#32780;&#20135;&#29983;&#27604;&#20256;&#32479;&#32593;&#32476;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#26377;&#25928;&#21033;&#29992;&#20854;&#32593;&#32476;&#21442;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#31232;&#30095;&#35757;&#32451;&#25216;&#26415;&#20248;&#21183;&#30340;&#20808;&#21069;&#35265;&#35299;&#65292;&#24182;&#35777;&#26126;&#36880;&#28176;&#21098;&#26525;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#12290;&#36825;&#23548;&#33268;&#32593;&#32476;&#27604;&#20256;&#32479;&#32593;&#32476;&#20135;&#29983;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#34920;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#65292;&#20165;&#20351;&#29992;&#23436;&#25972;&#32593;&#32476;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12479v1 Announce Type: cross  Abstract: Recent work has shown that deep reinforcement learning agents have difficulty in effectively using their network parameters. We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude pruning enables agents to maximize parameter effectiveness. This results in networks that yield dramatic performance improvements over traditional networks and exhibit a type of "scaling law", using only a small fraction of the full network parameters.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26680;&#24515;&#38382;&#39064;&#65306;&#22914;&#20309;&#23398;&#20064;&#26410;&#30693;&#30693;&#35782;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#21644;&#35782;&#21035;&#26410;&#30693;&#28857;&#26469;&#29420;&#31435;&#23398;&#20064;&#20197;&#21069;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#24187;&#35273;&#35780;&#20998;&#12289;&#23454;&#29616;&#39640;&#25928;LLM&#26356;&#26032;&#20197;&#21450;&#30693;&#35782;&#20132;&#27969;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.09147</link><description>&lt;p&gt;
&#26410;&#30693;&#20043;&#20013;&#65306;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Into the Unknown: Self-Learning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26680;&#24515;&#38382;&#39064;&#65306;&#22914;&#20309;&#23398;&#20064;&#26410;&#30693;&#30693;&#35782;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#21644;&#35782;&#21035;&#26410;&#30693;&#28857;&#26469;&#29420;&#31435;&#23398;&#20064;&#20197;&#21069;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#24187;&#35273;&#35780;&#20998;&#12289;&#23454;&#29616;&#39640;&#25928;LLM&#26356;&#26032;&#20197;&#21450;&#30693;&#35782;&#20132;&#27969;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20027;&#35201;&#38382;&#39064;&#65306;&#21363;&#22914;&#20309;&#23398;&#20064;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;LLM&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#33258;&#24049;&#30340;&#24187;&#35273;&#36827;&#34892;&#33258;&#25105;&#35780;&#20272;&#65292;&#20351;LLM&#33021;&#22815;&#29420;&#31435;&#22320;&#23398;&#20064;&#20197;&#21069;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20351;&#29992;&#24187;&#35273;&#35780;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#26410;&#30693;&#28857;&#8221;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#21644;&#19977;&#31181;&#20869;&#37096;&#26041;&#27861;&#26469;&#33258;&#21160;&#35782;&#21035;&#26410;&#30693;&#28857;&#12290;&#36825;&#26377;&#21161;&#20110;&#21019;&#24314;&#19968;&#20010;&#33258;&#23398;&#20064;&#24490;&#29615;&#65292;&#19987;&#27880;&#20110;&#26410;&#30693;&#28857;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#20174;&#32780;&#20943;&#23569;&#24187;&#35273;&#35780;&#20998;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#29992;&#20110;&#35780;&#20272;LLM&#33258;&#23398;&#20064;&#33021;&#21147;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#24494;&#35843;&#25110;&#23545;&#40784;&#30340;7B-Mistral&#27169;&#22411;&#22312;&#33258;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#33258;&#23398;&#20064;&#27010;&#24565;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#26356;&#26032;&#65292;&#24182;&#20026;&#30693;&#35782;&#20132;&#27969;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#23427;&#36824;&#21487;&#33021;&#22686;&#21152;&#20844;&#20247;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09147v1 Announce Type: new Abstract: We address the main problem of self-learning LLM: the question of what to learn. We propose a self-learning LLM framework that enables an LLM to independently learn previously unknown knowledge through self-assessment of their own hallucinations. Using the hallucination score, we introduce a new concept of Points in The Unknown (PiUs), along with one extrinsic and three intrinsic methods for automatic PiUs identification. It facilitates the creation of a self-learning loop that focuses exclusively on the knowledge gap in Points in The Unknown, resulting in a reduced hallucination score. We also developed evaluation metrics for gauging an LLM's self-learning capability. Our experiments revealed that 7B-Mistral models that have been finetuned or aligned are capable of self-learning considerably well. Our self-learning concept allows more efficient LLM updates and opens new perspectives for knowledge exchange. It may also increase public tru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#21512;&#25104;&#19981;&#21516;&#26102;&#26399;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#33258;&#36866;&#24212;&#28378;&#21160;&#31383;&#21475;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#20197;&#21450;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38750;&#31283;&#24577;&#25968;&#25454;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08672</link><description>&lt;p&gt;
&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#22312;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Model Assessment and Selection under Temporal Distribution Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#21512;&#25104;&#19981;&#21516;&#26102;&#26399;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#33258;&#36866;&#24212;&#28378;&#21160;&#31383;&#21475;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#20197;&#21450;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38750;&#31283;&#24577;&#25968;&#25454;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#24403;&#21069;&#26102;&#26399;&#21644;&#21382;&#21490;&#26102;&#26399;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#26410;&#30693;&#21644;&#21487;&#33021;&#20219;&#24847;&#30340;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28378;&#21160;&#31383;&#21475;&#26041;&#27861;&#26469;&#20272;&#35745;&#32473;&#23450;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#36825;&#31181;&#31574;&#30053;&#36824;&#36890;&#36807;&#20272;&#35745;&#20004;&#20010;&#20505;&#36873;&#27169;&#22411;&#20043;&#38388;&#30340;&#27867;&#21270;&#35823;&#24046;&#24046;&#24322;&#26469;&#26041;&#20415;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#20004;&#20004;&#27604;&#36739;&#25972;&#21512;&#21040;&#21333;&#22330;&#28120;&#27760;&#36187;&#20013;&#65292;&#20174;&#20505;&#36873;&#27169;&#22411;&#38598;&#21512;&#20013;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#27169;&#22411;&#36873;&#25321;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#26041;&#27861;&#23545;&#25968;&#25454;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs. To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model. This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors. We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates. Theoretical analyses and numerical experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23558;&#19987;&#23478;&#32452;&#21512;&#27169;&#22359;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#23588;&#20854;&#26159;&#36719;MoE&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#20197;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.08609</link><description>&lt;p&gt;
&#19987;&#23478;&#32452;&#21512;&#35299;&#38145;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21442;&#25968;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Mixtures of Experts Unlock Parameter Scaling for Deep RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23558;&#19987;&#23478;&#32452;&#21512;&#27169;&#22359;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#23588;&#20854;&#26159;&#36719;MoE&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#20197;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#65288;&#33258;&#25105;&#65289;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#36890;&#36807;&#23454;&#35777;&#32553;&#25918;&#23450;&#24459;&#39044;&#27979;&#30340;&#65306;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20854;&#35268;&#27169;&#25104;&#27604;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23547;&#25214;&#31867;&#20284;&#30340;&#32553;&#25918;&#23450;&#24459;&#20173;&#28982;&#22256;&#38590;&#65292;&#22240;&#20026;&#22686;&#21152;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#24448;&#24448;&#20250;&#25439;&#23475;&#20854;&#26368;&#32456;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#23558;&#19987;&#23478;&#32452;&#21512;&#65288;MoE&#65289;&#27169;&#22359;&#65292;&#29305;&#21035;&#26159;&#36719;MoE&#65288;Puigcerver&#31561;&#20154;&#65292;2023&#24180;&#65289;&#65292;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21508;&#31181;&#35757;&#32451;&#26041;&#26696;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#21152;&#20197;&#35777;&#26126;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#26469;&#22312;&#27169;&#22411;-free&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#21644;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#65292;&#25552;&#20379;&#20102;&#20840;&#23616;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#24335;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.07107</link><description>&lt;p&gt;
&#32034;crates&#24576;&#30097;&#30340;&#22238;&#22768;&#65306;&#22312;&#26657;&#20934;&#30340;&#35777;&#25454;&#22686;&#24378;&#23398;&#20064;&#20013;&#25509;&#21463;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07107
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#26469;&#22312;&#27169;&#22411;-free&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#21644;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#65292;&#25552;&#20379;&#20102;&#20840;&#23616;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#24335;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#28041;&#21450;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$&#26088;&#22312;&#35299;&#20915;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20998;&#21035;&#20272;&#35745;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#25152;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#23427;&#23558;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#19982;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#26174;&#24335;&#30340;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#30340;$\textit{&#20840;&#23616;}$&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#31616;&#21333;&#26041;&#24046;&#30340;$\textit{&#23616;&#37096;}$&#20272;&#35745;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#20197;&#21450;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#19968;&#22871;&#23567;&#22411;&#21270;&#30340;Atari&#28216;&#25103;&#65288;&#21363;MinAtar&#65289;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;CEQR-DQN&#22312;&#24471;&#20998;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#31867;&#20284;&#30340;&#29616;&#26377;&#26694;&#26550;&#12290;&#23427;&#33021;&#22815;&#20005;&#35880;&#22320;&#22788;&#29702;&#22806;&#37096;&#25968;&#25454;&#35266;&#27979;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel statistical approach to incorporating uncertainty awareness in model-free distributional reinforcement learning involving quantile regression-based deep Q networks. The proposed algorithm, $\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments. It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of out-of-distribution (OOD) observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed. Its ability to rigorously e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;ExGRG&#65292;&#23427;&#36890;&#36807;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;&#26469;&#35299;&#20915;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#25361;&#25112;&#65292;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#21644;&#22312;&#32447;&#25552;&#21462;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2402.06737</link><description>&lt;p&gt;
ExGRG: &#29992;&#20110;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;
&lt;/p&gt;
&lt;p&gt;
ExGRG: Explicitly-Generated Relation Graph for Self-Supervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;ExGRG&#65292;&#23427;&#36890;&#36807;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;&#26469;&#35299;&#20915;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#25361;&#25112;&#65292;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#21644;&#22312;&#32447;&#25552;&#21462;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#19968;&#31181;&#26080;&#38656;&#26114;&#36149;&#30340;&#26631;&#27880;&#26631;&#31614;&#32780;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#20869;&#23884;&#20449;&#21495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SSL&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#36890;&#36807;&#30452;&#35266;&#30340;&#25968;&#25454;&#22686;&#24378;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#22686;&#24378;&#25805;&#20316;&#25913;&#21464;&#20102;&#35821;&#20041;&#24182;&#21576;&#29616;&#20986;&#21453;&#30452;&#35266;&#30340;&#24615;&#36136;&#12290;&#38024;&#23545;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;&#65288;ExGRG&#65289;&#65292;&#20197;&#21462;&#20195;&#20165;&#20381;&#38752;&#20256;&#32479;&#30340;&#22522;&#20110;&#22686;&#24378;&#30340;&#38544;&#24335;&#20851;&#31995;&#22270;&#12290;ExGRG&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#21644;&#22312;&#32447;&#25552;&#21462;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#19981;&#21464;&#24615;&#30446;&#26631;&#20013;&#65292;&#20511;&#37492;&#20102;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#26144;&#23556;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;&#36890;&#36807;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#32467;&#21512;&#65292;&#25105;&#20204;&#30340;E&#27493;&#39588;&#28041;&#21450;&#20851;&#31995;&#22270;&#30340;&#29983;&#25104;&#65292;&#20197;&#35782;&#21035;...
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning (SSL) has emerged as a powerful technique in pre-training deep learning models without relying on expensive annotated labels, instead leveraging embedded signals in unlabeled data. While SSL has shown remarkable success in computer vision tasks through intuitive data augmentation, its application to graph-structured data poses challenges due to the semantic-altering and counter-intuitive nature of graph augmentations. Addressing this limitation, this paper introduces a novel non-contrastive SSL approach to Explicitly Generate a compositional Relation Graph (ExGRG) instead of relying solely on the conventional augmentation-based implicit relation graph. ExGRG offers a framework for incorporating prior domain knowledge and online extracted information into the SSL invariance objective, drawing inspiration from the Laplacian Eigenmap and Expectation-Maximization (EM). Employing an EM perspective on SSL, our E-step involves relation graph generation to identify can
&lt;/p&gt;</description></item><item><title>&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;&#65288;ICRH&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#22312;&#20248;&#21270;&#30446;&#26631;&#30340;&#21516;&#26102;&#21364;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#23548;&#33268;ICRH&#30340;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.06627</link><description>&lt;p&gt;
&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#25512;&#21160;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Feedback Loops With Language Models Drive In-Context Reward Hacking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06627
&lt;/p&gt;
&lt;p&gt;
&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;&#65288;ICRH&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#22312;&#20248;&#21270;&#30446;&#26631;&#30340;&#21516;&#26102;&#21364;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#23548;&#33268;ICRH&#30340;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#22806;&#37096;&#19990;&#30028;&#20135;&#29983;&#24433;&#21709;&#65306;&#23427;&#20204;&#26597;&#35810;&#21487;&#20197;&#35835;&#20889;&#32593;&#39029;&#30340;API&#65292;&#29983;&#25104;&#33021;&#22815;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#36816;&#34892;&#31995;&#32479;&#21629;&#20196;&#12290;&#36825;&#20123;&#20114;&#21160;&#24418;&#25104;&#20102;&#21453;&#39304;&#24490;&#29615;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#24433;&#21709;&#19990;&#30028;&#65292;&#21453;&#36807;&#26469;&#21448;&#24433;&#21709;&#21518;&#32493;&#30340;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;(ICRH)&#65292;&#21363;&#27979;&#35797;&#26102;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20248;&#21270;&#65288;&#21487;&#33021;&#38544;&#21547;&#30340;&#65289;&#30446;&#26631;&#30340;&#21516;&#26102;&#65292;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#20363;&#22914;&#65292;&#32771;&#34385;&#19968;&#20010;&#34987;&#37096;&#32626;&#29992;&#20110;&#22686;&#21152;Twitter&#21442;&#19982;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65307;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#26816;&#32034;&#20854;&#20197;&#21069;&#30340;&#25512;&#25991;&#65292;&#24182;&#20351;&#25512;&#25991;&#26356;&#20855;&#20105;&#35758;&#24615;&#65292;&#20174;&#32780;&#22686;&#21152;&#21442;&#19982;&#24230;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;&#23548;&#33268;ICRH&#30340;&#20004;&#20010;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;&#23545;&#20110;&#36825;&#20123;&#36807;&#31243;&#65292;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26159;&#19981;&#36275;&#22815;&#30340;-&#20182;&#20204;&#26080;&#27861;&#25429;&#25417;&#21040;&#21453;&#39304;&#25928;&#24212;&#65292;&#20063;&#19981;&#33021;&#25429;&#25417;&#21040;&#26368;&#26377;&#23475;&#30340;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06529</link><description>&lt;p&gt;
&#20869;&#30465;&#35268;&#21010;&#65306;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#36890;&#36807;&#36866;&#24403;&#30340;&#22522;&#30784;&#22609;&#36896;&#26469;&#31574;&#30053;&#24615;&#22320;&#36827;&#34892;&#39640;&#32423;&#34892;&#21160;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;LLM&#20135;&#29983;&#30340;&#24187;&#35273;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#33258;&#20449;&#22320;&#25191;&#34892;&#19982;&#29992;&#25143;&#30446;&#26631;&#19981;&#31526;&#25110;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#19981;&#23433;&#20840;&#30340;&#35745;&#21010;&#12290;&#27492;&#22806;&#65292;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#21487;&#33021;&#24341;&#21457;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#36873;&#39033;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;LLMs&#24517;&#39035;&#35782;&#21035;&#27492;&#31867;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#24341;&#23548;LLMs&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#24418;&#25104;&#24847;&#35782;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#25191;&#34892;&#35745;&#21010;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;&#26426;&#22120;&#20154;&#35268;&#21010;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#35777;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;</title><link>https://arxiv.org/abs/2402.06126</link><description>&lt;p&gt;
&#23398;&#20064;&#21464;&#24471;&#39640;&#25928;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learn To be Efficient: Build Structured Sparsity in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20197;&#20854;&#21313;&#20159;&#32423;&#21442;&#25968;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#20102;&#39640;&#26114;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;LLM&#20013;&#20986;&#29616;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#20026;&#36890;&#36807;&#20165;&#28041;&#21450;&#37096;&#20998;&#21442;&#25968;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21033;&#29992;&#36825;&#31181;&#33258;&#28982;&#24418;&#25104;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24573;&#35270;&#20102;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#31181;&#22266;&#26377;&#31232;&#30095;&#24615;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;LLM&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#26356;&#32467;&#26500;&#21270;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#26469;&#23398;&#20064;&#39640;&#25928;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)", &#26088;&#22312;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#22312;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#25240;&#34935;&#12290;&#27492;&#22806;&#65292;&#19982;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;ReLU&#27169;&#22411;&#30340;SOTA MoEfication&#26041;&#27861;&#19981;&#21516;&#65292;LTE&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20687;GPT&#21644;LLaMA&#36825;&#26679;&#20855;&#26377;&#36719;&#28608;&#27963;&#20989;&#25968;&#30340;LLM&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27169;&#22411;&#21644;&#21313;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LTE&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#22270;&#20013;&#23454;&#29616;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#27169;&#22411;&#36890;&#36807;&#22312;&#25972;&#20010;&#22270;&#20013;&#36827;&#34892;&#20449;&#24687;&#27969;&#23454;&#29616;&#20102;&#35266;&#23519;&#37051;&#22495;&#22823;&#23567;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Agent&#30340;&#21453;&#24212;&#24615;&#12289;&#36873;&#25321;&#21160;&#20316;&#30340;&#36136;&#37327;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05027</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#24490;&#29615;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#20013;&#23454;&#29616;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#22270;&#20013;&#23454;&#29616;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#27169;&#22411;&#36890;&#36807;&#22312;&#25972;&#20010;&#22270;&#20013;&#36827;&#34892;&#20449;&#24687;&#27969;&#23454;&#29616;&#20102;&#35266;&#23519;&#37051;&#22495;&#22823;&#23567;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Agent&#30340;&#21453;&#24212;&#24615;&#12289;&#36873;&#25321;&#21160;&#20316;&#30340;&#36136;&#37327;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#29615;&#22659;&#32473;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#20998;&#25955;&#24335;&#26041;&#27861;&#20013;&#65292;Agent&#22312;&#32473;&#23450;&#30340;&#22270;&#20013;&#25805;&#20316;&#65292;&#24182;&#26681;&#25454;&#37096;&#20998;&#25110;&#36807;&#26102;&#30340;&#35266;&#23519;&#20570;&#20986;&#20915;&#31574;&#12290;&#35266;&#23519;&#21040;&#30340;&#37051;&#22495;&#30340;&#22823;&#23567;&#38480;&#21046;&#20102;&#22312;&#19981;&#21516;&#22270;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24433;&#21709;&#21040;Agent&#30340;&#21453;&#24212;&#24615;&#12289;&#36873;&#25321;&#30340;&#21160;&#20316;&#36136;&#37327;&#21644;&#36890;&#20449;&#24320;&#38144;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22312;&#25972;&#20010;&#22270;&#20013;&#36827;&#34892;&#36830;&#32493;&#30340;&#20449;&#24687;&#27969;&#35299;&#20915;&#20102;&#35266;&#23519;&#21040;&#30340;&#37051;&#22495;&#22823;&#23567;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#65292;&#23427;&#19982;&#29615;&#22659;&#30340;&#27493;&#39588;&#36845;&#20195;&#65292;&#24182;&#20801;&#35768;&#33410;&#28857;&#36890;&#36807;&#19982;&#20854;&#37051;&#23621;&#20132;&#25442;&#28040;&#24687;&#26469;&#21019;&#24314;&#22270;&#30340;&#20840;&#23616;&#34920;&#31034;&#12290;&#26681;&#25454;Agent&#22312;&#22270;&#20013;&#30340;&#20301;&#32622;&#65292;Agent&#25509;&#25910;&#21040;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#22270;&#35266;&#23519;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#20197;&#20998;&#25955;&#30340;&#26041;&#24335;&#20351;&#29992;&#65292;&#24182;&#19982;&#36873;&#25321;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Graph-based environments pose unique challenges to multi-agent reinforcement learning. In decentralized approaches, agents operate within a given graph and make decisions based on partial or outdated observations. The size of the observed neighborhood limits the generalizability to different graphs and affects the reactivity of agents, the quality of the selected actions, and the communication overhead. This work focuses on generalizability and resolves the trade-off in observed neighborhood size with a continuous information flow in the whole graph. We propose a recurrent message-passing model that iterates with the environment's steps and allows nodes to create a global representation of the graph by exchanging messages with their neighbors. Agents receive the resulting learned graph observations based on their location in the graph. Our approach can be used in a decentralized manner at runtime and in combination with a reinforcement learning algorithm of choice. We evaluate our meth
&lt;/p&gt;</description></item><item><title>QuIP#&#26159;&#19968;&#31181;&#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;&#19979;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.04396</link><description>&lt;p&gt;
QuIP#: &#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#36827;&#34892;&#26356;&#22909;&#30340;LLM&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04396
&lt;/p&gt;
&lt;p&gt;
QuIP#&#26159;&#19968;&#31181;&#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;&#19979;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#36890;&#36807;&#23558;LLM&#30340;&#26435;&#37325;&#37327;&#21270;&#20026;&#20302;&#31934;&#24230;&#26469;&#20943;&#23569;&#20854;&#20869;&#23384;&#21344;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;QuIP#&#65292;&#19968;&#31181;&#20165;&#22522;&#20110;&#26435;&#37325;&#30340;PTQ&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#26032;&#25216;&#26415;&#65292;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;($\le$ 4&#27604;&#29305;&#27599;&#20010;&#26435;&#37325;)&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;QuIP#&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#21704;&#36798;&#29595;&#24503;&#21464;&#25442;&#25913;&#36827;&#20102;QuIP&#20013;&#30340;&#38750;&#30456;&#24178;&#22788;&#29702;&#65292;&#35813;&#26041;&#27861;&#26356;&#24555;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35770;&#29305;&#24615;&#12290;&#20854;&#27425;&#65292;QuIP#&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21033;&#29992;&#20102;&#38750;&#30456;&#24178;&#26435;&#37325;&#20855;&#26377;&#30340;&#29699;&#24418;&#20122;&#39640;&#26031;&#20998;&#24067;&#29305;&#24615;&#65306;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#22522;&#20110;&#39640;&#24230;&#23545;&#31216;$E_8$&#26684;&#20070;&#30340;&#30828;&#20214;&#39640;&#25928;&#20195;&#30721;&#20070;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;8&#32500;&#21333;&#20301;&#29699;&#35013;&#22635;&#12290;&#31532;&#19977;&#65292;QuIP#&#20351;&#29992;&#24494;&#35843;&#26469;&#25552;&#39640;&#23545;&#21407;&#22987;&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;QuIP#&#20248;&#20110;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#26032;&#30340;PTQ&#25193;&#23637;&#34892;&#20026;&#65292;&#24182;&#25903;&#25345;&#24555;&#36895;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves the incoherence processing from QuIP by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector quantization techniques to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04247</link><description>&lt;p&gt;
&#20248;&#20808;&#23433;&#20840;&#20445;&#38556;&#32780;&#38750;&#33258;&#27835;&#65306;&#31185;&#23398;&#20013;LLM&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#33258;&#20027;&#36827;&#34892;&#23454;&#39564;&#21644;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#33021;&#21147;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#30340;&#28431;&#27934;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#31354;&#30333;&#65292;&#23578;&#26410;&#23545;&#36825;&#20123;&#28431;&#27934;&#36827;&#34892;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#35823;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#38656;&#27714;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#27010;&#36848;&#20102;&#31185;&#23398;LLM&#26426;&#22120;&#20154;&#22266;&#26377;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#24847;&#22270;&#12289;&#29305;&#23450;&#30340;&#31185;&#23398;&#39046;&#22495;&#20197;&#21450;&#23427;&#20204;&#23545;&#22806;&#37096;&#29615;&#22659;&#21487;&#33021;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#28431;&#27934;&#30340;&#36215;&#28304;&#21644;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
&lt;/p&gt;</description></item><item><title>C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03181</link><description>&lt;p&gt;
C-RAG: &#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35777;&#29983;&#25104;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03181
&lt;/p&gt;
&lt;p&gt;
C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#22791;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#21644;&#38169;&#20301;&#12290;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RAG&#65289;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;RAG&#27169;&#22411;&#30340;&#29983;&#25104;&#39118;&#38505;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;RAG&#26159;&#21542;&#30830;&#23454;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#65292;2&#65289;&#22914;&#20309;&#23545;RAG&#21644;&#20256;&#32479;LLM&#30340;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#20197;&#21450;3&#65289;&#21738;&#20123;&#20805;&#20998;&#26465;&#20214;&#20351;&#24471;RAG&#27169;&#22411;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;C-RAG&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;RAG&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;RAG&#27169;&#22411;&#25552;&#20379;&#20102;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#65292;&#24182;&#30830;&#20445;&#20102;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#33324;&#26377;&#30028;&#39118;&#38505;&#19979;&#30340;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36719;&#36866;&#37197;&#22120;&#28151;&#21512;&#23454;&#29616;&#38899;&#39057;&#39057;&#35889;&#21464;&#25442;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#38899;&#39057;&#21644;&#35821;&#38899;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00828</link><description>&lt;p&gt;
&#36890;&#36807;&#36719;&#36866;&#37197;&#22120;&#28151;&#21512;&#23454;&#29616;&#38899;&#39057;&#39057;&#35889;&#21464;&#25442;&#30340;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36719;&#36866;&#37197;&#22120;&#28151;&#21512;&#23454;&#29616;&#38899;&#39057;&#39057;&#35889;&#21464;&#25442;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#38899;&#39057;&#21644;&#35821;&#38899;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#36817;&#24180;&#26469;&#24320;&#22987;&#20852;&#36215;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#22312;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#21487;&#25215;&#21463;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#27169;&#22411;&#23481;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21487;&#20197;&#24212;&#29992;&#20110;&#21464;&#25442;&#22120;&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36825;&#20123;&#26159;&#24403;&#21069;&#20247;&#22810;&#39046;&#22495;&#20013;&#39047;&#26377;&#25104;&#23601;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;MoE&#20027;&#35201;&#29992;&#20110;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#20294;&#20854;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#24212;&#29992;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#35797;&#22270;&#25581;&#31034;&#20351;&#29992;MoE&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#38899;&#39057;&#39057;&#35889;&#21464;&#25442;&#24494;&#35843;&#21040;&#38899;&#39057;&#21644;&#35821;&#38899;&#19979;&#28216;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36719;&#36866;&#37197;&#22120;&#28151;&#21512;&#65288;Soft-MoA&#65289;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36866;&#37197;&#22120;&#20316;&#20026;&#19987;&#23478;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#30340;&#36719;MoE&#26041;&#27861;&#65292;&#22312;&#36755;&#20837;&#35760;&#21495;&#21644;&#19987;&#23478;&#20043;&#38388;&#36827;&#34892;&#36719;&#20998;&#37197;&#65292;&#20197;&#20445;&#25345;&#35745;&#31639;&#26102;&#38388;&#26377;&#38480;&#12290;&#23545;4&#20010;&#22522;&#20934;&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Soft-MoA&#20248;&#20110;&#21333;&#19968;&#36866;&#37197;&#22120;&#26041;&#27861;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture of Experts (MoE) architectures have recently started burgeoning due to their ability to scale model's capacity while maintaining the computational cost affordable. Furthermore, they can be applied to both Transformers and State Space Models, the current state-of-the-art models in numerous fields. While MoE has been mostly investigated for the pre-training stage, its use in parameter-efficient transfer learning settings is under-explored. To narrow this gap, this paper attempts to demystify the use of MoE for parameter-efficient fine-tuning of Audio Spectrogram Transformers to audio and speech downstream tasks. Specifically, we propose Soft Mixture of Adapters (Soft-MoA). It exploits adapters as the experts and, leveraging the recent Soft MoE method, it relies on a soft assignment between the input tokens and experts to keep the computational time limited. Extensive experiments across 4 benchmarks demonstrate that Soft-MoA outperforms the single adapter method and performs on pa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#36870;&#36716;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.12192</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#21453;&#21521;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Text Embedding Inversion Security for Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.12192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#36870;&#36716;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#25991;&#26412;&#25968;&#25454;&#36890;&#24120;&#20197;&#23454;&#25968;&#23884;&#20837;&#34920;&#31034;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23884;&#20837;&#24335;&#26381;&#21153;&#65288;EaaS&#65289;&#30340;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23558;&#25935;&#24863;&#20449;&#24687;&#23384;&#20648;&#20026;&#23884;&#20837;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#23433;&#20840;&#28431;&#27934;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#30693;&#36947;&#24213;&#23618;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25991;&#26412;&#20063;&#21487;&#20197;&#20174;&#23884;&#20837;&#20013;&#37325;&#26500;&#12290;&#23613;&#31649;&#24050;&#32463;&#25506;&#35752;&#20102;&#38450;&#24481;&#26426;&#21046;&#65292;&#20294;&#36825;&#20123;&#26426;&#21046;&#19987;&#27880;&#20110;&#33521;&#35821;&#65292;&#20351;&#20854;&#20182;&#35821;&#35328;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#26412;&#25991;&#36890;&#36807;&#22810;&#35821;&#35328;&#23884;&#20837;&#36870;&#36716;&#25506;&#35752;&#20102;LLM&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#40657;&#30418;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#36870;&#36716;&#25915;&#20987;&#30340;&#38382;&#39064;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#23427;&#20204;&#21487;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;LLMs&#21487;&#33021;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#22522;&#20110;&#33521;&#35821;&#30340;&#38450;&#24481;&#21487;&#33021;&#26080;&#25928;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#65292;&#23545;b&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.12192v2 Announce Type: replace-cross  Abstract: Textual data is often represented as realnumbered embeddings in NLP, particularly with the popularity of large language models (LLMs) and Embeddings as a Service (EaaS). However, storing sensitive information as embeddings can be vulnerable to security breaches, as research shows that text can be reconstructed from embeddings, even without knowledge of the underlying model. While defence mechanisms have been explored, these are exclusively focused on English, leaving other languages vulnerable to attacks. This work explores LLM security through multilingual embedding inversion. We define the problem of black-box multilingual and cross-lingual inversion attacks, and thoroughly explore their potential implications. Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English based defences may be ineffective. To alleviate this, we propose a simple masking defense effective for b
&lt;/p&gt;</description></item><item><title>VideoPoet&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#22810;&#31181;&#26465;&#20214;&#20449;&#21495;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#35270;&#39057;&#21450;&#21305;&#37197;&#38899;&#39057;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#35270;&#39057;&#29983;&#25104;&#39046;&#22495;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.14125</link><description>&lt;p&gt;
VideoPoet&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#39057;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VideoPoet: A Large Language Model for Zero-Shot Video Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14125
&lt;/p&gt;
&lt;p&gt;
VideoPoet&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#22810;&#31181;&#26465;&#20214;&#20449;&#21495;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#35270;&#39057;&#21450;&#21305;&#37197;&#38899;&#39057;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#35270;&#39057;&#29983;&#25104;&#39046;&#22495;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VideoPoet&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#21508;&#31181;&#19981;&#21516;&#30340;&#26465;&#20214;&#20449;&#21495;&#20013;&#21512;&#25104;&#39640;&#36136;&#37327;&#35270;&#39057;&#21450;&#21305;&#37197;&#38899;&#39057;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;VideoPoet&#37319;&#29992;&#35299;&#30721;&#22120;-&#20165;Transformer&#26550;&#26500;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#21253;&#25324;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#12290;&#35757;&#32451;&#21327;&#35758;&#36981;&#24490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#24335;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#39044;&#35757;&#32451;&#21644;&#29305;&#23450;&#20219;&#21153;&#30340;&#36866;&#24212;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;VideoPoet&#22312;&#33258;&#22238;&#24402;Transformer&#26694;&#26550;&#20013;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#29983;&#25104;&#30446;&#26631;&#30340;&#28151;&#21512;&#12290;&#39044;&#35757;&#32451;&#30340;LLM&#20316;&#20026;&#19968;&#20010;&#22522;&#30784;&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23454;&#35777;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#33021;&#21147;&#65292;&#29305;&#21035;&#31361;&#20986;&#20102;VideoPoet&#29983;&#25104;&#39640;&#20445;&#30495;&#36816;&#21160;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14125v2 Announce Type: replace-cross  Abstract: We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/
&lt;/p&gt;</description></item><item><title>KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.06185</link><description>&lt;p&gt;
KnowGPT&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
KnowGPT: Black-Box Knowledge Injection for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06185
&lt;/p&gt;
&lt;p&gt;
KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#25552;&#20379;&#20114;&#21160;&#24335;API&#65292;&#21487;&#20197;&#20197;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#25110;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#32473;&#20986;&#19981;&#20934;&#30830;&#25110;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#36825;&#20123;&#30693;&#35782;&#24182;&#26410;&#21253;&#21547;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;LLMs&#24182;&#38750;&#24320;&#28304;&#65292;&#36825;&#20351;&#24471;&#20165;&#20351;&#29992;&#27169;&#22411;API&#27880;&#20837;&#30693;&#35782;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KnowGPT&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#22312;&#38382;&#31572;&#20013;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#12290;KnowGPT&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#30693;&#35782;&#22270;&#20013;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#20026;&#27599;&#20010;&#38382;&#39064;&#26500;&#24314;&#26368;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;KnowGPT&#26174;&#33879;&#22686;&#24378;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;KnowGPT&#24179;&#22343;&#25913;&#36827;&#20102;23%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06185v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs), such as ChatGPT, offer interactive APIs that can answer common questions at a human-expert level. However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus. Furthermore, many state-of-the-art LLMs are not open-source, making it challenging to inject knowledge with model APIs only. In this work, we introduce KnowGPT, a black-box knowledge injection framework for LLMs in question answering. KnowGPT leverages deep reinforcement learning (RL) to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed Bandit (MAB) to construct the most suitable prompt for each question. Our extensive experiments on three benchmark datasets showcase that KnowGPT significantly enhances the existing methods. Notably, KnowGPT achieves an average improvement of 23.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2312.04455</link><description>&lt;p&gt;
&#24378;&#21270;&#20851;&#27880;&#21147;&#20013;&#26368;&#30701;&#30340;&#25903;&#26609;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24847;&#35782;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#20851;&#27880;&#20998;&#37197;&#20013;&#30340;&#20869;&#22312;&#27874;&#24418;&#27169;&#24335;&#26174;&#33879;&#24433;&#21709;&#23427;&#20204;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21033;&#29992;LLMs&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#20851;&#38190;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#20013;&#20301;&#20110;&#20851;&#27880;&#27874;&#24418;&#30340;&#20302;&#35895;&#21306;&#22495;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#24573;&#35270;&#35813;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#26032;&#22411;&#25512;&#29702;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;LLMs&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#22788;&#29702;&#36755;&#20837;&#12290;&#27599;&#20010;&#36807;&#31243;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20934;&#35282;&#24230;&#36827;&#34892;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65292;&#20174;&#32780;&#21019;&#24314;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#20851;&#27880;&#27874;&#24418;&#12290;&#36890;&#36807;&#29992;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#20302;&#35895;&#34917;&#20607;&#21478;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#39640;&#23792;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;LLM&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchm
&lt;/p&gt;</description></item><item><title>PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.15042</link><description>&lt;p&gt;
PROXYQA&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15042
&lt;/p&gt;
&lt;p&gt;
PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38271;&#31687;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#29983;&#25104;&#38271;&#31687;&#20869;&#23481;&#65288;&#22914;&#25253;&#21578;&#21644;&#25991;&#31456;&#65289;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#24403;&#21069;&#30340;&#22522;&#20934;&#19981;&#36275;&#20197;&#20805;&#20998;&#35780;&#20272;LLMs&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#19988;&#20840;&#38754;&#30340;&#20869;&#23481;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#31181;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{ProxyQA}&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#65292;&#21253;&#25324;&#28145;&#20837;&#20154;&#24037;&#31574;&#21010;&#30340;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#30340;&#8220;&#20803;&#38382;&#39064;&#8221;&#12290;&#27599;&#20010;&#20803;&#38382;&#39064;&#37117;&#21253;&#21547;&#30456;&#24212;&#30340;&#24102;&#27880;&#37322;&#31572;&#26696;&#30340;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#12290;LLMs&#34987;&#35201;&#27714;&#26681;&#25454;&#36825;&#20123;&#20803;&#38382;&#39064;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#12290;&#21033;&#29992;&#35780;&#20272;&#22120;&#24182;&#23558;&#29983;&#25104;&#30340;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;\textsc{ProxyQA}&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#30340;&#34920;&#29616;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#22810;&#20010;LLMs&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \textit{meta-questions} spanning various domains. Each meta-question contains corresponding \textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \textit{proxy-questions}. We examine multiple LLMs, emphasizing \t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20984;&#20108;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;100&#20493;&#20197;&#19978;&#30340;&#23398;&#20064;&#26102;&#38388;&#25913;&#36827;&#21644;&#39640;&#36798;16%&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.09651</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#21644;&#23398;&#20064;&#30340;&#20984;&#20108;&#32423;&#20248;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning. (arXiv:2401.09651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20984;&#20108;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;100&#20493;&#20197;&#19978;&#30340;&#23398;&#20064;&#26102;&#38388;&#25913;&#36827;&#21644;&#39640;&#36798;16%&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20984;&#20108;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#21644;&#31526;&#21495;&#21442;&#25968;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;NeuPSL&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuPSL&#25512;&#29702;&#30340;&#24179;&#28369;&#21407;&#22987;&#21644;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#26174;&#31034;&#23398;&#20064;&#26799;&#24230;&#26159;&#26368;&#20248;&#23545;&#20598;&#21464;&#37327;&#30340;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#26032;&#30340;&#24418;&#24335;&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#20598;&#22359;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#65292;&#33258;&#28982;&#22320;&#21033;&#29992;&#20102;&#28909;&#21551;&#21160;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30456;&#27604;&#24403;&#21069;&#26368;&#22909;&#30340;NeuPSL&#25512;&#29702;&#26041;&#27861;&#30340;&#23398;&#20064;&#26102;&#38388;&#25913;&#36827;&#20102;100&#20493;&#20197;&#19978;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#30340;8&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#23398;&#20064;&#26694;&#26550;&#30456;&#27604;&#26367;&#20195;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#39640;&#36798;16%&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address a key challenge for neuro-symbolic (NeSy) systems by leveraging convex and bilevel optimization techniques to develop a general gradient-based framework for end-to-end neural and symbolic parameter learning. The applicability of our framework is demonstrated with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over 100x learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across $8$ datasets covering a range of tasks and demonstrate our learning framework achieves up to a 16% point prediction performance improvement over alternative learning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25253;&#21578;&#20102;MLCommons&#31185;&#23398;&#24037;&#20316;&#32452;&#22312;&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#23545;&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#30340;&#21442;&#32771;&#23454;&#29616;&#30340;&#20462;&#25913;&#65292;&#20197;&#23454;&#29616;&#25552;&#21069;&#20572;&#27490;&#12290;</title><link>http://arxiv.org/abs/2401.08636</link><description>&lt;p&gt;
MLCommons&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#19982;&#25552;&#21069;&#20572;&#27490;
&lt;/p&gt;
&lt;p&gt;
MLCommons Cloud Masking Benchmark with Early Stopping. (arXiv:2401.08636v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#21578;&#20102;MLCommons&#31185;&#23398;&#24037;&#20316;&#32452;&#22312;&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#23545;&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#30340;&#21442;&#32771;&#23454;&#29616;&#30340;&#20462;&#25913;&#65292;&#20197;&#23454;&#29616;&#25552;&#21069;&#20572;&#27490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#21578;&#20102;MLCommons&#31185;&#23398;&#24037;&#20316;&#32452;&#22312;&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24037;&#20316;&#12290; MLCommons&#26159;&#19968;&#20010;&#32852;&#30431;&#65292;&#24320;&#21457;&#21644;&#32500;&#25252;&#20960;&#20010;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#22312;&#32445;&#32422;&#22823;&#23398;&#21644;&#24343;&#21513;&#23612;&#20122;&#22823;&#23398;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#38598;&#32676;&#20197;&#21450;&#26222;&#36890;&#26700;&#38754;&#19978;&#36827;&#34892;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#30340;&#25551;&#36848;&#65292;&#24182;&#23545;&#25105;&#20204;&#22312;MLCommons&#22522;&#20934;&#27979;&#35797;&#23454;&#39564;&#20013;&#30340;&#25552;&#20132;&#36827;&#34892;&#20102;&#24635;&#32467;&#12290;&#23427;&#21253;&#25324;&#23545;&#20113;&#36974;&#25377;&#22522;&#20934;&#27979;&#35797;&#30340;&#21442;&#32771;&#23454;&#29616;&#30340;&#20462;&#25913;&#65292;&#20197;&#23454;&#29616;&#25552;&#21069;&#20572;&#27490;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#36890;&#36807;&#33258;&#23450;&#20041;&#25209;&#22788;&#29702;&#33050;&#26412;&#22312;&#32445;&#32422;&#22823;&#23398;&#30340;HPC&#19978;&#25191;&#34892;&#65292;&#35813;&#25209;&#22788;&#29702;&#33050;&#26412;&#36890;&#36807;&#25209;&#22788;&#29702;&#38431;&#21015;&#31995;&#32479;&#36816;&#34892;&#21508;&#31181;&#23454;&#39564;&#65292;&#24182;&#20801;&#35768;&#23545;&#35757;&#32451;&#36718;&#25968;&#36827;&#34892;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#25552;&#20132;&#21253;&#25324;&#20462;&#25913;&#21518;&#30340;&#20195;&#30721;&#65292;&#33258;&#23450;&#20041;&#30340;&#25209;&#22788;&#29702;&#33050;&#26412;&#26469;&#20462;&#25913;&#35757;&#32451;&#36718;&#25968;&#65292;&#25991;&#26723;&#21644;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we report on work performed for the MLCommons Science Working Group on the cloud masking benchmark. MLCommons is a consortium that develops and maintains several scientific benchmarks that aim to benefit developments in AI. The benchmarks are conducted on the High Performance Computing (HPC) Clusters of New York University and University of Virginia, as well as a commodity desktop. We provide a description of the cloud masking benchmark, as well as a summary of our submission to MLCommons on the benchmark experiment we conducted. It includes a modification to the reference implementation of the cloud masking benchmark enabling early stopping. This benchmark is executed on the NYU HPC through a custom batch script that runs the various experiments through the batch queuing system while allowing for variation on the number of epochs trained. Our submission includes the modified code, a custom batch script to modify epochs, documentation, and the benchmark results. We repor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;rebus&#35868;&#39064;&#19978;&#24615;&#33021;&#30340;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#21457;&#29616;&#19987;&#26377;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27979;&#35797;&#27169;&#22411;&#65292;&#20294;&#26368;&#20339;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20165;&#20026;24%&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#19978;&#30340;&#20027;&#35201;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.05604</link><description>&lt;p&gt;
REBUS: &#19968;&#31181;&#23545;&#31526;&#21495;&#29702;&#35299;&#36827;&#34892;&#40065;&#26834;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
REBUS: A Robust Evaluation Benchmark of Understanding Symbols. (arXiv:2401.05604v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;rebus&#35868;&#39064;&#19978;&#24615;&#33021;&#30340;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#21457;&#29616;&#19987;&#26377;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27979;&#35797;&#27169;&#22411;&#65292;&#20294;&#26368;&#20339;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20165;&#20026;24%&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#19978;&#30340;&#20027;&#35201;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;rebus&#35868;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;333&#20010;&#21407;&#22987;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#25991;&#23383;&#28216;&#25103;&#31034;&#20363;&#65292;&#28085;&#30422;&#20102;&#30005;&#24433;&#12289;&#20316;&#26354;&#23478;&#12289;&#20027;&#35201;&#22478;&#24066;&#21644;&#39135;&#29289;&#31561;13&#20010;&#31867;&#21035;&#12290;&#20026;&#20102;&#22312;&#35782;&#21035;&#25552;&#31034;&#30340;&#35789;&#35821;&#25110;&#30701;&#35821;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#27169;&#22411;&#24517;&#39035;&#32467;&#21512;&#22270;&#20687;&#35782;&#21035;&#21644;&#23383;&#31526;&#20018;&#25805;&#20316;&#65292;&#36827;&#34892;&#20551;&#35774;&#26816;&#39564;&#12289;&#22810;&#27493;&#25512;&#29702;&#21644;&#23545;&#20154;&#31867;&#35748;&#30693;&#30340;&#29702;&#35299;&#65292;&#36825;&#20351;&#24471;&#35780;&#20272;&#33021;&#21147;&#21464;&#24471;&#22797;&#26434;&#32780;&#22810;&#27169;&#24577;&#12290;&#25105;&#20204;&#21457;&#29616;&#19987;&#26377;&#27169;&#22411;&#22914;GPT-4V&#21644;Gemini Pro&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#27979;&#35797;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#22909;&#30340;&#27169;&#22411;&#20063;&#21482;&#26377;24%&#30340;&#26368;&#32456;&#20934;&#30830;&#29575;&#65292;&#31361;&#26174;&#20986;&#22312;&#25512;&#29702;&#26041;&#38754;&#38656;&#35201;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#24456;&#23569;&#29702;&#35299;&#35868;&#39064;&#30340;&#25152;&#26377;&#37096;&#20998;&#65292;&#20960;&#20046;&#24635;&#26159;&#26080;&#27861;&#20107;&#21518;&#35299;&#37322;&#27491;&#30830;&#31572;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#30340;&#20027;&#35201;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new benchmark evaluating the performance of multimodal large language models on rebus puzzles. The dataset covers 333 original examples of image-based wordplay, cluing 13 categories such as movies, composers, major cities, and food. To achieve good performance on the benchmark of identifying the clued word or phrase, models must combine image recognition and string manipulation with hypothesis testing, multi-step reasoning, and an understanding of human cognition, making for a complex, multimodal evaluation of capabilities. We find that proprietary models such as GPT-4V and Gemini Pro significantly outperform all other tested models. However, even the best model has a final accuracy of just 24%, highlighting the need for substantial improvements in reasoning. Further, models rarely understand all parts of a puzzle, and are almost always incapable of retroactively explaining the correct answer. Our benchmark can therefore be used to identify major shortcomings in the knowle
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#20110;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25925;&#38556;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.02429</link><description>&lt;p&gt;
&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65306;&#35843;&#26597;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Brain-Inspired Spiking Neural Networks for Industrial Fault Diagnosis: A Survey, Challenges, and Opportunities. (arXiv:2401.02429v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02429
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33041;&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#20110;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#24037;&#19994;&#25925;&#38556;&#35786;&#26029;&#65288;IFD&#65289;&#20316;&#20026;&#19968;&#38376;&#20851;&#27880;&#26816;&#27979;&#21644;&#25910;&#38598;&#24037;&#19994;&#35774;&#22791;&#20581;&#24247;&#29366;&#20917;&#37325;&#35201;&#20449;&#24687;&#30340;&#23398;&#31185;&#32780;&#20986;&#29616;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#23545;&#25925;&#38556;&#31867;&#22411;&#21644;&#20005;&#37325;&#31243;&#24230;&#30340;&#35782;&#21035;&#12290;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25925;&#38556;&#35782;&#21035;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23548;&#33268;&#23545;&#33258;&#21160;&#21270;&#35774;&#22791;&#30417;&#27979;&#30340;&#20851;&#27880;&#65292;&#20197;&#36991;&#20813;&#23433;&#20840;&#20107;&#25925;&#24182;&#20943;&#23569;&#23545;&#20154;&#21147;&#30340;&#20381;&#36182;&#12290;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#20986;&#29616;&#22312;&#22686;&#24378;&#26234;&#33021;IFD&#31639;&#27861;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#25968;&#25454;&#32972;&#26223;&#19979;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20316;&#20026;&#19968;&#31181;&#31616;&#21270;&#30340;&#20223;&#29983;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;ANNs&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#22914;&#36164;&#28304;&#21644;&#25968;&#25454;&#20381;&#36182;&#24615;&#20197;&#21450;&#21463;&#38480;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#22522;&#20110;&#33041;&#21551;&#21457;&#35745;&#31639;&#21407;&#29702;&#30340;&#31532;&#19977;&#20195;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent decades, Industrial Fault Diagnosis (IFD) has emerged as a crucial discipline concerned with detecting and gathering vital information about industrial equipment's health condition, thereby facilitating the identification of failure types and severities. The pursuit of precise and effective fault recognition has garnered substantial attention, culminating in a focus on automating equipment monitoring to preclude safety accidents and reduce reliance on human labor. The advent of artificial neural networks (ANNs) has been instrumental in augmenting intelligent IFD algorithms, particularly in the context of big data. Despite these advancements, ANNs, being a simplified biomimetic neural network model, exhibit inherent limitations such as resource and data dependencies and restricted cognitive capabilities. To address these limitations, the third-generation Spiking Neural Network (SNN), founded on principles of Brain-inspired computing, has surfaced as a promising alternative. Th
&lt;/p&gt;</description></item><item><title>TinyLlama&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22522;&#20110;Llama 2&#30340;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#65292;&#21033;&#29992;&#21508;&#31181;&#20808;&#36827;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#20294;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.02385</link><description>&lt;p&gt;
TinyLlama&#65306;&#19968;&#20010;&#24320;&#28304;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TinyLlama: An Open-Source Small Language Model. (arXiv:2401.02385v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02385
&lt;/p&gt;
&lt;p&gt;
TinyLlama&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22522;&#20110;Llama 2&#30340;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#65292;&#21033;&#29992;&#21508;&#31181;&#20808;&#36827;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#20294;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TinyLlama&#65292;&#19968;&#20010;&#26377;&#38480;&#30340;1.1B&#35821;&#35328;&#27169;&#22411;&#65292;&#22823;&#32422;&#39044;&#35757;&#32451;&#20102;1&#19975;&#20159;&#20010;&#26631;&#35760;&#65292;&#35757;&#32451;&#36718;&#25968;&#32422;&#20026;3&#36718;&#12290;TinyLlama&#22522;&#20110;Llama 2&#30340;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#65292;&#22312;&#24320;&#28304;&#31038;&#21306;&#30340;&#36129;&#29486;&#22522;&#30784;&#19978;&#65288;&#20363;&#22914;FlashAttention&#65289;&#65292;&#21033;&#29992;&#21508;&#31181;&#20808;&#36827;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23613;&#31649;&#35268;&#27169;&#30456;&#23545;&#36739;&#23567;&#65292;TinyLlama&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23427;&#26126;&#26174;&#20248;&#20110;&#20855;&#26377;&#31867;&#20284;&#35268;&#27169;&#30340;&#29616;&#26377;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26816;&#26597;&#28857;&#21644;&#20195;&#30721;&#21487;&#22312;GitHub&#19978;&#20844;&#24320;&#33719;&#21462;&#65292;&#32593;&#22336;&#20026;https://github.com/jzhang38/TinyLlama&#12290;
&lt;/p&gt;
&lt;p&gt;
We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConPreDiff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#39044;&#27979;&#26469;&#25913;&#21892;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#35299;&#30721;&#22120;&#40723;&#21169;&#27599;&#20010;&#28857;&#39044;&#27979;&#20854;&#37051;&#22495;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25512;&#29702;&#38454;&#27573;&#21435;&#38500;&#35299;&#30721;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#37325;&#24314;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2401.02015</link><description>&lt;p&gt;
&#25913;&#36827;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#21512;&#25104;&#19982;&#19978;&#19979;&#25991;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Diffusion-Based Image Synthesis with Context Prediction. (arXiv:2401.02015v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConPreDiff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#39044;&#27979;&#26469;&#25913;&#21892;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#35299;&#30721;&#22120;&#40723;&#21169;&#27599;&#20010;&#28857;&#39044;&#27979;&#20854;&#37051;&#22495;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25512;&#29702;&#38454;&#27573;&#21435;&#38500;&#35299;&#30721;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#37325;&#24314;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65292;&#26497;&#22823;&#25552;&#21319;&#20102;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;&#20687;&#32032;&#25110;&#29305;&#24449;&#32422;&#26463;&#22312;&#31354;&#38388;&#36724;&#19978;&#23545;&#25439;&#22351;&#22270;&#20687;&#36827;&#34892;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#28857;&#23545;&#28857;&#30340;&#37325;&#24314;&#21487;&#33021;&#26080;&#27861;&#23436;&#20840;&#20445;&#30041;&#27599;&#20010;&#39044;&#27979;&#20687;&#32032;/&#29305;&#24449;&#30340;&#37051;&#22495;&#19978;&#19979;&#25991;&#65292;&#24433;&#21709;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;ConPreDiff&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#39044;&#27979;&#25913;&#36827;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#21512;&#25104;&#12290;&#35757;&#32451;&#38454;&#27573;&#65292;&#22312;&#25193;&#25955;&#21435;&#22122;&#22359;&#30340;&#26411;&#31471;&#22686;&#21152;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#35299;&#30721;&#22120;&#65292;&#26126;&#30830;&#22320;&#40723;&#21169;&#27599;&#20010;&#28857;&#39044;&#27979;&#20854;&#37051;&#22495;&#19978;&#19979;&#25991;&#65288;&#21363;&#22810;&#27493;&#38271;&#29305;&#24449;/&#20196;&#29260;/&#20687;&#32032;&#65289;&#65292;&#24182;&#22312;&#25512;&#29702;&#38454;&#27573;&#21435;&#38500;&#35299;&#30721;&#22120;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#27599;&#20010;&#28857;&#21487;&#20197;&#26356;&#22909;&#22320;&#37325;&#24314;&#33258;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a new class of generative models, and have dramatically promoted image generation with unprecedented quality and diversity. Existing diffusion models mainly try to reconstruct input image from a corrupted one with a pixel-wise or feature-wise constraint along spatial axes. However, such point-based reconstruction may fail to make each predicted pixel/feature fully preserve its neighborhood context, impairing diffusion-based image synthesis. As a powerful source of automatic supervisory signal, context has been well studied for learning representations. Inspired by this, we for the first time propose ConPreDiff to improve diffusion-based image synthesis with context prediction. We explicitly reinforce each point to predict its neighborhood context (i.e., multi-stride features/tokens/pixels) with a context decoder at the end of diffusion denoising blocks in training stage, and remove the decoder for inference. In this way, each point can better reconstruct itself by 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.01537</link><description>&lt;p&gt;
&#27450;&#39575;&#30340;&#33402;&#26415;&#65306;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#30340;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. (arXiv:2401.01537v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01537
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#34892;&#19994;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#39046;&#22495;&#27491;&#22312;&#32463;&#21382;&#22686;&#38271;&#30340;&#23454;&#26045;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#38271;&#24341;&#21457;&#20102;&#23545;AI&#38450;&#24481;&#26426;&#21046;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26469;&#33258;&#19981;&#23436;&#20840;&#21487;&#20449;&#30340;&#31532;&#19977;&#26041;&#25552;&#20379;&#21830;&#30340;&#28508;&#22312;&#38544;&#34109;&#25915;&#20987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21548;&#35273;&#21518;&#38376;&#21487;&#33021;&#20351;&#29992;&#26576;&#20123;&#20462;&#25913;&#20316;&#20026;&#20854;&#21551;&#21160;&#26426;&#21046;&#12290;DynamicTrigger&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#34987;&#24341;&#20837;&#65292;&#29992;&#20110;&#36827;&#34892;&#20351;&#29992;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#26469;&#30830;&#20445;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#30340;&#21160;&#24577;&#21518;&#38376;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#27874;&#21160;&#30340;&#20449;&#21495;&#37319;&#26679;&#29575;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#22768;&#38899;&#35302;&#21457;&#22120;&#65288;&#27604;&#22914;&#25293;&#25163;&#22768;&#65289;&#23545;&#35828;&#35805;&#32773;&#36523;&#20221;&#36827;&#34892;&#25513;&#30422;&#65292;&#21487;&#20197;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65288;ASR&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#65292;DynamicTrigger&#22312;&#38544;&#34109;&#25915;&#20987;&#20013;&#26082;&#26377;&#25928;&#21448;&#38544;&#34109;&#65292;&#24182;&#22312;&#25915;&#20987;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The area of Machine Learning as a Service (MLaaS) is experiencing increased implementation due to recent advancements in the AI (Artificial Intelligence) industry. However, this spike has prompted concerns regarding AI defense mechanisms, specifically regarding potential covert attacks from third-party providers that cannot be entirely trusted. Recent research has uncovered that auditory backdoors may use certain modifications as their initiating mechanism. DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor attacks that use cleverly designed tweaks to ensure that corrupted samples are indistinguishable from clean. By utilizing fluctuating signal sampling rates and masking speaker identities through dynamic sound triggers (such as the clapping of hands), it is possible to deceive speech recognition systems (ASR). Our empirical testing demonstrates that DynamicTrigger is both potent and stealthy, achieving impressive success rates during covert attacks while 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#21253;&#21547;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#28201;&#39034;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.13544</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Piecewise polynomial regression of tame functions via integer programming. (arXiv:2311.13544v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#21253;&#21547;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#28201;&#39034;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20272;&#35745;&#23646;&#20110;&#19968;&#31867;&#29305;&#23450;&#30340;&#38750;&#20809;&#28369;&#20989;&#25968;&#30340;&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#21363;&#25152;&#35859;&#30340;&#28201;&#39034;&#20989;&#25968;&#12290;&#36825;&#20123;&#20989;&#25968;&#20986;&#29616;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65306;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;&#12289;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#20215;&#20540;&#20989;&#25968;&#25110;&#23567;&#20998;&#23376;&#30340;&#27874;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28201;&#39034;&#20989;&#25968;&#22312;&#20219;&#20309;&#23436;&#20840;&#32500;&#24230;&#30340;&#31435;&#26041;&#20307;&#19978;&#21487;&#29992;&#20998;&#27573;&#22810;&#39033;&#24335;&#26469;&#36924;&#36817;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#24418;&#24335;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#28201;&#39034;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of estimating functions belonging to a specific class of nonsmooth functions, namely so-called tame functions. These functions appear in a wide range of applications: training deep learning, value functions of mixed-integer programs, or wave functions of small molecules. We show that tame functions are approximable by piecewise polynomials on any full-dimensional cube. We then present the first ever mixed-integer programming formulation of piecewise polynomial regression. Together, these can be used to estimate tame functions. We demonstrate promising computational results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#65292;&#20294;&#36890;&#36807;&#20351;&#29992;&#22238;&#28335;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#33719;&#24471;&#22823;&#24133;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.08516</link><description>&lt;p&gt;
LLMs&#26080;&#27861;&#25214;&#21040;&#25512;&#29702;&#38169;&#35823;&#65292;&#20294;&#21487;&#20197;&#32416;&#27491;&#23427;&#20204;&#65281;&#65288;arXiv&#65306;2311.08516v2 [cs.AI] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
LLMs cannot find reasoning errors, but can correct them!. (arXiv:2311.08516v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#65292;&#20294;&#36890;&#36807;&#20351;&#29992;&#22238;&#28335;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#33719;&#24471;&#22823;&#24133;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#25105;&#32416;&#27491;&#22312;&#25913;&#21892;LLM&#36755;&#20986;&#30340;&#39118;&#26684;&#21644;&#36136;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65288;&#20363;&#22914;Chen&#31561;&#65292;2023&#65307;Madaan&#31561;&#65292;2023&#65289;&#65292;&#26368;&#36817;&#23545;&#36923;&#36753;&#25110;&#25512;&#29702;&#38169;&#35823;&#36827;&#34892;&#33258;&#25105;&#32416;&#27491;&#30340;&#23581;&#35797;&#36890;&#24120;&#20250;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#21464;&#20026;&#38169;&#35823;&#65292;&#20174;&#32780;&#24635;&#20307;&#34920;&#29616;&#21464;&#24046;&#65288;Huang&#31561;&#65292;2023&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20998;&#35299;&#20026;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65306;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#12290;&#23545;&#20110;&#38169;&#35823;&#21457;&#29616;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;BIG-Bench Mistake&#65292;&#36825;&#26159;&#19968;&#20010;Chain-of-Thought&#25512;&#29702;&#36712;&#36857;&#20013;&#30340;&#36923;&#36753;&#38169;&#35823;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20026;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#25552;&#20379;&#22522;&#20934;&#25968;&#65292;&#24182;&#35777;&#26126;LLM&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#12290;&#23545;&#20110;&#36755;&#20986;&#32416;&#27491;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#28335;&#26041;&#27861;&#65292;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#21487;&#20197;&#22823;&#24133;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;&#22238;&#28335;&#35299;&#37322;&#20026;&#23545;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#36731;&#37327;&#32423;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;60-70&#65285;&#20934;&#30830;&#29575;&#19979;&#20445;&#25345;&#26377;&#25928;&#24615;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we break down the self-correction process into two core components: mistake finding and output correction. For mistake finding, we release BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought reasoning traces. We provide benchmark numbers for several state-of-the-art LLMs, and demonstrate that LLMs generally struggle with finding logical mistakes. For output correction, we propose a backtracking method which provides large improvements when given information on mistake location. We construe backtracking as a lightweight alternative to reinforcement learning methods, and show that it remains effective with a reward model at 60-70% accuracy.
&lt;/p&gt;</description></item><item><title>VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2311.01623</link><description>&lt;p&gt;
VQPy&#65306;&#19968;&#31181;&#38754;&#21521;&#29616;&#20195;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01623
&lt;/p&gt;
&lt;p&gt;
VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#24403;&#20170;&#31995;&#32479;&#21644;&#26381;&#21153;&#20013;&#12290;&#22312;&#35270;&#39057;&#20998;&#26512;&#30340;&#21069;&#27839;&#26159;&#29992;&#25143;&#24320;&#21457;&#30340;&#35270;&#39057;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#29305;&#23450;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#65288;&#20363;&#22914;&#20154;&#65292;&#21160;&#29289;&#65292;&#27773;&#36710;&#31561;&#65289;&#19982;&#20256;&#32479;&#38754;&#21521;&#23545;&#35937;&#35821;&#35328;&#24314;&#27169;&#30340;&#23545;&#35937;&#30456;&#20284;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21517;&#20026;VQPy&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#31471;&#65288;&#19968;&#31181;Python&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#21487;&#20197;&#34920;&#36798;&#35270;&#39057;&#23545;&#35937;&#21450;&#20854;&#20132;&#20114;&#30340;&#32467;&#26500;&#65289;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#33258;&#21160;&#29983;&#25104;&#21644;&#20248;&#21270;&#31649;&#36947;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#26045;&#21644;&#24320;&#28304;&#20102;VQPy&#65292;&#23427;&#24050;&#32463;&#20316;&#20026;Cisco DeepVision&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#20135;&#21697;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#20934;&#30830;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36824;&#26174;&#31034;&#20986;&#23545;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00619</link><description>&lt;p&gt;
&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#25439;&#22833;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Loss Modeling for Multi-Annotator Datasets. (arXiv:2311.00619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00619
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#20934;&#30830;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36824;&#26174;&#31034;&#20986;&#23545;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#27491;&#24615;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#27880;&#37322;&#32773;&#30340;&#24847;&#35265;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#27880;&#37322;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#65292;&#20010;&#21035;&#27880;&#37322;&#32773;&#32463;&#24120;&#20250;&#25552;&#20379;&#25968;&#21315;&#20010;&#35780;&#20998;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#30130;&#21171;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27880;&#37322;&#36807;&#31243;&#21487;&#33021;&#20250;&#25345;&#32493;&#22810;&#22825;&#65292;&#21487;&#33021;&#23548;&#33268;&#23545;&#27880;&#37322;&#32773;&#30340;&#24847;&#35265;&#38543;&#26102;&#38388;&#30340;&#19981;&#20934;&#30830;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#26356;&#20934;&#30830;&#30340;&#22810;&#26679;&#24847;&#35265;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#26032;&#39062;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#28165;&#26970;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20462;&#25913;&#21487;&#20197;&#25913;&#21892;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#24212;&#29992;&#20110;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20173;&#28982;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accounting for the opinions of all annotators of a dataset is critical for fairness. However, when annotating large datasets, individual annotators will frequently provide thousands of ratings which can lead to fatigue. Additionally, these annotation processes can occur over multiple days which can lead to an inaccurate representation of an annotator's opinion over time. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, we demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17022</link><description>&lt;p&gt;
&#21463;&#25511;&#35299;&#30721;&#26469;&#33258;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#65292;&#29992;&#20110;&#25511;&#21046;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#33719;&#24471;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#20540;&#20989;&#25968;&#26469;&#35299;&#20915;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#20540;&#20989;&#25968;&#34987;&#31216;&#20026;&#21069;&#32512;&#35780;&#20998;&#22120;&#12290;&#21069;&#32512;&#35780;&#20998;&#22120;&#22312;&#25512;&#29702;&#26102;&#29992;&#20110;&#24341;&#23548;&#29983;&#25104;&#21521;&#26356;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21069;&#32512;&#35780;&#20998;&#22120;&#21487;&#20197;&#20174;&#65288;&#21487;&#33021;&#26159;&#65289;&#31163;&#31574;&#30053;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#20174;&#37096;&#20998;&#35299;&#30721;&#30340;&#21709;&#24212;&#32487;&#32493;&#35299;&#30721;&#26102;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;Reddit&#23545;&#35805;&#35821;&#26009;&#24211;&#19978;&#32463;&#39564;&#35777;&#26126;&#65292;CD&#20316;&#20026;&#19968;&#31181;&#25511;&#21046;&#26426;&#21046;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CD&#35774;&#35745;&#30340;&#27169;&#22359;&#21270;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#20219;&#20309;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CD&#21487;&#20197;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#22359;&#26041;&#24335;&#22312;&#25512;&#29702;&#26102;&#24212;&#29992;&#65292;&#21516;&#26679;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#23454;&#29616;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;Quality Diversity through Human Feedback&#65292;QDHF&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25512;&#26029;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#25193;&#23637;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;Quality Diversity&#65292;QD&#65289;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;QDHF&#22312;&#33258;&#21160;&#22810;&#26679;&#24615;&#21457;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;QD&#30456;&#21305;&#37197;&#30340;&#25628;&#32034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12103</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#23454;&#29616;&#36136;&#37327;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quality Diversity through Human Feedback. (arXiv:2310.12103v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#23454;&#29616;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;Quality Diversity through Human Feedback&#65292;QDHF&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25512;&#26029;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#25193;&#23637;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;Quality Diversity&#65292;QD&#65289;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;QDHF&#22312;&#33258;&#21160;&#22810;&#26679;&#24615;&#21457;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;QD&#30456;&#21305;&#37197;&#30340;&#25628;&#32034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#22312;&#25552;&#39640;&#23450;&#24615;&#20219;&#21153;&#30340;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#24403;&#20165;&#23558;&#20854;&#27010;&#24565;&#21270;&#20026;&#26368;&#22823;&#21270;&#24179;&#22343;&#20154;&#31867;&#20559;&#22909;&#30340;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26426;&#21046;&#26102;&#65292;&#29305;&#21035;&#26159;&#22312;&#35201;&#27714;&#22810;&#26679;&#21270;&#27169;&#22411;&#21709;&#24212;&#30340;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#65292;&#20854;&#25928;&#26524;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#33268;&#21147;&#20110;&#23547;&#25214;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QD&#65289;&#31639;&#27861;&#36890;&#24120;&#21463;&#21040;&#23545;&#25163;&#21160;&#23450;&#20041;&#22810;&#26679;&#24615;&#25351;&#26631;&#30340;&#20381;&#36182;&#32422;&#26463;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#36807;&#34701;&#21512;&#26469;&#33258;&#20004;&#32773;&#30340;&#35265;&#35299;&#65292;&#21487;&#20197;&#20811;&#26381;RLHF&#21644;QD&#30340;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#23454;&#29616;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QDHF&#65289;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25512;&#26029;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#25193;&#23637;&#20102;QD&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;QD&#26041;&#27861;&#30456;&#27604;&#65292;QDHF&#22312;&#33258;&#21160;&#22810;&#26679;&#24615;&#21457;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19982;QD&#30340;&#25628;&#32034;&#33021;&#21147;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) has exhibited the potential to enhance the performance of foundation models for qualitative tasks. Despite its promise, its efficacy is often restricted when conceptualized merely as a mechanism to maximize learned reward models of averaged human preferences, especially in areas such as image generation which demand diverse model responses. Meanwhile, quality diversity (QD) algorithms, dedicated to seeking diverse, high-quality solutions, are often constrained by the dependency on manually defined diversity metrics. Interestingly, such limitations of RLHF and QD can be overcome by blending insights from both. This paper introduces Quality Diversity through Human Feedback (QDHF), which employs human feedback for inferring diversity metrics, expanding the applicability of QD algorithms. Empirical results reveal that QDHF outperforms existing QD methods regarding automatic diversity discovery, and matches the search capabilities of QD with
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#28040;&#38500;&#23545;&#20110;&#24456;&#22823;&#27169;&#22411;&#26469;&#35828;&#22312;&#35745;&#31639;&#19978;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#20415;&#25463;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07579</link><description>&lt;p&gt;
In-Context Unlearning: &#22522;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#28040;&#38500;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-Context Unlearning: Language Models as Few Shot Unlearners. (arXiv:2310.07579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07579
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#28040;&#38500;&#23545;&#20110;&#24456;&#22823;&#27169;&#22411;&#26469;&#35828;&#22312;&#35745;&#31639;&#19978;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#20415;&#25463;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#23398;&#20064;&#26159;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#21435;&#38500;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#36817;&#26469;&#24341;&#36215;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#38656;&#35201;&#36981;&#23432;&#35832;&#22914;&#34987;&#36951;&#24536;&#26435;&#31561;&#38544;&#31169;&#27861;&#35268;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#22312;&#29256;&#26435;&#38382;&#39064;&#19978;LLM&#65288;&#35821;&#35328;&#27169;&#22411;&#65289;&#23588;&#20854;&#30456;&#20851;&#65292;&#20294;&#22312;&#38750;&#24120;&#22823;&#30340;&#27169;&#22411;&#19978;&#23454;&#29616;&#31934;&#30830;&#28040;&#38500;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36817;&#20284;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20123;&#31639;&#27861;&#20851;&#38190;&#20381;&#36182;&#20110;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#35775;&#38382;&#26469;&#26356;&#26032;&#23427;&#20204;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#30001;&#20110;&#35745;&#31639;&#32422;&#26463;&#25110;&#36890;&#36807;API&#35775;&#38382;LLM&#32780;&#26080;&#27861;&#28385;&#36275;&#36825;&#31181;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#28040;&#38500;&#29305;&#23450;&#30340;&#35757;&#32451;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;i
&lt;/p&gt;
&lt;p&gt;
Machine unlearning, the study of efficiently removing the impact of specific training points on the trained model, has garnered increased attention of late, driven by the need to comply with privacy regulations like the Right to be Forgotten. Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models. To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API. In this work, we propose a new class of unlearning methods for LLMs we call ''In-Context Unlearning'', providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35821;&#27861;&#38169;&#35823;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;&#26041;&#27861;ToolDec&#65292;&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#31639;&#27861;&#28040;&#38500;&#20102;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#65292;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;</title><link>http://arxiv.org/abs/2310.07075</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#23454;&#29616;&#26080;&#35821;&#27861;&#38169;&#35823;&#21644;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding. (arXiv:2310.07075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35821;&#27861;&#38169;&#35823;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;LLM&#24037;&#20855;&#20351;&#29992;&#26041;&#27861;ToolDec&#65292;&#36890;&#36807;&#26377;&#38480;&#29366;&#24577;&#35299;&#30721;&#31639;&#27861;&#28040;&#38500;&#20102;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#65292;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#28041;&#21450;&#23545;&#24037;&#20855;&#28436;&#31034;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#26679;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#24037;&#20855;&#65292;&#35201;&#20040;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#24037;&#20855;&#25991;&#26723;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#24037;&#20855;&#25968;&#37327;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#24120;&#24120;&#20135;&#29983;&#35821;&#27861;&#26080;&#25928;&#30340;&#24037;&#20855;&#35843;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToolDec&#65292;&#19968;&#31181;&#26377;&#38480;&#29366;&#24577;&#26426;&#24341;&#23548;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#29992;&#20110;&#24037;&#20855;&#22686;&#24378;&#30340;LLMs&#12290;ToolDec&#36890;&#36807;&#30830;&#20445;&#26377;&#25928;&#30340;&#24037;&#20855;&#21517;&#31216;&#21644;&#31867;&#22411;&#19968;&#33268;&#30340;&#21442;&#25968;&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#24037;&#20855;&#22686;&#24378;&#30340;LLMs&#20013;&#30340;&#24037;&#20855;&#30456;&#20851;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;ToolDec&#20351;LLM&#33021;&#22815;&#20165;&#20165;&#20351;&#29992;&#23427;&#20204;&#30340;&#21517;&#31216;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26377;&#25928;&#22320;&#36873;&#25321;&#24037;&#20855;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#25968;&#23398;&#20989;&#25968;&#12289;&#30693;&#35782;&#22270;&#35889;&#20851;&#31995;&#21644;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;RESTful API&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#22810;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#21450;&#20854;ToolDec&#22686;&#24378;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promising capabilities in using external tools to solve complex problems. However, existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls. In this paper, we propose ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates tool-related errors for any tool-augmented LLMs by ensuring valid tool names and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation. We evaluated multiple prior methods and their ToolDec-enhanced versions on a variety of tasks involving tools like math functions, knowledge graph relations, and complex real-world RESTful APIs
&lt;/p&gt;</description></item><item><title>COCONUT&#26159;&#19968;&#31181;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32463;&#39564;&#37325;&#25773;&#21644;&#23545;&#27604;&#24335;&#23398;&#20064;&#65292;&#22312;&#35821;&#38899;&#29702;&#35299;&#39046;&#22495;&#20013;&#35299;&#20915;&#20102;&#27169;&#22411;&#22312;&#25345;&#32493;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#38590;&#20197;&#20445;&#25345;&#20043;&#21069;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02699</link><description>&lt;p&gt;
&#25345;&#32493;&#23545;&#27604;&#24335;&#35821;&#38899;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Continual Contrastive Spoken Language Understanding. (arXiv:2310.02699v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02699
&lt;/p&gt;
&lt;p&gt;
COCONUT&#26159;&#19968;&#31181;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32463;&#39564;&#37325;&#25773;&#21644;&#23545;&#27604;&#24335;&#23398;&#20064;&#65292;&#22312;&#35821;&#38899;&#29702;&#35299;&#39046;&#22495;&#20013;&#35299;&#20915;&#20102;&#27169;&#22411;&#22312;&#25345;&#32493;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#38590;&#20197;&#20445;&#25345;&#20043;&#21069;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;&#35821;&#38899;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#31361;&#30772;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#24222;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#25345;&#32493;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#24448;&#24448;&#38590;&#20197;&#20445;&#25345;&#20043;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#37325;&#26032;&#35757;&#32451;&#20960;&#20046;&#24635;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#35774;&#32622;&#19979;&#23398;&#20064;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#29702;&#35299;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;COCONUT&#30340;CIL&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#32463;&#39564;&#37325;&#25773;&#21644;&#23545;&#27604;&#24335;&#23398;&#20064;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#23545;&#20165;&#23545;&#22238;&#25918;&#26679;&#26412;&#24212;&#29992;&#25913;&#36827;&#29256;&#26412;&#30340;&#26631;&#20934;&#26377;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#65292;COCONUT&#36890;&#36807;&#23558;&#21516;&#19968;&#31867;&#21035;&#30340;&#26679;&#26412;&#25289;&#36817;&#24182;&#23558;&#20854;&#20182;&#26679;&#26412;&#25512;&#24320;&#65292;&#20445;&#30041;&#20102;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural networks have shown impressive progress across diverse fields, with speech processing being no exception. However, recent breakthroughs in this area require extensive offline training using large datasets and tremendous computing resources. Unfortunately, these models struggle to retain their previously acquired knowledge when learning new tasks continually, and retraining from scratch is almost always impractical. In this paper, we investigate the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and we propose COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning. Through a modified version of the standard supervised contrastive loss applied only to the rehearsal samples, COCONUT preserves the learned representations by pulling closer samples from the same class and pushing away the others. Moreover, we leverage a multimodal contrastive loss that
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Causal Inference with Attention (CInA)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#27880;&#24847;&#21147;&#30340;&#23545;&#20598;&#20851;&#31995;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2310.00809</link><description>&lt;p&gt;
&#25351;&#21521;&#22240;&#26524;&#22522;&#30784;&#27169;&#22411;: &#22240;&#26524;&#25512;&#26029;&#19982;&#27880;&#24847;&#21147;&#30340;&#23545;&#20598;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Towards Causal Foundation Model: on Duality between Causal Inference and Attention. (arXiv:2310.00809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00809
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Causal Inference with Attention (CInA)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#27880;&#24847;&#21147;&#30340;&#23545;&#20598;&#20851;&#31995;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#23545;&#20598;&#36830;&#25509;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Causal Inference with Attention (CInA)&#30340;&#29702;&#35770;&#19978;&#23436;&#22791;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#30417;&#30563;&#22240;&#26524;&#23398;&#20064;&#65292;&#24182;&#22312;&#26032;&#25968;&#25454;&#30340;&#26410;&#35265;&#20219;&#21153;&#19978;&#23454;&#29616;&#38646;&#26679;&#26412;&#22240;&#26524;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for complex tasks. We propose a novel, theoretically sound method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that our approach
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25506;&#32034;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.03613</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#20005;&#35880;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating ChatGPT as a Recommender System: A Rigorous Approach. (arXiv:2309.03613v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25506;&#32034;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#22823;&#22411;AI&#35821;&#35328;&#27169;&#22411;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#23427;&#20204;&#22312;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#65292;&#21253;&#25324;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#65292;&#22240;&#27492;&#23545;&#20110;&#21508;&#31181;&#29305;&#23450;&#20219;&#21153;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#37322;&#25918;&#20102;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#30740;&#31350;&#30028;&#27491;&#22312;&#31215;&#26497;&#25506;&#32034;&#23427;&#20204;&#30340;&#24212;&#29992;&#65292;ChatGPT&#20063;&#22240;&#27492;&#33719;&#24471;&#20102;&#35748;&#21487;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#26377;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#20854;&#22312;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#20173;&#24453;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#25506;&#31350;ChatGPT&#20316;&#20026;&#38646;-shot&#25512;&#33616;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#21253;&#25324;&#35780;&#20272;&#20854;&#21033;&#29992;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#25512;&#33616;&#12289;&#37325;&#26032;&#25490;&#24207;&#29616;&#26377;&#25512;&#33616;&#21015;&#34920;&#12289;&#21033;&#29992;&#30456;&#20284;&#29992;&#25143;&#30340;&#20449;&#24687;&#20197;&#21450;&#22788;&#29702;&#20919;&#21551;&#21160;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;MovieLens Small&#12289;Last.FM&#21644;Facebook Bo&#65289;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#26469;&#35780;&#20272;ChatGPT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent popularity surrounds large AI language models due to their impressive natural language capabilities. They contribute significantly to language-related tasks, including prompt-based learning, making them valuable for various specific tasks. This approach unlocks their full potential, enhancing precision and generalization. Research communities are actively exploring their applications, with ChatGPT receiving recognition. Despite extensive research on large language models, their potential in recommendation scenarios still needs to be explored. This study aims to fill this gap by investigating ChatGPT's capabilities as a zero-shot recommender system. Our goals include evaluating its ability to use user preferences for recommendations, reordering existing recommendation lists, leveraging information from similar users, and handling cold-start situations. We assess ChatGPT's performance through comprehensive experiments using three datasets (MovieLens Small, Last.FM, and Facebook Bo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25554;&#20540;&#26469;&#35299;&#20915;&#22312;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#21644;&#38750;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20043;&#38388;&#35268;&#27169;&#24046;&#24322;&#24341;&#36215;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2309.01717</link><description>&lt;p&gt;
&#22312;&#19981;&#24179;&#34913;&#30340;&#30740;&#31350;&#25552;&#26696;&#20027;&#39064;&#25512;&#29702;&#20013;&#30340;&#36328;&#23398;&#31185;&#20844;&#24179;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21464;&#25442;&#22120;&#30340;&#20855;&#26377;&#36873;&#25321;&#24615;&#25554;&#20540;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference: A Hierarchical Transformer-based Method with Selective Interpolation. (arXiv:2309.01717v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01717
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25554;&#20540;&#26469;&#35299;&#20915;&#22312;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#21644;&#38750;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20043;&#38388;&#35268;&#27169;&#24046;&#24322;&#24341;&#36215;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#26696;&#20027;&#39064;&#25512;&#29702;&#30340;&#30446;&#26631;&#26159;&#20174;&#36164;&#21161;&#26426;&#26500;&#23450;&#20041;&#30340;&#23398;&#31185;&#20307;&#31995;&#20013;&#33719;&#21462;&#26368;&#21512;&#36866;&#30340;&#23398;&#31185;&#21010;&#20998;&#65292;&#28982;&#21518;&#26426;&#26500;&#23558;&#26681;&#25454;&#36825;&#31181;&#21010;&#20998;&#20174;&#20854;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21512;&#36866;&#30340;&#21516;&#34892;&#35780;&#23457;&#19987;&#23478;&#12290;&#33258;&#21160;&#21270;&#30340;&#20027;&#39064;&#25512;&#29702;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#20027;&#39064;&#22635;&#20889;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24357;&#34917;&#36164;&#21161;&#26426;&#26500;&#21644;&#39033;&#30446;&#30003;&#35831;&#20154;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#12290;&#29616;&#26377;&#26041;&#27861;&#23558;&#20854;&#24314;&#27169;&#20026;&#23618;&#27425;&#24615;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36845;&#20195;&#22320;&#25512;&#29702;&#26368;&#21512;&#36866;&#30340;&#20027;&#39064;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#21644;&#38750;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20043;&#38388;&#35268;&#27169;&#24046;&#24322;&#65292;&#23548;&#33268;&#33258;&#21160;&#25512;&#29702;&#31995;&#32479;&#23558;&#36328;&#23398;&#31185;&#25552;&#26696;&#24402;&#31867;&#20026;&#38750;&#36328;&#23398;&#31185;&#65292;&#36896;&#25104;&#22312;&#19987;&#23478;&#20998;&#37197;&#36807;&#31243;&#20013;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;&#25105;&#20204;&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#21602;&#65311;
&lt;/p&gt;
&lt;p&gt;
The objective of topic inference in research proposals aims to obtain the most suitable disciplinary division from the discipline system defined by a funding agency. The agency will subsequently find appropriate peer review experts from their database based on this division. Automated topic inference can reduce human errors caused by manual topic filling, bridge the knowledge gap between funding agencies and project applicants, and improve system efficiency. Existing methods focus on modeling this as a hierarchical multi-label classification problem, using generative models to iteratively infer the most appropriate topic information. However, these methods overlook the gap in scale between interdisciplinary research proposals and non-interdisciplinary ones, leading to an unjust phenomenon where the automated inference system categorizes interdisciplinary proposals as non-interdisciplinary, causing unfairness during the expert assignment. How can we address this data imbalance issue und
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#24178;&#39044;&#25512;&#24191;&#21040;&#19981;&#21516;&#24773;&#22659;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#24178;&#39044;&#26469;&#33258;&#21160;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#24403;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#26102;&#65292;CB2M&#33021;&#22815;&#26816;&#27979;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2308.13453</link><description>&lt;p&gt;
&#23398;&#20064;&#24178;&#39044;&#27010;&#24565;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Learning to Intervene on Concept Bottlenecks. (arXiv:2308.13453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#24178;&#39044;&#25512;&#24191;&#21040;&#19981;&#21516;&#24773;&#22659;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#24178;&#39044;&#26469;&#33258;&#21160;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#24403;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#26102;&#65292;CB2M&#33021;&#22815;&#26816;&#27979;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#32780;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#36890;&#36807;&#20854;&#27010;&#24565;&#34920;&#31034;&#25552;&#20379;&#22266;&#26377;&#30340;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#20204;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#26356;&#26032;&#27010;&#24565;&#20540;&#24182;&#32416;&#27491;&#27169;&#22411;&#30340;&#39044;&#27979;&#36755;&#20986;&#26469;&#36827;&#34892;&#24178;&#39044;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#20013;&#36825;&#20123;&#24178;&#39044;&#20165;&#24212;&#29992;&#20110;&#27169;&#22411;&#19968;&#27425;&#21518;&#21363;&#34987;&#20002;&#24323;&#12290;&#20026;&#20102;&#32416;&#27491;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36825;&#26159;CBM&#30340;&#19968;&#20010;&#25193;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CB2M&#36890;&#36807;&#21452;&#25240;&#21472;&#35760;&#24518;&#23398;&#20064;&#23558;&#24178;&#39044;&#30340;&#25512;&#24191;&#21040;&#36866;&#24403;&#30340;&#26032;&#24773;&#22659;&#20013;&#65292;&#20174;&#32780;&#33021;&#22815;&#23398;&#20064;&#26816;&#27979;&#38169;&#35823;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#30340;&#24178;&#39044;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CB2M&#33021;&#22815;&#20174;&#26368;&#21021;&#33719;&#24471;&#30340;&#23569;&#37327;&#24178;&#39044;&#20013;&#33258;&#21160;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22914;&#26524;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#65292;CB2M&#21487;&#20197;&#26816;&#27979;&#21040;CBM&#29942;&#39048;&#30340;&#28508;&#22312;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
While traditional deep learning models often lack interpretability, concept bottleneck models (CBMs) provide inherent explanations via their concept representations. Specifically, they allow users to perform interventional interactions on these concepts by updating the concept values and thus correcting the predictive output of the model. Traditionally, however, these interventions are applied to the model only once and discarded afterward. To rectify this, we present concept bottleneck memory models (CB2M), an extension to CBMs. Specifically, a CB2M learns to generalize interventions to appropriate novel situations via a two-fold memory with which it can learn to detect mistakes and to reapply previous interventions. In this way, a CB2M learns to automatically improve model performance from a few initially obtained interventions. If no prior human interventions are available, a CB2M can detect potential mistakes of the CBM bottleneck and request targeted interventions. In our experime
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03887</link><description>&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22870;&#21169;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#30340;&#20998;&#31867;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#28145;&#24230;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#28165;&#26970;&#22320;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#24402;&#22240;&#20110;&#25968;&#25454;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;ProtoPNet&#65289;&#65292;&#23427;&#22522;&#20110;&#36755;&#20837;&#30340;&#26377;&#24847;&#20041;&#37096;&#20998;&#26469;&#23581;&#35797;&#20998;&#31867;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#23398;&#20064;&#20174;&#22270;&#20687;&#30340;&#34394;&#20551;&#25110;&#19981;&#19968;&#33268;&#30340;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#21551;&#21457;&#65292;&#36890;&#36807;&#22312;CUB-200-2011&#25968;&#25454;&#38598;&#19978;&#25910;&#38598;&#20154;&#31867;&#21407;&#22411;&#36136;&#37327;&#30340;1-5&#20998;&#32423;&#27880;&#37322;&#65292;&#26500;&#24314;&#19968;&#20010;&#23398;&#20064;&#35782;&#21035;&#38750;&#34394;&#20551;&#21407;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#26032;&#21152;&#26435;&#12289;&#37325;&#36873;&#21644;&#37325;&#26032;&#35757;&#32451;&#30340;&#21407;&#22411;&#38646;&#20214;&#32593;&#32476;&#65288;R3-ProtoPNet&#65289;&#65292;&#35813;&#32593;&#32476;&#22312;ProtoPNet&#35757;&#32451;&#24490;&#29615;&#20013;&#22686;&#21152;&#20102;&#19977;&#20010;&#39069;&#22806;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The fir
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#22312;Poincar&#233;&#21452;&#26354;&#29699;&#27169;&#22411;&#20013;&#36816;&#29992;&#36229;bolic&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#21306;&#22495;&#20869;&#20687;&#32032;&#23884;&#20837;&#30340;&#21322;&#24452;&#21464;&#21270;&#20316;&#20026;&#26032;&#30340;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#65292;&#20197;&#25552;&#21319;&#22495;&#36716;&#31227;&#19979;&#35821;&#20041;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11180</link><description>&lt;p&gt;
&#36229;bolic&#27963;&#36291;&#23398;&#20064;&#22312;&#22495;&#36716;&#31227;&#19979;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Active Learning for Semantic Segmentation under Domain Shift. (arXiv:2306.11180v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11180
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#22312;Poincar&#233;&#21452;&#26354;&#29699;&#27169;&#22411;&#20013;&#36816;&#29992;&#36229;bolic&#27963;&#36291;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#21306;&#22495;&#20869;&#20687;&#32032;&#23884;&#20837;&#30340;&#21322;&#24452;&#21464;&#21270;&#20316;&#20026;&#26032;&#30340;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#65292;&#20197;&#25552;&#21319;&#22495;&#36716;&#31227;&#19979;&#35821;&#20041;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22495;&#36716;&#31227;&#19979;&#30340;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#22522;&#20110;&#22270;&#20687;&#21306;&#22495;&#21644;&#20266;&#26631;&#31614;&#30340;&#20027;&#21160;&#23398;&#20064;&#33719;&#21462;&#31574;&#30053;&#26159;&#26368;&#20808;&#36827;&#30340;&#12290;&#22312;&#21306;&#22495;&#20869;&#23384;&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#21487;&#20197;&#35782;&#21035;&#20986;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#20687;&#32032;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#20027;&#21160;&#23398;&#20064;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#35745;&#38480;&#21046;&#65292;&#20266;&#26631;&#31614;&#30340;&#21464;&#21270;&#20165;&#38480;&#20110;&#36873;&#25321;&#31867;&#21035;&#30340;&#36718;&#24275;&#65292;&#38480;&#21046;&#20102;&#26368;&#32456;&#30340;&#20027;&#21160;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;Poincar&#233;&#21452;&#26354;&#29699;&#27169;&#22411;&#20013;&#20351;&#29992;&#36229;bolic&#26041;&#27861;&#26469;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#21306;&#22495;&#20869;&#20687;&#32032;&#23884;&#20837;&#30340;&#21322;&#24452;&#21464;&#21270;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#33719;&#21462;&#31574;&#30053;&#12290;&#36825;&#28304;&#20110;&#19968;&#31181;&#26080;&#23618;&#27425;&#32422;&#26463;&#35757;&#32451;&#30340;&#36229;bolic&#31354;&#38388;&#30340;&#26032;&#39062;&#20960;&#20309;&#29305;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#31867;&#21035;&#34987;&#26144;&#23556;&#21040;&#20855;&#26377;&#30456;&#24403;&#20869;&#31867;&#21322;&#24452;&#26041;&#24046;&#30340;&#32039;&#20945;&#36229;bolic&#21306;&#22495;&#65292;&#22240;&#20026;&#27169;&#22411;&#23558;&#38590;&#20197;&#35299;&#37322;&#30340;&#31867;&#21035;&#25918;&#32622;&#22312;&#26356;&#23494;&#38598;&#30340;&#36229;bolic&#21306;&#22495;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the task of semantic segmentation (SS) under domain shift, active learning (AL) acquisition strategies based on image regions and pseudo labels are state-of-the-art (SoA). The presence of diverse pseudo-labels within a region identifies pixels between different classes, which is a labeling efficient active learning data acquisition strategy. However, by design, pseudo-label variations are limited to only select the contours of classes, limiting the final AL performance. We approach AL for SS in the Poincar\'e hyperbolic ball model for the first time and leverage the variations of the radii of pixel embeddings within regions as a novel data acquisition strategy. This stems from a novel geometric property of a hyperbolic space trained without enforced hierarchies, which we experimentally prove. Namely, classes are mapped into compact hyperbolic areas with a comparable intra-class radii variance, as the model places classes of increasing explainable difficulty at denser hyperbolic are
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CompanyKG&#65292;&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#21450;&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.10649</link><description>&lt;p&gt;
CompanyKG:&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;
&lt;/p&gt;
&lt;p&gt;
CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity Quantification. (arXiv:2306.10649v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CompanyKG&#65292;&#19968;&#31181;&#29992;&#20110;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#30340;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#34920;&#31034;&#65292;&#20197;&#21450;&#22810;&#20010;&#35780;&#20272;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#20272;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25237;&#36164;&#34892;&#19994;&#20013;&#65292;&#23545;&#20110;&#35768;&#22810;&#30446;&#30340;&#21253;&#25324;&#24066;&#22330;&#26144;&#23556;&#12289;&#31454;&#20105;&#23545;&#25163;&#20998;&#26512;&#21644;&#24182;&#36141;&#65292;&#36827;&#34892;&#32454;&#31890;&#24230;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#36890;&#24120;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;CompanyKG&#30340;&#30693;&#35782;&#22270;&#65292;&#29992;&#20110;&#34920;&#31034;&#21644;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#20844;&#21496;&#29305;&#24449;&#21644;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;1.17&#30334;&#19975;&#23478;&#20844;&#21496;&#34987;&#34920;&#31034;&#20026;&#33410;&#28857;&#65292;&#20016;&#23500;&#20102;&#20844;&#21496;&#25551;&#36848;&#23884;&#20837;; 15&#31181;&#19981;&#21516;&#30340;&#20844;&#21496;&#38388;&#20851;&#31995;&#23548;&#33268;&#20102;5106&#30334;&#19975;&#20010;&#24102;&#26435;&#37325;&#30340;&#36793;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#20844;&#21496;&#30456;&#20284;&#24615;&#37327;&#21270;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#32534;&#35793;&#20102;&#19977;&#20010;&#24102;&#26377;&#27880;&#37322;&#27979;&#35797;&#38598;&#30340;&#35780;&#20272;&#20219;&#21153;: &#30456;&#20284;&#24615;&#39044;&#27979;&#12289;&#31454;&#20105;&#23545;&#25163;&#26816;&#32034;&#21644;&#30456;&#20284;&#24615;&#25490;&#24207;&#12290;&#25105;&#20204;&#23545;11&#31181;&#21487;&#37325;&#29616;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20998;&#20026;&#33410;&#28857;&#12289;&#36793;&#21644;&#33410;&#28857;+&#36793;&#19977;&#32452;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;CompanyKG&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
In the investment industry, it is often essential to carry out fine-grained company similarity quantification for a range of purposes, including market mapping, competitor analysis, and mergers and acquisitions. We propose and publish a knowledge graph, named CompanyKG, to represent and learn diverse company features and relations. Specifically, 1.17 million companies are represented as nodes enriched with company description embeddings; and 15 different inter-company relations result in 51.06 million weighted edges. To enable a comprehensive assessment of methods for company similarity quantification, we have devised and compiled three evaluation tasks with annotated test sets: similarity prediction, competitor retrieval and similarity ranking. We present extensive benchmarking results for 11 reproducible predictive methods categorized into three groups: node-only, edge-only, and node+edge. To the best of our knowledge, CompanyKG is the first large-scale heterogeneous graph dataset or
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.06777</link><description>&lt;p&gt;
&#25552;&#39640;&#20915;&#31574;&#26641;&#35299;&#37322;&#24615;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Validity of Decision Trees as Explanations. (arXiv:2306.06777v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06777
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20351;&#29992;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#12290;&#36825;&#21487;&#20197;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31454;&#20105;[&#21442;&#35265;Grinsztajn&#31561;&#20154;&#65292;NeurIPS 2022&#65292;arXiv&#65306;2207.08815]&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#21487;&#35299;&#37322;&#24615;&#21462;&#20915;&#20110;&#26641;&#30340;&#28145;&#24230;&#21644;&#27599;&#20010;&#21494;&#33410;&#28857;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20302;&#28145;&#24230;&#30340;&#26641;&#65292;&#20854;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#27599;&#20010;&#21494;&#33410;&#28857;&#19978;&#30340;&#26368;&#22823;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#20174;&#20302;&#28145;&#24230;&#26641;&#30340;&#27599;&#20010;&#21494;&#33410;&#28857;&#8220;&#25346;&#36215;&#8221;&#36827;&#19968;&#27493;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;&#26080;&#38480;&#28145;&#24230;&#30340;&#26641;&#65289;&#12290;&#20302;&#28145;&#24230;&#26641;&#26131;&#20110;&#35299;&#37322;&#65292;&#32780;&#32508;&#21512;&#20302;&#28145;&#24230;&#21644;&#25346;&#36215;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#30340;&#25972;&#20307;&#32479;&#35745;&#24615;&#33021;&#20248;&#20110;&#20351;&#29992;&#32463;&#20856;&#26041;&#27861;&#65288;&#20363;&#22914;CART&#65289;&#35757;&#32451;&#30340;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#20248;&#21270;&#30340;XGBoost&#65289;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classification and forecasting with tabular data, one often utilizes tree-based models. This can be competitive with deep neural networks on tabular data [cf. Grinsztajn et al., NeurIPS 2022, arXiv:2207.08815] and, under some conditions, explainable. The explainability depends on the depth of the tree and the accuracy in each leaf of the tree. Here, we train a low-depth tree with the objective of minimising the maximum misclassification error across each leaf node, and then ``suspend'' further tree-based models (e.g., trees of unlimited depth) from each leaf of the low-depth tree. The low-depth tree is easily explainable, while the overall statistical performance of the combined low-depth and suspended tree-based models improves upon decision trees of unlimited depth trained using classical methods (e.g., CART) and is comparable to state-of-the-art methods (e.g., well-tuned XGBoost).
&lt;/p&gt;</description></item><item><title>EPIC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#36753;&#36317;&#31163;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#26377;&#32467;&#26500;&#21464;&#21270;&#30340;&#26032;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01310</link><description>&lt;p&gt;
EPIC: &#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20195;&#20215;&#23454;&#29616;&#30340;&#32534;&#36753;&#36335;&#24452;&#25554;&#20540;&#30340;&#22270;&#24418;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost. (arXiv:2306.01310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01310
&lt;/p&gt;
&lt;p&gt;
EPIC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#36753;&#36317;&#31163;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#26377;&#32467;&#26500;&#21464;&#21270;&#30340;&#26032;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#22270;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#32463;&#24120;&#38480;&#21046;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EPIC&#65288;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20195;&#20215;&#23454;&#29616;&#30340;&#32534;&#36753;&#36335;&#24452;&#25554;&#20540;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25554;&#20540;&#30340;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22270;&#32534;&#36753;&#36317;&#31163;&#26469;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#32467;&#26500;&#26377;&#25152;&#21464;&#21270;&#30340;&#26032;&#22270;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#24102;&#26631;&#31614;&#30340;&#22270;&#26469;&#23398;&#20064;&#22270;&#32534;&#36753;&#36317;&#31163;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#30693;&#35782;&#22312;&#21407;&#22987;&#22270;&#23545;&#20043;&#38388;&#21019;&#24314;&#20102;&#22270;&#32534;&#36753;&#36335;&#24452;&#12290;&#36890;&#36807;&#20174;&#22270;&#32534;&#36753;&#36335;&#24452;&#20013;&#38543;&#26426;&#25277;&#26679;&#30340;&#22270;&#24418;&#65292;&#25105;&#20204;&#20016;&#23500;&#20102;&#35757;&#32451;&#38598;&#20197;&#22686;&#24378;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based models have become increasingly important in various domains, but the limited size and diversity of existing graph datasets often limit their performance. To address this issue, we propose EPIC (Edit Path Interpolation via learnable Cost), a novel interpolation-based method for augmenting graph datasets. Our approach leverages graph edit distance to generate new graphs that are similar to the original ones but exhibit some variation in their structures. To achieve this, we learn the graph edit distance through a comparison of labeled graphs and utilize this knowledge to create graph edit paths between pairs of original graphs. With randomly sampled graphs from a graph edit path, we enrich the training set to enhance the generalization capability of classification models. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms existing augmentation methods in graph classification tasks.
&lt;/p&gt;</description></item><item><title>RE$^2$&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#21306;&#22495;&#32423;&#31354;&#38388;&#32467;&#26500;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#20851;&#31995;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14590</link><description>&lt;p&gt;
RE$^2$: &#38754;&#21521;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#30340;&#21306;&#22495;&#24863;&#30693;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
RE$^2$: Region-Aware Relation Extraction from Visually Rich Documents. (arXiv:2305.14590v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14590
&lt;/p&gt;
&lt;p&gt;
RE$^2$&#26041;&#27861;&#21033;&#29992;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#21306;&#22495;&#32423;&#31354;&#38388;&#32467;&#26500;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#20851;&#31995;&#39044;&#27979;&#33021;&#21147;&#65292;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#34920;&#21333;&#29702;&#35299;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#38656;&#35201;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24067;&#23616;&#32467;&#26500;&#65288;&#21363;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#65289;&#23545;&#20110;&#20851;&#31995;&#25277;&#21462;&#30340;&#37325;&#35201;&#24615;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; RE$^2$ &#30340;&#21306;&#22495;&#24863;&#30693;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#22359;&#20043;&#38388;&#30340;&#21306;&#22495;&#32423;&#31354;&#38388;&#32467;&#26500;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#20851;&#31995;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36793;&#32536;&#24863;&#30693;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#26469;&#23398;&#20064;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#21516;&#26102;&#32771;&#34385;&#23427;&#20204;&#30340;&#21306;&#22495;&#32423;&#34920;&#31034;&#25152;&#23450;&#20041;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#32422;&#26463;&#30446;&#26631;&#65292;&#26469;&#35268;&#33539;&#27169;&#22411;&#20197;&#31526;&#21512;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#22266;&#26377;&#32422;&#26463;&#26465;&#20214;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#35821;&#35328;&#21644;&#39046;&#22495;&#30340;&#24191;&#27867;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current research in form understanding predominantly relies on large pre-trained language models, necessitating extensive data for pre-training. However, the importance of layout structure (i.e., the spatial relationship between the entity blocks in the visually rich document) to relation extraction has been overlooked. In this paper, we propose REgion-Aware Relation Extraction (RE$^2$) that leverages region-level spatial structure among the entity blocks to improve their relation prediction. We design an edge-aware graph attention network to learn the interaction between entities while considering their spatial relationship defined by their region-level representations. We also introduce a constraint objective to regularize the model towards consistency with the inherent constraints of the relation extraction task. Extensive experiments across various datasets, languages and domains demonstrate the superiority of our proposed approach.
&lt;/p&gt;</description></item><item><title>Chronosymbolic Learning&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#23558;&#31526;&#21495;&#25512;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35299;&#20915;CHC&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;288&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#35768;&#22810;&#20855;&#26377;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.01206</link><description>&lt;p&gt;
Chronosymbolic Learning: &#32467;&#21512;&#31526;&#21495;&#25512;&#29702;&#19982;&#24402;&#32435;&#23398;&#20064;&#30340;&#26377;&#25928;CHC&#27714;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chronosymbolic Learning: Efficient CHC Solving with Symbolic Reasoning and Inductive Learning. (arXiv:2305.01206v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01206
&lt;/p&gt;
&lt;p&gt;
Chronosymbolic Learning&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#23558;&#31526;&#21495;&#25512;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35299;&#20915;CHC&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;288&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#35768;&#22810;&#20855;&#26377;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CHC (Constrained Horn Clauses)&#30340;&#27714;&#35299;&#26159;&#35768;&#22810;&#39564;&#35777;&#21644;&#20998;&#26512;&#20219;&#21153;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#25968;&#25454;&#39537;&#21160;&#27861;&#22312;&#25552;&#39640;CHC&#27714;&#35299;&#25928;&#29575;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#25163;&#21160;&#21019;&#24314;&#21644;&#35843;&#25972;&#21508;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#32321;&#29712;&#24037;&#20316;&#12290;&#20294;&#25968;&#25454;&#39537;&#21160;&#30340;CHC&#27714;&#35299;&#22120;&#19982;&#22522;&#20110;&#31526;&#21495;&#25512;&#29702;&#30340;&#27714;&#35299;&#22120;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;"Chronosymbolic Learning"&#65292;&#23427;&#23558;&#31526;&#21495;&#20449;&#24687;&#21644;&#25968;&#20540;&#25968;&#25454;&#28857;&#32479;&#19968;&#36215;&#26469;&#65292;&#23558;CHC&#31995;&#32479;&#39640;&#25928;&#22320;&#27714;&#35299;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Chronosymbolic Learning&#30340;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#22120;&#21644;&#19968;&#20010;BMC&#26679;&#24335;&#30340;&#25512;&#29702;&#22120;&#12290;&#23613;&#31649;&#35813;&#24037;&#20855;&#38750;&#24120;&#31616;&#21333;&#65292;&#20294;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#21147;&#21644;&#20581;&#22766;&#24615;&#12290;&#23427;&#22312;&#30001;288&#20010;&#22522;&#20934;&#27979;&#35797;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CHC&#27714;&#35299;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#21253;&#21547;&#38750;&#32447;&#24615;&#25972;&#25968;&#31639;&#26415;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving Constrained Horn Clauses (CHCs) is a fundamental challenge behind a wide range of verification and analysis tasks. Data-driven approaches show great promise in improving CHC solving without the painstaking manual effort of creating and tuning various heuristics. However, a large performance gap exists between data-driven CHC solvers and symbolic reasoning-based solvers. In this work, we develop a simple but effective framework, "Chronosymbolic Learning", which unifies symbolic information and numerical data points to solve a CHC system efficiently. We also present a simple instance of Chronosymbolic Learning with a data-driven learner and a BMC-styled reasoner. Despite its great simplicity, experimental results show the efficacy and robustness of our tool. It outperforms state-of-the-art CHC solvers on a dataset consisting of 288 benchmarks, including many instances with non-linear integer arithmetics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;&#65292;&#35752;&#35770;&#20102;&#19981;&#21516;&#30740;&#31350;&#39046;&#22495;&#21644;&#21019;&#24847;&#35774;&#35745;&#22312;&#20854;&#20013;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#26410;&#26469;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.13269</link><description>&lt;p&gt;
&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Game-based Platforms for Artificial Intelligence Research. (arXiv:2304.13269v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;&#65292;&#35752;&#35770;&#20102;&#19981;&#21516;&#30740;&#31350;&#39046;&#22495;&#21644;&#21019;&#24847;&#35774;&#35745;&#22312;&#20854;&#20013;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#20855;&#26377;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#24191;&#27867;&#29305;&#24449;&#65292;&#25104;&#20026;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#29702;&#24819;&#27979;&#35797;&#22522;&#22320;&#65292;&#20849;&#21516;&#30340;&#30740;&#31350;&#39046;&#22495;&#21253;&#25324;&#23398;&#20064;&#21644;&#20248;&#21270;&#12289;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#12289;&#21338;&#24328;&#35770;&#12289;&#35745;&#21010;&#19982;&#25490;&#31243;&#12289;&#35774;&#35745;&#21644;&#25945;&#32946;&#31561;&#12290;&#24050;&#23454;&#26045;&#20102;&#35768;&#22810;&#24320;&#28304;&#28216;&#25103;&#25110;&#22522;&#20110;&#28216;&#25103;&#30340;&#29615;&#22659;&#29992;&#20110;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#12290;&#38500;&#20102;&#21333;&#20154;&#25110;&#22810;&#20154;&#12289;&#21512;&#20316;&#25110;&#23545;&#25239;&#24615;&#28216;&#25103;&#22806;&#65292;&#22312;&#21019;&#24847;&#35774;&#35745;&#26041;&#38754;&#20063;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#24179;&#21488;&#20026;&#25506;&#32034;&#21644;&#27604;&#36739;&#20154;&#24037;&#26234;&#33021;&#30340;&#24605;&#24819;&#21644;&#25216;&#26415;&#25552;&#20379;&#20102;&#29702;&#24819;&#22522;&#20934;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;&#65292;&#35752;&#35770;&#20102;&#30001;&#36825;&#20123;&#24179;&#21488;&#28436;&#21464;&#24341;&#36215;&#30340;&#30740;&#31350;&#36235;&#21183;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Games have been the perfect test-beds for artificial intelligence research for the characteristics that widely exist in real-world scenarios. Learning and optimisation, decision making in dynamic and uncertain environments, game theory, planning and scheduling, design and education are common research areas shared between games and real-world problems. Numerous open-sourced games or game-based environments have been implemented for studying artificial intelligence. In addition to single- or multi-player, collaborative or adversarial games, there has also been growing interest in implementing platforms for creative design in recent years. Those platforms provide ideal benchmarks for exploring and comparing artificial intelligence ideas and techniques. This paper reviews the game-based platforms for artificial intelligence research, discusses the research trend induced by the evolution of those platforms, and gives an outlook.
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#38754;&#20020;&#26799;&#24230;&#33539;&#22260;&#21464;&#21270;&#22823;&#12289;&#26799;&#24230;&#20998;&#24067;&#38750;&#29420;&#31435;&#19988;&#19981;&#21516;&#12289;&#39640;&#26041;&#24046;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26799;&#24230;&#22788;&#29702;&#12289;&#31649;&#36947;&#35757;&#32451;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#22120;&#32467;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.01470</link><description>&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Optimize for Reinforcement Learning. (arXiv:2302.01470v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01470
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#38754;&#20020;&#26799;&#24230;&#33539;&#22260;&#21464;&#21270;&#22823;&#12289;&#26799;&#24230;&#20998;&#24067;&#38750;&#29420;&#31435;&#19988;&#19981;&#21516;&#12289;&#39640;&#26041;&#24046;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26799;&#24230;&#22788;&#29702;&#12289;&#31649;&#36947;&#35757;&#32451;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#22120;&#32467;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#30340;&#25968;&#25454;&#12289;&#35745;&#31639;&#21644;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#25163;&#21160;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#19982;&#30417;&#30563;&#23398;&#20064;&#26412;&#36136;&#19978;&#19981;&#21516;&#65292;&#36825;&#20123;&#23398;&#20064;&#20248;&#21270;&#22120;&#22312;&#31616;&#21333;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#21457;&#29616;&#20102;&#19977;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26799;&#24230;&#22312;&#23545;&#25968;&#19978;&#21464;&#21270;&#33539;&#22260;&#24456;&#22823;&#65292;&#32780;&#22312;&#32477;&#23545;&#20540;&#19978;&#33539;&#22260;&#36739;&#23567;&#65292;&#36825;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#21442;&#25968;&#26356;&#26032;&#12290;&#20854;&#27425;&#65292;&#20195;&#29702;&#26799;&#24230;&#20998;&#24067;&#38750;&#29420;&#31435;&#19988;&#19981;&#21516;&#65292;&#23548;&#33268;&#20803;&#35757;&#32451;&#25928;&#29575;&#20302;&#19979;&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#20195;&#29702;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#39640;&#24230;&#38543;&#26426;&#20132;&#20114;&#65292;&#20195;&#29702;&#26799;&#24230;&#23384;&#22312;&#36739;&#39640;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#22686;&#21152;&#20102;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#22788;&#29702;&#12289;&#31649;&#36947;&#35757;&#32451;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#22120;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, by leveraging more data, computation, and diverse tasks, learned optimizers have achieved remarkable success in supervised learning, outperforming classical hand-designed optimizers. Reinforcement learning (RL) is essentially different from supervised learning and in practice these learned optimizers do not work well even in simple RL tasks. We investigate this phenomenon and identity three issues. First, the gradients of an RL agent vary across a wide range in logarithms while their absolute values are in a small range, making neural networks hard to obtain accurate parameter updates. Second, the agent-gradient distribution is non-independent and identically distributed, leading to inefficient meta-training. Finally, due to highly stochastic agent-environment interactions, the agent-gradients have high bias and variance, which increase the difficulty of learning an optimizer for RL. We propose gradient processing, pipeline training, and a novel optimizer structure wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#27169;&#22411;&#65288;NCN&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#26469;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#35299;&#20915;&#38142;&#36335;&#19981;&#23436;&#25972;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00890</link><description>&lt;p&gt;
&#20855;&#26377;&#23436;&#25104;&#21151;&#33021;&#30340;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Neural Common Neighbor with Completion for Link Prediction. (arXiv:2302.00890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#27169;&#22411;&#65288;NCN&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#26469;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#35299;&#20915;&#38142;&#36335;&#19981;&#23436;&#25972;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;vanilla&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#22312;&#21508;&#31181;&#22270;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#36890;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#21482;&#20351;&#29992;&#20004;&#20010;&#21333;&#29420;&#30446;&#26631;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#24182;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#12290;&#20026;&#20102;&#25429;&#33719;&#25104;&#23545;&#20851;&#31995;&#65292;&#19968;&#20123;&#27169;&#22411;&#23558;&#25163;&#21160;&#21151;&#33021;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#20013;&#65292;&#24182;&#20351;&#29992;MPNN&#30340;&#36755;&#20986;&#26469;&#29983;&#25104;&#25104;&#23545;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#20854;&#20182;&#20154;&#30452;&#25509;&#23558;&#25163;&#21160;&#21151;&#33021;&#29992;&#20316;&#25104;&#23545;&#34920;&#31034;&#12290;&#23613;&#31649;&#27492;&#31616;&#21270;&#36991;&#20813;&#20102;&#23558;GNN&#36880;&#20010;&#38142;&#25509;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#38142;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#30001;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#21644;&#19981;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#29305;&#24449;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#26377;&#24456;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#20445;&#25345;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#65288;NCN&#65289;&#65292;&#23427;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;NCN&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26410;&#35266;&#23519;&#21040;&#30340;&#38142;&#25509;&#38382;&#39064;&#12290;&#22270;&#30340;&#19981;&#23436;&#25972;&#24615;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#24182;&#23548;&#33268;&#20998;&#24067;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Despite its outstanding performance in various graph tasks, vanilla Message Passing Neural Network (MPNN) usually fails in link prediction tasks, as it only uses representations of two individual target nodes and ignores the pairwise relation between them. To capture the pairwise relations, some models add manual features to the input graph and use the output of MPNN to produce pairwise representations. In contrast, others directly use manual features as pairwise representations. Though this simplification avoids applying a GNN to each link individually and thus improves scalability, these models still have much room for performance improvement due to the hand-crafted and unlearnable pairwise features. To upgrade performance while maintaining scalability, we propose Neural Common Neighbor (NCN), which uses learnable pairwise representations. To further boost NCN, we study the unobserved link problem. The incompleteness of the graph is ubiquitous and leads to distribution shifts between
&lt;/p&gt;</description></item></channel></rss>